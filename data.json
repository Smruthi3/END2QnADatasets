[
   {
      "null": 0,
      "x": "Support multiple simultaneous LR schedulers",
      "z": "A solution was implemented in #26423. Closing this PR, please feel free to re-open if needed.",
      "y": "This issue has been fixed"
   },
   {
      "null": 1,
      "x": "Want RTX 2080ti Support!!! RuntimeError: cublas runtime error : the GPU program failed to execute at /opt/conda/",
      "z": "We'll supply PyTorch binaries w/ CUDA 10 in the next release. For now you can build from source, as mentioned by @jendrikjoe",
      "y": "You need to build from source"
   },
   {
      "null": 2,
      "x": "Crash when reading pandas parquet file after importing pyTorch",
      "z": "uninstall the pyarrow installed by pip and then reinstall with conda works for me.",
      "y": "You need to uninstall the pyarrow installed by pip and then reinstall with conda"
   },
   {
      "null": 3,
      "x": "RuntimeError: Only tuples, lists and Variables supported as JIT inputs, but got dict",
      "z": "Solved.Just change the output of your model from dict to list\n",
      "y": "You need to change the output of your model from dict to list."
   },
   {
      "null": 4,
      "x": "dataparallel not working on nvidia gpus and amd cpus",
      "z": "I believe this is a duplicate of #1637. Specifically, see [this comment](https://github.com/pytorch/pytorch/issues/1637#issuecomment-338268158). I've had success on a threadripper machine by disabling IOMMU or changing the IOMMU setting to 'soft'.",
      "y": "It works fine on threadripper machine by disabling IOMMU or changing the IOMMU setting to 'soft'."
   },
   {
      "null": 5,
      "x": "Matrix multiplication operator",
      "z": "this is now fixed in master\\n",
      "y": "This issue has been fixed"
   },
   {
      "null": 6,
      "x": "Remove dampening from SGD",
      "z": "Made default to 0.\nfixed via 4eb12a2",
      "y": "This issue has been fixed"
   },
   {
      "null": 7,
      "x": "ImportError: No module named _C",
      "z": "@szagoruyko Could you please try opening torch in any directory other than repo's root? It's trying to load the `torch` dir instead of the python package and gives you this error.\\n",
      "y": "You need to open torch in any directory other than repo's root"
   },
   {
      "null": 8,
      "x": "Install Error, OSX 10.11.6, fresh miniconda install",
      "z": "i think that's because you have CC and CXX set\\n",
      "y": "The error is because you have CC and CXX set"
   },
   {
      "null": 9,
      "x": "MaxUnpool2d segfaults for some configurations",
      "z": "Probably fixed in #207. I can't reproduce it anymore, so I'm closing the issue.\\n",
      "y": "This issue has been fixed"
   },
   {
      "null": 10,
      "x": "[legacy-nn] losses need to return tensors and not numbers",
      "z": "i dont think we should fix legacy-nn",
      "y": "legacy-nn should not be modified."
   },
   {
      "null": 11,
      "x": "Optim API: per-layer learning rates etc.",
      "z": "this is easily solved by gradient rescale function, specifying per-layer learning rate is actually more pain\\n",
      "y": "this can be solved by gradient rescale function"
   },
   {
      "null": 12,
      "x": "Add logical AND/OR/NOT/XOR operations",
      "z": "Implemented in #342.",
      "y": "This issue has been fixed"
   },
   {
      "null": 13,
      "x": "Printing tensors is sometimes very slow",
      "z": "I've reproduced this, looking into fixing it.\\n",
      "y": "This issue has been fixed"
   },
   {
      "null": 14,
      "x": "numpy.__config__.show() for torch",
      "z": "This has been included in 29ea086 by @ezyang.",
      "y": "This issue has been fixed"
   },
   {
      "null": 15,
      "x": "Deterministic cudnn algorithms",
      "z": "Parallel data loader is now deterministic.\\n",
      "y": "Parallel data loader is now deterministic."
   },
   {
      "null": 16,
      "x": "torch dot function consistent with numpy",
      "z": "This has been fixed via #1563 , if we consider that we should follow the behavior of numpy.matmul instead of numpy.dot, according to the thread pointed out by @ngimel.",
      "y": "This issue has been fixed"
   },
   {
      "null": 17,
      "x": "automatically assign attributes that are variable as parameters?",
      "z": "@glample There's a very simple reason why we don't want people to save Variables as attributes - if they were created as a result of some computation, they will be kept around and they won't free the graph nodes preceding it.\\n\\nWe wanted to think of Modules as thin wrappers around functions that only hold some persistent state like parameters, never any temporary objects (this is handy e.g. for serialization - there's no need for `clearState()`, etc.).\\n\\nI agree it's quite a nice patter, but I'm not sure if we should allow this. @colesbury, any thoughts?\\n",
      "y": "Variables are not saved as attributes since they will be kept around and they won't free the graph nodes preceding it."
   },
   {
      "null": 18,
      "x": "embeddings layer with IntTensor / cuda.IntTensor inputs",
      "z": "This was added in #46758 and is now working just fine on master for dtype=torch.int.",
      "y": "This issue has been fixed"
   },
   {
      "null": 19,
      "x": "requirements.txt: cffi >= v1.4.0",
      "z": "Fixed in #161.",
      "y": "This issue has been fixed"
   },
   {
      "null": 20,
      "x": "More optimizers in torch.optim",
      "z": "Yes, the we only have tests for legacy optim at the moment, and we recommend against using it (it's only for legacy.nn). Thanks for pointing out the naming, I'll change it. #5 is also incorrect, we probably checked it because we implemented trainers and datasets.\\n\\nTo train `torch.nn` networks you should use `torch.optim`. We're aware most algorithms are missing and we'll be implementing them sometime soon. Sorry for the delays.\\n",
      "y": "They will be added soon"
   },
   {
      "null": 21,
      "x": "error: \u2018float* cblas_sgemm_alloc(CBLAS_IDENTIFIER, int, int, int)\u2019 is deprecated caused by outdated MKL",
      "z": "This is oneapi-src/oneDNN#440. According to the MKL-DNN developers, this can occur if you're compiling against an outdated MKL library. Try updating your MKL.",
      "y": "You need to update your MKL."
   },
   {
      "null": 22,
      "x": "How can i convert \u2018at::Tensor\u2019 to \u2018const float*\u2019 in libtorch?",
      "z": "`.data<float>()`",
      "y": "This can be done by `.data<float>()`"
   },
   {
      "null": 23,
      "x": "Bug in CosineAnnealingLR (division by zero)",
      "z": "I've confirmed that reverting #14010 fixes the problem. I'm going to revert it for now and then investigate more.",
      "y": "This issue has been fixed"
   },
   {
      "null": 24,
      "x": "Incorrect behaviour of min() and argmin()",
      "z": "@Markus-Goetz repasting my comment from #17738 (comment)\n\nthe only determinism we aim to have is hashed on device, and for CPU, single-threaded.\nEven across different kinds of GPUs, all bets are off.\n\nWe cannot guarantee cross-device, cross-CPU-type determinism due to severe performance cliffs that'll result from such a constraint.\n\nFor example, to guarantee that we pick the first argmax (or argmin) element, we have to add an additional pass in our CUDA kernel to sort or order the results, which costs performance.\n\nThis is inconsistent with numpy's, Eigen's, C++ STL etc.\n\nIf you see, you have given examples of CPU kernels, where this is easy to guarantee without significant performance regression.",
      "y": "The behavior is consistent for CPU, but it maybe different for multi-GPUs"
   },
   {
      "null": 25,
      "x": "[autodiff] Fix owning model used in `differentiate`",
      "z": "We seem to have agreed to leave this as is.",
      "y": "This issue cannot be resolved"
   },
   {
      "null": 26,
      "x": "Conv3d fail after curtain batch size",
      "z": "I can confirm this is now resolved!",
      "y": "This issue has been fixed."
   },
   {
      "null": 27,
      "x": "[JIT] state[input] != State::Unknown ASSERT FAILED at /pytorch/torch/csrc/jit/passes/specialize_autogradzero.cpp:57",
      "z": "@YashSinha1996\\r\\n\\r\\nI also have been working on pytorch-pretrained-BERT.\\r\\nI think that the error reported in this issue can be avoided by using reshape instead of contiguous and view. (But I'm not very sure because I'm a novice at pytorch.)\\r\\n\\r\\nThe modification could be\\r\\n```\\r\\n context_layer = torch.matmul(attention_probs, value_layer)\\r\\n- context_layer = context_layer.permute(0, 2, 1, 3).contiguous()\\r\\n+ context_layer = context_layer.permute(0, 2, 1, 3)\\r\\n new_context_layer_shape = context_layer.size()[:-2] + (self.all_head_size,)\\r\\n- context_layer = context_layer.view(*new_context_layer_shape)\\r\\n+ context_layer = context_layer.reshape(*new_context_layer_shape)\\r\\n return context_layer\\r\\n```\\r\\nin pytorch_pretrained_bert/modeling.py\\r\\n",
      "y": "This error can be avoided by using reshape instead of contiguous and view. "
   },
   {
      "null": 28,
      "x": "librosa tests on Windows don't work",
      "z": "I'm now fixing it with `conda install numba`, which will also install llvmlite.",
      "y": "Please use `conda install numba`"
   },
   {
      "null": 29,
      "x": "Batch Convolutional Layers - Similar to torch.bmm but for convolutional operators",
      "z": "This looks addressed, so I'm going to close it. Feel free to reopen if I'm mistaken.",
      "y": "This issue has been fixed."
   },
   {
      "null": 30,
      "x": "[jit] support general buffer mutation",
      "z": "Okay, assigning `self.vector = u.data` works. Since `u` had a gradient, that meant that for subsequent passes `self.vector` had a gradient, which didn't work well.",
      "y": "This issue can be resolved by `self.vector = u.data` "
   },
   {
      "null": 31,
      "x": "libtorch C++ library does not compile properly",
      "z": "@JerryShih Thanks! `export LD_LIBRARY_PATH=/home/bobw/pytorch/torch/lib/` (libiomp5.so dir) fixes it for me",
      "y": "Please set the variable `export LD_LIBRARY_PATH=/home/bobw/pytorch/torch/lib/` (libiomp5.so dir)"
   },
   {
      "null": 32,
      "x": "Excessive call to cudaGetDevice and cudaSetDevice",
      "z": "@omry you don't even need to change PyTorch source code. You can just hot patch the CUDA functions to be no-ops:\n\nhttps://gist.github.com/colesbury/b10069870419ca1fa9c3a2a8668edbe3\n\nYou'll see that outside the profiler the cudaGetDevice and cudaSetDevice functions do not affect performance. You can also use this under nvprof to avoid API tracing those functions.",
      "y": "Please use the hotfix here https://gist.github.com/colesbury/b10069870419ca1fa9c3a2a8668edbe3"
   },
   {
      "null": 33,
      "x": "[Tutorial]: Wrong example of Our Own Ring-Allreduce",
      "z": "@rohan-varma How about the following code? I think it provides the same functionality and is easier to understand.\\r\\n```python\\r\\nsend_req = dist.isend(send_buff, right)\\r\\ndist.recv(recv_buff, left) # recv is a blocking operation.\\r\\naccum[:] += recv_buff[:]\\r\\nsend_buff[:] = recv_buff[:]\\r\\n```",
      "y": "The following example is better \\r\\n```python\\r\\nsend_req = dist.isend(send_buff, right)\\r\\ndist.recv(recv_buff, left) # recv is a blocking operation.\\r\\naccum[:] += recv_buff[:]\\r\\nsend_buff[:] = recv_buff[:]\\r\\n```"
   },
   {
      "null": 34,
      "x": "the example program using libtorch is not linked against torch_cuda when USE_CUDA is ON",
      "z": "I just found out that this issue could be solved by using the `/INCLUDE` switch. torch_cuda.dll contains the function signature `?warp_size@cuda@at@@YAHXZ` (`int __cdecl at::cuda::warp_size(void)`). So adding `/INCLUDE:\\\"?warp_size@cuda@at@@YAHXZ\\\"` to the linker flags will force the library/executable to link against `torch_cuda.dll`. What do you think, @ezyang ?\\r\\n\\r\\nReference:\\r\\nhttps://docs.microsoft.com/en-us/cpp/build/reference/include-force-symbol-references?view=vs-2019",
      "y": "This issue could be solved by using the `/INCLUDE` switch"
   },
   {
      "null": 35,
      "x": "PytorchStreamReader failed reading zip archive: failed finding central directory (no backtrace available)",
      "z": "In my case, this error was caused by a corrupted saved file. So I switch to older checkpoints and the problem is gone.",
      "y": "Switching to older checkpoints can fix the problem"
   },
   {
      "null": 36,
      "x": "test_conv_transposed_large_cuda failed on Windows",
      "z": "The failure is seen in CI.",
      "y": "The failure is seen in CI."
   },
   {
      "null": 37,
      "x": "[docs] torch.onnx.export docs contains two descriptions for example_outputs arg",
      "z": "Fixed in #31826",
      "y": "This issue has been fixed."
   },
   {
      "null": 38,
      "x": "The Feature Request of Loading Quantized TorchScript Model on Windows with libtorch",
      "z": "Closing it because the original issue should be resolved. If users want 32-bit quantization support, they can open a new issue.",
      "y": "This issue has been fixed."
   },
   {
      "null": 39,
      "x": "Pip torch_nightly on macOS installs wrong build",
      "z": "I believe this should now be resolved for `macOS`:\\r\\n\\r\\n```\\r\\npytorch37 \u276f pip install --pre torch -f https://download.pytorch.org/whl/nightly/cpu/torch_nightly.html\\r\\nLooking in links: https://download.pytorch.org/whl/nightly/cpu/torch_nightly.html\\r\\nCollecting torch\\r\\n Downloading https://download.pytorch.org/whl/nightly/cpu/torch-1.5.0.dev20200110-cp37-none-macosx_10_9_x86_64.whl (82.4MB)\\r\\n```\\r\\n\\r\\nNot entirely sure about where we are on windows nightly builds though, cc @peterjc123",
      "y": "This issue has been fixed"
   },
   {
      "null": 40,
      "x": "how to set cuda stream by call Aten function",
      "z": "1. CUDAStreamGuard will reset the stream to the previously used stream once it goes out of scope, if instead you are using stream manually, you may forget to change it back, thus unexpectedly changing current stream for the user. \\r\\n2. Using setCurrentCUDAStream won't influence operations running on other streams. You need to make sure though that the default stream is synchronized with the stream your operation is running on. In many cases, using concurrent streams does not provide appreciable benefits.",
      "y": "1. CUDAStreamGuard will reset the stream to the previously used stream once it goes out of scope, if instead you are using stream manually, you may forget to change it back, thus unexpectedly changing current stream for the user. \\r\\n2. Using setCurrentCUDAStream won't influence operations running on other streams. You need to make sure though that the default stream is synchronized with the stream your operation is running on. In many cases, using concurrent streams does not provide appreciable benefits."
   },
   {
      "null": 41,
      "x": "`enable_grad` context doesn't work as expected in backward function of torch.autograd.Function",
      "z": "Thanks for the report, I will take a look why this happens. It might simply be that we don't restore the status in `autograd.grad` properly.",
      "y": "This error is because we don't restore the status in `autograd.grad` properly."
   },
   {
      "null": 42,
      "x": "dyld: Library not loaded: /usr/local/opt/openssl/lib/libssl.1.0.0.dylib Referenced from: /usr/local/bin/sccache",
      "z": "For posterity this is what CircleCI responded with:\\r\\n\\r\\n----\\r\\n\\r\\nWe have looked into this issue and have found that this is being caused by a dependency conflict.\\r\\n\\r\\nTo install libomp requires the following dependencies (taken from the build log):\\r\\n\\r\\n==> Installing dependencies for libomp: pkg-config, gdbm, openssl@1.1, readline, sqlite, xz, python, sphinx-doc and cmake\\r\\nThis package is specifically installing OpenSSL 1.1 and higher. When homebrew installs OpenSSL, it will change the symlinks in \\r\\n/usr/local/opt/openssl/ to point to the latest version (which truly resides in \\r\\n/usr/local/opt/openssl@1.1). This in turns means the file \\r\\n/usr/local/opt/openssl/lib/libssl.1.0.0.dylib is no longer in that directory as it has been replaced by \\r\\nlibssl.1.1.dylib.\\r\\n\\r\\nThe previous version of OpenSSL (1.0) can be found in \\r\\n/usr/local/opt/openssl@1.0/ which is where the previous version of the dylib, which is trying to be used, can be found (libssl.1.0.0.dylib).\\r\\n\\r\\nYour script seems to be using something that is trying to specifically use this version of the dylib. Best practise would be to instead use \\r\\n/usr/local/opt/openssl/lib/libssl.dylib which will always point to the latest version of openssl as it is not requesting a specific version. You may need to check the dependencies of the packages you are using and make sure to pin specific versions of these packages to prevent this kind of conflict.\\r\\n\\r\\nDo you know if any packages you use have been updated recently?",
      "y": "You may need to check the dependencies of the packages you are using and make sure to pin specific versions of these packages to prevent this kind of conflict."
   },
   {
      "null": 43,
      "x": "Distributed Using Gloo on Multiple Nodes Does not Work",
      "z": "I think that would create problems with the ranks. For worker_id 0, the ranks would be 0 and 1 and for worker_id 1 the ranks would be 1 and 2. Can you try with worker ids 0 and 2? Your ranks should be 0, 1, 2, 3 for world size 4.",
      "y": "Your ranks should be 0, 1, 2, 3 for world size 4."
   },
   {
      "null": 44,
      "x": "The guidelines for loading a PyTorch model in C++ do not work on Windows",
      "z": "You'll need to copy the PDBs along with your executable.",
      "y": "You'll need to copy the PDBs along with your executable."
   },
   {
      "null": 45,
      "x": "Fix 1.3.1 branch submodule fbjni dependency",
      "z": "Now that this has stayed unchanged for longer and `v1.5.0` is out, moving the tag seems less needed and less attractive (EDIT: and it can always still be done in the future). \\r\\n\\r\\nI propose to close this - will do so next week unless someone disagrees.",
      "y": "This issue has been fixed."
   },
   {
      "null": 46,
      "x": "Connection closed by peer when using L-BFGS and distributed computing (gloo)",
      "z": "Excellent!\n\nIt should be possible to come up with an optimizer that is distributed-aware, where all these decisions are made globally instead of by a single machine. Then you can use it in a distributed setting, but will have to average losses, or pick one of the machines are the primary.",
      "y": "It should be possible to come up with an optimizer that is distributed-aware, where all these decisions are made globally instead of by a single machine. Then you can use it in a distributed setting, but will have to average losses, or pick one of the machines are the primary."
   },
   {
      "null": 47,
      "x": "`torch.Size` is tranfered to`torch.Tensor`, values don't equal",
      "z": "`torch.Tensor(shape)` creates an empty tensor of shape `shape`. Don't use deprecated uppercase `Tensor`. Use `torch.tensor(shape)` to create a tensor containing the same value as `shape`.",
      "y": "`torch.Tensor(shape)` creates an empty tensor of shape `shape`. Don't use deprecated uppercase `Tensor`. Use `torch.tensor(shape)` to create a tensor containing the same value as `shape`."
   },
   {
      "null": 48,
      "x": "Why when I use torch.cuda.empty_cache(), it cost some gpu memory on other device?",
      "z": "@peterzpy see [here](https://discuss.pytorch.org/t/out-of-memory-when-i-use-torch-cuda-empty-cache/57898/3) and [here](https://github.com/pytorch/pytorch/issues/25752#issuecomment-528866347) for a solution. Basically, you need to specify the gpu device, before calling to `empty_cache`.",
      "y": "You need to specify the gpu device, before calling to `empty_cache`."
   },
   {
      "null": 49,
      "x": "Ruby Library",
      "z": "Also added a Homebrew formula so users can now do:\\r\\n\\r\\n```sh\\r\\nbrew install libtorch\\r\\n```\\r\\n\\r\\nhttps://github.com/Homebrew/homebrew-core/pull/47222",
      "y": "Users can execute :\\r\\n\\r\\n```sh\\r\\nbrew install libtorch\\r\\n```"
   },
   {
      "null": 50,
      "x": "[jit] Spurious error when type comments are found in the body of a function.",
      "z": "Oh sorry I see it has been fixed.",
      "y": "This issue has been fixed"
   },
   {
      "null": 51,
      "x": "[jit] Cannot create a `Tuple[List[T]]`",
      "z": "This is expected behavior. In python tuple(x) would return a tuple of [T, ...] which is an unknown length. In the second example you're creating a Tuple[List[T]], which has a fixed length so we can compile it.",
      "y": "This is expected behavior. In python tuple(x) would return a tuple of [T, ...] which is an unknown length."
   },
   {
      "null": 52,
      "x": "RuntimeError: stack.size() >= num_inputs INTERNAL ASSERT FAILED",
      "z": "Closed with #31724",
      "y": "This issue has been fixed"
   },
   {
      "null": 53,
      "x": "failed to convert torch.jit.ScriptModule to ONNX (crash)",
      "z": "@linronmo, as I explained above, the change for flatten could only be done for opset 11.\nSo you can export your model in opset_version=11, for that, you should use the parameter opset_version=11 in the exporter api with PyTorch nighly: torch.onnx.export(..., opset_version=11).",
      "y": "Please specify the parameter `torch.onnx.export(..., opset_version=11).`"
   },
   {
      "null": 54,
      "x": "[Bug report] RuntimeError: backward_input can only be called in training mode",
      "z": "@simon555 You only set training mode before the loop, but set eval mode in generate_predictions, which is called in the middle of the loop. So iterations after generate_predictions will fail.",
      "y": "The mode i.e. train/eval needs to be consistent throughout the training loop."
   },
   {
      "null": 55,
      "x": "[Caffe2] Windows build errors in generated file caffe2.pb.h",
      "z": "I remember I've seen this issue before.\\r\\n\\r\\nIt's related to a funny combination of `nvcc` and `protobuf` actually if my memory is correct.\\r\\n`PROTOBUF_CONSTEXPR` is defined to constexpr.\\r\\nHowever in nvcc 9.1 you need to get rid of it in order to do in-place initialization.\\r\\n\\r\\nSo...the solution is to patch protobuf...\\r\\n\\r\\nBTW this issue is on protobuf 3.5.\\r\\n`master` branch of protobuf should not have this issue but it has something else...",
      "y": "It's related to a funny combination of `nvcc` and `protobuf` actually if my memory is correct.\\r\\n`PROTOBUF_CONSTEXPR` is defined to constexpr.\\r\\nHowever in nvcc 9.1 you need to get rid of it in order to do in-place initialization.\\r\\n\\r\\nSo...the solution is to patch protobuf..."
   },
   {
      "null": 56,
      "x": "Segmentation fault on importing torch",
      "z": "I assume `pip install git+https://github.com/mwydmuch/ViZDoom` compiles ViZDoom, correct? In that case, since your system compiler is GCC 4.84, it will be incompatible with PyTorch bindings, since we compile with GCC 4.9 and the two are ABI incompatible, which leads to segfaults.\\r\\n\\r\\nYou should either:\\r\\nA) Compile PyTorch from source, with your system compiler. Then both projects will have been compiled with the same compiler and the problem goes away.\\r\\nB) Install GCC 4.9 ([see instructions here](https://gist.github.com/goldsborough/d466f43e8ffc948ff92de7486c5216d6)) and set the `CXX` environment variable before compiling VizDoom (`export CXX=g++-4.9`). Then VizDoom should be compiled with the same compiler as PyTorch and the problem should go away.\\r\\n\\r\\nEither way, you have to make sure both are compiled with the same compiler version.",
      "y": "You have to make sure both are compiled with the same compiler version. \\n Either compile PyTorch from source or install  GCC 4.9"
   },
   {
      "null": 57,
      "x": "[feature request] [pytorch] Convenience method for doing unsqueeze / squeeze several times",
      "z": "Why not using numpy manner? \\r\\n`x[..., None, None]` and `x[..., 0,0]` for your example",
      "y": "Use the following \\r\\n`x[..., None, None]` and `x[..., 0,0]` in your example"
   },
   {
      "null": 58,
      "x": "np.repeat vs torch.repeat",
      "z": "Easiest thing to do for now is to add a warning to the docs. One day we'll probably re-visit the all the differences between the Numpy API and pytorch API, but for now, just changing repeat might be disruptive to our users.\n\nFeel free to submit a pull request @PetrochukM (otherwise, I can submit one).",
      "y": "The two APIs have a different behavior."
   },
   {
      "null": 59,
      "x": "[JIT]torch._C._infer_size throws an exception when traced",
      "z": "Got it! I think this should work fine for most of our use cases, since this is only called when distribution instances are being constructed, after which the parameters are frozen and should have a fixed shape. One possible downside might be that we will need to pass tensors with the correct shape in the torch.trace annotation, and that can get tricky if it depends on the minibatch size. So eventually, it will be nice to have a more generic support for JIT. cc. @fritzo, @eb8680.",
      "y": "You need to pass tensors with the correct shape in the torch.trace annotation"
   },
   {
      "null": 60,
      "x": "question: where (if) are the caffe2 libraries?",
      "z": "Caffe2 only has packages in conda at the moment. 'conda install -c caffe2 caffe2' will install the Caffe2 libraries into an Anaconda env; they will be linked against other libraries in the Anaconda env as well.",
      "y": "To install the Caffe2 libraries, use  'conda install -c caffe2 caffe2' "
   },
   {
      "null": 61,
      "x": "failed to move parameters to GPU",
      "z": "or make self.conv = nn.ModuleList()",
      "y": "To fix this, use `self.conv = nn.ModuleList()`"
   },
   {
      "null": 62,
      "x": "Inconsistent behavior of F.conv2d(...,padding) and F.pad",
      "z": "Got it. I think you mean `torch.backends.cudnn.deterministic=True`",
      "y": "You need to set `torch.backends.cudnn.deterministic=True`"
   },
   {
      "null": 63,
      "x": "[Caffe2] ld: can't map file, errno=22 file '/usr/local/cuda/lib/stubs/cuda.framework' for architecture x86_64",
      "z": "You'll need to do a clean build from scratch. Also, you'll still need a cuda.framework fix of some sort or another. Working on a more robust version of this fix.",
      "y": "You'll need to do a clean build from scratch"
   },
   {
      "null": 64,
      "x": "[feature request] Clarify document to avoid \\\"Error Importing cuda extension\\\"",
      "z": "Once your extension is built, you can simply import it in Python, using the name you specified in your setup.py script. Just be sure to import torch first, as this will resolve some symbols that the dynamic linker must see:",
      "y": "You need to import torch first, as this will resolve some symbols that the dynamic linker must see:"
   },
   {
      "null": 65,
      "x": "RuntimeError: cuda runtime error (30) : unknown error at /pytorch/aten/src/THC/THCGeneral.cpp:70",
      "z": "is there something in your LD_LIBRARY_PATH that might override general RPATH? For example do you have cuda 9.2 in your LD_LIBRARY_PATH?",
      "y": "Please ensure that there isn't any path in LD_LIBRARY_PATH that might override general RPATH"
   },
   {
      "null": 66,
      "x": "[pytorch] [feature request] Add torch.broadcast (e.g. for using with torch.stack)",
      "z": "We have a helper `torch.distributions.utils.broadcast_all()` that should allow you to\\r\\n```py\\r\\ntorch.stack(broadcast_all(a, b))\\r\\n```",
      "y": "The helper function `torch.distributions.utils.broadcast_all()` that should allow you to\\r\\n```py\\r\\ntorch.stack(broadcast_all(a, b))\\r\\n```"
   },
   {
      "null": 67,
      "x": "Python interpreter died without Traceback when CPU Tensor divided by zero.",
      "z": "As tricky as this may be to fix, it's still a bug in a very real sense. Allowing C errors to creep into Python code is the result of a leaky abstraction.",
      "y": "This error is the result of a leaky abstraction."
   },
   {
      "null": 68,
      "x": "Inconsistent gradient results in F.grid_sample using torch.autograd.grad with create_graph=True",
      "z": " confirmed with @apaszke as well that this is expected behavior.\n\nHe says:\n\nbecause create_graph=True, this is expected.\nIn the a+b case, basically the gradient of the gradient (wrt parameters) is all 0, which means it doesn\u2019t require grad, because it effectively never touched a single variable that does require grad.",
      "y": "This is an expected behavior and not a bug."
   },
   {
      "null": 69,
      "x": "AttributeError: module 'torch._C' has no attribute '_TensorBase'",
      "z": "Can you try uninstall and reinstall? I'm assuming that you built from source. Can you also check if you have binary pytorch installed?",
      "y": "Try to reinstall and make sure that you have binary pytorch installed."
   },
   {
      "null": 70,
      "x": "Feature request: Object detection model zoo",
      "z": "Torchvision now has models for object detection and semantic segmentation officially supported",
      "y": "Torchvision now has models for object detection and semantic segmentation officially supported"
   },
   {
      "null": 71,
      "x": "dynamically change tensor with requires_grad=False by \\\"+=\\\" cause error but \\\"+\\\" doesn't",
      "z": "`x = x + y` is something very different from `x += y` for Python semantics. The latter creates a new variable, the former modifies the variable in place.",
      "y": "`x = x + y` is something very different from `x += y` for Python semantics. The latter creates a new variable, the former modifies the variable in place."
   },
   {
      "null": 72,
      "x": "python setup.py install failed. undefined references",
      "z": "We had the same issue. Problem was the version of MKL which was not good. Just running:\\r\\n\\r\\n```\\r\\nconda remove mkl mkl-include\\r\\nconda install numpy pyyaml mkl=2019.3 mkl-include setuptools cmake cffi typing\\r\\n```\\r\\n\\r\\nbefore the installation fixed the issue.",
      "y": "You need to run :\\r\\n\\r\\n```\\r\\nconda remove mkl mkl-include\\r\\nconda install numpy pyyaml mkl=2019.3 mkl-include setuptools cmake cffi typing\\r\\n```\\r\\n\\r\\nbefore the installation to fix the issue."
   },
   {
      "null": 73,
      "x": "THCudaCheck FAIL file=..\\\\aten\\\\src\\\\THC\\\\THCGeneral.cpp line=87 error=30 : unknown error",
      "z": "somehow restarting my machine fixed this issue. using torch v1.1.0",
      "y": "Restarting the machine can fix this issue."
   },
   {
      "null": 74,
      "x": "torchfile.T7ReaderException: unknown object type / typeidx: -1112529805",
      "z": "Please try appending the parameter long_size like this `load_lua(....., long_size=8)`.",
      "y": "You need to append the parameter long_size like this `load_lua(....., long_size=8)`."
   },
   {
      "null": 75,
      "x": "RuntimeError: CUDA error: unknown error",
      "z": "I had exactly the same issue. i got it fixed by installing the right pytorch version for my 10.1 CUDA with:\\r\\n`conda install pytorch torchvision cudatoolkit=10.1 -c pytorch`\\r\\ninstead of installing it with the installtion snippet generated for me by pytorch website:\\r\\n`conda install pytorch torchvision cudatoolkit=10.0 -c pytorch`\\r\\n",
      "y": "You need to install the right pytorch version for 10.1 CUDA with:\\r\\n`conda install pytorch torchvision cudatoolkit=10.1 -c pytorch`\\r\\ninstead of installing it with the installtion snippet generated by pytorch website:\\r\\n`conda install pytorch torchvision cudatoolkit=10.0 -c pytorch`\\r\\n"
   },
   {
      "null": 76,
      "x": "optim.lr_scheduler.CyclicLR (master only: not released) is buggy when not using momentum",
      "z": "I think you are correct, if you indent the second and third to last lines in the init it works.",
      "y": "It can be fixed by indenting the second and third to last lines in the init"
   },
   {
      "null": 77,
      "x": "Different deterministic behavior between CPU and CUDA for orthogonal initialization",
      "z": "This is expected. We do not guarantee that the random sequences generated on different devices will look the same. For performance reasons they are likely using different pseudo random generators, so it's almost impossible to make them equal.",
      "y": "This is expected. It is not guaranteed that the random sequences generated on different devices will look the same."
   },
   {
      "null": 78,
      "x": "C++ torch::Tensor serialization",
      "z": "Cool!! I just checked that I can both save and load torch::Tensors and std::vector<torch::Tensor>s with the latest macOS libtorch! Closing this now :)",
      "y": "Using  the latest macOS libtorch, one can both save and load torch::Tensor and std::vector<torch::Tensor>"
   },
   {
      "null": 79,
      "x": "Compiling from master yields std::runtime_error",
      "z": "@pietern Thanks for tracking this.\n\nYet, I've been able \"overcome\" this somehow by compiling PyTorch again with a git clean -fdx in between, thus I don't have the logs for the failed compilation.\n\nI just tried again with the same command line I ran this morning and the error doesn't occur. I'll investigate a little bit more on my side, to see if I can reproduce this, and will follow up on this topic.",
      "y": "You need to compile PyTorch again with a git clean -fdx in between, "
   },
   {
      "null": 80,
      "x": "Improved documentation of distributed launch utility",
      "z": "Thanks for reporting! We are aware of this confusing doc issue and are actively working on remediating it. Please track our progress in https://github.com/pytorch/pytorch/issues/60754. Closing in favor of 60754.",
      "y": "We are working on this."
   },
   {
      "null": 81,
      "x": "test_dataloader.py fails to pass test with error: Can't get attribute 'RandomDataset'... on MacOS",
      "z": "Created a conda env with Python=3.7, followed the same steps to install Pytorch and tried to reproduce the issue.\\r\\n\\r\\ntest_dataloader.py passes the test.\\r\\n\\r\\nSeems that it is an issue related to Python 3.8 on Mac",
      "y": "This is an issue related to Python 3.8 on Mac"
   },
   {
      "null": 82,
      "x": "[package] error in colab tutorial #60189",
      "z": "This error is intentional\u2014you need to specify how you will handle your dependencies. Further along in the tutorial there is guidance on how to resolve the error.",
      "y": "This error is intentional\u2014you need to specify how you will handle your dependencies. Further along in the tutorial there is guidance on how to resolve the error."
   },
   {
      "null": 83,
      "x": "USE_SYSTEM_ONNX: onnx/optimizer/optimize.h: No such file or directory",
      "z": "I got the same error, and I fixed it by updating the submodules:\\r\\n```\\r\\ngit submodule sync\\r\\ngit submodule update --init --recursive\\r\\n```",
      "y": "You need to update the submodules:\\r\\n```\\r\\ngit submodule sync\\r\\ngit submodule update --init --recursive\\r\\n```"
   },
   {
      "null": 84,
      "x": "Can I train AI If AI model is located in the another model\u2019s forward?",
      "z": "You may want to add model2 to an attribute of model1 in \\\\_\\\\_init\\\\_\\\\_. Then the parameters of model1 include that of model2 and so model2 would be trained.\\r\\n\\r\\nfyi questions are supposed to be posted to https://discuss.pytorch.org/ as it's more likely you will get help there.",
      "y": "You may want to add model2 to an attribute of model1 in \\\\_\\\\_init\\\\_\\\\_. Then the parameters of model1 include that of model2 and so model2 would be trained.\\r\\n\\r\\n"
   },
   {
      "null": 85,
      "x": "M1 Mac: `torch.dot()` returns unexpeted values for tensors of `torch.float32`",
      "z": "Will be fixed in next nightly build",
      "y": "This will be fixed in the next nightly build."
   },
   {
      "null": 86,
      "x": "it seems n_heads is not handled correctly in nn.MultiheadAttention",
      "z": "As a simple workaround, couldn't you pass embed_dim * num_heads as the embed_dim? When it is divided amongst the heads, it should result in what you want.",
      "y": "You can pass embed_dim * num_heads as the embed_dim"
   },
   {
      "null": 87,
      "x": "`test_transpose_inplace_view_xla` & `test_t_inplace_view_xla` are flaky",
      "z": "@imaginary-person This test was failing consistently on master yesterday so I disabled them for XLA first. @bdhirsh and I will look into the root cause later today. Thanks for reporting!",
      "y": "You need to diable them for XLA."
   },
   {
      "null": 88,
      "x": "Does the NCCL operation use the default stream as other computations?",
      "z": "The nccl implementation under the hood uses a custom stream pool and each time you launch collective operations, appropriate synchronizations are done automatically. So before the allreduce is launched, the nccl stream synchronizes with the default stream to capture computations appropriately. Also, after nccl allreduce is done, the default stream synchronizes with the nccl stream. As a result, you don't need to perform any synchronizations.\\r\\n\\r\\nIf you provide `async_op=True` to all_reduce, the synchronization will not be done after allreduce and only when you call `work.wait()`. So if you want to overlap computation that is not related to nccl allreduce, you can use the async_op option.",
      "y": "If you provide `async_op=True` to all_reduce, the synchronization will not be done after allreduce and only when you call `work.wait()`. So if you want to overlap computation that is not related to nccl allreduce, you can use the async_op option."
   },
   {
      "null": 89,
      "x": "Add container for recurrent nets",
      "z": "No, definitely not to `Module`. We can consider adding a base class for RNNs",
      "y": "No, definitely not to `Module`. We can consider adding a base class for RNNs"
   },
   {
      "null": 90,
      "x": "Support for einsum notation",
      "z": "Yeah, Pytorch lacks this. I can implement the feature",
      "y": "Support for einsum notation has been added"
   },
   {
      "null": 91,
      "x": "GPU usage extremely in-balance for segmentation task",
      "z": "The code hasn't been released yet. You can leave the device_ids empty and use CUDA_VISIBLE_DEVICES=0,1 to control the number of GPUs",
      "y": "Leave the device_ids empty and use CUDA_VISIBLE_DEVICES=0,1 to control the number of GPUs"
   },
   {
      "null": 92,
      "x": "How to select GPU programmatically in code",
      "z": "```\\r\\nimport os\\r\\nos.environ[\\\"CUDA_DEVICE_ORDER\\\"]=\\\"PCI_BUS_ID\\\" \\r\\nos.environ[\\\"CUDA_VISIBLE_DEVICES\\\"]=\\\"1\\\"\\r\\n```\\r\\n\\r\\nAre you looking for this?",
      "y": "```\\r\\nimport os\\r\\nos.environ[\\\"CUDA_DEVICE_ORDER\\\"]=\\\"PCI_BUS_ID\\\" \\r\\nos.environ[\\\"CUDA_VISIBLE_DEVICES\\\"]=\\\"1\\\"\\r\\n```\\r\\n"
   },
   {
      "null": 93,
      "x": "Pooling throws an exception in Tegra TX1",
      "z": "this is a weird error. Are you sure CuDNN is being detected at compile time?\n\nYou might have to set CUDNN_INCLUDE_DIR and CUDNN_LIB_DIR environment variables.\nYou can check cudnn's status with: print(torch.backends.cudnn.enabled)",
      "y": "You might have to set `CUDNN_INCLUDE_DIR` and `CUDNN_LIB_DIR` environment variables.\n"
   },
   {
      "null": 94,
      "x": "[Feature Request] Cyclical Learning Rates",
      "z": "Hi. I am the puller of LR Scheduler. IMO, you should be able to easily implement this using class LambdaLR. @ajbrock\n\n",
      "y": "This can be easily implemented using class `LambdaLR`"
   },
   {
      "null": 95,
      "x": "Get a single batch from DataLoader without iterating",
      "z": "`next(iter(data_loader))` ?",
      "y": "Use `next(iter(data_loader))` "
   },
   {
      "null": 96,
      "x": "Different behaviour of BCEWithLogitsLoss and BCELoss + Sigmoid",
      "z": "Thanks for reporting this @martinarjovsky.\n\n@soumith not sure this is a valid use of the losses - i.e. the output and target have different shapes (one is batched and one is not)?\n\nBecause of the different shapes the broadcasting has side effects (https://github.com/gchanan/pytorch/wiki/Broadcasting-Notes#backwards-compatibility)\n\nAccording to the docstrings of binary_cross_entropy and binary_cross_entropy_with_logits the input and target should have the same shape. Shall we just add a check and raise a ValueError if they are not? Happy to send a PR for this",
      "y": "The behavior is same if `binary_cross_entropy` and `binary_cross_entropy_with_logits` the input and target should have the same shape"
   },
   {
      "null": 97,
      "x": "[pylint] E1101:Module 'torch' has no 'squeeze' member",
      "z": "On VS code:\nAdd \"python.linting.enabled\": false to the settings file. Worked for me.",
      "y": "You need to change the settings for your editor. On VS code, you need to add \"python.linting.enabled\": false to the settings file."
   },
   {
      "null": 98,
      "x": "[Feature Request] Layer Normalization",
      "z": "I use this:\\r\\n\\r\\n```python\\r\\nclass LayerNorm(nn.Module):\\r\\n\\r\\n def __init__(self, features, eps=1e-6):\\r\\n super().__init__()\\r\\n self.gamma = nn.Parameter(torch.ones(features))\\r\\n self.beta = nn.Parameter(torch.zeros(features))\\r\\n self.eps = eps\\r\\n\\r\\n def forward(self, x):\\r\\n mean = x.mean(-1, keepdim=True)\\r\\n std = x.std(-1, keepdim=True)\\r\\n return self.gamma * (x - mean) / (std + self.eps) + self.beta\\r\\n```",
      "y": "This code segment can acheive layer normalization:\\r\\n\\r\\n```python\\r\\nclass LayerNorm(nn.Module):\\r\\n\\r\\n def __init__(self, features, eps=1e-6):\\r\\n super().__init__()\\r\\n self.gamma = nn.Parameter(torch.ones(features))\\r\\n self.beta = nn.Parameter(torch.zeros(features))\\r\\n self.eps = eps\\r\\n\\r\\n def forward(self, x):\\r\\n mean = x.mean(-1, keepdim=True)\\r\\n std = x.std(-1, keepdim=True)\\r\\n return self.gamma * (x - mean) / (std + self.eps) + self.beta\\r\\n```"
   },
   {
      "null": 99,
      "x": "Import fails after Conda install",
      "z": "your `cudatoolkit` from anaconda is build 2, which had a bug. Anaconda team fixed this in build 3. Do this: `conda install cudatoolkit`, and that should install build 3.",
      "y": "You need to install the latest build via `conda install cudatoolkit`"
   },
   {
      "null": 100,
      "x": "ReduceOps are breaking Pyro test",
      "z": "I found the bug, I'll send a patch soon.",
      "y": "This issue has been fixed."
   },
   {
      "null": 101,
      "x": "Linux CPU build script fails as MKL header files not found",
      "z": "If you are not using conda, pip install mkl-devel gets you the headers. I haven't looked at your CI script. @SsnL can we add this to the error message?\n\n",
      "y": "To install the header files, use `pip install mkl-devel`"
   },
   {
      "null": 102,
      "x": "RuntimeError: value cannot be converted to type uint8_t without overflow: 10000",
      "z": "yeah the tutorials needs to be updated. for now, you can try this:\\r\\n```\\r\\ncorrect += (predicted == labels).sum().item()\\r\\n```",
      "y": "You can use this:\\r\\n```\\r\\ncorrect += (predicted == labels).sum().item()\\r\\n```"
   },
   {
      "null": 103,
      "x": "CUDNN_STATUS_NOT_INITIALIZED when built from source",
      "z": "After these updates:\\r\\n- Nvidia driver: 384.81-> 387.26 \\r\\n- CUDA: V9.0.176 -> V9.1.85\\r\\n- CuDNN: 7005 -> 7102\\r\\n\\r\\nThe installation completes without error. Thanks!",
      "y": "You need to install the latest versions of Nvidia driver, CUDA, and CuDNN"
   },
   {
      "null": 104,
      "x": "I cannot initialize Tensor. (They will become torch.autograd.variable.Variable)",
      "z": "See https://github.com/pytorch/pytorch/pull/5225",
      "y": "This issue has been fixed."
   },
   {
      "null": 105,
      "x": "[feature request] Add underscore to nn.init functions",
      "z": "That makes sense, but we need to keep backward compatibility, so we need the non-`_` aliases too.",
      "y": "Non-underscore functions have been retained for backward compatibility. "
   },
   {
      "null": 106,
      "x": "How can I access the model's attribution created during forward pass when using dataparallel?",
      "z": "Returning what you want from the top-level forward function should defiantly work. You could do this pretty easily with an additional wrapper. As such:\\r\\n\\r\\n```python\\r\\n\\r\\nclass NormalModel(nn.Module):\\r\\n pass\\r\\n\\r\\nclass ExtraOutputWrapper(nn.Module):\\r\\n def __init__(self, *args, **kw):\\r\\n self.wrapped = NormalModel(*args, **kw)\\r\\n\\r\\n def forward(self, input):\\r\\n normal_output = self.wrapped(input)\\r\\n extra_output = self.wrapped.layer_of_interest.property\\r\\n return normal_output, extra_output\\r\\n```\\r\\n\\r\\nThen wrap `ExtraOutputWrapper` with `DataParallel` and it will return the property of interest. This seems fairly clean to me. Is there an issue with this?",
      "y": "You can use the following code:\\r\\n\\r\\n```python\\r\\n\\r\\nclass NormalModel(nn.Module):\\r\\n pass\\r\\n\\r\\nclass ExtraOutputWrapper(nn.Module):\\r\\n def __init__(self, *args, **kw):\\r\\n self.wrapped = NormalModel(*args, **kw)\\r\\n\\r\\n def forward(self, input):\\r\\n normal_output = self.wrapped(input)\\r\\n extra_output = self.wrapped.layer_of_interest.property\\r\\n return normal_output, extra_output\\r\\n```\\r\\n\\r\\nThen wrap `ExtraOutputWrapper` with `DataParallel` and it will return the property of interest."
   },
   {
      "null": 107,
      "x": "Linking Error: relocation R_X86_64_32 against `cpuinfo_x86_linux_init' can not be used when making a shared object",
      "z": "The most recent version of cpuinfo should fix this. We have to update the submodule.",
      "y": "Install the latest version of cpuinfo to fix this."
   },
   {
      "null": 108,
      "x": "fatal error: torch/torch.h: No such file or directory",
      "z": "Right now you have to `build install` pytorch for that header to be installed correctly. There's three options:\\r\\n1. Merge https://github.com/pytorch/pytorch/pull/5772 which fixes this and looks ready anyway,\\r\\n2. `python setup.py build install`\\r\\n3. `python run_test.py --exclude cpp_extensions`\\r\\n\\r\\n(1) is probably best",
      "y": "There are three options:\\r\\n1. Merge https://github.com/pytorch/pytorch/pull/5772 which fixes this and looks ready anyway,\\r\\n2. `python setup.py build install`\\r\\n3. `python run_test.py --exclude cpp_extensions`\\r\\n\\r\\n(1) is probably best"
   },
   {
      "null": 109,
      "x": "pytorch installation error on macOS 10.13.3",
      "z": "^ resolved by reinstalling tbb",
      "y": "This error can be resolved by reinstalling TBB."
   },
   {
      "null": 110,
      "x": "Pretrained Model Loading Error",
      "z": "If you were to load the file using pickle alone, I'd suggest trying to load it using `encoding='latin1'` for example.\\r\\n```python\\r\\nwith open(myfile, 'rb') as f:\\r\\n data = pickle.load(f, encoding='latin1')\\r\\n```\\r\\nThis has solved issues for me when deserializing some pickle files saved on python2 and loaded on python3. But to make it work out-of-the-box on `torch.load` might require extending the API.",
      "y": "Add the `encoding=latin1` argument to `pickle.load`"
   },
   {
      "null": 111,
      "x": "Bazel - Pybind - Pytorch - Undefined Symbol",
      "z": "Resolved. Turns out you need to add 'linkshared=True' and compile it as a binary",
      "y": "You need to add 'linkshared=True' and compile it as a binary"
   },
   {
      "null": 112,
      "x": "TypeError: __init__() should return None, not 'int' in validation Dataset",
      "z": "Closing, as this does not look like a bug in PyTorch, but rather a mis-use of  Python data model.\\r\\nFor example, following code raises the same runtime error:\\r\\n```\\r\\nclass Bar:\\r\\n    def __init__(self, bar):\\r\\n        self.bar = bar\\r\\n        return bar\\r\\n\\r\\nif __name__ == \\\"__main__\\\":\\r\\n    x = Bar(5)\\r\\n```\\r\\n```\\r\\n% python3 bar.py\\r\\nTraceback (most recent call last):\\r\\n  File \\\"bar.py\\\", line 7, in <module>\\r\\n    x = Bar(5)\\r\\nTypeError: __init__() should return None, not 'int'\\r\\n```\\r\\nPlease do not hesitate to comment/update an example, if you believe that PyTorch behavior is incorrect in this case.",
      "y": "This is an expected behavior."
   },
   {
      "null": 113,
      "x": "torch.linalg.cholesky fails for some PSD matrices",
      "z": "Thanks for your response. Makes total sense - the context of the problem is trying to add a small amount of (tikhonov) regularization to make cholesky stable so aware of the stability problem just my fix was mixing precisions as you say. With double precision numpy==pytorch. Feel free to close.\n\n",
      "y": "Using double precision will not give any errors."
   },
   {
      "null": 114,
      "x": "Scribe stats reporting is broken in GHA due to secrets access from fork PRs",
      "z": "Verified that scribe proxy is working now",
      "y": "This issue has been fixed."
   },
   {
      "null": 115,
      "x": "HTTP Error 403 for torch.hub ResNet",
      "z": "As a workaround, can you please adding the following line before making any \\\"torch.hub\\\" calls:\\r\\n```\\r\\ntorch.hub._validate_not_a_forked_repo=lambda a,b,c: True\\r\\n```",
      "y": "Please add the following line before making any \\\"torch.hub\\\" calls:\\r\\n```\\r\\ntorch.hub._validate_not_a_forked_repo=lambda a,b,c: True\\r\\n```"
   },
   {
      "null": 116,
      "x": "torch.permute missing in docs",
      "z": "This has been fixed in master.\\r\\n\\r\\nThat being said, the docs are not well formatted, so I'll fix that.",
      "y": "This issue has been fixed."
   },
   {
      "null": 117,
      "x": "Conv2d triggers assertion in mkl-dnn when padding=(n, 3)",
      "z": "@sakaia , I verify that MKLDNN v0.21.1 can solve this problem, we have upgrade mkldnn to 0.21.1, you can test it your side by pip install --pre torch torchvision -f https://download.pytorch.org/whl/nightly/cpu/torch_nightly.html or build the pytorch source code according to README.md. Thanks!",
      "y": "Installing MKLDNN v0.21.1 can fix this issue."
   },
   {
      "null": 118,
      "x": "PyTorch is not using the GPU specified by CUDA_VISIBLE_DEVICES",
      "z": "More comment:\\r\\n\\r\\nUsing the following command instead\\r\\n`CUDA_DEVICE_ORDER=PCI_BUS_ID CUDA_VISIBLE_DEVICES=3 python test.py`\\r\\nwould fix the problem.\\r\\n\\r\\nBut I am not sure if this is the expected behavior...",
      "y": "Use the following command \\r\\n`CUDA_DEVICE_ORDER=PCI_BUS_ID CUDA_VISIBLE_DEVICES=3 python test.py`"
   },
   {
      "null": 119,
      "x": "Allow parallel sending to device in DataLoader",
      "z": "No the push occurs inside the transform. It would be possible to design a custom collate_fn but that would be inefficient. The whole point of doing this is because pushing to GPU is so time consuming. Therefore, the best way is to send to GPU in a data transform.",
      "y": "The best way is to send to GPU in a data transform."
   },
   {
      "null": 120,
      "x": "distributed all_reduce deadlocks in v1.1",
      "z": "Pytorch 1.1 uses nccl 2.4.2 which has a known issue of hanging with long running jobs that was fixed for 2.4.6. https://github.com/NVIDIA/nccl/commit/f40ce73e8987d2990e4b9ef6c75f4b3423acce78\\r\\nWorkaround is to export NCCL_LL_THRESHOLD=0. \\r\\ncc @pietern, @mrshenli to bump nccl submodule.",
      "y": "Installing nccl 2.4.6 can fix this issue."
   },
   {
      "null": 121,
      "x": "Build error with MSVC (aten\\\\src\\\\ATen\\\\native\\\\quantized\\\\Copy.cpp)",
      "z": "move the sentence \\\"float* src_data = src.data<float>();\\\" into the function of \\\"AT_DISPATCH_QINT_TYPES ...\\\" solves my problem. It looks like:\\r\\n\\r\\n AT_DISPATCH_QINT_TYPES(self.scalar_type(), \\\"Copy\\\", [&]() {\\r\\n float* src_data = src.data<float>();\\r\\n\\tscalar_t* self_data = self.data<scalar_t>();\\r\\n for (int i = 0; i < self.numel(); ++i) {\\r\\n self_data[i] = quantize_val<scalar_t>(\\r\\n self.q_scale().to<float>(),\\r\\n self.q_zero_point().to<int32_t>(),\\r\\n src_data[i]);\\r\\n }\\r\\n });\\r\\n\\r\\n Hope that will help you.",
      "y": "Moving the sentence \\\"float* src_data = src.data<float>();\\\" into the function of \\\"AT_DISPATCH_QINT_TYPES ...\\\" can solve this problem."
   },
   {
      "null": 122,
      "x": "RuntimeError: ONNX export failed: Couldn't export operator aten::softmax",
      "z": "I just checked the nightly version! There it works fine! Thanks! :)",
      "y": "This works fine for the nightly version."
   },
   {
      "null": 123,
      "x": "MultiheadAttention is not scriptable",
      "z": "Thanks for bringing up the issue and we have fixed it. We also added two JIT unit tests to cover the applications of torch.nn.MultiheadAttention module (i.e. \"test_torchscript_multi_head_attn\" and \"test_scriptmodule_multi_head_attn_cuda\" in test_jit.py). Those two could possibly be good examples for JIT scriptable.",
      "y": "This issue has been fixed."
   },
   {
      "null": 124,
      "x": "Building libtorch-dependent project with CMake",
      "z": "I don't know the answer to this, but I suggest using new cmake 'targets' feature, more like\\r\\n\\r\\n```\\r\\nfind_package(Torch REQUIRED)\\r\\n\\r\\nadd_library(a)\\r\\ntarget_link_libraries(a PRIVATE Torch)\\r\\n```\\r\\ninstead of messing with TORCH_INCLUDE_DIRS and TORCH_LIBRARIES themselves.",
      "y": "You can use new cmake 'targets' feature, e.g.\\r\\n\\r\\n```\\r\\nfind_package(Torch REQUIRED)\\r\\n\\r\\nadd_library(a)\\r\\ntarget_link_libraries(a PRIVATE Torch)\\r\\n```"
   },
   {
      "null": 125,
      "x": "Parameter not registering if .to(device) is used",
      "z": "this is totally expected.\\r\\n\\r\\nParameters can only be leaf Tensors, not Tensors that are a result of an operation on another Tensor.\\r\\n\\r\\nWhat you want is:\\r\\n\\r\\n```\\r\\nself.par = torch.nn.Parameter(torch.rand(5, device='cuda'))\\r\\n```",
      "y": "You need to use:\\r\\n\\r\\n```\\r\\nself.par = torch.nn.Parameter(torch.rand(5, device='cuda'))\\r\\n```"
   },
   {
      "null": 126,
      "x": "Windows CPU debug build fails at linking stage",
      "z": "Hi,\\r\\nYes. With #17494 I can build without set BUILD_TEST=OFF\\r\\n\\r\\nThanks!",
      "y": "Build will succeed without setting BUILD_TEST=OFF\\r\\n\\"
   },
   {
      "null": 127,
      "x": "Conda did not install cudnn for pytorch",
      "z": "Hi,\\r\\neven if it's not listed explicitly with `conda list`, your installation should have cudnn. You can check if this is the case executing `torch.backends.cudnn.is_available()`.",
      "y": "Even if it's not listed explicitly with `conda list`, your installation should have cudnn. You can check if this is the case executing `torch.backends.cudnn.is_available()`."
   },
   {
      "null": 128,
      "x": "Tensor unfold backward is slow",
      "z": "This was fixed by gh-36612, closing.",
      "y": "This issue has been fixed."
   },
   {
      "null": 129,
      "x": "cuDNN error: CUDNN_STATUS_EXECUTION_FAILED when calling .cuda() on RNN layer",
      "z": "CUDA 9 and RTX 2080 Ti simply aren't compatible and dont play well togethere.\\r\\nAn older CuDNN version working is likely a side-effect rather than expectation.\\r\\nUse CUDA10 and CUDA10 versions of CuDNN etc. for RTX 2080 which is Turing architecture",
      "y": "Use the latest versions of CUDA10 and CuDNN"
   },
   {
      "null": 130,
      "x": "PyTorch not releasing autograd buffers associated to tensors created with `.from_numpy()`",
      "z": "Sorry I think I misunderstood your point above then, I though you were still seeing a problem even when .item() is used.\nIn that case it is (unfortunately) the expected behavior here.\n\nIt is easy to free any buffers encountered during the backward pass and raise a proper error if the user try to use them again.\nBut it is much harder to delete the graph structure itself while we're traversing it to compute the backward. Also the memory usage due to a single graph is small compared to the other objects in general and so should not be a problem (unless of course the graph keeps growing as in your case).\n\nAnother approach to solve this problem would be to use the `with torch.no_grad():` block (or the functions decorator equivalent) around any operations for which you won't need to compute backward. This way, it won't even build the graph and so will be faster and less memory hungry.",
      "y": "Enclose operations that do not require the backward computations within the  `with torch.no_grad():` block"
   },
   {
      "null": 131,
      "x": "Is it possible to integrate jax into pytorch ?",
      "z": "1.1, we already have with torch.jit\\r\\n1.2 seems interesting\\r\\n2. probably not relevant\\r\\n\\r\\n1.2 is tracked in https://github.com/pytorch/pytorch/issues/1642\\r\\n\\r\\nOverall, there isn't really the concept of \\\"jax in pytorch\\\" or \\\"pytorch in jax\\\" in the same sense that they're both frontends",
      "y": "Jax and PyTorch are both frontends and cannot be integrated."
   },
   {
      "null": 132,
      "x": "ONNX exporter for slice operation isses onnx:Slice for dimensions that are not sliced, includiung batch dimension - which breaks TRT5",
      "z": "Looks like it's not an issue with latest TRT6 and parser, so we can close it.",
      "y": "This issue has been fixed."
   },
   {
      "null": 133,
      "x": "Sharing/Transferring gradients from models across multiple GPU(s) in multiprocessing",
      "z": "@subho406 seems to work fine for me (with 1.1.0 and the latest `examples` repo), just fyi",
      "y": "This issue has been fixed."
   },
   {
      "null": 134,
      "x": "Why torch wheel is so huge (582MB)?",
      "z": "The number of GPU architectures targeted by the binary is a large contributor to binary size. If you'd like a smaller GPU binary, you can change TORCH_CUDA_ARCH_LIST accordingly. For example, if you want to only support compute 5.0 w/forward compatibility, you can set TORCH_CUDA_ARCH_LIST=5.0+PTX as opposed to the more comprehensive list built by default by the PyTorch devs.\\r\\n\\r\\n(I'm not a PyTorch developer, just someone who has built smaller versions of the library.)",
      "y": "You can specify the GPU architectures during installation to reduce the size."
   },
   {
      "null": 135,
      "x": "We should not mark non-floating point Tensors as requirering gradients, ever",
      "z": "Gradients are only defined for continuous functions. So if you have an `integer` type, gradients don't really make sense.",
      "y": "Gradients are only defined for continuous functions. So if you have an `integer` type, gradients don't really make sense."
   },
   {
      "null": 136,
      "x": "Pickling of _VariableFunctions no longer works in 1.5",
      "z": "If any newcomers come here and wonder how to fix this. Upgrading to 1.5.1 fixed the issue. Downgrading to 1.4.1 may too.\\r\\n\\r\\nThat is\\r\\n`conda install pytorch==1.5.1 torchvision==0.6.1 cudatoolkit=10.1 -c pytorch` if you use vision\\r\\n`conda install pytorch==1.5.1 cudatoolkit=10.1 -c pytorch` if you use NLP.\\r\\n\\r\\ndepending on your cuda version.",
      "y": "Installing the latest version of PyTorch can fix the issue."
   },
   {
      "null": 137,
      "x": "Defaulting to ninja build doesn't forward includes in PyTorch C++/CUDA extensions",
      "z": "include_dirs now supports both absolute and relative paths.",
      "y": "include_dirs now supports both absolute and relative paths."
   },
   {
      "null": 138,
      "x": "Empty GPU memory cache after Jupyter notebook interrupted",
      "z": "jupyter notebook holds reference to the exception when interrupted (for things like %debug), which holds reference to the stack frames, which hold reference to the variables. so empty_cache won't work. this is not really a pytorch issue per se.",
      "y": "This issue is not related to PyTorch."
   },
   {
      "null": 139,
      "x": "torch.utils.checkpoint.checkpoint + torch.cuda.amp fails",
      "z": "Thanks for raising this issue!\\r\\nMy best guess is that `CheckpointFunction.backward` uses the stored mixed-precision tensors from its `forward`, but breaks the autocasting contract for the backward.\\r\\nIf you run `scaler.scale(loss).backward()` inside the `autocast` block, it should work for now as a workaround.\\r\\n\\r\\nCC @mcarilli",
      "y": "Running `scaler.scale(loss).backward()` inside the `autocast` block, should work"
   },
   {
      "null": 140,
      "x": "Windows 10 Libtorch installation issue.",
      "z": "Okay. I resolved it myself. \ud83d\udc4d \\r\\nThe tutorial needs to add this line in CMakeLists.txt file\\r\\n``` set(CMAKE_PREFIX_PATH \\\"libtorch/share/cmake/Torch\\\") ```\\r\\n\\r\\nwhich should point to where Torch is unzipped appropriately.\\r\\nPlease update the docs. This issue is quite prelevant\\r\\n",
      "y": "You need to add this line in CMakeLists.txt file\\r\\n``` set(CMAKE_PREFIX_PATH \\\"libtorch/share/cmake/Torch\\\") ```"
   },
   {
      "null": 141,
      "x": "torch.remainder gives a remainder larger than the divisor",
      "z": "Thanks for the report!\\r\\n\\r\\nIt turns out that this problem is due to numerical precision issue when you subtract two larger numbers with only small difference, such as 100000000002 - 100000000001. Even the numpy single precision result of 1.024195 is not accurate (from c function `fmodf`). The accurate double precision result is 0.577396.\\r\\n\\r\\nThe current `torch.remainder` impl for both cpu and gpu have this problem, e.g. https://github.com/pytorch/pytorch/blob/253943d5a7bb90a420e4d94366101915823c7929/aten/src/ATen/native/cuda/BinaryArithmeticKernel.cu#L80-L85\\r\\n\\r\\nI also checked the `torch.fmod` impl which are correct for both cpu and gpu. You can temporarily use `torch.fmod` as a substitute for `torch.remainder`. I will provide a fix to `torch.remainder` soon, and let it use the native c/cuda function of `fmod`.",
      "y": "This is due to numerical precision issues. Please use `torch.fmod` instead of `torch.remainder`"
   },
   {
      "null": 142,
      "x": "torch.cdist [Cuda out of memory Tried to allocate 108GB of memory]",
      "z": "I'm closing it as after building my env from scratch it now works. (I was already on torch 1.5, so I can't directly point on the problem but ...)",
      "y": "Building environment from scracth will fix the issue."
   },
   {
      "null": 143,
      "x": "C++ API for Transformer model in libtorch 1.5.0",
      "z": "This is available since 1.7",
      "y": "This is available in the recent versions of PyTorch."
   },
   {
      "null": 144,
      "x": "RuntimeError: Error in dlopen or dlsym: libcaffe2_nvrtc.so: cannot open shared object file: No such file or directory ( torch_geometric/utils/loop.py)",
      "z": "@AugF `LD_LIBRARY_PATH` should be `/home/xxx/anaconda3/envs/pyg/lib/python3.8/site-packages/torch/lib:${LD_LIBRARY_PATH}`.\\r\\nThere should be a `/lib` after `site-packages/torch/`",
      "y": "`LD_LIBRARY_PATH` should be `/home/xxx/anaconda3/envs/pyg/lib/python3.8/site-packages/torch/lib:${LD_LIBRARY_PATH}`."
   },
   {
      "null": 145,
      "x": "Better testing on CPUs without AVX capabilities",
      "z": "In that case we can run cpp tests under qemu, which can be configured not to support any vecorized instructions.",
      "y": "You can use a machine emulator such as QEMU."
   },
   {
      "null": 146,
      "x": "[BatchNorm] Unexpected behaviour with track_running_stats",
      "z": "Closed by #38084",
      "y": "This issue has been fixed."
   },
   {
      "null": 147,
      "x": "`torch.log10` with float32 produces different results on different CPU",
      "z": "@mruberry \\r\\nYes, that makes sense. I wanted to leave a record of this finding and double check if there is something that we can work on.\\r\\n\\r\\nWhile filling out this issue I also found that `float64` produces consistent result, so we are going to use `float64` in torchaudio to resolve the issue we are having.\\r\\n\\r\\nThanks!",
      "y": "Using `float64` can resolve this issue."
   },
   {
      "null": 148,
      "x": "Falling to turn shape into Tensor",
      "z": "Do\\r\\n```python\\r\\na = torch.ones(10, 10)\\r\\nb = torch.tensor(a.shape)\\r\\nprint(b)\\r\\n# tensor([10, 10])\\r\\n```\\r\\nAlso, questions like this are better suited for https://discuss.pytorch.org/",
      "y": "This can be done via \\r\\n```python\\r\\na = torch.ones(10, 10)\\r\\nb = torch.tensor(a.shape)\\r\\nprint(b)\\r\\n# tensor([10, 10])\\r\\n```"
   },
   {
      "null": 149,
      "x": "[pytorch] [feature request] Entropy function",
      "z": "you can calculate this via `distributions.Categorical(probs=p).entropy()`?",
      "y": "This can ve calculated via `distributions.Categorical(probs=p).entropy()`"
   },
   {
      "null": 150,
      "x": "mse_loss reduction='none' is ignored when required_grads is True",
      "z": "Many thanks @li-roy, with 1.0.0.dev20181018 it works smoothly.",
      "y": "This issue has been fixed."
   },
   {
      "null": 151,
      "x": "3x regression in JIT LSTM speeds",
      "z": "Yes the slowdown was due to memory leak",
      "y": "This issue arises because of memory leaks"
   },
   {
      "null": 152,
      "x": "windows pytorch 0.4.1 error=48 : no kernel image is available for execution on the device",
      "z": "The new binaries are updated for Windows.\\r\\n\\r\\nIf you have previously installed via anaconda, you have to do:\\r\\n\\r\\n```\\r\\nconda uninstall -y pytorch\\r\\nconda clean -t -y\\r\\n```\\r\\n\\r\\nand then reinstall pytorch.\\r\\n\\r\\n\\r\\nIf you have installed via `pip` command on https://pytorch.org, first uninstall pytorch via `pip uninstall torch` and then rerun that command (so that the new wheel is downloaded and installed)",
      "y": "You need to uninstall and reinstall PyTorch."
   },
   {
      "null": 153,
      "x": "Quadro m2000m not able to get pytorch working with gpu",
      "z": "The new binaries are updated for Windows.\\r\\n\\r\\nIf you have previously installed via anaconda, you have to do:\\r\\n\\r\\n```\\r\\nconda uninstall -y pytorch\\r\\nconda clean -t -y\\r\\n```\\r\\n\\r\\nand then reinstall pytorch.\\r\\n\\r\\n\\r\\nIf you have installed via `pip` command on https://pytorch.org, first uninstall pytorch via `pip uninstall torch` and then rerun that command (so that the new wheel is downloaded and installed)",
      "y": "You need to uninstall and reinstall PyTorch."
   },
   {
      "null": 154,
      "x": "Wrong Warning \\\"compiler (c++) may be ABI-incompatible with PyTorch!\\\"",
      "z": "@johnmarkwayve no, the warning is only raised if you're using a binary build of Pytorch, i.e. from pip. It's never raised if you compile from source.",
      "y": "The error is not encountered if compiled from source."
   },
   {
      "null": 155,
      "x": "[Performance Issue] Inference time increases on CPU the more you train the model on a TitanX.",
      "z": "Yes @vadimkantorov , that was the solution!!\nI enabled \"set_flush_denorm\" in case of CPU and now different checkpoints have the same inference time. Many thanks!",
      "y": "Inference time is the same if `set_flush_denorm` is used with CPU. "
   },
   {
      "null": 156,
      "x": "CreateNet(train_net) cannot find blob created by the RunNetOnce(init_net) in Caffe2 (C++)",
      "z": "I just found out that it works, if I add \\\"filter\\\" as external input for the \\\"train_net\\\" like so:\\r\\n\\r\\n train_net.add_external_input (\\\"filter\\\");\\r\\n\\r\\nbefore executing CreateNet(train_net).",
      "y": "Adding `filter` as external input for the model works."
   },
   {
      "null": 157,
      "x": "MSELoss wrongly sums instead of averages when reduction='elementwise_mean'",
      "z": "Yes but it\u2019s fixed on master and will be included in the next release.\\n\\nOn Wed, Sep 19, 2018 at 03:52 simama <notifications@github.com> wrote:\\n\\n> I am on 0.4.1 version and it seems this bug is still not fixed.\\n>\\n> \u2014\\n> You are receiving this because you were mentioned.\\n> Reply to this email directly, view it on GitHub\\n> <https://github.com/pytorch/pytorch/issues/10148#issuecomment-422696370>,\\n> or mute the thread\\n> <https://github.com/notifications/unsubscribe-auth/AFaWZfuFXXaUvgqzfKfZO7_kPUMt0gsQks5ucfe-gaJpZM4VrZIT>\\n> .\\n>\\n",
      "y": "This issue has been fixed."
   },
   {
      "null": 158,
      "x": "Backward engine computes unnecessary dependencies",
      "z": "Fixed in #752.",
      "y": "This issue has been fixed."
   },
   {
      "null": 159,
      "x": "F.relu(inplace) followed by F.dropout(inplace) breaks backward pass",
      "z": "Yeah, that is expected. You can't apply in-place operations to leaf Variables. Just remove the `inplace` flag and it should be good.",
      "y": "Removing the `inplace` flag should make it work."
   },
   {
      "null": 160,
      "x": "torch.range is upper-bound inclusive, while python range and numpy arange are upper-bound exclusive",
      "z": "I think we should make a `torch.arange` that is equivalent to `numpy.arange` and depreceate `torch.range` properly. It is used quite extensively all over the place.\\r\\nwdyt @colesbury @apaszke ?",
      "y": "`torch.range` needs to be deprecated"
   },
   {
      "null": 161,
      "x": "LSTM forget gate bias initialization",
      "z": "Yes, the ordering of weights a biases is the same for all implementations and is `ingate, forgetgate, cellgate, outgate`. You need to initialize the values between 1/4 and 1/2 of the bias vector to the desired value.",
      "y": "The ordering of weights a biases is the same for all implementations and is `ingate, forgetgate, cellgate, outgate`. You need to initialize the values between 1/4 and 1/2 of the bias vector to the desired value."
   },
   {
      "null": 162,
      "x": "Error while saving my network",
      "z": "this is a syntax error. `torch.save` does not have the order of arguments as you've used.",
      "y": "Using the correct arguments will not give this error."
   },
   {
      "null": 163,
      "x": "Build fails on Ubuntu 14.04 + conda latest",
      "z": "CUDA Version 8.0.27 has this issue. it is a pre-release version. The stable CUDA8 version is 8.0.44",
      "y": "Installing the latest stable release fixes the issue."
   },
   {
      "null": 164,
      "x": "view() after transpose() raises non contiguous error",
      "z": "Yes, this is expected, as `view` is only supposed to work on contiguous tensors, and transposing a tensor makes it non-contiguous. You can use `.contiguous()` after `transpose` to fix your issue",
      "y": "This is expected, as `view` is only supposed to work on contiguous tensors, and transposing a tensor makes it non-contiguous. You can use `.contiguous()` after `transpose` to fix your issue"
   },
   {
      "null": 165,
      "x": "Errors import torch installed form source on macOS",
      "z": "Run python from a different directory than the repository root.",
      "y": "You need to open torch in any directory other than repo's root"
   },
   {
      "null": 166,
      "x": "Add support for variable length sequences in cuDNN RNNs",
      "z": "@glample CUDNN already supports variable length sequences through a packed-array API; PyTorch just doesn't currently use that functionality",
      "y": "CUDNN already supports variable length sequences through a packed-array API; PyTorch just doesn't currently use that functionality"
   },
   {
      "null": 167,
      "x": "Maxout Layer",
      "z": "For ones who need Maxout, I changed the above code to make it work. \\r\\n\\r\\n\\r\\n```python\\r\\nclass Maxout(nn.Module):\\r\\n\\r\\n def __init__(self, d_in, d_out, pool_size):\\r\\n super().__init__()\\r\\n self.d_in, self.d_out, self.pool_size = d_in, d_out, pool_size\\r\\n self.lin = nn.Linear(d_in, d_out * pool_size)\\r\\n\\r\\n\\r\\n def forward(self, inputs):\\r\\n shape = list(inputs.size())\\r\\n shape[-1] = self.d_out\\r\\n shape.append(self.pool_size)\\r\\n max_dim = len(shape) - 1\\r\\n out = self.lin(inputs)\\r\\n m, i = out.view(*shape).max(max_dim)\\r\\n return m\\r\\n```\\r\\n",
      "y": "The Maxout layer can be implemented as follows \\r\\n\\r\\n\\r\\n```python\\r\\nclass Maxout(nn.Module):\\r\\n\\r\\n def __init__(self, d_in, d_out, pool_size):\\r\\n super().__init__()\\r\\n self.d_in, self.d_out, self.pool_size = d_in, d_out, pool_size\\r\\n self.lin = nn.Linear(d_in, d_out * pool_size)\\r\\n\\r\\n\\r\\n def forward(self, inputs):\\r\\n shape = list(inputs.size())\\r\\n shape[-1] = self.d_out\\r\\n shape.append(self.pool_size)\\r\\n max_dim = len(shape) - 1\\r\\n out = self.lin(inputs)\\r\\n m, i = out.view(*shape).max(max_dim)\\r\\n return m\\r\\n```\\r\\n"
   },
   {
      "null": 168,
      "x": "Unefined reference to C10::Error::Error when linking against libTorch",
      "z": "I think you're missing `\\\"-D_GLIBCXX_USE_CXX11_ABI=0\\\"` in your build flags. We provide this in our TorchConfig.cmake, which is why we recommend cmake as the easiest way to build LibTorch. That said, you can add that to your QtCreator config. See https://github.com/pytorch/pytorch/blob/master/cmake/TorchConfig.cmake.in for the relevant file\\r\\n\\r\\nPlease let me know if that fixes the issue.",
      "y": "You need to use the build flags `QMAKE_CXXFLAGS += -D_GLIBCXX_USE_CXX11_ABI=0`"
   },
   {
      "null": 169,
      "x": "can't convert CUDA tensor to numpy. Use Tensor.cpu() to copy the tensor to host memory first.",
      "z": "`.numpy()` shares memory with the input CPU tensor, so `cuda_t.cpu().numpy()` is different with `cpu_t.numpy()` and we want to explicitly ask users to convert to CPU tensor.",
      "y": "`.numpy()` shares memory with the input CPU tensor, so `cuda_t.cpu().numpy()` is different with `cpu_t.numpy()` and we want to explicitly ask users to convert to CPU tensor."
   },
   {
      "null": 170,
      "x": "GPU Memery leak",
      "z": "i find a solution yesterday\\r\\n\\r\\nbatch_y_predlabel has to be **detach** so the GPU memory can be free.\\r\\n\\r\\nI don't know if it's normal that a GPU tensor transfer to the CPU in the compute graph still take GPU memory?\\r\\nfor gradient compute I imagine?",
      "y": "To free up GPU memory, you need to detach the predictions."
   },
   {
      "null": 171,
      "x": "[Build error] libnccl.so: error adding symbols: File in wrong format",
      "z": "it's very likely that you installed x64 libnccl, instead of ppc64 version",
      "y": "You need to install the correct version for your platform"
   },
   {
      "null": 172,
      "x": "Can cuda10 use pytorch-0.4.1?",
      "z": "no",
      "y": "Not, it cannot."
   },
   {
      "null": 173,
      "x": "cosine_similarity function produces results more than 1.0",
      "z": "Probably better to re-order the computations to improve numerical precision. Might want to look at SciPy:\\r\\n\\r\\nhttps://github.com/scipy/scipy/blob/453932337f4a67170e4e7fda3f808b273a787a41/scipy/spatial/distance.py#L717-L721\\r\\n\\r\\nI think the issue is that we're doing:\\r\\n\\r\\n```\\r\\nx / (sqrt(x) * sqrt(x)) # bad\\r\\n```\\r\\n\\r\\nvs.\\r\\n\\r\\n```\\r\\nx / sqrt(x * x) # good\\r\\n```\\r\\n\\r\\nI don't know enough about floating point arithmetic to argue why the second is more accurate, but it seems to be. (You can look at the min/max over a large random tensor)",
      "y": "This error can be fixed by reordering the computations to improve numerical precisions."
   },
   {
      "null": 174,
      "x": "Is mkl-dnn enabled in the latest binary distribution v1.0.1?",
      "z": "Yes.",
      "y": "Yes."
   },
   {
      "null": 175,
      "x": "nn.LSTM gives nondeterministic results with dropout and multiple layers",
      "z": "Closed and fixed in cudnn_7.6.1 @ngimel",
      "y": "This issue has been fixed."
   },
   {
      "null": 176,
      "x": "Add support for mixture models in torch.distributions",
      "z": "`MixtureSameFamily` should be easy to implement. We don't use this in Pyro since we usually keep the mixture component id as an explicit variable and enumerate over that variable:\\r\\n```py\\r\\ncomponent = pyro.sample(\\\"component\\\", dist.Categorical(probs),\\r\\n                        infer={\\\"enumerate\\\": \\\"parallel\\\"})\\r\\nassert component.reshape(-1).shape == probs.shape[-1:]\\r\\nvalue = pyro.sample(\\\"mixture\\\", MyDistribution(my_params[component]))\\r\\n```\\r\\ncc @martinjankowiak",
      "y": "`MixtureSameFamily` can be implemented as:\\r\\n```py\\r\\ncomponent = pyro.sample(\\\"component\\\", dist.Categorical(probs),\\r\\n                        infer={\\\"enumerate\\\": \\\"parallel\\\"})\\r\\nassert component.reshape(-1).shape == probs.shape[-1:]\\r\\nvalue = pyro.sample(\\\"mixture\\\", MyDistribution(my_params[component]))\\r\\n```"
   },
   {
      "null": 177,
      "x": "IndexError while trying to save torchscript",
      "z": "Closing this since I can't reproduce and we fixed some similar errors recently, but feel free to re-open if you're still getting this error",
      "y": "This issue has been fixed."
   },
   {
      "null": 178,
      "x": "[JIT] b->inputs().size() == b->outputs().size() ASSERT FAILED",
      "z": "Closing as this has been fixed (the repro in the initial post now works as intended)",
      "y": "This issue has been fixed."
   },
   {
      "null": 179,
      "x": "[CPU] several inplace functions fail since 1.0.1 on certain hw",
      "z": "I can reproduce this with OMP_NUM_THREADS=2. I haven't seen it with OMP_NUM_THREADS=1.",
      "y": "Setting `OMP_NUM_THREADS=1` will not raise an error."
   },
   {
      "null": 180,
      "x": "module' object has no attribute '_dl'",
      "z": "it's not a problem of spacy, it looks like an incomplete or corrupt pytorch install.\\r\\n\\r\\nTry:\\r\\n\\r\\n```\\r\\npip uninstall torch\\r\\npip uninstall torch\\r\\npip uninstall torch\\r\\npip install torch\\r\\n```",
      "y": "You need to uninstall and reinstall PyTorch."
   },
   {
      "null": 181,
      "x": "IndyLSTM & IndyGRU in PyTorch",
      "z": "it's not widely used enough yet, to be pushed into core (feel free to reopen once it becomes more of a standard).\\r\\n\\r\\nAdditionally, we are working on a user handwritten RNNs being fast, rather than adding more fundamental multi-layer RNNs into core.\\r\\nSo, I'm closing the feature request.",
      "y": "This feature will not be implemented."
   },
   {
      "null": 182,
      "x": "Multi-GPU RuntimeError: Expected tensor for argument #1 'input' to have the same device as tensor for argument #2 'weight'",
      "z": "your input is on gpu 1 but your net work is on gpu 0.",
      "y": "Model and data need to be on the same GPU."
   },
   {
      "null": 183,
      "x": "CUDA unavailable when pytorch 1.3.0. installed with cudatoolkit 10.1",
      "z": "I got the same issue with Pytorch 1.3.1 + CUDA 10.0. Finally, I got it resolved with:\\r\\n```\\r\\npip install torch==1.3.1+cu100 torchvision==0.4.2+cu100 -f https://download.pytorch.org/whl/torch_stable.html\\r\\n```",
      "y": "You need to downgrade your version of CUDA."
   },
   {
      "null": 184,
      "x": "error executing torch_shm_manager in cifar10_tutorial.py",
      "z": "Experienced the same issue on OSX. Setting `num_workers=0` on the DataLoaders solved it, even though the tutorial only recommends it for Windows.\\r\\nIt should probably be better clarified/fixed though.",
      "y": "Setting `num_workers=0` will not raise an error."
   },
   {
      "null": 185,
      "x": "\\\"module has no attribute 'downsample'\\\" when scripting torchvision's resnet",
      "z": "I believe you need to use a nightly version of torchvision (and thus a nightly version of PyTorch) for resnet to be scriptable.",
      "y": "You need to install the latest version of PyTorch."
   },
   {
      "null": 186,
      "x": "Failed to load model on mobile for device type \\\"c10::DeviceType::CUDA\\\"",
      "z": "@ljk53 sorry late response. \\r\\nI confirmed the model converted to cpu() is able to load on the phone.",
      "y": "You need to move the model to `.cpu()`"
   },
   {
      "null": 187,
      "x": "Didn't find kernel to dispatch to for operator 'quantized::conv2d'",
      "z": "Hi @thiyagu145, the error message you've encountered says that you're trying to run the quantized Conv2d operator, but you're passing in an unquantized tensor.\n\nPlease take a look for where we use QuantStub in the tutorial and workflow documentation. Once this is present and the values are passed through them, the issue should be solved",
      "y": "Passing a quantized vector to the operator will not raise an error."
   },
   {
      "null": 188,
      "x": "Why t.arange(0,3) create an int type Tensor?",
      "z": "torch.arange returns an int64 tensor by defaut. It mimics numpy behavior. if you want different dtype, give dtype as argument, such as `t.arange(0, 3, dtype=torch.float32)`",
      "y": "You need to specify the datatype as an argument."
   },
   {
      "null": 189,
      "x": "Loading opencv image to pytorch tensor",
      "z": "I found it at https://pytorch.org/cppdocs/api/classat_1_1_tensor.html#exhale-class-classat-1-1-tensor.\\r\\nHere is a demo:\\r\\n```C++\\r\\n tensor_image = tensor_image.permute({0, 3, 1, 2});\\r\\n```",
      "y": "The order of dimensions is different in OpenCV and PyTorch."
   },
   {
      "null": 190,
      "x": "Bug in transferring model from pytorch --> caffe2",
      "z": "I modified the tutorial and removed the `pixel_shuffle` part and composed the network only with ReLU and conv layers (just to see if `pytorch` and `caffe2` give the same output), and **now it works fine**.\\r\\n\\r\\nSince `pixel_shuffle` is only on the master branch, I assume that it's still buggy?\\r\\nAnyway, I'll just test if all the layers I personally need output the same thing between the two frameworks and proceed.",
      "y": "Remove the `pixel_shuffle` part and composed the network only with ReLU and conv layers will not give an error."
   },
   {
      "null": 191,
      "x": "Using net.cuda crashes the kernel",
      "z": "If you look into the list of types in the `got (...)` part, you'll find a mix of CPU and CUDA tensors, with `input` and `output` being on CPU, while `weight` and `bias` is on the GPU.\\r\\n\\r\\nYou probably forgot to send the input to the GPU. Alternatively, keep in mind that `.cuda()` is an out of place operation i.e.\\r\\n```python\\r\\ninput.cuda()\\r\\nmodel(Variable(input))\\r\\n```\\r\\nwill fail. You need to overwrite the reference with a new CUDA tensor:\\r\\n```python\\r\\ninput = input.cuda()\\r\\nmodel(Variable(input))\\r\\n```",
      "y": "You need to overwrite the reference with a new CUDA tensor:\\r\\n```python\\r\\ninput = input.cuda()\\r\\nmodel(Variable(input))\\r\\n```"
   },
   {
      "null": 192,
      "x": "Gumbel noise",
      "z": "Here you go. Much more readable and no modules required:\\r\\n```python\\r\\nimport torch.nn.functional as F\\r\\nfrom torch.autograd import Variable\\r\\n\\r\\ndef sampler(input, tau, temperature):\\r\\n noise = torch.rand(input.size())\\r\\n noise.add_(1e-9).log_().neg_()\\r\\n noise.add_(1e-9).log_().neg_()\\r\\n noise = Variable(noise)\\r\\n x = (input + noise) / tau + temperature\\r\\n x = F.softmax(x.view(input.size(0), -1))\\r\\n return x.view_as(input)\\r\\n```\\r\\nWe're using GitHub for bug reports only, if you have questions please post the on [our forums](https://discuss.pytorch.org).",
      "y": "The code is as follows:\\r\\n```python\\r\\nimport torch.nn.functional as F\\r\\nfrom torch.autograd import Variable\\r\\n\\r\\ndef sampler(input, tau, temperature):\\r\\n noise = torch.rand(input.size())\\r\\n noise.add_(1e-9).log_().neg_()\\r\\n noise.add_(1e-9).log_().neg_()\\r\\n noise = Variable(noise)\\r\\n x = (input + noise) / tau + temperature\\r\\n x = F.softmax(x.view(input.size(0), -1))\\r\\n return x.view_as(input)\\r\\n```"
   },
   {
      "null": 193,
      "x": "GOMP_4.0 not found",
      "z": "I fixed the problem by importing opencv before torch.",
      "y": "Importing OpenCV before PyTorch fixes the issue."
   },
   {
      "null": 194,
      "x": "Cannot install `torchvision 0.1.7` by using conda",
      "z": "i see. in this case, just remove torchvision (conda uninstall torchvision) and then install it via pip.\\r\\n\\r\\npip install torchvision",
      "y": "Installing torchvision through pip fixes this issue."
   },
   {
      "null": 195,
      "x": "nn.Module not importing parameters contained in lists",
      "z": "This behaviour is expected.\\r\\nThere is a detailed discussion in https://discuss.pytorch.org/t/list-of-nn-module-in-a-nn-module/219",
      "y": "This behaviour is expected."
   },
   {
      "null": 196,
      "x": "libTH doesn't recognize Intel MKL in its default location",
      "z": "I encounter the same error, and I solve it by running command 'conda install mkl' in my activated conda env.\\r\\n\\r\\n> File \\\"/usr/local/lib/python3.5/site-packages/torch/__init__.py\\\", line 45, in <module>\\r\\n> from torch._C import *\\r\\n> ImportError: dlopen(/usr/local/lib/python3.5/site-packages/torch/_C.cpython-35m-darwin.so, 10): Library not loaded: @rpath/libmkl_intel_lp64.dylib\\r\\n> Referenced from: /usr/local/lib/python3.5/site-packages/torch/lib/libTH.1.dylib\\r\\n> Reason: image not found",
      "y": "Running the command `conda install mkl` fixes this issue."
   },
   {
      "null": 197,
      "x": "[build/nccl] failed to build libnccl on Debian unstable",
      "z": "@apaszke Thanks, the fix is to to export the two environment variables:\\r\\n```\\r\\nexport CUDA_HOME=/usr\\r\\nexport CUDA_LIB=/usr/lib/$(shell dpkg-architecture -qDEB_HOST_MULTIARCH)\\r\\n```",
      "y": "You need to exprt the two environment variables:\\r\\n```\\r\\nexport CUDA_HOME=/usr\\r\\nexport CUDA_LIB=/usr/lib/$(shell dpkg-architecture -qDEB_HOST_MULTIARCH)\\r\\n```"
   },
   {
      "null": 198,
      "x": "Allow optimizers to skip nn.Parameters that have requires_grad=False",
      "z": "So I don't really think that it makes sense to allow such parameters. If you don't want to optimize some tensors, they're not parameters - they're fixed. You probably don't want to count them in. And if you really need to then\\r\\n```pytorch\\r\\noptimizer.SGD(filter(lambda p: p.requires_grad, model.parameters()), lr=1e-3)\\r\\n```\\r\\nshould do the trick.",
      "y": "This can be done via ```pytorch\\r\\noptimizer.SGD(filter(lambda p: p.requires_grad, model.parameters()), lr=1e-3)\\r\\n```"
   },
   {
      "null": 199,
      "x": "cuda runtime error (8) : invalid device function - adding cuda tensors",
      "z": "@adelsalehali1982 Your error is unrelated to this issue, and happens because the code you are running supposes that 4 GPUs are used (if gpus are enabled), and you probabmy have less than 4 GPUs in your machine. You can fix that by changing the line with DataParallel to only use 1 or 2 GPUs",
      "y": "You need to use a more recent GPU for this to work."
   },
   {
      "null": 200,
      "x": "\\\"Symbol not found\\\" when \\\"import torch\\\" on Mac OS",
      "z": "As a workaround, \\r\\n\\r\\n\\r\\n`pip3 install torchtext==0.4`\\r\\n\\r\\nsolved the issue.",
      "y": "Install torchtext from pip instead of source to fix this issue."
   },
   {
      "null": 201,
      "x": "How to use cudnn in pytorch\uff1f",
      "z": "To verify that pytorch uses cudnn:\\r\\n```\\r\\n>>> torch.backends.cudnn.version()\\r\\n6021\\r\\n```",
      "y": "To verify that pytorch uses cudnn:\\r\\n```\\r\\n>>> torch.backends.cudnn.version()```"
   },
   {
      "null": 202,
      "x": "[Minor Bug] Pylint E1101 Module 'torch' has no 'from_numpy' member",
      "z": "On VS code: \\r\\nAdding `\\\"python.linting.enabled\\\": false` also worked in this case.",
      "y": "You need to set the following flag `\"python.linting.enabled\\\": false` "
   },
   {
      "null": 203,
      "x": "ImportError: No module named 'tools.setup_helpers'",
      "z": "@soumith I expect people will just keep doing this until there's a working package on PyPI. That's just the first thing people are going to try for a Python project.\\r\\n\\r\\nMaybe you could upload an `0.2` wheel that just prints a more helpful error like:\\r\\n\\r\\n> Installation from PyPI not supported yet (see status at `https://github.com/pytorch/pytorch/issues/566`). For now, please uninstall this package (`pip uninstall pytorch`) and follow the instructions at `http://pytorch.org/` to install with miniconda.",
      "y": "Install using instructions from PyTorch's website."
   },
   {
      "null": 204,
      "x": "from caffe2.python import core, net_drawer, net_printer, visualize, workspace, utils",
      "z": "It is pydot.\\r\\n",
      "y": "You need to install pydot."
   },
   {
      "null": 205,
      "x": "ImportError: No module named future.utils",
      "z": "sudo pip install future\\r\\n or\\r\\neasy_install future",
      "y": "You need to install the package 'future'"
   },
   {
      "null": 206,
      "x": "Raise correct error type when passing invalid covariance matrix to MultivariateNormal",
      "z": "As mentioned in #12102, the issue is that arg checking [happens](https://github.com/pytorch/pytorch/blob/master/torch/distributions/distribution.py#L30) in `Distribution.__init__()`, but `MultivariateNormal.__init__()` performs some linear algebra before calling `super(...).__init__()`. I think the solution is to move any linear algebra operations below the `super(...).__init__()` call in `MultivariateNormal.__init__()`.",
      "y": "The solution is to move any linear algebra operations below the `super(...).__init__()` call in `MultivariateNormal.__init__()`."
   },
   {
      "null": 207,
      "x": "Install only caffe2",
      "z": "it is no longer possible to only install CAFFE2.",
      "y": "It is no longer possible to only install CAFFE2."
   },
   {
      "null": 208,
      "x": "Illegal instruction (core dumped) on Debug CPU build",
      "z": "Could you try `ATEN_CPU_CAPABILITY=default cmd_to_run_your_code`?",
      "y": "Setting `ATEN_CPU_CAPABILITY=default cmd_to_run_your_code` will fix this issue."
   },
   {
      "null": 209,
      "x": "OMP: Warning #190 because of fork not waiting for parallel region to end",
      "z": "I am also having this issue. It persists across data (MNIST, CIFAR10) and various architectures. I do not get the warning if I set `pin_memory=False`. I think it might be due to having three Dataloaders in my script like @FunkyKoki (i.e. train, test, validation) and iterating over two of them (test, val) while inside the loop of the other (train).",
      "y": "The warning does not appear if the argument `pin_memory=False` is given."
   },
   {
      "null": 210,
      "x": "torch.jit.trace returns unwrapped C type",
      "z": "@NeilWangziyu You could also consider trying building PyTorch from source, if nightly doesn't work for you. Saving should definitely work now. @driazati added it 2 months ago in this PR: #20386",
      "y": "Installing PyTorch from source fixes the error."
   },
   {
      "null": 211,
      "x": "I can't import PyTorch, libomp.dylib can't be loaded.",
      "z": "`brew install libomp` solves the problem.",
      "y": "To solve the problem, run `brew install libomp`."
   },
   {
      "null": 212,
      "x": "Slow distributed training",
      "z": "After using OMP_NUM_THREADS=1, the speed is back to normal. Thanks, @VitalyFedyunin.",
      "y": "Setting the flag `OMP_NUM_THREADS=1` makes it faster."
   },
   {
      "null": 213,
      "x": "Unable to compile an older version of PyTorch",
      "z": "Try `git submodule sync` and then `git submodule update --init`?",
      "y": "Run the following: `git submodule sync` and then `git submodule update --init`"
   },
   {
      "null": 214,
      "x": "torch.arange always generate constant result in tracing",
      "z": "Hi @lara-hdr, \\r\\n\\r\\nDo you mind linking to where you set `traceable=true` exactly? I'm not too familiar with that part of the code base.\\r\\n\\r\\n@zou3519 do you know why this has to be implemented in the python arg parser? it's also a schematized aten op. \\r\\n\\r\\n\\r\\n\\r\\n",
      "y": "This issue has been fixed."
   },
   {
      "null": 215,
      "x": "Unable to import 1.1 when installing with pip",
      "z": "Probably duplicate of #20030\\r\\ntry `brew install libomp `",
      "y": "To solve the problem, run `brew install libomp`."
   },
   {
      "null": 216,
      "x": "Is the `device=` parameter required in torch.FloatTensor and similar ones",
      "z": "`torch.FloatTensor` is a legacy constructor and doesn't support all types.\\r\\nTo answer your question, i think `torch.cuda.FloatTensor` might be what you are looking for -- but please dont use either. Just use `torch.empty(..., device='cuda')`",
      "y": "Please use  `torch.empty(..., device='cuda')` instead of `torch.cuda.FloatTensor` or `torch.FloatTensor`. "
   },
   {
      "null": 217,
      "x": "NCCL hang in PyTorch Distributed Data Parallel for Mixed Precision Training",
      "z": "I ran with n_procs=1 and suddenly a magic new error message appeared telling me to try adding `find_unused_parameters=True` to `DistributedDataParallel`. When I did that, it worked! Thank you for good error messages!!!",
      "y": "You need to set `n_proc=1` and `find_unused_parameters=True` to `DistributedDataParallel`."
   },
   {
      "null": 218,
      "x": "[FR] make IncompatibleKeys print nicer when there is no error",
      "z": "@designnner This is not an error. It's just the `repr` that isn't ideal. Feel free to ignore.",
      "y": "This error can be ignored."
   },
   {
      "null": 219,
      "x": "GRUcell has a wrong formula",
      "z": "either is fine, because it's all about semantics of z. that is, if z is considered an update gate, z * n makes sense, while (1-z)* n makes sense if we think of z as a leaky coefficient. in the end, they are equivalent.",
      "y": "The formula is correct."
   },
   {
      "null": 220,
      "x": "TensorBoard logging requires TensorBoard with Python summary writer installed. This should be available in 1.14 or above",
      "z": "I think it's a version problem.\\r\\njust run this:\\r\\n`pip install tensorboard==1.14.0`\\r\\n(not pip install tensorboard==1.14)",
      "y": "You need to install the latest version of Tensorboard."
   },
   {
      "null": 221,
      "x": "Building from source error: command 'gcc' failed with exit status 1",
      "z": "I have the same problem (`error: expected ')' before 'PRId64'` etc.) with gcc 4.8.5 on Linux with the current head ( 02450fff3)\\r\\n\\r\\nWhat works for me is to add `#define __STDC_FORMAT_MACROS` at the beginning of the following four files:\\r\\n* torch/csrc/Storage.cpp\\r\\n* torch/csrc/Tensor.cpp\\r\\n* torch/csrc/cuda/Storage.cpp\\r\\n* torch/csrc/cuda/Tensor.cpp\\r\\n\\r\\n(see also https://github.com/pytorch/pytorch/compare/master...andreh7:2017-11-10-prid64-fix -- this can be turned into a pull request very easily)\\r\\n\\r\\nThis fix is the same as #3574 but for different files. \\r\\n\\r\\n ",
      "y": "You need to add `#define __STDC_FORMAT_MACROS` at the beginning of the following four files:\\r\\n* torch/csrc/Storage.cpp\\r\\n* torch/csrc/Tensor.cpp\\r\\n* torch/csrc/cuda/Storage.cpp\\r\\n* torch/csrc/cuda/Tensor.cpp"
   },
   {
      "null": 222,
      "x": "RuntimeError when using DistributedDataParallel",
      "z": "I encountered this error message when using multiple multi-gpu machines. It did not occur when using a single multi-gpu machine - albeit with DataParallel, not DistributedDataParallel - nor when using multiple single-gpu machines (also didn't occur when using multi-gpu machines with ``CUDA_VISIBLE_DEVICES=1`` on both). I didn't try combinations of single- and multi- gpu as the OP here felt that caused his problem. PyTorch version 0.3.0.post4.\\r\\n\\r\\nI solved the problem by deleting a ``nn.Linear`` that I assigned to an attribute during the ``__init__`` of a custom ``nn.Module`` but never used during ``forward``.\\r\\n\\r\\nIMO this case deserves a better error message or should be documented with DistributedDataParallel.\\r\\n\\r\\nThanks!",
      "y": "The problem can be solved by deleting a ``nn.Linear`` that is assigned to an attribute during the ``__init__`` of a custom ``nn.Module``"
   },
   {
      "null": 223,
      "x": "GPU Memory Leak at Master Branch",
      "z": "Closing since this issue is stale, please reopen with a repro script if you still see the memory leak.",
      "y": "This issue has been fixed."
   },
   {
      "null": 224,
      "x": "Support view() on batch dimensions for non-contiguous tensors?",
      "z": "There's a good reason for the `view` invariant - most of such reshapes are impossible to pull off using stride tricks if then tensor isn't contiguous. On the other hand, making it contiguous inside `view` would mean that sometimes the returned tensor shares storage with input, and sometimes doesn't. This is important for cases like these: `x.view(-1)[::x.size(1) + 1] += c` (add `c` to diagonal of matrix `x`). If you know/suspect that tensors might sometimes be non-contiguous just add `.contiguous()` before `.view()` it's a no-op if the tensor already is contiguous",
      "y": "This feature will not be implemented."
   },
   {
      "null": 225,
      "x": "Slight memory leak for LSTM",
      "z": "Thanks @ngimel !\n\nI'll close this issue for now. If you see this issue, please upgrade to cudnn 7.1+, driver 384.69+.\n\n@Evpok @jiesutd @bangbangjim See above.",
      "y": "This issue has been fixed."
   },
   {
      "null": 226,
      "x": "Error in nll_loss - multi-target not supported",
      "z": "CrossEntropyLoss takes a 1D tensor. If your `target` has size `(32, 1)`, you need to squeeze the last dimension with `target.squeeze(1)` so it becomes a 1D tensor.",
      "y": "You need to squeeze the last dimension with `target.squeeze(1)` so it becomes a 1D tensor."
   },
   {
      "null": 227,
      "x": "softmax doesn't support negative dimensions",
      "z": "if it didn't support negative dims in 0.2.0 then i wont mark it as a release blocker for 0.3",
      "y": "This feature will not be implemented."
   },
   {
      "null": 228,
      "x": "torch.load() requires model module in the same folder",
      "z": "PyTorch internally uses pickle and it's a limitation of pickle. You can try meddling with `sys.path` to include the directory where `module.py` is. This is exactly why we recommend saving only the state dicts and not whole model objects.",
      "y": "You need to add the directory with your module to `sys.path`."
   },
   {
      "null": 229,
      "x": "Pytorch AssertionError: Torch not compiled with CUDA enabled",
      "z": "you are running this on OSX. The OSX binary of PyTorch does not come with GPU support.\\r\\n\\r\\nThe code you linked to still has some GPU stuff remaining (as you see from the stack-trace.\\r\\n\\r\\nChange these two lines to get_iterator:\\r\\nhttps://github.com/eladhoffer/captionGen/blob/48552694775ef11f5fa68c100584f40d98e4b690/main.py#L96\\r\\n\\r\\n`get_iterator(..., pin_memory=False)`\\r\\n",
      "y": "PyTorch does not support GPU on OSX"
   },
   {
      "null": 230,
      "x": "Variable methods which need to change before we combine Variable and Tensor",
      "z": "We have additional work to do before we can combined Variable and Tensor, but all these methods are implemented.",
      "y": "These methods are already implemented."
   },
   {
      "null": 231,
      "x": "using torch.utils.data.Dataset to make my dataset, find the index is out of the len defined in the __len__",
      "z": "This confirms my worst fears, python allows for pathologic iterables that have a length but never end... The fact that the `reversed` trick works is even weirder from a logical point of view: Reversing an infinite iterable results in a finite iterable... :woozy_face:\\r\\n\\r\\nAfter digging a bit through the [python reference](https://docs.python.org/3/reference/datamodel.html#object.__getitem__), the intended way of handling this is by raising an `IndexError` from `__getitem__` for invalid indices. Upon encountering this, the for loop will stop automagically. Changing my above example like below makes the iteration work as expected\\r\\n\\r\\n```python\\r\\nfrom torch.utils.data import Dataset\\r\\n\\r\\nclass TestDataset(Dataset):\\r\\n def __init__(self):\\r\\n super().__init__()\\r\\n\\r\\n def __getitem__(self, i):\\r\\n if i < 0 or i >= len(self): # These two lines\\r\\n raise IndexError() # are new\\r\\n return 0\\r\\n\\r\\n def __len__(self):\\r\\n return 10\\r\\n\\r\\n\\r\\ndataset = TestDataset()\\r\\nfor i, data in enumerate(dataset):\\r\\n print(i)\\r\\n assert i < len(dataset)\\r\\n```\\r\\n\\r\\nI believe this issue can be closed, as it's not pytorch specific. (Of course, @minghuisvn you're free to object, as I kind of highjacked your issue thread :wink:)",
      "y": "This issue is not related to PyTorch, but to Python."
   },
   {
      "null": 232,
      "x": "Windows source build fails with 'error LNK2019' at linking stage",
      "z": "I found what causing the problem, and wanted to inform you since it might be useful. `BUILD_TEST=0` variable is causing this problem at linking stage. If that not being set it builds successfully.",
      "y": "Unset the flag `BUILD_TEST=0` to fix the error."
   },
   {
      "null": 233,
      "x": "Too few arguments to vulkanOptimizeForMobile()",
      "z": "The issue was fixed in https://github.com/pytorch/pytorch/pull/45052",
      "y": "This issue has been fixed."
   },
   {
      "null": 234,
      "x": "CXXABI_* and GLIBCXX_* not found on gcc 4.8.2 after building Pre-cxx11 ABI Libtorch from source using gcc 5.4.0",
      "z": "Can you share a few more details how you've built gcc-5.4.0?\\r\\nBy default, GCC is coupled with a version of `libstdc++`, but there is a way to configure the build to use version already available in the system path. \\r\\nOtherwise, it sort of expected behaviour: shared library/executable compiled against newer version of libstdc++ is not compatible with an old one.\\r\\n\\r\\nPyTorch binary is build using `devtoolset-7`, which can be installed using something like the following \\r\\nhttps://github.com/pytorch/builder/blob/589a615fc8a8ee24690a1037ba583d32f22bc3a3/manywheel/Dockerfile#L12-L14\\r\\n\\r\\nFollowing article describes process of installing newer toolchain on CentOS in a bit more detail: https://ahelpme.com/linux/centos7/how-to-install-new-gcc-and-development-tools-under-centos-7/",
      "y": "Building PyTorch binary using `devtoolset-7` can fix this issue."
   },
   {
      "null": 235,
      "x": "nn.Module.script",
      "z": "PyText is replacing `ScriptVocab` with TorchText's `Vocab`. However we have to add this `to_ivalue` to our transforms in order to scriptify them because of this issue. It would be great if users like us don't need to add this additional function to all layers to keep our interface clean. Thanks! CC @hudeven",
      "y": "Please use  TorchText's `Vocab` instead of `ScriptVocab`."
   },
   {
      "null": 236,
      "x": "Add RMSE loss function",
      "z": "Actually, there is probably no advantage in having RMSE as a loss function. It's more computationally expensive than MSE and I don't see how it can be helpful to compensate for that. What I actually wanted is ready-made RMSE metric, not a loss function. Not sure if having a library of metrics is in scope for pytorch, so feel free to close this if the issue is not useful.",
      "y": "This feature will not be implemented."
   },
   {
      "null": 237,
      "x": "Is there a typo in ReLU6 declaration ?",
      "z": "nope if you look at the documentation of `relu6` and `hardtanh` it's reasonable to do subclass. Closing since it's resolved.",
      "y": "No, there isn't."
   },
   {
      "null": 238,
      "x": "Ellipsis support for view and reshape functions",
      "z": "I'm afraid this could be an ill-defined operation in general (specially if mixed with `-1` reshaping).\\r\\n\\r\\nFor the most common cases of flattening only a subset of dimensions, we have `tensor.flatten(start, end)`, which IMO is a good compromise in veneral.",
      "y": "This feature will not be implemented."
   },
   {
      "null": 239,
      "x": "Static Quantized model accuracy varies greatly with Calibration data",
      "z": "Using numeric suite I was able to debug and fix the issue.\nAppreciate all your help and time.",
      "y": "This issue can be fixed using numeric suite."
   },
   {
      "null": 240,
      "x": "test_nn.py returns inconsistent result in different test setup",
      "z": "Here's a reliable reproduction\\r\\n```\\r\\nimport torch\\r\\n\\r\\ninput_channels = 3\\r\\noutput_channels = 3\\r\\nbatch_size = 2\\r\\ndepth=3\\r\\nheight = 5\\r\\nwidth = 5\\r\\nkernel = 1\\r\\nstride = 1\\r\\nwith torch.backends.mkldnn.flags(enabled=False):\\r\\n conv_op = torch.nn.Conv3d(\\r\\n input_channels,\\r\\n output_channels,\\r\\n kernel,\\r\\n bias=False, # No bias\\r\\n ).to(dtype=torch.double)\\r\\n input = torch.randn(batch_size, input_channels, depth, height, width, dtype=torch.double, requires_grad=True)\\r\\n out = conv_op(input)\\r\\n gO = torch.rand_like(out)\\r\\n out.backward(gO)\\r\\n print(conv_op.weight.grad)\\r\\n```\\r\\nThe issue is on cpu, not on cuda.",
      "y": "This issue does not arise while using GPU."
   },
   {
      "null": 241,
      "x": "Nightly builds for cp38 missing for non-windows targets",
      "z": "Looks like we neglected to add them to `master`, submitted #34732 to remedy that",
      "y": "This issue has been fixed."
   },
   {
      "null": 242,
      "x": "Wrong Result when Converting Odd Integers Larger than 2^24 to Tensor",
      "z": "This is expected, as it's a float32 limitation.\\r\\nFrom the [Wikipedia article](https://en.wikipedia.org/wiki/Single-precision_floating-point_format):\\r\\n> Integers between 0 and 16777216 can be exactly represented (also applies for negative integers between \u221216777216 and 0)\\r\\nIntegers between `2**24=16777216` and `2**25=33554432` round to a multiple of 2 (even number)\\r\\nIntegers between `2**25` and `2**26` round to a multiple of 4\\r\\n...",
      "y": "This is an expected behavior."
   },
   {
      "null": 243,
      "x": "Building wheel for torch (setup.py) ... error - While running pip install torch",
      "z": "You could not do `pip install torch` because the Windows packages are not hosted on PyPI. Instead, please enter the commands in https://pytorch.org.",
      "y": "Install using instructions from PyTorch's website."
   },
   {
      "null": 244,
      "x": "load_state_dict_from_url error with weights downloaded from Google Drive ?",
      "z": "That URL is not a PyTorch file (.pth). It's a webpage with a link to a PyTorch file.\\r\\n\\r\\nYou need a direct download URL. Google drive doesn't support that directly, but you can use something like https://sites.google.com/site/gdocs2direct/",
      "y": "Please use a direct download URL, something like `https://sites.google.com/site/gdocs2direct/`"
   },
   {
      "null": 245,
      "x": "Support arbitrary types in jit",
      "z": "We have support for classes in TorchScript, is there something in particular that's missing that you're looking for? NamedTuples are also supported. We also have other things like enum.Enum and @dataclass on our roadmap but no concrete timeline yet. We're also introducing a way to bind C++ classes into TorchScript in our next release.",
      "y": "These methods are already implemented."
   },
   {
      "null": 246,
      "x": "[C++ API] Support for CIFAR10 and CIFAR100 Datasets",
      "z": "Not yet, will try to add one by this weekend as an example",
      "y": "They will be added soon"
   },
   {
      "null": 247,
      "x": "Cannot use setup.py install",
      "z": "> ModuleNotFoundError: No module named 'past'\\r\\n\\r\\nModule `past` is part of `python3-future` package on Ubuntu (or just `pip3 install future`)",
      "y": "You need to install the package 'future'"
   },
   {
      "null": 248,
      "x": "torch.cat is moving Tensors across devices silently",
      "z": "One case (hopefully supported in future) where moving between devices makes sense:\n`x = torch.cat([torch.rand(3, device = 'cuda'), 1])` for appending python scalars to a tensor",
      "y": "This is an expected behavior."
   },
   {
      "null": 249,
      "x": "Compilation error on aarch64",
      "z": "I ran into the same issue on aarch64. I am able to work around it by downloading sleef 3.5.1, install it to system and set the USE_SYSTEM_SLEEF=ON flag.\n\nHope it helps",
      "y": "You need to set the flag `USE_SYSTEM_SLEEF=ON`."
   },
   {
      "null": 250,
      "x": "[feature request] Allow `torch.unsqueeze` to insert multiple new dims",
      "z": "@tshadley \\r\\n\\r\\n> ```\\r\\n> import torch\\r\\n> t_b = t[...,(None,)*3]\\r\\n> print(t_b.shape)\\r\\n> ```\\r\\n\\r\\nTo unsqueeze at the end, you could try to use `t_b = t[(..., ) + (None, ) * 3]` (works for me on version 1.1.0).",
      "y": "You can use `t_b = t[(..., ) + (None, ) * 3]`"
   },
   {
      "null": 251,
      "x": "[feature request] add `torch.find` to find the indices of values",
      "z": "A not very optimized version of this function can be obtained with a one-liner I believe (for 1d `values`)\\r\\n```python\\r\\ndef find(tensor, values):\\r\\n return torch.nonzero(tensor[..., None] == values)\\r\\n```",
      "y": "You can use \\r\\n```python\\r\\ndef find(tensor, values):\\r\\n return torch.nonzero(tensor[..., None] == values)\\r\\n```"
   },
   {
      "null": 252,
      "x": "Segmentation Fault using dist.broadcast() with openmpi",
      "z": "Actually I tested a simple cuda program doing MPI_Bcast/MPI_allreduce and confirmed that it also segfaults there with openmpi 1.10, while it works fine on openmpi 2.1+. I'm more inclined to a openmpi issue in this case. I will make a PR to add this warning to the doc I think.",
      "y": "Installing the latest version of openmpi fixes this issue."
   },
   {
      "null": 253,
      "x": ".topk() returns incorrect values + indeces on non-contiguous tensors (CUDA)",
      "z": "The reason this fails is `topk` needs contiguous inputs and the outputs of Beta.sample aren't.\\r\\nCan you retitle the bug report to that?\\r\\nI'll submit a PR to check contiguous in topk.",
      "y": "You need to pass contiguous inputs to `topk`"
   },
   {
      "null": 254,
      "x": "OOM when using Adam optimizer compared to SGD when using same batch size.",
      "z": "Adam is more stateful than SGD, so it is expected that it uses more memory (proportional to the total size of the optimized parameters).",
      "y": "Adam is more stateful than SGD, so it is expected that it uses more memory."
   },
   {
      "null": 255,
      "x": "log_prob returns positive values for small cov",
      "z": "The values of the `pdf` can be arbitrarily large but never negative. This is completely acceptable, and I am sure this is not a bug.\\r\\n\\r\\nSimple example: normal distribution with standard deviation = 0.000001 and mean = 0. The `pdf` at `x = 0` is `1000 / (2 * pi)`, and the natural log of this is clearly greater than 0.",
      "y": "The values of the `pdf` can be arbitrarily large but never negative. "
   },
   {
      "null": 256,
      "x": "\\\"ImportError: No module named tools.setup_helpers.env\\\" when \\\"python setup.py egg_info\\\"",
      "z": "Does it work if you do \"FULL_CAFFE2=1 python setup.py install\" instead?",
      "y": "Please install using the flag `FULL_CAFFE2=1`"
   },
   {
      "null": 257,
      "x": "[bug] Multiplication of tensor with numpy scalar does not always work",
      "z": "The problem is that the left operand is the default operand to execute the `__mul__`.\\r\\nThe numpy scalar's `__mul__` will then call the tensor's `__array__` and that fails (and there isn't a way out).\\r\\nThe solution seems to be to set a high `__array_priority__`, then the scalar (and an array) will call the Tensor's `__rmul__` instead.\\r\\nStandard numpy scalars have a strongly negative priority, standard ndarrays have one of 0.\\r\\nSo in case 2:\\r\\n```\\r\\ntensor.__array_priority__ = 1000\\r\\nprint (scalar * tensor)\\r\\n```\\r\\nworks!\\r\\n\\r\\nI'm happy to send a PR.\\r\\n",
      "y": "The problem is that the left operand is the default operand to execute the `__mul__`.\\r\\nThe numpy scalar's `__mul__` will then call the tensor's `__array__` and that fails (and there isn't a way out).\\r\\nThe solution seems to be to set a high `__array_priority__`, then the scalar (and an array) will call the Tensor's `__rmul__` instead."
   },
   {
      "null": 258,
      "x": "git clone --recursive https://github.com/caffe2/caffe2.git gives error that Eigen repository is not found.",
      "z": "Sorry, I did not realize you were trying to install detectron. It looks like detectron expects to find Caffe2 through find_package, which right now requires Caffe2 to be built from source. What's causing that error message is that a source build (through Caffe2's cmake) populates a \"Caffe2 target\" that tells other cmake projects (Detectron) where to find Caffe2. These extra cmake files aren't included in the pre-built packages because you shouldn't need cmake to install and use Caffe2.\n\nWhen installing from source, do still read through https://caffe2.ai/docs/faq.html#why-do-i-get-import-errors-in-python-when-i-try-to-use-caffe2 and follow it's recommendations; it can save you from a lot of frustrating errors.",
      "y": "Installing Caffe2 from source fixes the error."
   },
   {
      "null": 259,
      "x": "[jit] support at::optional",
      "z": "cc @wanchaol \\r\\n\\r\\n@SsnL the nan is a temporary stopgap. I believe the end state is indeed to support `at::optional` or something similar.",
      "y": "This is an expected behavior."
   },
   {
      "null": 260,
      "x": "[distributions] dirichlet pathwise gradient does not work well with .expand",
      "z": "I think this should be closed now.",
      "y": "This issue has been fixed."
   },
   {
      "null": 261,
      "x": "[feature request] Add option to return matched / unmatched / unexpected in `load_state_dict`",
      "z": "After a lot of discussion, we decided not to move on with this proposal.",
      "y": "This feature will not be implemented."
   },
   {
      "null": 262,
      "x": "RuntimeError: cuda runtime error (30) on Ubuntu18,CUDA9.1,cudnn7.0.5 when torch.cuda.is_available() returns True",
      "z": "maybe \\\"sudo python\\\" can solve it",
      "y": "Use `sudo python` to fix this."
   },
   {
      "null": 263,
      "x": "Add python 3.7 to binary install page",
      "z": "python 3.7 binaries are live on PyPI, conda and on https://pytorch.org",
      "y": "python 3.7 binaries are live on PyPI, conda and on https://pytorch.org"
   },
   {
      "null": 264,
      "x": "Caffe2 Train your own image",
      "z": "@aswin1980 Check out the \\\"CIFAR10_Part1\\\" and \\\"CIFAR10_Part2\\\" tutorials in the [caffe2/tutorials](https://github.com/caffe2/tutorials/) repo. Part 1 specifically shows how to take a custom image dataset (in this case .png mirror of the CIFAR-10 dataset), format it, create image LMDBs, and train a model on them.",
      "y": "Please refer to the \\\"CIFAR10_Part1\\\" and \\\"CIFAR10_Part2\\\" tutorials in the [caffe2/tutorials](https://github.com/caffe2/tutorials/) repo. "
   },
   {
      "null": 265,
      "x": "Conda Install: PackageNotFoundError",
      "z": "@pjh5 \\r\\n\\r\\nUpdating conda to 4.5.4 and then running `conda create -n pytorch python=3` instead of `conda create -n pytorch anaconda` fixed it! I'm able to properly install pytorch now by running `conda install pytorch torchvision -c pytorch`. \\r\\n\\r\\nThanks for the help.",
      "y": "This can be fixed by updating conda to 4.5.4 and then running `conda create -n pytorch python=3` instead of `conda create -n pytorch anaconda`."
   },
   {
      "null": 266,
      "x": "Always get error \\\"ConnectionResetError: [Errno 104] Connection reset by peer\\\"",
      "z": "for me, I found set thread number equal 0 will solve this problem, namely:\\r\\nnum_workers=0\\r\\n",
      "y": "Setting `num_workers=0` will fix this problem"
   },
   {
      "null": 267,
      "x": "[Bug] Segmentation fault when importing fastText (with v0.4.0)",
      "z": "For the record, the problem was:\\r\\n - in a conda environment, I installed pytorch with `conda install`(as described on pytorch web site) and fastText with `pip install .` from their git clone.\\r\\n - that resulted in a segfault when doing `import fastText` and `import torch` \\r\\n\\r\\nReason:\\r\\n - pytorch is compiled with gcc 4.9.2\\r\\n - conda's default gcc is 4.8.5\\r\\n\\r\\nFix:\\r\\n - install gcc-4.9 in conda (e.g. `conda install -c serge-sans-paille gcc_49`)\\r\\n - install pytorch with `conda install` (in my case, `conda install pytorch torchvision cuda90 -c pytorch`)\\r\\n - install fastText with gcc-4.9 compiler: `CC=gcc-4.9 pip install .` in the fastText git clone\\r\\n\\r\\nThat's it! Thanks a lot @weiyangfb and @SsnL for your help!",
      "y": "To fix:\\r\\n - install gcc-4.9 in conda\\r\\n - install pytorch with `conda install` r\\n - install fastText with gcc-4.9 compiler: `CC=gcc-4.9 pip install .` in the fastText git clone"
   },
   {
      "null": 268,
      "x": "How to set USE_OPENVB=ON and BUILD_CAFF2=ON when build from source",
      "z": "@Tianji95 this problem is outdated. The FULL_CAFFE2 flag no longer exists and is no longer needed.",
      "y": "This issue has been fixed."
   },
   {
      "null": 269,
      "x": "[feature request] torch.isinf, torch.isfinite",
      "z": "`torch.isinf` merged in, `torch.isfinite` not implemented yet",
      "y": "`torch.isinf` has been merged; `torch.isfinite` has not been implemented yet"
   },
   {
      "null": 270,
      "x": "cannot reload on CPU model saved on GPU",
      "z": "When you do `torch.load(.....)`, set `torch.load(...., map_location='cpu')`. If you don't, since you are loading GPU tensors, PyTorch will try to reconstruct GPU tensors, and fail.",
      "y": "When you do `torch.load(.....)`, set `torch.load(...., map_location='cpu')`."
   },
   {
      "null": 271,
      "x": "setting CUDA_VISIBLE_DEVICES just has no effect",
      "z": "You need to do that before import pytorch.",
      "y": "You need to do that before import pytorch."
   },
   {
      "null": 272,
      "x": "[feature request] nn.Identity",
      "z": "You can use nn.Sequential() to simulate identity",
      "y": "You can use nn.Sequential() to simulate identity"
   },
   {
      "null": 273,
      "x": "Big drop in performance for larger batch size for otherwise same training script",
      "z": "You should tune different hyparams (e.g., lr) to accommodate different batch size. Some say that larger batch size requires larger lr too.",
      "y": "You should tune different hyparams (e.g., lr) to accommodate different batch size. Larger batch size requires larger lr too."
   },
   {
      "null": 274,
      "x": "torch.save() and nn.DataParallel()",
      "z": "I usually do:\\r\\n```\\r\\ntry:\\r\\n state_dict = model.module.state_dict()\\r\\nexcept AttributeError:\\r\\n state_dict = model.state_dict()\\r\\n```",
      "y": "Use the following :\\r\\n```\\r\\ntry:\\r\\n state_dict = model.module.state_dict()\\r\\nexcept AttributeError:\\r\\n state_dict = model.state_dict()\\r\\n```"
   },
   {
      "null": 275,
      "x": "torch.Tensor.__repr__ is slow",
      "z": "This has been fixed in master. Here are my timings: ...",
      "y": "This issue has been fixed."
   },
   {
      "null": 276,
      "x": "[feature request] Convert \\\"indices\\\" variable in \\\"torch.utils.data.dataset.random_split\\\" to list",
      "z": "I think we can just add a .tolist() after the code in here. Could you send a PR?",
      "y": "You can add a `.tolist()` after the code."
   },
   {
      "null": 277,
      "x": "problem building with ROCm",
      "z": "There's a \\\"hipify\\\" step to replace all the CUDA references with HIP/ROCm references in-place: \\\"python tools/amd_build/build_pytorch_amd.py\\\". Also, don't forget to set env var USE_ROCM to 1. After that, you should be able to build using the normal \\\"python setup.py\\\" step.",
      "y": "Please set the environment variable `USE_ROCM=1`"
   },
   {
      "null": 278,
      "x": "RNN weights are not Xavier-initialized",
      "z": "Also, adding to @Kaixhin 's point, if you really want Xavier initialization, you could try initializing manually using `nn.init.xavier_uniform` / `nn.init.xavier_normal`.",
      "y": "You cant try initializing manually using `nn.init.xavier_uniform` / `nn.init.xavier_normal`."
   },
   {
      "null": 279,
      "x": "Sequential does not allow muti-output modules such as RNN, LSTM",
      "z": "As I said in forum, this is not a bug.",
      "y": "This is an expected behavior."
   },
   {
      "null": 280,
      "x": "Weights won't update during backpropogation",
      "z": "your learning rate simply has to be increased, and as Issam pointed out SGD has quite a bit of variance in the gradients.",
      "y": "Your learning rate has to be increased."
   },
   {
      "null": 281,
      "x": "error: identifier \\\"__half_as_ushort\\\" is undefined",
      "z": "It tends out to be my own installation problem. cuda_fp16.h is not inside /usr/local/cuda/include but I've found it in /usr/local/cuda/targets/x86_64-linux/include. Maybe because I used to have an older version of cuda installed.\\r\\n\\r\\nAnyway, I completely removed cuda and reinstalled it. All the header files are now in place and built with no problem.",
      "y": "This error can be resolved by reinstalling CUDA."
   },
   {
      "null": 282,
      "x": "When I was training a CNN+GRU model with CTC loss, I got the nan loss after several batches.",
      "z": "I meet the same error on pytorch 1.0.0.dev20181115 with inner ctc loss, but did not encounter this situation at pytorch 0.4 with warpctc loss",
      "y": "Install the latest version of PyTorch to fix this."
   },
   {
      "null": 283,
      "x": "Memory leak from Function.save_for_backward() when looping over batch",
      "z": "the `@staticmethod` stuff is the right / official way. We'll change the tutorials right away.",
      "y": "Use static methods to fix this issue."
   },
   {
      "null": 284,
      "x": "Segfault in neg, introduced in 3e6e81d",
      "z": "Fixed in #3433",
      "y": "This issue has been fixed."
   },
   {
      "null": 285,
      "x": "`torch.utils.data.DataLoader`",
      "z": "this is a HDF5 issue. The problem is that HDF5 concurrent reads aren't safe: \\r\\nhttps://github.com/pandas-dev/pandas/issues/12236\\r\\nhttps://github.com/pandas-dev/pandas/issues/14692\\r\\n\\r\\nTo actually allow concurrent reads for a file you have to use SWMR feature of HDF5: https://support.hdfgroup.org/HDF5/docNewFeatures/NewFeaturesSwmrDocs.html\\r\\n\\r\\n\\r\\n",
      "y": "This is a HDF5 issue. The problem is that HDF5 concurrent reads aren't safe. To actually allow concurrent reads for a file you have to use SWMR feature of HDF5."
   },
   {
      "null": 286,
      "x": "inconsistent behavior of max",
      "z": "this was fixed in master. will be part of the next release.",
      "y": "This issue has been fixed."
   },
   {
      "null": 287,
      "x": "Kaiming/Xavier initializer cannot deal with bias term",
      "z": "Correct me if I am wrong. I think those methods are not supposed to deal with bias terms. In the paper, bias terms are initialized to zeros, which you can achieve by `layer.bias.data.zero_()`",
      "y": "This can be achieved by `layer.bias.data.zero_()`"
   },
   {
      "null": 288,
      "x": "Implementation Discussion: Native CTC",
      "z": "You would have to use 1.0rc, I wasn't quite finished in time for 0.4.1.",
      "y": "This feature has been implemented."
   },
   {
      "null": 289,
      "x": "Can I plz has determinant function?",
      "z": "fixed on master via #3816",
      "y": "This feature has been implemented."
   },
   {
      "null": 290,
      "x": "Recent bug in torch.cat() on Variables?",
      "z": "Great, this is also fixed on the `v0.3.0` branch as of 9a67882",
      "y": "This issue has been fixed."
   },
   {
      "null": 291,
      "x": "cuda out of memory error when GPU0 memory is fully utilized",
      "z": "@TomHeaven did you set CUDA_VISIBLE_DEVICES outside the python process? if that's the case pytorch should not even have driver-level access to your GPU0.\\nIdeally: `CUDA_VISIBLE_DEVICES=1 python foo.py`",
      "y": "You need to set `CUDA_VISIBLE_DEVICES` outside the python process"
   },
   {
      "null": 292,
      "x": "RuntimeError: CUDA error (3): initialization error",
      "z": "as mentioned in http://pytorch.org/docs/master/notes/multiprocessing.html#sharing-cuda-tensors\\r\\n\\r\\ninsert this to the top of your script\\r\\n\\r\\n```\\r\\nimport torch\\r\\ntorch.multiprocessing.set_start_method(\\\"spawn\\\")\\r\\n```",
      "y": "Insert this to the top of your script\\r\\n\\r\\n```\\r\\nimport torch\\r\\ntorch.multiprocessing.set_start_method(\\\"spawn\\\")\\r\\n```"
   },
   {
      "null": 293,
      "x": "RuntimeError: context has already been set(multiprocessing)",
      "z": "Hi @pancho111203 ,\\r\\n\\r\\nYou might have other files in your project which also have a `if __name__ == '__main__':`. One workaround is to call the `set_start_method` with the `force` argument as: `set_start_method('forkserver', force=True)`. This solved the issue for me.\\r\\n\\r\\nRegards\\r\\nNabarun",
      "y": "You can call the `set_start_method` with the `force` argument as: `set_start_method('forkserver', force=True)`."
   },
   {
      "null": 294,
      "x": "Gradient Ascent Cross Entropy Loss",
      "z": "@cdjhz `(-loss).backward(); optimizer.step()`\n",
      "y": "Use this: `(-loss).backward(); optimizer.step()`"
   },
   {
      "null": 295,
      "x": "ParameterList and ModuleList with named modules or parameters",
      "z": "fixed via #3505",
      "y": "This issue has been fixed."
   },
   {
      "null": 296,
      "x": "PyTorch Implementation of Michael Jordan\u2019s lab's Perturbed SGD?",
      "z": "I think adding noise to gradients is simple. A simple `apply_` call should be able to iterate through all model parameters and add the noise to the gradients, after which `step` is called.\\r\\n\\r\\nI initially proposed adding Noisy SGD, but the proposal was rejected considering its triviality. Ref: https://github.com/pytorch/pytorch/pull/4332",
      "y": "A simple `apply_` call should be able to iterate through all model parameters and add the noise to the gradients, after which `step` is called."
   },
   {
      "null": 297,
      "x": "NameError: name 'logging' is not defined",
      "z": "I think this has been fixed on master, because the point of error is non-existent on master.",
      "y": "This issue has been fixed."
   },
   {
      "null": 298,
      "x": "Builing for a specific SM number",
      "z": "Yes. By example:\\r\\n```\\r\\nTORCH_CUDA_ARCH_LIST=\\\"5.2;6.1;7.0\\\" python setup.py install\\r\\n```",
      "y": "You can do this via:\\r\\n```\\r\\nTORCH_CUDA_ARCH_LIST=\\\"5.2;6.1;7.0\\\" python setup.py install\\r\\n```"
   },
   {
      "null": 299,
      "x": "[jit] torch.empty_like is different from eager mode",
      "z": "I think I can fix this - it's probably not an issue with `empty_like` in particular, but with all optional arguments. https://github.com/pytorch/pytorch/pull/22055 fixes one case of this.",
      "y": "This is not an issue with `empty_like` in particular, but with all optional arguments."
   },
   {
      "null": 300,
      "x": "RuntimeError: cuDNN error: CUDNN_STATUS_INTERNAL_ERROR in 1.1.0",
      "z": "Try to use this command from the [website](https://pytorch.org/get-started/locally/):\\r\\n```\\r\\npip3 install torch torchvision\\r\\n```\\r\\n\\r\\nThis should install PyTorch 1.2, CUDA10.0 for Python3.6 on a Linux OS.\\r\\nJust select whatever config matches your current setup and use the shown install command.",
      "y": "Use this command:\\r\\n```\\r\\npip3 install torch torchvision\\r\\n```"
   },
   {
      "null": 301,
      "x": "CI failure points to nonexistent code...",
      "z": "fixed via #23304 by @zou3519 :D",
      "y": "This issue has been fixed."
   },
   {
      "null": 302,
      "x": "torch.onnx._export does not support tensor sum with multiple dims",
      "z": "@yil8 - yes, you can install the nightly build of PyTorch to test this.",
      "y": "Install the latest version of PyTorch to fix this."
   },
   {
      "null": 303,
      "x": "issue with ONNX and PyTorch",
      "z": "@arijit17 - It is hard to say where exactly the issue is without looking at your model and export code. Could you please share the repro? \\r\\nFYI - if your model has input-dependent control flow, then ideally you will have to convert those modules to `ScriptModule` and decorate those modules' `forward()` method with with `@torch.jit.script` to enable correct JIT compilation. But once you do that the export process itself should not change.\\r\\nIf you can share a repro, we might be able to help better.",
      "y": "You have to convert those modules to `ScriptModule` and decorate those modules' `forward()` method with with `@torch.jit.script` to enable correct JIT compilation."
   },
   {
      "null": 304,
      "x": "Wrong distribution sampled by torch.multinomial on CUDA",
      "z": "Closed by #22183",
      "y": "This issue has been fixed."
   },
   {
      "null": 305,
      "x": "Inplace error if DistributedDataParallel module that contains a buffer is called twice",
      "z": "The cause here is that by default DDP modules broadcast the contents of the root process module's buffers to all processes at every forward pass, and this broadcasting counts as an inplace operation for the purposes versioning. The fix is to disable the broadcasting by setting `broadcast_buffers=False` in the DDP module constructor. Thanks @pietern for the help.",
      "y": "To fix this , disable the broadcasting by setting `broadcast_buffers=False` in the DDP module constructor."
   },
   {
      "null": 306,
      "x": "LR scheduler design bug !",
      "z": "It's in master: you need to compile from source for now.",
      "y": "This issue has been fixed."
   },
   {
      "null": 307,
      "x": "Segmentation fault Autograd",
      "z": "@shoukang by any chance you can try installing new Pytorch, we have done a lot in terms of threading recently. And your problem might be related.",
      "y": "This issue has been fixed."
   },
   {
      "null": 308,
      "x": "Why aren't torch.functional.sigmoid and torch.nn.functional.relu deprecated like torch.nn.functional.tanh?",
      "z": "Actually, in the current code, sigmoid is also deprecated in nn https://github.com/pytorch/pytorch/blob/master/torch/nn/functional.py#L1390. But not relu. Are you now saying that sigmoid is a general purpose mathematical function but relu isn't ?",
      "y": "In the current code, sigmoid is also deprecated in nn, but not relu."
   },
   {
      "null": 309,
      "x": "Building from source failed. Multiple errors in the printout",
      "z": "You can find good answer here: https://github.com/torch/torch7/issues/1190#issuecomment-498934400 (credits to @talkenig)\\r\\n\\r\\nCopy pasting for future reference:\\r\\n\\r\\nHad the same problem and got to the bottom of it. My configuration is a Tesla T4 with 410.92 driver and CUDA 9.2, Ubuntu 18.10.\\r\\nThe thing is, the torch installer tries for some reason to use the highest compute capability supported by the device (or the driver - not sure which), but ignores the compute capability supported by the CUDA toolkit.\\r\\nSo, in my case the device supports compute capability 7.5, but CUDA 9.2 supports only 7.0 or 7.2 (not sure which one). You can guess I got the same nvcc fatal : Unsupported gpu architecture 'compute_75'\\r\\nThe solution is to force the nvcc compile options to use a lower compute capability. This can be achieved by setting the following environment variable:\\r\\n\\r\\n`export TORCH_CUDA_ARCH_LIST=\\\"7.0\\\"`\\r\\n\\r\\nJust before running ./install.sh from the torch directory.\\r\\nNote that the list may contain more than one compute capability, e.g. it can be \\\"6.0 6.2 7.0 7.2\\\". This will be reflected in the CUDA_NVCC_FLAGS -gencode arch=compute_70,code=sm_70 and those will be outputted to the terminal during the build.\\r\\n",
      "y": "To fix this, export the following environment variable:\\r\\n\\r\\n`export TORCH_CUDA_ARCH_LIST=\\\"7.0\\\"`"
   },
   {
      "null": 310,
      "x": "[dataloader] Add a context= argument for multiprocessing",
      "z": "This is expected, because thed spawned workers does not see the dataset def.\\n I think the proper way to solve this is to add a `context=` argument to data loader, so that a global start method needs not be set.",
      "y": "This is expected, because thed spawned workers does not see the dataset def.\\n The proper way to solve this is to add a `context=` argument to data loader."
   },
   {
      "null": 311,
      "x": "Torch crashes when calling torch.rand(2,3)",
      "z": "your processor is old enough that it doesn't support SSE4.1/SSE4.2/SSE4.3 instruction. We do not support processors that dont have these features in binaries.\\r\\nYour only choice is to install from source: https://github.com/pytorch/pytorch#from-source",
      "y": "Your processor is old enough that it doesn't support SSE4.1/SSE4.2/SSE4.3 instruction. You have to install from source."
   },
   {
      "null": 312,
      "x": "No method to set the timeout for distributed Gloo backend",
      "z": "@ejoebstl this would helps a lot, i used to solve this problem by changing the default timeout and recompile the whole pytorch o(\u2565\ufe4f\u2565)o",
      "y": "This can be fixed by changing the default timeout and recompiling PyTorch"
   },
   {
      "null": 313,
      "x": "[PyTorch] Build error (NCCL) on Ubuntu 16.04",
      "z": "A simple workaround is to set `WITH_SYSTEM_NCCL` to `False` to force compile with provided NCCL. https://github.com/pytorch/pytorch/blob/master/tools/setup_helpers/nccl.py#L74\\r\\n\\r\\nAfter modifying this, I can compile PyTorch without error. \\r\\n",
      "y": "A simple workaround is to set `WITH_SYSTEM_NCCL` to `False` to force compile with provided NCCL. "
   },
   {
      "null": 314,
      "x": "Anaconda3, Ubuntu 16.04 Python 3.6 Caffe2 installation issue",
      "z": "\"It looks like you installed with setup_caffe2.py, as setuptools (the Python package behind setup.py scripts) is what installs .egg files.\\r\\n\\r\\nCould you try `pip uninstall caffe2` and `conda install -c caffe2 caffe2-cuda9.0-cudnn7` ? This way should be much faster too.\\r\\n\\r\\nIf the pip uninstall caffe2 doesn't seem to work, then you can manually uninstall by deleting every file and folder under /home/sam/anaconda3/envs/caffe36/ that has 'caffe' or 'caffe2' in the name.",
      "y": "You need to uninstall and reinstall Caffe2."
   },
   {
      "null": 315,
      "x": "BatchNorm2d when batch size 1 works, what is it doing?",
      "z": "That is normalizing `[B x C x *]` over the dimensions `[*]`",
      "y": "It is normalizing `[B x C x *]` over the dimensions `[*]`"
   },
   {
      "null": 316,
      "x": "can't rebuild with NO_CUDA=1 after clean",
      "z": "Doing the following seems to have fixed it; I'm not sure which of these is okay for clean to leave around:\\r\\n`\\r\\nrm -rf aten/src/ATen/Config.h aten/build/ third_party/build/ third_party/aten/\\r\\n`",
      "y": "Please run :\\r\\n`\\r\\nrm -rf aten/src/ATen/Config.h aten/build/ third_party/build/ third_party/aten/\\r\\n`"
   },
   {
      "null": 317,
      "x": "Where is the Caffe2 website?",
      "z": "https://github.com/caffe2/caffe2.github.io",
      "y": "It is at `https://github.com/caffe2/caffe2.github.io`."
   },
   {
      "null": 318,
      "x": "Error building from source CMakeFiles/Makefile2:201: recipe for target 'src/ATen/CMakeFiles/ATen.dir/all' failed",
      "z": "I had the same problem and renamed `THGeneral.h` from `conda/envs/<ENV_NAME>/` to something similar to what @rgreenblatt  said, and it was installed normally. Good luck to friends who are having the same problem.\\r\\n\\r\\n```\\r\\nmv ~/anaconda3/envs/<ENV_NAME>/include/TH/THGeneral.h ~/anaconda3/envs/<ENV_NAME>/include/TH/THGeneral.h.old\\r\\n```",
      "y": "Please run \\r\\n```\\r\\nmv ~/anaconda3/envs/<ENV_NAME>/include/TH/THGeneral.h ~/anaconda3/envs/<ENV_NAME>/include/TH/THGeneral.h.old\\r\\n```"
   },
   {
      "null": 319,
      "x": "Add `torch.pi` like `numpy.pi` and possibly other constants",
      "z": "Aren't all of those constants in math?",
      "y": "They exist in `math`"
   },
   {
      "null": 320,
      "x": "torch.irfft produces \\\"cuFFT error: CUFFT_ALLOC_FAILED\\\" when called after torch.rfft",
      "z": "Me either, it seems to work now. Maybe you fixed something along the way :) Maybe this can be closed, having fewer temp tensors along the way also helps. Might be even better if complex product gets implemented some day.\\nIt would be nice if the OOM exception was some standard PyTorch exception, not a CUDA one.",
      "y": "Having fewer temp tensors along the way can help."
   },
   {
      "null": 321,
      "x": "[feature request] Complex multiplication",
      "z": "One verbose way may be a kwarg `complex = True` to `torch.mul`. Some alternative ideas: `mul_complex` / `mulc`",
      "y": "Add a kwarg `complex = True` argument to `torch.mul`."
   },
   {
      "null": 322,
      "x": "cuda runtime error (48): no kernel image is available for execution on the device",
      "z": "Hi, your problem is stated in this warning here\\r\\n```\\r\\n/home/azat/anaconda2/lib/python2.7/site-packages/torch/cuda/__init__.py:97: UserWarning:\\r\\n Found GPU0 GeForce 820M which is of cuda capability 2.1.\\r\\n PyTorch no longer supports this GPU because it is too old.\\r\\n```\\r\\n\\r\\nYou can build from source to use some functionality, but there are many operations that require compute capability greater than SM_21 to perform.",
      "y": "You can build from source to use some functionality. Some operations require a more recent GPU."
   },
   {
      "null": 323,
      "x": "Stop using undefined tensors to represent zero gradients in engine",
      "z": "going to close this because I believe the issues with undefined tensors in the engine have been addressed.",
      "y": "This issue has been fixed."
   },
   {
      "null": 324,
      "x": "Proposal: rename upsample to resample",
      "z": "This function has been deprecated in favor of interpolate so I think this issue has been addressed.",
      "y": "Use the `interpolate` function instead."
   },
   {
      "null": 325,
      "x": "RuntimeError for indexing with high dimensional tensor only when using cuda",
      "z": "As for pytorch 0.4 (nightly build 2018.04.20, installed via conda) this problem does not exist anymore. And the following code works:\\r\\n```python\\r\\nimport torch\\r\\nx = torch.randn(2, 2, 2, 2, 2, 2).cuda()\\r\\ni = torch.cuda.LongTensor([0, 0, 0, 0, 0, 0])\\r\\nx[i,:]\\r\\n```\\r\\nClosing the issue.",
      "y": "This issue has been fixed."
   },
   {
      "null": 326,
      "x": "RuntimeError: reduce failed to synchronize: unspecified launch failure",
      "z": "I was having similar error. Make sure your layer has values that make sense to the BCELoss. If you for example output negative values and pass them to the logarithm, the training will fail.\\r\\n\\r\\nIn my case I was missing the last sigmoid activation to shrink the numbers between 0 and 1.",
      "y": "Make sure your layer has values that make sense to the BCELoss."
   },
   {
      "null": 327,
      "x": "Zombie process when use GPU",
      "z": "For anyone who still suffer from this issue, try the following command:\\r\\n`fuser -k /dev/nvidia*`\\r\\nor\\r\\n`kill $(lsof -t /dev/nvidia*)`",
      "y": "Use the following command:\\r\\n`fuser -k /dev/nvidia*`\\r\\nor\\r\\n`kill $(lsof -t /dev/nvidia*)`"
   },
   {
      "null": 328,
      "x": "torch.HalfTensor' object has no attribute 'mean'",
      "z": "we don't have math for CPU Half type (it would be very slow), convert it to cuda or CPU Float.",
      "y": "Please convert tensor to cuda or CPU Float."
   },
   {
      "null": 329,
      "x": "import torch; libcublas.so.9.0 error",
      "z": "I had the exact same linking problem when trying to compile pytorch 0.3 with CUDA 9.1. I couldn't figure out how it manages to find and link to cublas.8. I gave up and installed the `pip` wheel package which contains CUDA, CuDNN everything inside.",
      "y": "Installing from pip fixes this error."
   },
   {
      "null": 0,
      "x": "Expose find Dangling Impls to Python",
      "z": "> Can I have instructions on how to run tests within the test_dispatch.py? I know how to run cpp tests in the codebase but not Python ones.\n \n In an open source build can do `python test/test_dispatch.py`\n \n > Since the Dispatcher::singleton().findDanglingImpls() returns std::vector<c10::OperatorHandle>, should I just return the above vector directly to the Python layer? I'm not sure if we have already had bindings for OperatorHandle.\n \n Nope there are no bindings, and probably don't want to actually directly bind OperatorHandle as it wasn't designed for Python binding (unless... you really want to?) See original:\n \n > Ideally it would be easy to get the (1) name of dangling impls and (2) filename/lineno where the dangling impl occurred.",
      "y": "In an open source build can do `python test/test_dispatch.py` \n There are no bindings, and probably don't want to actually directly bind OperatorHandle as it wasn't designed for Python binding"
   },
   {
      "null": 1,
      "x": "Inconsistent CUDA errors using PyTorch Docker image",
      "z": "We usually close the issues that cannot be reproduced and thus aren't actionable. Feel free to reopen if needed.",
      "y": "Run the example on another machine, and didn't encounter any issues. The issue lies with the specific configuration of the original machine"
   },
   {
      "null": 2,
      "x": "Parametrization goes into infinite recursion when trying to print a module",
      "z": "Note that this is not specific to parametrization, anything your do such a cycle of Modules, you will get a recursion error when printing:\n \n ```python\n \n import torch\n \n \n \n a = torch.nn.Linear(2, 2)\n \n b = torch.nn.Linear(2, 2)\n \n \n \n a.b = b\n \n b.a =a \n \n \n \n print(a) # RecursionError\n \n ```",
      "y": "Proposed approach - One can make it work modifying just the internal tensor storage via the function set_ (https://pytorch.org/docs/stable/generated/torch.Tensor.set_.html). This should work in the case when, for example, you are sharing the mask between a number of layers. You would then set a new mask as:\n \n model.linear.mask.set_(torch.zeros(model.linear.weight.shape))\n A simplified implementation could then be:\n \n import torch\n import torch.nn as nn\n import torch.nn.utils.parametrize as parametrize\n \n class WeightMaskParametrization(nn.Module):\n  def __init__(self, mask):\n  super().__init__()\n  self.mask = mask\n \n  def forward(self, w):\n  return self.mask * w\n \n model = nn.Linear(3, 4)\n model.register_buffer('mask', torch.ones_like(model.weight))\n parametrize.register_parametrization(model, 'weight', WeightMaskParametrization(model.mask))\n \n print(model.weight) # print original weight\n model.mask.set_(torch.zeros_like(model.weight))\n print(model.weight) # print zeros\n In this simplified example, it would be even better to have the mask live in the class WeightMaskParametrization rather than in nn.Linear, and thus, be managed directly by this class, but I guess that the real example motivating this issue may not allow for that.\n \n Another thing to point out is that this WeightMaskParametrization is just pruning in disguise. We will eventually move all the pruning methods into parametrizations, and the base parametrization will certainly take the form of WeightMaskParametrization, with a few more bells and whistles."
   },
   {
      "null": 3,
      "x": "Improvement to unsupported backward OpInfo test",
      "z": "this is addressed in #60157",
      "y": null
   },
   {
      "null": 4,
      "x": "Something like nn.Dropout2d which does channel dropout but for 1d data. Could be called nn.Dropout1d",
      "z": "Hey @chanshing, thanks for the suggestion! Unless I'm mistaken, contrary to what the docs say, `Dropout2d` supports entire channel dropout for 1D data without the need for a dummy axis:\n \n \n \n ```python\n \n >>> torch.nn.Dropout2d()(torch.randn(2, 3, 4))\n \n tensor([[[ 2.0013, 0.5137, 4.6231, -0.8030],\n \n  [ 0.2068, 1.2131, 1.2506, 2.1023],\n \n  [ 0.0000, -0.0000, 0.0000, -0.0000]],\n \n \n \n  [[-2.2049, -4.3484, 0.4871, 1.2764],\n \n  [-0.0000, 0.0000, 0.0000, 0.0000],\n \n  [-1.0111, -0.5624, 0.7527, -0.0970]]])\n \n ```\n \n \n \n Possible alternative may be to support a generic, properly-documented `Dropout` with configurable dims over which to dropout. See https://github.com/pytorch/pytorch/issues/46184 as well.",
      "y": "Contrary to what the docs say, `Dropout2d` supports entire channel dropout for 1D data without the need for a dummy axis:\n \n ```python\n >>> torch.nn.Dropout2d()(torch.randn(2, 3, 4))\n tensor([[[ 2.0013, 0.5137, 4.6231, -0.8030],\n  [ 0.2068, 1.2131, 1.2506, 2.1023],\n  [ 0.0000, -0.0000, 0.0000, -0.0000]],\n \n  [[-2.2049, -4.3484, 0.4871, 1.2764],\n  [-0.0000, 0.0000, 0.0000, 0.0000],\n  [-1.0111, -0.5624, 0.7527, -0.0970]]])\n ```\n \n Possible alternative may be to support a generic, properly-documented `Dropout` with configurable dims over which to dropout. See https://github.com/pytorch/pytorch/issues/46184 as well."
   },
   {
      "null": 5,
      "x": "Issue in executing the script module in C++",
      "z": "You need to call result.toTuple() instead of result.toTensor():\n ```C++\n auto result = model({inputs});\n for (auto& t : result.toTuple())\n {\n  // t is a Tensor now\n }\n ```\n \n So you utilise the same code path for all models, I recommend pushing all tensors to a vector. You can also interogate the result to find what type it is.",
      "y": "Call result.toTuple() instead of result.toTensor():\n ```C++\n auto result = model({inputs});\n for (auto& t : result.toTuple())\n {\n  // t is a Tensor now\n }\n ```\n \n Recommend to push all tensors to a vector. You can also interogate the result to find what type it is."
   },
   {
      "null": 6,
      "x": "Deterministic indexing operation fails in indices size check / missing broadcast",
      "z": "Too old `setuptools` version, the `.major` attribute came in in `49.6.0` (says https://github.com/spyder-ide/spyder/pull/15886).\n \n \n \n @andfoy that seems too recent to require, and distros may not have it. I think you need to change that in a way so it works with any `setuptools` version.\n \n \n \n FYI @ezyang who landed this - it's going to break elsewhere too.",
      "y": "Old `setuptools` version, the `.major` attribute came in in `49.6.0` (says https://github.com/spyder-ide/spyder/pull/15886)."
   },
   {
      "null": 7,
      "x": "How to get in touch about a security issue?",
      "z": "Reach out to @jspisak",
      "y": "If you believe you have found a security vulnerability in PyTorch, we encourage you to let us know right away. We will investigate all legitimate reports and do our best to quickly fix the problem.\n \n Please refer to the following page for our responsible disclosure policy, reward guidelines, and those things that should not be reported:\n \n https://www.facebook.com/whitehat"
   },
   {
      "null": 8,
      "x": "Help, nn.functional.interpolate make no sense at image resizing",
      "z": "I suspect you want to use https://pytorch.org/vision/stable/transforms.html#torchvision.transforms.Resize to resize images.\n \n And please use https://discuss.pytorch.org/ to ask questions about PyTorch APIs\n \n Do not hesitate to open a new issue if you believe that torch.nn.functional.interpolate does not work as documented.",
      "y": "Use torchvision.transforms.Resize to resize images"
   },
   {
      "null": 9,
      "x": "TensorPipe: build error: 'CU_POINTER_ATTRIBUTE_DEVICE_ORDINAL' was not declared in this scope",
      "z": "So in truth we're not exactly tracking what version is the strict minimum supported, but for certain we don't support anything older than 9.2, because in https://github.com/pytorch/pytorch/pull/36848 we removed workaround for pre-9.2. Hence I'm in fact updating that error message you pointed to in https://github.com/pytorch/pytorch/pull/61462. I hope you managed to get CUDA updated...",
      "y": "Upgrade CUDA"
   },
   {
      "null": 10,
      "x": "BN+ReLU cause \"RuntimeError: MALFORMED INPUT: bad dtype in CompareSelect\" error in fp16, traced module",
      "z": "This looks like a bug in the current fuser. While we're looking at it, you could disable it by adding\n \n torch._C._jit_override_can_fuse_on_gpu(False)",
      "y": "Smaller reproducer that doesn't require CUDA \n \n import torch\n import torch._C._te as te\n \n input_str = \"\"\"\n graph(%x : Half(3, 3, strides=[3, 1], requires_grad=0, device=cpu),\n  %weight : Half(3, strides=[1], requires_grad=0, device=cpu),\n  %bias : Half(3, strides=[1], requires_grad=0, device=cpu),\n  %running_mean : Half(3, strides=[1], requires_grad=0, device=cpu),\n  %running_var : Half(3, strides=[1], requires_grad=0, device=cpu)):\n  %5 : bool = prim::Constant[value=1]()\n  %6 : float = prim::Constant[value=0.001]()\n  %7 : float = prim::Constant[value=0.10000000000000001]()\n  %8 : bool = prim::Constant[value=0]()\n  %input.1 : Half(3, 3, strides=[3, 1], requires_grad=0, device=cpu) = aten::batch_norm(%x, %weight, %bias, %running_mean, %running_var, %8, %7, %6, %5)\n  %10 : Half(3, 3, strides=[3, 1], requires_grad=0, device=cpu) = aten::relu(%input.1)\n  return (%10)\n \"\"\"\n \n \n class kernel_arena_scope(object):\n  def __enter__(self):\n  self.scope = torch._C._te.KernelScope()\n \n  def __exit__(self, typ, val, traceback):\n  self.scope = None\n \n with kernel_arena_scope():\n  graph = torch._C.parse_ir(input_str)\n  print(graph)\n  kernel = te.TensorExprKernel(graph) # Fails\n  print(kernel.get_code_text(\"asm\"))"
   },
   {
      "null": 11,
      "x": "mean' reduction result in CrossEntropyLoss mismatches with manually computing mean",
      "z": "This isn't a bug, but the documentation isn't clear -- you'd only find the explanation if you look at the documentation for NLLLoss.\n \n \n \n See https://github.com/pytorch/pytorch/issues/31295 for the case in NLLLoss.",
      "y": "This isn't a bug, look at the documentation for NLLLoss.\n \n See https://github.com/pytorch/pytorch/issues/31295 for the case in NLLLoss."
   },
   {
      "null": 12,
      "x": "forward compatibility was attempted on non supported HW",
      "z": "Just reboot or reset nvidia drivers.\n \n <https://stackoverflow.com/a/45319156/1391392>",
      "y": "Reboot or reset nvidia drivers.\n <https://stackoverflow.com/a/45319156/1391392>"
   },
   {
      "null": 13,
      "x": "torch.cuda.is_available() is False",
      "z": "> Is CUDA available: Yes\n \n \n \n But when you run `python -m torch.utils.collect_env`, it reports that `torch.cuda.is_available()` is True. If you are asking why `torch.distributed.is_available()` is always `False` on Windows, then it is because it is not supported yet (see https://github.com/pytorch/pytorch/issues/37068).",
      "y": "`torch.distributed.is_available()` is always `False` on Windows, it is because it is not supported yet (see https://github.com/pytorch/pytorch/issues/37068)."
   },
   {
      "null": 14,
      "x": "Using all_gather() in the forward pass in DDP throws RuntimeError",
      "z": "Hey @Ze-Yang, setting `find_unused_parameters=True` in DDP ctor would avoid the error, but I don't think this is what you want. IIUC, the root cause is that `dist.all_gather` is not an autograd function, so that all operations prior to `all_gather` is not linked to the `out` tensor in the autograd graph. As a result, DDP would treat those tensors as unused parameters.\n \n \n \n To address this problem, you can either implement an autograd function for `dist.all_gather` (see [this example](https://github.com/pytorch/pytorch/blob/b35cdc5200af963e410c0a25400fd07f30b89bca/torch/nn/parallel/_functions.py) for scatter and gather) or try if [RPC and distributed autograd](https://pytorch.org/docs/master/rpc.html) (have to use master or v1.6 release cut) can handle it for you. From performance's perspective, the former option might be better.",
      "y": "To address this problem, you can either implement an autograd function for `dist.all_gather` (see [this example](https://github.com/pytorch/pytorch/blob/b35cdc5200af963e410c0a25400fd07f30b89bca/torch/nn/parallel/_functions.py) for scatter and gather) or try if [RPC and distributed autograd](https://pytorch.org/docs/master/rpc.html) (have to use master or v1.6 release cut) can handle it for you."
   },
   {
      "null": 15,
      "x": "Implement autograd functions for c10d communication operations",
      "z": "That would be awesome if we can generalize autograd functions for c10d operations. TSC folks are manually implementing backward grad propagation for alltoall and reduce_scatter",
      "y": "implemented this in chainermn"
   },
   {
      "null": 16,
      "x": "pca_lowrank memory allocation",
      "z": "> A simple solution that worked for me is changing the above (and two other similar lines) line to:\n \n > \n \n > ```\n \n > (Q, _) = matmul(A_H-M_H, Q).qr()\n \n > ```\n \n \n \n This would be even worse because this would generate more large temporary tensors (`A_H - M_H`) during the iteration.\n \n Notice that `Q` has small number of columns so that `matmul(A_H, Q) - matmul(M_H, Q)` is very memory efficient compared to `matmul(A_H - M_H, Q)`.",
      "y": "matmul doesn't support broadcasting the singleton dimension \n Define M = C.as_strided(A.shape, (0, 1)) and then use _svd_lowrank(A, q, niter=niter, M=M)."
   },
   {
      "null": 17,
      "x": "How to do matrix multiplication between two 2D sparse directly and quickly",
      "z": "I found the TensorFlow has this function:\n \n tf.sparse_matmul(\n \n  a,\n \n  b,\n \n  transpose_a=False,\n \n  transpose_b=False,\n \n  a_is_sparse=False,\n \n  b_is_sparse=False,\n \n  name=None\n \n )\n \n I sincerely hope that PyTorch also has it!",
      "y": "Look for similar function in pytorch \n tf.sparse_matmul(\n  a,\n  b,\n  transpose_a=False,\n  transpose_b=False,\n  a_is_sparse=False,\n  b_is_sparse=False,\n  name=None\n )"
   },
   {
      "null": 18,
      "x": "Unable to use Pytorch with CUDA",
      "z": "Duplicate #20635 \n \n \n \n You can add `torch.cuda.current_device()` after `import torch` and it should fix the issue temporarily",
      "y": "add `torch.cuda.current_device()` after `import torch` and it should fix the issue"
   },
   {
      "null": 19,
      "x": "CPU torch.norm gives strange results for LargeTensor on Colab",
      "z": "This is a related to https://github.com/pytorch/pytorch/issues/20551",
      "y": "PyTorch 0.4 did the accumulation using double https://github.com/pytorch/pytorch/blob/v0.4.1/aten/src/TH/generic/THTensorMath.cpp#L4307\n \n Now it's using float accumulation:\n https://github.com/pytorch/pytorch/blob/master/aten/src/ATen/native/cpu/ReduceOpsKernel.cpp#L57\n \n CUDA uses float accumulation, but is saved because the necessary parallelism forces a form of pairwise summation. We should probably do the same thing for CPU"
   },
   {
      "null": 20,
      "x": "pytorch won't install from wheel",
      "z": "I'm experiencing the same issue, except with ```torch==1.7.1+cu110```.",
      "y": "workaround works, but it's very portable...\n wget https://download.pytorch.org/whl/cpu/torch-1.6.0%2Bcpu-cp37-cp37m-linux_x86_64.whl\n pip install torch-1.6.0+cpu-cp37-cp37m-linux_x86_64.whl \n Or use ```torch==1.7.1+cu110```."
   },
   {
      "null": 21,
      "x": "cannot import name 'ngrams_iterator'",
      "z": "I think this should be reported in https://github.com/pytorch/text .\n \n By the way you may want to fill the issue template and provide as much as information as possible, otherwise no one can simply reproduce your problem.",
      "y": "report to https://github.com/pytorch/tex"
   },
   {
      "null": 22,
      "x": "PyPI is slow please create git releases",
      "z": "Releases are already hosted on S3 you can find installation instructions that relate to https://download.pytorch.org on https://pytorch.org/get-started\n \n \n \n Example:\n \n \n \n ```\n \n pip install -f https://download.pytorch.org/whl/cu102/torch_stable.html torch\n \n ```",
      "y": "Releases are already hosted on S3 you can find installation instructions that relate to https://download.pytorch.org on https://pytorch.org/get-started\n \n \n \n Example:\n \n \n \n ```\n \n pip install -f https://download.pytorch.org/whl/cu102/torch_stable.html torch\n \n ```"
   },
   {
      "null": 23,
      "x": "Tensor print format issue on Nvidia Jetson devices",
      "z": "Tried the master branch code on Jetson (GCC7), and the results are correct. \n \n \n \n Compared to v1.7.1, a major change in the master branch is that vec256_float_neon.h has a new compiler check and only compiles for clang and GCC>8.3. So, for GCC7, the code that causes this issue is skipped.\n \n \n \n So this issue is kind of already fixed in the master branch ... by essentially disabling neon for Jetson.",
      "y": "disable neon for Jetson"
   },
   {
      "null": 24,
      "x": "Roll-up: remaining TH functions",
      "z": "LegacyFunctionsCPU is now gone, as of #58780.",
      "y": "LegacyFunctionsCPU is no longer available"
   },
   {
      "null": 25,
      "x": "type_as() method change device too",
      "z": "Because `type()` is like `torch.cuda.FloatTensor`. Which contains both the device and dtype.\n For backward compatibility, this function supports all of them. But the new `.to()` API should remove that confusion by only accepting clear arguments.",
      "y": "`type()` is like `torch.cuda.FloatTensor` which contains both the device and dtype."
   },
   {
      "null": 26,
      "x": "Error instaling using PIP and CUDA on windows 10",
      "z": "Please use 64-bit Python instead.",
      "y": "Please use 64-bit Python instead."
   },
   {
      "null": 27,
      "x": "torch.tril_indices returns a float tensor on master",
      "z": "Yes specifying the `dtype` works",
      "y": "specify `dtype` to make it work"
   },
   {
      "null": 28,
      "x": "Failed to build pytorch 1.4.0 on Ubuntu 18.04.4 LTS",
      "z": "The issue is with the system-wide pybind11-dev package (Version: 2.0.1-4). After removing it pytorch builds just fine.",
      "y": "The issue is with the system-wide pybind11-dev package (Version: 2.0.1-4). After removing it pytorch builds just fine."
   },
   {
      "null": 29,
      "x": "Complex number printing inconsistent with float",
      "z": "@anjali411,\n \n \n \n In the context to pretty printing multiple complex values it would be beneficial to keep the same formatting for every entry in the tensor. It would be nice to have the following:\n \n 1. `, ` should line up across multiple lines of output.\n \n 2. `+/-` should line up across multiple lines of output.\n \n 3. When the output is not abbreviated with `...` for length, it should be possible to copy the output string into an `eval(str)` and convert the number back into a python list or pytorch tensor.",
      "y": "In the context to pretty printing multiple complex values it would be beneficial to keep the same formatting for every entry in the tensor. It would be nice to have the following:\n 1. `, ` should line up across multiple lines of output.\n 2. `+/-` should line up across multiple lines of output.\n 3. When the output is not abbreviated with `...` for length, it should be possible to copy the output string into an `eval(str)` and convert the number back into a python list or pytorch tensor."
   },
   {
      "null": 30,
      "x": "[Complex] Incorrect Complex Tensor inference",
      "z": "I think I've run into this issue before. There needed to be a way to specify a number in another number system, so `j` is a suffix to the number 2, not a variable. See the following code snippet.\n \n ``` Python\n j = 10\n c_num = eval(\"1/2j\") # c_num = -0.5j (j is the suffix of 2)\n c_num = eval(\"(1/2)*j\") #c_num = 5 (j is a variable = 10)\n ```\n \n Python is an interpreted language so it would be very difficult interpret` j` as a suffix of `1/2` without breaking the case where `j` is a variable. I think the solution is to change hypothesis to the following:\n \n ``` Python\n j = 10\n c_num = eval(\"1j/2\") # c_num = 0.5j (j is the suffix of 1)\n ```\n \n It's confusing, but this method allows people to use a variable named `j`.",
      "y": "There needed to be a way to specify a number in another number system, so `j` is a suffix to the number 2, not a variable. See the following code snippet.\n \n ``` Python\n j = 10\n c_num = eval(\"1/2j\") # c_num = -0.5j (j is the suffix of 2)\n c_num = eval(\"(1/2)*j\") #c_num = 5 (j is a variable = 10)\n ```\n \n Python is an interpreted language so it would be very difficult interpret` j` as a suffix of `1/2` without breaking the case where `j` is a variable. I think the solution is to change hypothesis to the following:\n \n ``` Python\n j = 10\n c_num = eval(\"1j/2\") # c_num = 0.5j (j is the suffix of 1)\n ```\n \n It's confusing, but this method allows people to use a variable named `j`."
   },
   {
      "null": 31,
      "x": "[feature request] random integer generator",
      "z": "@Naman-ntc I recommend you to read the blogs on pytorch.org to understand the building system and the `ATEN` document.",
      "y": "read the blogs on pytorch.org to understand the building system and the `ATEN` document."
   },
   {
      "null": 32,
      "x": "Error in Function backward when forward output is in-place of forward input and input is not leaf",
      "z": "There are a few problems with your implementation. Most notably, you fail to use `ctx.mark_dirty` to signal that you've modified `a`. Secondly, you should use `.view(1)` instead of `.resize(1)`, because the later results in the output having **unspecified** contents. Finally, I tried reproducing your issues on `master`, but it seems to work just fine, so I'm closing the issue as resolved. Thanks for the report!",
      "y": "use `.view(1)` instead of `.resize(1)`, because the later results in the output having **unspecified** contents"
   },
   {
      "null": 33,
      "x": "[BUG?]The same code behaves differently on pytorch based on py2 and py3 whose version are both 0.3.0.post_4",
      "z": "from a rough scan, this looks like that it might be the culprit:\n \n ```\n \n  scale = math.sqrt(3 / fan_in)\n \n ```\n \n in py2, `int / int` is an `int`; in py3, it is a `float`.",
      "y": "from a rough scan, this looks like that it might be the culprit:\n \n ```\n \n  scale = math.sqrt(3 / fan_in)\n \n ```\n \n in py2, `int / int` is an `int`; in py3, it is a `float`."
   },
   {
      "null": 34,
      "x": "\"torch.sum()\" over a ByteTensor gives incorrect result when the \"dim\" is specified",
      "z": "This has been fixed in 0.4. Now reductions on byte tensors return the result as a long tensor (except if the explicit `dtype` is passed)",
      "y": "This has been fixed in 0.4. Now reductions on byte tensors return the result as a long tensor (except if the explicit `dtype` is passed)"
   },
   {
      "null": 35,
      "x": "Support F.normalize on 1-dim tensors without explicit dim",
      "z": "`normalize` already works on 1-dim, it's just that default `dim` is off for 1-dim tensors. making `dim = 0` or `dim=-1` for 1-dim tensors would solve this particular case.\n \n \n \n IMO , it makes less confusion when things work on 1-dim (good for quick testing purposes) by default compared to when they don't, even if such support is added gradually (like it happened for negative dims), it saves boiler-plate.",
      "y": "`normalize` already works on 1-dim, it's just that default `dim` is off for 1-dim tensors. making `dim = 0` or `dim=-1` for 1-dim tensors would solve this particular case."
   },
   {
      "null": 36,
      "x": "Add support similar to `tf.images.resize_images`.",
      "z": "this should be done, indeed it's much more elegant to have a differentiable resize_images, and we can likely build it on top of `grid_sample` that we have.",
      "y": "We now have nn.functional.interpolate that does the same thing except for bicubic method. https://pytorch.org/docs/master/nn.html?highlight=interpolate#torch.nn.functional.interpolate"
   },
   {
      "null": 37,
      "x": "memory leaky on DataLoader",
      "z": "The reason in your last snippet is that the loss is still in scope when you do the testing loop, so it holds up the whole computation graph. You can either delete it before the testing starts `del loss` and `del classes`, or put the train loop in a separate function (and thus the scope ends with the function)",
      "y": "loss is still in scope when you do the testing loop, so it holds up the whole computation graph. You can either delete it before the testing starts `del loss` and `del classes`, or put the train loop in a separate function (and thus the scope ends with the function)"
   },
   {
      "null": 38,
      "x": "Distributed communication never frees target memory",
      "z": "Hi, thanks for the clarification. With the latest sources from master, it works for me as well (while 0.3.1 still fails). Seems to be solved, thanks! :+1:",
      "y": "suggest switching to a different backend (e.g. gloo or MPI), because TCP is really nearly a debug-mode thing and will likely not get anywhere close to peak performance."
   },
   {
      "null": 39,
      "x": "KLDivLoss behaves differently on CPU/GPU",
      "z": "Thanks for the report, @yuandong-tian and the confirmation @reynoldscem. I think I found the bug. Fix incoming.",
      "y": "Bug Fixed"
   },
   {
      "null": 40,
      "x": "nccl2 backend distributed package",
      "z": "You have to get a nccl-dev package e.g. from here http://developer.download.nvidia.com/compute/machine-learning/repos/ubuntu1604/x86_64",
      "y": "get a nccl-dev package e.g. from here http://developer.download.nvidia.com/compute/machine-learning/repos/ubuntu1604/x86_64"
   },
   {
      "null": 41,
      "x": "torch.poisson does not work with FloatTensor on master",
      "z": "I think I found the bug. Fix incoming soon.",
      "y": "Bug Fixed"
   },
   {
      "null": 42,
      "x": "[feature request] same behavior in multi-gpu DataParallel vs single GPU",
      "z": "Your criterion call is outside the data parallel. The size_average divides by the *full* batch size. There's still no dependence on the number of GPUs:\n \n \n \n ```python\n \n import torch\n \n import torch.nn as nn\n \n from torch.autograd import Variable\n \n input = Variable(torch.randn(8, 10).cuda())\n \n target = Variable(torch.ones(8).long().cuda())\n \n criterion = nn.CrossEntropyLoss().cuda()\n \n \n \n linear = nn.Linear(10, 10).cuda()\n \n dp = nn.DataParallel(linear, [0, 1])\n \n \n \n criterion(linear(input), target).backward()\n \n print(linear.weight.grad)\n \n \n \n dp.zero_grad()\n \n criterion(dp(input), target).backward()\n \n print(linear.weight.grad)\n \n ```",
      "y": "Criiterion call is outside the data parallel. The size_average divides by the *full* batch size. There's still no dependence on the number of GPUs:\n \n ```python\n import torch\n import torch.nn as nn\n from torch.autograd import Variable\n input = Variable(torch.randn(8, 10).cuda())\n target = Variable(torch.ones(8).long().cuda())\n criterion = nn.CrossEntropyLoss().cuda()\n \n linear = nn.Linear(10, 10).cuda()\n dp = nn.DataParallel(linear, [0, 1])\n \n criterion(linear(input), target).backward()\n print(linear.weight.grad)\n \n dp.zero_grad()\n criterion(dp(input), target).backward()\n print(linear.weight.grad)\n ```"
   },
   {
      "null": 43,
      "x": "Assigning to an index of a sparse tensor is a no-op",
      "z": "FWIW, scipy does not allow assigning to a COO sparse matrix, and will complain when assigning to a CSR one:\n \n ```\n \n >>> m = scipy.sparse.rand(3, 4, density=0.25)\n \n >>> m\n \n <3x4 sparse matrix of type '<class 'numpy.float64'>'\n \n with 3 stored elements in COOrdinate format>\n \n >>> m[0,0] = 100\n \n Traceback (most recent call last):\n \n  File \"<stdin>\", line 1, in <module>\n \n TypeError: 'coo_matrix' object does not support item assignment\n \n \n \n >>> m = scipy.sparse.rand(3, 4, density=0.25, format='csr')\n \n >>> m[0,0] = 100\n \n .../python3.8/site-packages/scipy/sparse/_index.py:82: \\\n \n  SparseEfficiencyWarning: Changing the sparsity structure of a csr_matrix is expensive. \\\n \n  lil_matrix is more efficient.\n \n  self._set_intXint(row, col, x.flat[0])\n \n \n \n >>> m.todense()\n \n matrix([[100. , 0. , 0. , 0.2128684 ],\n \n  [ 0. , 0. , 0. , 0.93232551],\n \n  [ 0. , 0.98662986, 0. , 0. ]])\n \n ```",
      "y": "scipy does not allow assigning to a COO sparse matrix, and will complain when assigning to a CSR one:\n ```\n >>> m = scipy.sparse.rand(3, 4, density=0.25)\n >>> m\n <3x4 sparse matrix of type '<class 'numpy.float64'>'\n with 3 stored elements in COOrdinate format>\n >>> m[0,0] = 100\n Traceback (most recent call last):\n  File \"<stdin>\", line 1, in <module>\n TypeError: 'coo_matrix' object does not support item assignment\n \n >>> m = scipy.sparse.rand(3, 4, density=0.25, format='csr')\n >>> m[0,0] = 100\n .../python3.8/site-packages/scipy/sparse/_index.py:82: \\\n  SparseEfficiencyWarning: Changing the sparsity structure of a csr_matrix is expensive. \\\n  lil_matrix is more efficient.\n  self._set_intXint(row, col, x.flat[0])\n \n >>> m.todense()\n matrix([[100. , 0. , 0. , 0.2128684 ],\n  [ 0. , 0. , 0. , 0.93232551],\n  [ 0. , 0.98662986, 0. , 0. ]])\n ```"
   },
   {
      "null": 44,
      "x": "@torch.jit.script causes compilation error on Ampere architecture (RTX 3090)",
      "z": "CUDA 11.0 supports sm_80 but nvrtc shipped with it does not... I can try to clamp it on the right side to compute_75. :crossed_fingers:",
      "y": "A look at the documentation of nvrtc included with CUDA Toolkit 11.0 reveals that it indeed does not support the Ampere architecture (compute_80). This support is introduced in CUDA Toolkit 11.1.\n \n Unfortunately, the conda repositories do not contain builds of PyTorch against CUDA 11.1."
   },
   {
      "null": 45,
      "x": "Errors in Eigen library when building pytorch mobile for Android",
      "z": "I finally figured it out. The problem was in the Clang C++ version delivered with the Android NDK. I rolled back from 22.0.6917172 to 21.0.6113669 and now it works.",
      "y": "The problem is in the Clang C++ version delivered with the Android NDK. I rolled back from 22.0.6917172 to 21.0.6113669 and now it works."
   },
   {
      "null": 46,
      "x": "nn.utils.spectral_norm() does not perform normalization of the weight matrix.",
      "z": "It does power iteration at forward, which updates the parameters. If you compute again after forward, the matrices match:\n \n ```py\n \n In [1]: import torch\n \n  ...: print(torch.__version__)\n \n  ...:\n \n  ...: linear = torch.nn.Linear(3, 4)\n \n  ...: norm_layer = torch.nn.utils.spectral_norm(linear)\n \n  ...:\n \n  ...: print('Normalized weight matrix with spectral_norm(): ', norm_layer.weight)\n \n  ...: print('Original weight matrix: ', norm_layer.weight_orig)\n \n  ...:\n \n  ...: sigma = torch.dot(norm_layer.weight_u, torch.mv(norm_layer.weight_orig, norm_layer.weight_v))\n \n  ...: print('Normalized weight matrix by hands: ', norm_layer.weight_orig / sigma)\n \n 1.7.0a0+33e2665\n \n Normalized weight matrix with spectral_norm(): tensor([[ 0.4658, 0.3948, -0.1485],\n \n  [-0.3340, 0.4385, -0.2675],\n \n  [-0.2579, 0.4863, -0.2420],\n \n  [ 0.3940, 0.4005, -0.3857]])\n \n Original weight matrix: Parameter containing:\n \n tensor([[ 0.4658, 0.3948, -0.1485],\n \n  [-0.3340, 0.4385, -0.2675],\n \n  [-0.2579, 0.4863, -0.2420],\n \n  [ 0.3940, 0.4005, -0.3857]], requires_grad=True)\n \n Normalized weight matrix by hands: tensor([[-2.9618, -2.5104, 0.9443],\n \n  [ 2.1241, -2.7887, 1.7012],\n \n  [ 1.6401, -3.0922, 1.5386],\n \n  [-2.5056, -2.5471, 2.4527]], grad_fn=<DivBackward0>)\n \n \n \n In [2]: norm_layer(torch.randn((3)))\n \n Out[2]: tensor([-0.5175, 0.6595, 1.0445, 0.0289], grad_fn=<AddBackward0>)\n \n \n \n In [3]: print('Normalized weight matrix with spectral_norm(): ', norm_layer.weight)\n \n  ...: print('Original weight matrix: ', norm_layer.weight_orig)\n \n Normalized weight matrix with spectral_norm(): tensor([[ 0.4720, 0.4001, -0.1505],\n \n  [-0.3385, 0.4444, -0.2711],\n \n  [-0.2614, 0.4928, -0.2452],\n \n  [ 0.3993, 0.4059, -0.3909]], grad_fn=<DivBackward0>)\n \n Original weight matrix: Parameter containing:\n \n tensor([[ 0.4658, 0.3948, -0.1485],\n \n  [-0.3340, 0.4385, -0.2675],\n \n  [-0.2579, 0.4863, -0.2420],\n \n  [ 0.3940, 0.4005, -0.3857]], requires_grad=True)\n \n \n \n In [4]: sigma = torch.dot(norm_layer.weight_u, torch.mv(norm_layer.weight_orig, norm_layer.weight_v))\n \n  ...: print('Normalized weight matrix by hands: ', norm_layer.weight_orig / sigma)\n \n Normalized weight matrix by hands: tensor([[ 0.4720, 0.4001, -0.1505],\n \n  [-0.3385, 0.4444, -0.2711],\n \n  [-0.2614, 0.4928, -0.2452],\n \n  [ 0.3993, 0.4059, -0.3909]], grad_fn=<DivBackward0>)\n \n \n \n ```",
      "y": "It does power iteration at forward, which updates the parameters. If you compute again after forward, the matrices match:\n \n ```py\n \n In [1]: import torch\n \n  ...: print(torch.__version__)\n \n  ...:\n \n  ...: linear = torch.nn.Linear(3, 4)\n \n  ...: norm_layer = torch.nn.utils.spectral_norm(linear)\n \n  ...:\n \n  ...: print('Normalized weight matrix with spectral_norm(): ', norm_layer.weight)\n \n  ...: print('Original weight matrix: ', norm_layer.weight_orig)\n \n  ...:\n \n  ...: sigma = torch.dot(norm_layer.weight_u, torch.mv(norm_layer.weight_orig, norm_layer.weight_v))\n \n  ...: print('Normalized weight matrix by hands: ', norm_layer.weight_orig / sigma)\n \n 1.7.0a0+33e2665\n \n Normalized weight matrix with spectral_norm(): tensor([[ 0.4658, 0.3948, -0.1485],\n \n  [-0.3340, 0.4385, -0.2675],\n \n  [-0.2579, 0.4863, -0.2420],\n \n  [ 0.3940, 0.4005, -0.3857]])\n \n Original weight matrix: Parameter containing:\n \n tensor([[ 0.4658, 0.3948, -0.1485],\n \n  [-0.3340, 0.4385, -0.2675],\n \n  [-0.2579, 0.4863, -0.2420],\n \n  [ 0.3940, 0.4005, -0.3857]], requires_grad=True)\n \n Normalized weight matrix by hands: tensor([[-2.9618, -2.5104, 0.9443],\n \n  [ 2.1241, -2.7887, 1.7012],\n \n  [ 1.6401, -3.0922, 1.5386],\n \n  [-2.5056, -2.5471, 2.4527]], grad_fn=<DivBackward0>)\n \n \n \n In [2]: norm_layer(torch.randn((3)))\n \n Out[2]: tensor([-0.5175, 0.6595, 1.0445, 0.0289], grad_fn=<AddBackward0>)\n \n \n \n In [3]: print('Normalized weight matrix with spectral_norm(): ', norm_layer.weight)\n \n  ...: print('Original weight matrix: ', norm_layer.weight_orig)\n \n Normalized weight matrix with spectral_norm(): tensor([[ 0.4720, 0.4001, -0.1505],\n \n  [-0.3385, 0.4444, -0.2711],\n \n  [-0.2614, 0.4928, -0.2452],\n \n  [ 0.3993, 0.4059, -0.3909]], grad_fn=<DivBackward0>)\n \n Original weight matrix: Parameter containing:\n \n tensor([[ 0.4658, 0.3948, -0.1485],\n \n  [-0.3340, 0.4385, -0.2675],\n \n  [-0.2579, 0.4863, -0.2420],\n \n  [ 0.3940, 0.4005, -0.3857]], requires_grad=True)\n \n \n \n In [4]: sigma = torch.dot(norm_layer.weight_u, torch.mv(norm_layer.weight_orig, norm_layer.weight_v))\n \n  ...: print('Normalized weight matrix by hands: ', norm_layer.weight_orig / sigma)\n \n Normalized weight matrix by hands: tensor([[ 0.4720, 0.4001, -0.1505],\n \n  [-0.3385, 0.4444, -0.2711],\n \n  [-0.2614, 0.4928, -0.2452],\n \n  [ 0.3993, 0.4059, -0.3909]], grad_fn=<DivBackward0>)\n \n \n \n ```"
   },
   {
      "null": 47,
      "x": "Enable PyTorch compilation on Apple Silicon",
      "z": "Hey guys, I successfully built a pytorch1.8.0a0 on my macbook air with m1 chip~~\n \n \n \n The python version I'm using is python 3.9.1 which is installed by [conda-forge](https://github.com/conda-forge/miniforge).\n \n \n \n ### 1. Building Guide:\n \n \n \n 1. fix deps:\n \n \n \n `conda install setuptools cffi typing_extensions future six requests dataclasses pkg-config libuv`\n \n I didn't install cmake because the system itself is shipped already with a cmake. You can also `conda install cmake`.\n \n \n \n \n \n 2. clone pytorch repo and build:\n \n \n \n First:\n \n  ```shell\n \n  git clone --recursive https://github.com/pytorch/pytorch\n \n  cd pytorch\n \n  ```\n \n  Then I modified two lines of the CMakeLists.txt in the pytorch git directory:\n \n  (1) `option(USE_OPENMP \"Use OpenMP for parallel code\" ON)` to `option(USE_OPENMP \"Use OpenMP for parallel code\" OFF)`\n \n  (2) `USE_MKLDNN \"Use MKLDNN. Only available on x86 and x86_64.\" ON` to `USE_MKLDNN \"Use MKLDNN. Only available on x86 and x86_64.\" OFF` \n \n \n \n Then:\n \n ```shell\n \n export CMAKE_PREFIX_PATH=${CONDA_PREFIX:-\"$(dirname $(which conda))/../\"}\n \n MACOSX_DEPLOYMENT_TARGET=11.1 CC=clang CXX=clang++ python setup.py install\n \n  ```\n \n make sure here `python` is pointed to the version installed by conda-forge. \n \n \n \n ### 2. Install from whl file\n \n \n \n I tried to build a wheel file, the download link is [torch-1.8.0a0-cp39-cp39-macosx_11_0_arm64](https://github.com/wizyoung/AppleSiliconSelfBuilds/blob/main/builds/torch-1.8.0a0-cp39-cp39-macosx_11_0_arm64.whl), you may try to install directly from it!\n \n \n \n ### 3. Speed test\n \n \n \n From the comment of https://github.com/pytorch/pytorch/issues/48145#issuecomment-730297957 we can do a simple benchmark:\n \n \n \n > The following code shows roughly 46it/s running on a MacBook Air with M1 chip, I'm literally impressed by the performance of M1:\n \n > \n \n > ```\n \n > from tqdm import tqdm\n \n > import torch\n \n > \n \n > @torch.jit.script\n \n > def foo():\n \n > x = torch.ones((1024 * 12, 1024 * 12), dtype=torch.float32)\n \n > y = torch.ones((1024 * 12, 1024 * 12), dtype=torch.float32)\n \n > z = x + y\n \n > return z\n \n > \n \n > \n \n > if __name__ == '__main__':\n \n > z0 = None\n \n > for _ in tqdm(range(10000000000)):\n \n > zz = foo()\n \n > if z0 is None:\n \n > z0 = zz\n \n > else:\n \n > z0 += zz\n \n > ```\n \n > \n \n > The Nvidia 3090 with the above code shows 670it/s. So 6.8% of 3090 running just on CPU, not bad!\n \n > Also the 3900x cpu shows just 21it/s.\n \n > \n \n > Hope gets GPU and neural engine support soon.\n \n \n \n On my macbook air, the speed is around 44~45 it/s. \n \n \n \n Update: The speed on my 2018 late Mac mini speed is 20 ~ 21 it/s.\n \n Update2: Speed on TITAN XP GPU: ~320 it/s",
      "y": "I successfully built a pytorch1.8.0a0 on my macbook air with m1 chip~~\n \n The python version I'm using is python 3.9.1 which is installed by [conda-forge](https://github.com/conda-forge/miniforge).\n \n ### 1. Building Guide:\n \n 1. fix deps:\n \n `conda install setuptools cffi typing_extensions future six requests dataclasses pkg-config libuv`\n I didn't install cmake because the system itself is shipped already with a cmake. You can also `conda install cmake`.\n \n \n 2. clone pytorch repo and build:\n \n First:\n  ```shell\n  git clone --recursive https://github.com/pytorch/pytorch\n  cd pytorch\n  ```\n  Then I modified two lines of the CMakeLists.txt in the pytorch git directory:\n  (1) `option(USE_OPENMP \"Use OpenMP for parallel code\" ON)` to `option(USE_OPENMP \"Use OpenMP for parallel code\" OFF)`\n  (2) `USE_MKLDNN \"Use MKLDNN. Only available on x86 and x86_64.\" ON` to `USE_MKLDNN \"Use MKLDNN. Only available on x86 and x86_64.\" OFF` \n \n Then:\n ```shell\n export CMAKE_PREFIX_PATH=${CONDA_PREFIX:-\"$(dirname $(which conda))/../\"}\n MACOSX_DEPLOYMENT_TARGET=11.1 CC=clang CXX=clang++ python setup.py install\n  ```\n make sure here `python` is pointed to the version installed by conda-forge. \n \n ### 2. Install from whl file\n \n I tried to build a wheel file, the download link is [torch-1.8.0a0-cp39-cp39-macosx_11_0_arm64](https://github.com/wizyoung/AppleSiliconSelfBuilds/blob/main/builds/torch-1.8.0a0-cp39-cp39-macosx_11_0_arm64.whl), you may try to install directly from it!\n \n ### 3. Speed test\n \n From the comment of https://github.com/pytorch/pytorch/issues/48145#issuecomment-730297957 we can do a simple benchmark:\n \n > The following code shows roughly 46it/s running on a MacBook Air with M1 chip, I'm literally impressed by the performance of M1:\n > \n > ```\n > from tqdm import tqdm\n > import torch\n > \n > @torch.jit.script\n > def foo():\n > x = torch.ones((1024 * 12, 1024 * 12), dtype=torch.float32)\n > y = torch.ones((1024 * 12, 1024 * 12), dtype=torch.float32)\n > z = x + y\n > return z\n > \n > \n > if __name__ == '__main__':\n > z0 = None\n > for _ in tqdm(range(10000000000)):\n > zz = foo()\n > if z0 is None:\n > z0 = zz\n > else:\n > z0 += zz\n > ```\n > \n > The Nvidia 3090 with the above code shows 670it/s. So 6.8% of 3090 running just on CPU, not bad!\n > Also the 3900x cpu shows just 21it/s.\n > \n > Hope gets GPU and neural engine support soon.\n \n On my macbook air, the speed is around 44~45 it/s. \n \n Update: The speed on my 2018 late Mac mini speed is 20 ~ 21 it/s.\n Update2: Speed on TITAN XP GPU: ~320 it/s"
   },
   {
      "null": 48,
      "x": "The `normalize` function failed in the TorchScript interpreter with AMP enabled.",
      "z": "Based on https://github.com/pytorch/pytorch/issues/38958#issuecomment-635472379:\n \n \"autocast interaction with jit scripting is very much WIP\".\n \n \n \n > Best recommendation right now is, don't run a scripted model under autocast. If you must use autocast and jit together, try tracing instead. Run the model under autocast at trace time. The tracing will include the eager casting decisions autocast makes, and casts will be baked into the resulting jitted module.\n \n \n \n Given your code snippet, running the method in `autocast` and tracing seems to work in the latest nightly:\n \n ```python\n \n def my_norm(x: Tensor):\n \n  with autocast():\n \n  return F.normalize(x, p=2.)\n \n \n \n device = torch.device('cuda:0')\n \n scripted_norm = torch.jit.trace(my_norm, torch.rand((32, 32), device=device))\n \n scripted_norm(torch.rand((32, 32), device=device))\n \n ```\n \n Note that `F.normalize` is an op that can autocast to `float32` as described [here](https://pytorch.org/docs/stable/amp.html#op-eligibility).",
      "y": "Based on https://github.com/pytorch/pytorch/issues/38958#issuecomment-635472379:\n \n \"autocast interaction with jit scripting is very much WIP\".\n \n \n \n > Best recommendation right now is, don't run a scripted model under autocast. If you must use autocast and jit together, try tracing instead. Run the model under autocast at trace time. The tracing will include the eager casting decisions autocast makes, and casts will be baked into the resulting jitted module.\n \n \n \n Given your code snippet, running the method in `autocast` and tracing seems to work in the latest nightly:\n \n ```python\n \n def my_norm(x: Tensor):\n \n  with autocast():\n \n  return F.normalize(x, p=2.)\n \n \n \n device = torch.device('cuda:0')\n \n scripted_norm = torch.jit.trace(my_norm, torch.rand((32, 32), device=device))\n \n scripted_norm(torch.rand((32, 32), device=device))\n \n ```\n \n Note that `F.normalize` is an op that can autocast to `float32` as described [here](https://pytorch.org/docs/stable/amp.html#op-eligibility)."
   },
   {
      "null": 49,
      "x": "[feature request] consistent default values for leaky_relu",
      "z": "`1e-2` is 0.01. So the default values do match.",
      "y": "1e-2 is 0.01"
   },
   {
      "null": 50,
      "x": "Segmentation fault while training",
      "z": "FYI I've had the same issue via an older horovod Dockerfile setup with the following docker config:\n \n ```\n \n FROM nvidia/cuda:9.0-devel-ubuntu16.04\n \n ENV PYTORCH_VERSION=0.4.1\n \n ENV CUDNN_VERSION=7.3.1.20-1+cuda9.0\n \n ENV NCCL_VERSION=2.3.5-2+cuda9.0\n \n ENV HOROVOD_VERSION=0.15.2\n \n ```\n \n with `openmpi-3.1.2`. \n \n \n \n Moving to \n \n ```\n \n ENV PYTORCH_VERSION=1.0.0\n \n ENV CUDNN_VERSION=7.4.1.5-1+cuda9.0\n \n ```\n \n seems to have fixed this issue.",
      "y": "FYI I've had the same issue via an older horovod Dockerfile setup with the following docker config:\n \n ```\n \n FROM nvidia/cuda:9.0-devel-ubuntu16.04\n \n ENV PYTORCH_VERSION=0.4.1\n \n ENV CUDNN_VERSION=7.3.1.20-1+cuda9.0\n \n ENV NCCL_VERSION=2.3.5-2+cuda9.0\n \n ENV HOROVOD_VERSION=0.15.2\n \n ```\n \n with `openmpi-3.1.2`. \n \n \n \n Moving to \n \n ```\n \n ENV PYTORCH_VERSION=1.0.0\n \n ENV CUDNN_VERSION=7.4.1.5-1+cuda9.0\n \n ```\n \n seems to have fixed this issue."
   },
   {
      "null": 51,
      "x": "Windows builds with CUDA 9.2",
      "z": "Update: CUDA 9.2 is added into our Windows AMI. We might want to consider adding a CUDA 9.2 Windows CI, or changing current CI to CUDA 9.2.",
      "y": "CUDA 9.2 is added into our Windows AMI"
   },
   {
      "null": 52,
      "x": "[Feature Request] Inverse Hyperbolic Functions",
      "z": "A more numerically stable atanh one-liner is `torch.log1p(2*x/(1-x)) / 2` \n \n \n \n From https://www.plunk.org/~hatch/rightway.php",
      "y": "A more numerically stable atanh one-liner is `torch.log1p(2*x/(1-x)) / 2` \n \n \n \n From https://www.plunk.org/~hatch/rightway.php"
   },
   {
      "null": 53,
      "x": "cuda runtime error(59): device-side assert when running torch.topk",
      "z": "I kind of had the same problem. I was trying to modify the transfer learning tutorial (https://pytorch.org/tutorials/beginner/transfer_learning_tutorial.html) with the hymenoptera dataset (bees and ants dataset) to make it fit to my project. The problem was that i had 3 classes and the tutorial has 2. So i found that you have to define the number of outputs from the fully connected network (the output layer). So the only thing i had to do was changing \"model_ft.fc = nn.Linear(num_ftrs, 2)\" to \"model_ft.fc = nn.Linear(num_ftrs, 3)\". Hope it helps somebody :)",
      "y": "define the number of outputs from the fully connected network (the output layer). So the only thing i had to do was changing \"model_ft.fc = nn.Linear(num_ftrs, 2)\" to \"model_ft.fc = nn.Linear(num_ftrs, 3)"
   },
   {
      "null": 54,
      "x": "Disabling MPI fails, unable to build on Centos7",
      "z": "USE_MPI is not well defined for `python setup.py install`.\n \n \n \n Here's what you need:\n \n \n \n `USE_DISTRIBUTED=0 python setup.py install`",
      "y": "USE_MPI is not well defined for `python setup.py install`.\n \n \n \n Here's what you need:\n \n \n \n `USE_DISTRIBUTED=0 python setup.py install`"
   },
   {
      "null": 55,
      "x": "[feature request] Matrix rank",
      "z": "@vishwakftw I think so :). We have all ingredients for `np.linalg_matrix_rank`. The `hermitian=True` case can use `symeig`.",
      "y": "We have all ingredients for `np.linalg_matrix_rank`. The `hermitian=True` case can use `symeig`."
   },
   {
      "null": 56,
      "x": "Initialization can be surprisingly important, and under-rated",
      "z": "Just want to point out that the initialization schemes are documented by @vishwakftw at https://github.com/pytorch/pytorch/pull/9038 . Thanks @vishwakftw !",
      "y": "See https://github.com/pytorch/pytorch/pull/9038"
   },
   {
      "null": 57,
      "x": "libtorch.so.1: undefined symbol:",
      "z": "export LD_LIBRARY_PATH=/usr/local/cuda-9.0/lib64\n This helps me.",
      "y": "export LD_LIBRARY_PATH=/usr/local/cuda-9.0/lib64"
   },
   {
      "null": 58,
      "x": "a strange torch.no_grad behaviour when used with lazy_property from distributions",
      "z": ":) I think we fixed this in https://github.com/pytorch/pytorch/pull/7708, which is part of 0.4.1. Can you try that?",
      "y": "Fixed in https://github.com/pytorch/pytorch/pull/7708, which is part of 0.4.1."
   },
   {
      "null": 59,
      "x": "[feature request] Treat tensor as tuple of tensors in torch.cat",
      "z": "I think the request of making `torch.cat` support a single tensor as input makes sense.\n \n We already consider a tensor as an iterable (we can do `for batch in torch.rand(3, 10)` and we will have tensors of size 10).\n I believe `torch.cat` should iterate over any sequence of tensors and concatenate them over the specified dimension.\n \n I'd not vote for adding `cat` as a method though. Most of the time we use it with tuples / lists, so this pattern wouldn't make sense in this case.",
      "y": "We already consider a tensor as an iterable (we can do `for batch in torch.rand(3, 10)` and we will have tensors of size 10).\n I believe `torch.cat` should iterate over any sequence of tensors and concatenate them over the specified dimension."
   },
   {
      "null": 60,
      "x": "Differences between .data and .detach",
      "z": "Here's an example. If you use `detach()` instead of `.data`, gradient computation is guaranteed to be correct..\n \n \n \n ```\n \n >>> a = torch.tensor([1,2,3.], requires_grad = True)\n \n >>> out = a.sigmoid()\n \n >>> c = out.detach()\n \n >>> c.zero_() \n \n tensor([ 0., 0., 0.])\n \n \n \n >>> out # modified by c.zero_() !!\n \n tensor([ 0., 0., 0.])\n \n \n \n >>> out.sum().backward() # Requires the original value of out, but that was overwritten by c.zero_()\n \n RuntimeError: one of the variables needed for gradient computation has been modified by an inplace operation\n \n ```\n \n \n \n As opposed to using `.data`:\n \n ```\n \n >>> a = torch.tensor([1,2,3.], requires_grad = True)\n \n >>> out = a.sigmoid()\n \n >>> c = out.data\n \n >>> c.zero_()\n \n tensor([ 0., 0., 0.])\n \n \n \n >>> out # out was modified by c.zero_()\n \n tensor([ 0., 0., 0.])\n \n \n \n >>> out.sum().backward()\n \n >>> a.grad # The result is very, very wrong because `out` changed!\n \n tensor([ 0., 0., 0.])\n \n ```\n \n \n \n I'll leave this issue open: we should add an example to the migration guide and clarify that section.",
      "y": "Here's an example. If you use `detach()` instead of `.data`, gradient computation is guaranteed to be correct..\n \n \n \n ```\n \n >>> a = torch.tensor([1,2,3.], requires_grad = True)\n \n >>> out = a.sigmoid()\n \n >>> c = out.detach()\n \n >>> c.zero_() \n \n tensor([ 0., 0., 0.])\n \n \n \n >>> out # modified by c.zero_() !!\n \n tensor([ 0., 0., 0.])\n \n \n \n >>> out.sum().backward() # Requires the original value of out, but that was overwritten by c.zero_()\n \n RuntimeError: one of the variables needed for gradient computation has been modified by an inplace operation\n \n ```\n \n \n \n As opposed to using `.data`:\n \n ```\n \n >>> a = torch.tensor([1,2,3.], requires_grad = True)\n \n >>> out = a.sigmoid()\n \n >>> c = out.data\n \n >>> c.zero_()\n \n tensor([ 0., 0., 0.])\n \n \n \n >>> out # out was modified by c.zero_()\n \n tensor([ 0., 0., 0.])\n \n \n \n >>> out.sum().backward()\n \n >>> a.grad # The result is very, very wrong because `out` changed!\n \n tensor([ 0., 0., 0.])\n \n ```\n \n \n \n I'll leave this issue open: we should add an example to the migration guide and clarify that section."
   },
   {
      "null": 61,
      "x": "Segmentation fault with cpp_extensions example",
      "z": "It seems that you are using GCC 4.8\n \n Didn't you get the following message from `cpp_extensions`?\n \n ```\n \n Your compiler (g++ 4.8) may be ABI-incompatible with PyTorch!\n \n Please use a compiler that is ABI-compatible with GCC 4.9 and above.\n \n See https://gcc.gnu.org/onlinedocs/libstdc++/manual/abi.html.\n \n \n \n See https://gist.github.com/goldsborough/d466f43e8ffc948ff92de7486c5216d6\n \n for instructions on how to install GCC 4.9 or higher.\n \n ```\n \n \n \n Could you try updating gcc to 4.9 or higher?",
      "y": "It seems that you are using GCC 4.8\n you would have get the following message from `cpp_extensions`?\n ```\n Your compiler (g++ 4.8) may be ABI-incompatible with PyTorch!\n Please use a compiler that is ABI-compatible with GCC 4.9 and above.\n See https://gcc.gnu.org/onlinedocs/libstdc++/manual/abi.html.\n \n See https://gist.github.com/goldsborough/d466f43e8ffc948ff92de7486c5216d6\n for instructions on how to install GCC 4.9 or higher.\n ```\n try updating gcc to 4.9 or higher"
   },
   {
      "null": 62,
      "x": "RecursionError when using torch.utils.checkpoint",
      "z": "I encountered the exact same issue when using massive amount of checkpoint-ing, and to workaround it I increased the max recursion depth like this \n `sys.setrecursionlimit(3000)`\n notice that you can see the current recursion limit like this:\n `sys.getrecursionlimit()`\n for me it was 1000\n \n note: I don't know why there's a deep recursion in this scenario, as the checkpoint calls are not nested, I assume that it's related to the autograd mechanism that travels on the graph or something of this nature.",
      "y": "increased the max recursion depth like this `sys.setrecursionlimit(3000)`"
   },
   {
      "null": 63,
      "x": "It seems can't compile pytorch 0.4.0 from source on macOS.",
      "z": "What is your cmake version? I can compile on a mac and my version is 3.9.4. You can see it via `cmake --version`",
      "y": "Use cmake version 3.9.4. You can see it via `cmake --version`"
   },
   {
      "null": 64,
      "x": "pip install torch error",
      "z": "`pip install torch` is not supported on Windows. Please try the commands on http://pytorch.org.",
      "y": "`pip install torch` is not supported on Windows. Please try the commands on http://pytorch.org."
   },
   {
      "null": 65,
      "x": "torch.std() returns nan for single item tensors.",
      "z": "This is more about numpy parity rather than correctness. Hence I removed the hipri flag.",
      "y": "np default is ddof=0, but pytorch has default unbiased=True."
   },
   {
      "null": 66,
      "x": "Indexing with dtype torch.uint8",
      "z": "Is it enough to only modify `dtype` in this function?\n \n ```\n \n non_final_mask = torch.tensor(tuple(map(lambda s: s is not None,\n \n  batch.next_state)), device=device, dtype=torch.bool)\n \n ```",
      "y": "modify `dtype` in this function"
   },
   {
      "null": 67,
      "x": "windows release of pytorch 1.3 is missing several required cuda libraries",
      "z": "Well, the suffix of those DLLs is changed to `_10.dll` instead of `_101.dll`.",
      "y": "The suffix of DLLs is changed to `_10.dll` instead of `_101.dll`."
   },
   {
      "null": 68,
      "x": "[jit] `torch.isfinite` is broken",
      "z": "> Hmm actually, looks like we're not correctly resolving `torch.isfinite` to torch.functional.isfinite.\n \n > \n \n > A workaround is to call `torch.functional.isfinite(x)`\n \n \n \n I've tried, it won't work. F.isfinite(x) is returning 'bool' while scripting and Tensor in Python\n \n \n \n ```\n \n Variable 'bool_tensor' is annotated with type Tensor but is being assigned to a value of type bool:\n \n at box_regression.py:84:8\n \n  \"\"\"\n \n  deltas (Tensor): some tensor\n \n  \"\"\"\n \n  bool_tensor : torch.Tensor = torch.functional.isfinite(deltas)\n \n  ~~~~~~~~~~~ <--- HERE\n \n ```",
      "y": "we're not correctly resolving `torch.isfinite` to torch.functional.isfinite.\n  A workaround is to call `torch.functional.isfinite(x)`"
   },
   {
      "null": 69,
      "x": "RuntimeError: Didn't find engine for operation quantized::conv_prepack NoQEngine",
      "z": "cc @supriyar @dskhudia @jamesr66a \n \n \n \n There are two libraries we're currently using for quantized operations: fbgemm and qnnpack. Fbgemm today doesn't support window build: https://github.com/pytorch/FBGEMM/issues/150 . Would be nice if someone gave it a try.\n \n \n \n For qnnpack - we disable it by default on non-mobile builds:\n \n https://github.com/pytorch/pytorch/blob/0ae0c9788edf4ee31c3c48b906bb9a65b871bd03/aten/src/ATen/Context.cpp#L119-L121\n \n \n \n Afaik, the performance is not that great for qnnpack on x86 cpus and there were some other stability issues. @supriyar - do you know if it's better to reenable it?\n \n \n \n As a temp workaround - try setting `torch.backends.quantized.engine = 'qnnpack'` to see whether it works. It's not an official workaround - just something to try.",
      "y": "try setting `torch.backends.quantized.engine = 'qnnpack'`"
   },
   {
      "null": 70,
      "x": "Multiprocess data loader crash/hang in v1.3",
      "z": "This was a packaging issue that was fixed via https://github.com/pytorch/builder/commit/2ac74c1d5669ad2c32b873461970d33cc4b5c93c\n \n 1.3.0.post2 or 1.3.1 both have the fix, so @pdwilson12 please upgrade.",
      "y": "This was a packaging issue that was fixed via https://github.com/pytorch/builder/commit/2ac74c1d5669ad2c32b873461970d33cc4b5c93c\n \n 1.3.0.post2 or 1.3.1 both have the fix, please upgrade."
   },
   {
      "null": 71,
      "x": "Using tensor cores",
      "z": "There is not a special API and, unfortunately, the cases where tensor cores are enabled are not very easy to describe. NVIDIA has some performance guides for Tensor Cores that can be useful references: https://developer.download.nvidia.com/video/gputechconf/gtc/2019/presentation/s9926-tensor-core-performance-the-ultimate-guide.pdf\n \n \n \n Essentially operations that internally perform GEMMs can use Tensor Cores on fp16 inputs when those inputs' sizes meet special criteria.",
      "y": "NVIDIA has some performance guides for Tensor Cores that can be useful references: https://developer.download.nvidia.com/video/gputechconf/gtc/2019/presentation/s9926-tensor-core-performance-the-ultimate-guide.pdf"
   },
   {
      "null": 72,
      "x": "\"no module named torch\". But installed pytorch 1.3.0 with conda in Ubuntu 18.04.02 Server Edition",
      "z": "You have two python environments: \n \n 1. Conda with Python 3.7 (Inferred from code snippet 3)\n \n 2. Virtualenv with Python 3.8 (Inferred from code snippet 4)\n \n \n \n As you can see from code snippet 1, torch is successfully installed into the first python environment. But from code snippet 4, it implies that torch is not installed in the second python environment. So the solution is to deactivate the second environment through `deactivate`, or to reactivate the first environment by calling `conda activate`.",
      "y": "Activate or deactive one of the enviornment"
   },
   {
      "null": 73,
      "x": "Torch from sourc and Torchvision from pip",
      "z": "`pip install --user --no-dependencies torchvision`",
      "y": "`pip install --user --no-dependencies torchvision`"
   },
   {
      "null": 74,
      "x": "Cannot compile CPP extensions for benchmarking",
      "z": "`cpuinfo` is a submodule in the pytorch repo, the missing header lives at `third_party/cpuinfo/include/cpuinfo.h`. When you see this error, it's most likely that you're trying to run the benchmarks without having installed pytorch from sources (which will instill that header file), but instead you're trying to use a pytorch install from a conda package or wheel, or you've installed into the wrong environment.",
      "y": "`cpuinfo` is a submodule in the pytorch repo, the missing header lives at `third_party/cpuinfo/include/cpuinfo.h`. When you see this error, it's most likely that you're trying to run the benchmarks without having installed pytorch from sources (which will instill that header file), but instead you're trying to use a pytorch install from a conda package or wheel, or you've installed into the wrong environment."
   },
   {
      "null": 75,
      "x": "\"undefined symbol: PySlice_Unpack\" of pytorch 1.0.0 on Ubuntu 14.04",
      "z": "@vishwakftw Thank you. It works when I use Python 3.6.3.",
      "y": "use Python 3.6.3."
   },
   {
      "null": 76,
      "x": "Can not build pytorch tag v1.0.0 from source",
      "z": "I downgraded cudnn from 7.4.1 to 7.3.0, then it works..",
      "y": "downgrade cudnn from 7.4.1 to 7.3.0"
   },
   {
      "null": 77,
      "x": "Wrong recursive module::load",
      "z": "That makes sense. I'll send a fix.",
      "y": "Fixed"
   },
   {
      "null": 78,
      "x": "torch.jit.trace incorrect for function outputting tuple of size 1",
      "z": "verified #15289 will fix this.",
      "y": "Fixed"
   },
   {
      "null": 79,
      "x": "what is the algorithm theory of torch.nn.AdaptiveMaxPool2d?",
      "z": "I originally wrote it in lua-torch to be able to reproduce Spatial Pyramid Pooling for Object Detection. I'm not sure if there are any particular papers focusing specifically on this layer. The original discussion can be found in https://github.com/torch/nn/issues/141\n \n You can find the current implementation in https://github.com/pytorch/pytorch/blob/master/aten/src/THNN/generic/SpatialAdaptiveMaxPooling.c, the cuda implementation is in the THCUNN folder",
      "y": "You can find the current implementation in https://github.com/pytorch/pytorch/blob/master/aten/src/THNN/generic/SpatialAdaptiveMaxPooling.c, the cuda implementation is in the THCUNN folder"
   },
   {
      "null": 80,
      "x": "RuntimeError: std::exception for conv2d with groups > 1",
      "z": "My thanks to @gauss256 for the suggestion of creating a new user + installing Anaconda + installing PyTorch. That solved it for me.\n \n I then compared the `LD_LIBRARY_PATH` for the two users and found that removing `/usr/local/lib` from the `LD_LIBRARY_PATH` for the first user (for whom this was not working) fixed the issue.\n \n Then I investigated the libraries at `/usr/local/lib` and found `libmkldnn.so.0`.\n \n I added back `/usr/local/lib` to `LD_LIBRARY_PATH` and the error appeared again.\n \n Since I suspected the error was related to mkldnn, I moved this file `libmkldnn.so.0` out of `/usr/local/lib` to another location.\n \n Then I tried running the code again and it worked (with `/usr/local/lib` included in `LD_LIBRARY_PATH` but with the file `libmkldnn.so.0` removed from `/usr/local/lib`)\n \n So this issue (at least for me) was caused by having a system wide mkldnn installation conflicting (and overriding) the conda mkldnn.\n \n I'm adding this comment to aid in debugging in case someone else faces a similar issue.",
      "y": "creating a new user + installing Anaconda + installing PyTorch"
   },
   {
      "null": 81,
      "x": "torch::jit::trace C++ equivalent of torch.jit.trace",
      "z": "And will it possible to convert a torch::script::Module to torch::nn::Module ?",
      "y": "It's not possible to trace scalar values in C++\n \n plan to merge script::Module and nn::Module in a future release"
   },
   {
      "null": 82,
      "x": "libcudart.so not found when compiling with NO_DISTRIBUTED=1",
      "z": "The reason for the issue is that if libcudart.so is not dynamically linked, ctypes cannot see a cudart symbol. I cant figure out why.\n Basically this line is failing:\n \n pytorch/torch/cuda/__init__.py\n \n Line 63 in 517c7c9\n \n  if hasattr(lib, 'cudaGetErrorName'): \n lib = ctypes.cdll.LoadLibrary(None)\n if hasattr(lib, 'cudaGetErrorName'):\n It so accidentally happens that when distributed backend is compiled, the CMake also dynamically links with libcudart.so, and hence this failure is not seen.\n The short-term fix is quite simple, to switch to linking against libcudart.so.\n \n The long-term fix is to figure out what ctypes' problem is",
      "y": "The short-term fix is quite simple, to switch to linking against libcudart.so.\n \n The long-term fix is to figure out what ctypes' problem is"
   },
   {
      "null": 83,
      "x": "When will the pytorch 1.0 stable version release?",
      "z": "friday",
      "y": "7th Dec,2018"
   },
   {
      "null": 84,
      "x": "DataLoader error with multiple workers - \"RuntimeError: unable to open shared memory object XXX in read-write mode\"",
      "z": "See the row `-n` in results of `ulimit -a`. You might want to increase that number. FYI I have 500,000 in my system. \n \n Closing, please feel free to reopen if that doesn't solve your problem.",
      "y": "increasing your open files limit (try ulimit)\n trying another multiprocessing sharing strategy (https://pytorch.org/docs/master/multiprocessing.html#sharing-strategies)"
   },
   {
      "null": 85,
      "x": "The sort order for tensor.unique() defaults to \"unsorted\" but in practice is ascending on gpu and descending on cpu",
      "z": "This is actually not a problem I think.\n \n \n \n By default, `sorted=False`, and the fact that the output seems to be sorted is an implementation detail.\n \n \n \n ```python\n \n a=tensor([1,1,2,3]).cuda()\n \n b=tensor([1,1,2,3]).cpu()\n \n \n \n print(a.unique(sorted=True))\n \n print(b.unique(sorted=True))\n \n ```\n \n returns, as expected\n \n ```\n \n tensor([1, 2, 3])\n \n tensor([1, 2, 3], device='cuda:0')\n \n ```\n \n \n \n See this for example\n \n ```python\n \n a = torch.tensor([1.5,1,2,3])\n \n a.unique()\n \n > tensor([3.0000, 2.0000, 1.0000, 1.5000])\n \n a.cuda().unique()\n \n > tensor([1.0000, 1.5000, 2.0000, 3.0000], device='cuda:0')\n \n ```",
      "y": "Fixed soerted = True"
   },
   {
      "null": 86,
      "x": "issues to run minimal pytorch example",
      "z": "Fixed this issue by downloading the two missing libraries from [Intels MKL Mac releases page](https://github.com/intel/mkl-dnn/releases) and copying them both (`libmklml.dylib`, `libiomp5.dylib`) to `libtorch/lib`.",
      "y": "Fixed this issue by downloading the two missing libraries from [Intels MKL Mac releases page](https://github.com/intel/mkl-dnn/releases) and copying them both (`libmklml.dylib`, `libiomp5.dylib`) to `libtorch/lib`."
   },
   {
      "null": 87,
      "x": "v1.7 requirement \"dataclasses\" is not compatible with python > 3.6",
      "z": "@lemondante \n \n You can patch the wheel to remove the requirement in the metadata.\n \n Or install it in two steps from the latest requirements (or manually) : \n \n ```\n \n pip install -r https://raw.githubusercontent.com/pytorch/pytorch/master/requirements.txt\n \n pip install <torch_wheel> --no-deps\n \n ```",
      "y": "patch the wheel to remove the requirement in the metadata.\n Or install it in two steps from the latest requirements (or manually) : \n ```\n pip install -r https://raw.githubusercontent.com/pytorch/pytorch/master/requirements.txt\n pip install <torch_wheel> --no-deps\n ```"
   },
   {
      "null": 88,
      "x": "[ONNX] error with pytorch \"RuntimeError: Unexpected node type: onnx::Cast\"",
      "z": "@oneway234 - Several updates have been added to ONNX exporter in PyTorch since PyTorch 1.3.1. Could you please try with the latest version of PyTorch (1.7) and see if this issue is resolved.",
      "y": "Use the latest version of PyTorch (1.7)"
   },
   {
      "null": 89,
      "x": "clip_grad_norm_ silently passes when not finite",
      "z": "Adding the argument seems cost free, so might as well do it.",
      "y": "argument error_if_nonfinite=True"
   },
   {
      "null": 90,
      "x": "x.shape return tuple instead of torch.Size",
      "z": "I believe this is the same bug as #47090 but with a different reproducer.\n \n \n \n Edit: Confirmed, the branch that fixed that also fixes this. Being worked on in #47110.",
      "y": "Bug Fixed"
   },
   {
      "null": 91,
      "x": "new_ones() missing 1 required positional arguments: \"size\"",
      "z": "Yes: the fast.ai library uses subclassing to add semantics and/or metadata to Tensors, eg: TensorImage, TensorMask, TensorPoint, TensorBBox, etc. Issue #22402 has some discussion and elaborates further on that.",
      "y": "new_zeros does work:\n \n from torch import Tensor\n \n class MyTensor(Tensor):\n  pass\n \n a = MyTensor([1,2,3])\n a.new_zeros(320).shape"
   },
   {
      "null": 92,
      "x": "batch_norm const running_mean/_var modified in-place",
      "z": "@cpuhrsch yes it is by design. See https://github.com/zdevito/ATen/issues/27",
      "y": "By design"
   },
   {
      "null": 93,
      "x": "torch.distributed.broadcast_object_list doesn't broadcast custom class object",
      "z": "you are printing local dummy() object, not the object broadcast from rank 0. \n \n \n \n Try to revise codes like this: \n \n \n \n  if dist.get_rank() == 0:\n \n  objects = [dummy()] \n \n  else:\n \n  objects = [None]\n \n  torch.distributed.broadcast_object_list(objects, src=0)\n \n \n \n  print(objects)",
      "y": "printing local dummy() object, not the object broadcast from rank 0. \n \n Try to revise codes like this: \n \n  if dist.get_rank() == 0:\n  objects = [dummy()] \n  else:\n  objects = [None]\n  torch.distributed.broadcast_object_list(objects, src=0)\n \n  print(objects)"
   },
   {
      "null": 94,
      "x": "Tensorboard: ValueError: Duplicate plugins for name projector",
      "z": "@Biaocsu, I also met the problem but I fixed it. My method is:\n\n1. I downloaded a test script from https://raw.githubusercontent.com/tensorflow/tensorboard/master/tensorboard/tools/diagnose_tensorboard.py\n2. I run it and it told me that I have two tensorboards with a different version. Also, it told me how to fix it.\n3. I followed its instructions and I can make my tensorboard work.\n\nI think this error means that you have two tensorboards installed so the plugin will be duplicated.  Another method would be helpful that is to reinstall the python environment using conda.\n\nHope to help you.",
      "y": " this error means that you have two tensorboards installed so the plugin will be duplicated.  Another method could be helpful is to reinstall the python environment using conda"
   },
   {
      "null": 95,
      "x": "Data Loader does not work with Hdf5 file, when num_worker >1",
      "z": "**Solution**\n \n \n \n This issue could be solved and the solution is simple: \n \n 1. Do not open hdf5 inside `__init__`\n \n 2. Open the hdf5 at the first data iteration.\n \n \n \n Here is an illustration:\n \n ```python\n \n class LXRTDataLoader(torch.utils.data.Dataset):\n \n  def __init__(self):\n \n  \"\"\"do not open hdf5 here!!\"\"\"\n \n \n \n  def open_hdf5(self):\n \n  self.img_hdf5 = h5py.File('img.hdf5', 'r')\n \n  self.dataset = self.img_hdf5['dataset'] # if you want dataset.\n \n \n \n  def __getitem__(self, item: int):\n \n  if not hasattr(self, 'img_hdf5'):\n \n  self.open_hdf5()\n \n  img0 = self.img_hdf5['dataset'][0] # Do loading here\n \n  img1 = self.dataset[1]\n \n  return img0, img1\n \n ```\n \n Then the dataloader with `num_workers` > 1 could just be normally used.\n \n ```python\n \n train_loader = torch.utils.data.DataLoader(\n \n  dataset=train_tset,\n \n  batch_size=32,\n \n  num_workers=4\n \n  )\n \n ```\n \n \n \n \n \n \n \n **Explanation**\n \n The multi-processing actually happens when you create the data iterator (e.g., when calling `for datum in dataloader:`):\n \n https://github.com/pytorch/pytorch/blob/461014d54b3981c8fa6617f90ff7b7df51ab1e85/torch/utils/data/dataloader.py#L712-L720\n \n In short, it would create multiple processes which \"copy\" the state of the current process. Thus the opened hdf5 file object would be dedicated to each subprocess if we open it at the first data iteration. \n \n \n \n If you somehow create an hdfs file in` __init__` and set up the `num_workers' > 0, it might cause two issues:\n \n 1. The writing behavior is non-determistic. (We do not need to write to hdf5, thus this issue is ignored.)\n \n 2. The state of the hdfs is copied, which might not faithfully indicate the current state. \n \n \n \n In the previous way, we bypass this two issues.",
      "y": "**Solution**\n \n \n \n This issue could be solved and the solution is simple: \n \n 1. Do not open hdf5 inside `__init__`\n \n 2. Open the hdf5 at the first data iteration.\n \n \n \n Here is an illustration:\n \n ```python\n \n class LXRTDataLoader(torch.utils.data.Dataset):\n \n  def __init__(self):\n \n  \"\"\"do not open hdf5 here!!\"\"\"\n \n \n \n  def open_hdf5(self):\n \n  self.img_hdf5 = h5py.File('img.hdf5', 'r')\n \n  self.dataset = self.img_hdf5['dataset'] # if you want dataset.\n \n \n \n  def __getitem__(self, item: int):\n \n  if not hasattr(self, 'img_hdf5'):\n \n  self.open_hdf5()\n \n  img0 = self.img_hdf5['dataset'][0] # Do loading here\n \n  img1 = self.dataset[1]\n \n  return img0, img1\n \n ```\n \n Then the dataloader with `num_workers` > 1 could just be normally used.\n \n ```python\n \n train_loader = torch.utils.data.DataLoader(\n \n  dataset=train_tset,\n \n  batch_size=32,\n \n  num_workers=4\n \n  )\n \n ```\n \n \n \n \n \n \n \n **Explanation**\n \n The multi-processing actually happens when you create the data iterator (e.g., when calling `for datum in dataloader:`):\n \n https://github.com/pytorch/pytorch/blob/461014d54b3981c8fa6617f90ff7b7df51ab1e85/torch/utils/data/dataloader.py#L712-L720\n \n In short, it would create multiple processes which \"copy\" the state of the current process. Thus the opened hdf5 file object would be dedicated to each subprocess if we open it at the first data iteration. \n \n \n \n If you somehow create an hdfs file in` __init__` and set up the `num_workers' > 0, it might cause two issues:\n \n 1. The writing behavior is non-determistic. (We do not need to write to hdf5, thus this issue is ignored.)\n \n 2. The state of the hdfs is copied, which might not faithfully indicate the current state. \n \n \n \n In the previous way, we bypass this two issues."
   },
   {
      "null": 96,
      "x": "DataLoader, when num_worker is great than 1, the data loaded are wrong, or a error a reported!",
      "z": "h5py has a parallel mode that depends on mpi4py. I had the above problem and solved it by doing the following...\n \n (i) pip uninstall h5py\n \n (ii) pip install mpi4py\n \n (iii) pip install h5py.",
      "y": "h5py has a parallel mode that depends on mpi4py. I had the above problem and solved it by doing the following...\n \n (i) pip uninstall h5py\n \n (ii) pip install mpi4py\n \n (iii) pip install h5py."
   },
   {
      "null": 97,
      "x": "DataParallel with Multi-GPU module (eg. 2 model replica on 4 GPU)",
      "z": "The walk-around we found is to `DaraParallel` the head of the model and the tail of the model separately. \n \n And wrap the data transfer in the `forward` function.\n \n Here is a working example. Hopefully, someone finds this useful. \n \n Thanks. This issue can be closed. \n \n ```\n \n \n \n class ConvBlck(nn.Module):\n \n  def __init__(self, in_channel, out_channel, kernel_size, stride=1):\n \n  super(ConvBlck, self).__init__()\n \n  self.blck = nn.Sequential( OrderedDict([\n \n  ('conv', nn.Conv2d( in_channel, out_channel, kernel_size=kernel_size, stride=stride, padding=kernel_size//2)),\n \n  ('relu', nn.ReLU() )]) )\n \n  def forward(self, x):\n \n  return self.blck(x)\n \n \n \n class SixConv(nn.Module):\n \n  def __init__(self):\n \n  super(SixConv, self).__init__()\n \n  tmp = 5\n \n  self.conv1 = nn.Sequential( ConvBlck(1,1<<tmp,5,2), ConvBlck(1<<tmp,1<<(tmp+1),5,2) )\n \n  self.conv2 = nn.Sequential( ConvBlck(1<<(tmp+1),1<<tmp,5,2), nn.AdaptiveAvgPool2d(1) , nn.Conv2d(1<<tmp, 10, 1, 1) )\n \n \n \n  def forward(self, x):\n \n  x = self.conv1(x)\n \n  next_index = x.device.index + 1\n \n  x = x.cuda(next_index)\n \n  x = self.conv2(x)\n \n  return x\n \n model = SixConv()\n \n model.conv1 = nn.DataParallel(model.conv1,device_ids=[0,2]);\n \n model.conv2 = nn.DataParallel(model.conv2,device_ids=[1,3]);\n \n crite = torch.nn.CrossEntropyLoss()\n \n optim = torch.optim.Adam( model.parameters() , lr=0.001)\n \n model.conv1.to( 'cuda:0' )\n \n model.conv2.to( 'cuda:1' )\n \n \n \n ```",
      "y": "The walk-around we found is to `DaraParallel` the head of the model and the tail of the model separately. \n \n And wrap the data transfer in the `forward` function.\n \n Here is a working example. Hopefully, someone finds this useful. \n \n ``` \n class ConvBlck(nn.Module):\n \n  def __init__(self, in_channel, out_channel, kernel_size, stride=1):\n \n  super(ConvBlck, self).__init__()\n \n  self.blck = nn.Sequential( OrderedDict([\n \n  ('conv', nn.Conv2d( in_channel, out_channel, kernel_size=kernel_size, stride=stride, padding=kernel_size//2)),\n \n  ('relu', nn.ReLU() )]) )\n \n  def forward(self, x):\n \n  return self.blck(x)\n \n \n \n class SixConv(nn.Module):\n \n  def __init__(self):\n \n  super(SixConv, self).__init__()\n \n  tmp = 5\n \n  self.conv1 = nn.Sequential( ConvBlck(1,1<<tmp,5,2), ConvBlck(1<<tmp,1<<(tmp+1),5,2) )\n \n  self.conv2 = nn.Sequential( ConvBlck(1<<(tmp+1),1<<tmp,5,2), nn.AdaptiveAvgPool2d(1) , nn.Conv2d(1<<tmp, 10, 1, 1) )\n \n \n \n  def forward(self, x):\n \n  x = self.conv1(x)\n \n  next_index = x.device.index + 1\n \n  x = x.cuda(next_index)\n \n  x = self.conv2(x)\n \n  return x\n \n model = SixConv()\n \n model.conv1 = nn.DataParallel(model.conv1,device_ids=[0,2]);\n \n model.conv2 = nn.DataParallel(model.conv2,device_ids=[1,3]);\n \n crite = torch.nn.CrossEntropyLoss()\n \n optim = torch.optim.Adam( model.parameters() , lr=0.001)\n \n model.conv1.to( 'cuda:0' )\n \n model.conv2.to( 'cuda:1' )\n \n \n \n ```"
   },
   {
      "null": 98,
      "x": "index_put_ has unreasonable checks",
      "z": "You are not passing the `indices` in the right format. `index_put_` expect the index tensor to be the transpose of what you passed.\n \n So this works\n \n \n \n ```python\n \n target = torch.zeros([5,3])\n \n indices = torch.LongTensor([[0,1], [1, 2], [2, 2], [3, 0], [4, 1]])\n \n value = torch.ones(indices.shape[0])\n \n target.index_put_(tuple(indices.t()), value)\n \n ```\n \n \n \n Also, it is generally better to use advanced indexing in such cases, as it's more clear what it is doing\n \n ```python\n \n target[idx1, idx2] = 1\n \n ```\n \n \n \n Closing as not a bug.",
      "y": "pass the `indices` in the right format"
   },
   {
      "null": 0,
      "x": "upgrade MKL-DNN 0.20.1 to DNNL 1.1",
      "z": "In the past MKL GEMMs have been faster than MKL-DNN gemms. The claim from Intel is that with DNNL 1.1 it will be faster ..ie no regressions in GEMMs.",
      "y": "The claim from Intel is that with DNNL 1.1 it will be faster"
   },
   {
      "null": 1,
      "x": "Future callbacks in RPC should capture and restore autograd context id",
      "z": "FYI, worked on a similar issue recently. https://github.com/pytorch/pytorch/pull/36395\nAdding extra argument seems more like a temporary fix rather than a scalable one.\n\nI think we should consider leveraging ThreadLocalState.cpp this time. It also transfers profiler thread-local state. Otherwise, an ad-hoc fix will still leave the profiler broken after RPC thread switch.",
      "y": "ad-hoc fix will still leave the profiler broken after RPC thread switch,  we should consider leveraging ThreadLocalState.cpp"
   },
   {
      "null": 2,
      "x": "[Feature] View a floating point tensor as complex tensor, and vice versa",
      "z": "> Another benefit is, when `view_as_floating` is implemented, then `torch.real` could be implemented as `torch.view_as_floating(z)[..., 0]` which is copy-free.\n\nAgain, a little nuisance about naming, since in theory one can imagine complex integer pairs (quantized or maybe some Zn for cryptography needs), then `view_as_floating` would be confusing",
      "y": "When implementing `view_as_floating` , `torch.real` could be implemented as `torch.view_as_floating(z)[..., 0]` which is copy-free."
   },
   {
      "null": 3,
      "x": "Should torch.real, torch.imag be implemented as a view?",
      "z": "Right now, tensor.imag is disabled for real tensors. This is because if we return a new tensor of zeros, the user would be able to update the tensor returned by tensor.imag which should not be allowed as numpy returns a read-only array, and pytorch doesn't support read-only tensors yet.",
      "y": "tensor.imag is disabled for real tensors"
   },
   {
      "null": 4,
      "x": "Can you add an overloaded operator &,|,^ to a libtorch tensor",
      "z": "Overload bitwise NOT, AND, OR, XOR operators for at::Tensor",
      "y": "We can Overload bitwise NOT, AND, OR, XOR operators"
   },
   {
      "null": 5,
      "x": "torch._C._cuda_getDriverVersion() reporting CUDA version instead of NVIDIA driver version",
      "z": "it doesn't return the CUDA version. It returns \"Returns the latest version of CUDA supported by the driver.\".\nIf you have CUDA10 installed, but driver that only supports CUDA9, it will return 9000",
      "y": "Returns the latest version of CUDA supported by the driver"
   },
   {
      "null": 6,
      "x": "gradcheck does not work for self-overlapping inputs",
      "z": "This is an error in gradcheck rather than in gradient computation. It's not specific to mv and would happen for self-overlapping inputs. Add a check for stride==0 in gradcheck ",
      "y": "This is an error in gradcheck not in gradient computation Add a check for stride==0 in gradcheck "
   },
   {
      "null": 7,
      "x": "libtorch 1.5.0 libiomp5.dylib contains erroneous link to /DLC/torch/libiomp5.dylib instead of using @rpath",
      "z": "I had the same issue and fixed it with:\n\n```shell\ninstall_name_tool -id @rpath/libiomp5.dylib libiomp5.dylib\n```\n\n",
      "y": "Use ```shell\ninstall_name_tool -id @rpath/libiomp5.dylib libiomp5.dylib\n```"
   },
   {
      "null": 8,
      "x": "RuntimeError: CUDA error: an illegal memory access was encountered",
      "z": "> I tried your \"kalman_filter.py\" script on today's master commit 1.6.0a0+21ba3b4, with small changes like this\n> \n> ```python\n> if __name__ == '__main__':\n>     device = 'cuda'\n> \n>     kf = KalmanFilter(device)\n>     mean, covariance = kf.initiate(torch.tensor([10, 15, 0.5, 10], device=device))\n>     mean, covariance = kf.predict(mean, covariance)\n>     mean, covariance = kf.update(mean, covariance, torch.tensor([12, 20, 0.6, 11], device=device))\n> \n>     mean_2, covariance_2 = kf.initiate(torch.tensor([12, 13, 0.7, 5], device=device))\n>     mean_2, covariance_2 = kf.predict(mean_2, covariance_2)\n>     mean_2, covariance_2 = kf.update(mean_2, covariance_2, torch.tensor([13, 14, 0.7, 8], device=device))\n> \n>     squared_maha = kf.gating_distance(torch.cat((mean, mean_2), dim=0),\n>                                       torch.cat((covariance, covariance_2), dim=0),\n>                                       torch.tensor([[12, 20, 0.6, 11],\n>                                                     [20, 16, 0.4, 18]], device=device))\n>     print(squared_maha)\n> ```\n> \n> The script finished well with no exception and gave same output as CPU. Can you try with nightly build (and maybe latest driver, cuda, cudnn, etc) to verify that?\n> \n> My environment (MAGMA 2.5.3)\n> \n> ```\n> PyTorch version: 1.6.0a0+21ba3b4\n> Is debug build: No\n> CUDA used to build PyTorch: 10.2\n> \n> OS: Fedora 31 (Workstation Edition)\n> GCC version: (GCC) 8.3.1 20190223 (Red Hat 8.3.1-2)\n> CMake version: version 3.15.3\n> \n> Python version: 3.7\n> Is CUDA available: Yes\n> CUDA runtime version: 10.2.89\n> GPU models and configuration: GPU 0: GeForce RTX 2070 SUPER\n> Nvidia driver version: 440.31\n> cuDNN version: /usr/local/cuda-10.2/targets/x86_64-linux/lib/libcudnn.so.7\n> \n> Versions of relevant libraries:\n> [pip3] numpy==1.17.3\n> [pip3] torch==1.6.0a0+21ba3b4\n> [pip3] torchvision==0.7.0a0+f9ef235\n> [conda] Could not collect\n> ```\n\nIn fact I have bypassed cholesky in some alternative way:\nhttps://github.com/GlassyWing/sort_torch/blob/3d4c809dee6ad35b2f7d5a4b75f9e30b07722441/deep_sort/sort/kalman_filter.py#L245\n\nIf you need to test, you need to cancel cholesky's comment and comment out the replacement method. \n\nIn previous attempts, I found that there will be no errors if run alone, but if run in multiple threads or used with other models, it may throw an exception. I have not found a reproducible pattern, but under multi-threading, each thread has a Kalman filter, and it is more likely to cause errors.",
      "y": "there will be no errors if run alone, but if run in multiple threads or used with other models, it may throw an exception."
   },
   {
      "null": 9,
      "x": "Will the next version of libtorch provide modules instead of header files in the c++20 standard?",
      "z": "You mentioned `instead of`. Even if under the most optimistic condition that all the dependencies get the support for the C++ modules before our next release, we won't stop to provide C++ headers. The reason is clear. We need to support the old toolchains and old CUDA. It is impossible to deprecate the C++ headers only in several months.",
      "y": "In order to support the old toolchains and old CUDA we need C++ headers."
   },
   {
      "null": 10,
      "x": "Some operations crash autograd if parameter size is changed.",
      "z": "I think you want to do something similar to what we do for `resize_()` [here](https://github.com/pytorch/pytorch/blob/cd9a357f32c7548d444b6e97aa52ded531d68f38/torch/csrc/autograd/VariableTypeManual.cpp#L240-L260). Basically, the Variable version of the function is made by hand, and will clear out the grad accumulator by setting it to a nullptr. And then does\n```cpp\n  {\n    at::AutoNonVariableTypeMode non_var_type_mode(true);\n    self_.set_(args);\n  }\n```\nto call the Tensor version of `set_` that will be routed to the different implementations in aten.",
      "y": "the Variable version of the function is made by hand, and will clear out the grad accumulator by setting it to a nullptr. And then does ```cpp { at::AutoNonVariableTypeMode non_var_type_mode(true); self_.set_(args); } ``` to call the Tensor version of `set_` that will be routed to the different implementations in aten."
   },
   {
      "null": 11,
      "x": "Make StorageImpl untyped for non-POD types",
      "z": "> But for the resizing functions, THStorage and THCStorage use some slightly different logic since one has to do a cuda memcopy.\n\nAh yes, this is going to be a bit tricky.\n\nA lot of the \"stuff\" you need is available (Allocator in particular lets you reallocate using the same allocator that originally allocated some data), but resizing also involves a copy to preserve the old data, and we don't have this abstracted right now. So in this case I wouldn't try to remove the resize functions, as we really do need two different versions. Does that help?",
      "y": "for the resizing functions, THStorage and THCStorage use some slightly different logic since one has to do a cuda memcopy.So in this case I wouldn't try to remove the resize functions, as we really do need two different versions"
   },
   {
      "null": 12,
      "x": "Gradient checkpointing fails to backprop in some cases",
      "z": "This is expected behavior I'm afraid.\n\n- The first one raise an error because, since you checkpoint all the way to the end, you try to backward something that does not require gradient and so it fails.\n- The second one works because the first module is not checkpointed and it's output requires grad. So the next checkpoint will work\n- The third one, the first modules won't work, but since the last one is not checkpointed, it produces an output that requires gradient and so the backward does not through a runtime error.\n- The last one works for the same reason as the second one.\n\nMaking the input to your model require gradients will fix that no?\n\nAlso looks like we have a warn-once for the \"Be careful gradients are going to be None\". Maybe we should make it more verbose :D ",
      "y": "There could be multiple reasons - The first one raise an error because, since you checkpoint all the way to the end, second one works because the first module is not checkpointed and it's output requires grad, the first modules won't work, but since the last one is not checkpointed, it produces an output that requires gradient and so the backward does not through a runtime error."
   },
   {
      "null": 13,
      "x": "max_pool2d doesn't check if input is contiguous: max_pool2d_with_indices_out_cuda_frame failed with error code 0 with specific inputs",
      "z": "Thanks for the report @fanbeatsman!\n\nIt seems the error is thrown, as `problemTensor` is not contiguous, as can be seen by the strides.\nCalling `problem_tensor = problem_tensor.contiguous()` solves the issue.\n\n@xwang233 will have a look at it.\n",
      "y": "use `problem_tensor = problem_tensor.contiguous()`"
   },
   {
      "null": 14,
      "x": "LocalFileSystem' object has no attribute 'makedirs'",
      "z": "This worked for me: \n- Uninstalled tensorflow, \n- Reinstalled Tensorboard. \nPS: Restart Kernel and tensorboard.",
      "y": "You can try to \n- Uninstall tensorflow, \n- Reinstall Tensorboard. \nRestart Kernel and tensorboard."
   },
   {
      "null": 15,
      "x": "A minor bug in unused grad_input check of run_backward()",
      "z": "[autograd] fix allow_unused checking for C++ API",
      "y": "Bux is fixed in [autograd] fix allow_unused checking for C++ API"
   },
   {
      "null": 16,
      "x": "Can't compile pytorch from source",
      "z": "https://github.com/pytorch/pytorch/pull/35157/commits/5c318611978a7f9add5b889ad70e4af5b10a9c00\ngcc don't support MOV V8.4s, V9.4s \nchange MOV V8.4s, V9.4s  to MOV V8.16b, V9.16b works",
      "y": "gcc don't support MOV V8.4s, V9.4s change MOV V8.4s, V9.4s to MOV V8.16b, V9.16b works"
   },
   {
      "null": 17,
      "x": "Runtime Error when using DistributedDataParallel with torch.no_grad()",
      "z": "Not so stick to torch1.0.0. So I used torch1.1.0 nightly and this can be solved. Thank you for your reply and contribution.",
      "y": "using torch1.1.0 nightly this can be solved"
   },
   {
      "null": 18,
      "x": "Export to ONNX doesn't support basic operations",
      "z": "This error is not from ONNX exporter. In this case, torch operator Conv2d is using an internal operator 'unfolded2d_copy' which is not implemented for float16 data type.\n\nIn this case, the issue you reported is not related to ONNX exporter.",
      "y": "torch operator Conv2d is using an internal operator 'unfolded2d_copy' which is not implemented for float16 data type, it is not an ONNX error"
   },
   {
      "null": 19,
      "x": "promotion of float with int never increases precision of float",
      "z": "> that doesn't affect this case though. Even if using half + long, the result is half, not get_default_dtype. If that's desired that's fine, but it seems odd to me to go this far with it:\n\nRight, but I don't think that's a common pattern, at least with the new APIs.  The common pattern is to just not specify a dtype or to use something like Apex to handle it for you.\n\n> Right now we have torch.promote_types(torch.half, torch.long) => torch.half (not promoting), but torch.promote_types(torch.complex64, torch.double) => torch.complex128 (promoting) and that seems sensible/reasonable but still slightly unexpected given our category/kind docs.\n\nI agree this seems a little strange but I don't think it's really a comparable situation; a `torch.double` can be perfectly represented as a `torch.complex128`; it \"is a\" `complex128`.  A `torch.int64` isn't a `torch.double`, and can't be perfectly represented as a `torch.double` anyway.  And again, if we take the view that you \"opt-in\" to using `torch.double` (because none of the APIs give you doubles unless you ask), then it seems fine to promote in that way because you've already opted-in at that point.",
      "y": "`torch.double` can be perfectly represented as a `torch.complex128`; it \"is a\" `complex128`. A `torch.int64` isn't a `torch.double`, and can't be perfectly represented as a `torch.double`"
   },
   {
      "null": 20,
      "x": "Compiling error with tag v1.4.0",
      "z": "It's my fault to forget to update submodule.\n```\n# you need to update submodule after checkout\ngit submodule sync\ngit submodule update --init --recursive\n<env variables...> python3 ./setup.py bdist_wheel\n```\n",
      "y": "``` # you need to update submodule after checkout git submodule sync git submodule update --init --recursive <env variables...> python3 ./setup.py bdist_wheel ```"
   },
   {
      "null": 21,
      "x": "[Bug] Weird bug when using ATen __rshift__() on cuda tensors",
      "z": "Sorry about this @zou3519 , but I have found a fix as well. Shall I send out a PR?",
      "y": "This issue has been fixed"
   },
   {
      "null": 22,
      "x": "MacOS conda caffe2 package incorrect protobuf versions.",
      "z": "All our packages our actually built with a statically linked custom protobuf with hidden visibility. They're not actually built against any protobuf in conda. This is to avoid tons of tricky problems involving incompatible protobuf versions and incompatible c++ compilers (e.g. ser https://github.com/caffe2/caffe2/issues/1980 ). Getting this to work requires a lot of care and separate packages for gcc4.8 or gcc5+, which is confusing to users and can mess with their conda environments.\n\nUnfortunately, I'm going to consider your use case as advanced enough that we aren't going to support it in the pre-built conda packages but rather ask that you build from source, when you can build protobuf and link it dynamically into Caffe2. If you encounter any issues linking against Caffe2 when building from source though, please open another issue to let us know as that is something that we want to be easy",
      "y": "All pytorch packages are actually built with a statically linked custom protobuf with hidden visibility. They're not actually built against any protobuf in conda. This is to avoid tons of tricky problems involving incompatible protobuf versions and incompatible c++ compilers."
   },
   {
      "null": 23,
      "x": "LSTM segmentation fault in docker",
      "z": "You can build a docker image from source with -  https://github.com/pytorch/pytorch#docker-image",
      "y": "you can build a docker image from source"
   },
   {
      "null": 24,
      "x": "Caffe2 fails to build with TensorRT on the Jetson TX2",
      "z": "This is most likely due to recent change in https://github.com/pytorch/pytorch/pull/8064.  Try cmake with `-DCAFFE2_LINK_LOCAL_PROTOBUF=OFF` and see if it unblocks you. \n\n@bddppq We make need to further patch the cmakefile... ",
      "y": "Try cmake with `-DCAFFE2_LINK_LOCAL_PROTOBUF=OFF`"
   },
   {
      "null": 25,
      "x": "GRUCELL crashes python for more than 2 layers",
      "z": "conda create -n pyto python=3.6 -y\nactivate pyto\n\nconda install numpy pyyaml mkl mkl-include setuptools cmake cffi typing pandas seaborn plotly scipy statsmodels jupyter notebook cython -y\npip install cufflinks \npip install sklearn \nconda install tbb -y\n\ncd C:\\Users\\Gabi\\Downloads\ngit clone --recursive https://github.com/pytorch/pytorch\ncd pytorch\n\nset USER_LDFLAGS=/LIBPATH:C:\\ProgramData\\Miniconda3\\envs\\pyto\\Library\\lib\nset \"VS150COMNTOOLS=C:\\Program Files (x86)\\Microsoft Visual Studio\\2017\\Enterprise\\VC\\Auxiliary\\Build\"\nset CMAKE_GENERATOR=Visual Studio 15 2017 Win64\nset DISTUTILS_USE_SDK=1\ncall \"%VS150COMNTOOLS%\\vcvarsall.bat\" x64 -vcvars_ver=14.14\nset CMAKE_INCLUDE_PATH=C:\\ProgramData\\Miniconda3\\Library\\include\nset LIB=C:\\ProgramData\\Miniconda3\\Library\\lib;C:\\ProgramData\\Miniconda3\\envs\\pyto\\Library\\lib;%LIB%\npython setup.py install",
      "y": "conda create -n pyto python=3.6 -y\nactivate pyto\n\nconda install numpy pyyaml mkl mkl-include setuptools cmake cffi typing pandas seaborn plotly scipy statsmodels jupyter notebook cython -y\npip install cufflinks \npip install sklearn \nconda install tbb -y\n\ncd C:\\Users\\Gabi\\Downloads\ngit clone --recursive https://github.com/pytorch/pytorch\ncd pytorch\n\nset USER_LDFLAGS=/LIBPATH:C:\\ProgramData\\Miniconda3\\envs\\pyto\\Library\\lib\nset \"VS150COMNTOOLS=C:\\Program Files (x86)\\Microsoft Visual Studio\\2017\\Enterprise\\VC\\Auxiliary\\Build\"\nset CMAKE_GENERATOR=Visual Studio 15 2017 Win64\nset DISTUTILS_USE_SDK=1\ncall \"%VS150COMNTOOLS%\\vcvarsall.bat\" x64 -vcvars_ver=14.14\nset CMAKE_INCLUDE_PATH=C:\\ProgramData\\Miniconda3\\Library\\include\nset LIB=C:\\ProgramData\\Miniconda3\\Library\\lib;C:\\ProgramData\\Miniconda3\\envs\\pyto\\Library\\lib;%LIB%\npython setup.py install"
   },
   {
      "null": 26,
      "x": "[Feature request] Equivalent of softmax_cross_entropy_with_logits",
      "z": "But those aren't equivalent. CrossEntropyLoss expects class indices for the targets, while softmax_cross_entropy_with_logits can work with probability distributions.",
      "y": "They are not equivalent CrossEntropyLoss expects class indices for the targets, and softmax_cross_entropy_with_logits works with probability distributions."
   },
   {
      "null": 27,
      "x": "No module named caffe2.python",
      "z": "You are right. It looks like it was my bad. I was executing it from the pytorch dir. If I change the dir to any other it is fine.",
      "y": "Make sure you are not in pytorch dir"
   },
   {
      "null": 28,
      "x": "cuda runtime error (77) : an illegal memory access at pytorch\\aten\\src\\thcunn\\generic/SpatialClassNLLCriterion.cu",
      "z": "your `masks` are having out-of-bounds memory accesses. It's likely that `masks` has value less than 0, or greater than (nClasses-1)",
      "y": "`masks` are having out-of-bounds memory accesses"
   },
   {
      "null": 29,
      "x": "public/cuda.cmake uses cuda_select_nvcc_arch_flags",
      "z": "This PR puts the discretional inclusion of Modules_CUDA_fix to public/cuda.cmake and also installs the corresponding files. Manually verified the build process but the installation path is not tested (only did a dummy install and eyeballed that the files are there).",
      "y": " Modules_CUDA_fix to public/cuda.cmake installs the corresponding files"
   },
   {
      "null": 30,
      "x": "TypeError: __init__() got an unexpected keyword argument 'target_tensor'",
      "z": "From the [docs](https://pytorch.org/docs/stable/_modules/torch/utils/data/dataset.html#TensorDataset), there is no keyword called `target_tensor` or `data_tensor`. You can pass a variable number of arguments to the constructor\n```python\nx = torch.linspace(1, 10, 10)\ny = torch.linspace(10, 1, 10)\ndataset = Data.TensorDataset(x, y)\n```",
      "y": "there is no keyword called target_tensor, you can pass variable length argument to the constructor ```python\nx = torch.linspace(1, 10, 10)\ny = torch.linspace(10, 1, 10)\ndataset = Data.TensorDataset(x, y)\n```"
   },
   {
      "null": 31,
      "x": "Does torch.Tensor work well with share memory list?",
      "z": "Because NumPy arrays don't get moved to shared memory by default (changes in one process probably won't be seen in another one). This is expected, so I'll close the issue.",
      "y": "NumPy arrays don't get moved to shared memory by default"
   },
   {
      "null": 32,
      "x": "Loaded network with load_state_dict has different shape but works anyway",
      "z": "check for exact shape match before loading   `if input_param.shape != param.shape:\n                    # local shape should match the one in checkpoint\n                    error_msgs.append('Size mismatch: copying a param of {} from checkpoint, `",
      "y": " fixed in pr `if input_param.shape != param.shape:\n                    # local shape should match the one in checkpoint\n                    error_msgs.append('Size mismatch: copying a param of {} from checkpoint, `"
   },
   {
      "null": 33,
      "x": "Batchnorm gives different results depending on whether cudnn is enabled",
      "z": "I was not able to reproduce non-deterministic results as long as bn weights are initialized to a fixed value. You are right that in your case the expected results is 0, as expected results is (x-mean)/var*weight[+bias], and, as long as mean is exactly equal to tensor values (as it should be), result is 0. However, due to some quirks of how cudnn computes output, a small (on the order of 1 ulp) error creeps in. With the default epsilon 1/var is approx 300, weight is on the order of 1, to get the result you are getting x-mean should be on the order of 1e-5. Given that x and mean are 100, it's enough for mean to have a 1e-7 relative error (which is approx 1 ulp) to produce the results you are seeing, which may happen due to fp arithmetic being inexact. ",
      "y": "the issue might be related to fp arithmetic being inexact. "
   },
   {
      "null": 34,
      "x": "Error in variance/stdv calculations",
      "z": "There is a `unbiased` option in the `var()` and `std()` function. If you set it to `False`, the answers match.",
      "y": " use `unbiased` option in the `var()` and `std()` function to match the answers."
   },
   {
      "null": 35,
      "x": "[Bug] Failure to acquire gradient with requires_grad and backward(gradient) in cuda (with 0.4.0)",
      "z": "> x = torch.randn(2, 2, requires_grad=True).cuda(0)\n\nIf you want gradients to accumulate in `x`, you should change the above line to \n``` \nx = torch.randn(2, 2, requires_grad=True, device='cuda:0')\n```\n\nThe reason why your code doesn't work is because of \n> x = torch.randn(2, 2, requires_grad=True).cuda(0)\n\nWhat this is doing is creating a \"leaf\" tensor `(torch.randn(2, 2, requires_grad=True)` and then assigning a copy of it (that is on CUDA device 0) to `x`. Gradients can only accumulate in leaf tensors.",
      "y": " The code is creating a \"leaf\" tensor `(torch.randn(2, 2, requires_grad=True)` and then assigning a copy of it (that is on CUDA device 0) to `x`. Gradients can only accumulate in leaf tensors. If you want gradients to accumulate in `x`, you should change code to\n``` \nx = torch.randn(2, 2, requires_grad=True, device='cuda:0')\n```\n"
   },
   {
      "null": 36,
      "x": "Possible bug in KL divergence",
      "z": "the documentation of KLDivLoss specifies that the `input` already has to be a set of log-probabilities, i.e. probably the output of a LogSoftmax or Sigmoid layer...\n\n> As with NLLLoss, the input given is expected to contain log-probabilities, however unlike ClassNLLLoss, input is not restricted to a 2D Tensor, because the criterion is applied element-wise.",
      "y": "the `input`  has to be a set of log-probabilities,As with NLLLoss, the input given is expected to contain log-probabilities"
   },
   {
      "null": 37,
      "x": "[Bug] Memory leak on Convnet on CPU",
      "z": "The 15GB of RAM for this input size wouldn't surprise me, because we use the (memory consuming) unfolding of the image to perform the convolution.\nIn your example, the unfolded image has size of roughly `16 * 64 * 9 * 9 * 224 * 224 * 4` which is roughly 15GB ([exact code here](https://github.com/pytorch/pytorch/blob/master/aten/src/THNN/generic/SpatialConvolutionMM.c#L192)).\n\nThe dimensions of the input image are too big for the kernel size.",
      "y": "The dimensions of the input image are too big for the kernel size, the unfolded image has size of roughly `16 * 64 * 9 * 9 * 224 * 224 * 4` which is roughly 15GB"
   },
   {
      "null": 38,
      "x": "[Bug] Model restoration on non cuda computer",
      "z": "According to [docs](http://pytorch.org/docs/master/torch.html#torch.load), I believe you should be loading your models with\n```python\ntorch.load(file, map_location='cpu')\n```",
      "y": "Load the model with \n```python\n   torch.load(file, map_location='cpu')\n```"
   },
   {
      "null": 39,
      "x": "Runtime Error with DataLoader: exited unexpectedly",
      "z": "In GPU mode (this works)..... \n\ndataset = UP_Dataset()\ntrain_loader = DataLoader(dataset=dataset,\n                          batch_size=100,\n                          shuffle=True,\n                          num_workers=0)                              # change num_workers=0\n",
      "y": "For GPU mode we can use -\n\ndataset = UP_Dataset()\ntrain_loader = DataLoader(dataset=dataset,\n                          batch_size=100,\n                          shuffle=True,\n                          num_workers=0)                              # change num_workers=0\n"
   },
   {
      "null": 40,
      "x": "ONNX support for AdaptiveMax/AvgPool ?",
      "z": "Hi @Scitator !\nYes, as long as you are just using adaptive pooling to do global pooling you can simply define the following module:\n```\nclass MyAdaptiveMaxPool2d(nn.Module):\n    def __init__(self, sz=None):\n        super().__init__()\n       \n\n    def forward(self, x): \n        inp_size = x.size()\n        return nn.functional.max_pool2d(input=x,\n                  kernel_size= (inp_size[2], inp_size[3]))\n```\n\nI also recently opened a discussion in the forums about how to replace adaptive pooling in general:\nhttps://discuss.pytorch.org/t/how-to-replicate-f-adaptive-max-pool-using-f-max-pool/16851 \n\nCheers!\n",
      "y": "use this module \n```\nclass MyAdaptiveMaxPool2d(nn.Module):\n    def __init__(self, sz=None):\n        super().__init__()\n       \n\n    def forward(self, x): \n        inp_size = x.size()\n        return nn.functional.max_pool2d(input=x,\n                  kernel_size= (inp_size[2], inp_size[3]))\n```"
   },
   {
      "null": 41,
      "x": "ONNX export does not support parallel model converted using nn.DataParallel(model) and will throw error message \"untraced buffer\"",
      "z": "```python\nstate_dict = torch.load('/path/to/your/.pth/model')\nmodel.load_state_dict(state_dict)\nmodel.eval()\ndummy_input = Variable(torch.randn(B, C, H, W))\ntorch.onnx.export(model.module, dummy_input, '/path/to/output/onnx/model', export_params = True)\n```\nThis works for me.",
      "y": "```\nstate_dict = torch.load('/path/to/your/.pth/model')\nmodel.load_state_dict(state_dict)\nmodel.eval()\ndummy_input = Variable(torch.randn(B, C, H, W))\ntorch.onnx.export(model.module, dummy_input, '/path/to/output/onnx/model', export_params = True)\n```\nThis should work"
   },
   {
      "null": 42,
      "x": "Scalar variable's .data attribute returns a non-scalar",
      "z": "Yes, we don't have scalar tensors.  But we are merging Variable and Tensor soon and .data will essentially be a no-op.",
      "y": "There are no scalar tensors"
   },
   {
      "null": 43,
      "x": "Determining `requires_grad` automatically",
      "z": "This is now in master, see the inputs= kwarg for .backward().",
      "y": "Give inputs= kwarg for .backward()"
   },
   {
      "null": 44,
      "x": "`Interrupted system call` error appearing after updating install today",
      "z": "num_workers=0 resolved it. It definitely seems to be that dataloader error, but it didn't add too much to my running time so we're set for now. Thanks for the help!",
      "y": "give num_workers=0  in dataloader"
   },
   {
      "null": 45,
      "x": "DataLoader returning non-CUDA tensors",
      "z": "@abhaikollara It reserves an area in CPU memory so pulling tensors in that area to GPU is fast. See http://pytorch.org/docs/master/notes/cuda.html#use-pinned-memory-buffers",
      "y": "It reserves an area in CPU memory so pulling tensors in that area to GPU is fast"
   },
   {
      "null": 46,
      "x": "torch.cat behaves differently on Tensor vs Variable (also a backward compatibility issue)",
      "z": "Hey @B1azingB1ade, actually after the Tensor/Variable merge they both throw this error in pytorch master. Discussed with @colesbury and we would like to keep it requiring tuple of Tensors since that's closer to the definition of 'concatenate'.\nThe cat on single Tensor can easily be done by view/reshape. Or if you would like to use torch.cat, torch.cat(list(x), dim=1) also works. ",
      "y": "The cat on single Tensor can easily be done by view/reshape. Or by torch.cat(list(x), dim=1) ."
   },
   {
      "null": 47,
      "x": "Feature Request: More general learning-rate scheduling",
      "z": "I'm a python noob and was going crazy trying to use this \"module\".\n\n```\nAttributeError: module 'torch.optim' has no attribute 'lr_scheduler'\n```\n\nFinally got it working by using:\n\n```\nfrom torch.optim import lr_scheduler\n```\n",
      "y": "```from torch.optim import lr_scheduler```"
   },
   {
      "null": 48,
      "x": "torch.arange type",
      "z": "Currently, the only way to return `long` tensors from `arange` is by using the following constructor\n```python\nidx = torch.arange(0, 10, out=torch.LongTensor())\n```\nBut this is indeed annoying, and will hopefully be fixed when https://github.com/pytorch/pytorch/issues/1433#issuecomment-330336116 is implemented",
      "y": "We can return `long` tensors from `arange` by using the following constructor ```python idx = torch.arange(0, 10, out=torch.LongTensor()) ```"
   },
   {
      "null": 49,
      "x": "torch.zeros_like() does not work",
      "z": "I was able to use torch.zeros(a.size()) as an alternative. Thanks for the info about documentation.",
      "y": "torch.zeros(a.size())  can be used as an alternative"
   },
   {
      "null": 50,
      "x": "nn.Linear requires 2D input",
      "z": "The issue has been fixed in latest release",
      "y": "fixed in latest release"
   },
   {
      "null": 51,
      "x": "Add get_lr in ReduceLROnPlateau",
      "z": "Hi, I'm the puller of lr_scheduler.\n\nActually, `get_lr` is designed to be more like an internal function for computing instead of fetching the learning rate (although it is not going to make inconsistent outcome unless the LRs are changed externally) Sorry for the confusion.\n\nFor fetching, you could use `[ group['lr'] for group in optim.param_groups ]` for now (Yep, it's tedious). Maybe what needs to be done is to rename `get_lr` to `compute_lr` and create another function called `get_lr`. But this shortcut also seems weird, as it wraps too many functionalities of the optimizer.",
      "y": "`get_lr` is designed to be more like an internal function for computing use `[ group['lr'] for group in optim.param_groups ]`"
   },
   {
      "null": 52,
      "x": "optimizer load_state_dict() problem?",
      "z": "Try moving optimizer state to the GPU memory manually after loading it from the checkpoint.\n```model = Model()\nmodel.load_state_dict(checkpoint['model'])\nmodel.cuda()\noptimizer = optim.Adam(model.parameters())\noptimizer.load_state_dict(checkpoint['optimizer'])\nfor state in optimizer.state.values():\n    for k, v in state.items():\n        if isinstance(v, torch.Tensor):\n            state[k] = v.cuda()```\nI agree that having an optimizer.cuda() method for this operation would be nice.",
      "y": "move optimizer state to the GPU memory manually\n```model = Model()\nmodel.load_state_dict(checkpoint['model'])\nmodel.cuda()\noptimizer = optim.Adam(model.parameters())\noptimizer.load_state_dict(checkpoint['optimizer'])\nfor state in optimizer.state.values():\n    for k, v in state.items():\n        if isinstance(v, torch.Tensor):\n            state[k] = v.cuda()```"
   },
   {
      "null": 53,
      "x": "register_hook-modified gradient cannot be applied to optimizer",
      "z": "Yes, it is expected. If your look at [the formula of Adam](https://arxiv.org/pdf/1412.6980.pdf), you'll find that Adam is scaling invariant. \u200aIt means that if we have some objective function f(x) and we change it to k*f(x) (where k is some constant), there will be no effect on performance. What you did is the same as scaling the objective function by a constant (since you are requiring and modifying just `a`).",
      "y": "It is expected result, for instance Adam is scaling invariant. \u200aIt means that if we have some objective function f(x) and we change it to k*f(x) (where k is some constant), there will be no effect on performance."
   },
   {
      "null": 54,
      "x": "Behavior of __setattr__ and __getattr__ not consistent",
      "z": "I don't think this is a bug, `__getattr__` is called when an attribute lookup has not found the attribute in the usual places (i.e. it is not an instance attribute nor is it found in the class tree for self). For example, this works fine:\n\n```\nimport torch\nfrom torch import nn\nmod = nn.Module()\nmod.val = 5\nprint(mod.val)\n>>>5\ngetattr(mod, 'val')\n>>>5\n```\n\nBut this `getattr(mod, 'val2')` throws the error you reference",
      "y": " `__getattr__` is called when an attribute lookup has not found the attribute in the usual places\nFor example, this works fine:\n\n```\nimport torch\nfrom torch import nn\nmod = nn.Module()\nmod.val = 5\nprint(mod.val)\n>>>5\ngetattr(mod, 'val')\n>>>5\n```\n\nBut this `getattr(mod, 'val2')` throws the error"
   },
   {
      "null": 55,
      "x": "DataParallel doesn't replicate module's member variables",
      "z": "You can register `counter` as a buffer. Then PyTorch can take care of it.\n\n`self.register_buffer('counter', torch.zeros(1))`\n\nThis way you will get consistent outputs.",
      "y": "for consistent outputs use - \n`self.register_buffer('counter', torch.zeros(1))`\n\n"
   },
   {
      "null": 56,
      "x": "CrossEntropyLoss is negative",
      "z": "negative values exist in the latest version of nn.BCELoss().\ne.g., criterion = nn.BCELoss()\nprint(criterion(torch.zeros(5,1),torch.zeros(5,1)))\nI got: tensor(1.00000e-12 *\n       -1.0001)\n\nmy workaround:\ncriterion(torch.zeros(5,1).clamp(1e-8,1-1e-7),torch.zeros(5,1))",
      "y": "negative values exist in the latest version of nn.BCELoss().\nworkaround:\ncriterion(torch.zeros(5,1).clamp(1e-8,1-1e-7),torch.zeros(5,1))"
   },
   {
      "null": 57,
      "x": "[feature request] zeros_like and ones_like for Variables",
      "z": "We'll get these functions automatically from the ATen bindings in Variable",
      "y": "For this use ATen bindings in Variable"
   },
   {
      "null": 58,
      "x": "Add unit tests that load jit models to protect against issues with API changes",
      "z": "Closing as we now have tests that do this as well as backcompat tests for operators.",
      "y": "The tests have been already added"
   },
   {
      "null": 59,
      "x": "ValueError: only one element tensors can be converted to Python scalars",
      "z": "Had this problem with pytorch 1.1.0, fixed after upgrading to 1.2",
      "y": "Upgrad to pytorch 1.2"
   },
   {
      "null": 60,
      "x": "Tensorboard: ValueError: Duplicate plugins for name projector",
      "z": "@Biaocsu, I also met the problem but I fixed it. My method is:\n\n1. I downloaded a test script from https://raw.githubusercontent.com/tensorflow/tensorboard/master/tensorboard/tools/diagnose_tensorboard.py\n2. I run it and it told me that I have two tensorboards with a different version. Also, it told me how to fix it.\n3. I followed its instructions and I can make my tensorboard work.\n\nI think this error means that you have two tensorboards installed so the plugin will be duplicated.  Another method would be helpful that is to reinstall the python environment using conda.\n\nHope to help you.",
      "y": " this error means that you have two tensorboards installed so the plugin will be duplicated.  Another method could be helpful is to reinstall the python environment using conda"
   },
   {
      "null": 61,
      "x": "Installing pytorch nightly with pip fails",
      "z": "pip install torch_nightly --no-index -f https://download.pytorch.org/whl/nightly/cu100/torch_nightly.html\nAlso note that these kind of errors can depend on the version of pip that you use (e.g. pypa/pip#4580). The nightly is definitely present, so if it doesn't work for you please upgrade to the latest pip and try again.\n\nEDIT: finally also verified that the cp37-cp37m, cp36-cp36m and cp35-cp35m wheels are all present and all from July 24th.",
      "y": "to install pytorch nightly use - \npip install torch_nightly --no-index -f https://download.pytorch.org/whl/nightly/cu100/torch_nightly.html"
   },
   {
      "null": 62,
      "x": "Build Pytorch/Libtorch with TBB support is failing",
      "z": "Confirmed that upgrading to Cmake 3.13.3 is fixing the issue! Thanks a lot @ilia-cher",
      "y": "upgrade to Cmake 3.13.3 "
   },
   {
      "null": 63,
      "x": "Difference to numpy linspace with endpoint False",
      "z": "This is not a bug, it's simply different default for dtype:\n```python\nIn [3]: torch.linspace(0, 1, 10+1).numpy()[:-1]\nOut[3]: array([0. , 0.1, 0.2, 0.3, 0.4, 0.5, 0.6, 0.7, 0.8, 0.9], dtype=float32)\n\nIn [4]: np.linspace(0, 1, 10, endpoint=False)\nOut[4]: array([0. , 0.1, 0.2, 0.3, 0.4, 0.5, 0.6, 0.7, 0.8, 0.9])\n\nIn [5]: np.linspace(0, 1, 10, endpoint=False) - torch.linspace(0, 1, 10+1).numpy()[:-1]\nOut[5]: \narray([ 0.00000000e+00, -1.49011611e-09, -2.98023223e-09, -1.19209289e-08,\n       -5.96046446e-09,  0.00000000e+00, -2.38418578e-08,  1.19209290e-08,\n       -1.19209289e-08,  2.38418579e-08])\n\nIn [6]: np.linspace(0, 1, 10, endpoint=False) - torch.linspace(0, 1, 10+1, dtype=torch.float64).numpy()[:-1]\nOut[6]: array([0., 0., 0., 0., 0., 0., 0., 0., 0., 0.])\n```\n\nThe differences are simply of order `float32.eps`. \n\nThat said, adding an `endpoint=True` keyword is a good idea.",
      "y": "it's simply different default for dtype:\n```python\nIn [3]: torch.linspace(0, 1, 10+1).numpy()[:-1]\nOut[3]: array([0. , 0.1, 0.2, 0.3, 0.4, 0.5, 0.6, 0.7, 0.8, 0.9], dtype=float32)\n\nIn [4]: np.linspace(0, 1, 10, endpoint=False)\nOut[4]: array([0. , 0.1, 0.2, 0.3, 0.4, 0.5, 0.6, 0.7, 0.8, 0.9])\n\nIn [5]: np.linspace(0, 1, 10, endpoint=False) - torch.linspace(0, 1, 10+1).numpy()[:-1]\nOut[5]: \narray([ 0.00000000e+00, -1.49011611e-09, -2.98023223e-09, -1.19209289e-08,\n       -5.96046446e-09,  0.00000000e+00, -2.38418578e-08,  1.19209290e-08,\n       -1.19209289e-08,  2.38418579e-08])\n\nIn [6]: np.linspace(0, 1, 10, endpoint=False) - torch.linspace(0, 1, 10+1, dtype=torch.float64).numpy()[:-1]\nOut[6]: array([0., 0., 0., 0., 0., 0., 0., 0., 0., 0.])\n```\n\nThe differences are simply of order `float32.eps`. "
   },
   {
      "null": 64,
      "x": "Creating an nn.Module instance with a buffer inside the init function signature of another nn.Module and then moving the main module to CUDA results in a global CUDA device being set for internal nn.Module buffers for future module instances",
      "z": "Don't use defaults like that in `def __init__(self, x, test1 = Test1()):`.\n\nThis creates a single instance that will be passed to future constructor calls as default. This is a well-known trap when using default parameters.\n\nTry:\n```\nt = Test2(torch.ones(5), Test1()).cuda()\nt = Test2(torch.ones(5), Test1()).cuda()\n```\nand your bug will vanish.",
      "y": "Don't use defaults in `def __init__(self, x, test1 = Test1()):`\nTry:\n```\nt = Test2(torch.ones(5), Test1()).cuda()\nt = Test2(torch.ones(5), Test1()).cuda()\n```"
   },
   {
      "null": 65,
      "x": "libtorch: Error when using torch::from_blob",
      "z": "use float -> torch::kFloat32",
      "y": "use float -> torch::kFloat32"
   },
   {
      "null": 66,
      "x": "Numerically inconsistent of division between numpy and torch",
      "z": "This is a little confusing and it's happening because PyTorch's default datatype is float32 while NumPy's is float64.\n\n```\nnp.divide(416, 1080)\n# 0.3851851851851852\ntorch.div(torch.tensor((416.), dtype=torch.double), torch.tensor(1080)).item()\n# 0.3851851851851852\ntorch.div(torch.tensor((416.), dtype=torch.float), torch.tensor(1080)).item()\n# 0.385185182094574\n```\n\nWhen performing the division with the same datatype the result is the same. ",
      "y": "PyTorch's default datatype is float32 while NumPy's is float64. When performing the division with the same datatype the result will be same."
   },
   {
      "null": 67,
      "x": "unable to cast Python instance to C++ type in .backward()",
      "z": "I found the bug. My fault,  to send sparse tensors using rpc, I convert the tensor to a list containing indices, values, and size. The future being returned from the parameter server for sparse layers contained a list, not a tensor. I converted the sparse format to tensor in the callback and got it working. \n\nexample of code that works\n```python\n\n    @staticmethod\n    @rpc.functions.async_execution\n    def average_gradient(\n        ps_rref,\n        received_batch_number,\n        param_loc,\n        gradient\n    ):\n        self = ps_rref.local_value()\n        if type(gradient) is list:\n            gradient = self.sparse_rpc_format_to_tensor(gradient)\n        if not self.use_cuda_rpc:\n            gradient = gradient.cuda(self.rank)\n        fut = torch.futures.Future()\n        with self.lock:\n            if self.batch_number < received_batch_number:\n                self.batch_number = received_batch_number\n                self.clear_batch_state()\n            self.process_gradient(gradient, param_loc)\n            if param_loc not in self.futures:\n                self.futures[param_loc] = []\n            self.futures[param_loc].append(fut)\n            if len(self.futures[param_loc]) == self.trainer_count:\n                self.record_straggler_end(self.param_key(param_loc))\n                param_loc_avg = self.average(param_loc)\n                if not self.use_cuda_rpc:\n                    param_loc_avg = param_loc_avg.cpu()\n                if param_loc_avg.is_sparse:\n                    param_loc_avg = self.sparse_tensor_to_rpc_format(param_loc_avg)\n                for cur_fut in self.futures[param_loc]:\n                    cur_fut.set_result(param_loc_avg)\n                self.record_batch_end(self.param_key(param_loc))\n        return fut\n\n    @staticmethod\n    def process_bucket(state, bucket):\n        cref = state.cref\n        tensor = bucket.get_tensor()\n        tensors_count = len(cref.bucket_to_parameters(bucket))\n        sparse = tensor.is_sparse\n        if not cref.use_cuda_rpc:\n            tensor = tensor.cpu()\n        if sparse:\n            tensor = cref.sparse_tensor_to_rpc_format(tensor)\n        ps = cref.ps\n        ps_args = [\n            cref.ps_rref,\n            state.batch_number,\n            state.param_loc,\n            tensor\n        ]\n        fut = cref.send_async_request(\n            state.get_key(),\n            cref.ps_rref,\n            ps.average_gradient,\n            *ps_args\n        )\n        state.param_loc += tensors_count\n\n        def callback(fut):\n            tensor = fut.wait()\n            if type(tensor) is list:\n                tensor = cref.sparse_rpc_format_to_tensor(tensor)\n            if not cref.use_cuda_rpc:\n                tensor = tensor.cuda(cref.rank)\n            return [tensor]\n\n        return fut.then(callback)\n```\n\n\n",
      "y": " convert the sparse format to tensor in the callback\nexample -\n```python\n\n    @staticmethod\n    @rpc.functions.async_execution\n    def average_gradient(\n        ps_rref,\n        received_batch_number,\n        param_loc,\n        gradient\n    ):\n        self = ps_rref.local_value()\n        if type(gradient) is list:\n            gradient = self.sparse_rpc_format_to_tensor(gradient)\n        if not self.use_cuda_rpc:\n            gradient = gradient.cuda(self.rank)\n        fut = torch.futures.Future()\n        with self.lock:\n            if self.batch_number < received_batch_number:\n                self.batch_number = received_batch_number\n                self.clear_batch_state()\n            self.process_gradient(gradient, param_loc)\n            if param_loc not in self.futures:\n                self.futures[param_loc] = []\n            self.futures[param_loc].append(fut)\n            if len(self.futures[param_loc]) == self.trainer_count:\n                self.record_straggler_end(self.param_key(param_loc))\n                param_loc_avg = self.average(param_loc)\n                if not self.use_cuda_rpc:\n                    param_loc_avg = param_loc_avg.cpu()\n                if param_loc_avg.is_sparse:\n                    param_loc_avg = self.sparse_tensor_to_rpc_format(param_loc_avg)\n                for cur_fut in self.futures[param_loc]:\n                    cur_fut.set_result(param_loc_avg)\n                self.record_batch_end(self.param_key(param_loc))\n        return fut\n\n    @staticmethod\n    def process_bucket(state, bucket):\n        cref = state.cref\n        tensor = bucket.get_tensor()\n        tensors_count = len(cref.bucket_to_parameters(bucket))\n        sparse = tensor.is_sparse\n        if not cref.use_cuda_rpc:\n            tensor = tensor.cpu()\n        if sparse:\n            tensor = cref.sparse_tensor_to_rpc_format(tensor)\n        ps = cref.ps\n        ps_args = [\n            cref.ps_rref,\n            state.batch_number,\n            state.param_loc,\n            tensor\n        ]\n        fut = cref.send_async_request(\n            state.get_key(),\n            cref.ps_rref,\n            ps.average_gradient,\n            *ps_args\n        )\n        state.param_loc += tensors_count\n\n        def callback(fut):\n            tensor = fut.wait()\n            if type(tensor) is list:\n                tensor = cref.sparse_rpc_format_to_tensor(tensor)\n            if not cref.use_cuda_rpc:\n                tensor = tensor.cuda(cref.rank)\n            return [tensor]\n\n        return fut.then(callback)\n```\n\n\n"
   },
   {
      "null": 68,
      "x": "Unclear documentation or unintended behaviour for Lazy modules",
      "z": "Yeah, it's fixed in the nightly version. Thanks a lot @albanD @gchanan !",
      "y": "it's fixed in the nightly version"
   },
   {
      "null": 69,
      "x": "PyTorch fails to detect Intel\u00ae oneAPI Math Kernel Library during build",
      "z": "Well, particularly my issue can be quick fixed by changing\nhttps://github.com/pytorch/pytorch/blob/60931611581f7d9fa8f40baee58533955d13b8ce/cmake/Modules/FindMKL.cmake#L43\n\nto something like:\n```\n  IF (EXISTS \"/opt/intel/oneapi/mkl\")\n    SET(DEFAULT_INTEL_MKL_DIR \"/opt/intel/oneapi/mkl/latest\")\n  ELSE()\n    SET(DEFAULT_INTEL_MKL_DIR \"/opt/intel/mkl\")\n  ENDIF()\n```\nHowever, there exists also **DEFAULT_INTEL_COMPILER_DIR** and **INTEL_OMP_DIR** which points to compiler runtime and threading related files and these directories structure was changed more substantially.\nAlso, platform (Win/Lin/Mac) specific paths should be corrected as well, so I guess Intel guys, who know all internal changes that come with OneAPI framework, would be very useful.",
      "y": "issue can be quick fixed by changing\nhttps://github.com/pytorch/pytorch/blob/60931611581f7d9fa8f40baee58533955d13b8ce/cmake/Modules/FindMKL.cmake#L43\n\nto :\n```\n  IF (EXISTS \"/opt/intel/oneapi/mkl\")\n    SET(DEFAULT_INTEL_MKL_DIR \"/opt/intel/oneapi/mkl/latest\")\n  ELSE()\n    SET(DEFAULT_INTEL_MKL_DIR \"/opt/intel/mkl\")\n  ENDIF()\n```"
   },
   {
      "null": 70,
      "x": "On master branch, `test_lkj_cholesky_log_prob` fails on MacOS 10.13 with `ATEN_CPU_CAPABILITY=default`",
      "z": "Hmm, replacing `==` with `torch.allclose` fixes the issue (and difference if default codepath between computed distribution and the actual value is 1e-16 )",
      "y": " replacing `==` with `torch.allclose`"
   },
   {
      "null": 71,
      "x": "Nightly PyTorch builds can not be installed using pip-21.1.1",
      "z": "Don't you need to add the `--pre` flag in order to install nightly builds?\n\n```\npython3 -mpip install -v torch -f https://download.pytorch.org/whl/nightly/cpu/torch_nightly.html --pre\n```\n\nCan't reproduce when attempting to install with `--pre` flag:\n<details>\n<summary> macOS </summary>\n\n```\n\u276f python3 -m pip install -f https://download.pytorch.org/whl/nightly/cpu/torch_nightly.html --pre torch\nLooking in links: https://download.pytorch.org/whl/nightly/cpu/torch_nightly.html\nCollecting torch\n  Downloading https://download.pytorch.org/whl/nightly/cpu/torch-1.9.0.dev20210517-cp36-none-macosx_10_9_x86_64.whl (127.8MB)\n```\n\n</details>\n\n<details>\n<summary> Linux </summary>\n\n```\n\u276f python3 -mpip install torch -f https://download.pytorch.org/whl/nightly/cpu/torch_nightly.html --pre\n\nLooking in links: https://download.pytorch.org/whl/nightly/cpu/torch_nightly.html\nCollecting torch\n  Downloading https://download.pytorch.org/whl/nightly/cpu/torch-1.9.0.dev20210517%2Bcpu-cp38-cp38-linux_x86_64.whl (175.4 MB)\n```\n\n</details>",
      "y": "add the `--pre` flag\n```\npython3 -mpip install -v torch -f https://download.pytorch.org/whl/nightly/cpu/torch_nightly.html --pre\n```"
   },
   {
      "null": 72,
      "x": "Error when doing CUDA Conv2d with 1x1 kernel.",
      "z": "To clarify, instead of:\n\n```python\nx = Variable(torch.randn(1, 1, 100, 100))\nx.cuda()  # This creates a copy on the GPU and immediately discards it. \"x\" is still on the CPU\n```\n\nYou should write:\n\n```python\nx = Variable(torch.randn(1, 1, 100, 100).cuda())\n```\n",
      "y": "instead of:\n\n```python\nx = Variable(torch.randn(1, 1, 100, 100))\nx.cuda()  # This creates a copy on the GPU and immediately discards it. \"x\" is still on the CPU\n```\n\ntry:\n\n```python\nx = Variable(torch.randn(1, 1, 100, 100).cuda())\n```"
   },
   {
      "null": 73,
      "x": "Add CrossEntropyLoss2d",
      "z": "To me these 2d modules are confusing e.g.\n[SpatialSoftmax.py](https://github.com/pytorch/pytorch/blob/0f65c9267d5ec55584b0ec65acb5374c95af9c16/torch/legacy/nn/SpatialSoftMax.py) and [Softmax.py](https://github.com/pytorch/pytorch/blob/0f65c9267d5ec55584b0ec65acb5374c95af9c16/torch/legacy/nn/SoftMax.py) look identical. In the outer wrapper code the difference seems to be an argument check, e.g. input to the Softmax module has to be 2D and input to the Softmax2d module has to be 4d - but in the end they call the same function.\n\nIn `torch.nn.functional` there are no '2d' versions of these functions but you can simply call\n`torch.nn.functional.log_softmax` or `torch.nn.functional.softmax` with 1d,2d,3d,4d tensors.\n\nIt took me a couple of minutes to understand this and I could imagine that it confuses also other users if they port a legacy model to the new function style and search for the corresponding 2d functions. Maybe it would be worth to add a hint in the documentation?",
      "y": " In the outer wrapper code the difference seems is an argument check, e.g. input to the Softmax module has to be 2D and input to the Softmax2d module has to be 4d "
   },
   {
      "null": 74,
      "x": "Gradients are zero when run on GPU (x.cuda())",
      "z": "I was actually looking at that and was scratching my head. It didn't occur to me that .cuda() was an immutable operation adding a node to the compute graph.",
      "y": ".cuda() is an immutable operation"
   },
   {
      "null": 75,
      "x": "CrossEntropyLoss masking",
      "z": "@greydanus  since it's all PyTorch in the implementation it's fine to do this manually without any performance hits. In fact, my performance even increased quite a bit because the CrossEntropyLoss does additional heavy lifting for ignoring of certain label values - a feature that most people won't use, but will pay in performance for. This almost seems like a poor choice for the library.\n\nAnyway, assume `y_hat` is `(N,C)` class of raw scores (from a fully connected layer) and `y` are the true indices as `LongTensor`. We can do:\n\n```python\nlogp = F.log_softmax(y_hat) # get (N,C) log probabilities\n# prepare an (N,C) array of 1s at the locations of ground truth\nymask = logp.data.new(logp.size()).zero_() # (N,C) all zero\nymask.scatter_(1, y.view(-1,1), 1) # have to make y into shape (N,1) for scatter_ to be happy\nymask = Variable(ymask)\n# pluck\nlogpy = (logp * ymask).sum(1) # this hurts in my heart\n```\n\n`logpy` becomes `(N,)` Tensor of the log probabilities of the correct classes. This works because during backpropagation the sum distributes the gradients equally to all channels, and then the gradient will get blocked everywhere where `ymask == 0` in each row, and will only flow through the elements of the correct classes. And if we wanted to weigh each example differently we would do\n\n```python\nnegative_log_likelihood_loss = -(logpy * per_example_weights).mean()\n```\n\n(or even pass the weights in during the `scatter_` call)\n\n\n------------------------\n**EDIT:**\nsimple gather can be very fast:\n\n```python\nlogp = F.log_softmax(y_hat)\nlogpy = torch.gather(logp, 1, y).view(-1)\n```\n",
      "y": "simple gather can be very fast:\n\n```python\nlogp = F.log_softmax(y_hat)\nlogpy = torch.gather(logp, 1, y).view(-1)\n```"
   },
   {
      "null": 76,
      "x": "rebuild pip wheels with manylinux",
      "z": "I had the same problem and I've found the solution. Basically, pip is trying to run \"pip install torch\" because torch is listed in the dependencies and it does not detect the previously build version with wheel. So just run \"pip install --no-deps torchvision\" and it should work.\n\nAnd this could be permanently fixed by updating the setup.py file in torchvision repository.",
      "y": "run \"pip install --no-deps torchvision\" "
   },
   {
      "null": 77,
      "x": "ModuleNotFoundError: No module named 'torch._C'",
      "z": "@phenixcx the problem is that you have a folder called `torch` in the same directory which is being picked up. Do this: `cd ..` (to change directory), and then start `python` and `import torch`, it should work.",
      "y": "problem is that you have a folder called `torch` in the same directory which is being picked up.\n Do this: `cd ..` (to change directory), and then start `python` and `import torch`,"
   },
   {
      "null": 78,
      "x": "MaxUnpool2d breaks for certain shaped inputs",
      "z": "this is because MaxPool's downsampling is ambiguous in input shape (several input shapes can be mapped to the same output shape). \n\nThat's why, we provide: `output_size` as the third optional argument to the call operator: https://github.com/pytorch/pytorch/blob/master/torch/nn/modules/pooling.py#L243\n(i should document this).\n\nYou can do:\n\n```\nimport torch\nfrom torch import nn\nfrom torch.autograd import Variable\n\ndata = Variable(torch.rand(1, 3, 540, 960))\n\npool = nn.MaxPool2d(2, 2, return_indices=True)\nunpool = nn.MaxUnpool2d(2, 2)\n\nout, indices1 = pool(data)\nsize1 = out.size()\nout, indices2 = pool(out)\nsize2 = out.size()\nout, indices3 = pool(out)\nsize3 = out.size()\n\nout = unpool(out, indices3, output_size=size2)\nout = unpool(out, indices2, output_size=size1)\nout = unpool(out, indices1)\n```",
      "y": "MaxPool's downsampling is ambiguous in input shape provide: `output_size` as the third optional argument to the call operato\n```\nimport torch\nfrom torch import nn\nfrom torch.autograd import Variable\n\ndata = Variable(torch.rand(1, 3, 540, 960))\n\npool = nn.MaxPool2d(2, 2, return_indices=True)\nunpool = nn.MaxUnpool2d(2, 2)\n\nout, indices1 = pool(data)\nsize1 = out.size()\nout, indices2 = pool(out)\nsize2 = out.size()\nout, indices3 = pool(out)\nsize3 = out.size()\n\nout = unpool(out, indices3, output_size=size2)\nout = unpool(out, indices2, output_size=size1)\nout = unpool(out, indices1)\n```"
   },
   {
      "null": 79,
      "x": "Flag to check if a Module is on CUDA similar to is_cuda for Tensors",
      "z": "Alternatively (assuming your model is on a single device):\n```python\nnext(model.parameters()).is_cuda\n```",
      "y": "You can check is using \n```python\nnext(model.parameters()).is_cuda\n```"
   },
   {
      "null": 80,
      "x": "CUDNN batchnorm backprop doesn't work properly in evaluation mode",
      "z": "cudnn does not support backprop in evaluate mode, we should add a fallback to nn for this case",
      "y": "cudnn does not support backprop in evaluate mode"
   },
   {
      "null": 81,
      "x": "1>E:\\software\\libtorch\\include\\torch/csrc/utils/variadic.h(195): error C2951: \u6a21\u677f \u58f0\u660e\u53ea\u80fd\u5728\u5168\u5c40\u3001\u547d\u540d\u7a7a\u95f4\u6216\u7c7b\u8303\u56f4\u5185\u4f7f\u7528",
      "z": "Please add `::` before the usage of `std` in the corresponding line.",
      "y": "add `::` before the usage of `std`"
   },
   {
      "null": 82,
      "x": "OSError: [WinError 193] %1 is not a valid Win32 application",
      "z": "lt is likely that your environment is messed up. As you can see from the traceback, there are two python environments involved here:\n1. C:\\Users\\nouma\\AppData\\Roaming\\Python\\Python37\n2. C:\\Users\\nouma\\Anaconda3\n\nPlease make sure your `PATH` is clean and you can actually remove one of them first.",
      "y": "there are two python environments involved here,make sure your `PATH` is clean and you can actually remove one of them first."
   },
   {
      "null": 83,
      "x": "PyTorch1.2 ONNX dynamic_axis is not working",
      "z": "@gbmarc1 Thanks for adding more info.\nPlease use newer version PyTorch. This issue is already resolved.\nhttps://github.com/pytorch/pytorch/blob/8a2dcff189b7b20e1d247f36ee89b902e5be5a88/torch/onnx/utils.py#L763",
      "y": "Please use newer version PyTorch."
   },
   {
      "null": 84,
      "x": "Free Memory after CUDA out of memory error",
      "z": "> I cannot reproduce this locally\n\nI kind of expected this. I noticed that, in a notebook, it happens more frequently if the exception is not handled within the function but by the notebook environment. The modified function\n\n```python\nimport torch\n\ndef oom(raise_=False):\n    try:\n        x = torch.randn(100, 10000, device=1)\n        for i in range(100):\n            l = torch.nn.Linear(10000, 10000)\n            l.to(1)\n            x = l(x)\n    except RuntimeError as e:\n        print(e)\n        print('at iteration', i)\n        if raise_:\n            raise\n```\ncalled two times with \n```python\noom(True)\n```\nalmost always causes the error in my environment.\n\n> Could you check what happens if you call torch.cuda.empty_cache() between the two calls of oom(). You must call this in the command line and not in the oom() function itself.\n\nYes, it frees a few MB of memory from the GPU (visible in `nvidia-smi`), but the remaining few GBs still remain on the GPU. Following calls of `oom()` still result in instant failure during the first iteration. The only way I can reliably free the memory is by restarting the notebook / python command line.\n\nCan this be related to the PyTorch and CUDA versions I'm using? I am limited to CUDA 9, so I sticked to PyTorch 1.0.0 instead of the newest version.",
      "y": "free the memory by restarting the notebook / python command line."
   },
   {
      "null": 85,
      "x": "There is a small wrong mistake in the pytorch official web file",
      "z": "0.3.1 docs are frozen but it\u2019s already been fixed in master, so should be ok starting from the next release",
      "y": "0.3.1 docs are frozen but it\u2019s already been fixed in master,"
   },
   {
      "null": 86,
      "x": "gpu memory not released after run `sudo kill [pytorch process id]`",
      "z": "This is usually because some process are still alive. You can find them by doing `lsof /dev/nvidia0` and finding if any python process is listed there that should not be here. Then you can kill that process.",
      "y": " some process are still alive. You can find them by doing `lsof /dev/nvidia0`"
   },
   {
      "null": 87,
      "x": "Segmentation fault when cat list with all-empty cuda tensors",
      "z": "There's a check that should exclude zero-dim tensors from `cat` (`a.dim()` should be 0 in this case), so I'm wondering why that's not happening right now...\n\nedit: Nevermind, I was running an old build. I pulled the latest master and with `a` being an empty tensor (with shape (0,)), cat crashes.",
      "y": "`a` being an empty tensor (with shape (0,)), cat crashes"
   },
   {
      "null": 88,
      "x": "ReLU(inplace=True) seems something wrong internal",
      "z": "@sonack it's an in-place operation because `+=` is the in-place add.\nYou avoid the in-place by doing\n```python\nout = out + residual\n```\nYou can verify that with other data structures as well:\n```python\na = []\nb = a  # b is the same as a\na += [5]\nprint(b)  # prints [5]\n\n# now\na = []\nb = a  # b is the same as a\na = a + [5]\nprint(b)  # prints []\n```",
      "y": "it's an in-place operation because `+=` is the in-place add\nYou avoid the in-place by doing\n```python\nout = out + residual\n```"
   },
   {
      "null": 89,
      "x": "Crash when dividing Variable(LongTensor) by a float",
      "z": "This is probably the same issue as #5008 (the float is being cast into a long). I'll take a look into it.",
      "y": "the float is being cast into a long"
   },
   {
      "null": 90,
      "x": "nn.LSTM.cuda() leads to CuDNNError",
      "z": "Hi @jiacheng-xu , your code snippet works on my pytorch0.3.1 build. Could you try deleting ~/.nv if it exists and rerun?\n",
      "y": "try deleting ~/.nv if it exists"
   },
   {
      "null": 91,
      "x": "torch.Size doesn't accept pytorch scalar",
      "z": "@adamlerer everything should be called 'Tensor' now and nothing should be called 'Variable', so that should be fixed as well\n\nAs a workaround, you can do `torch.Size([torch.arange(10).max().long().item()])` for now",
      "y": "everything should be called 'Tensor' now and nothing should be called 'Variable'\nAs a workaround, you can do `torch.Size([torch.arange(10).max().long().item()])` "
   },
   {
      "null": 92,
      "x": "BCELoss with weights for labels (like weighted_cross_entropy_with_logits in TF)",
      "z": "I follow @velikodniy to add the Weighted BCEloss, where the weights can be computed dynamically for each batch:\n``` python\ndef weighted_binary_cross_entropy(sigmoid_x, targets, pos_weight, weight=None, size_average=True, reduce=True):\n    \"\"\"\n    Args:\n        sigmoid_x: predicted probability of size [N,C], N sample and C Class. Eg. Must be in range of [0,1], i.e. Output from Sigmoid.\n        targets: true value, one-hot-like vector of size [N,C]\n        pos_weight: Weight for postive sample\n    \"\"\"\n    if not (targets.size() == sigmoid_x.size()):\n        raise ValueError(\"Target size ({}) must be the same as input size ({})\".format(targets.size(), sigmoid_x.size()))\n\n    loss = -pos_weight* targets * sigmoid_x.log() - (1-targets)*(1-sigmoid_x).log()\n\n    if weight is not None:\n        loss = loss * weight\n\n    if not reduce:\n        return loss\n    elif size_average:\n        return loss.mean()\n    else:\n        return loss.sum()\n```\n``` python\nclass WeightedBCELoss(Module):\n    def __init__(self, pos_weight=1, weight=None, PosWeightIsDynamic= False, WeightIsDynamic= False, size_average=True, reduce=True):\n        \"\"\"\n        Args:\n            pos_weight = Weight for postive samples. Size [1,C]\n            weight = Weight for Each class. Size [1,C]\n            PosWeightIsDynamic: If True, the pos_weight is computed on each batch. If pos_weight is None, then it remains None.\n            WeightIsDynamic: If True, the weight is computed on each batch. If weight is None, then it remains None.\n        \"\"\"\n        super().__init__()\n\n        self.register_buffer('weight', weight)\n        self.register_buffer('pos_weight', pos_weight)\n        self.size_average = size_average\n        self.reduce = reduce\n        self.PosWeightIsDynamic = PosWeightIsDynamic\n\n    def forward(self, input, target):\n        # pos_weight = Variable(self.pos_weight) if not isinstance(self.pos_weight, Variable) else self.pos_weight\n        if self.PosWeightIsDynamic:\n            positive_counts = target.sum(dim=0)\n            nBatch = len(target)\n            self.pos_weight = (nBatch - positive_counts)/(positive_counts +1e-5)\n\n        if self.weight is not None:\n            # weight = Variable(self.weight) if not isinstance(self.weight, Variable) else self.weight\n            return weighted_binary_cross_entropy(input, target,\n                                                 self.pos_weight,\n                                                 weight=self.weight,\n                                                 size_average=self.size_average,\n                                                 reduce=self.reduce)\n        else:\n            return weighted_binary_cross_entropy(input, target,\n                                                 self.pos_weight,\n                                                 weight=None,\n                                                 size_average=self.size_average,\n                                                 reduce=self.reduce)\n```",
      "y": " add the Weighted BCEloss, where the weights can be computed dynamically for each batch:\n``` \ndef weighted_binary_cross_entropy(sigmoid_x, targets, pos_weight, weight=None, size_average=True, reduce=True):\n    \"\"\"\n    Args:\n        sigmoid_x: predicted probability of size [N,C], N sample and C Class. Eg. Must be in range of [0,1], i.e. Output from Sigmoid.\n        targets: true value, one-hot-like vector of size [N,C]\n        pos_weight: Weight for postive sample\n    \"\"\"\n    if not (targets.size() == sigmoid_x.size()):\n        raise ValueError(\"Target size ({}) must be the same as input size ({})\".format(targets.size(), sigmoid_x.size()))\n\n    loss = -pos_weight* targets * sigmoid_x.log() - (1-targets)*(1-sigmoid_x).log()\n\n    if weight is not None:\n        loss = loss * weight\n\n    if not reduce:\n        return loss\n    elif size_average:\n        return loss.mean()\n    else:\n        return loss.sum()\n```\n``` \nclass WeightedBCELoss(Module):\n    def __init__(self, pos_weight=1, weight=None, PosWeightIsDynamic= False, WeightIsDynamic= False, size_average=True, reduce=True):\n        \"\"\"\n        Args:\n            pos_weight = Weight for postive samples. Size [1,C]\n            weight = Weight for Each class. Size [1,C]\n            PosWeightIsDynamic: If True, the pos_weight is computed on each batch. If pos_weight is None, then it remains None.\n            WeightIsDynamic: If True, the weight is computed on each batch. If weight is None, then it remains None.\n        \"\"\"\n        super().__init__()\n\n        self.register_buffer('weight', weight)\n        self.register_buffer('pos_weight', pos_weight)\n        self.size_average = size_average\n        self.reduce = reduce\n        self.PosWeightIsDynamic = PosWeightIsDynamic\n\n    def forward(self, input, target):\n        # pos_weight = Variable(self.pos_weight) if not isinstance(self.pos_weight, Variable) else self.pos_weight\n        if self.PosWeightIsDynamic:\n            positive_counts = target.sum(dim=0)\n            nBatch = len(target)\n            self.pos_weight = (nBatch - positive_counts)/(positive_counts +1e-5)\n\n        if self.weight is not None:\n            # weight = Variable(self.weight) if not isinstance(self.weight, Variable) else self.weight\n            return weighted_binary_cross_entropy(input, target,\n                                                 self.pos_weight,\n                                                 weight=self.weight,\n                                                 size_average=self.size_average,\n                                                 reduce=self.reduce)\n        else:\n            return weighted_binary_cross_entropy(input, target,\n                                                 self.pos_weight,\n                                                 weight=None,\n                                                 size_average=self.size_average,\n                                                 reduce=self.reduce)\n```"
   },
   {
      "null": 93,
      "x": "Unable to build from latest master - nccl.h not found",
      "z": "fixed by the linked PR. Please reopen if there are further issues",
      "y": "This issue is fixed"
   },
   {
      "null": 94,
      "x": "ONNX export runtime error - tuple appears in op that does not forward tuples",
      "z": "@houseroad \n\nthe problem was solved after removing `DataParallel`.",
      "y": "This can be solved by removing removing `DataParallel`."
   },
   {
      "null": 95,
      "x": "Cannot convert certain empty tensors from numpy",
      "z": "> the error still persists. Using PyTorch 1.0.1.post2\n\nIt seems the fix is in master:\nhttps://github.com/pytorch/pytorch/blob/master/aten/src/ATen/native/Resize.h#L37-L45\nBut not in 1.0.1:\nhttps://github.com/pytorch/pytorch/blob/v1.0.1/aten/src/ATen/native/Resize.h#L37-L41\nSo you'll need to use a nightly build or include a workaround in your code.",
      "y": "you'll need to use a nightly build"
   },
   {
      "null": 96,
      "x": "Default chosen cuDNN convolution algorithm for V100 uses twice as much memory",
      "z": "This is pretty old. I'm OK with just calling it fixed and someone file a new bug if there's still problems.",
      "y": "Bug is fixed"
   },
   {
      "null": 97,
      "x": "When CUDA 10 support is planned?",
      "z": "We're running CI builds against CUDA 10, and I believe the current plan for pre-built binaries is the next release.",
      "y": "It has been released"
   },
   {
      "null": 98,
      "x": "Chaining Operations - Misunderstanding?",
      "z": "after masking your tensor have less elements in method (1) and more elements with a bunch of zeros in method (2). the denominator is different.",
      "y": "after masking your tensor have less elements in method (1) and more elements with a bunch of zeros in method (2)"
   },
   {
      "null": 99,
      "x": "Suppress Scientific Notation",
      "z": "This issue has been resolved by https://github.com/pytorch/pytorch/pull/16876 and `torch.set_printoptions(sci_mode=True)`. Closing this issue.",
      "y": "Issue can be resolved using `torch.set_printoptions(sci_mode=True)`."
   },
   {
      "null": 100,
      "x": "torch.sigmoid behaves inconsistently for 32- and 64-bit NaN inputs",
      "z": "sigmoid() doesn't use avx_mathfun.h anymore. https://github.com/pytorch/pytorch/pull/7341 switched `Vec256<float>::exp()` to use Sleef_expf8_u10 (and Sleef has proper handling of NaN).\n\nThe PR that changed `Vec256<float>::exp()` predates this issue, but wasn't in the stable release (0.4.1) at the time.\n\nNote that the open linked Sleef issue is not about NaN handling. It's about about Sleef functions not getting inlined, which hurts perf (and was the reason we were using avx_mathfun.h in 0.4.1)",
      "y": "sigmoid() doesn't use avx_mathfun.h anymore."
   },
   {
      "null": 101,
      "x": "What is the net *.pb file encoding?",
      "z": "I figured this out. I just needed to read the file in binary mode, ie: mode='rb'",
      "y": "read the file in binary mode"
   },
   {
      "null": 102,
      "x": "C++ API, IValue toTensor() didn't work",
      "z": "> Could we get a repro script? (and `torchscript_version.pt` or the python source for it). The module's output might be an IValue that is a list, could you check if it contains the output you are looking for?\n\nYes, the output is a list. And I call `.toTuple()` `->elements()` `.toTensor()` and it works. Thanks for your remind.",
      "y": "call `.toTuple()` `->elements()` `.toTensor()` "
   },
   {
      "null": 103,
      "x": "Assigning a parameter to an indexed tensor that was produced by DDP no longer works in torch nightly (1.7)",
      "z": "Hi,\n\nAfter looking into this her are a few comments:\n- You should not modify the input to your net when you use DDP. As this input will sometimes be a view of the original input and sometimes not. So you will end up modifying your dataset for some samples at each forward which is most likely not what you want to do.\n- @mrshenli will look into removing the Scatter op that creates a view here as it is not always needed. This will improve perf and remove this error (even though the warning on the previous point will still apply)\n- I will send a patch to fix the internal assert error that happens because we do an inplace op where the input that is modified does not require grads but the other does. And so the inplace checks do no run as it only check for the first input's requires_grad to know if the function is differentiable.",
      "y": "You should not modify the input to your net when you use DDP"
   },
   {
      "null": 104,
      "x": "libtorch_cpu.a is huge (1 GB) with build_android.sh",
      "z": "Thanks for the fix! '-g0' flag solved the problem. It reduced overall size by ~460 MB which is perfect.\nHad no clue about NDK generating debug symbols for release builds behind the scenes, now it makes sense why the resulting binary was huge.\n\nWe do use both \"--whole-archive\" and \"--gc-sections\" at the same time. And we will consider limiting ops we use for our modules to trim down size even more.\nThanks for the quick and thorough response!",
      "y": " '-g0' flag solves the problem"
   },
   {
      "null": 105,
      "x": "`__torch_function__` does not get call for torch.Tensor's `__getitem__` syntax",
      "z": "In the case where we run into this, `MyTensor` is not a Tensor (nor does it contain one, it is a symbolic representation of a Tensor), so trying to use `as_subclass` isn't really viable. When I looked at the code for handling `__getitem__` in C++ it looks like it is only checking `self` for `__torch_function__`, but is ignoring the potential `__torch_function__` on the index variable.",
      "y": " `MyTensor` is not a Tensor so trying to use `as_subclass` isn't really viable"
   },
   {
      "null": 106,
      "x": "I'm compiling the pytorch 1.6.0 stable version on CUDA 11.1 and CUDNN 8.04 with 3090 and failed.",
      "z": "> > 1.6 does not support CUDA 11, please use a newer version\n> \n> Hi, may I get to know what version will be capable to use with cuda 11.1?\n\ngeiniyigelianjie\n\n[https://github.com/pytorch/pytorch/issues/45028]\n\nneirongdazhiruxia:\n1. shiyong nvidia-docker cuda11.0-devel\n```\nREPOSITORY                         TAG                             IMAGE ID            CREATED             SIZE\nnvidia/cuda                        11.0-cudnn8-devel-ubuntu16.04   1741134981df        4 days ago          7.46GB\n```\n2. anzhuang anaconda3\n```\napt-get update\napt-get install wget\nwget https://repo.anaconda.com/archive/Anaconda3-2020.07-Linux-x86_64.sh\nchmod +x Anaconda3-2020.07-Linux-x86_64.sh\n./Anaconda3-2020.07-Linux-x86_64.sh\n```\n3. anzhuang \"pytorch nightly\" (kenengxuyaofanqiang)\n\n`conda install pytorch torchvision cudatoolkit=11 -c pytorch-nightly`\n\n",
      "y": "1.6 does not support CUDA 11, please use a newer version"
   },
   {
      "null": 107,
      "x": "How can I fix NAN loss (or very large MSE losses)?",
      "z": "Please use [PyTorch forum](https://discuss.pytorch.org/) for this sort of questions. Higher chance of getting answers there.\n\nBtw, from what I see (didnt went through the code thoroughly) you are not iterating through the dataloader properly. Basically, you use the 1st batch over and over (`iter` gives you an iterator, and with `next` you get the first chunk).\n\nInstead of:\n```python\nfor step in range(1, len(train_loader) + 1):\n    batch = next(iter(train_loader)) # <- issue here\n    ...\n```\ntry:\n```python\nfor step, batch in enumerate(train_loader):\n    ...\n```\n\nLet me know if this solves the issue.",
      "y": "Instead of:\n```\nfor step in range(1, len(train_loader) + 1):\n    batch = next(iter(train_loader)) # <- issue here\n    ...\n```\ntry:\n```\nfor step, batch in enumerate(train_loader):\n    ...\n```"
   },
   {
      "null": 108,
      "x": "Add `inputs` argument to `autograd.backward()`",
      "z": "SGTM",
      "y": "feature added"
   },
   {
      "null": 109,
      "x": "JIT LibTorch: Profiling Mode Enabled causes memory leak",
      "z": "Hi, thanks for the issue. Do you have a repro you could share by any chance ?",
      "y": "Bug is fixed"
   },
   {
      "null": 110,
      "x": "Warnings during compiling: floating-point value does not fit in required integral type",
      "z": "These compiler-warnings arise during compilation of the templated function uniform_int(). The warnings are misleading because they arise from the way the compiler compiles templated functions, but the if-else statements in the function obviate the possibilities that the warnings describe. So, the purpose of a fix would be to fix the compiler warnings, and not to fix any sort of a bug.",
      "y": "compiler-warnings arise during compilation of the templated function uniform_int()"
   },
   {
      "null": 111,
      "x": "Couldn't build multi scaled kernel nested model",
      "z": "@Jimut123 please post this question to https://discuss.pytorch.org/\nBut if you believe this is a bug in PyTorch, please do not hesitate to re-open the issue.",
      "y": "The issue is shared on pytorch discuss"
   },
   {
      "null": 112,
      "x": "torch.median returns the smaller element when the median value lies between two elements.",
      "z": "Aside from the indices, we might need some special handling for the gradient if we'd like to take the mean of the two middle elements. ",
      "y": " need some special handling for the gradient if we'd like to take the mean of the two middle elements. "
   },
   {
      "null": 113,
      "x": "Deadlock with shared CUDA tensors and multiprocessing (spawn)",
      "z": "Thanks Sam I have fix for it. Will submit PR shortly.",
      "y": "This issue is fixed"
   },
   {
      "null": 114,
      "x": "throw error when EmbeddingBag(..., sparse=True) and slice the embedding's weight",
      "z": "@albanD \n> The thing is that even slice forward does not support it. I think this is mainly because slice returns views but returning a view for this sparse Tensor might not be possible.\n\nI don't think forward is related though. The problem is `grad_out` is sparse. When the dense view receives a sparse gradient, it should be able to backpropagate through slice okay (by offsetting the indices). ",
      "y": " `grad_out` is sparse, When the dense view receives a sparse gradient, it should be able to backpropagate through slice by offsetting the indices"
   },
   {
      "null": 115,
      "x": "torch.scatter_ returns incorrect result when running on CPU using int64 indexes",
      "z": "This is fixed on master, see #38646 ",
      "y": "This issue is fixed"
   },
   {
      "null": 116,
      "x": "Required some argument in dataloader for setting randomstate",
      "z": "The RNG consumed by [`RandomSampler`](https://github.com/pytorch/pytorch/blob/479b04e26a59d20b72ad5fdaec819caaad49af75/torch/utils/data/sampler.py#L68) (created with `shuffle=True`) is the default RNG. So seeding the default RNG will make the same code return same ordering.\n\nIf you want the ordering the be the same with different code (really code that consumes different amount of RNG), for now you can create a custom `RandomSampler` class that uses a given RNG state. But I agree that we should really add a `generator=` kwarg for both `DataLoader` and all random samplers.",
      "y": " you can create a custom `RandomSampler` class that uses a given RNG state"
   },
   {
      "null": 117,
      "x": "masked_fill_ (and possibly others) produces a different output than masked_fill on cpu",
      "z": "You are trying to do an inplace operation on a self-overlapping tensor that results from `expand`. This is not going to work. Results on the GPU are different because when you are copying tensor to the GPU it becomes contiguous and not self-overlapping. The only thing we can do here is issue a warning or disallow inplace masked_fill on a self-overlapping tensor.",
      "y": "You are trying to do an inplace operation on a self-overlapping tensor that results from `expand`."
   },
   {
      "null": 118,
      "x": "The error of `torch.nn.SyncBatchNorm.convert_sync_batchnorm`",
      "z": "sorry, I fix the issue that I create optimizer before exec `model = torch.nn.SyncBatchNorm.convert_sync_batchnorm(model)`",
      "y": "The bug is fixed"
   },
   {
      "null": 119,
      "x": "Training using \u201cmp.spawn\u201d, can not reproduce the training results",
      "z": "seed in worker function, not main. the process that runs training never executed main.",
      "y": "seed in worker function, not main"
   },
   {
      "null": 120,
      "x": "nonzero function in C++ much slower than python(CUDA)",
      "z": "@glaringlee\nThanks.  \nIt's right that sleep doesn't matters.But it really helps me to find the problem. I find the problem is in C++ model infer is also asynchronous, so it's wrong to measure nonzero function without synchronize with cuda after the model inferrence. I will write a new issue about how to call cudaDeviceSynchronize() in torch C++.",
      "y": "the problem is in C++ model infer is also asynchronous, so it's wrong to measure nonzero function without synchronize with cuda after the model inferrence"
   },
   {
      "null": 121,
      "x": "[SSL: CERTIFICATE_VERIFY_FAILED] For Inception Resnet V2 on Google Colab",
      "z": "PyTorch has nothing to do with this, it's pretrained-models's hosting issue, maybe [this one](https://github.com/Cadene/pretrained-models.pytorch/issues/193). The workaround mentioned in the comments works btw",
      "y": "it's pretrained-models's hosting issue"
   },
   {
      "null": 122,
      "x": "Add custom request headers to torch.hub.download_url_to_file",
      "z": "Hi @pmeier, we'll happily accept a PR that adding `headers={}` to `download_url_to_file` API. Would you mind sending a PR for it? Please feel free to request me as reviewer there. Thanks! ",
      "y": "Added to the pr"
   },
   {
      "null": 123,
      "x": "Missing info in Tensorboard's add_graph()",
      "z": "Duplicate of #37415\n\nI think this issue can be closed. Looks to me as if this was addressed and closed with PR #37504.",
      "y": "Bug is fixed"
   },
   {
      "null": 124,
      "x": "torch.split(..) / torch.chunk(..) does not remove one dimension from tensor in some cases",
      "z": "Neither chunk nor split are expected to remove a dimension, because they are not guaranteed to produce chunks that can be seen as tensors with fewer dimensions than the source. You probably want torch.unbind. ",
      "y": "Neither chunk nor split are expected to remove a dimension use torch.unbind. "
   },
   {
      "null": 125,
      "x": "Nightly build includes absolute path in cmake file",
      "z": "I looked into this a little and don't think it's a problem. Those libraries are only linked to statically, so the built binaries don't look for them when installed on your machine. Checking the ldd and objdump outputs on some built linux binaries confirms that there are no unexpected cuda libraries that need to be linked to",
      "y": "there are no unexpected cuda libraries that need to be linked to"
   },
   {
      "null": 126,
      "x": "torch.nn.utils.rnn.pack_padded_sequence segment fault if not in decreasing order",
      "z": "cc @nairbv",
      "y": "Bug is fixed"
   },
   {
      "null": 127,
      "x": "l1loss different results based on arguments position",
      "z": "fixed on master",
      "y": "This issue is fixed in master"
   },
   {
      "null": 128,
      "x": "How do you convert the tensor to a float",
      "z": "`x.data<float>()` and note this only works if it's contiguous",
      "y": "`x.data<float>()` "
   },
   {
      "null": 129,
      "x": "the running time difference between the ATen library and pytorch ?",
      "z": "FWIW, your Python script only times how long it takes to schedule the operations for execution. You need to synchronize on the device before the `end = time.time()` call, e.g., with `torch.cuda.synchronize()`.",
      "y": "You need to synchronize on the device before the `end = time.time()` call, e.g., with `torch.cuda.synchronize()`."
   },
   {
      "null": 130,
      "x": "Feature request:  'concat' for Variable",
      "z": "Don't call the function directly, use [`torch.cat([var1, var2, var3], dim)`](http://pytorch.org/docs/torch.html#torch.cat).",
      "y": "Don't call the function directly, use [`torch.cat([var1, var2, var3], dim)`]"
   },
   {
      "null": 131,
      "x": "torch.load() and torch.save() of big tensors is slow due to tar",
      "z": "I've rewritten `load` and `save` to no longer use tar. It's 5-10x faster on my machine now, basically matching the speed of `cat` for simple files. I have to write some tests before sending it out but you can assign this to me.",
      "y": "rewritten `load` and `save` to no longer use tar. It's 5-10x faster"
   },
   {
      "null": 132,
      "x": "LSTM output dimensions",
      "z": "The way I handle this is by making each RNN layer of size 1, and using this after each BRNN:\n\n```python\n # (TxNxD*2) -> (TxNxD) by sum\nif self.bidirectional:\n     x = x.view(x.size(0), x.size(1), 2, -1).sum(2).view(x.size(0), x.size(1), -1) \n```\nNot as simple as just taking the output of the BRNN but hopefully that helps...",
      "y": "making each RNN layer of size 1, and using this after each BRNN solves this \n ```python # (TxNxD*2) -> (TxNxD) by sum if self.bidirectional: x = x.view(x.size(0), x.size(1), 2, -1).sum(2).view(x.size(0), x.size(1), -1) ```"
   },
   {
      "null": 133,
      "x": "Resize gradients computed by basic math Functions to match original input sizes",
      "z": "this was fixed at some point, we forgot to close the issue.",
      "y": "This issue is fixed"
   },
   {
      "null": 134,
      "x": "Backprop issue I can't figure out",
      "z": "Yup, confirmed to be a cuDNN problem (costs after 1,5k iter):\n\n|       | CPU  | GPU  | cuDNN |\n|-------|------|------|-------|\n| `hn1` | 0.27 | 0.26 | 0.67  |\n| `hn2` | 0.28 | 0.28 | 0.27  |",
      "y": "cuDNN problem"
   },
   {
      "null": 135,
      "x": "torch.cat bug/unexpected behavior",
      "z": "Sure, just wanted to provide little context :)",
      "y": "bug is fixed"
   },
   {
      "null": 136,
      "x": "[RFC] Join-based API to support uneven inputs in DDP",
      "z": "If users would like to handle uneven inputs, they can try different ways, as @mrshenli  mentioned in the forum: 1) if know the number of inputs, they will know there is uneven inputs or not, and drop the tail data  2) if not know the number of input, they can pipeline data reading and identify end of data on their own, then drop the tail data\n\nFor DDP API, I think this proposal provides another option (continue training for uneven inputs) for users and will be useful for quite a lot of cases, so voting for keep current design in this proposal. \n",
      "y": " keeping current design in this proposal"
   },
   {
      "null": 137,
      "x": "Scripted Model gave totally wrong result on iOS, but was correct on C++ frontend",
      "z": "Hi @seeker-Liu, I've received your demo project from @xta0 and built & run it. I compared the value of the `ans` (the variable `ans` from your iOS code and the variable `ans` from your C++ code), as well as the value of each `onset_accessor[i][j]` and `frame_accessor[i][j]`. The error between them can be almost ignored, the average error is about 10e-8. \n\nThe error calculation is attached. \n\n- `result_cxx.txt`: values of `onset_accessor` (column 1) and `frame_accessor` (column 2) from C++\n- `result_ios.txt`: values of `onset_accessor` (column 1) and `frame_accessor` (column 2) from iOS \n- You can run `python main.py` to see the result, please make sure your `pandas` has been installed, normally it can be installed by `pip install pandas`\n\nresult:\n```\nonset average error: 1.6914125348246035e-09\nframe average error: 2.9308559916846613e-08\n```\n\n[err_calc.zip](https://github.com/pytorch/pytorch/files/4685906/err_calc.zip)\n",
      "y": "The error between them can be almost ignored, the average error is about 10e-8. \n\nThe error calculation is attached. \n\n- `result_cxx.txt`: values of `onset_accessor` (column 1) and `frame_accessor` (column 2) from C++\n- `result_ios.txt`: values of `onset_accessor` (column 1) and `frame_accessor` (column 2) from iOS \n- You can run `python main.py` to see the result, please make sure your `pandas` has been installed, normally it can be installed by `pip install pandas`\n"
   },
   {
      "null": 138,
      "x": "Can't create tensor from tensor list, but tensor from numpy arrays list works fine",
      "z": "Cool! I'll do my best on Saturday! Thanks for incentivizing! (:",
      "y": "Bug is fixed"
   },
   {
      "null": 139,
      "x": "ValueError: can't optimize a non-leaf Tensor",
      "z": "Please ask questions on the forums, discuss.pytorch.org. ",
      "y": "Solution can be found in pytorch discuss"
   },
   {
      "null": 140,
      "x": "internal assert failed bug due to argmax gradfn",
      "z": "Seems to work in the nightly: `pip3 install --upgrade --pre torch torchvision -f https://download.pytorch.org/whl/nightly/cu102/torch_nightly.html`",
      "y": "works in the nightly: `pip3 install --upgrade --pre torch torchvision -f https://download.pytorch.org/whl/nightly/cu102/torch_nightly.html`"
   },
   {
      "null": 141,
      "x": "Adding typing_extensions as a dependency of pytorch",
      "z": "Looks like it should be pretty straightforward, it's pure Python and packaged in conda as well, so don't see any major stumbling blocks. I'll give it a go.",
      "y": "Bug is fixed"
   },
   {
      "null": 142,
      "x": "Docs of KLDivLoss seems incorrect",
      "z": "The `x_n` seems to be accepted in log-probs format while `y_n` in probs format:\n```\nAs with NLLLoss, the input given is expected to contain log-probabilities and is not restricted to a 2D Tensor. The targets are given as probabilities (i.e. without taking the logarithm).\n```\nso there seems no error in the formula",
      "y": "The `x_n` seems to be accepted in log-probs format while `y_n` in probs format:\n```\nAs with NLLLoss, the input given is expected to contain log-probabilities and is not restricted to a 2D Tensor. The targets are given as probabilities (i.e. without taking the logarithm).\n```"
   },
   {
      "null": 143,
      "x": "Store TORCH_CUDA_ARCH_LIST in torch and use it for C++ extensions",
      "z": "In the meantime, one can use [CUDA bin utils](https://docs.nvidia.com/cuda/cuda-binary-utilities/index.html#cuobjdump) to determine this via scripting.  A developer could verify the list upon deploying the dependency before compiling:\n```\n$ cuobjdump ~/.local/lib/python3.6/site-packages/torch/lib/libtorch_cuda.so -lelf | awk -F. '{print $3}' | grep sm | sort -u\nsm_35\nsm_37\nsm_50\nsm_60\nsm_61\nsm_70\nsm_75\n```",
      "y": " one can use [CUDA bin utils](https://docs.nvidia.com/cuda/cuda-binary-utilities/index.html#cuobjdump) to determine this via scripting\n```\n$ cuobjdump ~/.local/lib/python3.6/site-packages/torch/lib/libtorch_cuda.so -lelf | awk -F. '{print $3}' | grep sm | sort -u\nsm_35\nsm_37\nsm_50\nsm_60\nsm_61\nsm_70\nsm_75\n```"
   },
   {
      "null": 144,
      "x": "Crash on exit in Python 3.9.0a6",
      "z": "Sounds like a duplicate of https://github.com/pytorch/pytorch/issues/50014 (destruction of GIL in autograd threads)",
      "y": "destruction of GIL in autograd threads"
   },
   {
      "null": 145,
      "x": "AssertionError: tensor(1.3351e-05) not less than or equal to 1e-05 :",
      "z": "This should have been fixed in 1.5, refer #34764.",
      "y": "This issue is fixed"
   },
   {
      "null": 146,
      "x": "Installation on windows python 3.8 32bits",
      "z": "@chenchangquan5 `set USE_MKLDNN=0`.",
      "y": " `set USE_MKLDNN=0`"
   },
   {
      "null": 147,
      "x": "jit much slower with pytorch 1.5 (on custom rnns)",
      "z": "@vincentqb  \n```python\n        torch._C._jit_set_profiling_executor(False)\n        torch._C._jit_set_profiling_mode(False)\n```\n\nAlso, please see for more details:\n\nhttps://github.com/pytorch/pytorch/blob/master/benchmarks/fastrnns/bench.py#L212-L241",
      "y": "```python\n        torch._C._jit_set_profiling_executor(False)\n        torch._C._jit_set_profiling_mode(False)\n```"
   },
   {
      "null": 148,
      "x": "Implementing deg2rad, rad2deg",
      "z": "Cool!\n\nTake a look at how similar functions, like sin, are implemented. See: https://github.com/pytorch/pytorch/blob/master/aten/src/ATen/native/UnaryOps.cpp for the device-independent part of the code. Then you have the CPU and CUDA-specific parts in ATen/native/cpu and ATen/native/cuda, respectively. \n\n",
      "y": "you have the CPU and CUDA-specific parts in ATen/native/cpu and ATen/native/cuda, respectively. \n"
   },
   {
      "null": 149,
      "x": "Implementing HyperLSTM",
      "z": "I see, that makes sense. Thanks for the response @mruberry.\n\n> If, however, there's some functionality that HyperLSTM needs that's tricky to implement in PyTorch, then it makes sense to file a more focused issue requesting that functionality.\n\nThere is one thing, however I don't know if I should open another issue or if it's not really worth discussing.\n\nInside the hyperLSTM, two regular LSTM's exist. However, one of them needs LayerNormalization applied to the variable `c` (the long term memory) before it is multiplied by the output gate. The original paper on LayerNormalization also describes the same technique. \n\nIs it worthy to open another issue to request this feature be implemented (a boolean `layerNorm` passed to the LSTM)? Or is this is a niche use-case where it would be better off if I just implement my own variant of an LSTM that includes LayerNormalization.",
      "y": "The  feature will be added"
   },
   {
      "null": 150,
      "x": "Is grad attribute in Variable immutable?",
      "z": "This is out of date. You can assign it now and it works as expected.",
      "y": "You can assign it now and it works as expected"
   },
   {
      "null": 151,
      "x": "CuDNN ConvTranspose1d issue with transposed cuda Variable (can't convert to contiguous)",
      "z": "This has been fixed on master:\n![image](https://user-images.githubusercontent.com/5652049/32459250-fd587eaa-c2fc-11e7-8dd3-ca35df49f98d.png)\nYou can build pytorch from source to avoid this bug or wait for the next release.",
      "y": "This has been fixed on master"
   },
   {
      "null": 152,
      "x": "Unable to use cat on torch.LongTensor",
      "z": "@aleSuglia it's not a bug, `torch.zeros(x.size())` returns a `torch.FloatTensor` and `x` is a `torch.LongTensor`. You need to convert `x` to `long()` as well.\n\n`torch.cat((x, torch.zeros(x.size()).type_as(x)))` or alternatively `torch.cat((x, x.new(*x.size()).fill_(0)))`",
      "y": "`torch.zeros(x.size())` returns a `torch.FloatTensor` and `x` is a `torch.LongTensor`. You need to convert `x` to `long()` as well."
   },
   {
      "null": 153,
      "x": "Low performance issue on CPU-only machine",
      "z": "@SsnL I noticed that the other modifications also have similar bugs.  \n\n- ger operation \n[Intel manual](https://software.intel.com/en-us/mkl-developer-reference-c-cblas-ger) explains that \n~~~\nFor Layout = CblasColMajor, the value of lda must be at least max(1, m).\n\nFor Layout = CblasRowMajor, the value of lda must be at least max(1, n).\n~~~\nHowerver, the modification does not check the transposing info and give the limit condition\n~~~\n(lda >= THMax(1, n)) && (lda <= INT_MAX) &&\n~~~\n\n- gemv operation\n[Intel manual](https://software.intel.com/en-us/mkl-developer-reference-c-cblas-gemv) explains that\n~~~ \nFor Layout = CblasColMajor, the value of lda must be at least max(1, m).\n\nFor Layout = CblasRowMajor, the value of lda must be at least max(1, n).\n~~~\nA similar bug also exsits.\n~~~\n(lda >= THMax(1, n)) && (lda <= INT_MAX) &&\n~~~",
      "y": "bug is fixed"
   },
   {
      "null": 154,
      "x": "indexing operator `[]` inconsistent",
      "z": "> Yes, indexing with a list of indices will never return a view (even if they form a consecutive range)\n\nOk, understood.\n\n> This works fine for me.\n> What's your PyTorch version?\n```\n(root) ~ $ cat /tmp/test3.py\nimport torch\n\nidxes = torch.LongTensor([0, 2])\na = torch.rand(5, 4)\na[idxes, 1:2] = 7  # gives exception\nprint('a', a)\n\n(root) ~ $ python /tmp/test3.py\nTraceback (most recent call last):\n  File \"/tmp/test3.py\", line 5, in <module>\n    a[idxes, 1:2] = 7  # gives exception\nTypeError: indexing a tensor with an object of type torch.LongTensor. The only supported types are integers, slices, numpy scalars and torch.LongTensor or torch.ByteTensor as the only argument.\n\n(root) ~ $ conda list | grep sou\npytorch                   0.2.0                py36_1cu75    soumith\ntorchvision               0.1.9                    py36_1    soumith\n```\n",
      "y": "indexing with a list of indices will never return a view"
   },
   {
      "null": 155,
      "x": "torch.svd() get error, w/ CUDA 9.0 w/o MAGMA",
      "z": "@zhanghang1989 now we have magma for cuda90 with `conda install magma-cuda90 -c soumith`",
      "y": "now we have magma for cuda90 with `conda install magma-cuda90 -c soumith`"
   },
   {
      "null": 156,
      "x": "Installing from source failing on Ubuntu 17.10",
      "z": "@vickylance \n\n1) Install CUDA and CUDNN from the respective .deb files.\n2) Add a PPA that contains the version of gcc/g++ that you want: ```sudo add-apt-repository ppa:ubuntu-toolchain-r/test```.\n3) ```sudo apt install g++-6 gcc-6```\n3) Set $CC and $CXX to point to the right version of the tools.\n4) Clone pytorch, install with ```python setup.py install```",
      "y": "1) Install CUDA and CUDNN from the respective .deb files.\n2) Add a PPA that contains the version of gcc/g++ that you want: ```sudo add-apt-repository ppa:ubuntu-toolchain-r/test```.\n3) ```sudo apt install g++-6 gcc-6```\n3) Set $CC and $CXX to point to the right version of the tools.\n4) Clone pytorch, install with ```python setup.py install```"
   },
   {
      "null": 157,
      "x": "CrossEntropyLoss for 3D and higher",
      "z": "agreed, would be good to have. We'll get this done.",
      "y": "feature added"
   },
   {
      "null": 158,
      "x": "undefined symbol: cudnnSetConvolutionGroupCount while running with cuDNN 7.0.3 and CUDA 9",
      "z": "There turned out to be sneaky conda package.\nFixed by doing `conda uninstall cudnn` and recompiling.",
      "y": "Fixed by doing `conda uninstall cudnn` and recompiling."
   },
   {
      "null": 159,
      "x": "[Feature Request] Make \"forward\" handle large inputs in batches",
      "z": "I'm not sure we want to add such convenience function (but others might have different opinions).\nI think that there can be weird edge cases (for example, the batch might not be the first dimension as in some rnns I think), and for the user it can be implemented in one line\n```python\ntorch.cat([model(x) for x in input.split(batch_size, 0)], 0)\n```\nWhat do you think?",
      "y": "there can be weird edge cases (for example, the batch might not be the first dimension as in some rnns I think), and for the user it can be implemented in one line\n```\ntorch.cat([model(x) for x in input.split(batch_size, 0)], 0)\n```"
   },
   {
      "null": 160,
      "x": "Sort sequences internally in pack_padded_sequence",
      "z": "Would really love to have this feature! Has this been implemented yet?",
      "y": "feature added"
   },
   {
      "null": 161,
      "x": "Documentation: Indexing output from bidirectional RNN (GRU,LSTM)",
      "z": "From what I've understand so far about non `Cell` suffixed CuDNN variants:\n\n1. You need to give them a tensor where the sequences are sorted by length in a decreasing order. If the sequences are guaranteed to always have the same length, you can skip this step. Otherwise, cf. `pack_padded_sequence()`\n2. After calling the RNN, you receive a tuple of 2 items: **packed** `Variable` of all hidden states (`hs`) and a normal `Variable` containing the last hidden states (`ht`).\n3. For `hs` you unpack it using `pad_packed_sequence()` to get a normal `Variable`.\n4. `ht` contains the **correct** forward and backward states for each sequence so you don't have to do something to recover or mask out 0's etc **but** this tensor does not concatenate the forward-backward states although `hs` returns them in a concatenated fashion.\n\n```python\n# An input of 5 timesteps and 2 sequences. The shorter one is 0-padded.\n# hidden_dim = 3, bidirectional=True, num_layers=1\nIn [525]: input_\nOut[525]:\n\n 1  3\n 3  5\n 3  2\n 2  0\n 1  0\n[torch.LongTensor of size 5x2]\n\n# hs and ht are the return values of GRU here (for LSTM you'll also have c_t)\nIn [526]: print(hs[:, 1], ht[:, 1])\n\nVariable containing:\n   (( forward states ))                      (( backward states ))\n-0.0982  0.0275 -0.3005            0.3609 -0.4958  0.3408\n-0.1710 -0.0576 -0.3759            0.2550 -0.3478  0.2796\n-0.1935  0.0484 -0.4111            0.2088 -0.2813  0.1440\n 0.0000  0.0000  0.0000            0.0000  0.0000  0.0000\n 0.0000  0.0000  0.0000            0.0000  0.0000  0.0000\n[torch.FloatTensor of size 5x6]\n\n Variable containing:\n-0.1935  0.0484 -0.4111\n 0.3609 -0.4958  0.3408\n[torch.FloatTensor of size 2x3]\n```\nHere you can see that the last state for the forward sequence (3->5->2) is the third row's first 3 elements `-0.1935  0.0484 -0.4111` that you also find in the `ht` variable in the first row.\n\nThe last state for the backward sequence (2->5->3) is the first row's second part `0.3609 -0.4958  0.3408` that you also find in the `ht`variable in the second row.\n\nSo if you want to apply attention the first tensor is the one that you'll need. If you want to just take the last states, second tensor is at your help.\n\nBut if `num_layers > 1`, the second tensor becomes a mess :) Overall, I think this part of the PyTorch API really needs more intuitive handling. \n",
      "y": "1. You need to give them a tensor where the sequences are sorted by length in a decreasing order. If the sequences are guaranteed to always have the same length, you can skip this step. Otherwise, cf. `pack_padded_sequence()`\n2. After calling the RNN, you receive a tuple of 2 items: **packed** `Variable` of all hidden states (`hs`) and a normal `Variable` containing the last hidden states (`ht`).\n3. For `hs` you unpack it using `pad_packed_sequence()` to get a normal `Variable`.\n4. `ht` contains the **correct** forward and backward states for each sequence so you don't have to do something to recover or mask out 0's etc **but** this tensor does not concatenate the forward-backward states although `hs` returns them in a concatenated fashion.\n\n```python\n# An input of 5 timesteps and 2 sequences. The shorter one is 0-padded.\n# hidden_dim = 3, bidirectional=True, num_layers=1\nIn [525]: input_\nOut[525]:\n\n 1  3\n 3  5\n 3  2\n 2  0\n 1  0\n[torch.LongTensor of size 5x2]\n\n# hs and ht are the return values of GRU here (for LSTM you'll also have c_t)\nIn [526]: print(hs[:, 1], ht[:, 1])\n\nVariable containing:\n   (( forward states ))                      (( backward states ))\n-0.0982  0.0275 -0.3005            0.3609 -0.4958  0.3408\n-0.1710 -0.0576 -0.3759            0.2550 -0.3478  0.2796\n-0.1935  0.0484 -0.4111            0.2088 -0.2813  0.1440\n 0.0000  0.0000  0.0000            0.0000  0.0000  0.0000\n 0.0000  0.0000  0.0000            0.0000  0.0000  0.0000\n[torch.FloatTensor of size 5x6]\n\n Variable containing:\n-0.1935  0.0484 -0.4111\n 0.3609 -0.4958  0.3408\n[torch.FloatTensor of size 2x3]\n```"
   },
   {
      "null": 162,
      "x": "Docker build fail",
      "z": "How do you fix this issue? I met the same issue.",
      "y": "The bug is fixed"
   },
   {
      "null": 163,
      "x": "Assert MKL conditions in THBlas.c",
      "z": "They will not pass for things like expanded tensors:\n\n```python\nx = torch.randn(1).expand(5, 5)\nx.mm(x)\n```",
      "y": "They will not pass for things like expanded tensors:\n\n```\nx = torch.randn(1).expand(5, 5)\nx.mm(x)\n```"
   },
   {
      "null": 164,
      "x": "binary cross entropy requires double tensor for target",
      "z": "@Kuzphi  @zou3519 @soumith I think he might see this page http://pytorch.org/docs/master/nn.html#torch.nn.functional.binary_cross_entropy , it is not BCELoss page. As you mentioned, the documentation is wrong.",
      "y": " the documentation is wrong"
   },
   {
      "null": 165,
      "x": "pytorch distributed timeout when running with number processes > 16",
      "z": "@apaszke, sorry for delayed response.  I've added `setTimeout()`, like follows:\n```c++\n  // NOTE: this function needs to be thread safe\n  std::shared_ptr<context_type> createContext(\n    const DataChannelGloo::Group& group,\n    const std::string& prefix\n  ) {\n    auto context = std::make_shared<context_type>(\n        group.mustGetGroupRank(_rank), group.size());\n    prefix_store_type prefix_store(prefix, *group._store);\n    context->setTimeout(std::chrono::minutes(15));\n    context->connectFullMesh(prefix_store, _device);\n    return context;\n  }\n```\n\nSeems that it helped a bit, but I will hold conclusions for a little while since there is another failure mode described below.\n\nI have added distributed validation to [ImageNet training example](https://github.com/pytorch/examples/blob/master/imagenet/main.py), like this:\n```python\n    metrics = torch.FloatTensor([losses.avg, top1.avg, top5.avg])\n    if args.distributed:\n        dist.all_reduce(metrics)\n        metrics /= dist.get_world_size()\n```\n\nUnfortunately, I get errors running that operation:\n```\n[1,4]<stderr>:Traceback (most recent call last):\n[1,4]<stderr>:  File \"torch_imagenet_gloo_dv_smooth_warmup.py\", line 376, in <module>\n[1,4]<stderr>:    main()\n[1,4]<stderr>:  File \"torch_imagenet_gloo_dv_smooth_warmup.py\", line 180, in main\n[1,4]<stderr>:    prec1 = validate(val_loader, model, criterion, epoch)\n[1,4]<stderr>:  File \"torch_imagenet_gloo_dv_smooth_warmup.py\", line 298, in validate\n[1,4]<stderr>:    dist.all_reduce(metrics)\n[1,4]<stderr>:  File \"/usr/local/lib/python2.7/dist-packages/torch/distributed/__init__.py\", line 344, in all_reduce\n[1,4]<stderr>:    return torch._C._dist_all_reduce(tensor, op, group)\n[1,4]<stderr>:RuntimeError: [/mnt/share/asergeev/torch_imagenet/pytorch/torch/lib/gloo/gloo/transport/tcp/pair.cc:696] Socket closed [xx.xx.xx.xx]:1157\n```\n\nI tried to add MPI barrier before that operation, but it did not help.\n\nI have tried the same code with NCCL2 transport and it works flawlessly and initializes much faster.",
      "y": "added `setTimeout()`, like follows:\n```c++\n  // NOTE: this function needs to be thread safe\n  std::shared_ptr<context_type> createContext(\n    const DataChannelGloo::Group& group,\n    const std::string& prefix\n  ) {\n    auto context = std::make_shared<context_type>(\n        group.mustGetGroupRank(_rank), group.size());\n    prefix_store_type prefix_store(prefix, *group._store);\n    context->setTimeout(std::chrono::minutes(15));\n    context->connectFullMesh(prefix_store, _device);\n    return context;\n  }\n```\n code with NCCL2 transport works flawlessly and initializes much faster."
   },
   {
      "null": 166,
      "x": "off_t' undeclared with gcc 4.8.5 on Linux",
      "z": "Let's consider adding CI against gcc 4.8. Probably doing just builds (not test) should be OK.",
      "y": "adding CI against gcc 4.8"
   },
   {
      "null": 167,
      "x": "conda install torch?",
      "z": "@soumith - OK, I understand now, thanks for your help!\n\nTo run https://github.com/jcjohnson/neural-style I need torch, not pytorch, and unfortunately there's no easy install via conda.\n\n> torch is a separate product from pytorch, pytorch has no depedency on torch. \n\nWell, except from the name, the expectation that `pytorch` is strongly connected to `torch` is probably a mistake that new users like me that haven't used either package before are likely to make? So I still think a sentence at http://pytorch.org either in the install or on the about page explicitly stating that one doesn't need http://torch.ch/ would be helpful (especially given their complex install).",
      "y": "torch is a separate product from pytorch, pytorch has no depedency on torch. "
   },
   {
      "null": 168,
      "x": "DISABLED test_send_recv_any_source_autograd_profiler (__main__.TestMPIWithFork)",
      "z": "Should be fixed after https://github.com/pytorch/pytorch/pull/57253",
      "y": "bug is fixed"
   },
   {
      "null": 169,
      "x": "Unable to trace RRef's when using torch.jit.script with remote functions.",
      "z": "I don't think you can create `torch.jit.Futures` in your own code; you need use `torch.jit.fork`, which returns a `Future`.",
      "y": "you need use `torch.jit.fork`, which returns a `Future`."
   },
   {
      "null": 170,
      "x": "Failed to build pytorch on mac os. Building pthreadpool",
      "z": "`pthreadpool` dependency is added on mobile build, so disabling `USE_PYTORCH_QNNPACK` solved the issue.\n\nThe final command to build:\n```\nDEBUG=1 USE_PYTORCH_QNNPACK=0 INTERN_BUILD_MOBILE=0 CC=clang CXX=clang++ BUILD_CAFFE2=0 BUILD_CUSTOM_PROTOBUF=0 USE_OPENMP=0 USE_DISTRIBUTED=0 USE_MKLDNN=0 USE_CUDA=0 BUILD_TEST=0 USE_FBGEMM=0 USE_NNPACK=0 USE_QNNPACK=0 USE_XNNPACK=0 python setup.py develop\n```\n\nThanks for sharing the link",
      "y": "`pthreadpool` dependency is added on mobile build, so disabling `USE_PYTORCH_QNNPACK` solves the issue.\n\nThe final command to build:\n```\nDEBUG=1 USE_PYTORCH_QNNPACK=0 INTERN_BUILD_MOBILE=0 CC=clang CXX=clang++ BUILD_CAFFE2=0 BUILD_CUSTOM_PROTOBUF=0 USE_OPENMP=0 USE_DISTRIBUTED=0 USE_MKLDNN=0 USE_CUDA=0 BUILD_TEST=0 USE_FBGEMM=0 USE_NNPACK=0 USE_QNNPACK=0 USE_XNNPACK=0 python setup.py develop\n```"
   },
   {
      "null": 171,
      "x": "Profiling within a callback function does not raise an error but fails to execute a print statement",
      "z": "@anj-s I think there's a typo in the code, profiler should be invoked with\n\n`with torch.autograd.profiler.profile(use_cuda=True)`, not `with torch.autograd.profiler(use_cuda=True)`. \n\nAlso we should add a `wait` to `return_future.then` which helps raise the error @mrzzd mentioned.\n\nWith these 2 changes I'm able to get the profiler to work, although get another error when exporting trace:\n```\nRuntimeError: Got the following error when running the callback: ValueError: negat\nive seek position -1\n```\n\nthis appears to be an edge case when we export trace but there is nothing in the trace (cc @ilia-cher - I think that can repro without distributed and we should fix that). Adding some tensor operations seems to mitigate that. Here is an example that works for me: P411576109 \n\n> cc @rohan-varma do you know why future's method might fail here\n\nYeah, looks like it does silently failed but we can get the failure by try/catch or calling wait. Generally if callbacks fail they will fail silently if we don't explicitly wait or catch errors on them, since they are non blocking by design. ",
      "y": "profiler should be invoked with\n\n`with torch.autograd.profiler.profile(use_cuda=True)`, not `with torch.autograd.profiler(use_cuda=True)`. \n\nAlso we should add a `wait` to `return_future.then` which helps raise the error"
   },
   {
      "null": 172,
      "x": "Nightly CUDA 11.1 Libtorch Builds failing, undefined symbol '__gmon_start__'",
      "z": "@driazati might be, although unlikely, as debug symbols should not be mapped in VM, i.e. should not affect PLT limits. But out of abundance of caution, can you please try kicking the build with option disabled?",
      "y": " try kicking the build with option disabled"
   },
   {
      "null": 173,
      "x": "macOS conda nightlies failing when importing torch, 'Library not loaded: @rpath/libmkl_intel_lp64.1.dylib'",
      "z": "https://github.com/pytorch/builder/pull/730 might resolve this, will close after next nightly build",
      "y": "the issue is fixed"
   },
   {
      "null": 174,
      "x": "PyTorch Installation for Windows using Conda gives unrecognized arguments:nvidia",
      "z": "Yea, this is introduced by https://github.com/pytorch/pytorch.github.io/pull/688",
      "y": "this is introduced by https://github.com/pytorch/pytorch.github.io/pull/688"
   },
   {
      "null": 175,
      "x": "`Warning: Leaking Caffe2 thread-pool after fork` when using `DataLoader` with `num_workers>0` and `pin_memory=True`",
      "z": "I personally believe that if this is normal behaviour, then this warning should be removed.\nThis is a very basic use case in PyTorch, which, in my opinion, shouldn't trigger a warning unless something is wrong.\n\nAlso, I don't really understand the point of the warning. \"The Caffe2 thread-pool was leaked\". As a user, do I need to do anything about that? Should I even care?",
      "y": "Bug is fixed"
   },
   {
      "null": 176,
      "x": "layer identifier in powerSGD_hook",
      "z": "I think this would be a better question for the forums\n\nhttps://discuss.pytorch.org/\n\nClosing, in favor of posting on the forums",
      "y": "closed in in favor of posting on the forums"
   },
   {
      "null": 177,
      "x": "COO to CSR tensor conversion is slow",
      "z": "How about using `torch.searchsorted` for this?",
      "y": "use  `torch.searchsorted`"
   },
   {
      "null": 178,
      "x": "`coalesce` creates overflowed indices in large sparse COO tensors",
      "z": "HIgh priority for silent wrong behavior and crashes",
      "y": "Bug is fixed"
   },
   {
      "null": 179,
      "x": "test_eig_with_eigvec_cuda_float64 (TestTensorDeviceOpsCUDA) is failing intermittently on ROCm",
      "z": "The tests from `test_torch.py` are being removed in https://github.com/pytorch/pytorch/pull/56284.\nAs for the validness of skipping the ROCm build, I can build it later and see how MAGMA-based functions work and what skips we can avoid.",
      "y": "The tests from `test_torch.py` are being removed in https://github.com/pytorch/pytorch/pull/56284."
   },
   {
      "null": 180,
      "x": "std: ambiguous in C++17.h, upgrading from LibTorch 1.6.0 to 1.8.1 with MSVC 2019 and c++17 standard ",
      "z": "@hcyang The workaround is to append `::` before `std::function` calls in `C++17.h`. Further investigation is in progress.",
      "y": "append `::` before `std::function` calls in `C++17.h`."
   },
   {
      "null": 181,
      "x": null,
      "z": null,
      "y": null
   },
   {
      "null": 182,
      "x": "custom collect_fn return None but collate_fn does not accept None",
      "z": "Thanks for your response and the link. I am using filtering in my Dataset to solve the problem now. Really looking forward to seeing and using the new Dataloader!",
      "y": "use filtering in Dataset to solve the problem"
   },
   {
      "null": 183,
      "x": "error: identifier \"cusparseScsrmm2\" is undefined",
      "z": "Does this failure reproduces if PyTorch-1.7 or later is used? (Please note, that CUDA-11 was not yet available when PyTorch-1.3 was released)",
      "y": "CUDA-11 was not yet available when PyTorch-1.3 was released"
   },
   {
      "null": 184,
      "x": "torch.tensor with list of arrays orders of magnitude slower than np.array",
      "z": "Thanks for the report @futscdav. This is a duplicate of gh-13918. That has 2 PRs pending to fix the issue. gh-51731 seems to be the preferred fix, but it hasn't moved in a month - I'll ping on there. And will close this as a duplicate.",
      "y": "Bug is fixed"
   },
   {
      "null": 185,
      "x": "Expected to have finished reduction in the prior iteration before starting a new one.",
      "z": "Thanks for the comments @mrshenli!, rohan-varma\n\nI passed the \"plugins=DDPPlugin(find_unused_parameters=True)\" into pl.trainer() and worked.",
      "y": "pass the \"plugins=DDPPlugin(find_unused_parameters=True)\" into pl.trainer() "
   },
   {
      "null": 186,
      "x": "Unskip CUDA grad/gradgrad checks or no longer mark as slow test",
      "z": "@krshrimali maybe something you'd be interested in investigating? We'd need to know how much time each of these tests adds to the CI, and then we can determine if we can re-enable them",
      "y": "Bug is fixed"
   },
   {
      "null": 187,
      "x": "torch.size for cfloat or cdouble",
      "z": "well, `x.numel() * (1 + x.is_complex())`? or you cam `x.view_as_complex` in forward.",
      "y": "`x.numel() * (1 + x.is_complex())`? or you cam `x.view_as_complex` in forward."
   },
   {
      "null": 188,
      "x": "Native Declared Functions Don't Support Full Aliasing Specification",
      "z": "I can also work around by removing the annotations, but i\u2019d rather not, because then my schema won\u2019t be telling the whole truth about its inplace effects and aliasing.",
      "y": "work around by removing the annotations"
   },
   {
      "null": 189,
      "x": "Meshes not showing in tensorboard",
      "z": "Thanks. \n\nI have raised it here [PyTorch Forum](https://discuss.pytorch.org/t/meshes-not-showing-in-tensorboard/62565)",
      "y": " raised it here PyTorch Forum"
   },
   {
      "null": 190,
      "x": "cuDNN built against wrong CUDA version (10.0 instead of 9.0) when building from source -> CUDNN_STATUS_NOT_INITIALIZED",
      "z": "Questions about build issues are really best asked in the forums: https://discuss.pytorch.org/.\n\nIf you go to https://pytorch.org/, however, you can find PyTorch 1.3 or nightlies with CUDA 10.1 support now. ",
      "y": "you can find PyTorch 1.3 or nightlies with CUDA 10.1 support "
   },
   {
      "null": 191,
      "x": "clang 9 segfaults when trying to compile PyTorch",
      "z": "Just to add, this now affects Xcode 11.4 (Apple Clang 11.0.3) which is based on LLVM 9.0.0.",
      "y": " this now affects Xcode 11.4 (Apple Clang 11.0.3) which is based on LLVM 9.0.0."
   },
   {
      "null": 192,
      "x": "Load data directly from GPU without copy to CPU",
      "z": "I've posted this a few times but cannot find it right now.\n\nExample for changing data from a cv::cuda::GpuMat to a torch::Tensor (supporting float type and byte type, converting to float)\n\n```C++\nvoid deleter(void *arg){};\n    torch::Tensor matToTensor(const cv::cuda::GpuMat &image, int device)\n    {\n        bool isByte = (image.type() & 0xF) < 2;\n        auto chans = image.channels();\n        std::vector<int64_t> dims = {image.rows, image.cols, chans};\n        std::vector<int64_t> strides = {(int64_t)image.step1(), chans, 1};\n        auto options = torch::TensorOptions().dtype(isByte ? torch::kByte : torch::kFloat).device(torch::kCUDA, device);\n        auto tensorImage = torch::from_blob(image.data, dims, strides, deleter, options);\n\n        if (isByte)\n        {\n            tensorImage = tensorImage.to(torch::kFloat);\n        }\n\n        return tensorImage;\n    }\n```\n\nThe part you are interested in: **torch::from_blob**",
      "y": "Example for changing data from a cv::cuda::GpuMat to a torch::Tensor (supporting float type and byte type, converting to float)\n\n```C++\nvoid deleter(void *arg){};\n    torch::Tensor matToTensor(const cv::cuda::GpuMat &image, int device)\n    {\n        bool isByte = (image.type() & 0xF) < 2;\n        auto chans = image.channels();\n        std::vector<int64_t> dims = {image.rows, image.cols, chans};\n        std::vector<int64_t> strides = {(int64_t)image.step1(), chans, 1};\n        auto options = torch::TensorOptions().dtype(isByte ? torch::kByte : torch::kFloat).device(torch::kCUDA, device);\n        auto tensorImage = torch::from_blob(image.data, dims, strides, deleter, options);\n\n        if (isByte)\n        {\n            tensorImage = tensorImage.to(torch::kFloat);\n        }\n\n        return tensorImage;\n    }```"
   },
   {
      "null": 193,
      "x": "The number of batches in epoch is affected by (num_workers + IterableDataset)",
      "z": "Thank you for confirming this behavior is by design.",
      "y": "Behaviour is by design"
   },
   {
      "null": 194,
      "x": "Upgrade pytorch to use XNNPACK instead of NNPACK for android",
      "z": "Hi Rudolf.  This is work in progress and should be done soon.  The bulk of the work is already done, but an efficient integration requires a bit of work on the PyTorch side to cache one-time computations, which is where my focus is currently.  I'll update this issue when I am done.  Cheers.",
      "y": "Bug is fixed"
   },
   {
      "null": 195,
      "x": "Problem installation via pip pytorch 1.3.0 and 1.2.0 on windows cuda 10.0",
      "z": "As for cuda 10.0 and pytorch 1.2.0, the version specifier is 1.2.0. And we don't build pytorch 1.3.x binaries with CUDA 10.0, so you should use the CUDA 10.1 ones instead, which has the version specifer of 1.3.0 and 1.3.1.",
      "y": "uda 10.0 and pytorch 1.2.0, the version specifier is 1.2.0. And we don't build pytorch 1.3.x binaries with CUDA 10.0"
   },
   {
      "null": 196,
      "x": "Add high level autograd functions",
      "z": "Hi,\n\nYes I saw this package. It is indeed very interesting but quite orthogonal to what is proposed here. They use the engine to compute quantities that the autograd engine cannot compute natively (using hooks mainly). This proposal would be using the vanilla autograd engine for all computations (only use gradients and gradients of gradients).",
      "y": "This proposal would be using the vanilla autograd engine for all computations (only use gradients and gradients of gradients)."
   },
   {
      "null": 197,
      "x": "Pytorch android Tensor.Shape() function not producing expected results.",
      "z": "Currently tensor.shape() simply returns a raw long[]: https://github.com/pytorch/pytorch/blob/master/android/pytorch_android/src/main/java/org/pytorch/Tensor.java#L351\n\nYou should be able to use \"System.out.println(**Arrays.toString**(tensor.shape()));\" to print its content correctly.\n\n@dreiss @IvanKobzarev - do you think it's worth creating a shape type?",
      "y": "tensor.shape() simply returns a raw long[]\nYou should be able to use \"System.out.println(**Arrays.toString**(tensor.shape()));\" to print its content correctly."
   },
   {
      "null": 198,
      "x": "How to convert Tensor back to BitMap or any image format in Android?",
      "z": "Hi I have the exact same issue. @ljk53 I have tried ur suggested answers but the Tensor.getRawDataBuffer() function is a private function therefore I can't use it. \nI can call getDataAsFloatArray() but I have no idea How to convert a float to a bitmap. \nWhen I try to call getDataAsBufferArray() the program will crash because my Tesnor is a float. ",
      "y": "This issue is fixed"
   },
   {
      "null": 199,
      "x": "AssertionError: Torch not compiled with CUDA enabled",
      "z": "I had the same problem (Win10, CUDA installed prior to making conda env)\n\nThe option to install using pip worked for me (inside a miniconda env, python 3.7.7)\n\nFrom the pytorch website:\n\n`pip install torch===1.5.0 torchvision===0.6.0 -f https://download.pytorch.org/whl/torch_stable.html`",
      "y": "option to install using pip worked for me (inside a miniconda env, python 3.7.7)\n\nFrom the pytorch website:\n\n`pip install torch===1.5.0 torchvision===0.6.0 -f https://download.pytorch.org/whl/torch_stable.html`"
   },
   {
      "null": 200,
      "x": "Make it an option to have a different mask for each sequence in a batch for the Transformer",
      "z": "Thanks for the proposal. Yes, it's all the way to `nn.MultiheadAttention`. We have a plan to refactor nn.MultiheadAttention by splitting the long function into several pieces. And we can make an option to accept a 3-D mask there.",
      "y": "it's all the way to `nn.MultiheadAttention`"
   },
   {
      "null": 201,
      "x": "cuda_tensor.norm(dim=(X, Y)) is broken",
      "z": "There's a simple workaround until the fix gets released. If you pass `p=2`, then it calculates it as you expect.\n\nNot sure why it works, when the default `p='fro'` doesn't.",
      "y": "pass `p=2`, then it calculates it as you expec"
   },
   {
      "null": 202,
      "x": "Distributed Package asynchronous send/receive not working as expected (Gloo)",
      "z": "Hi there! Thank you for the detailed report. You're right and this is a bug. We do properly set the completed flag for collectives but not for the send/recv operations. It's not that the operation hasn't completed yet (it has), but the `completed_` flag in `ProcessGroup::Work` isn't updated accordingly. Fixing this is a bit more complex than simply fixing the boolean, because Gloo's send/recv doesn't have a non-blocking way to check if the operation completed or not.\n\nTo fix this properly, we need to:\n1) Update Gloo to allow non-blocking check for send/recv completion.\n2) Update the bindings to call this function when `is_completed()` is called.",
      "y": "Bug is fixed\n1) Update Gloo to allow non-blocking check for send/recv completion.\n2) Update the bindings to call this function when `is_completed()` is called."
   },
   {
      "null": 203,
      "x": "conv1d fails in PyTorch 1.0",
      "z": "@gauss256 creating a new user + Anaconda + PyTorch solved it for me.\n\nThank you for the suggestion.\n\nI then compared the `LD_LIBRARY_PATH` for the two users and found that removing `/usr/local/lib` from the `LD_LIBRARY_PATH` for the first user (for whom this was not working) fixed the issue.\n\nThen I investigated the libraries at `/usr/local/lib` and found `libmkldnn.so.0`.\n\nI added back `/usr/local/lib` to `LD_LIBRARY_PATH` and the error appeared again.\n\nSince I suspected the error was related to mkldnn, I moved this file `libmkldnn.so.0` out of `/usr/local/lib` to another location.\n\nThen I tried running the code again and it worked (with `/usr/local/lib` included in  `LD_LIBRARY_PATH` but with the file `libmkldnn.so.0` removed from `/usr/local/lib`)\n\nSo this issue (at least for me) was caused by having a system wide mkldnn installation conflicting (and overriding) the conda mkldnn.\n\nI'm adding this comment to aid in debugging in case someone else faces a similar issue.",
      "y": "this issue is caused by having a system wide mkldnn installation conflicting (and overriding) the conda mkldnn."
   },
   {
      "null": 204,
      "x": "v1.0.0 nn.utils.weight_norm seems to nullify gradients of unrelated parameters if wrapped in DataParallel",
      "z": "@mrshenli Yes, we are still experiencing the issue with the 1.1.0 release too. FYI, [this repo provides a temporary workaround for `DataParallel`](https://github.com/r9y9/wavenet_vocoder/commit/6b9c932fdb2e53e406cfe460d700868891a9efb0), and [`DistributedDataParallel` does not suffer from this](https://github.com/ksw0306/FloWaveNet/pull/22).",
      "y": "Bug is fixed "
   },
   {
      "null": 205,
      "x": "[jit] CUDA fusion: a PTX JIT compilation failed",
      "z": "`PYTORCH_FUSION_DEBUG=1`  will have kernels printed for both.",
      "y": "the issue is fixed"
   },
   {
      "null": 206,
      "x": "Build command `python setup.py rebuild_libtorch` does not exist",
      "z": "Yes this has been deprecated. We've been moving setup.py commands into cmake itself. Cmake handles rebuilding only what's needed itself, though there are still some bugs around this related to ninja and nvcc. The only commands now are 'python setup.py build' or 'python setup.py develop'",
      "y": "this has been deprecated"
   },
   {
      "null": 207,
      "x": "better cmake checks on compiler version for libtorch binaries",
      "z": "@Maslino you have to use gcc >= 4.9.2 to compile. And if you use gcc >= 5.1, you have to set C++ flags: `-D_GLIBCXX_USE_CXX11_ABI=0`",
      "y": "use gcc >= 4.9.2 to compile. And if you use gcc >= 5.1, you have to set C++ flags: `-D_GLIBCXX_USE_CXX11_ABI=0`"
   },
   {
      "null": 208,
      "x": "PyTorch 1.0 fails to build with fbgemm enabled",
      "z": "cherry-picked onto v1.0.1. Closing now. Binaries will be out soon.",
      "y": "This issue is fixed"
   },
   {
      "null": 209,
      "x": "Blocking: Do modules wait?",
      "z": "every (or almost every) operation in CUDA in PyTorch is asynchronous, and thus non-blocking.\nCheck the `Asynchronous Execution` section in the docs https://pytorch.org/docs/stable/notes/cuda.html",
      "y": "every operation in CUDA in PyTorch is asynchronous, and thus non-blocking."
   },
   {
      "null": 210,
      "x": "[JIT] Trace->Script + Inplace causes shapes to be fixed where they should not",
      "z": "Indeed, it's working again now. Thank you again.",
      "y": "Bug is fixed now"
   },
   {
      "null": 211,
      "x": "error: unknown type name 'mkldnn_shuffle_desc_t'",
      "z": "Nevermind, so I solved it via \n```\ncd third_party/ideep # this is on branch \n grokmachine@Dendis-MacBook-Pro \ue0b0 | ~/dev/facebook/pytorch/third_party/ideep \ue0b0 \uf113  \uf126 remotes/origin/mkldnn_0.17 \ue0b0 git submodule update --init --recursive\n grokmachine@Dendis-MacBook-Pro \ue0b0 | ~/dev/facebook/pytorch/third_party/ideep \ue0b0 \uf113  \uf126 remotes/origin/mkldnn_0.17 \ue0b0 git checkout master\nM        mkl-dnn\nPrevious HEAD position was d06f361 Fix klocwork issues\nSwitched to branch 'master'\nYour branch is up to date with 'origin/master'.\n```\nthen I did a `MACOSX_DEPLOYMENT_TARGET=10.14 CC=cc MAX_JOBS=25 CXX=c++ python3 setup.py install` and it solved the problem",
      "y": "```\ncd third_party/ideep # this is on branch \n grokmachine@Dendis-MacBook-Pro \ue0b0 | ~/dev/facebook/pytorch/third_party/ideep \ue0b0 \uf113  \uf126 remotes/origin/mkldnn_0.17 \ue0b0 git submodule update --init --recursive\n grokmachine@Dendis-MacBook-Pro \ue0b0 | ~/dev/facebook/pytorch/third_party/ideep \ue0b0 \uf113  \uf126 remotes/origin/mkldnn_0.17 \ue0b0 git checkout master\nM        mkl-dnn\nPrevious HEAD position was d06f361 Fix klocwork issues\nSwitched to branch 'master'\nYour branch is up to date with 'origin/master'.\n```\nthen I did a `MACOSX_DEPLOYMENT_TARGET=10.14 CC=cc MAX_JOBS=25 CXX=c++ python3 setup.py install`"
   },
   {
      "null": 212,
      "x": "model.cuda() doesn't automatically detect layers initialized in a list",
      "z": "It's expected. Use `nn.ModuleList`.",
      "y": "Use `nn.ModuleList`"
   },
   {
      "null": 213,
      "x": "[JIT] undefined symbol: cuCtxGetCurrent",
      "z": "A hot fix for this is running `export LD_LIBRARY_PATH=LD_LIBRARY_PATH:/usr/local/cuda/lib64`. I can reproduce this locally with a CUDA 10 setup",
      "y": "fix for this is `export LD_LIBRARY_PATH=LD_LIBRARY_PATH:/usr/local/cuda/lib64`."
   },
   {
      "null": 214,
      "x": "[distributed] NCCL dist.barrier doesn't respect default device",
      "z": "Meanwhile, this workaround I wrote still works correctly:\n```\ndef barrier():\n    t = torch.randn((), device='cuda')\n    dist.all_reduce(t)\n    torch.cuda.synchronize()\n\n```",
      "y": "A workaround \n```\ndef barrier():\n    t = torch.randn((), device='cuda')\n    dist.all_reduce(t)\n    torch.cuda.synchronize()\n\n```"
   },
   {
      "null": 215,
      "x": "Dilated conv in v1.0.0 is too slow.",
      "z": "This problem comes from CuDNN.\n\nCuDNN has multiple algorithms to do convolutions, that they can choose from. They have a heuristic function that picks a particular algorithm on a particular GPU, for specific sizes of convolution / dilation etc.\n\nIn CuDNN 7.4, which is the latest version, and which is what PyTorch ships with, it looks like they introduced performance regression in this version compared to CuDNN 7.1 (which PyTorch used before).\n\n`torch.backends.cudnn.benchmark=True` tells PyTorch / CuDNN to try every algorithm for a given {input size, kernel size, kernel stride, dilation, padding}, and pick the fastest.\n\nThe reason it fixes the problem for @knazeri is because they must be doing something like image classification, where the `input size` does not change across iterations. So, CuDNN only searches all algorithms (for the fastest algorithm) once in training -- when it encounters `input size` for the first time.\n\nThe reason it makes @zimenglan-sysu-512 's problem much worse is because they must be doing object detection or something similar, where images of different `input size` are passed in at every iteration, so CuDNN is searching all algorithms every iteration (because `input size` changes at every iteration), and searching all the list of algorithms takes 10x longer.",
      "y": "CuDNN is searching all algorithms every iteration (because `input size` changes at every iteration), and searching all the list of algorithms takes 10x longer."
   },
   {
      "null": 216,
      "x": "libtorch hardcoded libculibos.a causes build errors for C++ programs on Arch Linux",
      "z": "Okay I see the problem. I'll look into a fix",
      "y": "This has been fixed"
   },
   {
      "null": 217,
      "x": "Exporting model to onnx increases the model size",
      "z": "> I solved it as mentioned [here](https://github.com/onnx/onnx/issues/3278#issuecomment-781948998).\n> For anyone who stumbles upon such issue, I've written a [notebook here](https://github.com/thehetpandya/onnx-shared-weights-remove/blob/main/onnx_remove_shared_weights.ipynb) that might help you.\n\nReally thanks a lot",
      "y": "This is fixed"
   },
   {
      "null": 218,
      "x": "Pytorch DataLoader freezes when num_workers > 0 in jupyter notebook(windows 10)",
      "z": "@mobassir94 \nI am not an expert for notebook, but I suggest to try to put your code into a separate file and import it to your script, then call it within `if __name__ == '__main__'`.\n\n> when i tried it on syder ide,it worked there with number of workers > 0 but it gradually increase memory usage and give OOM after few epochs,,even if i set 2 workers only,it will give me OOM and consume all memory,how to fix this? if i use 0 workers then everything works fine but training becomes painfully slow\n\nCan you share a minimum script for me to reproduce the OOM? It's too hard to read through your code. I would suggest to delete data loader and create a new one after each epoch. As I am not sure how you implement your dataset, it's possible that some reference on the data within the computation graph prevents PyTorch cleaning the cache.",
      "y": "try to put your code into a separate file and import it to your script, then call it within `if __name__ == '__main__'`."
   },
   {
      "null": 219,
      "x": "torch.script crashes with Union type",
      "z": "This crash is fixed on master. As @ezyang, someone is working on adding Union support.",
      "y": null
   },
   {
      "null": 220,
      "x": "autograd.grad with set_detect_anomaly(True) will cause memory leak",
      "z": "@albanD I think your suspicions are correct. I've created a PR that /should/ fix this. At the very least we know its python related bc the following doesn't leak:\n\n```\n#include <c10/cuda/CUDACachingAllocator.h>\n\ntorch::autograd::DetectAnomalyGuard detect_anomaly;\nfor(int i = 0; i < 10; i++) {\n  auto x = torch::ones({10, 30000}).cuda().requires_grad_();\n  auto y = x.exp();\n  auto grad = torch::autograd::grad({y}, {x}, {torch::ones_like(y)}, true, true);\n\n  auto stats = c10::cuda::CUDACachingAllocator::getDeviceStats(0);\n  std::cout << \"allocated: \" << stats.allocated_bytes[0].current / (1024 * 1024) << std::endl;\n}\n\n```",
      "y": "``` #include <c10/cuda/CUDACachingAllocator.h> torch::autograd::DetectAnomalyGuard detect_anomaly; for(int i = 0; i < 10; i++) { auto x = torch::ones({10, 30000}).cuda().requires_grad_(); auto y = x.exp(); auto grad = torch::autograd::grad({y}, {x}, {torch::ones_like(y)}, true, true); auto stats = c10::cuda::CUDACachingAllocator::getDeviceStats(0); std::cout << \"allocated: \" << stats.allocated_bytes[0].current / (1024 * 1024) << std::endl; } ```"
   },
   {
      "null": 221,
      "x": "Perf regression for YoloV3 CPU eager eval",
      "z": "The yolov3 cpu train result regression has been fixed after #52909 is merged.",
      "y": "Issue is fixed"
   },
   {
      "null": 222,
      "x": "Perf regression for background matting CUDA train",
      "z": "ah, great! so iiuc you have shown that the dataloader code got slower due to respecting the num_threads, and this is expected behavior, so the new (slow) speed is indeed correct.  thanks for tracking this down.  ",
      "y": "dataloader code got slower due to respecting the num_threads, and this is expected behavior, so the new (slow) speed is indeed correct."
   },
   {
      "null": 223,
      "x": "DataLoader for Video Loading that utilizes the GPU",
      "z": "That's correct, and don't forget to use `pin_memory`.\nYou can also use `default_collate_fn` to transform batch into tensor, since there are some optimizations.",
      "y": "use `pin_memory`.\nYou can also use `default_collate_fn` to transform batch into tensor"
   },
   {
      "null": 224,
      "x": "[docs] Improve documentation for LayerNorm",
      "z": "This is also importing for porting TF code in PyTorch. E.g. LayerNormalization https://www.tensorflow.org/api_docs/python/tf/keras/layers/LayerNormalization only requires to specify the dimensions, not their shapes.\n\nAlso, it says: \"Given a tensor inputs, moments are calculated and normalization is performed across the axes specified in axis.\". (axis by default is set to -1). Is TF's instance of `LayerNormalization()` with default parameters expressable as PyTorch LayerNorm?\n\nIn TF it's also not clear whether it does flattens/aggregates only specified dimensions, all except them, or all dimensions except batch dim.",
      "y": "Documentation is updated"
   },
   {
      "null": 225,
      "x": null,
      "z": null,
      "y": null
   },
   {
      "null": 226,
      "x": "CUDA error: an illegal memory access was encountered: on RTX3090 (using multiple GPUs)",
      "z": "I was running into a similar problem with PyTorch's language modeling scripts (e.g. run_clm.py), and I was able to fix it by disabling \"Hardware-accelerated GPU scheduling\" in Windows settings.",
      "y": "disable \"Hardware-accelerated GPU scheduling\" in Windows settings"
   },
   {
      "null": 227,
      "x": "Nans in matrix multiplication on ARM",
      "z": "@mdouze can you please try https://download.pytorch.org/whl/nightly/cpu/torch-1.8.0.dev20201210-cp38-cp38-linux_aarch64.whl ?\nAnd I guess it's due to the bug in implementation of missing NEON intrinsics, that was fixed by  https://github.com/pytorch/pytorch/pull/50389",
      "y": "it's due to the bug in implementation of missing NEON intrinsics"
   },
   {
      "null": 228,
      "x": "If a module passed to DistributedDataParallel has no parameter required gradient, expect_sparse_gradient[0] in _ddp_init_helper function will raise error.",
      "z": "Emm, to compute perceptron loss such as fixed VGG-19.",
      "y": "bug is fixed"
   },
   {
      "null": 229,
      "x": "Wrong crossentropy loss calculation",
      "z": "Since you passed the weights, the mean should be a weighted mean.\n\nLet's say...\n`a = (-np.log(np.exp(0.5) / \u2211exp(x[j])))`\n`b = (-np.log(np.exp(0.2) / \u2211exp(x[j])))`\n\nThen...\n**sum**\n`6*a + 4*b`\n**mean**\n`(6*a + 4*b)/(6+4)`",
      "y": "you passed the weights, the mean should be a weighted mean.\n\nLet's say...\n`a = (-np.log(np.exp(0.5) / \u2211exp(x[j])))`\n`b = (-np.log(np.exp(0.2) / \u2211exp(x[j])))`\n\nThen...\n**sum**\n`6*a + 4*b`\n**mean**\n`(6*a + 4*b)/(6+4)`"
   },
   {
      "null": 230,
      "x": "assignment bug of advanced indexing",
      "z": "Already fixed in master and throws this as expected:\n`RuntimeError: unsupported operation: some elements of the input tensor and the written-to tensor refer to a single memory location. Please clone() the tensor before performing the operation`",
      "y": "This is expected behaviour"
   },
   {
      "null": 231,
      "x": "Adding Mish Activation Function",
      "z": "We don't think this method should be in pytorch core, as opposed to your own personal repository or something like https://github.com/pytorch/contrib , at least not yet in time.\n\nOur reservation is that we want to include methods that the community uses as a standard, or else the code maintenance problem balloons up for us.\nWe do show discretion based on what the paper shows as evidence, for example BatchNorm was included within weeks of it's publish date (in Torch).\nIn terms of rejected methods, we've rejected (then) newly minted papers such as Swish ( #3260 , #3182 ), Yellowfin ( https://github.com/pytorch/pytorch/issues/1960 ) and many others, and rightly so, these haven't become standardized in the community (like LSTM / Transformer / BatchNorm).\n\nIf you have a differing opinion, let us know why, and we can re-think.\n\ntl;dr: The paper doesn't show evidence that makes it a method that has obvious long-term success. If the paper does have long-term success in the field we will include it",
      "y": "want to include methods that the community uses as a standard"
   },
   {
      "null": 232,
      "x": "automatic transition to 'cuda' if available",
      "z": "The system works as designed, we don't move tensors between devices implicitly. It's a breaking compatibility changes so we are not going to do this.",
      "y": "It's a breaking compatibility changes so we are not going to do this."
   },
   {
      "null": 233,
      "x": "Replace \"NVIDIA driver\" with \"CUDA Toolkit\" in _check_driver() error messages",
      "z": "it doesn't return the CUDA version. It returns \"Returns the latest version of CUDA supported by the driver.\".\n\nIf you have CUDA10 installed, but driver that only supports CUDA9, it will return `9000`",
      "y": "It returns \"Returns the latest version of CUDA supported by the driver.\".\n\nIf you have CUDA10 installed, but driver that only supports CUDA9, it will return `9000`"
   },
   {
      "null": 234,
      "x": "Reference cycle in _LRScheduler",
      "z": "@ezyang Sure, I can do that. But I'm wondering what would be the ideal implementation? Comments in #20124 suggest that the current code for detecting calls to `optim.step` may be missing a few cases.",
      "y": "calls to `optim.step` may be missing a few cases"
   },
   {
      "null": 235,
      "x": "Move QNNPACK micro kernels under ATen",
      "z": "@AshkanAliabadi , seems like we went with the fork.  Should we keep this open or ok to close?",
      "y": "This issue has been fixed"
   },
   {
      "null": 236,
      "x": "FasterRCNN and MaskRCNN doesn't work with DataParallel or DistributedDataParallel",
      "z": "We don't need `args.local_rank` because of the `use_env` argument from launch that I pasted just before.\nSame thing for the comment on device",
      "y": "We don't need `args.local_rank` because of the `use_env` argument from launch"
   },
   {
      "null": 237,
      "x": "Unable to  subscirpt self-defined class in torchscript function",
      "z": "Thanks for the report! @eellison, can you look? I thought we got magic methods for class types wired up",
      "y": "BUg is fixed now"
   },
   {
      "null": 238,
      "x": "Implement pep 503 Simple Repository API for deployment",
      "z": "bumping priority based on user activity",
      "y": "Feature is added"
   },
   {
      "null": 239,
      "x": "Number of prefetch in DataLoader",
      "z": "TBH make the prefetch size tied to the number of workers is very inconvenient, especially in this hardcoded way.\nFor instance, while training something video-related, I might need both RGB and optical flow across multiple frames as the network input. The I/O overhead is huge and I'd like to use multiple workers to hide it.\nBut, since the prefetch size is directly tied into the number of workers, and it happened that I only have 32 or 64GB of RAM, I can only use like 20 workers and that cannot hide the overhead completely (I'm using ramdisk), otherwise, I got the OOM error.",
      "y": "TBH make the prefetch size tied to the number of workers is very inconvenient, especially in this hardcoded way."
   },
   {
      "null": 240,
      "x": "\"Using PyTorch C++ Frontend\" TorchModule generator architecture is erroneous",
      "z": "@jlin27 Would you like to look at @BynaryCobweb's suggestion? Thanks!",
      "y": "This issue is fixed"
   },
   {
      "null": 241,
      "x": "Missing pip install wheels from the PyTorch official website",
      "z": "You can use command line to download the wheels:\n\nto see the content of the URL\n`$ curl https://download.pytorch.org/whl/cu90/torch_stable.html`\n\ne.g. download torch-1.1.0-cp36-cp36m-linux_x86_64.whl \n`$ wget https://download.pytorch.org/whl/cu90/torch-1.1.0-cp36-cp36m-linux_x86_64.whl`\n\nSee relative issue #25448 .",
      "y": "use command line to download the wheels:\n\nto see the content of the URL\n`$ curl https://download.pytorch.org/whl/cu90/torch_stable.html`\n\ne.g. download torch-1.1.0-cp36-cp36m-linux_x86_64.whl \n`$ wget https://download.pytorch.org/whl/cu90/torch-1.1.0-cp36-cp36m-linux_x86_64.whl`\n\nSee relative issue #25448 ."
   },
   {
      "null": 242,
      "x": "Dropout behaves differently on different devices",
      "z": "we dont guarantee that the RNG outputs the same sequence across different GPU models.\n\nThe guarantees of RNG are in https://pytorch.org/docs/stable/notes/randomness.html?highlight=deterministic",
      "y": "we dont guarantee that the RNG outputs the same sequence across different GPU models."
   },
   {
      "null": 243,
      "x": "No type hints on nn.Identity",
      "z": "@eduidl Thank you. Do you mind opening a PR with that diff? Tag me as reviewer.",
      "y": "This issue is fixed"
   },
   {
      "null": 244,
      "x": "PyTorch C++ API as a static lib: how to compile ?",
      "z": "libtorch has download links on pytorch.org that look like this:\n\n```\nhttps://download.pytorch.org/libtorch/cu100/libtorch-cxx11-abi-shared-with-deps-1.2.0.zip\n```\n\nYou can replaced \"shared\" with \"static\" and get URLs like this:\n\n```\nhttps://download.pytorch.org/libtorch/cu100/libtorch-cxx11-abi-static-with-deps-1.2.0.zip\n```\n\nThey exist, we generate them. I just dont remember if they work properly, but they do pass some smoke tests.",
      "y": "libtorch has download links on pytorch.org that look like this:\n\n```\nhttps://download.pytorch.org/libtorch/cu100/libtorch-cxx11-abi-shared-with-deps-1.2.0.zip\n```\n\nYou can replaced \"shared\" with \"static\" and get URLs like this:\n\n```\nhttps://download.pytorch.org/libtorch/cu100/libtorch-cxx11-abi-static-with-deps-1.2.0.zip\n```"
   },
   {
      "null": 245,
      "x": "toDense is misdocumented as a method on sparse tensors",
      "z": "@Nikronic new issue please! thanks!",
      "y": "This issue is fixed"
   },
   {
      "null": 246,
      "x": "backward_hook triggered despite RemovableHandle.remove()",
      "z": "Thanks, indeed this seems to do the trick\n\n```\nimport torch\nimport torch.nn as nn\n\n\ndef tensor_hook_adder(module, input, output):\n    def tensor_backwards(grad):\n        setattr(module, 'backprops', grad)\n    output.register_hook(tensor_backwards)\n\nlayer = nn.Linear(2, 2, bias=False)\nlayer.weight.data.copy_(2*torch.eye(2))\nmodel = layer\nlayer.register_forward_hook(tensor_hook_adder)\n\noutput = model(torch.tensor([1.,2]))\nloss = output[0]**2+2*output[1]**2\nloss.backward()\nassert torch.allclose(layer.backprops, torch.tensor([4, 16.]))\n```",
      "y": "Use\n```\nimport torch\nimport torch.nn as nn\n\n\ndef tensor_hook_adder(module, input, output):\n    def tensor_backwards(grad):\n        setattr(module, 'backprops', grad)\n    output.register_hook(tensor_backwards)\n\nlayer = nn.Linear(2, 2, bias=False)\nlayer.weight.data.copy_(2*torch.eye(2))\nmodel = layer\nlayer.register_forward_hook(tensor_hook_adder)\n\noutput = model(torch.tensor([1.,2]))\nloss = output[0]**2+2*output[1]**2\nloss.backward()\nassert torch.allclose(layer.backprops, torch.tensor([4, 16.]))\n```"
   },
   {
      "null": 247,
      "x": "[NGC Container]Runtime Error - Nvidia Nsight System",
      "z": "You are right. I just tested a few containers and it seems to be missing in `19.08`.\nLet me ask our build team, what happened.",
      "y": "The issue is fixed"
   },
   {
      "null": 248,
      "x": "[InstanceNorm] Unexpected behaviour with track_running_stats set to True in evaluation mode",
      "z": "This sounds like a silent correctness issue so I marked it as high-priority but someone with more context behind the normalization layers should take a look at this.",
      "y": "Bug is fixed"
   },
   {
      "null": 249,
      "x": "[Android] Latest nightly build causes error: library \"libpytorch_jni.so\" not found",
      "z": "We are transitioning to use lite interpreter for Android, targeting smaller library size. Currently the new nightly build is lite interpreter. The new library name is `libpytorch_jni_lite.so`. \n\nTo use lite interpreter, the model can be generated following the example:\n```\nimport torch\n\nmodel = torch.hub.load('pytorch/vision:v0.7.0', 'deeplabv3_resnet50', pretrained=True)\nmodel.eval()\n\nscripted_module = torch.jit.script(model)\n# Export full jit version model (not compatible lite interpreter), leave it here for comparison\nscripted_module.save(\"deeplabv3_scripted.pt\")\n# Export lite interpreter version model (compatible with lite interpreter)\nscripted_module._save_for_lite_interpreter(\"deeplabv3_scripted.ptl\")\n```\n\nTo load the model:\n```\nimport org.pytorch.LiteModuleLoader\n...\nmModule = LiteModuleLoader.load(MainActivity.assetFilePath(getApplicationContext(), \"deeplabv3_scripted.ptl\"));\n...\n```\n\nThis tutorial will be updated soon, but it can still be used as a reference now: https://pytorch.org/tutorials/prototype/lite_interpreter.html",
      "y": "To use lite interpreter, the model can be generated following the example:\n```\nimport torch\n\nmodel = torch.hub.load('pytorch/vision:v0.7.0', 'deeplabv3_resnet50', pretrained=True)\nmodel.eval()\n\nscripted_module = torch.jit.script(model)\n# Export full jit version model (not compatible lite interpreter), leave it here for comparison\nscripted_module.save(\"deeplabv3_scripted.pt\")\n# Export lite interpreter version model (compatible with lite interpreter)\nscripted_module._save_for_lite_interpreter(\"deeplabv3_scripted.ptl\")\n```\n\nTo load the model:\n```\nimport org.pytorch.LiteModuleLoader\n...\nmModule = LiteModuleLoader.load(MainActivity.assetFilePath(getApplicationContext(), \"deeplabv3_scripted.ptl\"));\n..."
   },
   {
      "null": 250,
      "x": "NcclErrorHandlingTest.test_nccl_errors_blocking_abort frequently fails",
      "z": "@malfet Closing this out since I think this is a duplicate of https://github.com/pytorch/pytorch/issues/58856, feel free to reopen if this is actually a separate issue.",
      "y": "The issue is already fixed"
   },
   {
      "null": 251,
      "x": "Enabling AVX512 vectorization for `qadaptive_avg_pool2d_nhwc_kernel` & `qavg_pool2d_nhwc_kernel`",
      "z": "It looks like v1.9.0 is almost finalized & only low risk PRs would be accepted until tomorrow. As such #56992 is unlikely to be included in v1.9.0. So I've more time than I initially thought & will try to dig into the avg pool source code myself. Thank you!",
      "y": "This issue is fixed"
   },
   {
      "null": 252,
      "x": "Running some CI tests with `xlarge` `resource_class` VMs for testing AVX512 support",
      "z": "cc @seemethere but my guess is this is not worth resolving until GitHub Actions migrations is complete.",
      "y": "The feature has been added"
   },
   {
      "null": 253,
      "x": "I got the error \"couldn't find \"libpytorch_jni.so\"\" when using pytorch Android",
      "z": "Thank you for your answer, the problem has been solved.",
      "y": "To use lite interpreter, the model can be generated following the example:\n```\nimport torch\n\nmodel = torch.hub.load('pytorch/vision:v0.7.0', 'deeplabv3_resnet50', pretrained=True)\nmodel.eval()\n\nscripted_module = torch.jit.script(model)\n# Export full jit version model (not compatible lite interpreter), leave it here for comparison\nscripted_module.save(\"deeplabv3_scripted.pt\")\n# Export lite interpreter version model (compatible with lite interpreter)\nscripted_module._save_for_lite_interpreter(\"deeplabv3_scripted.ptl\")\n```\n\nTo load the model:\n```\nimport org.pytorch.LiteModuleLoader\n...\nmModule = LiteModuleLoader.load(MainActivity.assetFilePath(getApplicationContext(), \"deeplabv3_scripted.ptl\"));\n..."
   },
   {
      "null": 254,
      "x": "Circular padding in Conv2d applies padding across the wrong dimension (regression from 1.4)",
      "z": "could be my fault... I'll take a look.",
      "y": "The issue is fixed"
   },
   {
      "null": 255,
      "x": "Let future expose a then() API",
      "z": "Does it make sense to create both `add_done_callback(cb) -> None` and `then(cb) -> Future`? \n(Given that the former is lighter weight?)\n\n(Next, somebody will ask for `via(executor)` and `thenError()` :) )",
      "y": "Feature is added"
   },
   {
      "null": 256,
      "x": "convert_sync_batchnorm should respect device affinity",
      "z": "To add some context, we're seeing this issue come up for DDP users where the typical flow is something like:\n```\nmodel.to(cuda_idx)\nmodel = torch.nn.SyncBatchNorm.convert_sync_batchnorm(model)\nmodel = DDP(model, device_ids=[cuda_idx])\n```\n\nThe error that's raised by DDP here indicates that sync BN ended up placing some parameters back on the CPU, even though the model was moved to cuda above. The current workaround is to move to cuda after converting to sync BN, but I don't think the order should have to matter.",
      "y": "workaround is to move to cuda after converting to sync BN"
   },
   {
      "null": 257,
      "x": "TensorPipe dependency breaks build for non-X86; it expects x86intrin.h",
      "z": "Thanks for flagging this! It looks like it's a useless include which we forgot to remove. I have https://github.com/pytorch/tensorpipe/pull/164 ready to address that.",
      "y": "This issue is fixed"
   },
   {
      "null": 258,
      "x": "hardsigmoid cuda_dispatch_ptr INTERNAL ASSERT FAILED",
      "z": "closing this, since the milestone label is there we won't forget it if we do a 1.5.1.",
      "y": "Bug is fixed"
   },
   {
      "null": 259,
      "x": "DISABLED test_profiler_with_sync_rpc_udf (__main__.RpcTestWithSpawn)",
      "z": "This is likely caused by the same problem as #37557. @rohan-varma and I are working on a fix.",
      "y": "This has been fixed"
   },
   {
      "null": 260,
      "x": "Build documentation without emitting warnings",
      "z": "After gh-41068 the only warnings are emitted by [quantization](https://pytorch.org/docs/master/quantization.html) which needs some curation. In particular, it has these sections that I think should be separate pages:\n```\ntorch.nn.intrinsic\ntorch.nn.instrinsic.qat\ntorch.nn.intrinsic.quantized\ntorch.nn.qat\ntorch.nn.quantized\ntorch.nn.quantized.dynamic\n```\n\nThe remaining warnings are due to repeating names of some of the classes in those modules. For instance, ConvBn2d appears in both [torch.nn.intrinsic](https://pytorch.org/docs/master/quantization.html#id2) and [torch.nn.intrinsic-qat](https://pytorch.org/docs/master/quantization.html#torch-nn-instrinsic-qat)",
      "y": "only warnings are emitted by [quantization](https://pytorch.org/docs/master/quantization.html) which needs some curation. In particular, it has these sections that I think should be separate pages:\n```\ntorch.nn.intrinsic\ntorch.nn.instrinsic.qat\ntorch.nn.intrinsic.quantized\ntorch.nn.qat\ntorch.nn.quantized\ntorch.nn.quantized.dynamic\n```\n\nThe remaining warnings are due to repeating names of some of the classes in those modules."
   },
   {
      "null": 261,
      "x": "CUDA debug build failed on Windows",
      "z": "@malfet It should work too. And that's my plan for fixing it in the short term.",
      "y": "This has been fixed"
   },
   {
      "null": 262,
      "x": "DistributedDataSampler converts NamedTuple to regular tuple",
      "z": "cc @mrshenli",
      "y": "Bug is fixed"
   },
   {
      "null": 263,
      "x": "quantization.test_quantize.TestGraphModePostTrainingStatic fails in tsan",
      "z": "I think there are known issues with QNNPACK tests when ASAN/UBSAN is enabled. This test was recently added to run on qnnpack backend, I guess it is failing since then. I've disabled QNNPACK tests when TSAN is enabled as well now. \ncc @kimishpatel for visibility.",
      "y": "issues with QNNPACK tests when ASAN/UBSAN is enabled. This is fixed now"
   },
   {
      "null": 264,
      "x": "[Feature] Elementwise operator complex_tensor.normalize()",
      "z": "Hey @zasdfgbnm great point! We [discussed](https://github.com/pytorch/pytorch/issues/36323#issuecomment-613569847) to add this feature as `torch.sgn`",
      "y": "this feature is added as `torch.sgn`"
   },
   {
      "null": 265,
      "x": "Quantization: FakeQuant and Observers should sync enabled flags with DDP",
      "z": "https://github.com/pytorch/pytorch/blob/8d6a8d2b3fd2a6ec788378843fc518824acf274b/torch/nn/parallel/distributed.py#L147 has some more context.\n\nYep, the quantization methods should be created before DDP.  This issue is just to track that currently some of the quantization flags are not buffers, so they will not be replicated properly if wrapped with DDP and then changed in the middle of training.",
      "y": "the quantization methods should be created before DDP"
   },
   {
      "null": 266,
      "x": "pytorch-mobile memory leak or emptyCache api",
      "z": "**module code**\n```\nimport torch\n\nprint(torch.__version__)\n\n\n# simple module\n\nclass SimpleModule(torch.nn.Module):\n    def __init__(self):\n        super(SimpleModule, self).__init__()\n        self.conv = torch.nn.Sequential(\n            torch.nn.Conv2d(1, 32, 3, 2),\n            torch.nn.ReLU(),\n            torch.nn.Conv2d(32, 32, 3, 2),\n            torch.nn.ReLU()\n        )\n        \n    def forward(self, chunk):\n        chunk = torch.as_tensor(chunk).unsqueeze(0)\n        y = chunk.unsqueeze(1)  \n        y = self.conv(y)\n        b, c, t, f = y.size()\n        y.transpose(1, 2).contiguous().view(b, t, c * f)\n        return y\n    \n\nsimple_cell = SimpleModule()\n\nchunk = torch.randn(100, 80)\nprint(simple_cell(chunk))\n\nsimple_torchscript = torch.jit.script(simple_cell)\nsimple_torchscript.save(\"simple.pt\")\n```\n\n**C++ sample code\uff1a**\n```\n#include <iostream>\n#include \"torch/csrc/api/include/torch/torch.h\"\n#include \"torch/csrc/api/include/torch/utils.h\"\n#include <caffe2/utils/threadpool/ThreadPool.h>\n#include <caffe2/utils/threadpool/ThreadPoolMobile.h>\n#include \"torch/script.h\"\n#include <utility>\n#include <unistd.h>\n#include <vector>\n#include <pthread.h>\n\nusing std::cout;\nusing std::endl;\ntorch::jit::Module module_;\nstruct JITCallGuard {\n  // AutoGrad is disabled for mobile by default.\n  torch::autograd::AutoGradMode no_autograd_guard{false};\n  // VariableType dispatch is not included in default mobile build. We need set\n  // this guard globally to avoid dispatch error (only for dynamic dispatch).\n  // Thanks to the unification of Variable class and Tensor class it's no longer\n  // required to toggle the NonVariableTypeMode per op - so it doesn't hurt to\n  // always set NonVariableTypeMode for inference only use case.\n  torch::AutoNonVariableTypeMode non_var_guard{true};\n  // Disable graph optimizer to ensure list of unused ops are not changed for\n  // custom mobile build.\n  torch::jit::GraphOptimizerEnabledGuard no_optimizer_guard{false};\n};\n\nvoid *autoTest(void *args) {\n  printf(\"start load \\n\");\n  try {\n    JITCallGuard guard;\n    module_ = torch::jit::load(std::move(\"./simple.pt\"));\n  } catch (const c10::Error &e) {\n    printf(\"load module resource failed.\");\n    return 0;\n  }\n  module_.eval();\n  printf(\"load over \\n\");\n  usleep(1000 * 1000);\n\n  // run forward\n  caffe2::mobile_threadpool()->setNumThreads(1);\n  torch::jit::getProfilingMode() = false;\n  torch::jit::getExecutorMode() = false;\n  torch::jit::setGraphExecutorOptimize(false);\n  printf(\"start recycle \\n\");\n  int count = 3;\n  while (count >= 0) {\n    float *data = new float[8000];\n    torch::Tensor datas = torch::from_blob(data, {100, 80});\n    std::vector<torch::jit::IValue> inputs{};\n    inputs.emplace_back(datas);\n    JITCallGuard guard;\n    module_.forward(inputs);\n    delete[] data;\n    usleep(500 * 1000);\n    printf(\"current count is %d .\\n\", count);\n    count--;\n  }\n  pthread_exit(NULL);\n}\n\n\nint main() {\n  cout << \"Test TorchScript Memory Leak pytorch 1.5.0\" << endl;\n  for (int i = 0; i < 200; i++) {\n    cout << \"times is \" << i << endl;\n    pthread_t tid;\n    pthread_attr_t attr;\n    void *status;\n    pthread_attr_init(&attr);\n\n    pthread_attr_setdetachstate(&attr, PTHREAD_CREATE_JOINABLE);\n    int ret = pthread_create(&tid, &attr, autoTest, NULL);\n    if (ret != 0) {\n      cout << \"pthread_create error: error_code=\" << ret << endl;\n      return 0;\n    }\n    pthread_attr_destroy(&attr);\n    int rc = pthread_join(tid, &status);\n    if (rc) {\n      cout << \"Error:unable to join,\" << rc << endl;\n      exit(-1);\n    }\n    cout << \"Main: completed thread id : \"\n            \"exiting with status :\" << status << endl;\n\n  }\n  return 0;\n}\n```\n\nget memory info by\n```\npid=`ps -A | grep Mem| awk '{print $2}'`;while [ 1 ];do cat /proc/${pid}/status | grep VmRSS;sleep 3;done\n```\nCMakeList\n```\ncmake_minimum_required(VERSION 3.6)\n\nset(PLATFORM_ANDROID TRUE)\nset(ANDROID_STL c++_static)\nset(CMAKE_VERBOSE_MAKEFILE ON)\n# NDK r20b\nset(ANDROID_NDK \"path/to/your/ndk\")\nset(CMAKE_TOOLCHAIN_FILE \"${ANDROID_NDK}/build/cmake/android.toolchain.cmake\")\nset(ANDROID_ABI arm64-v8a)\nset(ANDROID_NATIVE_API_LEVEL android-28)\n\n\nproject(MemoryLeak)\n\nset(CMAKE_BUILD_TYPE Release)\n\nif (CMAKE_BUILD_TYPE MATCHES \"Debug\" OR CMAKE_BUILD_TYPE MATCHES \"None\")\n    message(STATUS \"CMAKE_BUILD_TYPE is Debug\")\n    set(CMAKE_CXX_FLAGS \"${CMAKE_CXX_FLAGS} -fPIC -O0 -Wall -g -ggdb\")\n    set(CMAKE_C_FLAGS \"${CMAKE_C_FLAGS} -fPIC -O0 -Wall -g -ggdb\")\nelseif (CMAKE_BUILD_TYPE MATCHES \"Release\")\n    message(STATUS \"CMAKE_BUILD_TYPE is Release\")\n    set(CMAKE_CXX_FLAGS \"${CMAKE_CXX_FLAGS} -std=c++14 -O3 -fPIC -fpermissive\")\n    set(CMAKE_C_FLAGS \"${CMAKE_C_FLAGS} -fPIC -O3\")\n    set(CMAKE_EXE_LINKER_FLAGS \"${CMAKE_EXE_LINKER_FLAGS} -Wl,-s\")\nelseif (CMAKE_BUILD_TYPE MATCHES \"RelWitchDebInfo\")\n    message(STATUS \"CMAKE_BUILD_TYPE is RelWitchDebInfo\")\nelseif (CMAKE_BUILD_TYPE MATCHES \"MinSizeRel\")\n    message(STATUS \"CMAKE_BUILD_TYPE is MinSizeRel\")\n    set(CMAKE_CXX_FLAGS \"${CMAKE_CXX_FLAGS} -std=c++14 -O3 -fPIC -fpermissive\")\n    set(CMAKE_C_FLAGS \"${CMAKE_C_FLAGS} -fPIC -O3\")\nelse ()\n    message(STATUS \"unknown CMAKE_BUILD_TYPE = \" ${CMAKE_BUILD_TYPE})\n    set(CMAKE_CXX_FLAGS \"${CMAKE_CXX_FLAGS} -std=c++14 -fPIC -O0 -Wall -g -ggdb\")\n    set(CMAKE_C_FLAGS \"${CMAKE_C_FLAGS} -fPIC -O0 -Wall -g -ggdb\")\nENDif ()\n# set STL C++ 14\nset(CMAKE_CXX_STANDARD 14)\n\ninclude_directories(\n        ${PROJECT_SOURCE_DIR}/pytorch/include/\n        ${PROJECT_SOURCE_DIR}/pytorch/include/torch/csrc/api/include\n)\n\nlink_directories(\n        ${PROJECT_SOURCE_DIR}/pytorch/lib/\n)\n\n\nadd_executable(MemoryLeak main.cpp)\ntarget_link_libraries(\n        MemoryLeak\n        -Wl,--gc-sections\n        -Wl,--whole-archive\n        torch.a\n        torch_cpu.a\n        -Wl,--no-whole-archive\n        c10.a\n        nnpack.a\n        XNNPACK.a\n        pytorch_qnnpack.a\n        eigen_blas.a\n        cpuinfo.a\n        clog.a\n        log\n        m\n        z\n)\n```\nI found use pthread will come out memory leak.use main function will not, NOTE: in android environment",
      "y": "**module code**\n```\nimport torch\n\nprint(torch.__version__)\n\n\n# simple module\n\nclass SimpleModule(torch.nn.Module):\n    def __init__(self):\n        super(SimpleModule, self).__init__()\n        self.conv = torch.nn.Sequential(\n            torch.nn.Conv2d(1, 32, 3, 2),\n            torch.nn.ReLU(),\n            torch.nn.Conv2d(32, 32, 3, 2),\n            torch.nn.ReLU()\n        )\n        \n    def forward(self, chunk):\n        chunk = torch.as_tensor(chunk).unsqueeze(0)\n        y = chunk.unsqueeze(1)  \n        y = self.conv(y)\n        b, c, t, f = y.size()\n        y.transpose(1, 2).contiguous().view(b, t, c * f)\n        return y\n    \n\nsimple_cell = SimpleModule()\n\nchunk = torch.randn(100, 80)\nprint(simple_cell(chunk))\n\nsimple_torchscript = torch.jit.script(simple_cell)\nsimple_torchscript.save(\"simple.pt\")\n```\n\n**C++ sample code\uff1a**\n```\n#include <iostream>\n#include \"torch/csrc/api/include/torch/torch.h\"\n#include \"torch/csrc/api/include/torch/utils.h\"\n#include <caffe2/utils/threadpool/ThreadPool.h>\n#include <caffe2/utils/threadpool/ThreadPoolMobile.h>\n#include \"torch/script.h\"\n#include <utility>\n#include <unistd.h>\n#include <vector>\n#include <pthread.h>\n\nusing std::cout;\nusing std::endl;\ntorch::jit::Module module_;\nstruct JITCallGuard {\n  // AutoGrad is disabled for mobile by default.\n  torch::autograd::AutoGradMode no_autograd_guard{false};\n  // VariableType dispatch is not included in default mobile build. We need set\n  // this guard globally to avoid dispatch error (only for dynamic dispatch).\n  // Thanks to the unification of Variable class and Tensor class it's no longer\n  // required to toggle the NonVariableTypeMode per op - so it doesn't hurt to\n  // always set NonVariableTypeMode for inference only use case.\n  torch::AutoNonVariableTypeMode non_var_guard{true};\n  // Disable graph optimizer to ensure list of unused ops are not changed for\n  // custom mobile build.\n  torch::jit::GraphOptimizerEnabledGuard no_optimizer_guard{false};\n};\n\nvoid *autoTest(void *args) {\n  printf(\"start load \\n\");\n  try {\n    JITCallGuard guard;\n    module_ = torch::jit::load(std::move(\"./simple.pt\"));\n  } catch (const c10::Error &e) {\n    printf(\"load module resource failed.\");\n    return 0;\n  }\n  module_.eval();\n  printf(\"load over \\n\");\n  usleep(1000 * 1000);\n\n  // run forward\n  caffe2::mobile_threadpool()->setNumThreads(1);\n  torch::jit::getProfilingMode() = false;\n  torch::jit::getExecutorMode() = false;\n  torch::jit::setGraphExecutorOptimize(false);\n  printf(\"start recycle \\n\");\n  int count = 3;\n  while (count >= 0) {\n    float *data = new float[8000];\n    torch::Tensor datas = torch::from_blob(data, {100, 80});\n    std::vector<torch::jit::IValue> inputs{};\n    inputs.emplace_back(datas);\n    JITCallGuard guard;\n    module_.forward(inputs);\n    delete[] data;\n    usleep(500 * 1000);\n    printf(\"current count is %d .\\n\", count);\n    count--;\n  }\n  pthread_exit(NULL);\n}\n\n\nint main() {\n  cout << \"Test TorchScript Memory Leak pytorch 1.5.0\" << endl;\n  for (int i = 0; i < 200; i++) {\n    cout << \"times is \" << i << endl;\n    pthread_t tid;\n    pthread_attr_t attr;\n    void *status;\n    pthread_attr_init(&attr);\n\n    pthread_attr_setdetachstate(&attr, PTHREAD_CREATE_JOINABLE);\n    int ret = pthread_create(&tid, &attr, autoTest, NULL);\n    if (ret != 0) {\n      cout << \"pthread_create error: error_code=\" << ret << endl;\n      return 0;\n    }\n    pthread_attr_destroy(&attr);\n    int rc = pthread_join(tid, &status);\n    if (rc) {\n      cout << \"Error:unable to join,\" << rc << endl;\n      exit(-1);\n    }\n    cout << \"Main: completed thread id : \"\n            \"exiting with status :\" << status << endl;\n\n  }\n  return 0;\n}\n```\n\nget memory info by\n```\npid=`ps -A | grep Mem| awk '{print $2}'`;while [ 1 ];do cat /proc/${pid}/status | grep VmRSS;sleep 3;done\n```\nCMakeList\n```\ncmake_minimum_required(VERSION 3.6)\n\nset(PLATFORM_ANDROID TRUE)\nset(ANDROID_STL c++_static)\nset(CMAKE_VERBOSE_MAKEFILE ON)\n# NDK r20b\nset(ANDROID_NDK \"path/to/your/ndk\")\nset(CMAKE_TOOLCHAIN_FILE \"${ANDROID_NDK}/build/cmake/android.toolchain.cmake\")\nset(ANDROID_ABI arm64-v8a)\nset(ANDROID_NATIVE_API_LEVEL android-28)\n\n\nproject(MemoryLeak)\n\nset(CMAKE_BUILD_TYPE Release)\n\nif (CMAKE_BUILD_TYPE MATCHES \"Debug\" OR CMAKE_BUILD_TYPE MATCHES \"None\")\n    message(STATUS \"CMAKE_BUILD_TYPE is Debug\")\n    set(CMAKE_CXX_FLAGS \"${CMAKE_CXX_FLAGS} -fPIC -O0 -Wall -g -ggdb\")\n    set(CMAKE_C_FLAGS \"${CMAKE_C_FLAGS} -fPIC -O0 -Wall -g -ggdb\")\nelseif (CMAKE_BUILD_TYPE MATCHES \"Release\")\n    message(STATUS \"CMAKE_BUILD_TYPE is Release\")\n    set(CMAKE_CXX_FLAGS \"${CMAKE_CXX_FLAGS} -std=c++14 -O3 -fPIC -fpermissive\")\n    set(CMAKE_C_FLAGS \"${CMAKE_C_FLAGS} -fPIC -O3\")\n    set(CMAKE_EXE_LINKER_FLAGS \"${CMAKE_EXE_LINKER_FLAGS} -Wl,-s\")\nelseif (CMAKE_BUILD_TYPE MATCHES \"RelWitchDebInfo\")\n    message(STATUS \"CMAKE_BUILD_TYPE is RelWitchDebInfo\")\nelseif (CMAKE_BUILD_TYPE MATCHES \"MinSizeRel\")\n    message(STATUS \"CMAKE_BUILD_TYPE is MinSizeRel\")\n    set(CMAKE_CXX_FLAGS \"${CMAKE_CXX_FLAGS} -std=c++14 -O3 -fPIC -fpermissive\")\n    set(CMAKE_C_FLAGS \"${CMAKE_C_FLAGS} -fPIC -O3\")\nelse ()\n    message(STATUS \"unknown CMAKE_BUILD_TYPE = \" ${CMAKE_BUILD_TYPE})\n    set(CMAKE_CXX_FLAGS \"${CMAKE_CXX_FLAGS} -std=c++14 -fPIC -O0 -Wall -g -ggdb\")\n    set(CMAKE_C_FLAGS \"${CMAKE_C_FLAGS} -fPIC -O0 -Wall -g -ggdb\")\nENDif ()\n# set STL C++ 14\nset(CMAKE_CXX_STANDARD 14)\n\ninclude_directories(\n        ${PROJECT_SOURCE_DIR}/pytorch/include/\n        ${PROJECT_SOURCE_DIR}/pytorch/include/torch/csrc/api/include\n)\n\nlink_directories(\n        ${PROJECT_SOURCE_DIR}/pytorch/lib/\n)\n\n\nadd_executable(MemoryLeak main.cpp)\ntarget_link_libraries(\n        MemoryLeak\n        -Wl,--gc-sections\n        -Wl,--whole-archive\n        torch.a\n        torch_cpu.a\n        -Wl,--no-whole-archive\n        c10.a\n        nnpack.a\n        XNNPACK.a\n        pytorch_qnnpack.a\n        eigen_blas.a\n        cpuinfo.a\n        clog.a\n        log\n        m\n        z\n)\n```\nI found use pthread will come out memory leak.use main function will not, NOTE: in android environment"
   },
   {
      "null": 267,
      "x": "Very slow for gradient penalty!",
      "z": "Gradient penalty requires computing second order derivatives, and just like computing the first one is 2-3x more expensive than the actual computation, the second one is 2-3x more expensive again, yielding a 4-9x slowdown. Closing as not a bug.",
      "y": "Gradient penalty requires computing second order derivatives, and just like computing the first one is 2-3x more expensive than the actual computation, the second one is 2-3x more expensive again, yielding a 4-9x slowdown"
   },
   {
      "null": 268,
      "x": "error when installing Caffe2: undefined reference to `void caffe2::math::BiasCHW",
      "z": "I had the same error. After #7738 I'm now able to build on TX2 with Jetpack 3.2. I had to update eigen3 to the latest version on the git mirror and cmake with the `-DUSE_MPI=OFF` flag. Thanks for the fix @yinghai",
      "y": "update eigen3 to the latest version on the git mirror and cmake with the `-DUSE_MPI=OFF` flag"
   },
   {
      "null": 269,
      "x": "Batchnorm1d cannot work with batch size == 1",
      "z": "This only fails because you have a batch size of 1 and a single feature per channel.\nI'd recommend to use `drop_last=False` on the DataLoader to avoid the issue.\nI don't really have a view on doing magic to avoid this - people will fall into traps when using a batch size of one and no training happens.",
      "y": "This only fails because you have a batch size of 1 and a single feature per channel.\nIuse `drop_last=False` on the DataLoader to avoid the issue."
   },
   {
      "null": 270,
      "x": "in-place Arithmetic assignment operators gives wrong answers",
      "z": "Thanks for the report, @calebh. When tensors share the same storage, in-place operations are incorrect. You should generally avoid in-place operations because they can have other bad side effects, like making backward computation impossible (autograd will throw an error if that is the case though).\n\nIdeally we'd throw an error or warn if the tensors involved in an in-place operation have overlapping storage, but determining that fully is equivalent to finding a solution to a linear diophantine equation in `n` variables, where `n` is proportional to the number of dimensions in the input tensors.",
      "y": "Ideally pytorch throw an error or warn if the tensors involved in an in-place operation have overlapping storage"
   },
   {
      "null": 271,
      "x": "ATen C++ tensor creation places tensors on devices inconsistently from torch.* Python calls",
      "z": "@mcarilli for context, ATen currently does not have any concept of a \"device\" beyond CPU vs. CUDA (i.e. the \"ordinal\" part is missing from ATen's model). I'm working on changing this right now, so you can expect this to be fixed soon",
      "y": "The issue is fixed"
   },
   {
      "null": 272,
      "x": "better error if file does not support seek (torch.load/save)",
      "z": "My bad, I should have provided some code.\n\nOn python 3: (in python 2 the function is just `urllib.urlopen`):\n\n```\nimport torch\nimport urllib\nresource = urllib.request.urlopen('https://download.pytorch.org/test_data/linear.pt')\nmodel = torch.load(resource)\n---------------------------------------------------------------------------\nUnsupportedOperation                      Traceback (most recent call last)\n<ipython-input-15-62e11b1f7972> in <module>()\n      2 import urllib\n      3 resource = urllib.request.urlopen('https://download.pytorch.org/test_data/linear.pt')\n----> 4 model = torch.load(resource)\n\n~/pytorch/pytorch/torch/serialization.py in load(f, map_location, pickle_module)\n    301         f = open(f, 'rb')\n    302     try:\n--> 303         return _load(f, map_location, pickle_module)\n    304     finally:\n    305         if new_fd:\n\n~/pytorch/pytorch/torch/serialization.py in _load(f, map_location, pickle_module)\n    448\n    449     f_is_real_file = _is_real_file(f)\n--> 450     if f_is_real_file and f.tell() == 0:\n    451         # legacy_load requires that f has fileno()\n    452         # only if offset is zero we can attempt the legacy tar file loader\n\nUnsupportedOperation: seek\n```\nIt would be good if we checked if the file is seekable so that we can provide a better error message. Something along the lines of \"Error: you can only torch.load from a file that is seekable. Please read the file into a seekable file on disk using request.urlretrieve or a string buffer like io.BytesIO and try to load from it instead.\"\n\n",
      "y": "Better error message has been added"
   },
   {
      "null": 273,
      "x": "[pytorch] [feature request] Flatten convenience method",
      "z": "@chopin57otu You are right, it's not visible in `stable` docs. However, it is there in the `master` docs: https://pytorch.org/docs/master/torch.html?highlight=flatten#torch.flatten",
      "y": "Feature is added in pytorch"
   },
   {
      "null": 274,
      "x": "torch.empty after construction puts a high load on CPU for a long time when the size is big",
      "z": "Sorry, my fault. \n\nI was testing in Jupyter Notebook and forgot that it added additional complexity to \"vanilla\" python. So there's no such problem in the python shell.\n\nThis problem gradually builds up with each \"Kernel Restart\". May be it's related to lazy deallocation of memory or maybe it has another reason. Anyway hard restart of Jupyter Notebook cleans out this problem.\n\nSo I don't see anything wrong on PyTorch side here.\n\nUPDATE:\nCorrection - the problem is \"Variable Inspector\" plugin.",
      "y": "the problem is \"Variable Inspector\" plugin."
   },
   {
      "null": 275,
      "x": "[pytorch] [Feature Request] SoftArgMax Function for differentiable argmax",
      "z": "Check https://github.com/MWPainter/cvpr2019/blob/master/stitched/soft_argmax.py#L117. /cc @MWPainter if he want to contribute with a PR to the pytroch repo.",
      "y": "use softmax and dot product with indices"
   },
   {
      "null": 276,
      "x": "Cannot compile Caffe2 on Mac",
      "z": "Some more datapoints. This also breaks the Anaconda builds when using the default gflags from the anaconda channel, but seems to work when you use the gflags from conda-forge instead. The gflags in the anaconda channel has not been updated for months, so that shouldn't be the cause. The cause must be some recent change in our build somewhere; the failures go back about a day, so a recent commit is probably the cause. None of the recent commits look that relevant though. Also, all the linux builds are fine, so whatever change caused this somehow only affects osx",
      "y": "The bug is fixed"
   },
   {
      "null": 277,
      "x": "Compiling C/C++ extension receive undefined symbol: _ZTIN5torch8autograd8Variable4ImplE on importing into python.",
      "z": "Hello,\n\nI figured it out by reading the tutorial (https://pytorch.org/tutorials/advanced/cpp_extension.html) more carefully\n\nOnce your extension is built, you can simply import it in Python, using the name you specified in your setup.py script. Just be sure to import torch first, as **this will resolve some symbols that the dynamic linker must see.**\n\nIf I import torch before the module it works.",
      "y": "import torch before the module it works"
   },
   {
      "null": 278,
      "x": "Tag v0.4.0 eigen submodule no longer downloads",
      "z": "I'll fix this tomorrow. I'll fix the v0.4.0 tag itself.",
      "y": "This has been fixed"
   },
   {
      "null": 279,
      "x": "torch.cdist gradients are NAN for p<1 and very small differences in a given dimension (0<delta<~e-45)",
      "z": "This seems like a nasty numerical bug, upgrading priority",
      "y": "The bug is fixed"
   },
   {
      "null": 280,
      "x": "`F.logsigmoid(input, out=blah)` crashes",
      "z": "(about naming: at some point in the past I suggested that `log_softmax` and `log_sigmoid` used underscores consistently)",
      "y": "`log_softmax` and `log_sigmoid` used underscores consistently"
   },
   {
      "null": 281,
      "x": "INTERNAL ASSERT FAILED at mmdet/ops/nms/src/nms_cpu.cpp:7",
      "z": "@modjtabaf \n\nLooks like this bug needs to be reported to https://github.com/open-mmlab/mmdetection instead. The error message says `please report a bug to PyTorch` because the code in that repo is using assert macro from PyTorch [here](https://github.com/open-mmlab/mmdetection/blob/ef58bc62d3b5ee992b37e9277a7aa7701d8c1757/mmdet/ops/nms/src/nms_cpu.cpp#L7). \n\nI am closing this issue, but please feel free to reopen if you believe something needs to be fixed in PyTorch. Thanks!\n\n",
      "y": "The issue is fixed"
   },
   {
      "null": 282,
      "x": "Multiprocessing on Python 3.8 fails with cannot pickle '_io.TextIOWrapper'",
      "z": "@seemethere @osalpekar Haven't looked too deeply, but this is likely because of a spawn vs fork multiprocessing issue. In py3.8, spawn is now default instead of fork on mac. We use a bunch of tempfiles in these tests, and spawn method can't pickle those, see below for a somewhat related repro:\n\nhttps://gist.github.com/rohan-varma/1fe828948e0b48bb6c9be38465c51647\n\nEdit: fix at https://github.com/pytorch/pytorch/pull/36542",
      "y": "this is likely because of a spawn vs fork multiprocessing issue. In py3.8, spawn is now default instead of fork on mac."
   },
   {
      "null": 283,
      "x": "Load pytorch tensor created by torch.save(tensor_name, tensor_path) in c++ libtorch failed.",
      "z": "This is likely a duplicate of https://github.com/pytorch/pytorch/issues/20356, and we can try the solution @driazati suggested: https://github.com/pytorch/pytorch/issues/20356#issuecomment-567663701",
      "y": "The feature is added"
   },
   {
      "null": 284,
      "x": "Build fails with gcc 9.1 + CUDA-10.2",
      "z": "@seemethere @malfet should we set up a GCC 9.X CI?",
      "y": "The issue is fixed"
   },
   {
      "null": 285,
      "x": "The mean method results in different outputs",
      "z": "> BTW, why does contiguous method help in this case?\n\nI haven't looked closely, but my guess is that it forces multiple smaller sums. The 4th and 1st means get implemented as the same thing basically `b.reshape(1024*224*224, 3).sum(0)/N`. The 3rd gets implemented as `tmp.sum(0).sum((2,3)) / N`.\n\nIf you need batch normalization, use the built-in batch normalization function. Otherwise, use double precision as I suggested above. I don't think this is going to be improved anytime soon.",
      "y": " it forces multiple smaller sums. The 4th and 1st means get implemented as the same thing basically `b.reshape(1024*224*224, 3).sum(0)/N`. The 3rd gets implemented as `tmp.sum(0).sum((2,3)) / N`.\n\nIf you need batch normalization, use the built-in batch normalization function. Otherwise, use double precision"
   },
   {
      "null": 286,
      "x": "torch.save does not use zipfile serialization",
      "z": "Given https://github.com/pytorch/pytorch/issues/40140 ...",
      "y": "The issue is fixed"
   },
   {
      "null": 287,
      "x": "Multi channel linear layer",
      "z": "We typically only add modules to the core PyTorch if they're widely applicable and described in a significant research paper. I don't think this reaches that bar yet. You are welcome to distribute it in your own package.\n\nI'd recommend using a torch.bmm instead of `*` and `sum`. You'll likely get better performance and lower memory usage over a wide range of sizes. For example, set up your tensor as:\n\n```\nmy_tensor = torch.randn(channel_size, batch_size, input_size)\nweight = torch.randn(channel_size, input_size, output_size)\nbias = torch.randn(channel_size, 1, output_size)\n\noutput = torch.bmm(my_tensor, weight) + bias\n```",
      "y": "using a torch.bmm instead of `*` and `sum`. You'll likely get better performance and lower memory usage over a wide range of sizes. For example, set up your tensor as:\n\n```\nmy_tensor = torch.randn(channel_size, batch_size, input_size)\nweight = torch.randn(channel_size, input_size, output_size)\nbias = torch.randn(channel_size, 1, output_size)\n\noutput = torch.bmm(my_tensor, weight) + bias\n```"
   },
   {
      "null": 288,
      "x": "Exception in find_cuda_windows_lib",
      "z": "These functions are already removed in master.",
      "y": "These functions are removed"
   },
   {
      "null": 289,
      "x": "Autograd view CreationMeta are not properly propagated when chaining views",
      "z": "I think, related: https://github.com/pytorch/pytorch/issues/31819",
      "y": "The issue is fixed"
   },
   {
      "null": 290,
      "x": "Add TracedModule attribute for the constants table",
      "z": "Looks like I'm not getting a ton of free time to work on this issue, but I'll write what needs to be done here, and if someone wants to put up a PR I am happy to review.\n\nThe way we pretty print `.code` is with `PythonPrint`. This function populates one of its arguments `tensor_table` with tensor constants (see \"Uses of tensor constants\" [here](https://github.com/pytorch/pytorch/blob/master/torch/csrc/jit/docs/serialization.md#printing-code-objects-as-python-source) for a more detailed description). Indices in this tensor table correspond to `CONSTANTS.c*` in the pretty printed code. \n\nRight now in the python binding for `.code` ([here](https://github.com/pytorch/pytorch/blob/999d7f6ab2d1196a1f6e2bb7123e9da0e5a2592d/torch/csrc/jit/python/script_init.cpp#L896)) populates this `tensor_table` and throws it away. We just want to keep it and return it to the user.\n\nSome additional notes:\n1. The constants table is only associated with a single `PythonPrint` pass, so you shouldn't store them separately from `.code` (they might get out of sync). We should probably have a `.code_with_constants` or something that returns a `(code_str, constants_table)`. \n2. Worth creating a little wrapper for the constants table in Python that overrides `getattr` so that `.c0` returns the 0th element of the table, so that you can just assign the constants table to  `CONSTANTS` and all the calls to `CONSTANTS.cN` will just work.\n",
      "y": "The feature is added"
   },
   {
      "null": 291,
      "x": "THCUNN/BCECriterion.cu:42: Assertion `input >= 0. && input <= 1.` failed.",
      "z": "i have met this question too, but when i debug this problem.\ni discovered that this may happens when your network initiated with a bad startup.\nusually the output is too big , eg , sigmoid(20.) == 1\nso, this will occurs when you want to do (sth + sigmoid(20.)) with BCE ,this output will over then one.\n\nsolution:\ntorch.nn.init.xavier_normal_(weight)\ntorch.nn.init.constant_(bias, 0)\nmay help to you ",
      "y": "this may happens when your network initiated with a bad startup.\nusually the output is too big , eg , sigmoid(20.) == 1\nso, this will occurs when you want to do (sth + sigmoid(20.)) with BCE ,this output will over then one.\n\nsolution:\ntorch.nn.init.xavier_normal_(weight)\ntorch.nn.init.constant_(bias, 0)"
   },
   {
      "null": 292,
      "x": "Add torch.sgn to return complex sign",
      "z": "@kshitij12345 Hi, yes I am working on it.",
      "y": "The feature is added"
   },
   {
      "null": 293,
      "x": "torch.triangular_solver doesn't work on batched inputs",
      "z": "Yes, the issue seems to be fixed now. Thank you!",
      "y": "The issue is fixed now"
   },
   {
      "null": 294,
      "x": "Explicitly Define Gradient for Certain Computation",
      "z": "@occam-ra-zor \nI guess this tutorial might be useful. You can implement your Function/Operator the way `MyRelu` is defined.\nhttps://pytorch.org/tutorials/beginner/examples_autograd/two_layer_net_custom_function.html",
      "y": "You can implement your Function/Operator the way `MyRelu` is defined.\nhttps://pytorch.org/tutorials/beginner/examples_autograd/two_layer_net_custom_function.html"
   },
   {
      "null": 295,
      "x": "Bug when an inplace op is done on the output of the forward of a autograd.Function and this output is a view of an intermediary result of the forward.",
      "z": "@colesbury Yes, I do. I guess I can fix this issue by modifying `CheckpointFunction`.",
      "y": "It has been fixed by modifying `CheckpointFunction`."
   },
   {
      "null": 296,
      "x": "Support matmul for scalar tensors",
      "z": "The reason for this treatment is given in PEP 465 -- https://www.python.org/dev/peps/pep-0465/#semantics\n\nIt allowed special treatment for rank-1 tensors because \"practicality beats purity\", but choose not to extend to rank-0 tensors because there's already a * operator which works for this case, so the case for practicality is not as strong.\n\nIt seems unlikely to change in numpy, and this doesn't seem like a strong-enough reason to deviate from numpy compatibility, so will close this issue.\n\nSomething like #26566 would allow a workaround for users without creating backward compatibility issues.",
      "y": "The feature is not added"
   },
   {
      "null": 297,
      "x": "Build failure (sleef.h not found) in some hard-to-understand situations",
      "z": "To fix, add to [caffe2/CMakeLists.txt](https://github.com/pytorch/pytorch/blob/50d82f51223726725317919d429a02979d9bc39c/caffe2/CMakeLists.txt#L1155) after line 1155\n\n`target_include_directories(${test_name} PRIVATE $<BUILD_INTERFACE:${CMAKE_BINARY_DIR}/include>)`\n",
      "y": "To fix, \n`target_include_directories(${test_name} PRIVATE $<BUILD_INTERFACE:${CMAKE_BINARY_DIR}/include>)`"
   },
   {
      "null": 298,
      "x": "AttributeError: module 'torch.jit' has no attribute 'unused'",
      "z": "`entrypoints = torch.hub.list('pytorch/vision', force_reload=True)` loads from vision master which depends on pytorch master. I think `entrypoints = torch.hub.list('pytorch/vision:v0.4.0', force_reload=True)` should work. Hub CI tests against latest pytorch & vision conda releases. \nI agree it's good to annotate the current stable releases for these repos on Hub webpage... ",
      "y": "`entrypoints = torch.hub.list('pytorch/vision:v0.4.0', force_reload=True)` should work"
   },
   {
      "null": 299,
      "x": "BC check test was failing on Friday and Saturday",
      "z": "Another thing I'm concerned about is whether or not the script picks up ALL bc-breaking changes, or bails out on the first one. If it bails out on the first one then after master is broken you will miss any further breaking changes.",
      "y": "bug is fixed now"
   },
   {
      "null": 300,
      "x": "Where did 1.2.0 go on the website?",
      "z": "This is fixed now, I manually fixed them.\nI have to see why they were overwritten with a previous version.",
      "y": "This is fixed now"
   },
   {
      "null": 301,
      "x": "[jit] `zip` and `enumerate` can't be used in a list comprehension",
      "z": "Similar issue to this, I believe: https://github.com/pytorch/pytorch/issues/22483",
      "y": "This has been fixed"
   },
   {
      "null": 302,
      "x": "MultiheadAttention and DDP incompatability",
      "z": "@mrshenli I have discussed with Shen offline. I will submit a PR to fix it soon.",
      "y": "This issue is fixed now"
   },
   {
      "null": 303,
      "x": "spectral_norm used in RNN causes \"parameter types mismatch\" in GPU",
      "z": "I came across the same Bug, did this problem be solved? @VitalyFedyunin @pbelevich @oh-y ",
      "y": "The issue is closed without any fix"
   },
   {
      "null": 304,
      "x": "[jit] `zeros_like` needs full Tensor options to work",
      "z": "```Tensor Options Creates Schema Mismatch for ops which have a (ScalarType dtype, Layout layout, Device device) tuple```",
      "y": "```Tensor Options Creates Schema Mismatch for ops which have a (ScalarType dtype, Layout layout, Device device) tuple```"
   },
   {
      "null": 0,
      "x": "ONNX export failure: Exporting the operator hardsigmoid to ONNX opset version 12 is not supported",
      "z": "For Hardsigmoid symbolic you can also try exporting to ONNX::Hardsigmoid directly.",
      "y": "Directly export to ONNX::Hardsigmoid for Hardsigmoid symbols."
   },
   {
      "null": 1,
      "x": "ROCm CI is intermittently failing with std::out_of_range",
      "z": "Add range assert in autograd engine queue lookup",
      "y": "In the autograd engine queue lookup, add a range assert."
   },
   {
      "null": 2,
      "x": "Building with MSVC 2019 Community Edition fails",
      "z": "Reproduced locally and can be fixed by adding the assignment operator for `torch::OrderedDict<Key, Value>::Item`.\n \n \n ```cpp\n \n \n  Item& operator=(const Item& other)\n \n \n  {\n \n \n  pair_ = other.pair_;\n \n \n  return *this;\n \n \n  }\n \n \n ```",
      "y": "Add the assignment operator for `torch::OrderedDict<Key, Value>::Item`."
   },
   {
      "null": 3,
      "x": "torch.log() returns -inf/nan on exponential input",
      "z": "Issue is that the `pow` does not promote dtypes (dtype of x is inferred to be `Long/int64`) and overflows leading to incorrect values. However the behavior is consistent with numpy. \n \n \n \n \n \n The reason it works in Python and Math is that plain int is unbounded and `2**100` and others do not overflow which math.log treats as double precision float and everything works.\n \n \n \n \n \n (simplest solution is to initialise `x` as `x=torch.tensor(2.)`, post which the results match as expected)\n \n \n \n \n \n ```python\n \n \n import torch\n \n \n import numpy as np\n \n \n from math import *\n \n \n \n \n \n x = torch.tensor(2)\n \n \n a = x.numpy()\n \n \n \n \n \n print(\"NUMPY POWER\")\n \n \n print(a**62 )\n \n \n print(a**63 )\n \n \n print(a**64 )\n \n \n print(a**100 )\n \n \n \n \n \n print(\"TORCH POWER\")\n \n \n print(x**62, torch.pow(x, 62).dtype)\n \n \n print(x**63, torch.pow(x, 63).dtype)\n \n \n print(x**64, torch.pow(x, 64).dtype)\n \n \n print(x**100, torch.pow(x, 100).dtype)\n \n \n \n \n \n print(\"NUMPY LOG\")\n \n \n print(np.log(a ** 62)) # valid\n \n \n print(np.log(a ** 63)) # invalid (nan)\n \n \n print(np.log(a ** 64)) # invalid (-inf)\n \n \n print(np.log(a **100)) # invalid (-inf)\n \n \n print(100 * np.log(a)) # valid\n \n \n \n \n \n print(\"TORCH LOG\")\n \n \n print(torch.log(x ** 62)) # valid\n \n \n print(torch.log(x ** 63)) # invalid (nan)\n \n \n print(torch.log(x ** 64)) # invalid (-inf)\n \n \n print(torch.log(x **100)) # invalid (-inf)\n \n \n print(100 * torch.log(x)) # valid\n \n \n \n \n \n print(\"MATH LOG\")\n \n \n # all valid (return float)\n \n \n print(log(2 **62))\n \n \n print(log(2 **63))\n \n \n print(log(2 **64))\n \n \n print(log(2 **100))\n \n \n print(100 *log(2))\n \n \n ```\n \n \n \n \n \n Output:\n \n \n <details>\n \n \n \n \n \n ```\n \n \n NUMPY POWER\n \n \n 4611686018427387904\n \n \n -9223372036854775808\n \n \n 0\n \n \n 0\n \n \n TORCH POWER\n \n \n tensor(4611686018427387904) torch.int64\n \n \n tensor(-9223372036854775808) torch.int64\n \n \n tensor(0) torch.int64\n \n \n tensor(0) torch.int64\n \n \n NUMPY LOG\n \n \n 42.97512519471661\n \n \n test_log.py:20: RuntimeWarning: invalid value encountered in log\n \n \n  print(np.log(a ** 63)) # invalid (nan)\n \n \n nan\n \n \n test_log.py:21: RuntimeWarning: divide by zero encountered in log\n \n \n  print(np.log(a ** 64)) # invalid (-inf)\n \n \n -inf\n \n \n test_log.py:22: RuntimeWarning: divide by zero encountered in log\n \n \n  print(np.log(a **100)) # invalid (-inf)\n \n \n -inf\n \n \n 69.31471805599453\n \n \n TORCH LOG\n \n \n tensor(42.9751)\n \n \n tensor(nan)\n \n \n tensor(-inf)\n \n \n tensor(-inf)\n \n \n tensor(69.3147)\n \n \n MATH LOG\n \n \n 42.97512519471661\n \n \n 43.66827237527655\n \n \n 44.3614195558365\n \n \n 69.31471805599453\n \n \n 69.31471805599453\n \n \n ```\n \n \n \n \n \n </details>",
      "y": "Use float datatype inputs or convert it to numpy."
   },
   {
      "null": 4,
      "x": "__torch_function__ documentation for inheriting from tensor seems incomplete or incorrect",
      "z": "Ah, yes. This happens when we try to print the `__repr__` of `args` without skipping `__torch_function__`, and it comes back into `__torch_function__` unconditionally. This should be solved by #55093.",
      "y": "This happens when we try to print the `__repr__` of `args` without skipping `__torch_function__`"
   },
   {
      "null": 5,
      "x": "`nn.Conv3d` throws incorrect error message",
      "z": "Thanks @dlmgary for reporting this! I agree it's a confusing error message.\n \n \n \n In the details, this comes from: https://github.com/pytorch/pytorch/blob/980d6f258912e8d4474c5ccfe1a1e4f88f226af5/aten/src/ATen/native/ConvolutionMM3d.cpp#L243\n \n \n \n Since the in-place op is called on the output tensor (with the same dtype as the input), the error message centers around output's dtype.\n \n \n \n May be worth clearing this up with a check higher up.",
      "y": "the in-place op is called on the output tensor (with the same dtype as the input)"
   },
   {
      "null": 6,
      "x": "Is there a way to remove all zero elements in one line?",
      "z": "> I think torch.nonzero might interest you.\n \n > Reference: https://pytorch.org/docs/stable/generated/torch.nonzero.html\n \n > \n \n > **Note** : I think this question is more suited for the [forums](https://discuss.pytorch.org/)\n \n \n \n Thanks, @kshitij12345. I found a solution in the forums here: https://discuss.pytorch.org/t/how-to-remove-an-element-from-a-1-d-tensor-by-index/23109/6.\n \n Sorry for that, I'm new to PyTorch and programming, but I now know how to ask questions. Have a nice day: )",
      "y": "remove an element from 1-d tensor"
   },
   {
      "null": 7,
      "x": "Backprop second time on spectral_norm",
      "z": "Looks like there are some inplace ops in there that break double backward. We should fix that.",
      "y": "there are some inplace ops in there that break double backward"
   },
   {
      "null": 8,
      "x": "quantize_per_tensor and quantize_per_channel should work on fp16 tensors",
      "z": "correct. The model would still be in fp32, it's just the forward pass which is run in fp16 to speed up training. This should be independent from any post training quantization strategies.",
      "y": "The model would still be in fp32, it's just the forward pass which is run in fp16 to speed up training. This should be independent from any post training quantization strategies."
   },
   {
      "null": 9,
      "x": "Query on PyTorch threading model",
      "z": "The text seems to describe this scenario - two application threads use openmp (e.g. `parallel for` or use ops that use openmp), in this case OpenMP would create two thread pools. It can be checked by e.g. running `matmul` in parallel with `matmul` launched with `torch.jit.fork`.",
      "y": "two application threads use openmp (e.g. `parallel for` or use ops that use openmp), in this case OpenMP would create two thread pools. It can be checked by e.g. running `matmul` in parallel with `matmul` launched with `torch.jit.fork`."
   },
   {
      "null": 10,
      "x": "torch.cat results does not have `requires_grad` if under an autograd function",
      "z": "hmm this is expected? things run in `Function::forward` are without autograd graph tracking.",
      "y": "Things run in `Function::forward` are without autograd graph tracking."
   },
   {
      "null": 11,
      "x": "How to access model embedded functions?",
      "z": "please use [PyTorch forum](https://discuss.pytorch.org/) for future questions. Thanks!",
      "y": "Use <model>.<function>"
   },
   {
      "null": 12,
      "x": "It'd best to set persistent_workers default value as True on Windows",
      "z": "Thank your explanation.",
      "y": "Default persistent_worker from False to True on Windows"
   },
   {
      "null": 13,
      "x": "torch.autograd.Function with multiple outputs returns outputs not requiring grad",
      "z": "Well, returning tuples from a python function is always dangerous, because doing `return a[0], a[1]` is the same as `return a`.\n \n So effectively, in your custom function here, you actually return the Tensors \"unpacked\" and so they are properly post-processed by the custom Function. And since there were not inputs that required gradients (no Tensor in this case), then the outputs don't need to require gradients either.",
      "y": "Returning tuples from a python function is always dangerous, because doing `return a[0], a[1]` is the same as `return a`.\n \n So effectively, in your custom function here, you actually return the Tensors \"unpacked\" and so they are properly post-processed by the custom Function. And since there were not inputs that required gradients (no Tensor in this case), then the outputs don't need to require gradients either."
   },
   {
      "null": 14,
      "x": "OpInfo addmv port from method_tests errors",
      "z": "The test_variant_consistency_eager failure might be the result of a bad torch.addmv_() bug that you discovered, @Lilyjjo.\n \n \n \n ```\n \n a = torch.randn((1,))\n \n b = torch.randn((3, 4))\n \n c = torch.randn((4,))\n \n \n \n # this should error out\n \n a.addmv_(b, c)\n \n : tensor([ 2.2854, 1.4802, -1.4424])\n \n ```\n \n \n \n It looks like torch.addmv_() allows inplace operations to change the size of the tensor they're operating on! I filed https://github.com/pytorch/pytorch/issues/55589 to track that more particular issue.",
      "y": "torch.addmv_() allows inplace operations to change the size of the tensor they're operating on"
   },
   {
      "null": 15,
      "x": "RuntimeError: Expected to have finished reduction in the prior iteration before starting a new one.",
      "z": "It works. Thanks. @mrshenli",
      "y": "include the loss computation in the forward function and let the forward function directly return the loss tensors. Then by setting find_unused_parameters=True, DDP should be able to traverse the graph from the loss and identify unused ones."
   },
   {
      "null": 16,
      "x": "`Softplus` forward and backward discrepancy",
      "z": "Thank you for reporting this issue, @shmsong. The team agrees it's a high priority.",
      "y": "The fix converts the binary TensorIterator used by softplus backwards to a ternary one, adding in the original input for comparison against beta * threshold."
   },
   {
      "null": 17,
      "x": "addmv_() allows resizing the tensor it operates on and produces wrong results",
      "z": "It not only resizes, it produces bogus values:\n \n ```\n \n In [3]: a = torch.full((1,), 0.5, device=\"cuda\")\n \n  ...: \n \n  ...: b = torch.ones((3, 4), device=\"cuda\")\n \n  ...: c = torch.ones((4,), device=\"cuda\")\n \n In [4]: out=torch.addmv(a,b,c)\n \n In [5]: out\n \n Out[5]: tensor([4.5000, 4.5000, 4.5000], device='cuda:0')\n \n In [6]: a\n \n Out[6]: tensor([0.5000], device='cuda:0')\n \n In [7]: a.addmv_(b,c)\n \n Out[7]: tensor([4.5000, 8.5000, 8.5000], device='cuda:0')\n \n ```",
      "y": "It produces bogus values."
   },
   {
      "null": 18,
      "x": "Verify that attempting to resize a tensor with an inplace operation throws a runtime error",
      "z": "Programmatic detection can work in `test_variant_consistency_eager`, where we actually compute the forward of the input (so we can check the shape of the output), however it won't work in `test_inplace_grad` and `test_inplace_gradgrad` (unless we add forward checks just for that).\n \n \n \n So I think it would be better to mark the SampleInput. \n \n * `test_variant_consistency_eager` will confirm that inplace on that sample actually fails (i.e. samples can't be incorrectly marked).\n \n * `test_inplace_grad{grad}` can just trust the marking and skip accordingly.\n \n * Information about whether the input broadcasts will be visible at sampling itself.\n \n \n \n Reference for `test_inplace_grad{grad}`\n \n \n \n https://github.com/pytorch/pytorch/blob/c7312f5271b9ce9ac988fffde90818354e5841b8/test/test_ops.py#L104-L117",
      "y": "Add broadcasts_input and verifies the behaviour for inplace_variant."
   },
   {
      "null": 19,
      "x": "Modification of tests for one method causes failures for some others",
      "z": "> IMO we should reseed before sampling randomly for each test, so that test ordering changes don't perturb RNG state.\n \n \n \n Porting the test to an OpInfo will achieve this because when using OpInfos the generation of sample inputs occurs WITHIN each test, not outside of it, unlike the deprecated method_tests().\n \n \n \n We could fix method_tests(), but the real fix is to kill them (by porting them to OpInfos).",
      "y": "Port them to OpInfos."
   },
   {
      "null": 20,
      "x": "torch.linalg: unclear \"synchronizes that device with the CPU\"",
      "z": "Thank you for reporting this issue @twoertwein. @raisinglc's explanation is in the right ballpark that we're trying to explain these algorithms move data from the GPU to the CPU. Typically GPU kernels are enqueued by the CPU and run asynchronously on the GPU, allowing both the CPU and GPU to run in a parallel. When an algorithm requires data from the GPU, however, the CPU waits for the GPU to provide it, and the GPU has to transfer the data to the CPU. This can be a surprising performance hit, and most PyTorch operators do not perform this cross-device data transfer.\n \n \n \n Exactly WHAT data is being moved from the GPU to the CPU isn't clear from this documentation, but its intent is to warn users who may find this behavior surprising. Maybe there's a clearer way to express this.",
      "y": "Typically GPU kernels are enqueued by the CPU and run asynchronously on the GPU, allowing both the CPU and GPU to run in a parallel. When an algorithm requires data from the GPU, however, the CPU waits for the GPU to provide it, and the GPU has to transfer the data to the CPU. This can be a surprising performance hit, and most PyTorch operators do not perform this cross-device data transfer."
   },
   {
      "null": 21,
      "x": "test_zero_redundancy_optimizer.py fails when run more than 4 GPU setup",
      "z": "Sorting or not the parameters will not change the distribution AFAIK, or it will change the precise partitioning but will not be more effective size-wise or speed-wise, the partitioning is still greedy so it will go through all the buckets and fill up the emptiest one, then repeat. In the end, space-wise there's only so much you can do with full tensors, another take is to flatten all the tensors (like FSDP does for instance) and equal-partition from there. Space is not really an issue empirically, except for pathological models (with one really really big layer), I don't think that there's much to win there.\n \n \n \n Speed wise other things could be done though, started in https://github.com/facebookresearch/fairscale/pull/598 but not really worth it in the end I think, redundant work with FSDP. What's lost right now is that all the ranks have the parameters in no particular order, so during the backward pass you have to wait for all the reductions before all the optimizers can step. *If* you partition along the real order of the parameters (ie: not `parameters()` but the orders with which the tensors are being used, which you can get during the backward pass), then the rank's optimizer could step() as soon as it got its grads, and start broadcasting the updated shards early on, during the backward pass. I would expect the speed up to be significant (given enough CUDA streams), but again this is duplicated engineering on the FairScale side given FSDP\n \n cc @mrshenli",
      "y": "I would expect the speed up to be significant (given enough CUDA streams), but again this is duplicated engineering on the FairScale side given FSDP."
   },
   {
      "null": 22,
      "x": "CUDA error when used torch.mm() with gpu in pytorch1.8.0",
      "z": "I can reproduce the problem using torch-1.8 on RTX 2080 (sm_75) for the following trivial case:\n \n ```\n \n $ python -c \"import torch;x=torch.eye(3, 3, device='cuda');print(torch.mm(x,x))\"\n \n ```\n \n torch-1.8 (unlike 1.7) is shipped without sm_75 cubins due to the size considerations:\n \n ```\n \n Analyzing /home/nshulga/.local/lib/python3.7/site-packages/torch/lib/libtorch_cuda.so\n \n .nv_fatbin size 705.5MiB\n \n  sm_37: 72.5MiB\n \n  sm_50: 171.1MiB\n \n  sm_60: 179.7MiB\n \n  sm_70: 193.0MiB\n \n  sm_35: 39.0MiB\n \n  sm_61: 50.2MiB\n \n __nv_relfatbin size 35.4MiB\n \n  sm_35: 5.3MiB\n \n  sm_50: 7.4MiB\n \n  sm_60: 7.8MiB\n \n  sm_70: 14.9MiB\n \n  sm_37: 54.5KiB\n \n ```",
      "y": "torch-1.8 (unlike 1.7) is shipped without sm_75 cubins due to the size considerations."
   },
   {
      "null": 23,
      "x": "Failed to compute shorthash for libnvrtc.so when compiling application with libtorch 1.8.0",
      "z": "I'm having the same issue, and it looks like the problem is not with `CUDA_NVRTC_LIB`, but rather that the `PYTHON_EXECUTABLE` isn't defined\n \n \n \n I added this message after the call the `execute_process`:\n \n \n \n ```\n \n execute_process(\n \n  COMMAND \"${PYTHON_EXECUTABLE}\" -c\n \n  \"import hashlib;hash=hashlib.sha256();hash.update(open('${CUDA_NVRTC_LIB}','rb').read());print(hash.hexdigest()[:8])\"\n \n  RESULT_VARIABLE _retval\n \n  OUTPUT_VARIABLE CUDA_NVRTC_SHORTHASH)\n \n \n \n message(STATUS \"PYTHON_EXECUTABLE=(${PYTHON_EXECUTABLE}), _retval=${_retval}. CUDA_NVRTC_LIB=${CUDA_NVRTC_LIB}\")\n \n ```\n \n \n \n and this is the output:\n \n ```\n \n -- PYTHON_EXECUTABLE=(), _retval=No such file or directory. CUDA_NVRTC_LIB=/usr/local/cuda-11.1/lib64/libnvrtc.so\n \n CMake Warning at torch-v1.8.0/share/cmake/Caffe2/public/cuda.cmake:199 (message):\n \n  Failed to compute shorthash for libnvrtc.so\n \n Call Stack (most recent call first):\n \n  torch-v1.8.0/share/cmake/Caffe2/Caffe2Config.cmake:88 (include)\n \n  torch-v1.8.0/share/cmake/Torch/TorchConfig.cmake:68 (find_package)\n \n  CMakeLists.txt:5 (find_package)\n \n ```",
      "y": "Problem is not with `CUDA_NVRTC_LIB`, but rather that the `PYTHON_EXECUTABLE` isn't defined."
   },
   {
      "null": 24,
      "x": "AdamW variable referenced before assignment",
      "z": "Hi, you're right, missread the commit. Should be fixed then.",
      "y": "The current error is that beta1 is referenced before assignment."
   },
   {
      "null": 25,
      "x": "Selection of test classes / methods through run_test.py is broken",
      "z": "Nice investigation @pmeier. Now that you found the root cause, I'd say fix it anyway if it's easy to do so. If it takes more than 1-2 hours then never mind, given that it will become irrelevant once we manage to switch to pytest.",
      "y": "enable test selection for default test handler."
   },
   {
      "null": 26,
      "x": "Status of 64-bit Arm support in PyTorch 1.8",
      "z": "Hi @malfet,\n \n \n \n I've just been giving the PyTorch 1.8 whl a go. I `pip install`ed `torch-1.8.0-cp36-cp36m-manylinux2014_aarch64.whl` on a CentOS 8.3.2011 machine, but I'm getting the following error:\n \n \n \n ```\n \n OSError: <path-to-my-python-venv>/lib64/python3.6/site-packages/torch/lib/libtorch_global_deps.so: ELF load command alignment not page-aligned\n \n ```\n \n \n \n This appears to be an issue previously encountered with NumPy ManyLinux builds (https://github.com/numpy/numpy/issues/16677) where the whl was built with the wrong pagesize.\n \n \n \n The same issue appears to affect the nightly .whls from https://download.pytorch.org/whl/nightly/cpu/torch_nightly.html too.\n \n \n \n Ubuntu 20.10 with Python 3.8.6 appears to work as expected.",
      "y": "issue previously encountered with NumPy ManyLinux builds."
   },
   {
      "null": 27,
      "x": "Backward compatibility problem of LSTMs in v1.8.0",
      "z": "not opposed to a fix for this particular issue. Happy to review the PR if you open it!",
      "y": "Users should be only saving state_dicts to avoid such issues."
   },
   {
      "null": 28,
      "x": "[bug] test_autograd sometimes runs CUDA tests on CPU",
      "z": "To add on this, while `det` in forward is using a LU decomposition, the `det_backward` is using `svd_backward`, which is only double backward stable for inputs with distinct singular values. We will need to reimplement `det_backward` which uses `lu_backward` instead. `lu_backward` needs to be implemented in ATen, as the current backward for LU is done via `autograd.Function`. I will submit an issue about it.",
      "y": "`det` in forward is using a LU decomposition, the `det_backward` is using `svd_backward`, which is only double backward stable for inputs with distinct singular values."
   },
   {
      "null": 29,
      "x": "Lazy Module Documentation suggestions",
      "z": "Thanks for the heads up, will try to work on it this week.",
      "y": "Lazy Modules Documentation Clarifications."
   },
   {
      "null": 30,
      "x": "Undocumented change of behavior for Embeddings in PyTorch 1.8",
      "z": "I'm not convinced that either #46714 or #47184 are fully correct. Following is what I consider to be the \"ideal semantics\" for `padding_idx` that adhere to the principle of least astonishment, and the current state of the semantics in 1.8.0:\n \n \n \n ### Ideal semantics\n \n \n \n  The crux of the issue is that `padding_idx` is used within `Embedding` to accomplish two things:\n \n 1. To \"fix\" the value of the embedding vector at `padding_idx` by preventing gradient updates to it\n \n 2. To pad the output with zeros wherever `padding_idx` is encountered in the input\n \n \n \n The important part is 1, and 2 is unnecessary to handle separately because it can be achieved from 1. Ideally, `padding_idx` should only be used to indicate which embedding vector should be exempt from gradient updates. Otherwise, indexing should work as normal with no special handling needed - if the embedding vector at `padding_idx` is all zeros (AKA its initial value), normal indexing will produce the behavior of 2. It's important to note that 2 is not desired for all use cases!\n \n \n \n This should be the case for both the module and functional forms, and it gives the user maximum flexibility to set the embedding vector at `padding_idx` to whatever is useful for their purposes without confusing semantics (arg naming aside).\n \n \n \n ### 1.8.0 semantics\n \n \n \n `padding_idx` passed to the `Embedding` constructor:\n \n * Embedding vector at `padding_idx` will be initialized to all zeros\n \n * Embedding vector at `padding_idx` can be manually set to something else after initialization\n \n * Embedding vector at `padding_idx` will not receive gradient updates\n \n * Calling the module with `padding_idx` as an input gives zeros unconditionally (changed accidentally due to #46714)\n \n  * Note that this seems to be where the majority of the backlash is coming from. It used to be the case that the embedding vector at `padding_idx` was always returned, whether it was all zeros or some manually set value, and those are the expected semantics\n \n  * **Ideal behavior**: always return the embedding vector at `padding_idx`, regardless of whether it's all zeros or some manually set value\n \n \n \n `padding_idx` passed to `from_pretrained()`:\n \n * Embedding vector at `padding_idx` will be overwritten with all zeros (changed due to #47184)\n \n  * **Ideal behavior**: do not override the vector with all zeros during loading\n \n * Embedding vector at `padding_idx` can be manually set to something else **after loading**\n \n * Embedding vector at `padding_idx` will not receive gradient updates\n \n * Calling the module with `padding_idx` as an input gives either zeros or the manual value set **after loading** (changed due to #47184)\n \n  * Note that the old behavior allowed for saving / loading a non-zero padding embedding vector. After #47184, if a non-zero padding embedding vector is saved, it will be overwritten with zeros when loaded. As a user, I'd find this surprising, although it is possible to work around by setting a manual value after loading\n \n  * **Ideal behavior**: always return the embedding vector at `padding_idx`, regardless of whether it's all zeros or some manually set value\n \n \n \n `padding_idx` passed to `F.embedding()`:\n \n * Embedding vector at `padding_idx` will not receive gradient updates\n \n * Calling the function with `padding_idx` as an input gives zeros unconditionally (changed due to #46714)\n \n  * Note that the old behavior returned the embedding vector at `padding_idx` regardless of whether or not it was all zeros. After #46714, this changed to always produce zeros\n \n  * **Ideal behavior**: always return the embedding vector at `padding_idx`, regardless of whether it's all zeros or some manually set value\n \n \n \n ### TL;DR\n \n \n \n I think both #46714 and #47184 stray from the ideal `padding_idx` semantics and should not have been introduced. It seems they both came about as a way to reconcile with confusing documentation. Imo `padding_idx` should always have been documented as a way to set a specific embedding vector to a fixed value (default all zeros) that it is not affected by gradient updates.",
      "y": "always return the embedding vector at `padding_idx`, regardless of whether it's all zeros or some manually set value"
   },
   {
      "null": 31,
      "x": "DDP.no_sync() function + torch.cuda.synchronize(device=rank)",
      "z": "Closing issue as this is a question and not a bug report, please use http://discuss.pytorch.org/c/distributed/ for questions instead. Thank you!",
      "y": "calling `backward()` on a loss tensor produced by DDP will always sync grads if not under no_sync() context manager. To manually sync grads, you can use a local model on all ranks and call `dist.all_reduce()` on gradients after backwards pass is completed, but we recommend DDP for performance reasons."
   },
   {
      "null": 32,
      "x": "ImportError: libtinfo.so.5",
      "z": "I can confirm this issue on my Ubuntu. The fix is rather easy, for me I just install the `libncurses5` using:\n \n \n \n ```text\n \n sudo apt install libncurses5\n \n ```\n \n \n \n I wonder what introduced this particular dependency, though. Fact that `libncureses5` is somewhat an \"out-of-data\" version. The newer version is `libncurses6`.",
      "y": "install the `libncurses5` using:\n ```text\n \n sudo apt install libncurses5\n \n ```"
   },
   {
      "null": 33,
      "x": "SIGSEGV in torch.linalg.inv",
      "z": "The problem was that one of the arguments to the LAPACK call was not correct for 0x0 matrices. I submitted a fix for that.\n \n Even though our tests are failing because of 0x0 test cases, PyTorch v1.8.0 compiled with OpenBLAS is expected to work correctly for non-empty inputs.\n \n \n \n The `BLAS=Eigen` option is misleading and it affects only the Caffe2 code and is not used in ATen. https://github.com/pytorch/pytorch/blob/e5ecd1ddf84787789dfdadc8ff8af34a180180fd/cmake/Dependencies.cmake#L124-L126\n \n \n \n The following message is printed with `BLAS=Eigen`:\n \n ```\n \n -- Trying to find preferred BLAS backend of choice: Eigen\n \n CMake Warning at cmake/Dependencies.cmake:175 (message):\n \n  Preferred BLAS (Eigen) cannot be found, now searching for a general BLAS\n \n  library\n \n ```\n \n Then BLAS is searched with `find_package(BLAS)`. https://github.com/pytorch/pytorch/blob/e5ecd1ddf84787789dfdadc8ff8af34a180180fd/cmake/Dependencies.cmake#L174-L176\n \n CMake log is needed to know what BLAS and LAPACK libraries are picked actually. Or at least `print(torch.__config__.show())`, it should contain the line `Build settings: BLAS_INFO=...`.\n \n \n \n I assume OpenBLAS is picked up. I compiled with OpenBLAS 0.3.12 and I see the segfault for 0x0 input. `torch.inverse` is fine because it explicitly returns for this case without relying on the LAPACK library to do that.",
      "y": "one of the arguments to the LAPACK call was not correct for 0x0 matrices"
   },
   {
      "null": 34,
      "x": "Library location assumption in test/jit/test_backends.py, test/jit/test_torchbind.py, and test/test_fx.py",
      "z": "#61960 has been landed and solve this issue with an additional step of installing the torch wheel package",
      "y": "installing the torch wheel package"
   },
   {
      "null": 35,
      "x": "PyTorch 1.8 CUDNN_STATUS_NOT_INITIALIZED",
      "z": "same here, was fine on 1080ti, but broke for 2080ti.",
      "y": "was fine on 1080ti, but broke for 2080ti."
   },
   {
      "null": 36,
      "x": "libtorch 1.8.0 with CUDA 11.1: CUDA error: no kernel image is available for execution..",
      "z": "I downloaded the nighly build instead using CUDA 10.2 and that worked fine. The nightly build with CUDA 11.1 had the same problem that all CUDA based tests failed.",
      "y": "download the nighly build instead using CUDA 10.2"
   },
   {
      "null": 37,
      "x": "The signature of `torch.tensordot` in Python should be compatible with its signature in TorchScript",
      "z": "`tensordot` should be added to https://github.com/pytorch/pytorch/blob/master/torch/jit/_builtins.py#L101\n \n \n \n Also, we should make a pass on which other ops should be added\n \n [Optional] Create a mechanism to remind deves who touch `functional.py` to modify list in `builtins.py` as well.",
      "y": "`tensordot` should be added."
   },
   {
      "null": 38,
      "x": "OpInfo tests for inplace variants use the same sample inputs as the method",
      "z": "Probably a duplicate of https://github.com/pytorch/pytorch/issues/50747",
      "y": "add a new field to SampleInput to notify if self gets broadcasted or not."
   },
   {
      "null": 39,
      "x": "RuntimeError: cuDNN error: CUDNN_STATUS_NOT_SUPPORTED. This error may appear if you passed in a non-contiguous input.",
      "z": "I'll send a PR to take 256 length inputs to the native backend.",
      "y": "take 256 length inputs to the native backend."
   },
   {
      "null": 40,
      "x": "Compiling release/1.5 broken on Ubuntu 18.04",
      "z": "You may need to sync the submodules.\n \n ```bash\n \n git submodule sync\n \n git submodule update --init --recursive\n \n ```",
      "y": "sync the submodules.\n \n ```bash\n \n git submodule sync\n \n git submodule update --init --recursive\n \n ```"
   },
   {
      "null": 41,
      "x": "Sums of expanded and repeated tensors are different",
      "z": "With #39512 \n \n ```\n \n tensor(1.0100e+08) tensor(1.0100)\n \n tensor(1.0012e+08) tensor(1.0012)\n \n tensor(1.0100e+08) tensor(1.0100)\n \n ```",
      "y": "```\n \n tensor(1.0100e+08) tensor(1.0100)\n \n tensor(1.0012e+08) tensor(1.0012)\n \n tensor(1.0100e+08) tensor(1.0100)\n \n ```"
   },
   {
      "null": 42,
      "x": "Bad performance with python threads",
      "z": "#37461 might be good too - we would still want to enable original settings if user uses ATen/Parallel API, it is just we can't do much when user creates a new thread themselves and then directly executes code that uses OpenMP",
      "y": "enable original settings if user uses ATen/Parallel API"
   },
   {
      "null": 43,
      "x": "Torch norm error on gpu tensors",
      "z": "offtopic: why complex conjugate was being called for non-complex tensor? https://github.com/pytorch/pytorch/blob/master/aten/src/ATen/native/LinearAlgebra.cpp#L556. Probably this is what got fixed, right?\n \n \n \n Also I wonder if `sqrt((self*self).sum(dim)` has again numerical instability / gradient blowup at 0 of sqrt and no eps used",
      "y": "`sqrt((self*self).sum(dim)` has again numerical instability / gradient blowup at 0 of sqrt and no eps used"
   },
   {
      "null": 44,
      "x": "torch.cuda.is_available() get false on cuda10",
      "z": "> Removing high priority. 1.5 C47 (10.1 and 10.2 only)\n \n \n \n is this information mentioned somewhere? i was looking for any indication about this in the [release](https://github.com/pytorch/pytorch/releases/tag/v1.5.0) page, and there is none. i just installed pytorch 1.5 using `pip install torch` on a gpu device with cuda 10.0 and pytorch did not detect the gpu. i was suspecting to run to this issue since the pytorch [page](https://pytorch.org/) points to cuda 10.2 to install the latest version.\n \n \n \n if pytorch 1.5 supports only cuda 10.1/2, it would be helpful to mention it in the release page or in the doc. i was trying to upgrade from pytorch 1.4 until i run into this thread.\n \n thanks a lot.\n \n \n \n p.s. it is surprising. this [lib](https://pytorch-geometric.readthedocs.io/en/latest/notes/installation.html) seems to be able to test pytroch 1.5 with cuda 10.0 and pytorch seems to detect the gpu.",
      "y": "pytorch version is available for download, however, it does not support cuda 10"
   },
   {
      "null": 45,
      "x": "copy.deepcopy() breaks when pruning is set on sequential",
      "z": "cc @mickypaganini might be interested in this.",
      "y": "Reparametrizing the network necessarily results in the creation of new parameters in terms of the leaf params.\n Now, I don't know exactly why in deepcopy in tensor.py we explicitly check for leaves-only, and whether that can be relaxed.\n \n Yes, workaround would be to call prune.remove before the deepcopy (but that might not be suitable if the masks or the original parameters are needed), or to copy first and then prune."
   },
   {
      "null": 46,
      "x": "torch.norm is numerically unstable at zero for multidim reductions",
      "z": "I was mistaken! @mrshenli pointed out this issue does appear to be fixed with torch.linalg.norm. I must have failed to save my updated version of the script. Here's the script I'm using:\n \n \n \n ```\n \n import torch\n \n a = torch.zeros(3, 3, 3, requires_grad = True)\n \n print(torch.autograd.grad(torch.linalg.norm(a, dim = (1,)).sum(), (a,))[0])\n \n print(torch.autograd.grad(torch.linalg.norm(a, dim = (1, 2)).sum(), (a,))[0])\n \n print(torch.__version__)\n \n ```\n \n \n \n And the result:\n \n \n \n ```\n \n tensor([[[0., 0., 0.],\n \n  [0., 0., 0.],\n \n  [0., 0., 0.]],\n \n \n \n  [[0., 0., 0.],\n \n  [0., 0., 0.],\n \n  [0., 0., 0.]],\n \n \n \n  [[0., 0., 0.],\n \n  [0., 0., 0.],\n \n  [0., 0., 0.]]])\n \n tensor([[[0., 0., 0.],\n \n  [0., 0., 0.],\n \n  [0., 0., 0.]],\n \n \n \n  [[0., 0., 0.],\n \n  [0., 0., 0.],\n \n  [0., 0., 0.]],\n \n \n \n  [[0., 0., 0.],\n \n  [0., 0., 0.],\n \n  [0., 0., 0.]]])\n \n 1.7.0a0+24eea36\n \n ```\n \n \n \n Since we'll soon be deprecating torch.norm in favor of torch.linalg.norm, I think we can close this issue.",
      "y": "this issue does appear to be fixed with torch.linalg.norm."
   },
   {
      "null": 47,
      "x": "Upgrade NCCL submodule",
      "z": "Want to bump this error.\n \n \n \n \n \n PyTorch Version (e.g., 1.0): master\n \n OS (e.g., Linux): Linux\n \n How you installed PyTorch (conda, pip, source): pip\n \n Build command you used (if compiling from source): N/A\n \n Python version: 3.8\n \n CUDA/cuDNN version: 10.2\n \n GPU models and configuration: V100",
      "y": "Update NCCL from 2.4.8 to 2.7.3."
   },
   {
      "null": 48,
      "x": "Pytorch crashes while training a simple MNIST classification problem",
      "z": "Reproduced locally.\n \n ```cmd\n \n C:\\Users\\peter>pip install torch===1.5.0 torchvision===0.6.0 -f https://download.pytorch.org/whl/torch_stable.html\n \n Looking in indexes: https://pypi.tuna.tsinghua.edu.cn/simple\n \n Looking in links: https://download.pytorch.org/whl/torch_stable.html\n \n Collecting torch===1.5.0\n \n  Downloading https://download.pytorch.org/whl/cu102/torch-1.5.0-cp37-cp37m-win_amd64.whl (899.1MB)\n \n  | | 61kB 75kB/s eta 3:18:16\n \n ERROR: Operation cancelled by user\n \n WARNING: You are using pip version 19.2.3, however version 20.0.2 is available.\n \n You should consider upgrading via the 'python -m pip install --upgrade pip' command.\n \n \n \n C:\\Users\\peter>pip install torch==1.5.0 torchvision==0.6.0 -f https://download.pytorch.org/whl/torch_stable.html\n \n Looking in indexes: https://pypi.tuna.tsinghua.edu.cn/simple\n \n Looking in links: https://download.pytorch.org/whl/torch_stable.html\n \n Collecting torch==1.5.0\n \n  Downloading https://download.pytorch.org/whl/cu92/torch-1.5.0%2Bcu92-cp37-cp37m-win_amd64.whl (693.1MB)\n \n  | | 204kB 211kB/s eta 0:54:42\n \n ERROR: Operation cancelled by user\n \n WARNING: You are using pip version 19.2.3, however version 20.0.2 is available.\n \n You should consider upgrading via the 'python -m pip install --upgrade pip' command.\n \n ```\n \n Would you please fix the install commands? cc @seemethere @soumith",
      "y": "fix the install commands"
   },
   {
      "null": 49,
      "x": "RuntimeError: has_marked_unused_parameters_ INTERNAL ASSERT FAILED at /tmp/pip-req-build-p5q91txh/torch/csrc/distributed/c10d/reducer.cpp:290, please report a bug to PyTorch.",
      "z": "after I modified \"find_unused_parameters=True\" in torch.nn.parallel.DistributedDataParallel(model, device_ids=opt.gpu_ids, find_unused_parameters=True)\n \n it could run",
      "y": "modify \"find_unused_parameters=True\" in torch.nn.parallel.DistributedDataParallel(model, device_ids=opt.gpu_ids, find_unused_parameters=True)"
   },
   {
      "null": 50,
      "x": "In-place leakyReLu backward calculation is triggered with a non-positive slope which is not supported",
      "z": "The slope == 0.0 case is disabled for inplace backward calculation since I've thought the gradient calculation for slope==0 case is not predictable under our autograd infrastructure. We just revisited our infrastructure and confirmed the slope == 0 case can be supported. We will fix this shortly.",
      "y": "The slope == 0.0 case is disabled for inplace backward calculation"
   },
   {
      "null": 51,
      "x": "Unexpected result on c10::ArrayRef",
      "z": "Yeah, this is user error. Unfortunately it's not so easy to get a compiler warning in this situation; it's a general problem with view types in C++. Hopefully the \"Ref\" is enough to tell you that there is a view going on.",
      "y": "it's a general problem with view types in C++"
   },
   {
      "null": 52,
      "x": "Port torch/csrc/jit/runtime/register_prim_ops_c10.cpp to new operator registration API",
      "z": "Sure, I like working on the A54, and a new issue #37579 has been created for the port.",
      "y": "New port for registrations in `register_distributed_ops.cpp`"
   },
   {
      "null": 53,
      "x": "Inconsistent behavior in model.parameters() in Pytorch 1.5.0",
      "z": "I checked the history of parameters on Module and it seems that it has always returned an iterator. So I am not sure how your original code ever worked. But yes, `parameters()` is specified to return an iterator, and iterators are only iterable over once, so this is as expected. If you need to iterate multiple times, call `m.parameters()` multiple times.",
      "y": "`parameters()` is specified to return an iterator, and iterators are only iterable over once"
   },
   {
      "null": 54,
      "x": "Support custom gradient reduction algorithm in DDP",
      "z": "> For different processes using different batch sizes, will we need to resolve #33148 to support uneven batch sizes before doing this?\n \n \n \n Sorry that I didn't state that clearly. No, we don't need to resolve #33148 first. By batch size I mean the number of samples in one batch. The issue tracked in #33148 is for different number of batches/iterations. More specially, if we have 2 DDP processes, where one processing 1 batch and another processing 2 batch, it would hang. But if both of them process 1 batch and one process's batch contains 10 sample and another contains 20 sample, it won't hang. However, the gradient might need to be averaged in a weighted manner depending on the loss function, sth like `(10 * grad1 + 20 * grad2) / 30`.",
      "y": "if we have 2 DDP processes, where one processing 1 batch and another processing 2 batch, it would hang. But if both of them process 1 batch and one process's batch contains 10 sample and another contains 20 sample, it won't hang. However, the gradient might need to be averaged in a weighted manner depending on the loss function, sth like `(10 * grad1 + 20 * grad2) / 30`."
   },
   {
      "null": 55,
      "x": "torch.multinomial behaves abnormally with CUDA tensor",
      "z": "I could give it a try once 'triage review' is dropped.",
      "y": "Both with and withoutReplacement kernels don't properly advance rng state (they move it by 4, whereas each thread is likely generating much more than 4 numbers). For withoutReplacement, it happens when there are multiple distributions, but for withReplacement, all samples are generated in the single kernel, so in addition to ndistributions multiplier there's also nsamples/nwarps multiplier (nwarps = 4)."
   },
   {
      "null": 56,
      "x": "torch.utils.tensorboard.writer.SummaryWriter.add_graph missing documentation",
      "z": "@orionr Correct, tried your fix and it renders like before.",
      "y": "`torch._C._log_api_usage_once(\"tensorboard.logging.add_graph\")` should be below the docstring."
   },
   {
      "null": 57,
      "x": "torchfetch - detect hardware capabilities for PyTorch install",
      "z": "https://github.com/pytorch/pytorch/blob/master/torch/utils/collect_env.py already does some of this",
      "y": "Already done by https://github.com/pytorch/pytorch/blob/master/torch/utils/collect_env.py"
   },
   {
      "null": 58,
      "x": "module.h(483): error: a member with an in-class initializer must be const",
      "z": "I think CONSTEXPR_EXCEPT_WIN_CUDA just needs to be `const` when it's not `constexpr`.",
      "y": "CONSTEXPR_EXCEPT_WIN_CUDA just needs to be `const` when it's not `constexpr`."
   },
   {
      "null": 59,
      "x": "CUDA out of memory in subprocesses spawned by unit tests in Windows",
      "z": "This can be fixed by add 'poll()' after 'communicate()' maybe because the function poll will do job for recycling resources.\n \n But this test can be passed on windows for latest code after https://github.com/pytorch/pytorch/pull/42627, because the communicate() has been replaced by check_output(), then the processes execute serially.\n \n The test is enabled on windows by https://github.com/pytorch/pytorch/pull/42796",
      "y": "can be fixed by add 'poll()' after 'communicate()'"
   },
   {
      "null": 60,
      "x": "master Build failure : undefined reference to `void fbgemm::CodeGenBase<unsigned char, signed char, int, int>::storeCRegs<asmjit::x86::Zmm, 64>(asmjit::x86::Emitter*, int, int, asmjit::x86::Gp, asmjit::x86::Gp, bool)",
      "z": "Duplicate of #42415 . Fixed by https://github.com/pytorch/FBGEMM/pull/396.",
      "y": "Explicitly instantiate `storeCRegs<x86::Zmm, 64>` template"
   },
   {
      "null": 61,
      "x": "NCCL Alltoall Process Group introducing time-out of other NCCL tests",
      "z": "torch_python should never be directly linked with nccl, but rather relies on the function exported from torch_cuda (we already have some wrappers that re-rexport C nccl API as C++ nccl::send nccl::recv and so on...",
      "y": "torch_python should not be directly linked with nccl"
   },
   {
      "null": 62,
      "x": "NCCL operation fails with torch.int16 tensors",
      "z": "@mrshenli Thank you for the clarification. \n \n \n \n I am actually trying to reduce the number of bits communicated during allreduce. Using 32bit int or 16bit floats would increase communication. I will try to find a workaround. \n \n \n \n Anyways I am closing this issue for now.",
      "y": "NCCL's data types don't have 16-bits integer"
   },
   {
      "null": 63,
      "x": "inplace operation error when using nn.ReLU without setting inplace=True",
      "z": "> You must be missing something. The only thing that `model.train()` does is that it changes the `self.traning` flag of the main module(model) and its submodules(nn.Dropout(), etc.). This flag is important in cases where the module's behaviour between the training phase and the evaluation differs (nn.Dropout, nn.BatchNorm, etc.). `nn.ReLU` just ignores that flag.\n \n \n \n Thanks, I will check it!",
      "y": "The only thing that `model.train()` does is that it changes the `self.traning` flag of the main module(model) and its submodules(nn.Dropout(), etc.). This flag is important in cases where the module's behaviour between the training phase and the evaluation differs (nn.Dropout, nn.BatchNorm, etc.). `nn.ReLU` just ignores that flag."
   },
   {
      "null": 64,
      "x": "torch.as_strided segfault when stride is empty tuple",
      "z": "Just checked. It is gone in nightly version. Thanks! I will close this issue",
      "y": "It is gone in nightly version."
   },
   {
      "null": 65,
      "x": "Caffe2 Error: more than one operator \"+\" matches these operands, windows and Cuda 11",
      "z": "Please see https://github.com/pytorch/pytorch/pull/42420/files#diff-9ed210e75701d8760a0b7cc9e11498b3R40.",
      "y": "Add CUDA 11 builds for Windows CI"
   },
   {
      "null": 66,
      "x": "Exception raised in backward() loses backtrace information",
      "z": "Hi,\n \n \n \n Thanks for opening an issue for this (it was mentioned in https://github.com/pytorch/pytorch/issues/41659 but better to have an issue for it).\n \n This comes from the fact that the Future API eats up the original error and only throws a basic std::exception with the original message.\n \n We should change that as our custom error types contain much more info (python error type, cpp stack traces, etc).\n \n \n \n cc @pritamdamania87",
      "y": "Future API eats up the original error and only throws a basic std::exception with the original message.\n \n We should change that as our custom error types contain much more info (python error type, cpp stack traces, etc)."
   },
   {
      "null": 67,
      "x": "View/Reinterpret Tensor as a different type w/o copying",
      "z": "Related: https://github.com/pytorch/pytorch/issues/29013. Also somewhat related are current special view_as_real / view_as_complex (they are partly reinterpret as well, since they change dtype, but not storage)",
      "y": "Current special view_as_real / view_as_complex"
   },
   {
      "null": 68,
      "x": "LSTMCell and GRUCell need autocast patching",
      "z": "I saw your patch for RNNs has been merged (#42385) and many, many thanks for the patch! \n \n \n \n I would be very grateful if you would be so kind to to let me know what's the status on this one?\n \n \n \n PS: while they have a different interface, I know LSTM can be used instead of LSTMCell, but would still like to know about the patch before I start converting the whole code base...",
      "y": "The cudnn RNN API expects weights to occupy a flat buffer in memory with a particular layout. This PR implements a \"speed of light\" fix: _cudnn_rnn_cast_reflatten (the autocast wrapper assigned to _cudnn_rnn) copies weights to the right slices of a flat FP16 buffer with a single read/write per weight (as opposed to casting them to FP16 individually then reflattening the individual FP16 weights, which would require 2 read/writes per weight)."
   },
   {
      "null": 69,
      "x": "pyinstaller exe generated fails to run Faster RCNN model due to runtime exception cannot find nms function",
      "z": "It might be helpful for you issue, https://github.com/pytorch/vision/issues/1899#issuecomment-598200938",
      "y": "patch torchvision 0.2.2 or Keep recent torchvision & patch torch.jit in your entry point"
   },
   {
      "null": 70,
      "x": "`torch.cholesky_solve` free(): invalid pointer, Aborted (core dumped)",
      "z": "I just reported another issue with this API in #42695, please take a look. Apparently, it produces so many kinds of error message with abort, and even segfault. Thank you!",
      "y": "It produces many kinds of error message with abort, and even segfault"
   },
   {
      "null": 71,
      "x": "Linking torch_cpu files if compiled in a path containing whitespace",
      "z": "Tested that it works.",
      "y": "try to surround `$<TARGET_FILE:${SRC}>` with \\\" here:\n pytorch/cmake/public/utils.cmake\n \n Line 55 in 40ac95d\n \n ` ${DST} INTERFACE -WHOLEARCHIVE:$<TARGET_FILE:${SRC}>) `"
   },
   {
      "null": 72,
      "x": "Failed to export an ONNX attribute 'onnx::Div', since it's not constant",
      "z": "Hi ,\n \n The node where you are seeing this error is a \"Pad\" operation. In older versions of ONNX, the Pad operation took the lengths of the paddings as an attribute, i.e. these lengths have to be constant at compile time. in the U-Net model, the lengths of the paddings come from the output of previous nodes in the graph, which is why you could not export the model to ONNX. In version 11, the Pad operation in ONNX changed to take the lengths of the padding as an input, so if you try to export your model using `opset_version=11`, you should be able to export the model to ONNX.",
      "y": "In older versions of ONNX, the Pad operation took the lengths of the paddings as an attribute, i.e. these lengths have to be constant at compile time. in the U-Net model, the lengths of the paddings come from the output of previous nodes in the graph, which is why you could not export the model to ONNX. In version 11, the Pad operation in ONNX changed to take the lengths of the padding as an input, so if you try to export your model using `opset_version=11`, you should be able to export the model to ONNX."
   },
   {
      "null": 73,
      "x": "PyTorch 1.5 failed to import c:\\miniconda3-x64\\envs\\test\\lib\\site-packages\\torch\\lib\\caffe2_nvrtc.dll",
      "z": "Like peterjc123 said, I downloaded additional .dll files from [https://drive.google.com/u/0/uc?id=1injUyo3lnarMgWyRcXqKg4UGnN0ysmuq&export=download](https://drive.google.com/u/0/uc?id=1injUyo3lnarMgWyRcXqKg4UGnN0ysmuq&export=download) and copied them to `C:\\Windows\\System32` and it worked.",
      "y": "download additional .dll files from [https://drive.google.com/u/0/uc?id=1injUyo3lnarMgWyRcXqKg4UGnN0ysmuq&export=download](https://drive.google.com/u/0/uc?id=1injUyo3lnarMgWyRcXqKg4UGnN0ysmuq&export=download) and copy them to `C:\\Windows\\System32`"
   },
   {
      "null": 74,
      "x": "Avoid keeping two copies of gradients (param.grad and buckets) in DDP",
      "z": "I'm wondering how to make param.grad to point to offsets in bucket and meanwhile send out the data to all reduce? are you saying to let bucket content and param.grad to share the same storage? \n \n \n \n I saw comments in codes to avoid that, see \n \n \n \n \" // Assert that the grad tensor and the bucket don't share storage.\n \n  // If they did, we could avoid the copy altogether.\n \n  // The reason for not doing this is that existing code calls\n \n  // `detach_` from `zero_grad`, which is incompatible with views.\"\n \n \n \n But I do not find where 'zero_grad' is called. do you have context about it?",
      "y": "The problem is that the view/inplace logic links the history of all these Tensors together. And so doing a .detach_() would be tricky. For example we allow it for the base even if it has views and this lead to the following weird behavior:\n import torch\n \n a = torch.rand(10, requires_grad=True).clone()\n b = a.narrow(0, 0, 2)\n \n print(\"a: \", a.requires_grad, a.grad_fn)\n print(\"b: \", b.requires_grad, b.grad_fn)\n \n print(\"Detaching a inplace\")\n a.detach_()\n \n print(\"a: \", a.requires_grad, a.grad_fn)\n print(\"b: \", b.requires_grad, b.grad_fn)\n \n print(\"Modifying b inplace\")\n b += 1\n \n print(\"a: \", a.requires_grad, a.grad_fn)\n print(\"b: \", b.requires_grad, b.grad_fn)\n Giving\n \n a: True <CloneBackward object at 0x100e18350>\n b: True <SliceBackward object at 0x100e18350>\n Detaching a inplace\n a: False None\n b: True <SliceBackward object at 0x100e18350>\n Modifying b inplace\n a: True <CopySlices object at 0x100e18390>\n b: True <AsStridedBackward object at 0x100e18390>\n We do detach_ to try and keep references to the old Tensor valid yes. And it is also slightly more efficient if you don't need a new Tensor object to do this as you only set 2 fields."
   },
   {
      "null": 75,
      "x": "PyTorch C++ multithreading support on GPU",
      "z": "Pytorch engine does support multithreading in general, but there are some vaiables that are thread-local, in particular dispatch keys. You error indicates that tensor type dispatch key is not correctly propagated.",
      "y": "dispatch key is not correctly propagated"
   },
   {
      "null": 76,
      "x": "Segmentation fault when building LibTorch 1.5.0 on macOS Catalina",
      "z": "Can you please check if this fix to SobolEngineOps.cpp fixes it for you? \n \n https://github.com/pytorch/pytorch/pull/36711",
      "y": "fix to SobolEngineOps.cpp"
   },
   {
      "null": 77,
      "x": "torch.sum() CPU is much slower for bool/uint8 tensors than int32/float32 tensors",
      "z": "Yes, this sounds like a reasonable fix. Don't forget that uint8 tensors values can be up to 255, so you can safely sum only pretty small tensors. It probably makes sense to limit the fix to bool tensors only. Please don't change type promotion behavior (i.e. the output type by default should be int64, like it is now). Also can you please check if it will fix masked_select performance in #33269, at least for bool masks?",
      "y": "uint8 tensors values can be up to 255, so you can safely sum only pretty small tensors. It probably makes sense to limit the fix to bool tensors only."
   },
   {
      "null": 78,
      "x": "[Poll] Add Windows support to torch.distributed package",
      "z": "Hi all,\n \n The feature is now available https://pytorch.org/tutorials/intermediate/ddp_tutorial.html , It supports supports Gloo backend, FileStore and TcpStore. Could you give it a spin and let us know how did it go?",
      "y": "The feature is now available https://pytorch.org/tutorials/intermediate/ddp_tutorial.html , It supports supports Gloo backend, FileStore and TcpStore."
   },
   {
      "null": 79,
      "x": "torch.isfinite() doesn't work for fp16 on CPU",
      "z": "this should be addressed in the latest nighly build, can you please verify?",
      "y": "addressed in the latest nighly build"
   },
   {
      "null": 80,
      "x": "Nightly failed buiding on OSX with CUDA because of tuple undefined",
      "z": "cudnn 9.0 is no longer supported. Supported cuda versions are 9.2, 10.1, 10.2",
      "y": "cudnn 9.0 is no longer supported."
   },
   {
      "null": 81,
      "x": "Link error, Libtorch 1.5 on Windows",
      "z": "Hi, everyone. \n \n this problem still appears in libtorch version 1.6, while trying to add libtorch c++ cuda to my existing project.\n \n \n \n For future reference, the solution is: \n \n add `/INCLUDE:\"?warp_size@cuda@at@@YAHXZ\"` in Configuration Properties -> Linker -> Command Line -> Additional Options . by reading this comment from @peterjc123 https://github.com/pytorch/pytorch/issues/31611#issuecomment-594383154. use \"/\" not \"-\"\n \n \n \n thank you very much for the help.",
      "y": "add `/INCLUDE:\"?warp_size@cuda@at@@YAHXZ\"` in Configuration Properties -> Linker -> Command Line -> Additional Options"
   },
   {
      "null": 82,
      "x": "Incremental pruning reorders module hooks",
      "z": "could you please take a look? Thanks.",
      "y": "The `Parametrization` framework will handle this issue correctly. Two Pruning methods will only be folded into one if they were applied on the same weight one after the other with no other Parametrizations between them."
   },
   {
      "null": 83,
      "x": "torch.distributions.utils.broadcast_all does not support Tensor-like objects",
      "z": "Feel free to suggest a fix in the form of a PR.",
      "y": "Fix broadcast_all crashing on Tensor-likes"
   },
   {
      "null": 84,
      "x": "torch.cdist produces nan gradients in Pytorch 1.5, but not Pytorch 1.4",
      "z": "A more concise way to reproduce this:\n \n \n \n 1. [Download the tensors](https://figshare.com/s/47d6035b990fa62d730d)\n \n 2. Run the code\n \n ```python\n \n import torch\n \n \n \n emb1, emb2, cdist_grad = torch.load('cdist_grad.pt')\n \n \n \n emb1.retain_grad()\n \n d = torch.cdist(emb1, emb2)\n \n d.backward(cdist_grad)\n \n \n \n print(emb1.grad[0, 17])\n \n ```",
      "y": "Fix cdist backward calculation for p=2"
   },
   {
      "null": 85,
      "x": "CUBLAS_STATUS_EXECUTION_FAILED for ConvTranspose2d backward with FP16 inputs",
      "z": "At the [spot where the error occurs](https://github.com/pytorch/pytorch/blob/master/aten/src/ATen/native/cuda/NaiveConvolutionTranspose2d.cu#L736), the gemv arguments have the expected shape. If replace the gemv call with dummy in-place `add_(0.0)`s applied to the argument tensors, the repro runs end to end under cuda-memcheck without errors, so the argument tensors appear properly allocated.\n \n \n \n Looks more and more like a misconfigured cublas call or an internal cublas error. Tomorrow i'll figure out which.",
      "y": "misconfigured cublas call or an internal cublas error"
   },
   {
      "null": 86,
      "x": "pytorch1.5 does not support CUDA",
      "z": "Running on an EC2 p2 with the Deep Learning Ubuntu AMI and torch.cuda.is_available() was True. Then I pip installed torchvision, which also upgraded torch to 1.5, and now torch.cuda.is_available() is False.",
      "y": "Deep Learning Ubuntu AMI and torch.cuda.is_available() was True. Then I pip installed torchvision, which also upgraded torch to 1.5, and now torch.cuda.is_available() is False."
   },
   {
      "null": 87,
      "x": "Libtorch C++ model predict/forward propagation crashed on windows10, CUDA 10.0, VS 2017 15.7.6 ,RTX 2080, but libtorch C++ works with cpu successfully",
      "z": "Does this make any difference?\n \n \n \n It's the same code but loads directly to GPU instead of to CPU.\n \n \n \n ```\n \n auto module = torch::jit::load(argv[1], torch::kCUDA);\n \n auto inputs = torch::ones({ 1, 3, 224, 224 }, torch::kCUDA);\n \n auto output2 = module->forward({inputs});\n \n ```",
      "y": "```\n \n auto module = torch::jit::load(argv[1], torch::kCUDA);\n \n auto inputs = torch::ones({ 1, 3, 224, 224 }, torch::kCUDA);\n \n auto output2 = module->forward({inputs});\n \n ```"
   },
   {
      "null": 88,
      "x": "libtorch gpu efficiency",
      "z": "CUDA is asynchronous. The time duration you observed is the time needed to forward though the network.\n \n \n \n \n \n \n \n \n \n \n \n \n I also think this is a bug. I convert a pytorch model to cpp w/ and w/o mask_seleced operator. The inference time in cpp will increase about 60ms.\n \n  \n \n You are receiving this because you authored the thread.\n \n Reply to this email directly, view it on GitHub, or mute the thread.",
      "y": "CUDA is asynchronous. The time duration you observed is the time needed to forward though the network."
   },
   {
      "null": 89,
      "x": "[JIT] isinstance(m, nn.Linear) returns False in ScriptModules",
      "z": "workaround: \n \n ```\n \n linear = nn.Linear()\n \n # do stuff to it\n \n self.linear = linear\n \n ```\n \n An solution to this as pointed out is \"add another metaclass with an __instancecheck__ it looks like to make the isinstance check work right\", we would love to accept a PR for it so making it low pri for now.",
      "y": "add another metaclass with an __instancecheck__ it looks like to make the isinstance check work right"
   },
   {
      "null": 90,
      "x": "Linking torch libraries and yaml-cpp gives undefined reference to yaml-cpp libraries",
      "z": "I've experienced the same issue recently, having a complex project with yarp, gazebo, cuda and fcl. Project was compiling, but adding ${TORCH_LIBRARIES} in cmake's target_link_libraries resulted in multiple \"undefined references\" from fcl and yarp. Notably, all problematic functions had std::string in the signature. \n \n Solution that worked for me: downloading cxx11ABI libtorch (cpu version) from https://pytorch.org/get-started/locally/ (current link - https://download.pytorch.org/libtorch/cpu/libtorch-cxx11-abi-shared-with-deps-1.5.1%2Bcpu.zip), and replacing initially installed version (seems to me that by default users still download pre-cxx11abi release).\n \n \n \n Hope that helps.",
      "y": "downloading cxx11ABI libtorch (cpu version) from https://pytorch.org/get-started/locally/ (current link - https://download.pytorch.org/libtorch/cpu/libtorch-cxx11-abi-shared-with-deps-1.5.1%2Bcpu.zip), and replacing initially installed version (seems to me that by default users still download pre-cxx11abi release)."
   },
   {
      "null": 91,
      "x": "Provide a context manager to silence PyTorch exporter warning",
      "z": "As stated in the onnxruntime issue you might trip the assert when different input shapes are used, so be careful. Closing since ignoring warnings with just the `warning` module is pretty straightforward, you can do something like this to get rid of the `TracerWarning`s:\n \n \n \n ```python\n \n import warnings\n \n with warnings.catch_warnings():\n \n  warnings.filterwarnings(\n \n  action='ignore',\n \n  category=torch.jit.TracerWarning,\n \n  module=r'.*'\n \n  )\n \n  \n \n  assert x.shape[0] > 2, \"checking\"\n \n ```",
      "y": "you might trip the assert when different input shapes are used, so be careful. Ignoring warnings with just the `warning` module is pretty straightforward, you can do something like this to get rid of the `TracerWarning`s:\n \n \n \n ```python\n \n import warnings\n \n with warnings.catch_warnings():\n \n  warnings.filterwarnings(\n \n  action='ignore',\n \n  category=torch.jit.TracerWarning,\n \n  module=r'.*'\n \n  )\n \n  \n \n  assert x.shape[0] > 2, \"checking\"\n \n ```"
   },
   {
      "null": 92,
      "x": "ONNX export regression: examples/fast_neural_style fails to export (pytorch_nightly)",
      "z": "I will take a look.",
      "y": "The issue here seems to be with upsample upon export."
   },
   {
      "null": 93,
      "x": "nn.Linear module weight initialization does not match the documentation",
      "z": "We cant assign it to you in the UI (github doesnt' allow), but just assume you've been assigned.",
      "y": "nn.linear module weight initialization fix"
   },
   {
      "null": 94,
      "x": "Sending small model to GPU leads to 2GB increase in RAM",
      "z": "Loading CUDNN for the first time will likely load a lot of stuff in to RAM. It's the massive libraries (CUDNN is about 500MB alone) as well as some scratch stuff and kernels. This should be a once off occurance.\n \n \n \n If you then load another model after this, how much does your RAM increase?",
      "y": "It's the massive libraries (CUDNN is about 500MB alone) as well as some scratch stuff and kernels. This should be a one off occurance."
   },
   {
      "null": 95,
      "x": "Headers use 'slots' keyword which conflict with QT code.",
      "z": "If you don't want to do the text replacement, something like:\n \n ```\n \n #undef slots\n \n #include \"torch/torch.h\"\n \n #def slots Q_SLOTS\n \n ```\n \n is apparently what the QT documentation recommends. \n \n \n \n I'll put up a PR to remove `slot` and `slots` from ivalue.h but it will be hard to guard regressions like this in the future, so the above might be more robust.",
      "y": "text replacement, or:\n \n ```\n \n #undef slots\n \n #include \"torch/torch.h\"\n \n #def slots Q_SLOTS\n \n ```"
   },
   {
      "null": 96,
      "x": "pip install does not work",
      "z": "Hey I'm trying this as well and i get the same error if I use pytorch or torch. with pip or pip3\n \n \n \n I get this error \n \n > ERROR: Failed building wheel for torch\n \n > Running setup.py clean for torch\n \n > ERROR: Command errored out with exit status 1:\n \n > command: 'c:\\users\\visse\\appdata\\local\\programs\\python\\python37-32\\python.exe' -u -c 'import >sys, setuptools, tokenize; sys.argv[0] = '\"'\"'C:\\\\Users\\\\visse\\\\AppData\\\\Local\\\\Temp\\\\pip-install->yavfiact\\\\torch\\\\setup.py'\"'\"'; __file__='\"'\"'C:\\\\Users\\\\visse\\\\AppData\\\\Local\\\\Temp\\\\pip-install->yavfiact\\\\torch\\\\setup.py'\"'\"';f=getattr(tokenize, '\"'\"'open'\"'\"', open)(__file__);code=f.read().replace('\"'\"'\\r\\n'\"'\"', '\"'\"'\\n'\"'\"');f.close();exec(compile(code, __file__, >'\"'\"'exec'\"'\"'))' clean --all\n \n  > cwd: C:\\Users\\visse\\AppData\\Local\\Temp\\pip-install-yavfiact\\torch\n \n  >Complete output (2 lines):\n \n > running clean\n \n > error: [Errno 2] No such file or directory: '.gitignore'\n \n >\n \n  > ERROR: Failed cleaning build dir for torch\n \n >Failed to build torch\n \n >Installing collected packages: torch\n \n > Running setup.py install for torch: started\n \n > Running setup.py install for torch: finished with status 'error'\n \n > ERROR: Command errored out with exit status 1:\n \n > command: 'c:\\users\\visse\\appdata\\local\\programs\\python\\python37-32\\python.exe' -u -c >'import sys,setuptools, tokenize; sys.argv[0] = '\"'\"'C:\\\\Users\\\\visse\\\\AppData\\\\Local\\\\Temp\\\\pip->install-yavfiact\\\\torch\\\\setup.py'\"'\"'; __file__='\"'\"'C:\\\\Users\\\\visse\\\\AppData\\\\Local\\\\Temp\\\\pip->install-yavfiact\\\\torch\\\\setup.py'\"'\"';f=getattr(tokenize, '\"'\"'open'\"'\"', open)(__file__);code=f.read().replace('\"'\"'\\r\\n'\"'\"', '\"'\"'\\n'\"'\"');f.close();exec(compile(code, __file__, >'\"'\"'exec'\"'\"'))' install --record 'C:\\Users\\visse\\AppData\\Local\\Temp\\pip-record-8cw6stqy\\install->record.txt' --single-version-externally-managed --compile\n \n  cwd: C:\\Users\\visse\\AppData\\Local\\Temp\\pip-install-yavfiact\\torch\\\n \n  Complete output (23 lines):\n \n  running install\n \n  running build_deps\n \n  Traceback (most recent call last):\n \n  File \"<string>\", line 1, in <module>\n \n  File \"C:\\Users\\visse\\AppData\\Local\\Temp\\pip-install-yavfiact\\torch\\setup.py\", line 265, in <module>\n \n  description=\"Tensors and Dynamic neural networks in Python with strong GPU acceleration\",\n \n  File \"c:\\users\\visse\\appdata\\local\\programs\\python\\python37-32\\lib\\site-packages\\setuptools\\__init__.py\", line 145, in setup\n \n  return distutils.core.setup(**attrs)\n \n  File \"c:\\users\\visse\\appdata\\local\\programs\\python\\python37-32\\lib\\distutils\\core.py\", line 148,in setup\n \n  dist.run_commands()\n \n  File \"c:\\users\\visse\\appdata\\local\\programs\\python\\python37-32\\lib\\distutils\\dist.py\", line 966,in run_commands\n \n  self.run_command(cmd)\n \n  File \"c:\\users\\visse\\appdata\\local\\programs\\python\\python37-32\\lib\\distutils\\dist.py\", line 985,in run_command\n \n  cmd_obj.run()\n \n  File \"C:\\Users\\visse\\AppData\\Local\\Temp\\pip-install-yavfiact\\torch\\setup.py\", line 99, in run\n \n  self.run_command('build_deps')\n \n  File \"c:\\users\\visse\\appdata\\local\\programs\\python\\python37-32\\lib\\distutils\\cmd.py\", line 313, in run_command\n \n  self.distribution.run_command(command)\n \n  File \"c:\\users\\visse\\appdata\\local\\programs\\python\\python37-32\\lib\\distutils\\dist.py\", line 985,in run_command\n \n  cmd_obj.run()\n \n  File \"C:\\Users\\visse\\AppData\\Local\\Temp\\pip-install-yavfiact\\torch\\setup.py\", line 51, in run\n \n  from tools.nnwrap import generate_wrappers as generate_nn_wrappers\n \n  ModuleNotFoundError: No module named 'tools.nnwrap'\n \n ERROR: Command errored out with exit status 1: 'c:\\users\\visse\\appdata\\local\\programs\\python\\python37-32\\python.exe' -u -c 'import sys, setuptools, tokenize; sys.argv[0] = '\"'\"'C:\\\\Users\\\\visse\\\\AppData\\\\Local\\\\Temp\\\\pip-install-yavfiact\\\\torch\\\\setup.py'\"'\"'; __file__='\"'\"'C:\\\\Users\\\\visse\\\\AppData\\\\Local\\\\Temp\\\\pip-install-yavfiact\\\\torch\\\\setup.py'\"'\"';f=getattr(tokenize, '\"'\"'open'\"'\"', open)(__file__);code=f.read().replace('\"'\"'\\r\\n'\"'\"', '\"'\"'\\n'\"'\"');f.close();exec(compile(code, __file__, '\"'\"'exec'\"'\"'))' install --record 'C:\\Users\\visse\\AppData\\Local\\Temp\\pip-record-8cw6stqy\\install-record.txt' --single-version-externally-managed --compile Check the logs for full command output.`",
      "y": "get the same error if use pytorch or torch. with pip or pip3"
   },
   {
      "null": 97,
      "x": "Option to enable/disable autograd.profiler, move Chrome trace processing to C++",
      "z": "Yeah, I think it would help us a lot if you could give an example where it's not easy to restructure the code so that you trace only one iteration.",
      "y": "not easy to restructure the code so that you trace only one iteration."
   },
   {
      "null": 98,
      "x": "Document that autograd::Profiler::RecordFunction is available in Python",
      "z": "> because I doubt anyone imported it as an unqualified name\n \n \n \n true. but from experience, it's worth it now even just to avoid the drive by contributions from people who run style checkers and think it's really important to not shadow builtins. for numpy and scipy we have the same discussions about the same functions about once every 1-2 years ...",
      "y": "avoid the drive by contributions from people who run style checkers and think it's really important to not shadow builtins. for numpy and scipy we have the same discussions about the same functions"
   },
   {
      "null": 99,
      "x": "DistributedDataParallelTest .test_dist_broadcast_coalesced_gloo is flaky",
      "z": "Found the culprit in #19183. This is now broken on master.",
      "y": "Remove usages of TypeID"
   },
   {
      "null": 100,
      "x": "How install old version pytorch 0.4.1 from source?",
      "z": "Sorry the 0.4.1 source build is broken. IIRC, the nervanagpu submodule is optional. Here are steps to build:\n \n \n \n 1. `git clone --branch v0.4.1 https://github.com/pytorch/pytorch.git pytorch-0.4.1`\n \n 2. `cd pytorch-0.4.1`\n \n 3. `git rm --cached third_party/nervanagpu`\n \n 4. Delete these lines:\n \n \n \n https://github.com/pytorch/pytorch/blob/v0.4.1/.gitmodules#L19-L21\n \n \n \n 5. `git submodule update --init --recursive`\n \n 6. `python setup.py install`\n \n \n \n Be aware that git submodules have some weird state, so if you run into trouble I would recommend deleting your checkout and starting again.",
      "y": "the nervanagpu submodule is optional. Here are steps to build:\n \n \n \n 1. `git clone --branch v0.4.1 https://github.com/pytorch/pytorch.git pytorch-0.4.1`\n \n 2. `cd pytorch-0.4.1`\n \n 3. `git rm --cached third_party/nervanagpu`\n \n 4. Delete these lines:\n \n \n \n https://github.com/pytorch/pytorch/blob/v0.4.1/.gitmodules#L19-L21\n \n \n \n 5. `git submodule update --init --recursive`\n \n 6. `python setup.py install`"
   },
   {
      "null": 101,
      "x": "torch.potri example doesn't work because cholesky defaults to lower triangle and potri defaults to upper",
      "z": "Thank you for opening this issue. I will be sending in a PR soon that renames `potri` to `cholesky_inverse` where the `upper` argument will default to `False` to remain consistent with other `cholesky` functions (`cholesky` and `cholesky_solve`).",
      "y": "rename `potri` to `cholesky_inverse` where the `upper` argument will default to `False` to remain consistent with other `cholesky` functions"
   },
   {
      "null": 102,
      "x": "Sparse tensor creation ignores indices placement",
      "z": "I find `torch.sparse.FloatTensor' to be a good walk around.",
      "y": "`torch.sparse.FloatTensor'"
   },
   {
      "null": 103,
      "x": "Boolean tensor transpose bug in 1.3.0",
      "z": "As discussed in other reports, this breaks potentially a lot of code, including our own examples.",
      "y": "Compute correct strides after type promotion"
   },
   {
      "null": 104,
      "x": "Inconsistent results from `mask == 0`",
      "z": "Sure, could you just rename it to state that the issue is with the `mask == 0` line and has nothing to do with the masked_fill function?",
      "y": "issue is with the `mask == 0` line"
   },
   {
      "null": 105,
      "x": "Scripting module with type annotations and torch.jit.ignore fails",
      "z": "This was fixed by https://github.com/pytorch/pytorch/pull/29300",
      "y": "Use real argument names for Python functions"
   },
   {
      "null": 106,
      "x": "Conversion to .bfloat16() makes require_grad False",
      "z": "Yes. Adam says I should add complex. Would doing so and adding a comment to is complex that it means cont be acceptable to you?",
      "y": "should add complex"
   },
   {
      "null": 107,
      "x": "Unexpected behaviour of torch.BoolTensor on `not` operator",
      "z": "Hi,\n \n \n \n This is expected behavior. Constructors with capital letters (torch.FloatTensor, torch.BoolTensor etc) give you uninitialized memory. So The content could be anything.\n \n In some sense, not undefined is also undefined.",
      "y": "Constructors with capital letters (torch.FloatTensor, torch.BoolTensor etc) give you uninitialized memory."
   },
   {
      "null": 108,
      "x": "torch.mm seems to be broken. It doesn't enforce the dimensionality of out tensor",
      "z": "So I don't think this specific issue is high priority.\n \n \n \n And we'd need to get general agreement on the semantics before pursuing a fix.",
      "y": "The semantic of out= is definitely that it will resize if needed.\n The point of this api is to be able to reuse existing Tensors as buffers to reduce memory usage if needed."
   },
   {
      "null": 109,
      "x": "masked_scatter change elements even the the self and source tensor are of same values",
      "z": "I think your expectation about `masked_scatter` is not correct here and it does not (necessarily) satisfy the equation you showed `t.masked_scatter(mask, t) == t`.\n \n If carefully read documentation (of [masked_scatter_](https://pytorch.org/docs/stable/tensors.html#torch.Tensor.masked_scatter_)), it says:\n \n \n \n > The mask operates on the `self` tensor, not on the given `source` tensor.\n \n \n \n which means, for example, it does something like this:\n \n \n \n ```python\n \n import torch\n \n self = torch.tensor([1, 2, 3, 4, 5, 6, 7])\n \n mask = torch.tensor([1, 0, 0, 1, 0, 1, 1], dtype=torch.bool)\n \n src = torch.tensor([100, 200, 300, 400]) # src entry is picked up just \"from left to right\"\n \n \n \n self.masked_scatter(mask, src)\n \n # => tensor([100, 2, 3, 200, 5, 300, 400])\n \n ```\n \n \n \n So, I believe, it's only required that:\n \n \n \n - `mask.sum() <= src.numel()` (i.e. number of true entries in `mask` <= number of element of `src`)\n \n - `mask.shape() == self.shape()` (or `mask` is broadcast-able to `self`)\n \n \n \n For the reference, it's implemented like this (for cpu):\n \n \n \n https://github.com/pytorch/pytorch/blob/5da932ad728ed61f3fe09ccb7dfa8a4795a45efb/aten/src/TH/generic/THTensorEvenMoreMath.cpp#L241-L267\n \n \n \n Anyway, I hope my example helps you.",
      "y": "The mask operates on the `self` tensor, not on the given `source` tensor."
   },
   {
      "null": 110,
      "x": "[ONNX] incorrect export of opset10 slice with dynamic_axis",
      "z": "This warning is at autograd level for slice indexing, not exporter directly. But we might be able to remove it now. I'll check that. I think we can close the issue.",
      "y": "warning is at autograd level for slice indexing, not exporter directly"
   },
   {
      "null": 111,
      "x": "argmax for half datatype.",
      "z": "This is serious, we'll fix it.",
      "y": "``` if (iter.dtype(1) == kHalf) {\n  argmax_kernel_cuda_impl<at::Half, float>(iter);```"
   },
   {
      "null": 112,
      "x": "Python crashes after repeatedly calling `refine_names` to get named tensors",
      "z": "nvm, I just manually increase refcount for None after each refine_names call, similar to this https://stackoverflow.com/questions/17732816/is-there-any-way-to-manually-decrease-the-reference-count-of-an-object-in-python.",
      "y": "increase refcount for None after each refine_names call"
   },
   {
      "null": 113,
      "x": "GPU Memory Leak when using lambdas in nn.Modules",
      "z": "Thank you for the fast reply and for the hints. \n \n You are right, when using gc.collect() and empty_cache the gpu memory is freed. So, I guess there is no bug.",
      "y": "when using gc.collect() and empty_cache the gpu memory is freed"
   },
   {
      "null": 114,
      "x": "[type promotion] torch.exp does not work on integral types",
      "z": "I think we definitely want to support this. NumPy already has this, although I'm not sure precisely what algorithm they use for determining the output dtype.",
      "y": "NumPy already has this"
   },
   {
      "null": 115,
      "x": "Error Exporting Quantized model to ONNX",
      "z": "Any plan to support that?",
      "y": "We currently don't support ONNX for the quantized kernels."
   },
   {
      "null": 116,
      "x": "[JIT] `iterator expression` is expected to accept `Tensor`",
      "z": "I have a PR for this but it's not landed yet. This is a dup of https://github.com/pytorch/pytorch/issues/27255, closing.",
      "y": "iterator expression is expected to accept Tensor"
   },
   {
      "null": 117,
      "x": "pytorch 1.3, addition of different data typed tensors after permute giving incorrect results",
      "z": "> Why was this issue closed? I don't believe 1.3.1 has been released?\n \n \n \n Fixed on master. See here for 1.3.1 release tracking:\n \n https://github.com/pytorch/pytorch/issues/28919",
      "y": "fixed on master.\n ```\n >>> t1+m\n tensor([[2., 6.],\n  [4., 8.]]) ```"
   },
   {
      "null": 118,
      "x": "warn if user tries to build from source on windows & python 2.7",
      "z": "We should probably have a runtime warning if one runs on Windows under python 2.7",
      "y": "runtime warning if one runs on Windows under python 2.7"
   },
   {
      "null": 119,
      "x": "CUDA error: no kernel image is available for execution on the device Error from operator: output",
      "z": "Remove the quotation marks and try again.\n \n That is:\n \n ```cmd\n \n set TORCH_CUDA_ARCH_LIST=6.1\n \n ```",
      "y": "Remove the quotation marks"
   },
   {
      "null": 120,
      "x": "[pytorch] [feature request] torch.eye_like",
      "z": "I am sending in a PR.",
      "y": "rather not include this function as a built-in because it's different enough from the other _like functions (which work across all shapes) to be misleading. If we were to support N-dimensional eye(s) then it would be okay, but I don't think we have any rank 2+ functions defined for which identity makes sense."
   },
   {
      "null": 121,
      "x": "ModuleNotFoundError: No module named 'peachpy.x86_64.avx'",
      "z": "Are you submodules fully updated? Use `git submodule update --init --recursive` to update them.\n \n \n \n We are trying to keep the exact same installation instructions working as we integrate Caffe2 into Pytorch. For example, `python setup.py install` will still install Pytorch correctly. If you also want to build Caffe2, then you can use `FULL_CAFFE2=1 python setup.py install` to build all of Caffe2 along with Pytorch.",
      "y": "Use `git submodule update --init --recursive` to update submodules."
   },
   {
      "null": 122,
      "x": "DLL load failed: The specified module could not be found. After installed using pip in python 3.5 windows.",
      "z": "Hello. I experiencing this issue only on one of my window systems. I think my problem might be visual studio related but no Idea what else to do to fix it. I first tried installing the VC Redist package. Then, I installed the community edition for the Visual Studio 2017. However, I am still getting:\n \n \n \n C:\\Users\\Mauricio>where api-ms-win-crt-utility-l1-1-0.dll\n \n INFO: Could not find files for the given pattern(s).\n \n \n \n When I try the dependency walker on the \"_C.cp35-win_amd64.pyd\", it marks that all references are correct and the \"api-ms-win-crt-utility-l1-1-0.dll\" is being mapped to \"C:\\Windows\\system32\\ucrtbase.dll\", but on the python console I am still getting the following error:\n \n \n \n  from torch._C import *\n \n ImportError: DLL load failed: The specified procedure could not be found.\n \n \n \n I even moved from the CUDA 9.2 to cpu-only to reduce the dependencies, but still no luck. Any suggestions?",
      "y": "problem might be visual studio related"
   },
   {
      "null": 123,
      "x": "Got CUDNN_STATUS_NOT_INITIALIZED although PyTorch recognizes CUDA & CuDNN",
      "z": "I wouldn't rule out driver upgrade solving the issue.",
      "y": "driver upgrade solving the issue."
   },
   {
      "null": 124,
      "x": "concat tensor",
      "z": "You can specify the dimension where to concatenate the tensors.\n \n ```python\n \n a = torch.rand(3, 3)\n \n b = torch.rand(3, 6)\n \n c = torch.cat((a, b), dim=1)\n \n ```\n \n \n \n Also, questions like those are better suited for https://discuss.pytorch.org/",
      "y": "``python\n \n a = torch.rand(3, 3)\n \n b = torch.rand(3, 6)\n \n c = torch.cat((a, b), dim=1)\n \n ```"
   },
   {
      "null": 125,
      "x": "[feature request] Low-discrepancy quasi-random sampler (Sobol sequences)",
      "z": "Sorry for the delay. I did go through the CPython implementation, and I think this can be ported into ATen. I should be able to start working on it in a couple days and will keep you posted.",
      "y": "this can be ported into Aten"
   },
   {
      "null": 126,
      "x": "[feature request] Multivariate Gamma function",
      "z": "`ATen/native/random.cpp` if you are adding rng functions, and `ATen/native/misc.cpp` otherwise, probably?",
      "y": "`ATen/native/random.cpp` if you are adding rng functions, and `ATen/native/misc.cpp` otherwise"
   },
   {
      "null": 127,
      "x": "tensor.chunk returns wrong number of chunks",
      "z": "I can work on this. But first, I want to address how to deal with deprecation. Since we want a function whose behavior matches `numpy.array_split`, I suggest that we create a new function called `torch.tensor_split`, and then we can deprecate the `torch.chunk` method (or keep it, if there's any good reason). Does that plan sound good?",
      "y": "Since we want a function whose behavior matches `numpy.array_split`, I suggest that we create a new function called `torch.tensor_split`, and then we can deprecate the `torch.chunk` method"
   },
   {
      "null": 128,
      "x": "[pytorch] Intel MKL ERROR when doing torch.eig on a CUDA tensor",
      "z": "btw, just tried on AWS DLAMI (with MKL + PyTorch configured) and the crash isn't there anymore\n \n \n \n ```\n \n import os, torch, gzip\n \n os.system('wget https://github.com/pytorch/pytorch/files/2189021/S.pt7.gz')\n \n \n \n a = torch.load(gzip.open('S.pt7.gz'))\n \n \n \n print(a.device)\n \n # cuda:0\n \n \n \n a.eig()\n \n ```",
      "y": "tried on AWS DLAMI (with MKL + PyTorch configured) and the crash isn't there anymore"
   },
   {
      "null": 129,
      "x": "backward pass different behaviors with inplace operation",
      "z": "in the second case, b is the result of `1 / x`, and the derivative wrt `x` is `-1 / x^2`, which does not require `b` in the formula. In this case, if you make `c = ((-a).exp()+1); b = 1 / c` and then do an in-place operation on `c`, the backward will complain the same.",
      "y": "Dependant variables cause issue."
   },
   {
      "null": 130,
      "x": "[pytorch] [feature request] Pairwise distances between all points in a set (a true pdist)",
      "z": "> Reading the above thread has not made it clear to me what the currently best feasible solution is for batched pairwise distance. Can maybe someone who understands the details discussed summarize the thread and provide the current best way to code the function `parwise_dist` as used below?\n \n > \n \n > ```\n \n > X = torch.from_numpy(np.random.normal(size=(B, N, D)))\n \n > Y = torch.from_numpy(np.random.normal(size=(B, M, D)))\n \n > parwise_dist(X, Y) # Should be B x N x M\n \n > ```\n \n \n \n This issue has been addressed in the latest version of Pytorch 1.1.0. The documentation for it is still not up in the pytorch docs but you can see `torch.cdist` added in the release notes: https://github.com/pytorch/pytorch/releases/tag/v1.1.0",
      "y": "> ```\n \n > X = torch.from_numpy(np.random.normal(size=(B, N, D)))\n \n > Y = torch.from_numpy(np.random.normal(size=(B, M, D)))\n \n > parwise_dist(X, Y) # Should be B x N x M\n \n > ```"
   },
   {
      "null": 131,
      "x": "[jit][script] support tensor.expand([-1,-1,10,-1,-1])",
      "z": "It sounds like the conclusion is that the JIT should have better support for this",
      "y": "JIT should have better support for this"
   },
   {
      "null": 132,
      "x": "[jit] Can not pickle torch.futures.Future",
      "z": "that error message is actually part of the test, and is an expected log message when that test runs, if you look at the log, the test actually passed and just logged that error message. It is a bit confusing since Dr. CI picks it up as the failure reason. It looks like the actual CI error in that PR is coming from:\n \n \n \n ```\n \n Aug 17 18:06:29 ======================================================================\n \n Aug 17 18:06:29 ERROR [61.826s]: test_backward_ddp_inside (__main__.ProcessGroupDdpUnderDistAutogradTestWithSpawn)\n \n Aug 17 18:06:29 ----------------------------------------------------------------------\n \n Aug 17 18:06:29 Traceback (most recent call last):\n \n Aug 17 18:06:29 File \"/Users/distiller/workspace/miniconda3/lib/python3.7/site-packages/torch/testing/_internal/common_distributed.py\", line 223, in wrapper\n \n Aug 17 18:06:29 self._join_processes(fn)\n \n Aug 17 18:06:29 File \"/Users/distiller/workspace/miniconda3/lib/python3.7/site-packages/torch/testing/_internal/common_distributed.py\", line 330, in _join_processes\n \n Aug 17 18:06:29 self._check_return_codes(elapsed_time)\n \n Aug 17 18:06:29 File \"/Users/distiller/workspace/miniconda3/lib/python3.7/site-packages/torch/testing/_internal/common_distributed.py\", line 363, in _check_return_codes\n \n Aug 17 18:06:29 raise RuntimeError(error)\n \n Aug 17 18:06:29 RuntimeError: Processes 5 exited with error code 10\n \n Aug 17 18:06:29 \n \n Aug 17 18:06:29 --------------------------------------------------------------------\n \n ```\n \n \n \n Which looks like it is a known flaky test: https://github.com/pytorch/pytorch/issues/40434",
      "y": "Flaky test passed but error occurs."
   },
   {
      "null": 133,
      "x": "backward of torch.repeat slower than for torch.repeat_interleave",
      "z": "Regardless of a particular case, backward of `repeat` is inefficient due to https://github.com/pytorch/pytorch/blob/e7564b076c13325fba1704d9d07844a26041f10f/torch/csrc/autograd/FunctionsManual.cpp#L645. `sum_tensorlist` is inefficiently implemented as a sequential sum. `grad` is guaranteed to be evenly split into necessary-size pieces, so instead of chunk + sum_tensorlist `grad` tensor can be reshaped and `sum` can be called on it, that should give similar performance to`expand`.",
      "y": "backward of `repeat` is inefficient"
   },
   {
      "null": 134,
      "x": "TorchScript sets requires_grad to True under some circumstances",
      "z": "This only occurs with the legacy executor, which will no longer be on by default in the 1.7 release so this is low-pri",
      "y": "no longer be on by default in the 1.7 release"
   },
   {
      "null": 135,
      "x": "[JIT] Unable to cast Python instance to C++ type (compile in debug mode for details)",
      "z": "Looks like the error is coming from pybind11. I am not entirely sure how to enable debug mode, but I am fairly sure more info can be exposed if change this [line](https://github.com/pybind/pybind11/blob/dabbbf315d61d71e9d0a2cb68e0b9a86e4204459/include/pybind11/cast.h#L1707), more info will be displayed.\n \n \n \n Also, if you build Pytorch with \"DEBUG=1\" environment variable, you can use a c++ debugger to step into problematic code and get more info.",
      "y": "if you build Pytorch with \"DEBUG=1\" environment variable, you can use a c++ debugger to step into problematic code and get more info."
   },
   {
      "null": 136,
      "x": "Link error in windows build",
      "z": "cc malfet",
      "y": "In visual studio project settings, it's not necessary to add quota for the directory."
   },
   {
      "null": 137,
      "x": "F.mse_loss(a, b, reduction='elementwise_mean') value is incorrect and doesn't show deprecation warning when 2nd argument requires gradient",
      "z": "This has been fixed by https://github.com/pytorch/pytorch/pull/44437 and other PRs in the stack fixed similar issues for other loss functions.",
      "y": "modify the MSELoss CriterionTests to verify that the target derivative is checked."
   },
   {
      "null": 138,
      "x": "\u00e3\u20ac",
      "z": "I found out the solution!!\n \n \n \n Because our platform used IPonIB with Infiniband devices, then we need to set RDMA port by $ export NCCL_IB_HCA=mlx5_0, otherwise some DOWN state IB ports will be used by NCCL...",
      "y": "set RDMA port by $ export NCCL_IB_HCA=mlx5_0, otherwise some DOWN state IB ports will be used by NCCL"
   },
   {
      "null": 139,
      "x": "What dose _ctx in UniqueVoidPtr do?",
      "z": "In some cases, the `data_` pointer does not directly give you enough information to deallocate it; for example, suppose that you are pointing at some data owned by a dlpack structure; you are not allowed to directly deallocate the data, you have to deallocate the dlpack struct. Context lets you point to some side channel data that gives you enough information to deallocate it.",
      "y": "you are not allowed to directly deallocate the data, you have to deallocate the dlpack struct."
   },
   {
      "null": 140,
      "x": "Add IsEmpty() to the at::Tensor in libtorch",
      "z": "Why not `t.numel() == 0`?",
      "y": "`t.numel() == 0`"
   },
   {
      "null": 141,
      "x": "Neon intrinsic types issue",
      "z": "a patch from sebpop to address the [missing intrinsics in GCC7 ](https://github.com/pytorch/pytorch/pull/43683)",
      "y": "address the missing intrinsics"
   },
   {
      "null": 142,
      "x": "torch-1.6.0 contains AVX instruction in the default codepath",
      "z": "Probably can fix this by removing all of the inline methods from TensorIterator. Not sure how to fix this at a deeper level though.",
      "y": "remove all of the inline methods from TensorIterator."
   },
   {
      "null": 143,
      "x": "Question, why Data moving to CUDA is slightly different",
      "z": "Because `model` is a 'container' for parameters/tensors. `model.to()` will loop over all parameters and call `.to()` on those parameters. A simplified version of how `to()` is implemented for a model is this:\n \n ```python\n \n for key, param in self._parameters.items():\n \n  self._parameters[key] = param.to(device)\n \n ```\n \n \n \n I don't see a way around this. The model will get modified in-place since it is just a container. Note though that you can still do `model = model.to(device)` so that your code will be consistent.",
      "y": "Because `model` is a 'container' for parameters/tensors."
   },
   {
      "null": 144,
      "x": "Proposal: Generic Triplet-Margin Loss",
      "z": "Update from offline conversation :\n \n \n \n This seems like a cool improvement on an existing module, but the work to implement it in core might be significant. If it's pursued, it's recommend to start by prototyping the C++ changes required in a BC-preserving way.",
      "y": "work to implement it in core might be significant. If it's pursued, it's recommend to start by prototyping the C++ changes required in a BC-preserving way."
   },
   {
      "null": 145,
      "x": "Possible return value bug when a function is registered in boxed form and called unboxed",
      "z": "not a bad question at all, this code is tricky and the use-case is very particular. The motivation is explained in [this comment](https://github.com/pytorch/pytorch/blob/master/aten/src/ATen/core/boxing/impl/boxing.h#L233-L238):\n \n ```\n \n // 4. signatures returning a tuple of Tensor references, and taking the same\n \n // number of Tensor refs as their initial arguments.\n \n //\n \n // Note that the passed kernels are assumed to be for inplace/outplace ops,\n \n // and the generated BoxedKernelWrapper specializations will return a tuple\n \n // of those initial arguments.\n \n ```\n \n Basically, the \"inplace/outplace\" ops referred to in the comment are ops that fit the common pattern of returning one or more of their Tensor arguments - this specialization implements boxing wrappers for the \"or more\" case. (The [preceding one](https://github.com/pytorch/pytorch/blob/master/aten/src/ATen/core/boxing/impl/boxing.h#L191-L228) handles the single-return case.)\n \n \n \n Also note that these specializations are only over mutable Tensor references, so we won't lose in-place updates made by the kernel - they'll have modified the Tensor referent; the kernel won't be pushing a new reference onto the stack. (BTW if you're wondering why we go the trouble of carefully scooping off the actual passed arg(s) like this in the first place, the answer is pretty arcane, but comes down to avoiding an extra refcount in the Tensors being returned.)",
      "y": "the \"inplace/outplace\" ops referred to in the comment are ops that fit the common pattern of returning one or more of their Tensor arguments - this specialization implements boxing wrappers for the \"or more\" case."
   },
   {
      "null": 146,
      "x": "Query regarding support for RISC-V Vector ISA",
      "z": "If you have a RISC-V compiler you're welcome to try (cross-)compiling to it, but I'm pretty sure no one here has every tried.",
      "y": "try (cross-)compiling to RISC-V"
   },
   {
      "null": 147,
      "x": "Autocompletion does not work in torch.nn module",
      "z": "I can reproduce this with 1.6.0, and see it's already fixed in master.",
      "y": "fixed in master"
   },
   {
      "null": 148,
      "x": "[Feature Request] Deformable Convolution",
      "z": "Does pytorch have any plan to add deformable convolution to its library? so researchers who use pytorch can compete with state of the art methods that use deformable convolution as their baseline?",
      "y": "add deformable convolution to pytorch library"
   },
   {
      "null": 149,
      "x": "SSL Handshake Error when getting pretrained model",
      "z": "Note: I was able to temporarily hack around the problem with:\n \n \n \n  import torchvision.models\n \n  from torchvision.models.vgg import model_urls\n \n  \n \n  model_urls['vgg16'] = model_urls['vgg16'].replace('https://', 'http://')\n \n  vgg16 = torchvision.models.vgg16(pretrained=True)",
      "y": "``` import torchvision.models\n \n  from torchvision.models.vgg import model_urls\n \n  \n \n  model_urls['vgg16'] = model_urls['vgg16'].replace('https://', 'http://')\n \n  vgg16 = torchvision.models.vgg16(pretrained=True)\n \n ```"
   },
   {
      "null": 150,
      "x": "BCEWithLogitsLoss TypeError",
      "z": "We currently don't support automatic type casting in pytorch, and while I agree allowing long types for the target would make it more similar to `nll_loss`, we would then have incompatibilities with `mse_loss` and others (which only accepts the same type as the input for the target). Also note that the target in `bce_with_logits` is not restricted to `{0,1}`, but instead `[0,1]`.\n \n Not sure what is the best to do here (leave as is, or change all losses where it makes sense to accept `long` as well for the target).",
      "y": "No support for automatic type casting in pytorch at present."
   },
   {
      "null": 151,
      "x": "Model params change with 0 learning rate",
      "z": "Do you have `weight_decay`? That could also explain the change.",
      "y": "have `weight_decay`."
   },
   {
      "null": 152,
      "x": "Import fails on 0.2 release installed with Conda",
      "z": "i'm working on fixing this, it'll be fixed in about 4 hours (new fixed binaries are being generated)",
      "y": "deactivate your current environment and try the import again"
   },
   {
      "null": 153,
      "x": "0.2 building from source",
      "z": "I think that the real question is why python2 is being used to build when the invoking interpreter was python3!",
      "y": "while building ATen, it invokes \"python\" instead of which python was invoked in setup.py install.\n \n worth fixing via an env variable that's set in setup.py."
   },
   {
      "null": 154,
      "x": "LSTM mask / remove zeros",
      "z": "Just sum the time axis and divide it with length tensor.",
      "y": "Sum the time axis and divide it with length tensor."
   },
   {
      "null": 155,
      "x": "Errors on jupyter",
      "z": "Hi all! I had the same problem.\n \n For ubuntu the solution is\n \n ```\n \n sudo apt-get install libtcmalloc-minimal4\n \n export LD_PRELOAD=\"/usr/lib/libtcmalloc_minimal.so.4\"\n \n ```",
      "y": "For ubuntu the solution is\n \n ```\n \n sudo apt-get install libtcmalloc-minimal4\n \n export LD_PRELOAD=\"/usr/lib/libtcmalloc_minimal.so.4\"\n \n ```"
   },
   {
      "null": 156,
      "x": "Variable' object has no attribute 'shape' [v0.2]",
      "z": "This has just been fixed on master, and will be present in the next release https://github.com/pytorch/pytorch/pull/2306",
      "y": "add shape to pass-throughs"
   },
   {
      "null": 157,
      "x": "Advanced Indexing Error",
      "z": "A few other things that don't work:\n \n ```python\n \n a = torch.zeros(2, 2)\n \n \n \n a[[0,1], 0] # doesn't work\n \n a[torch.LongTensor([0, 1]), 0] # doesn't work\n \n a[torch.LongTensor([0, 1])][:, 0] # works\n \n \n \n a[[0, 1]][:, 0] # doesn't work\n \n a[[0, 1], :1] # doesn't work\n \n a[[0, 1], 0:1] # doesn't work\n \n a[[0, 1], 0:2] # works\n \n ```",
      "y": "``` a[torch.LongTensor([0, 1])][:, 0]\n a[[0, 1], 0:2]\n ```"
   },
   {
      "null": 158,
      "x": "Unhandled CUDA Error (1)",
      "z": "Has there been any progress on this issue? I have two GTX 1080 Ti cards and I've been getting this error since I upgraded to v0.2 (using conda). I tried \"pip install\" as suggested above, and it works, but it causes other problems.",
      "y": "try \"pip install\""
   },
   {
      "null": 159,
      "x": "Python3 crashes when importing torch when referencing it.",
      "z": "seems similar to https://github.com/pytorch/pytorch/issues/2314\n \n we'll look into this, so weird.",
      "y": "the problem occurs only if we install wheel files, and not via conda."
   },
   {
      "null": 160,
      "x": "BrokenPipeError: [Errno 32] Broken pipe",
      "z": "You can set num_workers to 0 to see the actual error. Did you have your `plotter` correctly configured?",
      "y": "set num_workers to 0 to see the actual error."
   },
   {
      "null": 161,
      "x": "Does CosineEmbeddingLoss support CUDA tensors?",
      "z": "try this instead:\n \n ```\n \n y = Variable(torch.cuda.FloatTensor([1.0] * input1.size()[0]))\n \n ```\n \n \n \n BTW, questions like this are better discussed on https://discuss.pytorch.org/.",
      "y": "```\n \n y = Variable(torch.cuda.FloatTensor([1.0] * input1.size()[0]))\n \n ```"
   },
   {
      "null": 162,
      "x": "jupyter kernel died at division",
      "z": "This is probably due to division by 0 due to conversion of 0.1 to 0.",
      "y": "Due to division by 0 due to conversion of 0.1 to 0."
   },
   {
      "null": 163,
      "x": "I want to know how to use the select(int64_t dim, int64_t index) in at::Tensor?What is the definition of a parameter ?",
      "z": "it's selecting a slice at a certain index of a certain dimension",
      "y": "Select a slice at an index of a dimension."
   },
   {
      "null": 164,
      "x": "Wrong results when exporting a traced LSTM",
      "z": "Issue is no longer reproducible in `torch-nightly-1.0.0.dev20190304`.",
      "y": "No longer reproducible in `torch-nightly-1.0.0.dev20190304`."
   },
   {
      "null": 165,
      "x": "[Windows] subprocess.CalledProcessError (exit status 1) while installing pytorch by python-32",
      "z": "> A suggestion: CMake / setup.py to error out if Python 32-bit version is used\n \n \n \n I don't mind, but it should be mentioned in readme anyway: https://github.com/pytorch/pytorch/pull/17689 .",
      "y": "CMake / setup.py to error out if Python 32-bit version is used"
   },
   {
      "null": 166,
      "x": "torch.arange dtype mismatch with and without jit",
      "z": "thanks, we'll look into this!",
      "y": "Fix dtype of arange in JIT and remove dtype of arange from TorchANI"
   },
   {
      "null": 167,
      "x": "Can i use torch.nn.modules.pixelshuffle for commercial products?",
      "z": "We are not lawyers and we dont know :) (I didn't open that patent link and I am not going to, fyi)",
      "y": "Patent won't impede."
   },
   {
      "null": 168,
      "x": "big difference between numpy.matmul and torch.matmul",
      "z": "Hi,\n \n \n \n Welcome to the wonderful world of float operations.\n \n tldr: this is expected behavior because float operations are inexact.\n \n The order in which the ops are done will change the result and if you accumulate a large number of values (millions in your case), this difference will grow quite a lot.\n \n The idea for matmul is the same as for sum below.\n \n \n \n Here is a small code sample where the exact same array is summed but in different order. You can check that changing the number of threads will give different results as well.\n \n \n \n ```python\n \n import torch\n \n import numpy as np\n \n \n \n \n \n # Moving to double should reduce the difference to something very small ~1e-10\n \n used_type = torch.float\n \n # used_type = torch.double\n \n \n \n # Changing the number of threads will potentially change the order of execution\n \n # And thus change all the results\n \n num_threads = 12\n \n \n \n torch.manual_seed(1)\n \n torch.set_num_threads(num_threads)\n \n big_size = 1000000\n \n a = torch.randn((big_size, 20), dtype=used_type)\n \n print(\"Running with type {} and {} threads\".format(used_type, num_threads))\n \n \n \n sum_a = a.sum().item()\n \n sum_a_np = np.sum(a.numpy())\n \n print(\"original diff: \", sum_a - sum_a_np)\n \n \n \n print(\"diffs: torch vs torch \\t| torch vs np \\t| np vs np\")\n \n for _ in range(20):\n \n  idx = torch.randperm(big_size)\n \n  shuffled_a = a.index_select(0, idx)\n \n  new_sum_a = shuffled_a.sum().item()\n \n  new_sum_a_np = np.sum(shuffled_a.numpy())\n \n  print(\"diffs: {} \\t| {} \\t| {}\".format(sum_a - new_sum_a, new_sum_a - new_sum_a_np, sum_a_np - new_sum_a_np))\n \n ```",
      "y": "Expected behavior because float operations are inexact.\n \n The order in which the ops are done will change the result and if you accumulate a large number of values (millions in your case), this difference will grow quite a lot."
   },
   {
      "null": 169,
      "x": "The function of torch.max?",
      "z": "> out=max_temp\n \n \n \n `max_temp` is a view. You asked to insert the value back into the view.",
      "y": "Find max value in a tensor along a dimension."
   },
   {
      "null": 170,
      "x": "NNPack support for arm ( linux )",
      "z": "It's not so much a tutorial, though I always thought about making one: https://lernapparat.de/pytorch-android/",
      "y": "Building with NNPack and QNNPack works out of the box on both raspbian and arm64"
   },
   {
      "null": 171,
      "x": "Assigning a slice of the same tensor is not consistent on GPU",
      "z": "```\n \n tensor[:-500] = tensor[500:]\n \n ```\n \n \n \n In such an operation, there are significant data races as you are copying overlapping sets of elements. While one Tensor location is modified, another might read from it, and the ordering here will not be consistent, it'll depend on which block on the GPU gets free at what nanosecond.\n \n \n \n We dont aim to have such an operation be ordered. There is no specified notion of correctness in this operation, unless you write it as:\n \n \n \n ```\n \n x = tensor[500:].clone()\n \n tensor[:-500] = x\n \n ```\n \n \n \n because, if you write it as a single-line in-place operation, one has to assume a particular ordering of traversing the indices (but there's no standard).\n \n \n \n There are many situations where you will get wrong results when inputs and outputs are the same, for example: `torch.sum(x, dim=1, out=x)`.\n \n \n \n The fact that it worked on the CPU is merely because the CPU algorithm is probably serial.\n \n \n \n There are a few other issues where we did specify this, but I haven't found one based on a quick glance.",
      "y": "If you write it as a single-line in-place operation, one has to assume a particular ordering of traversing the indices (but there's no standard)."
   },
   {
      "null": 172,
      "x": "torch.max and torch.min inconsistent on cpu/gpu for tensors with zero elements.",
      "z": "I don't think the GPU behavior is completely wrong, for a mathematical reason: it should be the case that `torch.max(torch.cat(xs, ys)) == torch.max(torch.max(xs), torch.max(ys))` for any xs and ys, including empty. In that case, `-Inf` is the correct neutral element to pick.\n \n \n \n The way Numpy gets out of this situation, is they ask you for an initial element to handle the case explicitly, and error if you don't provide it. So in the end I agree with your suggested course of action.",
      "y": "It should be the case that `torch.max(torch.cat(xs, ys)) == torch.max(torch.max(xs), torch.max(ys))` for any xs and ys, including empty. In that case, `-Inf` is the correct neutral element to pick."
   },
   {
      "null": 173,
      "x": "Device agnostic gradient reduction",
      "z": "Not for the forward pass, but the backwards pass needs to be aware that gradients may be produced on different devices. I meant the mode of execution where you use multiple devices in the forward pass, as something to be aware of when hooking into the backwards pass. I.e. we keep a list of gradients per device, assuming that we see the same gradient on every device. This is not the case when doing combined model parallel and data parallel. Between this and model replication/broadcast that is device aware (so that you can run 2 or more models in the same process, like DDP supports today), there is some work needed to support combined model parallelism within process and data parallelism across processes.\n \n \n \n For example, let's say you use devices 0 and 1 and wrap your model with DDP, you'd want to execute:\n \n \n \n ```\n \n model = DDP(model, device_ids=[[0,1], [2,3]])\n \n ```\n \n \n \n Currently, model replication only works if the model uses a single GPU.\n \n \n \n Though you could argue that this mode of execution is not preferred and you should be using different processes anyway, I think that we should keep it in mind when designing this new reducer.",
      "y": "The mode of execution where you use multiple devices in the forward pass, as something to be aware of when hooking into the backwards pass. I.e. we keep a list of gradients per device, assuming that we see the same gradient on every device. This is not the case when doing combined model parallel and data parallel."
   },
   {
      "null": 174,
      "x": "Default forward method for ModuleList and ModuleDict",
      "z": "the exact `forward` methods to be specified will get controversial, because the most natural usage of ModuleList or ModuleDict are not obvious.\n \n \n \n However, the biggest danger is that people will use the `forward` expecting it to do something smart wrt memory (or worse, do what people want which is again not obvious), but it'll default-compute all outputs.\n \n \n \n For example, viewing a `forward` for `ModuleList` similar to `Sequential` is actually not far-fetched (I actually thought of a `forward` for ModuleList as a first-reaction to be similar to Sequential)\n \n \n \n I thought a bit about it, and I'm a bit ambivalent, but am leaning towards conservatively not adding the functionality.",
      "y": "Viewing a `forward` for `ModuleList` similar to `Sequential` is actually not far-fetched (I actually thought of a `forward` for ModuleList as a first-reaction to be similar to Sequential)"
   },
   {
      "null": 175,
      "x": "[CI] Is approval of reviewers reasonable while one of CI facilities is failure?",
      "z": "There are a few reasons:\n \n \n \n 1. CircleCI randomly craps out on a small percentage of runs, before we even run any code. We've told CircleCI about this but a fix has been very slow coming\n \n 2. We have a few flaky tests. It takes a bit of time to diagnose and disable them, so a few of them stick around and intermittently terrorize diffs. Search for \"flaky\" to see some standing issues on them. They need some time and debugging to figure out why they are flaky; some involve quite complex multithreaded systems.\n \n 3. ROCm is still a little unstable, and is often a culprit.\n \n \n \n One of the things I intend to work on in the near future is a better system for reporting failures to users, and letting them know if they are likely spurious or not.",
      "y": "1. CircleCI randomly craps out on a small percentage of runs, before we even run any code. We've told CircleCI about this but a fix has been very slow coming\n \n 2. We have a few flaky tests. It takes a bit of time to diagnose and disable them, so a few of them stick around and intermittently terrorize diffs. Search for \"flaky\" to see some standing issues on them. They need some time and debugging to figure out why they are flaky; some involve quite complex multithreaded systems.\n \n 3. ROCm is still a little unstable, and is often a culprit."
   },
   {
      "null": 176,
      "x": "Binomial.log_prob returns -inf when actual probability is 1 if logit is large",
      "z": "My bad - adapted the example code to use the named `logits` arguments.",
      "y": "Both n and p are required."
   },
   {
      "null": 177,
      "x": "RuntimeError: Only tuples, lists and Variables supported as JIT inputs, but got NoneType",
      "z": "Thanks for the report! Can you provide a script that we can run to reproduce the error you are seeing? That would help us investigate.",
      "y": "For add_graph, you need to pass through a sample bit of data in order to trace the graph."
   },
   {
      "null": 178,
      "x": "Tweak Java API before 1.3 release",
      "z": "> Proposed change: consolidate factory methods.\n \n \n \n I like! Seems closer to the existing API.\n \n \n \n > Proposed non-change: getters.\n \n \n \n I don't like :(. They're not really \"getters\" in the Java sense, right? Like, for a given IValue, you can only call one of those methods without erroring. When I see `getX` I assume it's basically a data field on a struct and I'm more or less directly retrieving it. `IValue::toX` is more \"perform a checked cast of me to type X\".",
      "y": "Consolidate factory methods."
   },
   {
      "null": 179,
      "x": "Python tests use torch.Doubletensor as their default tensor type",
      "z": "There actually is an existing decorator, @default_tensor_type, defined in some test files https://github.com/pytorch/pytorch/blob/a3ef0251fb3dfd85e3f4c874fcf004188eb509fd/test/test_jit.py#L90 but it's better if you can use the @dtypes decorator https://github.com/pytorch/pytorch/blob/a3ef0251fb3dfd85e3f4c874fcf004188eb509fd/test/common_device_type.py#L287 The latter instantiates a variant of your test for each dtype you list, in addition to giving you precise control over dtypes. It's also thread safe. \n \n \n \n @dtypes does require you write your test generically, however. A non-generic version of it may be interesting, too.",
      "y": "use the @dtypes decorator"
   },
   {
      "null": 180,
      "x": "record_stream() on a shifted view tensor doesn't work",
      "z": "Thanks for reporting the issue. Given that there is discussion in the PR you put up I'm closing this one.",
      "y": "record_stream() for shifted view tensors"
   },
   {
      "null": 181,
      "x": "Check DataPtr's deleter to determine if it is allocated by CUDA in record_stream",
      "z": "This might be a good bootcamp task.",
      "y": "Raise error if a block can not be found from a CUDA tensor."
   },
   {
      "null": 182,
      "x": "the list has inconsistent indentation.",
      "z": "Looks corrected now:\n \n ![image](https://user-images.githubusercontent.com/8042156/66335650-4f324380-e8f0-11e9-8e1b-326356d0d50e.png)",
      "y": "Documentation now corrected."
   },
   {
      "null": 183,
      "x": "CTCLoss cuda backend computes wrong gradient when target (i.e. label) length is greater than 896 for double inputs or 1024 for float inputs",
      "z": "Thank you for the bug report with detailed analysis! This is gold to reproduce and fix bugs.\n \n Indeed at first glance it looks like there is some looping missing when the number of threads is smaller than length. This should not happen.",
      "y": "There is some looping missing when the number of threads is smaller than length"
   },
   {
      "null": 184,
      "x": "Issues with ConvReLU2d and LinearReLU doc issues",
      "z": "Can you help us with the linking issue (#1 above)?\n \n \n \n Can you focus on the \"same as X\" becoming \"as X (same)\" issue?",
      "y": "\"same as X\" becoming \"as X (same)\"."
   },
   {
      "null": 185,
      "x": "add_zero_attn in MultiheadAttention breaks causality",
      "z": "I have not yet fully analyzed the problem and I am not sure if the observed effect is a bug or by design.\n \n \n \n It seems that in the self-attention variant with the added zero k & v sequence-entries (produced internally by `add_zero_attn=True`) the gradient is composed of a component coming from the query and one from values and keys. The gradient from the query is causing the undesired effect. \n \n \n \n I modified the repro script a bit in order to get the effect without `add_zero_attn=True` (initial in-projection bias should be 0) and to split the contribution of query and key & value: \n \n \n \n ```\n \n import torch\n \n \n \n embedding_dim = 1\n \n batch_size = 1\n \n num_heads = 1\n \n seq_len = 4\n \n \n \n net = torch.nn.MultiheadAttention(embedding_dim, num_heads, add_zero_attn=False)\n \n mask = torch.cat([torch.ones(seq_len, seq_len).triu(), torch.zeros(seq_len, 1)], dim=1)\n \n mask[mask==1]=float('-inf')\n \n print(mask)\n \n \n \n for i in range(seq_len):\n \n  x = torch.ones(seq_len, batch_size, embedding_dim, requires_grad=True)\n \n  y = torch.ones(seq_len, batch_size, embedding_dim, requires_grad=True)\n \n  z = torch.cat([y, torch.zeros(1, 1, embedding_dim)]) # add zero sequence element\n \n  o, w = net(x, z, z, attn_mask=mask)\n \n  #print(w)\n \n  # o.shape is (seq_len, batch_size, embedding_dim)\n \n  o.mean([1, 2])[i].backward()\n \n  print(i, 'x:', x.grad.abs().sum([1, 2]).view(-1))\n \n  print(i, 'y:', y.grad.abs().sum([1, 2]).view(-1))\n \n ``` \n \n \n \n Output is:\n \n ```\n \n tensor([[-inf, -inf, -inf, -inf, 0.],\n \n  [0., -inf, -inf, -inf, 0.],\n \n  [0., 0., -inf, -inf, 0.],\n \n  [0., 0., 0., -inf, 0.]])\n \n 0 x: tensor([0., 0., 0., 0.])\n \n 0 y: tensor([0., 0., 0., 0.])\n \n 1 x: tensor([0.0000, 0.0148, 0.0000, 0.0000])\n \n 1 y: tensor([0.2801, 0.0000, 0.0000, 0.0000])\n \n 2 x: tensor([0.0000, 0.0000, 0.0127, 0.0000])\n \n 2 y: tensor([0.1798, 0.1798, 0.0000, 0.0000])\n \n 3 x: tensor([0.0000, 0.0000, 0.0000, 0.0105])\n \n 3 y: tensor([0.1323, 0.1323, 0.1323, 0.0000])\n \n ```",
      "y": "in the self-attention variant with the added zero k & v sequence-entries (produced internally by `add_zero_attn=True`) the gradient is composed of a component coming from the query and one from values and keys. The gradient from the query is causing the undesired effect."
   },
   {
      "null": 186,
      "x": "torch.histc segfaults if array has inf",
      "z": "Will take a look",
      "y": "torch.histc added a finite range check to resolve segfaults if tensor has inf. also added checks for nan values, min>max"
   },
   {
      "null": 187,
      "x": "No module named _C",
      "z": "You could try running the test from another directory, this sometimes happens if there is a file named `torch.py` or (more likely in this case) a folder named `torch/` with an `__init__.py` inside, then `import torch` tries to import that directly instead of the installed/symlinked version in your Python's `site-packages`.\n \n \n \n Something might be getting installed incorrectly, the `import my_module` resolution rules can be found [here](https://docs.python.org/3/tutorial/modules.html#the-module-search-path). \n \n \n \n Similar issues #574, #17203, #7",
      "y": "happens if there is a file named `torch.py` or (more likely in this case) a folder named `torch/` with an `__init__.py` inside, then `import torch` tries to import that directly instead of the installed/symlinked version in your Python's `site-packages`."
   },
   {
      "null": 188,
      "x": "Jacobian-vector equation in autograd_tutorial font size is too small",
      "z": "OK, I'll close this then?",
      "y": "PyTorch tutorials are using MathJax library for the equation\n Change the setting of Mathjax\n Math render -> preview HTML instead of HTML-CSS"
   },
   {
      "null": 189,
      "x": "[JIT] builtin function attributes do not recursively compile",
      "z": "Well, it doesn't work if it's a builtin function:\n \n ```\n \n class Add(nn.Module):\n \n  def __init__(self):\n \n  super(Double, self).__init__()\n \n  self.add = torch.add\n \n \n \n  def forward(self, input):\n \n  return self.add(input)\n \n ```",
      "y": "Doesn't work if it's a builtin function."
   },
   {
      "null": 190,
      "x": "[ONNX] Export torch.meshgrid",
      "z": "There is an open PR adding support for meshgrid in https://github.com/pytorch/pytorch/pull/26037",
      "y": "Add support for meshgrid"
   },
   {
      "null": 191,
      "x": "Raspberry Pi Zero W build fails",
      "z": "There was activity on https://github.com/Maratyszcza/pthreadpool by @Maratyszcza last week.\n \n \n \n This must be related.",
      "y": "run the command:\n git submodule update --remote third_party/protobuf"
   },
   {
      "null": 192,
      "x": "torch.split with tensor sizes fails in tracing",
      "z": "I think this also affects ONNX exporter.",
      "y": "Issue with the tracer to handle traced Tensor inputs where int/int list is expected."
   },
   {
      "null": 193,
      "x": "How can I implement \"nn.unFold\" on 5D tensor?",
      "z": "You might be able to get some inspiration from the implementation we had in the old torch7 library [here](https://github.com/torch/nn/blob/872682558c48ee661ebff693aa5a41fcdefa7873/lib/THNN/generic/VolumetricConvolutionMM.c#L99-L263).\n \n I'm not sure how efficient it will be though.",
      "y": "Refer to the implementation in the old torch7 library"
   },
   {
      "null": 194,
      "x": "JIT profiling executor does not fuse Relu and Dropout for a GPU",
      "z": "I love the option's name `--bad-jit` @kevinstephano \u00f0\u0178\u02dc\u201e \n \n \n \n I think I know what the problem is here, I'll test the fix in the next few days.",
      "y": "Turn off profiling graph exec"
   },
   {
      "null": 195,
      "x": "[quant] QuantizedCUDA",
      "z": "Sorry that it took this much time. I was unexpectedly busy with another matter. Please, see the linked PR for the current implementation, it compiles and successfully passes all tests. \n \n \n \n Please, could you review it in the nearest time? I have unexpected free 1-2 weeks due to the global quarantine and will be able to fix any suggestions quickly. I had to make several decisions during development(that should be managed by internal team agreements that I am not aware of), so it might require some adjustments.",
      "y": "QuantizedCUDA implementation"
   },
   {
      "null": 196,
      "x": "kthvalue/median with scalar and dim=1 inconsistent between CPU and CUDA",
      "z": "This is out of scope of what I'm currently fixing. I'm currently fixing the part of scalar handling across the TH->ATen boundary that is: \"you returned to me a size `(1,)` tensor, but did you really mean to return me a size `()` Tensor?\"\n \n \n \n Scalar handing within ops is a bit of a different issue and I believe @ezyang's analysis of this specific case is correct.",
      "y": "Scalar handing within ops"
   },
   {
      "null": 197,
      "x": "Regression on split operator benchmark after __torch_function__ merge",
      "z": "Just to clarify before I start putting in pull requests, the way forward is:\n \n \n \n 1. ASAP remove the `torch_function_dispatch` decorator from everything in `torch.functional`\n \n 2. Look into either:\n \n  A) Speed up the decorator so the overhead is smaller. I don't think we can ever get the overhead to zero but we can probably at least get it much lower by doing C++-level checks for if parameters are `torch.tensor` instances or not\n \n  B) Rewrite the operators in `torch.functional` to be in C++",
      "y": "remove the `torch_function_dispatch` decorator from everything in `torch.functional`"
   },
   {
      "null": 198,
      "x": "DISABLED test_max_pool2d (__main__.TestQuantizedOps)",
      "z": "Is this still an issue?",
      "y": "Hypothesis deadline testing is now disabled altogether so we can reenable this test (if needed, on a plane and haven\u2019t checked)"
   },
   {
      "null": 199,
      "x": "backward hooks on parameters don't work with distributed autograd.",
      "z": "FWIW, this is also needed to enable DDP.\n \n \n \n If done, we can easily mix and match RPC with DDP for combined model and data parallelism.",
      "y": "Needed to mix and match RPC with DDP for combined model and data parallelism."
   },
   {
      "null": 200,
      "x": "error occures when trying to import torch, \"ImportError: cannot import name 'ClassType'\"",
      "z": "I had a similar issue with the error message: ImportError: cannot import name 'nan'. The fix was to do: \n \n \n \n `pip3 uninstall torch`\n \n \n \n multiple times (3, in my case) untill the message `Cannot uninstall requirement torch, not installed` appeared. After this, a fresh install via:\n \n \n \n `pip3 install torch==0.3.1`\n \n \n \n seemed to fix the problem. I realise we are using different versions of pytorch, but hopefully this helps.",
      "y": "`pip3 install torch==0.3.1`"
   },
   {
      "null": 201,
      "x": "I can't build pytorch 1.3.1 from sources.",
      "z": "It worked with USE_NINJA=OFF. Thank you guys for your support.",
      "y": "USE_NINJA=OFF."
   },
   {
      "null": 202,
      "x": "Windows Build fails",
      "z": "This is the reply from MS. I think we should make the change.\n \n \n \n > See the C++ standard, [meta] / p4\n \n > \n \n > Unless otherwise specified, the behavior of a program that adds specializations for any of the templates\n \n > specified in this subclause [meta] is undefined.",
      "y": "See the C++ standard, [meta] / p4\n Unless otherwise specified, the behavior of a program that adds specializations for any of the templates.\n Specified in this subclause [meta] is undefined."
   },
   {
      "null": 203,
      "x": "[jit] runtime error with backward of matmul and squeeze",
      "z": "Hm, this does not reproduce for me on master. Can you install the latest nightly and see if it reproduces in your environment?",
      "y": "Install the latest nightly and see if it reproduces in your environment"
   },
   {
      "null": 204,
      "x": ".sum() not return ideal result?",
      "z": "it's probably overflowing your ByteTensor, try converting it to a larger type and doing the sum on that.",
      "y": "Try converting it to a larger type and doing the sum on that."
   },
   {
      "null": 205,
      "x": "Compilation error: libATen.so.1: undefined reference to `convolve_5x5_sse'",
      "z": "Your problem is probably different; you probably have two versions of cudnn installed and are linking against the wrong one; uninstalling one copy will help (we have a bug tracking at #4860",
      "y": "You probably have two versions of cudnn installed and are linking against the wrong one."
   },
   {
      "null": 206,
      "x": "Runtime error(s) for Conv2d second gradient",
      "z": "i could reproduce this as well. we'll look into this.",
      "y": "Always define outputs of ConvBackwardBackward"
   },
   {
      "null": 207,
      "x": "layer-by-layer profiling feature",
      "z": "In case this is useful to anyone else, I've created a python package for layer by layer profiling that extends the autograd profiler. It only supports forward computation.\n \n https://github.com/awwong1/torchprof\n \n https://pypi.org/project/torchprof/",
      "y": "Python package for layer by layer profiling that extends the autograd profiler."
   },
   {
      "null": 208,
      "x": "additional continuous builds",
      "z": "Windows CI is now enabled",
      "y": "Windows CI has been enabled."
   },
   {
      "null": 209,
      "x": "add torch.stft and torch.fft",
      "z": "Yep, that's what I did. I surely didn't implement my own fft.",
      "y": "For proper fft, I think we should use existing wheels, e.g., cuFFT and MKL/Eigen."
   },
   {
      "null": 210,
      "x": "saved_tensors attribute for grad_fn",
      "z": "I think now (or soon?) it might be better to export the model to ONNX and then use the visualization tool they provide to inspect the graph?",
      "y": "Export the model to ONNX and then use the visualization tool they provide to inspect the graph."
   },
   {
      "null": 211,
      "x": "tensor.max(other) not documented",
      "z": "Maybe we should just disable that.",
      "y": "Document it or disable it."
   },
   {
      "null": 212,
      "x": "broadcasting behaves differently on CPU and GPU",
      "z": "Testing environment:\n \n - Ubuntu 16.04 LTS\n \n - CUDA 8.0.61 (GeForce GTX 960)\n \n - pytorch 0.2.0_3",
      "y": "Manually try to broadcast before calling the fused kernels."
   },
   {
      "null": 213,
      "x": "Problem when I load a state dictionary",
      "z": "Same problem. Here is a quick script I used to solve the problem. Help for future visitors here! \u00b8 \n \n \n \n ```\n \n import argparse\n \n import torch\n \n \n \n parser = argparse.ArgumentParser()\n \n parser.add_argument(\"--source\", type=str, required=True)\n \n parser.add_argument(\"--dest\", type=str, required=True)\n \n \n \n args = parser.parse_args()\n \n \n \n model_state = torch.load(args.source)\n \n new_model_state = {}\n \n \n \n for key in model_state.keys():\n \n  new_model_state[key[7:]] = model_state[key]\n \n \n \n torch.save(new_model_state, args.dest)\n \n ```\n \n Run it as: `python remove_module.py --source <source pickle file> --dest <destination pickle file>`",
      "y": "```\n \n import argparse\n \n import torch\n \n \n \n parser = argparse.ArgumentParser()\n \n parser.add_argument(\"--source\", type=str, required=True)\n \n parser.add_argument(\"--dest\", type=str, required=True)\n \n \n \n args = parser.parse_args()\n \n \n \n model_state = torch.load(args.source)\n \n new_model_state = {}\n \n \n \n for key in model_state.keys():\n \n  new_model_state[key[7:]] = model_state[key]\n \n \n \n torch.save(new_model_state, args.dest)\n \n ```"
   },
   {
      "null": 214,
      "x": "Commit breaks CUDA9 builds [NativeFunctions: support backend-specific dispatch]",
      "z": "ArchLinux w/ gcc5, able to build successfully by the following change:\n \n ```diff\n \n diff --git i/setup.py w/setup.py\n \n index e484692f..75e1836e 100644\n \n --- i/setup.py\n \n +++ w/setup.py\n \n @@ -90,7 +90,7 @@ import distutils.sysconfig\n \n  cfg_vars = distutils.sysconfig.get_config_vars()\n \n  for key, value in cfg_vars.items():\n \n  if type(value) == str:\n \n - cfg_vars[key] = value.replace(\"-Wstrict-prototypes\", \"\")\n \n + cfg_vars[key] = value.replace(\"-Wstrict-prototypes\", \"\").replace(\"-fno-plt\", \"\")\n \n \n \n  ################################################################################\n \n  # Custom build commands\n \n ```",
      "y": "```diff\n \n diff --git i/setup.py w/setup.py\n \n index e484692f..75e1836e 100644\n \n --- i/setup.py\n \n +++ w/setup.py\n \n @@ -90,7 +90,7 @@ import distutils.sysconfig\n \n  cfg_vars = distutils.sysconfig.get_config_vars()\n \n  for key, value in cfg_vars.items():\n \n  if type(value) == str:\n \n - cfg_vars[key] = value.replace(\"-Wstrict-prototypes\", \"\")\n \n + cfg_vars[key] = value.replace(\"-Wstrict-prototypes\", \"\").replace(\"-fno-plt\", \"\")\n \n \n \n  ################################################################################\n \n  # Custom build commands\n \n ```"
   },
   {
      "null": 215,
      "x": "Calling float() in modules converts integer buffers or parameters to floating point",
      "z": "this makes sense. i think you should go for it and make integer types immune to .float(), .half() and .double() calls.",
      "y": "Make integer types immune to .float(), .half() and .double() calls."
   },
   {
      "null": 216,
      "x": "Scheduler.step() doesn't perform optimizer.step()",
      "z": "This confuses me as well, instead I was thinking scheduler.step() will implicitly perform optimizer.step() as well since scheduler wraps optimizer.\n \n It would be appreciated if one comment (or example code) can be added to the official documentation, saying that scheduler.step() performs on epoch-level that only changes learning rate, while optimizer.step() performs on batch-level that does update to parameters.\n \n Many thanks",
      "y": "Scheduler.step() performs on epoch-level that only changes learning rate, while optimizer.step() performs on batch-level that does update to parameters."
   },
   {
      "null": 217,
      "x": "Building from source on master is broken",
      "z": "you can do the following:\n \n \n \n I will assume you have all the dependencies mentioned at https://github.com/pytorch/pytorch#from-source\n \n ```\n \n git clone --recursive https://github.com/pytorch/pytorch.git\n \n cd pytorch\n \n git pull origin pull/3831/head:build_fix\n \n git checkout build_fix\n \n export CMAKE_PREFIX_PATH=\"$(dirname $(which conda))/../\" # [anaconda root directory]\n \n python setup.py install | tee build.log # Writes the build logs to build.log\n \n \n \n ```",
      "y": "```\n \n git clone --recursive https://github.com/pytorch/pytorch.git\n \n cd pytorch\n \n git pull origin pull/3831/head:build_fix\n \n git checkout build_fix\n \n export CMAKE_PREFIX_PATH=\"$(dirname $(which conda))/../\" # [anaconda root directory]\n \n python setup.py install | tee build.log # Writes the build logs to build.log\n \n \n \n ```"
   },
   {
      "null": 218,
      "x": "python3--no module named PIL",
      "z": "I would recommend uninstalling Pillow, PIL (if they exist) and reinstalling pillow.\n \n Something like:\n \n ```\n \n pip uninstall Pillow\n \n pip uninstall PIL\n \n pip install Pillow\n \n ```\n \n In the future you should post your question to https://discuss.pytorch.org/ , we like to keep issues on github to PyTorch-only bugs.",
      "y": "Uninstall and reinstall PIL"
   },
   {
      "null": 219,
      "x": "torch.tensor does not support list of complex tensor",
      "z": "I think the root cause is because tensor objects do not have `__complex__`:\n \n ```\n \n >>> complex(torch.tensor(1j))\n \n Traceback (most recent call last):\n \n  File \"<stdin>\", line 1, in <module>\n \n RuntimeError: value cannot be converted to type double without overflow: (0,1)\n \n >>> torch.tensor(1j).__complex__()\n \n Traceback (most recent call last):\n \n  File \"<stdin>\", line 1, in <module>\n \n AttributeError: 'Tensor' object has no attribute '__complex__'\n \n ```",
      "y": "Tensor objects do not have `__complex__`"
   },
   {
      "null": 220,
      "x": "segfault together with \"import cv2\"",
      "z": "This turns out to be a glibc bug https://sourceware.org/bugzilla/show_bug.cgi?id=20839 triggered by some libs in opencv. The latest opencv-python has added a workaround for it. (details at https://github.com/skvark/opencv-python/issues/381)",
      "y": "glibc bug triggered by some libs in opencv."
   },
   {
      "null": 221,
      "x": "MultivariateNormal backprop performance issue related to broadcasting",
      "z": "It seems to me that broadcasting tensors together and then slicing them after is a bad idea and you should avoid doing it. If there are ways we can make it easier to do the right thing in the API, those might be worth adding.",
      "y": "Broadcasting tensors together and then slicing them after is a bad idea"
   },
   {
      "null": 222,
      "x": "CrossEntropyLoss does not raise target out of bounds error on gpu",
      "z": "Fixed in 1.5.1, 1.6 and master. Closing, please reopen if issue persists.",
      "y": "Fixed in 1.5.1, 1.6 and master."
   },
   {
      "null": 223,
      "x": "Missing `IndexError` when accessing elements outside of shape of CUDA tensor",
      "z": "I agree that the CUDA one should indeed raise...\n \n On the CPU error message part, it's actually \"makes sense\" that you're essentially doing zz[0][2] where `2` is the size of dimension 0. If you call `zz[0, 2]` it'll correctly report the dimension. We could probably live the current error message for now but the CUDA issue might be a high pri.",
      "y": "If you call `zz[0, 2]` it'll correctly report the dimension."
   },
   {
      "null": 224,
      "x": "torch.cat is over 300x slower than torch.index_copy / numpy.concatenate on CPU",
      "z": ">Just to clarify, I see these issues regardless of the number of threads (torch.set_num_threads), but increasing the number of processes through mp.spawn exacerbates the slowdown.\n \n \n \n How are you using `torch.set_num_threads` and how many CPU cores do you have? With all 10 processes operating single threaded on a 24-core CPU, I get `cat`, `index_copy` and `np.concatenate` all taking around 1-4 ms. With your original script I see `cat` on some processes taking only 1-2 ms but also sometimes taking much longer; as high as 50 ms or even 100 ms. I presume you've cherry picked the figures in the original report to emphasize these values.\n \n \n \n What I think is happening is that each process creates its own OpenMP thread pool, causing over subscription of hardware threads. NumPy doesn't suffer because it's single threaded, `index_copy` does many smaller copies (one for each index) all of which are less than [`at::internal::GRAIN_SIZE`](https://github.com/pytorch/pytorch/blob/ccea3726da3aca338eb7753e84ba370a13148855/aten/src/ATen/Parallel.h#L14) which means the copies aren't multithreaded.\n \n \n \n To confirm this, I can increase the `shape` parameter to `(8, 128, 1000)` where `128*1000` is greater than the grain size, therefore `index_copy` is multithreaded. As expected, I see a huge drop in performance of `index_copy`, in line with the `cat` performance. If I run again with `torch.set_num_threads(2)` it uses at most 20 threads of my 24-core machine and both `cat` and `index_copy` slightly out-perform NumPy. \n \n \n \n My suggestion is to use `torch.set_num_threads(n_threads)` so that `n_threads * n_procs <=` your actual hardware threads and that way you won't run into over-subscription issues.",
      "y": "Each process creates its own OpenMP thread pool, causing over subscription of hardware threads. NumPy doesn't suffer because it's single threaded, `index_copy` does many smaller copies (one for each index) all of which are less."
   },
   {
      "null": 225,
      "x": "C++ torch::min_values, torch::max_values no longer found in current libtorch",
      "z": "In the latest version, the following equivalents are available torch::amin and torch::amax.",
      "y": "Latest version has torch::amin and torch::amax."
   },
   {
      "null": 226,
      "x": "Documentation mention parameters `verbose` for torch.optim.lr_scheduler but it does not exist",
      "z": "Verbose param was added very recently to the schedulers (last month). It should be there in the nightly release (link to the [source](https://github.com/pytorch/pytorch/blob/master/torch/optim/lr_scheduler.py)). I guess the docs (for version 1.6.0) are not synced properly with the source, but it should be fixed in next stable release.",
      "y": "Verbose param was added recently to the schedulers and it should be there in the nightly release and hence the docs are not synced properly with the source."
   },
   {
      "null": 227,
      "x": "disable instruction set when build libtorch",
      "z": "PyTorch should not have an AVX instruction in the default codepath, so compiling in the host that supports AVX is fine. But if you want to avoid compiling the code that contains AVX/AVX2 instruction, you can try to build with DISABLE_AVX/DISABLE_AVX2/C_AVX_FOUND/C_AVX2_FOUND set to OFF",
      "y": "Build with DISABLE_AVX/DISABLE_AVX2/C_AVX_FOUND/C_AVX2_FOUND set to OFF."
   },
   {
      "null": 228,
      "x": "Integer overflow when doing 1x1 convolution on very large tensor",
      "z": "Closing as a duplicate of #43476, I'll reprioritize that one.",
      "y": "Integer overflow when doing 1x1 convolution on very large tensor."
   },
   {
      "null": 229,
      "x": "fx: unable to symbolically trace simple nn.Sequential model",
      "z": "The source generated here is:\n \n ```\n \n def forward(self, input):\n \n  0 = getattr(self, \"0\")(input)\n \n  \n \n \n \n  return 0\n \n ```\n \n \n \n And the error is a result of `Sequential` using integers in strings as keys (which is invalid in Python and thus why the `getattr` is emitted rather than a `self.0` attribute fetch).\n \n \n \n I'll see how hard it is to sanitize the identifiers in this scenario",
      "y": "Error is a result of `Sequential` using integers in strings as keys (which is invalid in Python and thus why the `getattr` is emitted rather than a `self.0` attribute fetch)."
   },
   {
      "null": 230,
      "x": "Xcode 12 Build Error: LibTorch/install/lib/libpytorch_qnnpack.a , building for iOS, but linking in object file built for macOS",
      "z": "The issue was in CMake's version. Obviously, 3.14 doesn't work well with XCode 12. We just tried 3.18, it works fine. Anyway, the solution would be\n \n \n \n 1. upload a bug fix version to cocoapods - 1.6.1\n \n 2. If you guys can't wait, you can build the static libs from source. Make sure you're using the latest CMake (3.18).",
      "y": "1. upload a bug fix version to cocoapods - 1.6.1\n \n 2. Build the static libs from source. Make sure you're using the latest CMake (3.18)."
   },
   {
      "null": 231,
      "x": "Installation with non-root access",
      "z": "Have you tried `python setup.py install --user`?",
      "y": "Try `python setup.py install --user`."
   },
   {
      "null": 232,
      "x": "Add sparse softmax/log_softmax functionality (ignore zero entries)",
      "z": "Reopening as the PR 36305 resolves the issue for CPU, not for CUDA.",
      "y": "Sparse softmax support (CUDA)"
   },
   {
      "null": 233,
      "x": "The expanded size of the tensor (13) must match the existing size (12) at non-singleton dimension 1. Target sizes: [3, 13]. Tensor sizes: [12]",
      "z": "Please wait we will try to provide more context / traceback.",
      "y": "It's very likely a problem with a downstream library."
   },
   {
      "null": 234,
      "x": "Back propagation trough slicing with list breaks",
      "z": "I don't think this is a bug. You are backproping twice through the x->y graph.\n \n \n \n The difference in x->y is that the first version does a copy where the the second does a view. The copy relies on a buffer kept alive to to backprop while the second doesn't need one. So backproping the first version twice will error, as buffers are freed at backproping.\n \n \n \n In generally you should only backprop multiple times with `retain_graph=True`, as said in the error message.",
      "y": "You are backproping twice through the x->y graph."
   },
   {
      "null": 235,
      "x": "Official and nightly wheel structure plan",
      "z": "UPDATE: We're also planning to rename the `torch_nightly` wheels to `torch`, in order to ensure that you never have two copies of torch installed in the same environment.",
      "y": "Rename the `torch_nightly` wheels to `torch`."
   },
   {
      "null": 236,
      "x": "Mention future package dependency for python 2.7 in documentation",
      "z": "This should probably just be phrased as a suggestion for fixing the generic install documentation as requirements.txt includes future.",
      "y": "Fix the generic install documentation as requirements.txt includes future."
   },
   {
      "null": 237,
      "x": "Versions of TorchScript",
      "z": "No official schedule as of yet, but probably within the next few weeks",
      "y": "We've made some breaking serialization changes between versions (those will be detailed in the 1.2 release notes soon). We try to maintain backwards compatibility between versions, but we don't make any guarantees about forward compatibility at this point."
   },
   {
      "null": 238,
      "x": "Conv1D output changes",
      "z": "You are manually setting weight, but what about bias?",
      "y": "Set `bias=False.`"
   },
   {
      "null": 239,
      "x": "Documentation for Tensor.record_stream()",
      "z": "Tensor.record_stream can simplify this sort of use-case and we'll happily accept a PR documenting the method.\n \n \n \n The timeline (with record_stream commented out) looks like:\n \n \n \n ```\n \n GPU 1 default : [allocate copied] + -> + -> [copied.sum()] -> [delete copied]\n \n GPU 0 copy_stream: +-> + -> [copied.copy_(to_copy)] + ->\n \n GPU 0 default : [allocate to_copy] -> [fill to_copy with ones] -+-> [delete to_copy] -> [to_calc * 100] ->\n \n ```\n \n \n \n The `+` denote synchronization and the `->` denote sequential ordering within a stream. There are three synchronizations (ignoring the torch.cuda.synchronize calls at the beginning). The copy_stream.wait_stream is the first. The inter-GPU copy `to_copy.to(1)` does a synchronization before and after between copy_stream (on GPU 0) and the current stream on GPU 1 (which is GPU 1's default stream in this case)\n \n \n \n Note that there is no synchronization preventing the deletion of to_copy from happening before the use. (Also deletions always \"occur\" in the allocation stream regardless of the current stream).\n \n \n \n You can fix this with:\n \n \n \n ```python\n \n import torch\n \n \n \n # We will use GPU-0 and GPU-1.\n \n torch.cuda.synchronize(0)\n \n torch.cuda.synchronize(1)\n \n \n \n to_copy = torch.ones(100, device=0)\n \n to_calc = torch.rand(100, device=0)\n \n \n \n # Introduce a separate stream for copy and synchronize to the default stream.\n \n # The copy will be started when \"to_copy\" is ready.\n \n default_stream = torch.cuda.default_stream(0)\n \n copy_stream = torch.cuda.Stream(0)\n \n copy_stream.wait_stream(default_stream)\n \n \n \n with torch.cuda.stream(copy_stream):\n \n  # Both the copy and computation on \"to_calc\" will start at the same time.\n \n  # But the copy will be finished later.\n \n  torch.cuda._sleep(100000000)\n \n  copied = to_copy.to(1)\n \n \n \n # Here's any computation which allocates some new tensors on the default stream.\n \n to_calc * 100\n \n \n \n # Free \"to_copy\".\n \n default_stream.wait_stream(copy_stream)\n \n del to_copy\n \n \n \n to_calc * 200\n \n \n \n print(copied.sum().item())\n \n ```\n \n \n \n Now the timeline looks like:\n \n \n \n ```\n \n GPU 1 default : [allocate copied] + -> + -> [copied.sum()] -> [delete copied]\n \n GPU 0 copy_stream: +-> + -> [copied.copy_(to_copy)] + -> + \n \n GPU 0 default : [allocate to_copy] -> [fill to_copy with ones] -+-> [to_calc * 100] -> + -> [del to_copy] -> [to_cal * 200]\n \n ```",
      "y": "Tensor.record_stream can simplify this sort of use-case."
   },
   {
      "null": 240,
      "x": "Jetson TX2 crashes when running simple nn on GPU using libtorch",
      "z": "I am currently updating the jetson so will try again with the newer version in the next day or so",
      "y": "Update the jetson and try with the newer version."
   },
   {
      "null": 241,
      "x": "Dedicated channel for PyTorch nightlies; no more munging package names",
      "z": "I added you to the pytorch-nightly account.",
      "y": "Move nightlies to the new channel for Windows."
   },
   {
      "null": 242,
      "x": "Torch.Quantization: functions for determining supported modules/functionals/methods for quantized tensors",
      "z": "wouldn't it be sufficient (and pythonic) for the user to do:\n \n \n \n `dir(torch.nn.quantized)` and other equivalents?",
      "y": "`dir(torch.nn.quantized)`"
   },
   {
      "null": 243,
      "x": "Add Type Promotion for Bool types",
      "z": "It's also convenient to be able to do `(some_int64_tensor - other_int64_tensor).mean()`, this kind of computation is frequent in accuracy metrics calculation. Currently one should explicitly cast to float() before doing mean(): `RuntimeError: Can only calculate the mean of floating types. Got Bool instead.`",
      "y": "Currently one should explicitly cast to float() before doing mean()."
   },
   {
      "null": 244,
      "x": "conda installs CPU only version of pytorch-nightly during new package uploads (CPU and win64 are uploaded first)",
      "z": "The new nightly channel at https://anaconda.org/pytorch-nightly/pytorch should be fully operational. Please let me know if things don't work.\n \n \n \n FYI the cpu-only package was renamed to cpuonly.",
      "y": "The cpu-only package was renamed to cpuonly."
   },
   {
      "null": 245,
      "x": "Drop Python 2 support",
      "z": "Well, if 10% is the bar, looks like we're on track to drop support in 2020 :)",
      "y": "We're on track to drop support in 2020."
   },
   {
      "null": 246,
      "x": "Difference in the implementation of rmsprop with Tensorflow",
      "z": "Since I'm cc'd on this thread, I've had great success with my variant of RMSProp that tries to stay true to the TF version. I've trained quite a number of models with excellent results and so have quite a few others. Trying similar hparams with the PyTorch RMSProp results in unstable training and often immediate blow ups in training, it's basically not usable in my trials and I've never managed acceptable results.\n \n \n \n There are 3 main differences:\n \n * https://github.com/rwightman/pytorch-image-models/blob/master/timm/optim/rmsprop_tf.py#L80\n \n * https://github.com/rwightman/pytorch-image-models/blob/master/timm/optim/rmsprop_tf.py#L105\n \n * https://github.com/rwightman/pytorch-image-models/blob/master/timm/optim/rmsprop_tf.py#L111 \n \n \n \n I also tried changing a few order of ops to closer match TF but I doubt there was any impact whatsoever.",
      "y": "Variant of RMSProp that tries to stay true to the TF version."
   },
   {
      "null": 247,
      "x": "Weird result for inplace operation for a tensor with itself",
      "z": "duplicate of #8212",
      "y": "This is expected behavior and will not be fixed."
   },
   {
      "null": 248,
      "x": "pytorch cannot be installed under Windows 10 if Python 3.7 was installed from Microsoft Store",
      "z": "Okay, so for anyone else looking for the workaround: In the Windows Registry, within HKEY_LOCAL_MACHINE\\SYSTEM\\CurrentControlSet\\Control\\FileSystem you have to set LongPathsEnabled to 1 and reboot.",
      "y": "In the Windows Registry, within HKEY_LOCAL_MACHINE\\SYSTEM\\CurrentControlSet\\Control\\FileSystem you have to set LongPathsEnabled to 1 and reboot."
   },
   {
      "null": 249,
      "x": "[MKLDNN] Corrupted malloc metadata in mkldnn_convolution_backward_input",
      "z": "We will take a look at this. I can reproduce this issue on my skylake machine.",
      "y": "This problem has disappeared after using jemalloc by runing:\n `export LD_PRELOAD=jemalloc/lib/libjemalloc.so`\n So this problem is the alloctor's problem used by mkldnn."
   },
   {
      "null": 250,
      "x": "[JIT] Support mixed int/float math in python",
      "z": "Closed with https://github.com/pytorch/pytorch/pull/13793",
      "y": "Add ops between float & int, and change list equality output to be a boolean."
   },
   {
      "null": 251,
      "x": "[JIT] Don't support varargs in script",
      "z": "While discussing in https://github.com/pytorch/pytorch/issues/8814 about not supporting deprecated functions, I thought about this issue.\n \n \n \n We now have two ways of constructing tensors with `torch.zeros`, `torch.rand` etc: by either passing a tuple with the sizes (as in numpy, `torch.zeros((2, 3))`) or by passing the varargs (`torch.zeros(2, 3)`).\n \n \n \n I am personally used to passing the varargs (less things to type), but we before only had the `out` keyword argument, and it was rarely used. But now we have `dtype` / `device` etc and they are very handy and are going to be present a lot in the code.\n \n \n \n My question is: are we considering the `varargs` constructor deprecated in favor of the tuple construct? In this case, we might want to not support the varargs.\n \n \n \n > There should be one-- and preferably only one --obvious way to do it.\n \n > The Zen of Python\n \n \n \n cc @apaszke",
      "y": "We now have two ways of constructing tensors with `torch.zeros`, `torch.rand` etc: by either passing a tuple with the sizes (as in numpy, `torch.zeros((2, 3))`) or by passing the varargs (`torch.zeros(2, 3)`).\n \n \n \n I am personally used to passing the varargs (less things to type), but we before only had the `out` keyword argument, and it was rarely used. But now we have `dtype` / `device` etc and they are very handy and are going to be present a lot in the code."
   },
   {
      "null": 252,
      "x": "[JIT] Don't support pass tuple in a PythonOp",
      "z": "This is supported now as long as the Python Op has annotations",
      "y": "This is supported now for as long as the Python Op contains annotations."
   },
   {
      "null": 253,
      "x": "[CAffe2] Issue compiling with Cuda 9.2, YellowFinOp error",
      "z": "I solved it by compiling with gcc 6 instead of 7. I think some of the syntax isn't compatible with gcc 7.",
      "y": "Compile with gcc 6 instead of 7. Some of the syntax isn't compatible with gcc 7."
   },
   {
      "null": 254,
      "x": "Fatal error: THC/THC.h: no such file or directory",
      "z": "I think I'm seeing the same thing as you @vishwakftw, will let you know if I figure out what's up...",
      "y": "Try `export CUDA_NVCC_EXECUTABLE=$(which nvcc)`"
   },
   {
      "null": 255,
      "x": "Problem with terminating dataloader workers",
      "z": "Thank you!\n \n \n \n I just wanted to make sure that it is an expected behavior in pytorch <= 0.4 and not a bug.",
      "y": "It is an expected behavior in pytorch <= 0.4 and not a bug."
   },
   {
      "null": 256,
      "x": "[JIT] Don't support torch.tensor/Tensor/FloatTensor in Script",
      "z": "I believe torch.tensor() is already supported in JIT now, you can try it in 1.0.1 and in pytorch master.",
      "y": "torch.tensor() is already supported in JIT now."
   },
   {
      "null": 257,
      "x": "[bug report] ConvTranspose is not padding output with zero",
      "z": "It doesn't adds zeros to input either. It is used to resolve the ambiguity when one have `stride > 1`. E.g., consider a 3x3 conv with stride = 2. You would have:\n \n 1. `3x3 input => 1x1 output`\n \n 2. `4x4 input => 1x1 output`\n \n 3. `5x5 input => 2x2 output`\n \n 4. `6x6 input => 2x2 output`.\n \n \n \n So it is a many-to-one mapping. For convT to \"revert\" this shape change, it needs an extra argument like `output_padding` to know (e.g. from `2x2 input` whether it should get `5x5 output` or `6x6 output`).",
      "y": "It is used to resolve the ambiguity when one have `stride > 1`."
   },
   {
      "null": 258,
      "x": "Argmax performance slower than numpy",
      "z": "You can try to benchmark that by breaking it into two lines and using some cpp benchmarking. But I highly doubt that it is the issue because it exists in so many cuda function calls.",
      "y": "Benchmark that by breaking it into two lines and by using a cpp benchmarking."
   },
   {
      "null": 259,
      "x": "Pytorch 0.4.0: Model behavior changes heavily after save and load weights",
      "z": "Hi, in your `mydataset.py` (full script [here](https://gist.github.com/SsnL/84dea832cebf4ccc44c6f7126352508f)), there is this part\n \n ```py\n \n  for i, lab in enumerate(set(self.label_nms)):\n \n  self.label_ids[lab] = i\n \n ```\n \n \n \n Since `set` is unordered, the mapping between `label_nm` and `label_id` is not deterministic. (They maybe are Py2, but that is undefined behavior.) So you see different results on train vs test, and different test runs. \n \n \n \n Since those in `self.label_nms` are just strings of digits, I changed the dataset to just store `int(label_nm)` as labels. Train + test works as expected. [Here](https://gist.github.com/SsnL/5946f101e2ed7e83292054a1af258632) is a modified dataset file.\n \n \n \n ```\n \n Test set: Average loss: 1.7855, Accuracy: 44/90 (49%)\n \n Train Epoch: 20 [0/90 (0%)] Loss: 1.765595 Accuuracy: 0.48\n \n Test set: Average loss: 1.7540, Accuracy: 43/90 (48%)\n \n Test set: Average loss: 1.7540, Accuracy: 43/90 (48%)\n \n acc2= 0.4777777777777778\n \n \u00e2\u017e\u0153 ld_weights_debug python validate.py\n \n 0.4777777777777778\n \n \u00e2\u017e\u0153 ld_weights_debug\n \n ```",
      "y": "Since `set` is unordered, the mapping between `label_nm` and `label_id` is not deterministic. (They maybe are Py2, but that is undefined behavior.) So you see different results on train vs test, and different test runs."
   },
   {
      "null": 260,
      "x": "output_padding constraint",
      "z": "I see.\n \n I removed those checks, and handled that case in Conv, because it was needed for Conv double backward. When Conv with stride > 1, dilation > 1 happens, the double-backward will involve this condition, if I remember.",
      "y": "When Conv with stride > 1, dilation > 1 happens, the double-backward will involve this condition."
   },
   {
      "null": 261,
      "x": "Build CUDA extension in windows 10",
      "z": "C++ extensions are well supported on Windows, thanks to @peterjc123. \n \n \n \n @xiaoxiangyeyuwangye please check out https://github.com/pytorch/extension-cpp. C++ extensions are easier, actively maintained and the future.",
      "y": "C++ extensions are well supported on Windows."
   },
   {
      "null": 262,
      "x": "Runtime Error thrown when using Optimizer in a Pytorch Function: element 0 of tensors does not require grad and does not have a grad_fn",
      "z": "I think all you need to do is to wrap the call of the function (in the autograd.Function forward):\n \n ```\n \n  with torch.enable_grad():\n \n  theta = solve_logistic_regression(X, y, lamb)\n \n ```\n \n ...oh, and return something more reasonable than `0`.",
      "y": "Wrap the call of the function (in the autograd.Function forward) and return something more reasonable than `0`."
   },
   {
      "null": 263,
      "x": "torch.cuda.sparse.FloatTensor is not enabled.",
      "z": "Please shout if you have this problem, so I can prioritize it accordingly.",
      "y": "It was because we weren't initializing CUDA on all codepaths that we should have. Basically, if you do some cuda operation before you invoke torch.cuda.sparse.FloatTensor, that will be sufficient to workaround."
   },
   {
      "null": 264,
      "x": "NaN loss when using half precision",
      "z": "This worked for me only with changing eps=1e-4\n \n thanks!",
      "y": "Change eps=1e-4"
   },
   {
      "null": 265,
      "x": "import torch: DLL load failed: The operating system cannot run %1. (pip installation)",
      "z": "Replacing the existing numpy with `numpy?1.14.5+mkl?cp36?cp36m?win_amd64.whl` from https://www.lfd.uci.edu/~gohlke/pythonlibs/#numpy resolved the issue for me (but not before wasting hours in try-this-and-that).\n It would've been nice if this issue was mentioned on the PyTorch installation page.",
      "y": "Replace the existing numpy with `numpy?1.14.5+mkl?cp36?cp36m?win_amd64.whl`"
   },
   {
      "null": 266,
      "x": "[print] improve ModuleList print format",
      "z": "Yeah, we shouldn't print the values. But we don't show classes like `torch.FloatTensor` anymore, and explicit using these classes are discouraged. We should show `dtype` and `device` instead.\n \n \n \n We should remove the extra line either way.",
      "y": "Shouldn't print the values. But we don't show classes like `torch.FloatTensor` anymore, and explicit using these classes are discouraged. We should show `dtype` and `device` instead."
   },
   {
      "null": 267,
      "x": "[complex] torch.pow : Incorrect output",
      "z": "you need to modify your x initialization to something like `auto x = std::complex<float>(argc > 1? atof(argv[1]) : -1001.2,-1001.2);`, otherwise modern compilers are smart enough to evaluate const expression at compile time using double precision arithmetic",
      "y": "Modify x initialization to something like `auto x = std::complex<float>(argc > 1? atof(argv[1]) : -1001.2,-1001.2);`, If not, modern compilers are smart enough to evaluate const expression at compile time using double precision arithmetic."
   },
   {
      "null": 268,
      "x": "The RandomSampler is not be set as expected.",
      "z": "For the branch of false `self.replacement`, we should use `generator` rather than `self.generator`.\n \n We would like to accept a PR to fix the bug.",
      "y": "Use `generator` rather than `self.generator`."
   },
   {
      "null": 269,
      "x": "[JIT] nn.Sequential of nn.Module with input type List[torch.Tensor] inferred to torch.Tensor",
      "z": "The fix for this is to subclass `nn.Sequential` and redeclare `forward` with the input typed as a list of tensors. We'll add a note to the documentation, as it's not necessarily intuitive",
      "y": "Subclass `nn.Sequential` and redeclare `forward` with the input typed as a list of tensors."
   },
   {
      "null": 270,
      "x": "Why isn't torch._aminmax() present in the docs?",
      "z": "In [`/aten/src/ATen/native/native_functions.yaml`](https://github.com/pytorch/pytorch/blob/master/aten/src/ATen/native/native_functions.yaml), I see that the functions starting with an underscore are reserved for internal use, and are not a part of the public-facing API. So, I'm closing this issue.",
      "y": "Functions starting with an underscore are reserved for internal use, and are not a part of the public-facing API."
   },
   {
      "null": 271,
      "x": "Replace deprecated AT_ERROR with `TORCH_CHECK(false,` in `c10`",
      "z": "I want to take a shot at this one.",
      "y": "AT_ERROR should be replaced with TORCH_CHECK(false, ...) for user-facing errors, but TORCH_INTERNAL_ASSERT(...) for errors caused or encountered by the system that are independent of the user."
   },
   {
      "null": 272,
      "x": "can't run dictionary with string key as input to jit model in libtorch",
      "z": "Hey, the problem is that you're missing the double underscores on `__init___` and you have incorrect spacing. Missing the underscores means that you're defining a custom method called `init`, so Python generates a default implementation of `__init__` for you. This code works:\n \n \n \n ```python\n \n import torch\n \n from typing import Dict\n \n \n \n class M(torch.nn.Module):\n \n  def __init__(self, aligned_height, aligned_width, spatial_scale):\n \n  super(M, self).__init__()\n \n  self.aligned_width = int(aligned_width)\n \n  self.aligned_height = int(aligned_height)\n \n  self.spatial_scale = float(spatial_scale)\n \n \n \n  def forward(self, d: Dict[str, torch.Tensor]) -> torch.Tensor:\n \n  return d[\"feature\"]\n \n \n \n \n \n sc = torch.jit.script(M(1, 1, 0.125))\n \n torch.jit.save(sc, 'scriptmodule.pt')\n \n loaded = torch.jit.load(\"scriptmodule.pt\")\n \n d = dict(feature=torch.rand(2, 3))\n \n loaded(d)\n \n ```",
      "y": "Missing the double underscores on `__init___` and there is incorrect spacing. Missing the underscores means defining a custom method called `init`, so Python generates a default implementation of `__init__` ."
   },
   {
      "null": 273,
      "x": "conda installation from nighlty causes package conflicts and fails to install PyTorch",
      "z": "I guess the official main page should be updated, it doesn't even list availability of CUDA11.1, while it (wrongly) lists availability of nightly of 11.0.\n \n \n \n Stable:\n \n ![image](https://user-images.githubusercontent.com/1041752/109400054-c1bd1c80-7946-11eb-80d5-7a46021f1e60.png)\n \n \n \n Nightly:\n \n ![image](https://user-images.githubusercontent.com/1041752/109400091-f8933280-7946-11eb-81a0-89e98b6079fd.png)",
      "y": "Official main page should be updated, it doesn't even list availability of CUDA11.1, while it (wrongly) lists availability of nightly of 11.0."
   },
   {
      "null": 274,
      "x": "torch.norm behavior for negative p when reducing over empty dimension",
      "z": "I'm not a fan of the `np.linalg.norm` behavior, I'd have preferred if it acted like `min`/`max`.\n \n \n \n One argument for that is that a reduction should have a proper mathematical and not just a numerical identity. `sum([]) == 0` and `prod([]) == 1` holds for all dtypes, but if `max([])` would be `inf` rather than raise, what do you return for integer input ? Something like `iinfo(int32).max` would lead to a lot of bugs, and having a dtype-dependent output shape or exception seems wrong.",
      "y": "A reduction should have a proper mathematical and not just a numerical identity."
   },
   {
      "null": 275,
      "x": "Incorrect type annotation for DataLoader",
      "z": "Sounds good to fix all of those @ejguan. Note that `mypy-strict.ini` already has the `--no-implicit-optional` setting, which is applied to some of the key files like codegen and autograd. There's also other settings that differ between the two mypy ini files. Maybe we should open a new tracking issue and figure out which settings to apply to the whole codebase. For example, `implicit_reexport` is likely worth doing too, to avoid more `pyright`-user complaints.",
      "y": "`mypy-strict.ini` already has the `--no-implicit-optional` setting, which is applied to some of the key files like codegen and autograd. There's also other settings that differ between the two mypy ini files."
   },
   {
      "null": 276,
      "x": "Import Torch Error. TypeError: function() argument 1 must be code, not str",
      "z": "In that case, I suggest you should file an issue against https://github.com/tqdm/tqdm \n \n Please note, that tqdm dependency is optional so you can uninstall the package to workaround the issue.\n \n And for the reference, can you please share the version of tqdm package you are using? (Can be queried by running `import tqdm; print(tqdm.__version__)` cell)",
      "y": "tqdm dependency is optional so you can uninstall the package to workaround the issue."
   },
   {
      "null": 277,
      "x": "Illegal memory access in cuda max pooling for large inputs",
      "z": "Doing multiplication in int64_t is good, can you please submit a fix? Posting a couple benchmarks would also be good.",
      "y": "Multiplication in int64_t is good."
   },
   {
      "null": 278,
      "x": "setup.py sdist does not include third party submodules",
      "z": "> \n \n > \n \n > Worse than that, it actually does not include most of the C++ source code :(\n \n \n \n That portion should be easy to solve though with a `MANIFEST.in`: https://packaging.python.org/guides/using-manifest-in/",
      "y": "That portion should be easy to solve though with a `MANIFEST.in`."
   },
   {
      "null": 279,
      "x": "Make `aten::adaptive_avg_pool3d` work for quantized Tensor inputs",
      "z": "On it",
      "y": "Quantized adaptive_avg_pool3d"
   },
   {
      "null": 280,
      "x": "[dist_autograd] GPU continuations does not work in distributed autograd",
      "z": "It looks like the RPC tutorial at https://github.com/pytorch/examples/blob/master/distributed/rpc/parameter_server/rpc_parameter_server.py is also broken, with the same error @pritamdamania87 posted. This was working earlier, not sure at what point it regressed.",
      "y": "Add basic GPU support to distributed autograd."
   },
   {
      "null": 281,
      "x": "[JIT] Support `with torch.autograd.profiler.record_function`",
      "z": "Someone has submitted a PR for this.",
      "y": "TorchScript Context Manager Support."
   },
   {
      "null": 282,
      "x": "RuntimeError: Only Tensors of floating point dtype can require gradients",
      "z": "Hi,\n \n \n \n You should **never** use `.data` :D By using it you can many many things that you should not be able to do. We're in the progress of deprecating and removing it.\n \n \n \n > After this line, the module's weight will be a long Tensor with requires_grad=True\n \n \n \n Yes this test looks like it should be updated. Making nn.Parameter with long type does not make sense (and that's why the test does forward only and has to use the internal `_apply` API to be able to achieve this).\n \n I think we should just change these to regular Tensors to avoid these issues.",
      "y": "Never use `.data`."
   },
   {
      "null": 283,
      "x": "Py pip installation error,zipfile.BadZipFile: Bad CRC-32 for file 'torch/lib/cudnn64_7.dll",
      "z": "you may also want to try the `no-cache` option, wondering whether it is caused by a corrupted ache.\n \n \n \n `pip install torch===1.5.1 torchvision===0.6.1 -f https://download.pytorch.org/whl/torch_stable.html --no-cache`",
      "y": "Try the `no-cache` option."
   },
   {
      "null": 284,
      "x": "RuntimeError: _th_exp_out not supported on CUDAType for Long",
      "z": "BCEWithLogitsLoss expects both preds and labels to be floating point (they are not labels, they are logits btw, so it makes sense that they are floating point.",
      "y": "BCEWithLogitsLoss needs both preds and labels to be floating point"
   },
   {
      "null": 285,
      "x": "scatter_ throwing a RunTimeError",
      "z": "I can confirm this bug exists on 1.5.0 and 1.5.1 but appears to be fixed in master. I am closing this issue because it has been fixed on master.",
      "y": "Allow index/src to have one dimension more than `self.dim` if their sizes are 1"
   },
   {
      "null": 286,
      "x": "Exponential distribution invalid parameter",
      "z": "Distributions have a `validate_args` argument (default is False). If you set it True, I think it will do what you're looking for.",
      "y": "Set `validate_args` argument to True."
   },
   {
      "null": 287,
      "x": "\"log_softmax_lastdim_kernel_impl\" not implemented for 'Long'",
      "z": "CrossEntropyLoss expects floating point inputs and long labels.",
      "y": "CrossEntropyLoss needs floating point inputs and long labels."
   },
   {
      "null": 288,
      "x": "mypy doesn't recognize torch functions that start with `_`",
      "z": "gh-40499 indeed fixed this, so closing. thanks @diego-plan9, all",
      "y": "Add __all__ to torch/_C/_VariableFunctions.pyi."
   },
   {
      "null": 289,
      "x": "typing is missing for most optimizers",
      "z": "That may very well be true. 1.6.0 is coming soon so it's unlikely there will be a 1.5.2 release to fix this issue.",
      "y": "1.6.0 is coming soon so it's unlikely there will be a 1.5.2 release to fix this issue."
   },
   {
      "null": 290,
      "x": "[v1.6.0] Release Tracker",
      "z": "See discussion in [#40452](https://github.com/pytorch/pytorch/pull/40452#issuecomment-657649332) the user wants to see at least change to CUDAStream.h so that they can build 1.6 with cuda11. Let me know if you want a PR with change to CUDAStream.h change only, or if we should cherry-pick all of #40452\n \n \n \n --- \n \n @malfet: Cherry-picked https://github.com/pytorch/pytorch/pull/40452 as https://github.com/pytorch/pytorch/pull/41543 and merged.",
      "y": "Change to CUDAStream.h so that we can build 1.6 with cuda11"
   },
   {
      "null": 291,
      "x": "libcurand.so.8.0: cannot open shared object file",
      "z": "you have an older pytorch install somehow on your machine.",
      "y": "Install latest PyTorch version."
   },
   {
      "null": 292,
      "x": "AttributeError: 'Conv2d' object has no attribute 'padding_mode' when loading model from pytorch 1.0 to 1.1",
      "z": "This should work, we can probably fix it by making the deserializer smarter.",
      "y": "Comment out the lines that use self.padding_mode in module.py, the model can then be imported, and saving the state_dictionary instead of the whole model allows loading into an unmodified version of pytorch 1.1."
   },
   {
      "null": 293,
      "x": "torch.hub does not close the resource before removing",
      "z": "In the _get_cache_or_reload function, add the \"cached_zipfile.close()\" call after the cached_zipfile.extractall(hub_dir) function to fix the bug.",
      "y": "Add the \"cached_zipfile.close()\" call after the cached_zipfile.extractall(hub_dir) function in the _get_cache_or_reload function."
   },
   {
      "null": 294,
      "x": "grid_sample is not aligned",
      "z": "Yes. I agree that the convention for `F.grid_sample` should be updated to match that of `align_corners=False` in `F.interpolate`.\n \n This would allow `F.grid_sample` to be agnostic to the size of the sampled image.\n \n \n \n The way it is currently defined, -1 and 1 refer to the centers of the edge pixels, which is not a consistent location on images of different sizes. To make this size-agnostic, these should refer to the edges of the image themselves, which doesn't change upon upsampling/downsampling the image.\n \n \n \n I have been working for almost a year now with a wrapped version of `F.grid_sample` which does just that, similar to what you use, since I have needed very precise behavior for my research. Mine is a bit more simplified, since note, for example, that in the wrapping function it is sufficient to multiply the grid by `size` and divide by `size - 1` (your conversion simplifies to this if you work it out analytically).\n \n It seems appropriate, however, that this should be the default behavior.\n \n \n \n If we decide to update the convention here, then we should also update the convention in `F.affine_grid` to match this as well.\n \n \n \n One potential issue with updating the convention is that it would break any existing user code that uses `F.grid_sample` on grids not generated from `F.affine_grid`. A possible way forward could be to do something similar to the way `F.upsample` and `F.interpolate` were changed, by adding a flag that reverts to the old behavior, so that users can keep the existing convention if they choose.\n \n \n \n @SsnL and @soumith What do you think?",
      "y": "Convention for `F.grid_sample` should be updated to match that of `align_corners=False` in `F.interpolate`.\n This would allow `F.grid_sample` to be agnostic to the size of the sampled image."
   },
   {
      "null": 295,
      "x": "libtorch terminate called after throwing an instance of 'c10::Error'",
      "z": "This is the correct error.\n \n \n \n The return value of model->forward is a C10::IValue.\n \n Faceboxes model returns a Tuple, not a Tensor.\n \n \n \n You need to do this:\n \n \n \n ```C++\n \n torch::jit::Stack outputs = model->forward({input}).toTuple()->elements();\n \n ```\n \n \n \n The variable `outputs` now contains your bounding boxes and your confidence tensors.",
      "y": "```C++\n \n torch::jit::Stack outputs = model->forward({input}).toTuple()->elements();\n \n ```\n \n \n \n The variable `outputs` now contains your bounding boxes and your confidence tensors."
   },
   {
      "null": 296,
      "x": "pytorch1.1.0 windows than one operator \" \" matches these operands",
      "z": "No, you don't need to do that. Just change `setup.py` to the following:\n \n ```python\n \n #!/usr/bin/env python3\n \n import os\n \n import torch\n \n \n \n from setuptools import setup, find_packages\n \n from torch.utils.cpp_extension import BuildExtension, CUDAExtension\n \n \n \n cxx_args = ['-std=c++11']\n \n \n \n nvcc_args = [\n \n '-gencode', 'arch=compute_50,code=sm_50',\n \n '-gencode', 'arch=compute_52,code=sm_52',\n \n '-gencode', 'arch=compute_60,code=sm_60',\n \n '-gencode', 'arch=compute_61,code=sm_61',\n \n '-gencode', 'arch=compute_70,code=sm_70',\n \n '-gencode', 'arch=compute_70,code=compute_70',\n \n '-D__CUDA_NO_HALF_OPERATORS__' # <-- Just add this line\n \n ]\n \n \n \n setup(\n \n name='correlation_cuda',\n \n ext_modules=[\n \n CUDAExtension('correlation_cuda', [\n \n 'correlation_cuda.cc',\n \n 'correlation_cuda_kernel.cu'\n \n ], extra_compile_args={'cxx': cxx_args, 'nvcc': nvcc_args})\n \n ],\n \n cmdclass={\n \n 'build_ext': BuildExtension\n \n })\n \n ```",
      "y": "Change `setup.py` to the following:\n \n ```python\n \n #!/usr/bin/env python3\n \n import os\n \n import torch\n \n \n \n from setuptools import setup, find_packages\n \n from torch.utils.cpp_extension import BuildExtension, CUDAExtension\n \n \n \n cxx_args = ['-std=c++11']\n \n \n \n nvcc_args = [\n \n '-gencode', 'arch=compute_50,code=sm_50',\n \n '-gencode', 'arch=compute_52,code=sm_52',\n \n '-gencode', 'arch=compute_60,code=sm_60',\n \n '-gencode', 'arch=compute_61,code=sm_61',\n \n '-gencode', 'arch=compute_70,code=sm_70',\n \n '-gencode', 'arch=compute_70,code=compute_70',\n \n '-D__CUDA_NO_HALF_OPERATORS__' # <-- Just add this line\n \n ]\n setup(\n \n name='correlation_cuda',\n \n ext_modules=[\n \n CUDAExtension('correlation_cuda', [\n \n 'correlation_cuda.cc',\n \n 'correlation_cuda_kernel.cu'\n \n ], extra_compile_args={'cxx': cxx_args, 'nvcc': nvcc_args})\n \n ],\n \n cmdclass={\n \n 'build_ext': BuildExtension\n \n })\n \n ```"
   },
   {
      "null": 297,
      "x": "expected ) but found 'ident' here: quantized::fake_quantize_per_tensor_affine_forward",
      "z": "For me, the problem occurs when using Sphinx Autodoc. It seems that the module import fails when running Sphinx, but it works in plain Python...\n \n \n \n In case anybody else encounters this issue with Sphinx, setting `autodoc_mock_imports = ['torch']` in the conf.py file is another way to work around this problem.",
      "y": "Problem occurs when using Sphinx Autodoc. It seems that the module import fails when running Sphinx, but it works in plain Python."
   },
   {
      "null": 298,
      "x": "Stabilize gradient for unfolded Tensor",
      "z": "You are right. I didn't properly think this through. Thanks for the heads up.",
      "y": "If you do finite differences, the gradient check wont match I think, except if you change the behavior during forward to divide the pixels of the output image by the number of overlapping regions."
   },
   {
      "null": 299,
      "x": "10% difference noticed between jit and python model",
      "z": "If you are hard blocked by this, one way you can help the JIT team out is by minimizing the test case. So remove parts of the transformer network while preserving the numerical discrepancy.",
      "y": "Remove parts of the transformer network while preserving the numerical discrepancy."
   },
   {
      "null": 300,
      "x": "torch.Size is not pickleable in Python 2",
      "z": "yes that would be great!",
      "y": "torch dtype object is already serializable."
   },
   {
      "null": 301,
      "x": "NervanaSystems/nervanagpu Repository not found for 0.4.0",
      "z": "I remove `nervanagpu` by doing this:\n \n ```bash\n \n git submodule deinit third_party/nervanagpu\n \n git rm --cached third_party/nervanagpu\n \n ```",
      "y": "```bash\n \n git submodule deinit third_party/nervanagpu\n \n git rm --cached third_party/nervanagpu\n \n ```"
   },
   {
      "null": 302,
      "x": "Different behavior numpy / pytorch with broadcasting & in-place operators",
      "z": "This is a duplicate of https://github.com/pytorch/pytorch/issues/906 , https://github.com/pytorch/pytorch/issues/10756 and https://github.com/pytorch/pytorch/issues/8212\n \n \n \n We are discussing on a potential solution to this problem by solving a diophantine equation like numpy does https://github.com/pytorch/pytorch/issues/8212, but to the best of my knowledge nobody is working on it.\n \n \n \n I'm closing this issue as a duplicate, but I'm bumping the priority of of https://github.com/pytorch/pytorch/issues/8212 to high, because it's a recurrent issue.",
      "y": "Solution to this problem is by solving a diophantine equation like numpy does."
   },
   {
      "null": 303,
      "x": "cublas runtime error on torch.bmm() with CUDA10 and RTX2080Ti",
      "z": "You need cuda 10 version of pytorch to run on RTX.",
      "y": "cuda 10 version of pytorch needs to be installed to run on RTX."
   },
   {
      "null": 304,
      "x": "BatchNorm2d implementation returns different results than expected",
      "z": "you are not in eval mode",
      "y": "Need to be in eval mode."
   },
   {
      "null": 305,
      "x": "A bug for torch.clone() when clone a MKLDNN tensor",
      "z": "I have submitted a [PR](https://github.com/pytorch/pytorch/pull/20943) about it, thanks!",
      "y": "Fix the bug for torch.clone() when clone a MKLDNN tensor"
   },
   {
      "null": 306,
      "x": "torch.empty(n, dtype=torch.int) produces non-deterministic arrays, not empty ones",
      "z": "That's the expected behavior. `torch.empty` doesn't initialize the data, and just returns whatever was in that position in memory. If you want the data to be initialized with some number, use `torch.zeros` or `torch.full`",
      "y": "`torch.empty` doesn't initialize the data, and just returns whatever was in that position in memory. If you want the data to be initialized with some number, use `torch.zeros` or `torch.full`"
   },
   {
      "null": 307,
      "x": "[jit] Changes to TorchScript API",
      "z": "We should deprecate inheriting from `ScriptModule`. It's a very weird API that I often find hard to explain to people (in contrary to the simple `torch.jit.script` annotation).",
      "y": "Inheriting from `ScriptModule` should not be allowed."
   },
   {
      "null": 308,
      "x": "Tensor identity comparisons not working / unclear",
      "z": "Tensor.data returns an alias, just like detach() -- same underlying data but different Tensor object. Two evaluations of `x.data` do **not** refer to the same thing.\n \n \n \n > Shouldn't either both of the last two statements be True, or neither? It seems odd that 'is' gives a different result than id()==id().\n \n \n \n No, this is because `id` returns the memory address and memory addresses can be reused once an object goes out scope. `a.data` goes out of scope before `b.data` is evaluated. `id` only uniquely identify an object **during that object's lifetime!** The same often happens with core Python objects:\n \n \n \n ```\n \n id({}) == id({}) # True\n \n {} is {} # False\n \n ```\n \n \n \n If you keep the object in scope the id is unique:\n \n \n \n ```\n \n tmp0 = a.data\n \n tmp1 = b.data\n \n id(tmp0) == id(tmp1) # False\n \n ```",
      "y": "Tensor.data returns an alias like detach() -- same underlying data but different Tensor object. Two evaluations of `x.data` do **not** refer to the same thing."
   },
   {
      "null": 309,
      "x": "assignment to a twice-sliced tensor does nothing",
      "z": "assignment will call __newindex__ on the same tensor",
      "y": "Because assignment will call __newindex__ on the same tensor"
   },
   {
      "null": 310,
      "x": "Windows 10/Python 3.5 Pytorch 1.1.0 CPU increase versus 1.0.0 mnist",
      "z": "because of a stupid bug, 1.0.0 did not ship with OpenMP enabled. 1.1.0 shipped with OpenMP enabled, and uses multiple cores. This is in line with expectations. MNIST as a workload is really small, so OpenMP's multithread optimizations are probably slowing down the workload because most of the time is simply spent in thread overhead.\n \n This is fixed on master because we moved away from OpenMP to our own threadpool which has more reasonable characteristics.",
      "y": "Due to a bug, version 1.0.0 did not ship with OpenMP enabled. 1.1.0 shipped with OpenMP enabled, and uses multiple cores."
   },
   {
      "null": 311,
      "x": "JIT-compiled function produces incorrect results",
      "z": "Thanks for the repro steps. Looking into this.",
      "y": "The problem is that there is a dependency from `sqrt` to `+` in function `f`, due to which we need to add `SyncThreads` between those statements."
   },
   {
      "null": 312,
      "x": "Model output difference between android device and desktop",
      "z": "can you share the qconfig you used? If you are running things on `fbgemm`, `reduce_range` needs to be set to True to avoid overflow in some of the kernels.",
      "y": "On `fbgemm`, `reduce_range` needs to be set to True to avoid overflow in some of the kernels."
   },
   {
      "null": 313,
      "x": "[complex] torch.sigmoid: sigmoid does not support automatic differentiation for outputs with complex dtype",
      "z": "Thank you for filing this issue, @kshitij12345. Note that SciPy's expit does not even support complex input:\n \n \n \n ```\n \n scipy.special.expit(np.array(1, dtype=np.cdouble))\n \n : TypeError: ufunc 'expit' not supported for the input types, and the inputs could not be safely coerced to any supported types according to the casting rule ''safe''\n \n ```\n \n \n \n However, if the derivative is well-defined for complex inputs then we would accept a PR implementing the behavior.\n \n \n \n As an aside, PyTorch's use of the name \"sigmoid\" for this function seems odd. \"Logistic\" or \"expit\" are probably better names.",
      "y": "SciPy's expit does not support complex input."
   },
   {
      "null": 314,
      "x": "NCCL backend doesn't use MASTER_PORT during reconnect",
      "z": "In that case, is it ok to close out this issue? As Omkar mentioned above you need to restart everything and reinitialize a new process group or use torchelastic.",
      "y": "Restart everything and reinitialize a new process group or use torchelastic."
   },
   {
      "null": 315,
      "x": "PyTorch cannot be linked (libtorch_cuda.so) at the last step of compilation with CUDA 11.1 with PyTorch/builder",
      "z": "Try `export TORCH_CUDA_ARCH_LIST=\"8.6\"`.",
      "y": "`export TORCH_CUDA_ARCH_LIST=\"8.6\"`"
   },
   {
      "null": 316,
      "x": "[FR] add huber option for smooth_l1_loss",
      "z": "I'd start a little earlier in the discussions -- why did we add beta without huber? (See https://github.com/pytorch/pytorch/issues/16357 for the request for beta). Does anyone actually want `beta` and not `huber`?\n \n \n \n Maybe @fmassa has an opinion.",
      "y": "Add a new `huber` arg (default `False`) to the existing `SmoothL1Loss`."
   },
   {
      "null": 317,
      "x": "torch.nn.Module.named_parameters has wrong type annotation",
      "z": "I believe this issue can be closed since gh-49045 was merged.",
      "y": "add type annotations to torch.nn.modules.module"
   },
   {
      "null": 318,
      "x": "Torchscript does not work with type: ignore comments for mypy",
      "z": "The proper fix has landed, you shouldn't need to use the workaround in my earlier comment any more (provided you use recent enough pytorch build)",
      "y": "The bug is due to an assumption in TorchScript when parsing source code: In function declaration, any line that contains `# type:` is considered to be a type line. Then TorchScript lexer expects the type line to contain only type comment, nothing else."
   },
   {
      "null": 319,
      "x": "Enhanced generators with grad-mode decorators",
      "z": "Thank you for the suggestion. If you have a PR in mind, please do not hesitate to publish it.",
      "y": "Support for enhanced generators (generator-based coroutines) by grad-mode decorators."
   },
   {
      "null": 320,
      "x": "__torch_function__ PR may cause performance regression on GPU training",
      "z": "> @robieta Thanks for the fix. I've tested the head of #48966 PR at [6287057](https://github.com/pytorch/pytorch/commit/6287057c90a5a0450a0bfd2e0c1d0fbe8fa02b23). The performance of resnet is well restored. I hope to see that PR gets merged soon.\n \n \n \n Awesome! Yeah, it should go in shortly.",
      "y": "Skip the `if not torch.jit.is_scripting():` guards on functional and nn.functional by directly registering `has_torch_function` and `object_has_torch_function` to the JIT as statically False."
   },
   {
      "null": 321,
      "x": "[online docs] anchors to source code are missing",
      "z": "Thanks for the report. I think we tracked down the issue. I should have a fix in today.",
      "y": "In 1.6.0 the `id=\"Module.eval\"` exists."
   },
   {
      "null": 322,
      "x": "[docs] torch.nn.functional.one_hot docs missing",
      "z": "Just to help those that stumble across this question (like I did), you can find the documentation here:\n \n https://pytorch.org/docs/stable/nn.functional.html",
      "y": "Find the documentation here:\n \n https://pytorch.org/docs/stable/nn.functional.html"
   },
   {
      "null": 323,
      "x": "Missing gradient when autograd called inside a function on Multi-GPU (eg gradient penalty)",
      "z": "> curious: does this work pre [89d56ae](https://github.com/pytorch/pytorch/commit/89d56ae435373e54932566806c8432634e6a5cfa)\n \n \n \n Verified: it works on 89d56ae^1 and breaks on 89d56ae.\n \n \n \n @gchanan any suggestion on fixes?",
      "y": "The `use_count` of function `shared_ptr` are incorrect on C++ side as we create multiple `shared_ptr` from raw pointers when replicate modules."
   },
   {
      "null": 324,
      "x": "Apply for translation of the Chinese version, I hope to get authorization!",
      "z": "We are very happy to do this meaningful thing.\n \n We have iterated three Chinese versions, and we are very grateful to the translation and participation of many Chinese contributors. Of course, the most should be the pytorch tutorial, which really helps us to learn more comprehensive and systematic learning of deep learning.\n \n Everyone said that they really like your framework and your team. This is why we have been insisting on translation.",
      "y": "Iterated three Chinese versions."
   },
   {
      "null": 325,
      "x": "C++ link error",
      "z": "In case anyone has a similar issue, build the lib from source.\n \n This seems like an issue which your local project compiler is incompatible with the compiler building libtorch. \n \n More detail in #15138",
      "y": "Build the lib from source."
   },
   {
      "null": 326,
      "x": "cannot initialize type \"_CudaDeviceProperties\" error",
      "z": "OK, it is indeed a race. This diff, for example, \"fixes\" it:\n \n ```\n \n diff --git a/torch/cuda/__init__.py b/torch/cuda/__init__.py\n \n index 411cfb7315..5abb645fc5 100644\n \n --- a/torch/cuda/__init__.py\n \n +++ b/torch/cuda/__init__.py\n \n @@ -165,6 +165,7 @@ def _lazy_init():\n \n  global _initialized, _cudart, _original_pid, _queued_calls\n \n  if _initialized:\n \n  return\n \n + _initialized = True\n \n  if _in_bad_fork:\n \n  from sys import version_info\n \n  if version_info < (3, 4):\n \n @@ -181,7 +182,6 @@ def _lazy_init():\n \n  _cudart.cudaGetErrorName.restype = ctypes.c_char_p\n \n  _cudart.cudaGetErrorString.restype = ctypes.c_char_p\n \n  _original_pid = os.getpid()\n \n - _initialized = True\n \n  # Important to do this after _initialized, since some queued calls\n \n  # may themselves call _lazy_init()\n \n  for queued_call, orig_traceback in _queued_calls:\n \n ```\n \n \n \n If you look at the relevant segment:\n \n \n \n ```\n \n  torch._C._cuda_init()\n \n  _cudart = _load_cudart()\n \n  _cudart.cudaGetErrorName.restype = ctypes.c_char_p\n \n  _cudart.cudaGetErrorString.restype = ctypes.c_char_p\n \n  _original_pid = os.getpid()\n \n  _initialized = True\n \n ```\n \n \n \n The problem is that `_cuda_init` release the GIL at some point, which means that another Python thread can come in and trigger the same initialization (we aren't protected against the lock until we set `_initialized = True`.",
      "y": "`_cuda_init` release the GIL at some point, which means that another Python thread can come in and trigger the same initialization (we aren't protected against the lock until we set `_initialized = True`."
   },
   {
      "null": 327,
      "x": "High scope of error in how view() may be used",
      "z": "`view()` is the same as `reshape`, which is the same as [numpy's reshape](https://docs.scipy.org/doc/numpy-1.15.0/reference/generated/numpy.reshape.html).\n \n \n \n If you feel that more examples should be added to the doc, we will accept PR for that.",
      "y": "`view()` is the same as `reshape`."
   },
   {
      "null": 328,
      "x": "Error on backward pass with sparse matmul with transposed tensor",
      "z": "I think I figured out what is causing this, if so will put up a PR to fix it today",
      "y": "Don't attempt to multiply by a sparse matrix."
   },
   {
      "null": 329,
      "x": "Why is `torch.mean()` so different from `numpy.average()`?",
      "z": "I developed a [pytorch_math](https://github.com/DanielAtKrypton/pytorch_math) package that bridges the difference gap. See usage example [here](https://github.com/DanielAtKrypton/pytorch_math/blob/master/tests/average_test.py#L62).",
      "y": "`pytorch_math` package that bridges the difference gap."
   },
   {
      "null": 330,
      "x": "Make it possible to use mypy to typecheck code that uses PyTorch",
      "z": "> The main issue after that is how to make sure that the annotations are actually correct.\n \n \n \n After gh-52408 and gh-54234 we now have a nice compact way to add tests for type annotations. And anecdotally, the number of type annotation errors slipping through or breaking in CI has reduced over time.\n \n \n \n Mypy is upgraded to the latest version (0.812) and runs on almost all files now, there's only a handful of `ignore_errors` for files under `torch/` and (with 2 exceptions for which PRs have been open for some time). So we're in pretty decent shape here.\n \n \n \n There are some open issues left with `module: typing`, but those are all smaller things and can be handled like any other issue. I think we can declary victory here and close this tracking issue.",
      "y": "Mypy is upgraded to the latest version (0.812) and runs on almost all files now, there's only a handful of `ignore_errors` for files under `torch/` ."
   },
   {
      "null": 331,
      "x": "Weird behavior of torch.int()",
      "z": "number of precision for printing tensors is 4 by default. try [torch.set_printoptions](https://pytorch.org/docs/master/torch.html#torch.set_printoptions).\n \n ```python\n \n torch.set_printoptions(precision=10)\n \n v1 = torch.Tensor([1.00001, 0.99999])\n \n v2 = torch.Tensor([-0.99999, -1.000001])\n \n print(v1, v2) # tensor([1.0000100136, 0.9999899864]) tensor([-0.9999899864, -1.0000009537])\n \n print(v1.int(), v2.int()) # tensor([1, 0], dtype=torch.int32) tensor([ 0, -1], dtype=torch.int32)\n \n ```",
      "y": "Number of precision for printing tensors by default is 4."
   },
   {
      "null": 332,
      "x": "[JIT] Sometimes rewrite argument name",
      "z": "Amazing @zdevito :)",
      "y": "Preserve method parameter names."
   },
   {
      "null": 333,
      "x": "torch::serialization.load() doesn't support pathlib.Path object for the input argument",
      "z": "Seems reasonable to add. We'll happily accept a PR with the change and a test.",
      "y": "Load success from the any kind of '*.pth.tar' or some other extensions."
   },
   {
      "null": 334,
      "x": "test_proper_exit is flaky",
      "z": "Quick tip: \"module: build\" means build system problems; it doesn't refer to ci flakiness :)",
      "y": "\"module: build\" means build system problems; it doesn't refer to ci flakiness."
   },
   {
      "null": 335,
      "x": "Potential instability in ConvTranspose2d with cudnn",
      "z": "apparently a workaround is to use `cudnn.benchmark=True`. @ngimel is looking if a fix is possible. We'll try our best to get it into v1.0.1",
      "y": "Use `cudnn.benchmark=True` ."
   },
   {
      "null": 336,
      "x": "Allow C++ inference of JITted modules to run different modules on different streams",
      "z": "Thanks for the suggestion!\n \n \n \n On adding support of `with torch.cuda.stream(...):` in TorchScript, we'd prefer to avoid it or at least wait until it's absolutely necessary. The reason is that this API hard-codes pretty explicitly device (gpu) and it'd be harder to move the saved model to CPU later. Similarly, for inference cases, it might be preferable to control stream behavior in a central fashion - for example, be able to instruct jit::Interpreter to run only on X stream. Design-wise, it's preferable to expose more abstract parallelization primitives. We have a fork/join prototype (https://github.com/pytorch/pytorch/blob/master/test/test_jit.py#L12172) and it would be logical to extend this mechanism to introduce parallel streams.\n \n \n \n However, nothing prevents us from manipulating streams directly in C++. `torch.cuda.stream` are just a thin wrappers around c10::Stream and the current stream gets stored in a thread-local variable. Thus if one puts `CUDAStreamGuard` around module invocation in C++ it should make all computation run on that stream transparently. Let me know if it doesn't work for some reason.",
      "y": "Add support of `with torch.cuda.stream(...):` in TorchScript."
   },
   {
      "null": 337,
      "x": "torch.to_dense() adds random values",
      "z": "Your random indices tensor can generate multiple entries with value `1` but same index, hence the `2`s (and even `3`s if you are lucky enough).",
      "y": "Random indices tensor can generate multiple entries with value `1` but same index."
   },
   {
      "null": 338,
      "x": "Installing Windows 10",
      "z": "Hey @MohMehrnia \n \n \n \n Can you verify that you have the 64-bit installation of Python? By running this command: `python -c \"import struct; print(struct.calcsize('P') * 8)\"`. If it's 32-bit, try re-installing 64-bit Python.\n \n \n \n Others might have another idea well.\n \n \n \n Thanks\n \n James",
      "y": "Have the 64-bit installation of Python."
   },
   {
      "null": 339,
      "x": "About a conditional expression of cutoffs in torch.nn.AdaptiveLogSoftmaxWithLoss.",
      "z": "I think this has been fixed already (sorry @Joel-hanson ). The relevant work was done on Fix AdaptiveLogSoftmaxWithLoss's constructor by @t-ken1 and @wbydo",
      "y": "Fix AdaptiveLogSoftmaxWithLoss's constructor"
   },
   {
      "null": 340,
      "x": "[JIT] Additional list methods in script",
      "z": "Current status:\n \n \n \n - [x] clear (https://github.com/pytorch/pytorch/pull/17050)\n \n - [x] pop (https://github.com/pytorch/pytorch/pull/17015)\n \n - [x] reverse (https://github.com/pytorch/pytorch/pull/17001)\n \n - [x] copy (https://github.com/pytorch/pytorch/pull/17092)\n \n - [x] extend (https://github.com/pytorch/pytorch/pull/17092)\n \n - [x] insert (https://github.com/pytorch/pytorch/pull/17200)\n \n - [x] remove (https://github.com/pytorch/pytorch/pull/17200)\n \n - [x] index (https://github.com/pytorch/pytorch/pull/17446)\n \n - [x] count (https://github.com/pytorch/pytorch/pull/17446)\n \n - [ ] sort",
      "y": "Clear, Pop, Reverse, Copy, Extend, Insert, Remove, Index, Count are added. Sort is yet to be added."
   },
   {
      "null": 341,
      "x": "JIT Script module support for pad_packed_sequence and pack_padded_sequence",
      "z": "just to confirm: you get an error similar to this, right?\n \n \n \n ```python\n \n for operator (Tensor 0, Tensor 1, Tensor 2, Tensor 3) -> Tensor:\n \n expected a value of type Tensor for argument '2' but found bool\n \n \n \n rnn_input = pack_padded_sequence(\n \n  embedded,\n \n  seq_lengths.int(), \n \n  True,\n \n  ~~~~~~ <--- HERE\n \n  True)\n \n ```",
      "y": "Support added for pack_padded_sequence and pad_packed_sequence."
   },
   {
      "null": 342,
      "x": "[Feature] Sparse tensor persistence/save.",
      "z": "if I mark this as high-priority will it get fixed?",
      "y": "Implement pickle support for sparse tensors and torch.layout instances."
   },
   {
      "null": 343,
      "x": "[jit] Autodiff crash when some chunk outputs aren't used in backward",
      "z": "thanks! @wanchaol can you take a look?",
      "y": "Change the chunk autodiff formula to handle this case where some of the gradients are undefined."
   },
   {
      "null": 344,
      "x": "[Feature] 'None' for arbitrarily shaped buffer/parameter when loading models.",
      "z": "what you are asking for is to legitimize a hacky, possibly removable feature of `register_buffer`, so no :)\n \n \n \n I'd say, `state_dict` is such a simple stupid format, that writing a `load_model(model, state_dict)` that works for your model is the way to go",
      "y": "Write a `load_model(model, state_dict)` that works for your model."
   },
   {
      "null": 345,
      "x": "[python setup.py install] g++ error: stub.o file format not recognized",
      "z": "Closing this issue. I found what was wrong and **I fixed it**. The whole problem lies in the fact that Anaconda distribution comes with its own `ld` linker that is located in `/opt/anaconda/compiler_compat/` and it overshadows system `ld` residing at `/usr/bin`.\n \n \n \n You can see that the build command (which caused my error) uses Anaconda's `ld` instead of system `ld` as it specifies `-B` option which points to Anaconda's folder:\n \n ```bash\n \n g++ -pthread -shared -B /opt/anaconda/compiler_compat -L/opt/anaconda/lib -Wl,-rpath=/opt/anaconda/lib -Wl,--no-as-needed -Wl,--sysroot=/ build/temp.linux-x86_64-3.7/torch/csrc/stub.o -L/home/manjaro/Downloads/pytorch/torch/lib -lshm -ltorch_python -o build/lib.linux-x86_64-3.7/torch/_C.cpython-37m-x86_64-linux-gnu.so -Wl,-rpath,$ORIGIN/lib\n \n ```\n \n \n \n To fix my error I ran `python setup.py clean` and then I temporarily renamed Anaconda's `ld` linker to `ld-old` to make it _invisible_ during PyTorch installation. Removing `-B` option from the installation stage could fix it too. \n \n \n \n Probably, there is some incompatibility between my `ld` (version `GNU ld (GNU Binutils) 2.31.1\n \n `) and Anaconda's `ld` (version `GNU ld (crosstool-NG 1.23.0.444-4ea7) 2.31.1\n \n `) that caused the error. Or another explanation is that my system `g++` compiled `.o` object file that is incompatible with Anaconda's `ld` in the subsequent step of linking.\n \n \n \n What about changing `setup.py` to not to allow Anaconda's `ld` overshadow system `ld`?\n \n \n \n Thank you.",
      "y": "Run `python setup.py clean` and then temporarily rename Anaconda's `ld` linker to `ld-old` to make it _invisible_ during PyTorch installation or remove `-B` option from the installation stage."
   },
   {
      "null": 346,
      "x": "Incorrect size for __rpow__(scalar, float)",
      "z": "I've got a fix ready locally. Could I send in a PR?",
      "y": "Fix issue with scalars and __rpow__ ."
   },
   {
      "null": 347,
      "x": "[Feature Request] LuaRocks style package manager for community made packages",
      "z": "We have no specific plans to do a separate package manager. Use conda, which allows community channels.",
      "y": "Use conda."
   },
   {
      "null": 348,
      "x": "Inconsistency: can instantiate Tensor from List, but cannot instantiate Variable",
      "z": "`Variable`s can only be built from an existing `Tensor`. This is expected behavior.",
      "y": "`Variable`s can only be built from an existing `Tensor`."
   },
   {
      "null": 349,
      "x": "Reductions returning scalars cause implicit sync-point",
      "z": "Pretty pictures of this by the way:\n \n \n \n - in the absence of syncing everything back hostside:\n \n ![gpupipelinemultiple](https://user-images.githubusercontent.com/123560/27936428-9912147e-62a8-11e7-8e1c-9c8851dcb227.png)\n \n \n \n - in the presence of hostside syncpoints:\n \n ![reduceall_pipelinestall](https://user-images.githubusercontent.com/123560/27936437-a15714c2-62a8-11e7-8d7b-256fb3c3aabe.png)",
      "y": "Introduce a scalar type into PyTorch, autograd compatible. With this, the sync-points issue will be solved."
   },
   {
      "null": 350,
      "x": "[Feature request] Allow exceptions in load_state_dict",
      "z": "How would `ingored_keys` work? A workaround I'd recommend for partial updates is this:\n \n ```python\n \n new_params = model.state_dict()\n \n new_params.update(updated_params)\n \n model.load_state_dict(new_params)\n \n ```",
      "y": "```python\n \n new_params = model.state_dict()\n \n new_params.update(updated_params)\n \n model.load_state_dict(new_params)\n \n ```"
   },
   {
      "null": 351,
      "x": "Broadcasting doesn't match docs for conda package",
      "z": "indeed :)",
      "y": "Broadcasting will drop in the next release."
   },
   {
      "null": 352,
      "x": "torch.save is saving too much",
      "z": "This is especially needed because you can save something like: `torch.save([x[:10], x[2:4]])` and then when you load back, the expectation is that the loaded values will share storage (i.e. be two Tensors viewing a larger storage)",
      "y": "You can save something like: `torch.save([x[:10], x[2:4]])` and then when you load back, the expectation is that the loaded values will share storage."
   },
   {
      "null": 353,
      "x": "gpu version of pytorch not working on docker image",
      "z": "I cant reproduce it myself. One thing that'd help is if you post your docker build log.\n \n By default, the Dockerfile does find CUDA, so this is surprising / weird.",
      "y": "Issue not found with latest version of docker and pytorch."
   },
   {
      "null": 354,
      "x": "Documentation for how pytorch tensor.view() rearranges dimensions?",
      "z": "`torch.view` only changes the sizes of the tensor, while the underlying content remains the same, pretty much like numpy reshape, so the order is the same as the order in the underlying tensor.\n \n As an example, say we have a tensor `x` of size `5x4x3x2`. Doing `x.view(5, -1)` just rearranges the sizes (you can see by creating a `torch.arange(0, 5 * 4 * 3 * 2)` tensor, view it and view back).\n \n \n \n For the source code, it was moved to `C` for efficiency, but you can find the old python implementation [here](https://github.com/pytorch/pytorch/blob/518864a7e094f32044ef4c0de82c0f20f4ed99c2/torch/tensor.py#L178-L217).\n \n \n \n One more note, if your tensor is of shape `N x C x H x W`, and you want to apply softmax on `C`, you need to transpose it before viewing it. Something like\n \n \n \n ```python\n \n a = torch.rand(N, C, H, W)\n \n a_t = a.permute(0, 2, 3, 1).contiguous().view(-1, C)\n \n # ... perform softmax\n \n # view back\n \n res = res_t.view(N, H, W, C).permute(0, 3, 1, 2)\n \n ```",
      "y": "`torch.view` only changes the sizes of the tensor, while the underlying content remains the same, pretty much like numpy reshape, so the order is the same as the order in the underlying tensor."
   },
   {
      "null": 355,
      "x": "type error when calling 'adagrad' optimization method",
      "z": "I was encountering the same bug when i define the optmizer before move the model to cuda.\n \n \n \n optimizer = Adagrad(model.parameters()) # TypeERROR\n \n model.cuda() # TypeERROR\n \n \n \n however, change their order is FINE.\n \n \n \n model.cuda() # FINE\n \n optimizer = Adagrad(model.parameters()) # FINE",
      "y": "Interchange the order of model and optimizer definition."
   },
   {
      "null": 356,
      "x": "cublas runtime error when both bmm's arguments have been expanded",
      "z": "thanks for the bug report, we'll fix this.",
      "y": "Fix baddbmm for expanded tensors."
   },
   {
      "null": 357,
      "x": "[Feature Request] Add NoisyLinear layer",
      "z": "They've only just appeared in one paper from DM and I'm not totally convinced of the results myself, so \"reasonable adoption\" might take a while yet. On the other hand, noisy linear layers are fairly generic layers and could see use in problems outside of RL.",
      "y": "Noisy linear layers are fairly generic layers and could see use in problems outside of RL."
   },
   {
      "null": 358,
      "x": "MacOSX, CUDA, OS call failed or operation not supported on this OS",
      "z": "May be related? https://github.com/facebookresearch/deepmask/issues/85. Someone solved it by updating OSX from 10.11 to 10.12",
      "y": "Upgrade to CUDA 10.12."
   },
   {
      "null": 359,
      "x": "[Feature request] unique operation",
      "z": "Has there been progress on this ? this is an important operation! :)\n \n \n \n My code also looks like yours, but it seems very inefficient in cases where the number of classes in the dataset is 1000 and the minibatch has 1 or 2 classes.",
      "y": "Automatic update of fbcode/onnx."
   },
   {
      "null": 360,
      "x": "undefined symbol: THLongStorage_inferSizeN",
      "z": "updated.\n \n \n \n It seems like your do not need to re-install anaconda, find `libTH*` in your directory, you may find\n \n ```\n \n /opt/anaconda/lib/python3.6/site-packages/torch/lib/libTH.so.1\n \n /opt/anaconda/lib/python3.6/site-packages/torch/lib/libTHC.so.1\n \n /opt/anaconda/lib/python3.6/site-packages/torch/lib/libTHCS.so.1\n \n /opt/anaconda/lib/python3.6/site-packages/torch/lib/libTHCUNN.so.1\n \n /opt/anaconda/lib/python3.6/site-packages/torch/lib/libTHD.so.1\n \n /opt/anaconda/lib/python3.6/site-packages/torch/lib/libTHNN.so.1\n \n /opt/anaconda/lib/python3.6/site-packages/torch/lib/libTHPP.so.1\n \n /opt/anaconda/lib/python3.6/site-packages/torch/lib/libTHS.so.1\n \n /opt/anaconda/lib/libTH.so.1\n \n /opt/anaconda/lib/libTHNN.so.1\n \n /opt/anaconda/lib/libTHPP.so.1\n \n /opt/anaconda/lib/libTHS.so.1\n \n /opt/anaconda/pkgs/libtorch-0.1.12-0/lib/libTHS.so.1\n \n /opt/anaconda/pkgs/libtorch-0.1.12-0/lib/libTHPP.so.1\n \n /opt/anaconda/pkgs/libtorch-0.1.12-0/lib/libTHNN.so.1\n \n /opt/anaconda/pkgs/libtorch-0.1.12-0/lib/libTH.so.1\n \n /opt/anaconda/pkgs/pytorch-0.2.0-py36h04dd36b_2cu75/lib/python3.6/site-packages/torch/lib/libTHS.so.1\n \n /opt/anaconda/pkgs/pytorch-0.2.0-py36h04dd36b_2cu75/lib/python3.6/site-packages/torch/lib/libTHNN.so.1\n \n /opt/anaconda/pkgs/pytorch-0.2.0-py36h04dd36b_2cu75/lib/python3.6/site-packages/torch/lib/libTHCS.so.1\n \n /opt/anaconda/pkgs/pytorch-0.2.0-py36h04dd36b_2cu75/lib/python3.6/site-packages/torch/lib/libTHD.so.1\n \n /opt/anaconda/pkgs/pytorch-0.2.0-py36h04dd36b_2cu75/lib/python3.6/site-packages/torch/lib/libTHPP.so.1\n \n /opt/anaconda/pkgs/pytorch-0.2.0-py36h04dd36b_2cu75/lib/python3.6/site-packages/torch/lib/libTH.so.1\n \n /opt/anaconda/pkgs/pytorch-0.2.0-py36h04dd36b_2cu75/lib/python3.6/site-packages/torch/lib/libTHCUNN.so.1\n \n /opt/anaconda/pkgs/pytorch-0.2.0-py36h04dd36b_2cu75/lib/python3.6/site-packages/torch/lib/libTHC.so.1\n \n ```\n \n \n \n So, it is ok to uninstall `libtorch` from conda, and everything goes ok\n \n ```\n \n conda uninstall libtorch\n \n ```\n \n \n \n Thanks for updating to the exciting `0.2.0`.",
      "y": "It is ok to uninstall `libtorch` from conda."
   },
   {
      "null": 361,
      "x": "shared cuda tensor consumes GPU memory in every process",
      "z": "I ran that code in ubuntu 14.04, python 3.5.2.\n \n When I ran that code, main process consumed 327Mb of memory and sub processes consumed 311Mb so I thought that tensor is not properly shared.\n \n However, when I changed the size of the tensor to (1000,1000,200), main process consumed 1837Mb and sub processes consumed same 311Mb.\n \n I don't know why sub processes consumes so much memory but I think my problem is solved:) Thank you!",
      "y": "Change the size of the tensor to orders of (1000,1000,200)."
   },
   {
      "null": 362,
      "x": "An error occurred when installing pytorch from source",
      "z": "`nvcc` will use the temp directory for intermediate compilation files. The way to avoid this is by setting environment variable `TMPDIR` to some other directory: http://docs.nvidia.com/cuda/cuda-compiler-driver-nvcc/index.htmlcompiler-driver-nvcc/index.html #keeping-intermediate-phase-files",
      "y": "Set environment variable `TMPDIR` to some other directory."
   },
   {
      "null": 363,
      "x": "Completely deterministic network, but result is different from the past and different on different machines.",
      "z": "This is expected, some of our kernels are not deterministic (specially during backward).\n \n Might be good to refer to https://github.com/pytorch/pytorch/issues/15359\n \n \n \n I'm closing this issue in order to keep the discussion centralized.",
      "y": "Some of our kernels are not deterministic."
   },
   {
      "null": 364,
      "x": "Padding size should be less than the corresponding input dimension",
      "z": "> The expected behavior should be a warning to let the user decide if they still want to use this padding mode for images.\n \n \n \n Hmm, really? There is no proper output in this case, and you don't expect an error?",
      "y": "A warning to let the user decide if they still want to use this padding mode for images."
   },
   {
      "null": 365,
      "x": "training with Multi-GPU hangs",
      "z": "Thanks for you reply. I have solved the problem, it's the opened ACS of the PLX PCI-e switch caused the problem, so after i closed the switch, it's ok. Just like [here](https://www.supermicro.org.cn/support/faqs/faq.cfm?faq=20732)",
      "y": "Close ACS of the PLX PCI-e switch."
   },
   {
      "null": 366,
      "x": "logdet evals to -inf in an overly pessimistic way",
      "z": "lol we posted literally 10min apart. see https://github.com/pytorch/pytorch/issues/18448\n \n \n \n and pr is https://github.com/pytorch/pytorch/pull/18449",
      "y": "Improve numerical precision of (s)logdet."
   },
   {
      "null": 367,
      "x": "ImportError: cannot import name '_update_worker_pids'",
      "z": "pip install torch==1.0.1 torchvision==0.2.2",
      "y": "Install torch 1.0.1 and torchvision 0.2.2."
   },
   {
      "null": 368,
      "x": "Wrong BatchNorm2d momentum value in ONNX export",
      "z": "I can repro this on my side. I am taking a look.",
      "y": "Fix momentum setting in BatchNorm forward pass."
   },
   {
      "null": 369,
      "x": "Error trying to convert to JitScript",
      "z": "Oops - not sure what happened there: https://gist.github.com/emrul/3644b54326794068d0f43e1fce4c1308 (once again, please ignore the poor code - I am trying to incrementally do this work and just figuring out things from the error messages I get)",
      "y": "I just ran your gist on master and did not get an error."
   },
   {
      "null": 370,
      "x": "[docs] torch.(a)range dtype doc is wrong",
      "z": "https://pytorch.org/docs/master/torch.html#torch.arange already says \n \n \n \n ```\n \n dtype (torch.dtype, optional) the desired data type of returned tensor. Default: if None, uses a global default (see torch.set_default_tensor_type()). If dtype is not given, infer the data type from the other input arguments. If any of start, end, or stop are floating-point, the dtype is inferred to be the default dtype, see get_default_dtype(). Otherwise, the dtype is inferred to be torch.int64.\n \n ```\n \n \n \n #18604 fixes range though",
      "y": "Fixed range."
   },
   {
      "null": 371,
      "x": "torch.svd seems to have accuracy problem",
      "z": "You have a bug at https://github.com/wangg12/IRLS_tensorflow2/blob/721675bb9dd0ba19223e10b346aa639a159a58da/src/IRLS_pytorch.py#L111\n \n \n \n `y` has shape `torch.Size([32561, 1])` yet `y_pred` has shape `torch.Size([32561])`.\n \n \n \n For the record, you don't need to add explicit `expand` to broadcast everywhere, pytorch does automatic broadcasting, just like numpy.\n \n \n \n I fixed that, and use the same `pinv` method as you did in your `tf` script, removed every discrepancy between pt and tf scripts (e.g., l2 reg), and got the same accuracy.\n \n \n \n Here is the script: https://gist.github.com/SsnL/acd7ae2e096b01daab0fc7621dd47bc3\n \n \n \n Here is the output:\n \n \n \n ```\n \n Random Seed: 2196\n \n start training...\n \n L2 param(lambda): 20\n \n iter: 0\n \n  neg log likelihood: 23909.580078125\n \n  train acc: 0.24080955982208252, test acc: 0.23622629046440125\n \n  L2 norm of w: 0.11135528236627579\n \n iter: 1\n \n  neg log likelihood: 12469.365234375\n \n  train acc: 0.8444765210151672, test acc: 0.8457711935043335\n \n  L2 norm of w: 2.1622300148010254\n \n  diff of w_old and w: 2.1692447662353516\n \n iter: 2\n \n  neg log likelihood: 11109.1064453125\n \n  train acc: 0.8469641804695129, test acc: 0.8494564294815063\n \n  L2 norm of w: 3.1962523460388184\n \n  diff of w_old and w: 1.1507797241210938\n \n iter: 3\n \n  neg log likelihood: 10792.7138671875\n \n  train acc: 0.8475169539451599, test acc: 0.8501321077346802\n \n  L2 norm of w: 3.856876850128174\n \n  diff of w_old and w: 0.8543247580528259\n \n iter: 4\n \n  neg log likelihood: 10746.015625\n \n  train acc: 0.8479469418525696, test acc: 0.8507463335990906\n \n  L2 norm of w: 4.167870044708252\n \n  diff of w_old and w: 0.49175599217414856\n \n iter: 5\n \n  neg log likelihood: 10743.853515625\n \n  train acc: 0.8479162454605103, test acc: 0.8506848812103271\n \n  L2 norm of w: 4.240683078765869\n \n  diff of w_old and w: 0.1431959867477417\n \n iter: 6\n \n  neg log likelihood: 10743.8466796875\n \n  train acc: 0.8479162454605103, test acc: 0.8507463335990906\n \n  L2 norm of w: 4.244931221008301\n \n  diff of w_old and w: 0.01007823646068573\n \n iter: 7\n \n  neg log likelihood: 10743.84375\n \n  train acc: 0.8479162454605103, test acc: 0.8507463335990906\n \n  L2 norm of w: 4.244948387145996\n \n  diff of w_old and w: 4.568263830151409e-05\n \n training done.\n \n \n \n ```",
      "y": "No need to add explicit `expand` to broadcast everywhere, pytorch does automatic broadcasting, just like numpy."
   },
   {
      "null": 372,
      "x": "grid_sample cuDNN error",
      "z": "@acheketa Thanks, now it works",
      "y": "https://github.com/pytorch/pytorch/issues/18561#issuecomment-477906432"
   },
   {
      "null": 373,
      "x": "Checkpointing evaluates irrelevant tasks",
      "z": "This now has the correct behavior in master.\n \n Please re-open if you see more issues.",
      "y": "Documentation and warning about it would be a better choice."
   },
   {
      "null": 374,
      "x": "[FR] warn/error when MAGMA is built with a different CUDA",
      "z": "Related to https://github.com/pytorch/pytorch/issues/3990.",
      "y": "Raise an error when using magma built against wrong version of cuda."
   },
   {
      "null": 375,
      "x": "c++ error:\"std\":ambiguous symbol it is in visual studio 2017.",
      "z": "I tested on different configurations and found that problem can be solved by change the `conformance mode` property to **`No`**. You can find that option in `C/C++->Language`. That's a new feature in vs2017 and 2019.",
      "y": "change the `conformance mode` property to **`No`**."
   },
   {
      "null": 376,
      "x": "Fail in repeated evaluation of 2nd derivative of a custom autograd function",
      "z": "Fixed by https://github.com/pytorch/pytorch/pull/22983",
      "y": "Invert ownership between PyFunction and THPFunction."
   },
   {
      "null": 377,
      "x": "torch.matmul not working in cuda 9.0",
      "z": "cuda 9 doesn't support RTX cards afaik",
      "y": "Cuda 9 doesn't support NVidia RTX cards."
   },
   {
      "null": 378,
      "x": "torch.matrix_exp() doc is missing a signature",
      "z": "Hey, I raise a PR and hope it can solve this issue",
      "y": "Invert ownership between PyFunction and THPFunction."
   },
   {
      "null": 379,
      "x": "torch.linalg.eigh fails on gradcheck with complex Hermitian matrix",
      "z": "I think what is causing troubles here is that the sign or phase of eigenvectors is not unique, so is the choice for normalization of eigenvectors.\n \n The following `fcn` function passes the test\n \n ```python\n \n import torch\n \n mat = torch.randn((2, 2), dtype=torch.complex128).requires_grad_()\n \n def fcn(mat):\n \n  mat = (mat + mat.transpose(-2, -1).conj()) * 0.5\n \n  result = torch.linalg.eigh(mat)\n \n  return result[0], abs(result[1])\n \n \n \n torch.autograd.gradcheck(fcn, mat)\n \n ```",
      "y": "The sign or phase of eigenvectors is not unique, so is the choice for normalization of eigenvectors."
   },
   {
      "null": 380,
      "x": "Request for a test code snippet of Distributed Data Parallel.",
      "z": "this is the API document https://pytorch.org/docs/master/generated/torch.nn.parallel.DistributedDataParallel.html",
      "y": "Here is the API document - https://pytorch.org/docs/master/generated/torch.nn.parallel.DistributedDataParallel.html"
   },
   {
      "null": 381,
      "x": "Preserve PyObject even when it is dead from Python side",
      "z": "Yes, so the PyObject reference Tensor has to be made non-owning. We can use @swolchok's MaybeOwned class to conveniently do this.",
      "y": "PyObject reference Tensor has to be made non-owning."
   },
   {
      "null": 382,
      "x": "PYTORCH_TESTING_DEVICE_ONLY_FOR environment variable don't play well with development environment.",
      "z": "Are PYTORCH_TESTING_DEVICE_ONLY_FOR and PYTORCH_TESTING_DEVICE_EXCEPT_FOR documented somewhere?\n \n \n \n Is the idea that if the environment variable PYTORCH_TESTING_DEVICE_ONLY_FOR is set it contains one or more device types, and only device generic tests for those device types are run? If so, I would expect it to intersect its set with the available device types.\n \n \n \n Similarly, if PYTORCH_TESTING_DEVICE_EXCEPT_FOR is designed to eliminate one or more device types then I would expect it would remove the test bases for those device types.",
      "y": "If the environment variable PYTORCH_TESTING_DEVICE_ONLY_FOR is set it contains one or more device types, and only device generic tests for those device types are run."
   },
   {
      "null": 383,
      "x": "Error in optim/adamw.py",
      "z": "I confirm that `pytorch-1.8.1` doesn't have this fix included. And getting the same problem.",
      "y": "`pytorch-1.8.1` doesn't have this fix."
   },
   {
      "null": 384,
      "x": "_ConvNd weight initialization does not match docs",
      "z": "The docstring is correct in this case. For example:\n \n \n \n ```python\n \n import math\n \n import matplotlib.pyplot as plt\n \n \n \n import torch\n \n import torch.nn as nn\n \n \n \n in_channels = 120\n \n groups = 2\n \n kernel = (3, 8)\n \n m = nn.Conv2d(in_channels=in_channels, groups=groups,\n \n  out_channels=100, kernel_size=kernel)\n \n \n \n k = math.sqrt(groups / (in_channels * math.prod(kernel)))\n \n print(f\"k: {k:0.6f}\")\n \n \n \n print(f\"min weight: {m.weight.min().item():0.6f}\")\n \n print(f\"max weight: {m.weight.max().item():0.6f}\")\n \n ```\n \n \n \n outputs:\n \n ```\n \n k: 0.026352\n \n min weight: -0.026352\n \n max weight: 0.026352\n \n ```\n \n \n \n And when we plot the distribution, it is uniform with the correct bounds:\n \n \n \n ```python\n \n _ = plt.hist(m.weight.detach().numpy().ravel())\n \n ```\n \n \n \n ![Unknown](https://user-images.githubusercontent.com/5402633/119552979-21ba3800-bd69-11eb-8e10-e067c943abe3.png)\n \n \n \n \n \n I opened #58931 that adds a code comment to avoid future confusion.",
      "y": "Docstring is correct in this case."
   },
   {
      "null": 385,
      "x": "[ROCm] Tests fail on my system.",
      "z": "[here you go](https://gist.githubusercontent.com/Wulfsta/5d8b2b902e1068433aaf737657453a25/raw/18028b37ea909905d23b60805399df4ea0c6172b/gistfile1.txt). \n \n \n \n Edit: I obviously haven't parsed this whole file, but the problems appear to be limited to gradients and it looks like a lot of equality checks on NaNs are happening?",
      "y": "Problems appear to be limited to gradients and it looks like a lot of equality checks on NaNs are happening."
   },
   {
      "null": 386,
      "x": "JIT: torch.jit.Future annotations cannot be spread over multiple lines",
      "z": "This is now fixed in #56148. Now closing. If the issue still exists, please feel free to re-open the issue.",
      "y": "Add support for refinement for torch.jit.Future"
   },
   {
      "null": 387,
      "x": "\"Add annotations\" workflow fails when base branch is pinned to old commit",
      "z": "> Those are good pros and cons. Do you have a recommendation?\n \n \n \n @mruberry I plan to implement approach (2) soon.",
      "y": "Using a general strategy of expecting malformed input will prevent these sorts of failures from happening when future changes to these workflows are made."
   },
   {
      "null": 388,
      "x": "\"Build\" step in CircleCI outputs a ton of pthreadpool warnings",
      "z": "It's a duplicate of #33760, also I noticed it was only happening with certain flags on (i.e. in the logs above I had changed the `CFLAGS` to include `-gsplit-dwarf`.)",
      "y": "It is only happening with certain flags."
   },
   {
      "null": 389,
      "x": "Unable to build PyTorch without Numpy installed (again)",
      "z": "That's a caffe2 bug :-) Workaround is `BUILD_CAFFE2=0 BUILD_CAFFE2_OPS=0`. One day, it will become default :-)",
      "y": "`BUILD_CAFFE2=0 BUILD_CAFFE2_OPS=0`."
   },
   {
      "null": 390,
      "x": "[bug] run test local",
      "z": "Oh thanks. Didn't notice that!",
      "y": "Allow tests to run locally without setting environment variables."
   },
   {
      "null": 391,
      "x": "[doc] is_grad_enabled is not documented",
      "z": "Duplicate of https://github.com/pytorch/pytorch/issues/4474?",
      "y": "`torch.is_grad_enabled` isn't searchable."
   },
   {
      "null": 392,
      "x": "Print Bug of Tensor Grad After Backward",
      "z": "Unfortunately, this is expected. The multiplication `x[0, :, 0, 0] * var` saves `x[0, :, 0, 0]` for backwards, but then you mutate it and we are then unable to use it for backwards. The inplace formula doesn't have this problem because it knows the clobber is coming and does something special. A simple fix is `x[0, :, 0, 0] = x[0, :, 0, 0].clone() * var`. Could you tell us a little more about what you're trying to do?",
      "y": "Multiplication `x[0, :, 0, 0] * var` saves `x[0, :, 0, 0]` for backwards, but then you mutate it and we are then unable to use it for backwards."
   },
   {
      "null": 393,
      "x": "Add Kaiming initialization functions to C++ API",
      "z": "Hey @JoshVarty, thanks for the idea. I'd love it if you tried implementing this! It indeed sounds like a nicely contained, manageable task for a first PR. It shouldn't be too hard to port the Python code to C++ since our C++ tensor API is almost 1:1 the same.\n \n \n \n For the enum question, I'd much prefer using C++ enums over strings. You can create an `enum class Mode` in the `torch::nn::init` namespace with the appropriate members.\n \n \n \n I'd say, just give it a shot, and I'll give you supportive feedback once you have a minimum viable PR ready.\n \n \n \n And yes, you'd have to implement `calculate_gain` too.",
      "y": "It shouldn't be too hard to port the Python code to C++ since our C++ tensor API is almost 1:1 the same."
   },
   {
      "null": 394,
      "x": "torch.nn.Conv2d will try to allocate gpu memory far more than it needs.",
      "z": "Well cudnn automatically select the fastest possible algorithm (as long as enough memory is available).\n \n You can try and set `torch.backends.cudnn.deterministic=True` just before doing the forward on the problematic kernel. That will force it to use the default deterministic algorithm and hopefully use less memory.",
      "y": "Cudnn automatically select the fastest possible algorithm. Set `torch.backends.cudnn.deterministic=True` just before doing the forward on the problematic kernel."
   },
   {
      "null": 395,
      "x": "Mean and max slow",
      "z": "`mean` goes through recently added reduction kernels in TensorIterator. It is not without problems, as we've discussed in #12115, but it is fast. `max` uses older kernels in THC. At some point all THC reductions should be moved to TensorIterator reductions, but this future has not arrived yet (PRs welcome).",
      "y": "`mean` goes through recently added reduction kernels in TensorIterator and is fast. `max` uses older kernels in THC."
   },
   {
      "null": 396,
      "x": "Build libtorch with -D_GLIBCXX_USE_CXX11_ABI=1",
      "z": "Initially I want to use trained model from pytorch to detect hand gesture in C++. My original problem is that I can cmake the code calling either libtorch or OpenCV library successfully, but I cannot make it when I try to cmake the code calling both.\n \n I have solved the similar problems by changing to a lower OpenCV version. Here are some details.\n \n \n \n Original problem:\n \n \n \n CMakeFiles/ LibtorchTest.dir/main . CPP. O: in the function 'main':\n \n /home/gao/CLionProjects/LibtorchTest/ main.cpp:14 : undefined reference to 'CV:: imread (STD:: String const & int)'\n \n /home/gao/CLionProjects/LibtorchTest/ main.cpp:15 : for 'CV:: imshow (STD:: String const &, CV::_ Undefined reference in inputarray const &) '\n \n collect2: error: ld returned 1 exit status\n \n CMakeFiles/ LibtorchTest.dir/build . make:133 : recipe for target 'LibtorchTest' failed\n \n make[3]: *** [LibtorchTest] Error 1\n \n CMakeFiles/Makefile2:95: recipe for target 'CMakeFiles/ LibtorchTest.dir/all ' failed\n \n make[2]: *** [CMakeFiles/ LibtorchTest.dir/all ] Error 2\n \n CMakeFiles/Makefile2:102: recipe for target 'CMakeFiles/ LibtorchTest.dir/rule ' failed\n \n make[1]: *** [CMakeFiles/ LibtorchTest.dir/rule ] Error 2\n \n Makefile:138 : recipe for target 'LibtorchTest' failed\n \n make: *** [LibtorchTest] Error 2\n \n \n \n Problem solved:\n \n \n \n -- Caffe2: CUDA detected: 10.2\n \n -- Caffe2: CUDA nvcc is: /usr/local/cuda/bin/nvcc\n \n -- Caffe2: CUDA toolkit directory: /usr/local/cuda\n \n -- Caffe2: Header version is: 10.2\n \n -- Found cuDNN: v8.0.3 (include: /usr/include, library: /usr/lib/x86_64-linux-gnu/libcudnn.so)\n \n -- Autodetected CUDA architecture(s): 6.1\n \n -- Added CUDA NVCC flags for: -gencode;arch=compute_61,code=sm_61\n \n -- Torch library status:\n \n -- libraries: \n \n -- Found OpenCV: /home/gao/opencv-3.4.12/build (found version \"3.4.12\") \n \n -- OpenCV library status:\n \n -- version: 3.4.12\n \n -- libraries: opencv_calib3d;opencv_core;opencv_dnn;opencv_features2d;opencv_flann;opencv_highgui;opencv_imgcodecs;opencv_imgproc;opencv_ml;opencv_objdetect;opencv_photo;opencv_shape;opencv_stitching;opencv_superres;opencv_video;opencv_videoio;opencv_videostab;opencv_viz\n \n -- include path: ...too long, so I omit it)\n \n -- Configuring done\n \n -- Generating done\n \n -- Build files have been written to: /home/gao/CLionProjects/LibtorchTest/cmake-build-debug\n \n Scanning dependencies of target LibtorchTest\n \n [ 50%] Building CXX object CMakeFiles/LibtorchTest.dir/main.cpp.o\n \n [100%] Linking CXX executable LibtorchTest\n \n [100%] Built target LibtorchTest\n \n \n \n Hope my solved problem may help.",
      "y": "Change to a lower OpenCV version."
   },
   {
      "null": 397,
      "x": "load_lua() does not know how to deserialize Lua class nn.PixelShuffle.",
      "z": "if you want to load the weights from a lua torch .t7 file into python, you can use https://github.com/bshillingford/python-torchfile which should still be working afaik.",
      "y": "Use \"https://github.com/bshillingford/python-torchfile\" ."
   },
   {
      "null": 398,
      "x": "Inconsistent behavior for log_prob when values are outside of support",
      "z": "Hi @chentc777,\n \n \n \n You can consider setting `validate_args` to `True` for the distributions individually, or by setting a global flag using:\n \n ```\n \n torch.distributions.Distribution.set_default_validate_args(True)\n \n ```\n \n \n \n This will raise a `ValueError` if the values are outside the support.\n \n \n \n ```python\n \n >>> torch.distributions.Distribution.set_default_validate_args(False)\n \n >>> Beta(torch.tensor(1.),torch.tensor(1.)).log_prob(torch.tensor(5.))\n \n tensor(nan)\n \n >>> Exponential(torch.tensor(2.)).log_prob(torch.tensor(-5.))\n \n tensor(11.6094)\n \n >>> Gamma(torch.tensor(1.),torch.tensor(1.)).log_prob(torch.tensor(-1.))\n \n tensor(nan)\n \n >>> Geometric(torch.tensor(5.)).log_prob(torch.tensor(-1.))\n \n tensor(nan)\n \n ```\n \n \n \n ```python\n \n >>> torch.distributions.Distribution.set_default_validate_args(True)\n \n >>> Beta(torch.tensor(1.),torch.tensor(1.)).log_prob(torch.tensor(5.))\n \n ValueError: The value argument must be within the support\n \n >>> Exponential(torch.tensor(2.)).log_prob(torch.tensor(-5.))\n \n ValueError: The value argument must be within the support\n \n >>> Gamma(torch.tensor(1.),torch.tensor(1.)).log_prob(torch.tensor(-1.))\n \n ValueError: The value argument must be within the support\n \n >>> Geometric(torch.tensor(5.)).log_prob(torch.tensor(-1.))\n \n ValueError: The parameter probs has invalid values\n \n ```",
      "y": "Set `validate_args` to `True` for the distributions individually or by setting a global flag."
   },
   {
      "null": 399,
      "x": "Deepcopy doesn't copy to same gpu",
      "z": "I ran into the same problem when I tried to deepcopy a module.\n \n I'm now working around by adding a `.to(device=desired_device)` after deepcopy",
      "y": "Add a `.to(device=desired_device)` after deepcopy."
   },
   {
      "null": 400,
      "x": "test_distributed - failed gloo test_all_reduce_multigpu and test_barrier_group_cuda on 8-GPU machines",
      "z": "The barrier one is likely because of excessive initialization time when using CUDA, triggering the native timeout. Bumping that a bit should solve it. Looking into the allreduce multi GPU one.",
      "y": "Likely because of excessive initialization time when using CUDA, triggering the native timeout. Bumping that a bit should solve it."
   },
   {
      "null": 401,
      "x": "libtorch c++API: terminate called after throwing an instance of 'c10::Error'",
      "z": "> > > I have the same problem, too. I upgraded the package as you offered(https://drive.google.com/open?id=13qS1gup6E7nJ-l8Ahf4YUsA9gGGVzKWE ), but the bug still appear. What is wrong with this ?\n \n > > \n \n > > \n \n > > Maybe u should use pytorch-dev11.28 to trace the model. And make sure you have cmake and make your project again? I solve the problem by use libtorch 11.28 under cuda8 and cudnnV6.\n \n > \n \n > How to degrade to pytorch-dev11.28?\n \n > I am using libtorch downloaded on 11.23, but I still met this error\n \n \n \n u can find the 11.28 version package in conda repository(https://anaconda.org/pytorch/pytorch-nightly/files).",
      "y": "Use pytorch-dev11.28 to trace the model."
   },
   {
      "null": 402,
      "x": "[libtorch] build failed with opencv-4.0.0 using cmake (CPU and GPU)",
      "z": "Problem resolved, related issue is here: #14620",
      "y": "Export model.pt and build libtorch from the same version of Pytorch."
   },
   {
      "null": 403,
      "x": "Large documentation pages take a long time to load",
      "z": "This was fixed in the doc restructure in gh-37419. https://pytorch.org/docs/master/nn.html loads fast now. So closing this.",
      "y": "Fixed in the doc restructure in gh-37419."
   },
   {
      "null": 404,
      "x": "Comply with XDG (X Design Group) Base Directory Specification",
      "z": "Cheers! I found the specific commit here:\n \n \n \n https://github.com/pytorch/pytorch/pull/18758/commits/d079a6fbcafa59b419b3c6355a3b8a01d44a8b47",
      "y": "Comply with XDG_CACHE_HOME and keep TORCH_HOME ."
   },
   {
      "null": 405,
      "x": "tensor() and Tensor() return different values for a list of bools",
      "z": "Thank you for reporting this issue, @CamiloHernandez. We encourage people to use `torch.tensor` and not `torch.Tensor`, but I think we would take a PR fixing this behavior, anyway, since it's so clearly wrong.",
      "y": "Use `torch.tensor` and not `torch.Tensor` ."
   },
   {
      "null": 406,
      "x": "Which torch version is this?",
      "z": "Turns out need to export PYTORCH_BUILD_VERSION=1.4.0 PYTORCH_BUILD_NUMBER=1 for the version to be reported correctly.\n \n https://github.com/pytorch/pytorch/issues/9926\n \n This should be better documented.",
      "y": "Export PYTORCH_BUILD_VERSION=1.4.0 PYTORCH_BUILD_NUMBER=1 for the version to be reported correctly."
   },
   {
      "null": 407,
      "x": "Windows nightly build failed (CUDA + DEBUG + LIBTORCH)",
      "z": "No, it has nothing to do with magma. The warning is just about debug info since I didn't put `magma.pdb` in the package.",
      "y": "Calling benchmark.Timer with default num_threads=1 disables parallelism."
   },
   {
      "null": 408,
      "x": "Libtorch : Comparing identical shapes(sizes) always returns false when in debug mode",
      "z": "I couldnt replicate this issue in a new project. even the very same project that exibits this issue, doesnt behave incorrectly when copied and executed from another location in the same system. This makes me come to the conclusion that this might be very well a Visual Studio 2019 issue of some kind! Despite my attempts, I couldnt find out the cause so I close this issue and will reopen it in case I find something new. \n \n Thanks a lot for your generous time and kind help.",
      "y": "A Visual Studio 2019 issue."
   },
   {
      "null": 409,
      "x": "Difference behavior between FrozenBatchNorm2d and BatchNorm2d",
      "z": "they have different eps https://github.com/pytorch/vision/blob/fc69c22576cbccb59c581ddbaca4dedbdb279688/torchvision/ops/misc.py#L54",
      "y": "They have different eps."
   },
   {
      "null": 410,
      "x": "JIT changes method kwarg argument names",
      "z": "I recently fixed argument name capturing in tracing and I verified the repro provided and it is no longer throwing exception. Closing.",
      "y": "Fix argument name capturing in tracing."
   },
   {
      "null": 411,
      "x": "Add a torch.hub.load_local() function that can load models from any local directory with a hubconf.py",
      "z": "This was considered a lower pri in our plan - we'd be happy to take a PR implementing this functionality.",
      "y": "Allow torch.hub.load() to load models from any local directory with a hubconf.py."
   },
   {
      "null": 412,
      "x": "[jit] determine whether ModuleList is empty or its length",
      "z": "Hi, thanks for the issue and good repro! Hmm, I thought this was fixed for 1.6. It is fixed on master, though, so closing.\n \n ```\n \n model_jit = torch.jit.script(Test())\n \n print(model_jit.forward.code)\n \n \n \n def forward(self,\n \n  x: Tensor) -> Tensor:\n \n  if torch.gt((self.layers).__len__(), 0):\n \n  x1 = (getattr(self.layers, \"0\")).forward(x, )\n \n  x0 = x1\n \n  else:\n \n  x0 = x\n \n  return x0\n \n ```",
      "y": "```\n \n model_jit = torch.jit.script(Test())\n \n print(model_jit.forward.code)\n \n \n \n def forward(self,\n \n  x: Tensor) -> Tensor:\n \n  if torch.gt((self.layers).__len__(), 0):\n \n  x1 = (getattr(self.layers, \"0\")).forward(x, )\n \n  x0 = x1\n \n  else:\n \n  x0 = x\n \n  return x0\n \n ```"
   },
   {
      "null": 413,
      "x": "Make torch.nn.Hardswish ONNX exportable",
      "z": "You can get around this by using \n \n ```\n \n class Hardswish(nn.Module): # export-friendly version of nn.Hardswish()\n \n  @staticmethod\n \n  def forward(x):\n \n  # return x * F.hardsigmoid(x) # for torchscript and CoreML\n \n  return x * F.hardtanh(x + 3, 0., 6.) / 6. # for torchscript, CoreML and ONNX\n \n ```\n \n But pytorch told me to open a bug so here it is",
      "y": "```\n \n class Hardswish(nn.Module): # export-friendly version of nn.Hardswish()\n \n  @staticmethod\n \n  def forward(x):\n \n  # return x * F.hardsigmoid(x) # for torchscript and CoreML\n \n  return x * F.hardtanh(x + 3, 0., 6.) / 6. # for torchscript, CoreML and ONNX\n \n ```"
   },
   {
      "null": 414,
      "x": "torch\\include\\ATen/core/ivalue_inl.h... cl.exe' failed with exit status 2",
      "z": "@shihlun1208 can you please post the actual build logs rather than screenshots? Also, what version of pytorch you are trying to build? \n \n And is there a reason why you can't upgrade to a more recent version of VC++?",
      "y": "VS2019 is not supported with cuda 10."
   },
   {
      "null": 415,
      "x": "torch.empty_like supports memory_format=torch.preserve_format",
      "z": "Seems reasonable to me, I would just open a PR @vfdev-5.",
      "y": "Change the order of `TORCH_CHECK` and `if (options.layout() == kSparse && self.is_sparse())`"
   },
   {
      "null": 416,
      "x": "embedding_bag running on CPU produces wrong result when weight tensor is non-contiguous.",
      "z": "Mark high priority, cpu has a wrong behavior, this should be treated as a regression.",
      "y": "Let ModuleTest raise when they fail on non-contiguous inputs. Fix legacy modules.\n Fix BN (both THNN and cuDNN) not working on non-contiguous inputs.\n Fix CUDA EmbeddingBag not working on non-contiguous inputs. To prevent calling .contiguous() on in both forward and backward,\n a. prefix all current embedding_bag* functions with _, indicating that they require input to be contiguous (there is a check in each function).\n b. create embedding_bag, which makes input arguments .contiguous(), and calls _embedding_bag\n Make many ATen embedding* functions to work on non-contiguous inputs so we don't need to call input = input.contiguous() in Python nn.functional.embedding.\n Fix dense-sparse addition when the sparse input is not coalesced and indices or values tensor is not contiguous. This came up in the test cases of Embedding modules with sparse=True. Added tests.\n Update TensorUtils.cpp to use AT_* macros."
   },
   {
      "null": 417,
      "x": "torch.jit bug torch.cat() not working properly",
      "z": "> Hm, interesting, I picked one of the branches that had \"release\" and \"1.6\" in its name, maybe it was still a wrong one. Is it reproducible with more recent releases or master?\n \n \n \n No. Master build and nightly are fine. I tried 1.7 and it is fine too.",
      "y": "There's been a lot of changes in fusers/executors since 1.6, so it probably is no longer an issue."
   },
   {
      "null": 418,
      "x": "Make non-literal indexing for ModuleList / ModuleDict work with ModuleInterface type in TorchScript",
      "z": "This is one of the most common issues I face when trying to convert models to TorchScript. And i believe a quick temporary fix is possible. Simply iterating over every list with IF check to bypass unwanted iterations almost always works as a fix but is terribly inefficient.",
      "y": "Iterating over every list with IF check to bypass unwanted iterations almost always works as a fix but is terribly inefficient."
   },
   {
      "null": 419,
      "x": "nightly.py error",
      "z": "a fix is in at https://github.com/pytorch/pytorch/pull/43771",
      "y": "Nightly robustness fixes for linking across devices"
   },
   {
      "null": 420,
      "x": "[ERROR] PyTorch dependency on Windows Python Package not working",
      "z": "Well, the underlying problem is that the Windows packages are not hosted on PYPI. So instead of `pip install -r requirements.txt`, you may need to use `pip install -r requirements.txt -f https://download.pytorch.org/whl/torch_stable.html`.",
      "y": "Use `pip install -r requirements.txt -f https://download.pytorch.org/whl/torch_stable.html`."
   },
   {
      "null": 421,
      "x": "RuntimeError(\"{} is a zip archive (did you mean to use torch.jit.load()?)\".format(f.name)) when loading model weights",
      "z": "Hi feiyangsuo,\n \n \n \n You are right that it is indeed a zip file. See [PyTorch 1.6 release notes](https://github.com/pytorch/pytorch/releases/tag/v1.6.0) about the change (search for \"zip\" in the linked webpage). I would suggest upgrading to 1.6 and the issue should be gone. If that's not the case, please update the bug and we will investigate. Or if you have reasons to not upgrade to 1.6, let us know and we will see what we can do.\n \n \n \n Thanks,",
      "y": "Upgrade to 1.6."
   },
   {
      "null": 422,
      "x": "Fix exception chaining all over the codebase",
      "z": "Thank you for your quick response.\n \n \n \n Sure, I'll remove `raise_from()` and use `raise .. from ..` instead all over the codebase.\n \n \n \n > Although not all instances require exception chaining, for example implicit chaining is totally fine in cases that you've mentioned in the issue description:\n \n > https://github.com/pytorch/pytorch/blob/c7787f7fbf388f9fe326d581726e7ef3f2dce74b/torch/jit/annotations.py#L32-L35\n \n \n \n I agree that not all instances require exception chaining. However, I think we should not use implicit chaining in the above case because its error message `During handling of the above exception, another exception occurred` is confusing.\n \n \n \n For example, if we run the script below (`test.py`):\n \n ```python\n \n d = {\"dog\": 1}\n \n key = \"cat\"\n \n try: \n \n  print(d[key]) # raises KeyError\n \n except KeyError:\n \n  raise RuntimeError(\"no member called {}\".format(key))\n \n ```\n \n it'll give us an error message like this:\n \n ```\n \n Traceback (most recent call last):\n \n  File \"/home/nitta/tmp/test.py\", line 4, in <module>\n \n  print(d[key]) # raises KeyError\n \n KeyError: 'cat'\n \n \n \n During handling of the above exception, another exception occurred:\n \n \n \n Traceback (most recent call last):\n \n  File \"/home/nitta/tmp/test.py\", line 6, in <module>\n \n  raise RuntimeError(\"no member called {}\".format(key))\n \n RuntimeError: no member called cat\n \n ```\n \n \n \n `During handling of the above exception, another exception occurred` is somewhat confusing when debugging because the truth is that another exception occurred to explain the first exception (`KeyError` in this case) in a friendlier way, not because another **unexpected** exception occured while handling the first exception.\n \n \n \n In order to fix confusing messages like this, one option is:\n \n ```python\n \n try: \n \n  print(d[key]) # raises KeyError\n \n except KeyError as e:\n \n  raise RuntimeError(\"no member called {}\".format(key)) from e\n \n ```\n \n which outputs\n \n ```\n \n Traceback (most recent call last):\n \n  File \"/home/nitta/tmp/test.py\", line 4, in <module>\n \n  print(d[key]) # raises KeyError\n \n KeyError: 'cat'\n \n \n \n The above exception was the direct cause of the following exception:\n \n \n \n Traceback (most recent call last):\n \n  File \"/home/nitta/tmp/test.py\", line 6, in <module>\n \n  raise RuntimeError(\"no member called {}\".format(key)) from e\n \n RuntimeError: no member called cat\n \n ```\n \n \n \n However, in this case, the above message is a bit noisy and redundant.\n \n If we want to suppress the first error message, we should use `raise .. from None` so as to suppress the first exception message (see [Suppress Context in PEP 3134](https://www.python.org/dev/peps/pep-3134/#open-issue-suppressing-context)).\n \n ```python\n \n try: \n \n  print(d[key]) # raises KeyError\n \n except KeyError:\n \n  raise RuntimeError(\"no member called {}\".format(key)) from None\n \n ```\n \n which outputs\n \n ```\n \n Traceback (most recent call last):\n \n  File \"/home/nitta/tmp/test.py\", line 6, in <module>\n \n  raise RuntimeError(\"no member called {}\".format(key)) from None\n \n RuntimeError: no member called cat\n \n ```\n \n \n \n Do you think using `raise .. from None` is reasonable in the below case or simliar cases?\n \n https://github.com/pytorch/pytorch/blob/c7787f7fbf388f9fe326d581726e7ef3f2dce74b/torch/jit/annotations.py#L32-L35",
      "y": "``` try: \n  print(d[key]) # raises KeyError\n except KeyError as e:\n  raise RuntimeError(\"no member called {}\".format(key)) from e\n ```"
   },
   {
      "null": 423,
      "x": "PyTorch's div, which performs true division, cannot be exported to ONNX with consistent semantics",
      "z": "Sure thing: draft is https://github.com/pytorch/pytorch/pull/42907.",
      "y": "Throw a runtime error if a user tries to use div to perform integer division in 1.6"
   },
   {
      "null": 424,
      "x": "Segmentation fault in DataLoader worker in PyTorch 1.8.0 if set_num_threads is called beforehand",
      "z": "I managed to obtain a backtrace of the segfault following the instructions in #53894. It seems to happen in `set_num_threads`. I'm guessing that `pthreadpool_destroy` is being called on a thread pool that no longer exists after forking... but if that's the case why does it not crash when the initial `set_num_threads` is called with a low number? Anyway, I hope this helps narrow down the problem.\n \n \n \n ```\n \n #0 0x00007ffff7fa1aab in __pthread_clockjoin_ex (threadid=140735666181888, thread_return=0x0, clockid=0, abstime=0x0, block=true) at pthread_join_common.c:89\n \n  pd = 0x7fff9363d700\n \n  self = <optimized out>\n \n  result = <optimized out>\n \n  pd_result = <optimized out>\n \n #1 0x00007fffe70b88db in pthreadpool_destroy () from /home/ubuntu/.local/share/virtualenvs/test-2nMYMFG7/lib/python3.7/site-packages/torch/lib/libtorch_cpu.so\n \n No symbol table info available.\n \n #2 0x00007fffe4cc6a07 in caffe2::PThreadPool::set_thread_count(unsigned long) () from /home/ubuntu/.local/share/virtualenvs/test-2nMYMFG7/lib/python3.7/site-packages/torch/lib/libtorch_cpu.so\n \n No symbol table info available.\n \n #3 0x00007fffe38dacbf in at::set_num_threads(int) () from /home/ubuntu/.local/share/virtualenvs/test-2nMYMFG7/lib/python3.7/site-packages/torch/lib/libtorch_cpu.so\n \n No symbol table info available.\n \n #4 0x00007ffff571a9d6 in THPModule_setNumThreads(_object*, _object*) () from /home/ubuntu/.local/share/virtualenvs/test-2nMYMFG7/lib/python3.7/site-packages/torch/lib/libtorch_python.so\n \n No symbol table info available.\n \n #5 0x000055555566e803 in _PyCFunction_FastCallKeywords ()\n \n No symbol table info available.\n \n #6 0x0000555555703ed4 in ?? ()\n \n No symbol table info available.\n \n #7 0x0000555555700fe2 in _PyEval_EvalFrameDefault ()\n \n No symbol table info available.\n \n #8 0x00005555556716ba in _PyFunction_FastCallDict ()\n \n No symbol table info available.\n \n #9 0x00005555556fe0fd in _PyEval_EvalFrameDefault ()\n \n No symbol table info available.\n \n #10 0x0000555555670a0a in _PyFunction_FastCallKeywords ()\n \n No symbol table info available.\n \n #11 0x0000555555703dcb in ?? ()\n \n No symbol table info available.\n \n #12 0x00005555556fcc78 in _PyEval_EvalFrameDefault ()\n \n No symbol table info available.\n \n #13 0x0000555555670a0a in _PyFunction_FastCallKeywords ()\n \n No symbol table info available.\n \n #14 0x0000555555703dcb in ?? ()\n \n No symbol table info available.\n \n #15 0x00005555556fcc78 in _PyEval_EvalFrameDefault ()\n \n No symbol table info available.\n \n #16 0x0000555555670a0a in _PyFunction_FastCallKeywords ()\n \n No symbol table info available.\n \n #17 0x0000555555703dcb in ?? ()\n \n No symbol table info available.\n \n #18 0x00005555556fcc78 in _PyEval_EvalFrameDefault ()\n \n No symbol table info available.\n \n #19 0x0000555555671cfa in _PyObject_Call_Prepend ()\n \n No symbol table info available.\n \n #20 0x00005555556c6d1c in ?? ()\n \n No symbol table info available.\n \n #21 0x00005555556c2f19 in ?? ()\n \n No symbol table info available.\n \n #22 0x000055555566fd65 in _PyObject_FastCallKeywords ()\n \n No symbol table info available.\n \n #23 0x0000555555703f51 in ?? ()\n \n No symbol table info available.\n \n #24 0x00005555556fcbe8 in _PyEval_EvalFrameDefault ()\n \n No symbol table info available.\n \n #25 0x0000555555670a0a in _PyFunction_FastCallKeywords ()\n \n No symbol table info available.\n \n #26 0x0000555555703dcb in ?? ()\n \n No symbol table info available.\n \n #27 0x0000555555700fe2 in _PyEval_EvalFrameDefault ()\n \n No symbol table info available.\n \n #28 0x0000555555670a0a in _PyFunction_FastCallKeywords ()\n \n No symbol table info available.\n \n #29 0x0000555555703dcb in ?? ()\n \n No symbol table info available.\n \n #30 0x0000555555700fe2 in _PyEval_EvalFrameDefault ()\n \n No symbol table info available.\n \n ```",
      "y": "Register pthread_atfork that would leak pthread pool."
   },
   {
      "null": 425,
      "x": "RRef.to_here() does not synchronize CUDA Streams properly",
      "z": "Yeah the idea of storing CUDA events on the RRef makes sense to me. Although it must be said that I know nothing about RRefs so I don't know if other approaches could suit better.\n \n \n \n As for passing the devices to the callback, I am guessing that this is in order to avoid the callback itself having to inspect the value and extract the devices it uses, right? (And we know that inspecting a value is not easy) If so then, yes, I think passing a set of devices to the callback could make sense. Just to clarify though: this would be just a change in an internal API, it wouldn't mean that all end users will now need to expect an extra argument in their Python functions too right?",
      "y": "The ctx in the following code is an instance of CudaLazyStreamContext when CUDA is enabled. Its streams_\n field holds the devices and current streams where this request will be run on. The proposed solution is to\n extract the devices vector from the ctx and pass that to cb_->operator()(requestMessage); invocation, and\n then propogate it to the created RRef accordingly."
   },
   {
      "null": 426,
      "x": "Single-matrix cholesky much slower than batch mode with batch_size=1?",
      "z": "Thanks for the comments @rfeinman ! I think we don't have a `ldl` operator in pytorch right now, but that sounds like a good idea! If you didn't find an existing issue for this, feel free to file a new issue with your feature request and comments so that we can discuss and track it.",
      "y": "Have an `ldl` operator in pytorch."
   },
   {
      "null": 427,
      "x": "Irrelevant named tensor warnings",
      "z": "We've fixed the problems with max_pool{1, 2, 3}d on master with https://github.com/pytorch/pytorch/pull/60059. The fix will be in our next release.",
      "y": "Fixed the problems with max_pool{1, 2, 3}d."
   },
   {
      "null": 428,
      "x": "Currently, LibTorch cannot be downloaded",
      "z": "Created https://github.com/pytorch/pytorch.github.io/pull/660 to fix",
      "y": "Update libtorch links for CUDA 10.2 ."
   },
   {
      "null": 429,
      "x": "The latest code cannot build due to missing file CPUFuntion.h",
      "z": "Yes, I use pip install numpy pyyaml mkl mkl-include setuptools cmake cffi typing_extensions future six requests dataclasses\n \n except ninja, since install ninja will lead to other issues.\n \n I will pull top of tree again to see whether this issue can reproduce today.\n \n \n \n > Can you ensure that you have installed all dependencies in the repro steps? https://github.com/pytorch/pytorch#install-dependencies",
      "y": "Pulled the latest code, and compiled successfully."
   },
   {
      "null": 430,
      "x": "Improve torch.linalg documentation",
      "z": "Together with Mario, we've found out that some Unicode symbols (for example \u00e2\u20ac\u0153\u00e2\u201a\u2122\u00e2\u20ac",
      "y": "Some Unicode symbols (for example \u00e2\u20ac\u0153\u00e2\u201a\u2122\u00e2\u20ac"
   },
   {
      "null": 431,
      "x": "skip_if_not_multigpu decorator skipping/passing tests",
      "z": "1) The decorator implementation for `@skip_if_not_multigpu` is wrong so it is not running any of the tests that use it. PR to resolve: https://github.com/pytorch/pytorch/pull/54916.\n \n 2) That works, thank you!\n \n 3) That particular test garrett was running was only for RPC (tensorpipe)",
      "y": "The decorator implementation for `@skip_if_not_multigpu` is wrong so it is not running any of the tests that use it."
   },
   {
      "null": 432,
      "x": "torch.ceil wrong formula",
      "z": "cc @kshitij12345, our most recent unary ops maintainer",
      "y": "`ceil(x) != floor(x) + 1` when x is already an integer."
   },
   {
      "null": 433,
      "x": "Doc of SVD is erroneous and inconsistent",
      "z": "This should be solved by https://github.com/pytorch/pytorch/pull/54002 once it's merged.\n \n There's also the larger issued https://github.com/pytorch/pytorch/issues/54878 that aims to improve the docs of all `torch.linalg`.",
      "y": "Improve the docs of all `torch.linalg`."
   },
   {
      "null": 434,
      "x": "Maybe one more '=' sign?",
      "z": "Thanks for catching this! We would accept a PR to fix this.",
      "y": "Update previous-versions.md."
   },
   {
      "null": 435,
      "x": "CUDA error: device-side assert triggered(torch1.8.1+cuda11.1)",
      "z": "I think this issue has the same root cause as #54245 and #52663 and it was actually not fixed by switching from thrust to cub. The cause should be the `STB_GNU_UNIQUE` symbols as pointed out by @VoVAllen in https://github.com/NVIDIA/thrust/issues/1401#issuecomment-806403746. \n \n \n \n To verify the above claim, I ran [this python script](http://ppwwyyxx.com/blog/2021/Patch-STB_GNU_UNIQUE/) on `libtorch_cuda_cpp.so` inside pytorch, and the bug disappears. The script hides all symbols with `STB_GNU_UNIQUE` type.\n \n \n \n A proper fix is probably to try the `-fno-gnu-unique` option as mentioned in https://github.com/pytorch/pytorch/issues/52663#issuecomment-809092114.",
      "y": "Try the `-fno-gnu-unique` option."
   },
   {
      "null": 436,
      "x": "AccessDeniedAccess when download libtorch 1.8.1 for Linux for CUDA 10.2 from download.pytorch.org",
      "z": "Duplicate of #54855",
      "y": "Unable to download libtorch (1.8.1) from pytorch.org."
   },
   {
      "null": 437,
      "x": "The results of pipeline parallelism cannot be reproduced when using different numbers of partitions",
      "z": "I've fixed the tutorials to handle batch size correctly. Feel free to re-open this issue if there is still a problem that needs to be resolved.",
      "y": "Integrate Sentence Embedding training and fine-tuning in DVC pipeline."
   },
   {
      "null": 438,
      "x": "torch.jit.trace is not working and causing program stop working",
      "z": "thanks for your help I've fixed it by my self wrong pytorch version was installed:)",
      "y": "Check for wrong pytorch version."
   },
   {
      "null": 439,
      "x": "DDP checkpointing tests, test failures",
      "z": "to unblock you to land your PR, you can help skip the two tests, I will enable it after fixing. thanks!",
      "y": "Enable static graph training in DDP."
   },
   {
      "null": 440,
      "x": "F.embedding has unexpected behavior with non-2d weights",
      "z": "Thanks for reporting this! I was able to reproduce it on master.\n \n \n \n [F.embedding()](https://pytorch.org/docs/stable/nn.functional.html#embedding) is not intended to support non-2D weight, so we would accept a PR adding a dim check to ensure this.",
      "y": "F.embedding() is not intended to support non-2D weight."
   },
   {
      "null": 441,
      "x": "[torch.jit.script] Python type cannot be used as a value:",
      "z": "This is because `nn.Module` initialization isn't supported in TorchScript. Instead, it's recommended to be constructed as a submodule of another module.\n \n \n \n ```\n \n class ParentModule(torch.nn.Module):\n \n  def __init__(self):\n \n  super().__init__()\n \n  self.softmax = torch.nn.Softmax(dim=-1)\n \n  \n \n  def forward(self, x: torch.Tensor):\n \n  return self.softmax(x)\n \n \n \n m = ParentModule()\n \n scripted_m = torch.jit.script(m)\n \n print(scripted_m(torch.rand([1, 2, 3])))\n \n ```",
      "y": "`nn.Module` initialization isn't supported in TorchScript."
   },
   {
      "null": 442,
      "x": "[libtorch] tensor.to(torch::Device(torch::kCPU)) is VERY SLOW",
      "z": "Hi, it would be very helpful if you could post a script of your test. One possibility that comes to mind is that your measurement for the forward propagation is not synchronizing the CUDA device so when you move the data back to the CPU you have to wait for the computation for finish first.",
      "y": "Measurement for the forward propagation is not synchronizing the CUDA device."
   },
   {
      "null": 443,
      "x": "Improve torch.distributed.new_group() documentation in the context of SyncBatchNorm",
      "z": "> > it is not clear that when creating subgroups, every rank still needs to make the call regardless of whether they are in the group or not, as per the docs in https://pytorch.org/docs/master/distributed.html#torch.distributed.new_group.\n \n > \n \n > Don't the docs already mention this:\n \n > \n \n > > This function requires that all processes in the main group (i.e. all processes that are part of the distributed job) enter this function, even if they are not going to be members of the group. Additionally, groups should be created in the same order in all processes.\n \n \n \n I meant the docs/example in sync_batchnorm itself: https://pytorch.org/docs/stable/generated/torch.nn.SyncBatchNorm.html#torch.nn.SyncBatchNorm.convert_sync_batchnorm, where the example doesn't really show that very clearly. Although the user can also check the `new_group` docs itself of course, updating that example would probably also be useful.",
      "y": "This function requires that all processes in the main group (i.e. all processes that are part of the distributed job) enter this function, even if they are not going to be members of the group."
   },
   {
      "null": 444,
      "x": "Enable linear algebra functions on ROCm platform",
      "z": "> Some changes in code would need to be made so that only magma v2 API magma_v2.h is used (maybe v1 works fine as well, but it includes only cublas.h by default, while v2 seems to work with hipblas.h.\n \n \n \n In #49978 I try to change the magma includes to magma_v2.h.",
      "y": "Code changes so that only magma v2 API magma_v2.h is used (maybe v1 works fine as well, but it includes only cublas.h by default, while v2 seems to work with hipblas.h."
   },
   {
      "null": 445,
      "x": "the result of torch.addcmul(input, tensor1, tensor2, value) != the result of input\u00e2\u20ac\u2039+value*tensor1\u00e2\u20ac\u2039*tensor2",
      "z": "The difference is well below the order of 1e-5, which is expected for fp32 arithmetics. Afterall, floating point computation is not exact.",
      "y": "Difference is well below the order of 1e-5 and is expected for fp32 arithmetics."
   },
   {
      "null": 446,
      "x": "torch.multinomial selects elements with zero weight",
      "z": "Will follow up with @ngimel on how reliable is the failure. (I can reproduce it on my Linux box, but never on a Mac, so can it be a compiler problem?)",
      "y": "Problem where \"0\" was generated at the same position as a non-zero probability, effectively masking it."
   },
   {
      "null": 447,
      "x": "Support autograd in `torch.svd` with complex inputs",
      "z": "certainly!",
      "y": "Add autograd tests for complex matrix norm nuclear and +/-2 ."
   },
   {
      "null": 448,
      "x": "Problem with building from source in win7",
      "z": "Extracted error message:\n \n ```\n \n  Creating library lib\\cuda_atomic_ops_test.lib and object lib\\cuda_atomic_ops_test.exp\n \n cuda_atomic_ops_test_generated_cuda_atomic_ops_test.cu.obj : error LNK2019: unresolved external symbol \"float __cdecl pow(float,int)\" (?pow@@YAMMH@Z) referenced in function \"void __cdecl test_atomic_mul<float>(void)\" (??$test_atomic_mul@M@@YAXXZ)\n \n  Hint on symbols that are defined and could potentially match:\n \n  \"float __cdecl pow(float,float)\" (?pow@@YAMMM@Z)\n \n cuda_atomic_ops_test_generated_cuda_atomic_ops_test.cu.obj : error LNK2019: unresolved external symbol \"double __cdecl pow(double,int)\" (?pow@@YANNH@Z) referenced in function \"void __cdecl test_atomic_mul<double>(void)\" (??$test_atomic_mul@N@@YAXXZ)\n \n  Hint on symbols that are defined and could potentially match:\n \n  \"float __cdecl pow(float,float)\" (?pow@@YAMMM@Z)\n \n ```\n \n Looks like we should do an explicit type casting there.",
      "y": "Do an explicit type casting there."
   },
   {
      "null": 449,
      "x": "aarch64 CMake checks are incorrect on Apple Silicon",
      "z": "cc @janeyx99",
      "y": "No updates will be submitted to QNNPACK as it is an archive repository. None of the PyTorch operators depend on third_party/QNNPACK, only some legacy caffe2 ops."
   },
   {
      "null": 450,
      "x": "Test self.assertEqual with msg does not print numerical comparison results",
      "z": "\"testing\" could probably have a better, more distinct, name, but it's intended to be used for issues related to the torch.testing module vs. PyTorch's actual tests (which get \"module: tests\"). \n \n \n \n I think we would accept a PR combining custom messages and the default message. I don't have an immediate design for what these messages would look like. Maybe some other testing libraries have similar functionality and formatting we can use for inspiration? We also don't need to overthink it. We could probably print the \"default\" message and then just append the the provided msg string, for example.",
      "y": "It's intended to be used for issues related to the torch.testing module vs. PyTorch's actual tests."
   },
   {
      "null": 451,
      "x": "Compile error in Windows SDK",
      "z": "pthreadpool isn't our code. And we already cleaned up the imports in our code with https://github.com/pytorch/pytorch/pull/48009.",
      "y": "Reorganize and refine the Windows.h import in C++ files."
   },
   {
      "null": 452,
      "x": "Transcendental functions broken on Jetson Xavier NX",
      "z": "Thank you for looking into this issue! I included the output of this script in the issue description (the first comment at the top of the page).",
      "y": "If PyTorch for AARCH64 is compiled by clang, problem goes away."
   },
   {
      "null": 453,
      "x": "pytorch 1.4 can not load model saved by 1.7",
      "z": "Pytorch doesn't guarantee forward compatibility, but for this particular issue using `torch.save(_use_new_zipfile_serialization=False)` in 1.7 and load it in 1.4 might work. Please feel free to reopen if it doesn't fix. Thanks!",
      "y": "`torch.save(_use_new_zipfile_serialization=False)` in 1.7 and load it in 1.4."
   },
   {
      "null": 454,
      "x": "Could we wrap some pytorch operations into a large symbolic Op and export it to onnx ?",
      "z": "Hi, @spandantiwari \n \n How to turn off trace and turn on trace while in exporting to onnx.\n \n Following is my sample code and it won't work.\n \n \n \n ```\n \n import torch\n \n \n \n \n \n class TestBatchedNMSop(torch.autograd.Function):\n \n \n \n  @staticmethod\n \n  def forward(ctx,\n \n  boxes,\n \n  scores,\n \n  num_classes,\n \n  topk,\n \n  keep_topk,\n \n  score_threshold,\n \n  iou_threshould,\n \n  is_normalized=False,\n \n  clip_boxes=False,\n \n  share_location=False,\n \n  background_label_id=-1):\n \n  \n \n  return TestBatchedNMSop.output\n \n \n \n  @staticmethod\n \n  def symbolic(g,\n \n  boxes,\n \n  scores,\n \n  num_classes,\n \n  topk,\n \n  keep_topk,\n \n  score_threshold,\n \n  iou_threshould,\n \n  is_normalized=False,\n \n  clip_boxes=False,\n \n  share_location=False,\n \n  background_label_id=-1):\n \n  return g.op(\n \n  'BatchedNMS_TRT',\n \n  boxes,\n \n  scores,\n \n  numClasses_i=num_classes,\n \n  topK_i=topk,\n \n  keepTopK_i=keep_topk,\n \n  scoreThreshold_f=score_threshold,\n \n  iouThreshold_f=iou_threshould,\n \n  isNormalized_i=is_normalized,\n \n  clipBoxes_i=clip_boxes,\n \n  shareLocation_i=share_location,\n \n  backgroundLabelId_i=background_label_id,\n \n  outputs=len(TestBatchedNMSop.output))\n \n \n \n class TestModel(torch.nn.Module):\n \n  def __init__(self,\n \n  num_classes=80,\n \n  topk=10000,\n \n  keep_topk=1000,\n \n  score_threshold=0.05,\n \n  iou_threshould=0.5,\n \n  is_normalized=False,\n \n  clip_boxes=False,\n \n  share_location=False,\n \n  background_label_id=-1):\n \n  super(TestModel, self).__init__()\n \n  self.num_classes = num_classes\n \n  self.topk = topk\n \n  self.keep_topk = keep_topk\n \n  self.score_threshold = score_threshold\n \n  self.iou_threshould = iou_threshould\n \n  self.is_normalized = is_normalized\n \n  self.clip_boxes = clip_boxes\n \n  self.share_location = share_location\n \n  self.background_label_id = background_label_id\n \n  \n \n  def forward(self, boxes, scores):\n \n  # turn off tracing\n \n  #####################Dose not Work #####################\n \n  state = torch._C._get_tracing_state() # does not work\n \n  #####################Dose not Work #####################\n \n  # do normal process\n \n  batch_size = boxes.shape[0]\n \n  num_detections = torch.zeros(batch_size, 1) + 100\n \n  nmsed_boxes = torch.ones(batch_size, self.keep_topk, 4)\n \n  nmsed_scores = torch.ones(batch_size, self.keep_topk)\n \n  nmsed_classes = torch.ones(batch_size, self.keep_topk)\n \n  # directly save output, so our temporarily\n \n  # Customop does not need to implement any\n \n  # special code\n \n  output = (num_detections, nmsed_boxes, nmsed_scores, nmsed_classes)\n \n  setattr(TestBatchedNMSop, 'output', output)\n \n  # open tracing\n \n  #####################Dose not Work #####################\n \n  torch._C._set_tracing_state(state) # does not work\n \n  #####################Dose not Work #####################\n \n  # we do not need to save the output\n \n  # just call it for creating a correspond\n \n  # node in graph\n \n  return TestBatchedNMSop.apply(boxes, \n \n  scores, \n \n  self.num_classes,\n \n  self.topk, \n \n  self.keep_topk, \n \n  self.score_threshold, \n \n  self.iou_threshould,\n \n  self.is_normalized, \n \n  self.clip_boxes, \n \n  self.share_location, \n \n  self.background_label_id)\n \n \n \n boxes = torch.ones(1, 8732, 80, 4)\n \n scores = torch.ones(1, 8732, 80)\n \n \n \n model = TestModel().eval()\n \n torch.onnx.export(\n \n  model, \n \n  (boxes, scores),\n \n  \"batchednms_trt.onnx\",\n \n  input_names=['boxes', 'scores'],\n \n  output_names=['num_detections', 'nmsed_boxes', 'nmsed_scores', 'nmsed_classes'],\n \n  export_params=True,\n \n  keep_initializers_as_inputs=True,\n \n  enable_onnx_checker=False,\n \n  verbose=True)\n \n ```\n \n \n \n Could anyone please help me with this?\n \n Thanks.",
      "y": "Can export a PyTorch op as a custom op to ONNX through the PyTorch-ONNX exporter."
   },
   {
      "null": 455,
      "x": "On macOS Big Sur, parallel DataLoader causes global variables to be reinitialised",
      "z": "Seems spawn vs fork behavior. If you do **either** of the two things below, you should see the same behavior on your previous osx (which likely contains an earlier python version that still uses fork by default on osx):\n \n 1. move `set_var` outside the `if __name__ == '__main__':`\n \n 2. use `multiprocessing_context='fork'` in creating `DataLoader`",
      "y": "1. move `set_var` outside the `if __name__ == '__main__':`\n OR\n 2. use `multiprocessing_context='fork'` in creating `DataLoader`"
   },
   {
      "null": 456,
      "x": "`test_variant_consistency_jit` for BFloat16 are skipped in a confusing way",
      "z": "A way to make the code cleaner would just to skip bfloat16 for all of the JIT test cases instead of screening them out on the OpInfo level",
      "y": "Skip bfloat16 for all of the JIT test cases instead of screening them out on the OpInfo level."
   },
   {
      "null": 457,
      "x": "Profiling distributed NCCL collectives deadlocks when profiler run with use_cuda=True",
      "z": "just to clarify - this doesn't seem profiler specific (profiler just creates a bunch of cuda events on the devices) and this deadlock is triggered in other scenarios - @rohan-varma to confirm",
      "y": "It happens as a result of profiler creating events on each device. We can probably mitigate this by setting `CUDA_VISIBLE_DEVICES` properly."
   },
   {
      "null": 458,
      "x": "How to do polymorphism on torch::nn::ModuleHolder?",
      "z": "Thanks for your reply.\n \n I finally decide to use std::shared_ptr<M>, I will be losing the more natural constructor, but it is straight forward and it works.",
      "y": "Use std::shared_ptr<M> ."
   },
   {
      "null": 459,
      "x": "test_cholesky_solve_batched_many_batches_cuda_complex128 has cuda illegal memory access",
      "z": "I can repro that with 100% rate on cuda 11.1, 2070 or 3090. The failed line says it's `torch.Size([256, 256, 5, 5]) False`\n \n \n \n https://github.com/pytorch/pytorch/blob/d307601365c3b848072b8b8381208aedc1a0aca5/test/test_linalg.py#L1845",
      "y": "Workaround for MAGMA accessing illegal memory in batched cholesky."
   },
   {
      "null": 460,
      "x": "`torch.digamma`: Inconsistent with SciPy",
      "z": "This got fixed in https://github.com/pytorch/pytorch/pull/56689",
      "y": "Fix inconsistency of digamma with SciPy."
   },
   {
      "null": 461,
      "x": "If test suite triggers CUDA assert, should stop running tests",
      "z": "wdym by `failfast` option? If it's aborting after any failure in cuda test, then no, that's not what's requested here. We want to abort only if one of context-invalidating (sticky) errors happened.",
      "y": "Abort only if one of context-invalidating (sticky) errors happened."
   },
   {
      "null": 462,
      "x": "RuntimeError : PyTorch was compiled without NumPy support",
      "z": "You have to install another version of numpy.\n \n ```\n \n pip install numpy==1.15.0\n \n Collecting numpy==1.15.0\n \n  Downloading https://files.pythonhosted.org/packages/27/92/c01d3a6c58ceab0e6ec36ad3af41bc076014cc916afcb979ab4c9558f347/numpy-1.15.0-cp37-cp37m-manylinux1_x86_64.whl (13.8MB)\n \n  100% |\u00e2\u2013\u02c6\u00e2\u2013\u02c6\u00e2\u2013\u02c6\u00e2\u2013\u02c6\u00e2\u2013\u02c6\u00e2\u2013\u02c6\u00e2\u2013\u02c6\u00e2\u2013\u02c6\u00e2\u2013\u02c6\u00e2\u2013\u02c6\u00e2\u2013\u02c6\u00e2\u2013\u02c6\u00e2\u2013\u02c6\u00e2\u2013\u02c6\u00e2\u2013\u02c6\u00e2\u2013\u02c6\u00e2\u2013\u02c6\u00e2\u2013\u02c6\u00e2\u2013\u02c6\u00e2\u2013\u02c6\u00e2\u2013\u02c6\u00e2\u2013\u02c6\u00e2\u2013\u02c6\u00e2\u2013\u02c6\u00e2\u2013\u02c6\u00e2\u2013\u02c6\u00e2\u2013\u02c6\u00e2\u2013\u02c6\u00e2\u2013\u02c6\u00e2\u2013\u02c6\u00e2\u2013\u02c6\u00e2\u2013\u02c6| 13.9MB 2.9MB/s \n \n Installing collected packages: numpy\n \n  Found existing installation: numpy 1.15.4\n \n  Uninstalling numpy-1.15.4:\n \n  Successfully uninstalled numpy-1.15.4\n \n Successfully installed numpy-1.15.0\n \n ```\n \n Above downgrading 1.15.4 -> 1.15.0.\n \n \n \n Then install another torch version.\n \n ```\n \n pip install torch==0.4.1.post2\n \n Collecting torch==0.4.1.post2\n \n  Downloading https://files.pythonhosted.org/packages/d3/91/1b2871d6c8ca079254deae5872af32e02e9a85f07dd0834e8b3489ce138f/torch-0.4.1.post2-cp37-cp37m-manylinux1_x86_64.whl (519.5MB)\n \n  100% |\u00e2\u2013\u02c6\u00e2\u2013\u02c6\u00e2\u2013\u02c6\u00e2\u2013\u02c6\u00e2\u2013\u02c6\u00e2\u2013\u02c6\u00e2\u2013\u02c6\u00e2\u2013\u02c6\u00e2\u2013\u02c6\u00e2\u2013\u02c6\u00e2\u2013\u02c6\u00e2\u2013\u02c6\u00e2\u2013\u02c6\u00e2\u2013\u02c6\u00e2\u2013\u02c6\u00e2\u2013\u02c6\u00e2\u2013\u02c6\u00e2\u2013\u02c6\u00e2\u2013\u02c6\u00e2\u2013\u02c6\u00e2\u2013\u02c6\u00e2\u2013\u02c6\u00e2\u2013\u02c6\u00e2\u2013\u02c6\u00e2\u2013\u02c6\u00e2\u2013\u02c6\u00e2\u2013\u02c6\u00e2\u2013\u02c6\u00e2\u2013\u02c6\u00e2\u2013\u02c6\u00e2\u2013\u02c6\u00e2\u2013\u02c6| 519.5MB 170kB/s \n \n Installing collected packages: torch\n \n  Found existing installation: torch 0.4.1\n \n  Uninstalling torch-0.4.1:\n \n  Successfully uninstalled torch-0.4.1\n \n Successfully installed torch-0.4.1.post2\n \n ```\n \n Here upgrade to torch-0.4.1.post2 from torch-0.4.1",
      "y": "Install another version of numpy."
   },
   {
      "null": 463,
      "x": "Cannot find libcaffe2_gpu.so",
      "z": "What is the build command that you used to build Caffe2? \"from caffe2.python import core\" is not a build command. This is a snippet of python code which the Python executable will try to run based on the libraries that it has installed. You must install Caffe2 for Python to use it correctly, and to install Caffe2 you must either build it from source or download a prebuilt binary from conda.\n \n \n \n Please also post your full cmake output.\n \n \n \n You should read through https://caffe2.ai/docs/faq.html , it will help you debug many of these types of errors.",
      "y": "Add the location of the file to the `LD_LIBRARY_PATH` ."
   },
   {
      "null": 464,
      "x": "[Report] nightly build with Jupyter stuck",
      "z": "I have just cloned and installed the latest version of pytorch in a new environment, installed jupyter notebook after updating pip, and I have no problems now importing pytorch in a jupyter notebook.\n \n \n \n ![image](https://user-images.githubusercontent.com/9110200/39118797-8282cf1a-46ea-11e8-8829-67cba16d5da9.png)\n \n \n \n \n \n I'm closing this issue now as fixed, but let me know if it still doesn't work for you",
      "y": "Clone and install the latest version of pytorch in a new environment, install jupyter notebook after updating pip."
   },
   {
      "null": 465,
      "x": "[Caffe2] Unable to clone Caffe2 repo",
      "z": "The instructions from @dbermond are completely correct. The caffe2 code is at https://github.com/pytorch/pytorch",
      "y": "`$ git clone --recursive https://github.com/pytorch/pytorch.git`"
   },
   {
      "null": 466,
      "x": "[ONNX] Cannot export upsample op",
      "z": "I receive a PyTorch - ONNX Export Error with Opset 11 caused by F.Interpolate/nn.Upsampling...2d. See https://github.com/pytorch/pytorch/issues/30681\n \n \n \n PyTorch model export to ONNX fails when using PyTorch 1.3.1 or nightly and ONNX 1.5.0. Failure is caused by all upsample operations: \n \n \n \n - `nn.UpsamplingBilinear2d(scale_factor=2)`\n \n - `nn.UpsamplingNearest2d(scale_factor=2)`\n \n - `F.interpolate(x, scale_factor=2)`\n \n \n \n \n \n Error message:\n \n ```bash\n \n torch version 1.4.0.dev20191127\n \n onnx version 1.5.0\n \n torch.Size([1, 3, 256, 256]) torch.Size([1, 3, 512, 512])\n \n Traceback (most recent call last):\n \n  File \"/Users/glennjocher/PycharmProjects/iD/upsample_error_reproduce.py\", line 28, in <module>\n \n  onnx.checker.check_model(oml)\n \n  File \"/Users/glennjocher/opt/anaconda3/envs/pytorch_nightly/lib/python3.7/site-packages/onnx/checker.py\", line 86, in check_model\n \n  C.check_model(model.SerializeToString())\n \n onnx.onnx_cpp2py_export.checker.ValidationError: Node (Resize_23) has input size 4 not in range [min=2, max=2].\n \n \n \n ==> Context: Bad node spec: input: \"input\" input: \"20\" input: \"20\" input: \"23\" output: \"24\" name: \"Resize_23\" op_type: \"Resize\" attribute { name: \"coordinate_transformation_mode\" s: \"asymmetric\" type: STRING } attribute { name: \"cubic_coeff_a\" f: -0.75 type: FLOAT } attribute { name: \"mode\" s: \"nearest\" type: STRING } attribute { name: \"nearest_mode\" s: \"floor\" type: STRING }\n \n ```\n \n \n \n ## To Reproduce\n \n \n \n ```python\n \n # conda install pytorch torchvision -c pytorch-nightly\n \n # conda install -c conda-forge onnx\n \n import torch\n \n import torch.nn as nn\n \n import torch.nn.functional as F\n \n import onnx\n \n \n \n \n \n class TestModel(nn.Module):\n \n  def __init__(self):\n \n  super(TestModel, self).__init__()\n \n \n \n  def forward(self, x):\n \n  # m = nn.UpsamplingBilinear2d(scale_factor=2) # fails\n \n  # m = nn.UpsamplingNearest2d(scale_factor=2) # fails\n \n  x = F.interpolate(x, scale_factor=2) # fails\n \n  return x\n \n \n \n \n \n print('torch version %s' % torch.__version__)\n \n print('onnx version %s' % onnx.__version__)\n \n img = torch.randn(1, 3, 256, 256)\n \n tml = TestModel()\n \n print(img.shape, tml(img).shape)\n \n torch.onnx.export(tml, img, 'model.onnx', verbose=False, opset_version=11)\n \n \n \n oml = onnx.load('model.onnx') # onnx model\n \n onnx.checker.check_model(oml)\n \n ```",
      "y": "ONNX Interpolate Add Scales Params."
   },
   {
      "null": 467,
      "x": "[Caffe2] Unable to compile Caffe 2 with CUDA 9",
      "z": "Just tried. Yup works with 5.5.0",
      "y": "works with 5.5.0."
   },
   {
      "null": 468,
      "x": "error when loading model saved under newer version",
      "z": "loading model is backward compatible but not forward compatible.",
      "y": "Loading model is not forward compatible. It is backward compatible."
   },
   {
      "null": 469,
      "x": "[caffe2] EigenTranspose problem in math_cpu.cc",
      "z": "I think we can close the issue now because the eigen transpose is disabled in the latest commit #7112 .",
      "y": "Eigen transpose is disabled in the latest commit."
   },
   {
      "null": 470,
      "x": "backward not working properly in svd (pytorch-nightly)",
      "z": "It's a shorthand for \"reproduced\", which means that the bug still hasn't been fixed. Sorry, we'll try to get to it soon.",
      "y": "Fix SVD backward on non-square matrices when some=False ."
   },
   {
      "null": 471,
      "x": "Arguments are located on different GPU with torch.nn.EmbeddingBag",
      "z": "Thanks for reporting! This appears to be fixed on both 0.4 release (very soon) and master.",
      "y": "Fixed on both 0.4 release and master."
   },
   {
      "null": 472,
      "x": "[PyTorch] Printing large tensors is slow",
      "z": "I'm looking into this",
      "y": "Fix half tensor printing plus speedup large tensor printing."
   },
   {
      "null": 473,
      "x": "[PyTorch] Don't use scientific notation for printing integer tensors",
      "z": "duplicate (sort of) of https://github.com/pytorch/pytorch/issues/6811",
      "y": "Integer tensors won't be printed in scientific notation.\n Removed scaling factor for all tensors.\n Only leave spaces for negative signs if one of the printed elements is going to have an negative sign."
   },
   {
      "null": 474,
      "x": "[pytorch] randperm lacks CUDA implementation",
      "z": "Ok, I solved the issue by changing two lines in the `utils/data/sampler.py` file.\n \n \n \n ```python\n \n class RandomSampler(Sampler):\n \n  r\"\"\"Samples elements randomly, without replacement.\n \n \n \n  Arguments:\n \n  data_source (Dataset): dataset to sample from\n \n  \"\"\"\n \n \n \n  def __init__(self, data_source):\n \n  self.data_source = data_source\n \n \n \n  def __iter__(self):\n \n  cpu = torch.device('cpu')\n \n  return iter( torch.randperm( len(self.data_source), device = cpu).tolist())\n \n \n \n  def __len__(self):\n \n  return len(self.data_source)\n \n ```\n \n \n \n **if** it is okay, I can PR.",
      "y": "Change two lines in the `utils/data/sampler.py` file.\n \n \n \n ```python\n \n class RandomSampler(Sampler):\n \n  r\"\"\"Samples elements randomly, without replacement.\n \n \n \n  Arguments:\n \n  data_source (Dataset): dataset to sample from\n \n  \"\"\"\n \n \n \n  def __init__(self, data_source):\n \n  self.data_source = data_source\n \n \n \n  def __iter__(self):\n \n  cpu = torch.device('cpu')\n \n  return iter( torch.randperm( len(self.data_source), device = cpu).tolist())\n \n \n \n  def __len__(self):\n \n  return len(self.data_source)\n \n ```"
   },
   {
      "null": 475,
      "x": "Compilation errors on master, Ubuntu 17.10, CUDA 9, GCC 6.4 / GCC 5.4.1",
      "z": "Here is the output:\n \n \n \n ```\n \n $ grep \"_GLIBCXX_USE_C99\" /usr/include/x86_64-linux-gnu/c++/4.8/bits/c++config.h\n \n /* #undef _GLIBCXX_USE_C99 */\n \n #define _GLIBCXX_USE_C99_COMPLEX 1\n \n #define _GLIBCXX_USE_C99_COMPLEX_TR1 1\n \n #define _GLIBCXX_USE_C99_CTYPE_TR1 1\n \n #define _GLIBCXX_USE_C99_FENV_TR1 1\n \n #define _GLIBCXX_USE_C99_INTTYPES_TR1 1\n \n #define _GLIBCXX_USE_C99_INTTYPES_WCHAR_T_TR1 1\n \n /* #undef _GLIBCXX_USE_C99_MATH */\n \n #define _GLIBCXX_USE_C99_MATH_TR1 1\n \n #define _GLIBCXX_USE_C99_STDINT_TR1 1\n \n ```\n \n \n \n ```\n \n $ apt-cache showpkg libstdc++-4.8-dev\n \n Package: libstdc++-4.8-dev\n \n Versions: \n \n 4.8.5-4ubuntu6 (/var/lib/apt/lists/gb.archive.ubuntu.com_ubuntu_dists_artful_universe_binary-amd64_Packages) (/var/lib/dpkg/status)\n \n  Description Language: \n \n  File: /var/lib/apt/lists/gb.archive.ubuntu.com_ubuntu_dists_artful_universe_binary-amd64_Packages\n \n  MD5: a197f2aec835e5fc6f8f76039d8a7c4e\n \n  Description Language: \n \n  File: /var/lib/apt/lists/gb.archive.ubuntu.com_ubuntu_dists_artful_universe_binary-i386_Packages\n \n  MD5: a197f2aec835e5fc6f8f76039d8a7c4e\n \n  Description Language: en\n \n  File: /var/lib/apt/lists/gb.archive.ubuntu.com_ubuntu_dists_artful_universe_i18n_Translation-en\n \n  MD5: a197f2aec835e5fc6f8f76039d8a7c4e\n \n \n \n \n \n Reverse Depends: \n \n  g++-4.8,libstdc++-4.8-dev 4.8.5-4ubuntu6\n \n  libstdc++-4.8-dev:i386,libstdc++-4.8-dev 4.8.5-4ubuntu6\n \n  libstdc++-4.8-dev:i386,libstdc++-4.8-dev 4.8.5-4ubuntu6\n \n  libx32stdc++6-4.8-dbg,libstdc++-4.8-dev 4.8.5-4ubuntu6\n \n  libx32stdc++-4.8-dev,libstdc++-4.8-dev 4.8.5-4ubuntu6\n \n  libstdc++6-4.8-dbg,libstdc++-4.8-dev 4.8.5-4ubuntu6\n \n  libstdc++-4.8-pic,libstdc++-4.8-dev 4.8.5-4ubuntu6\n \n  libboost1.63-dev,libstdc++-4.8-dev\n \n  lib32stdc++6-4.8-dbg,libstdc++-4.8-dev 4.8.5-4ubuntu6\n \n  lib32stdc++-4.8-dev,libstdc++-4.8-dev 4.8.5-4ubuntu6\n \n  libboost1.62-dev,libstdc++-4.8-dev\n \n Dependencies: \n \n 4.8.5-4ubuntu6 - gcc-4.8-base (5 4.8.5-4ubuntu6) libgcc-4.8-dev (5 4.8.5-4ubuntu6) libstdc++6 (2 4.8.5-4ubuntu6) libc6-dev (2 2.13-0ubuntu6) libg++2.8-dev (0 (null)) libg++27-dev (0 (null)) libg++272-dev (3 2.7.2.8-1) libstdc++2.10-dev (3 1:2.95.3-2) libstdc++2.8-dev (0 (null)) libstdc++2.9-dev (0 (null)) libstdc++2.9-glibc2.1-dev (0 (null)) libstdc++3.0-dev (0 (null)) libstdc++-4.8-doc (0 (null)) libstdc++-4.8-dev:i386 (35 4.8.5-4ubuntu6) libstdc++-4.8-dev:i386 (38 4.8.5-4ubuntu6) \n \n Provides: \n \n 4.8.5-4ubuntu6 - libstdc++-dev (= ) \n \n Reverse Provides: \n \n ```\n \n \n \n ---\n \n \n \n Edit: I managed to get it to compile by defining the following in `c++config.h`:\n \n `#define _GLIBCXX_USE_C99 1`\n \n `#define _GLIBCXX_USE_C99_MATH 1`\n \n \n \n Thanks @sdmonov!",
      "y": "Define the following in `c++config.h`:\n \n `#define _GLIBCXX_USE_C99 1`\n \n `#define _GLIBCXX_USE_C99_MATH 1`"
   },
   {
      "null": 476,
      "x": "Unnecessary memcopies emitted by autograd engine",
      "z": "Redefining nn/functional.py linear as (as you recommend in 2):\n \n ```.py\n \n def linear(input, weight, bias=None):\n \n  \"\"\" \n \n  Applies a linear transformation to the incoming data: :math:`y = xA^T + b`. \n \n  \n \n  Shape: \n \n  - Input: :math:`(N, *, in\\_features)` where `*` means any number of \n \n  additional dimensions \n \n  - Weight: :math:`(out\\_features, in\\_features)` \n \n  - Bias: :math:`(out\\_features)` \n \n  - Output: :math:`(N, *, out\\_features)` \n \n  \"\"\"\n \n \n \n  input = input.contiguous()\n \n  sizes = input.size()[:-1]\n \n  input = input.view(-1, input.size(-1))\n \n  if input.dim() == 2 and bias is not None:\n \n  # fused op is marginally faster \n \n  output = torch.addmm(bias, input, weight.t())\n \n  return output.view(*sizes, -1)\n \n \n \n  output = input.matmul(weight.t())\n \n  if bias is not None:\n \n  output += bias\n \n  return output\n \n ```\n \n Was enough to avoid the mem copies in fairseq. Should we just route though to addmm for now until someone tackles 1?",
      "y": "Redefine nn/functional.py linear as :\n \n ```.py\n \n def linear(input, weight, bias=None):\n \n  \"\"\" \n \n  Applies a linear transformation to the incoming data: :math:`y = xA^T + b`. \n \n  \n \n  Shape: \n \n  - Input: :math:`(N, *, in\\_features)` where `*` means any number of \n \n  additional dimensions \n \n  - Weight: :math:`(out\\_features, in\\_features)` \n \n  - Bias: :math:`(out\\_features)` \n \n  - Output: :math:`(N, *, out\\_features)` \n \n  \"\"\"\n \n \n \n  input = input.contiguous()\n \n  sizes = input.size()[:-1]\n \n  input = input.view(-1, input.size(-1))\n \n  if input.dim() == 2 and bias is not None:\n \n  # fused op is marginally faster \n \n  output = torch.addmm(bias, input, weight.t())\n \n  return output.view(*sizes, -1)\n \n \n \n  output = input.matmul(weight.t())\n \n  if bias is not None:\n \n  output += bias\n \n  return output\n \n ```"
   },
   {
      "null": 477,
      "x": "[feature request] support batch diag",
      "z": "how about `torch.diag(batch=True)`? We can have similar extensions to other operators...",
      "y": "`torch.diag(batch=True)` and similar to other operators."
   },
   {
      "null": 478,
      "x": "[feature request] torch.where to support Tensors and python scalars",
      "z": "Tensors will be gone within a week or two. Won't fix.",
      "y": "Tensors will be gone within a week or two. Won't fix."
   },
   {
      "null": 479,
      "x": "discuss.pytorch.org is down",
      "z": "it'is not work.\n \n ![1](https://user-images.githubusercontent.com/5284540/69575091-a85a5180-0fda-11ea-9e7c-a4ec46201dd7.png)",
      "y": "Ran into unexpected maintenance."
   },
   {
      "null": 480,
      "x": "RuntimeError: cuda runtime error (2) : out of memory when using loss function",
      "z": "Because as long as output Variables are in scope, the underlying graphs can't be freed. You're meant to let go the loss at the end of iteration.",
      "y": "As long as output Variables are in scope, the underlying graphs can't be freed."
   },
   {
      "null": 481,
      "x": "Error message when wrong call to forward of RNN (nn.GRU and also nn.LSTM)",
      "z": "After a few attempts, it turns out that, at least in my case, the error occurs when the dimension 2 of the input is not correct.\n \n ```python\n \n import torch\n \n from torch import nn\n \n from torch.autograd import Variable\n \n \n \n model = nn.LSTM(5, 1)\n \n \n \n input = Variable(torch.rand(1, 1, 3)) # input = Variable(torch.rand(1, 1, 5)) to get it work :)\n \n h0 = Variable(torch.rand(1, 1, 1))\n \n c0 = Variable(torch.rand(1, 1, 1))\n \n \n \n print(model(input, (h0, c0)))\n \n \n \n '''\n \n ---------------------------------------------------------------------------\n \n NameError Traceback (most recent call last)\n \n <ipython-input-1-68e4878dc5e1> in <module>()\n \n  9 c0 = Variable(torch.rand(1, 1, 1))\n \n  10 \n \n ---> 11 print(model(input, (h0, c0)))\n \n \n \n /usr/local/lib/python3.5/dist-packages/torch/nn/modules/module.py in __call__(self, *input, **kwargs)\n \n  355 result = self._slow_forward(*input, **kwargs)\n \n  356 else:\n \n --> 357 result = self.forward(*input, **kwargs)\n \n  358 for hook in self._forward_hooks.values():\n \n  359 hook_result = hook(self, input, result)\n \n \n \n /usr/local/lib/python3.5/dist-packages/torch/nn/modules/rnn.py in forward(self, input, hx)\n \n  188 flat_weight = None\n \n  189 \n \n --> 190 self.check_forward_args(input, hx, batch_sizes)\n \n  191 func = self._backend.RNN(\n \n  192 self.mode,\n \n \n \n /usr/local/lib/python3.5/dist-packages/torch/nn/modules/rnn.py in check_forward_args(self, input, hidden, batch_sizes)\n \n  139 raise RuntimeError(\n \n  140 'input.size(-1) must be equal to input_size. Expected {}, got {}'.format(\n \n --> 141 fn.input_size, input.size(-1)))\n \n  142 \n \n  143 if is_input_packed:\n \n \n \n NameError: name 'fn' is not defined\n \n '''\n \n ```",
      "y": "The error occurs when the dimension 2 of the input is not correct."
   },
   {
      "null": 482,
      "x": "CUDNN_STATUS_EXECUTION_FAILED with RNN on GPU",
      "z": "When I replace:\n \n \n \n ```\n \n \n \n  def init_hidden(self):\n \n  document_rnn_init_h = nn.Parameter(nn.init.xavier_uniform(torch.Tensor(self.nb_layers, self.batch_size, self.embedding_size).type(torch.FloatTensor)), requires_grad=True)\n \n  if self.mode == 'GRU':\n \n  return document_rnn_init_h\n \n  elif self.mode == 'LSTM':\n \n  document_rnn_init_c = nn.Parameter(nn.init.xavier_uniform(torch.Tensor(self.nb_layers, self.batch_size, self.embedding_size).type(torch.FloatTensor)), requires_grad=True)\n \n  return (document_rnn_init_h, document_rnn_init_c)\n \n ```\n \n \n \n with\n \n \n \n ```\n \n  def init_hidden(self):\n \n  document_rnn_init_h = nn.Parameter(nn.init.xavier_uniform(torch.Tensor(self.nb_layers, self.batch_size, self.embedding_size).type(torch.FloatTensor).cuda()), requires_grad=True)\n \n  if self.mode == 'GRU':\n \n  return document_rnn_init_h\n \n  elif self.mode == 'LSTM':\n \n  document_rnn_init_c = nn.Parameter(nn.init.xavier_uniform(torch.Tensor(self.nb_layers, self.batch_size, self.embedding_size).type(torch.FloatTensor).cuda()), requires_grad=True)\n \n  return (document_rnn_init_h, document_rnn_init_c)\n \n ```\n \n \n \n (that is, ensuring that the hidden parameters are put onto CUDA appropriately), it works. So it sounds like we're missing (1) some checks that the tensors we're running are indeed CUDA tensors, and maybe (2) some implicit conversions which no longer exist.",
      "y": "Missing (1) some checks that the tensors we're running are indeed CUDA tensors, and maybe (2) some implicit conversions which no longer exist."
   },
   {
      "null": 483,
      "x": "ONNX export issue for model with multiple outputs",
      "z": "Can you share broader chunk of code? Or at least 'print' the output to see its type. What is the output type of the model? Afaik, only Variables or lists or tuples of Variables (maybe nested) are supported.\n \n \n \n @houseroad",
      "y": "Only Variables or lists or tuples of Variables (maybe nested) are supported."
   },
   {
      "null": 484,
      "x": "Documentation code for sampling from categorical distribution throws an error on master",
      "z": "I think this has been fixed on `master`.",
      "y": "Appears to be a bug in `.view()` ."
   },
   {
      "null": 485,
      "x": "Compilation errors, when compiling pytorch on Ubuntu 17.10 or newer with GCC 5",
      "z": "> I've build gcc7 and add it into `$PATH`. But it still get this error.\n \n > \n \n > ```shell\n \n > The C++ compiler does not support required functions. This is very likely\n \n > due to a known bug in GCC 5 (and maybe other versions) on Ubuntu 17.10 and\n \n > newer. For more information, see:\n \n > https://github.com/pytorch/pytorch/issues/522\n \n > ```\n \n > \n \n > The log is as follows.\n \n > \n \n > ```shell\n \n > (base) xxx@TENCENT64:~/gpt_model/pytorch/pytorch> python setup.py install\n \n > Building wheel torch-1.7.0a0+7a64b0c\n \n > -- Building version 1.7.0a0+7a64b0c\n \n > cmake -GNinja -DBUILD_PYTHON=True -DBUILD_TEST=True -DCMAKE_BUILD_TYPE=Release -DCMAKE_INSTALL_PREFIX=/data1/mm64/xxx/gpt_model/pytorch/pytorch/torch -DCMAKE_PREFIX_PATH=/home/xxx/anaconda3/lib/python3.7/site-packages -DJAVA_HOME=/home/mmdev/devpkg/java/jdk1.7.0_04 -DNUMPY_INCLUDE_DIR=/home/xxx/anaconda3/lib/python3.7/site-packages/numpy/core/include -DPYTHON_EXECUTABLE=/home/xxx/anaconda3/bin/python -DPYTHON_INCLUDE_DIR=/home/xxx/anaconda3/include/python3.7m -DPYTHON_LIBRARY=/home/xxx/anaconda3/lib/libpython3.7m.so.1.0 -DTORCH_BUILD_VERSION=1.7.0a0+7a64b0c -DUSE_NUMPY=True /data1/mm64/xxx/gpt_model/pytorch/pytorch\n \n > -- Performing Test SUPPORT_GLIBCXX_USE_C99\n \n > -- Performing Test SUPPORT_GLIBCXX_USE_C99 - Failed\n \n > CMake Error at cmake/MiscCheck.cmake:81 (message):\n \n > The C++ compiler does not support required functions. This is very likely\n \n > due to a known bug in GCC 5 (and maybe other versions) on Ubuntu 17.10 and\n \n > newer. For more information, see:\n \n > https://github.com/pytorch/pytorch/issues/5229\n \n > Call Stack (most recent call first):\n \n > CMakeLists.txt:475 (include)\n \n > \n \n > \n \n > -- Configuring incomplete, errors occurred!\n \n > See also \"/data1/mm64/xxx/gpt_model/pytorch/pytorch/build/CMakeFiles/CMakeOutput.log\".\n \n > See also \"/data1/mm64/xxx/gpt_model/pytorch/pytorch/build/CMakeFiles/CMakeError.log\".\n \n > Traceback (most recent call last):\n \n > File \"setup.py\", line 748, in <module>\n \n > build_deps()\n \n > File \"setup.py\", line 332, in build_deps\n \n > cmake=cmake)\n \n > File \"/data1/mm64/xxx/gpt_model/pytorch/pytorch/tools/build_pytorch_libs.py\", line 59, in build_caffe2\n \n > rerun_cmake)\n \n > File \"/data1/mm64/xxx/gpt_model/pytorch/pytorch/tools/setup_helpers/cmake.py\", line 329, in generate\n \n > self.run(args, env=my_env)\n \n > File \"/data1/mm64/xxx/gpt_model/pytorch/pytorch/tools/setup_helpers/cmake.py\", line 141, in run\n \n > check_call(command, cwd=self.build_dir, env=env)\n \n > File \"/home/xxx/anaconda3/lib/python3.7/subprocess.py\", line 363, in check_call\n \n > raise CalledProcessError(retcode, cmd)\n \n > subprocess.CalledProcessError: Command '['cmake', '-GNinja', '-DBUILD_PYTHON=True', '-DBUILD_TEST=True', '-DCMAKE_BUILD_TYPE=Release', '-DCMAKE_INSTALL_PREFIX=/data1/mm64/xxx/gpt_model/pytorch/pytorch/torch', '-DCMAKE_PREFIX_PATH=/home/xxx/anaconda3/lib/python3.7/site-packages', '-DJAVA_HOME=/home/mmdev/devpkg/java/jdk1.7.0_04', '-DNUMPY_INCLUDE_DIR=/home/xxx/anaconda3/lib/python3.7/site-packages/numpy/core/include', '-DPYTHON_EXECUTABLE=/home/xxx/anaconda3/bin/python', '-DPYTHON_INCLUDE_DIR=/home/xxx/anaconda3/include/python3.7m', '-DPYTHON_LIBRARY=/home/xxx/anaconda3/lib/libpython3.7m.so.1.0', '-DTORCH_BUILD_VERSION=1.7.0a0+7a64b0c', '-DUSE_NUMPY=True', '/data1/mm64/xxx/gpt_model/pytorch/pytorch']' returned non-zero exit status 1.\n \n > (base) xxx@TENCENT64:~/gpt_model/pytorch/pytorch> gcc --version \n \n > gcc (GCC) 7.5.0\n \n > Copyright (C) 2017 Free Software Foundation, Inc.\n \n > This is free software; see the source for copying conditions. There is NO\n \n > warranty; not even for MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE.\n \n > ```\n \n \n \n Solved this problem by remove the `build` directory and add `gcc7` into `$PATH`",
      "y": "Remove the `build` directory and add `gcc7` into `$PATH` ."
   },
   {
      "null": 486,
      "x": "\"Bus error\" on /dev/shm OOM; hard/impossible to fix",
      "z": "[We have always been doing the `write` check](https://github.com/pytorch/pytorch/blob/master/aten/src/TH/THAllocator.c#L324-L333). The problem is that touching only the first element will not catch all errors, since the OS is allocating the pages lazily and will send the bus error only once it runs out of them.",
      "y": "Touching only the first element will not catch all errors, since the OS is allocating the pages lazily and will send the bus error only once it runs out of them."
   },
   {
      "null": 487,
      "x": "Pytorch inconsistent behavior for boundary checks",
      "z": "Like @neerajprad mentioned, I think it would be useful to have an optional debug flag which, when set, would cause pytorch to check for boundary conditions for all distributions instead of having checks for individual distributions. Tensorflow has a similar flag, [validate_args](https://www.tensorflow.org/versions/r1.0/api_docs/python/tf/contrib/distributions/Bernoulli#validate_args), which is an optional argument for all distributions. Also, it seems like torch already has some of these checks in place for some distributions(may not be all). I would be happy to contribute the feature and the test cases, if this proposal sounds reasonable. Any suggestions on how to proceed?",
      "y": "It would be nice to have an optional debug flag which, when set, would cause pytorch to check for boundary conditions for all distributions instead of having checks for individual distributions. Tensorflow has a similar flag, [validate_args](https://www.tensorflow.org/versions/r1.0/api_docs/python/tf/contrib/distributions/Bernoulli#validate_args), which is an optional argument for all distributions. Also, it seems like torch already has some of these checks in place for some distributions(may not be all)."
   },
   {
      "null": 488,
      "x": "Training and learning rate finder utilities",
      "z": "the examples make a good start, and there's a lot more that the examples dont cover.\n \n \n \n Promising trainer abstractions in the pytorch community are:\n \n \n \n - ignite: http://github.com/pytorch/ignite\n \n - lightning: https://github.com/williamFalcon/pytorch-lightning\n \n - skorch: https://github.com/skorch-dev/skorch\n \n \n \n There are a few nice ones, but didn't get as much traction.",
      "y": "Promising trainer abstractions in the pytorch community are:\n \n \n \n - ignite: http://github.com/pytorch/ignite\n \n - lightning: https://github.com/williamFalcon/pytorch-lightning\n \n - skorch: https://github.com/skorch-dev/skorch"
   },
   {
      "null": 489,
      "x": "Crash using PyTorch built with TBB support and multiple DataLoader instance with pin_memory=True",
      "z": "Yes, we support TBB. @ilia-cher should be able to help.",
      "y": "Relax restrictions on set_num_threads."
   },
   {
      "null": 490,
      "x": "Remove use of Variable wrapper in test files?",
      "z": "go for it!",
      "y": "Migrate away from using legacy Variable constructor in test_nn.py."
   },
   {
      "null": 491,
      "x": "cdist gradient computation is broken",
      "z": "This happens when `x1` has a size `1xn`. Then the result and the incoming gradient have the size `1xm` (where m is the number of vectors in x2), `grad.t()` is reported as contiguous, but it's last stride is still `m`, and computing offsets based on the last stride here becomes incorrect https://github.com/pytorch/pytorch/blob/fbf991d06279ea6828ec133b186dbef3bc16522b/aten/src/ATen/native/cpu/DistanceOpsKernel.cpp#L392 (`grad_k += gs`).",
      "y": "This happens when `x1` has a size `1xn`. Then the result and the incoming gradient have the size `1xm` (where m is the number of vectors in x2), `grad.t()` is reported as contiguous, but it's last stride is still `m`, and computing offsets based on the last stride here becomes incorrect."
   },
   {
      "null": 492,
      "x": "Significant GPU Memory leak",
      "z": "Thank you for the answer.\n \n \n \n In case anyone is wondering, here is how to set process specific env variables:\n \n ```python\n \n import torch.multiprocessing as _mp\n \n import torch\n \n import os\n \n \n \n mp = _mp.get_context('spawn')\n \n \n \n \n \n class Process(mp.Process):\n \n  def __init__(self):\n \n  super().__init__()\n \n  print(\"Init Process\")\n \n  return\n \n \n \n  def run(self):\n \n  print(\"Hello World!\")\n \n  os.environ['CUDA_VISIBLE_DEVICES'] = '1'\n \n  print(torch.cuda.device_count())\n \n  print(torch.cuda.current_device())\n \n \n \n if __name__ == \"__main__\":\n \n  num_processes = 2\n \n  os.environ['CUDA_VISIBLE_DEVICES'] = '0,1,2,3'\n \n  processes = [Process() for i in range(num_processes)]\n \n  [p.start() for p in processes]\n \n  print(\"main: \" + os.environ['CUDA_VISIBLE_DEVICES'])\n \n  [p.join() for p in processes]\n \n ```\n \n It is important to set it in the run method of the process, as the `__init__` method is still called in the main process, therefore setting the env vars of the main process when set there.",
      "y": "Set it in the run method of the process, as the `__init__` method is still called in the main process, therefore setting the env vars of the main process when set there."
   },
   {
      "null": 493,
      "x": "multi_head_attention_forward produces NaN",
      "z": "It turns out there was an error in my padding mask, as I marked the padded locations with 0s and the non-padded locations with 1s. This resulted in totally masked rows, which caused the softmax to output NaNs as explained in #25110.",
      "y": "Error in padding mask, as padded locations were marked with 0s and the non-padded locations with 1s. This resulted in totally masked rows, which caused the softmax to output NaNs."
   },
   {
      "null": 494,
      "x": "lstm module with None in input on CUDA segfaults.",
      "z": "Oh, sorry, I read the repro completely wrong, there is a PackedSequence and it's being created with a CUDA tensor. Thank you for fact checking me, @ailzhang! If we're confident that the above example doesn't segfault without PackedSequence, then we should close this.",
      "y": "Example doesn't segfault without PackedSequence ."
   },
   {
      "null": 495,
      "x": "Exclude generated source docs from Google",
      "z": "Conversely, though I agree that they shouldn't appear in search engines, I do find reading the source useful, so it would be nice to have [source] links in the docs.",
      "y": "Have [source] links in the docs."
   },
   {
      "null": 496,
      "x": "[ONNX] export with dynamic_axes should generate Shape node for intermediate tensor .shape calls even without jit.script",
      "z": "This is not because of size. The problem is with arange op which is exported as a constant. This is fixed in this PR: https://github.com/pytorch/pytorch/issues/20075",
      "y": "Set traceable=true in arange's arg parser."
   },
   {
      "null": 497,
      "x": "TorchScript fails to compile methods with misindented comments",
      "z": "Duplicate of https://github.com/pytorch/pytorch/issues/25043",
      "y": "Allow for source code comments at any level of indentation."
   },
   {
      "null": 498,
      "x": "Allreduce for sparse tensors is not working",
      "z": "Could you take a look and repro?",
      "y": "Put sparse all reduce results to input tensors."
   },
   {
      "null": 499,
      "x": "On pytorch mobile (android), module forward returns org.pytorch.IValue type. But on my detection module, supposes to return a tuple IValue.",
      "z": "Hello @Sakulaki ,\n \n \n \n IValue works as a 'tagged union' for all supported types, extracting them calling appropriate for your type `IValue.to${TYPE}` method.\n \n So for the case of tuple it should be used smth like this:\n \n ```\n \n  final IValue output = mModule.forward(IValue.from(mInputTensor));\n \n  IValue[] outputTuple = output.toTuple();\n \n  for (int i = 0; i < outputTuple.length; i++) {\n \n  IValue tupleElement = outputTuple[i];\n \n  }\n \n ```\n \n Or did not I get your problem right?",
      "y": "IValue works as a 'tagged union' for all supported types, extracting them calling appropriate for your type `IValue.to${TYPE}` method."
   },
   {
      "null": 500,
      "x": "[android] initHybrid missing/broken in pytorch 1.4.0 nightly",
      "z": "The fix was merged into master as https://github.com/pytorch/pytorch/commit/3a19980b78b6638235edef367784ecc3dc37e364\n \n I have republished android nightlies, the error should not happen if you update to [the nightlies with version >= 78](https://oss.sonatype.org/service/local/repositories/snapshots/content/org/pytorch/pytorch_android/1.4.0-SNAPSHOT/pytorch_android-1.4.0-20191220.225919-78.aar) (gradle key `--refresh-dependencies`)",
      "y": "The error should not happen if you update to the nightlies with version >= 78 (gradle key `--refresh-dependencies`)"
   },
   {
      "null": 501,
      "x": "Matrix multiplication returns wrong result when middle dimension is 0",
      "z": "Looks like cublas bug. With the repro script and .ltrace.conf from this gist (put .ltrace.conf in the root directory) \n \n https://gist.github.com/ngimel/780f09d431906a522c4df2da3cd16fbd\n \n I'm getting reasonable looking arguments for cublas call, yet output seems not to be overwritten:\n \n ```\n \n (pytorch) ngimel@xxxxxxx:~/playground$ ltrace -f -e 'cublasSgemmStridedBatched' python bmm.py\n \n [pid 23443] --- Called exec() ---\n \n [pid 23444] --- Called exec() ---\n \n [pid 23444] +++ exited (status 0) +++\n \n [pid 23443] --- SIGCHLD (Child exited) ---\n \n [pid 23443] +++ exited (status 0) +++\n \n [pid 23439] --- SIGCHLD (Child exited) ---\n \n [pid 23439] libtorch_cuda.so->cublasSgemmStridedBatched(0x562bbf906850, 0, 0, 43200, 1, 0, 1.000000, 0, 43200, 43200, 0, 1, 1, 0.000000, 0x7ff76a400000, 43200, 43200, 10) = 0\n \n tensor([[[1., 1., 1., ..., 1., 1., 1.]],\n \n \n \n  [[1., 1., 1., ..., 1., 1., 1.]],\n \n \n \n  [[1., 1., 1., ..., 1., 1., 1.]],\n \n \n \n  ...,\n \n \n \n  [[1., 1., 1., ..., 1., 1., 1.]],\n \n \n \n  [[1., 1., 1., ..., 1., 1., 1.]],\n \n \n \n  [[1., 1., 1., ..., 1., 1., 1.]]], device='cuda:0')\n \n ```\n \n We could implement a workaround zeroing the output for this case, actual cublas bugfixes take a long time. \n \n cc @ptrblck",
      "y": "We could implement a workaround zeroing the output for this case, actual cublas bugfixes take a long time."
   },
   {
      "null": 502,
      "x": "Should we expose CircleCI CUDA10 tests as XImportant and run on PRs?",
      "z": "Let's do it!",
      "y": "Switch important CI from CUDA 9 to 10.1."
   },
   {
      "null": 503,
      "x": "Power8/P100 node pytorch compilation from source with cuda 10.1: bus error - out of memory",
      "z": "If I may... Haven't tried, but looks like setting `MAX_JOBS` in the env will do it:\n \n \n \n https://github.com/pytorch/pytorch/blob/master/setup.py#L11",
      "y": "Set `MAX_JOBS` in the env."
   },
   {
      "null": 504,
      "x": "Missing edge case information in BCELoss documentation",
      "z": "Your clamped log update makes sense. You can take over the issue @kurtamohler.",
      "y": "It is using a safe log function that replaces `log(0)` specifically with `log(EPS)` where EPS = 1e-12."
   },
   {
      "null": 505,
      "x": "torch.gather in pytorch.onnx and onnxruntime",
      "z": "> orch from master branch and it's now 1.5 and ONNX 1.6\n \n \n \n I tried this environment: torch1.4 + onnx 1.6. But there are still problems. This time the problem is on the clip OP. I'm not sure if there is a problem with the gather OP. Can you help me to transform the Class SpatialTransformer to the onnx file and verify on onnxruntime\u00ef\u00bc\u0178\n \n Here is my Class SpatialTransformer:\n \n ```python\n \n class SpatialTransformer(nn.Module):\n \n  def __init__(self):\n \n  super(SpatialTransformer, self).__init__()\n \n \n \n  def forward(self, right_input, disparity_samples):\n \n  B, C, H, W = right_input.shape\n \n  device = right_input.get_device()\n \n  left_y_coordinate = torch.arange(0.0, W).repeat(H)\n \n \n \n  left_y_coordinate = left_y_coordinate.view(H, W)\n \n \n \n  left_y_coordinate = left_y_coordinate.unsqueeze(0)\n \n  left_y_coordinate = left_y_coordinate.unsqueeze(0)\n \n  \n \n \n \n  right_y_coordinate = left_y_coordinate + disparity_samples\n \n \n \n  right_y_coordinate_a = torch.floor(right_y_coordinate)\n \n  right_y_coordinate_b = right_y_coordinate_a + 1\n \n \n \n  wa = right_y_coordinate_b - right_y_coordinate\n \n  wb = right_y_coordinate - right_y_coordinate_a\n \n \n \n  wa = wa.repeat(1, C, 1, 1)\n \n  wb = wb.repeat(1, C, 1, 1)\n \n \n \n  right_y_coordinate_a = right_y_coordinate_a.repeat(1, C, 1, 1).long()\n \n  right_y_coordinate_b = right_y_coordinate_b.repeat(1, C, 1, 1).long()\n \n \n \n \n \n  right_y_coordinate_a = torch.clamp(right_y_coordinate_a, min=0, max= W - 1)\n \n  right_y_coordinate_b = torch.clamp(right_y_coordinate_b, min=0, max= W - 1)\n \n \n \n  warped_right_feature_map_a = torch.ones_like(right_input)\n \n  warped_right_feature_map_b = torch.ones_like(right_input)\n \n \n \n \n \n  warped_right_feature_map_a = right_input.gather(dim=3, index=right_y_coordinate_a.long())\n \n  warped_right_feature_map_b = torch.gather(right_input, dim=3, index=right_y_coordinate_b.long()) \n \n \n \n  warped_right_feature_map = wa * warped_right_feature_map_a + wb * warped_right_feature_map_b\n \n \n \n  right_y_coordinate_1 = right_y_coordinate.repeat(1, C, 1, 1)\n \n  \n \n  warped_right_feature_map = (1.0 - ((right_y_coordinate_1 < 0).float() +\n \n  (right_y_coordinate_1 > torch.tensor([W - 1], dtype=torch.float32))).float()) * \\\n \n  (warped_right_feature_map) + torch.zeros_like(warped_right_feature_map)\n \n \n \n  print(\"new stn\")\n \n  return warped_right_feature_map \n \n ```",
      "y": "With the \"verbose = True\", torch.gather(right_input,dim=3, index=right_y_coordinate_a.long()) ."
   },
   {
      "null": 506,
      "x": "BrokenPipeError on Windows when setting the num_workers as 4 in DataLoader",
      "z": "Re-opening while the discussion goes on.",
      "y": "Increase the size of the page file."
   },
   {
      "null": 507,
      "x": "torch.as_tensor returns different value for 'is_leaf' for different dtypes",
      "z": "isn't this expected? it is a noop for all but the 3rd usage.",
      "y": "It is a noop for all but the 3rd usage."
   },
   {
      "null": 508,
      "x": "torch.no_grad() context manager seems to leak memory",
      "z": "The decorator `torch.no_grad` effectively creates a new function:\n \n ```python\n \n def decorate_no_grad(*args, **kwargs):\n \n  with torch.no_grad():\n \n  return apply_model(*args, **kwargs)\n \n ``` \n \n \n \n Now since `apply_model` is a generator function, it doesn't actually perform any computation when called. Instead it immediately returns a python generator object. i.e. the `torch.no_grad` context ends before any computation is done.\n \n \n \n It may be possible to detect that `apply_model` is a generator function and instead produce something like:\n \n ```python\n \n def generator_no_grad(*args, **kwargs):\n \n gen = apply_model(*args, **kwargs)\n \n while True:\n \n  with torch.no_grad():\n \n  try:\n \n  yield next(gen)\n \n  except StopIteration:\n \n  break\n \n ```",
      "y": "Since `apply_model` is a generator function, it doesn't actually perform any computation when called. Instead it immediately returns a python generator object. i.e. the `torch.no_grad` context ends before any computation is done."
   },
   {
      "null": 509,
      "x": "cuDNN convolution memory usage",
      "z": "For your testing purposes the workaround is to set `torch.backends.cudnn.deterministic=True` (and `benchmark=False`), it will pick implicit gemm algorithm that is 1) supported for all cases 2) uses relatively little memory. It is also a nice fallback for your second question, as far as I understand there should not be a situation where implicit gemm algorithm is not applicable. I thought pytorch should fall back to it when it fails to allocate requested workspace but looks like I'm wrong.",
      "y": "Set `torch.backends.cudnn.deterministic=True` (and `benchmark=False`)."
   },
   {
      "null": 510,
      "x": "PyTorch nightly fails on Windows",
      "z": "Duplicate of https://github.com/pytorch/pytorch/issues/31370.",
      "y": "Make fully_qualified_type_name_impl() compatible with VS2017 15.9"
   },
   {
      "null": 511,
      "x": "RuntimeError: Error in dlopen or dlsym: libcaffe2_nvrtc.so: cannot open shared object file: No such file or directory",
      "z": "I found a solution: include the lib path of anaconda into LD_LIBRARY_PATH before running pycharm.\n \n \n \n **For my own case:** run the following code line in the command line.\n \n \n \n export LD_LIBRARY_PATH=/data/xiaoshua/miniconda3/lib/python3.7/site-packages/torch/lib:$LD_LIBRARY_PATH\n \n \n \n **For your case:** change the path of \"/data/xiaoshua/miniconda3/\" to your own directory. Then, run the above export code line.",
      "y": "Include the lib path of anaconda into LD_LIBRARY_PATH before running pycharm."
   },
   {
      "null": 512,
      "x": "fused multiply add returns wrong result for bool tensor",
      "z": "The actual and expected result are the same? I think the expected result should be Falses.",
      "y": "Enabled 'add_cuda' for bool and fixed alpha scalar bug."
   },
   {
      "null": 513,
      "x": "RuntimeError when using multiple DistributedDataParallel model",
      "z": "Is there any way to see which parameters on earth are unused? It is useless to simply give this RuntimeError or set find_unused_parameters to True.",
      "y": "train your model on a single node without the DDP wrapper. after loss.backward() and before optimizer.step() call add the below lines\n \n ```for name, param in model.named_parameters():\n  if param.grad is None:\n  print(name)```\n This will print any param which did not get used in loss calculation, their grad will be None."
   },
   {
      "null": 514,
      "x": "2 tensors for the output...Abou the libtorch | torch.jit.trace",
      "z": "Since you are returning two Tensors, the result value is actually going to be a Tuple, not a Tensor, so toTensor will fail. You can call `toTuple` to get the Tuple object and then extract the tensor:\n \n ```\n \n auto output = module->forward(inputs);\n \n auto t = output.toTuple()->elements()[0].toTensor(); // 1st output;\n \n ```",
      "y": "Call `toTuple` to get the Tuple object and then extract the tensor."
   },
   {
      "null": 515,
      "x": "torch.nn.functional.gumbel_softmax yields NaNs",
      "z": "I ran into the same issue after installing the pip wheel https://download.pytorch.org/whl/cu100/torch-1.1.0-cp36-cp36m-linux_x86_64.whl\n \n Forking and making the changes proposed by @vlievin worked around the issue.",
      "y": "gumbel_softmax stability issue."
   },
   {
      "null": 516,
      "x": "Reduction operations magical auto-casting non documented",
      "z": "We have (since 0.4 or 1.0) type promotion for reduction, so the result of summing any integer type (including `uint8`) returns a `int64` by default.\n \n \n \n This is the same behavior as in numpy.\n \n \n \n If you want to return the same type, you can specify a `dtype` in the function.\n \n \n \n ---\n \n tl;dr: the docs might be outdated, but the behavior is intended",
      "y": "Specify a `dtype` in the function."
   },
   {
      "null": 517,
      "x": "Pytorch 1.1 with distributed data parallel",
      "z": "Yes, `export OMP_NUM_THREADS=1` works for me.",
      "y": "`export OMP_NUM_THREADS=1` ."
   },
   {
      "null": 518,
      "x": "Conv3d fails with bigger batch size",
      "z": "We can work around this in pytorch, by splitting a single convolution into a few calls, each with less than 2**31 elements. One of our team members will work on this.",
      "y": "Split a single convolution into a few calls, each with less than 2**31 elements."
   },
   {
      "null": 519,
      "x": "RuntimeError: CUDA error: invalid configuration argument",
      "z": "Same issue with the launch config, I should have done a better job with my last PR :man_facepalming:\n \n \n \n Although I added striding on channel dimension, I forgot to limit the launch config on GPU *grid* properties (https://docs.nvidia.com/cuda/cuda-runtime-api/structcudaDeviceProp.html#structcudaDeviceProp_192d195493a9d36b2d827aaf3ffd89f1e)\n \n \n \n Will tag my PR shortly.",
      "y": "Add striding on channel dimension and limit the launch config on GPU *grid* properties."
   },
   {
      "null": 520,
      "x": "nn.parallel.DistributedDataParallel nccl backend multiple-gpu multiple-node deadlock",
      "z": "There is a known problem with NCCL 2.4.2 where it locks up. You can try exporting the environment variable `NCCL_LL_THRESHOLD=0` to see if that fixes it. Otherwise, please try a newer version of PyTorch (e.g. the nightlies) that uses a newer version of NCCL where that particular bug is fixed.\n \n \n \n You can also try running with the `gloo` backend to double check there is nothing wrong with your code.",
      "y": "Export the environment variable `NCCL_LL_THRESHOLD=0`"
   },
   {
      "null": 521,
      "x": "DataLoader runs too many threads",
      "z": "First of all, I want to apologize for an inaccuracy in my previous reply. The threads (apart from the few `Queue` putter threads) could come from either the sampler or the possible copy in `pin_memory`. \n \n \n \n Both the sampler and the copy, if they actually occur, are necessary. So there is no such thing as slowing data processing down. Sampler initialization is one time at construction time, and the copy is after fetching each batch. They are not blocking data processing because they are part of the process of fetching data.",
      "y": "Both the sampler and the copy, if they actually occur, are necessary. So there is no such thing as slowing data processing down. Sampler initialization is one time at construction time, and the copy is after fetching each batch. They are not blocking data processing because they are part of the process of fetching data."
   },
   {
      "null": 522,
      "x": "SyncBatchNorm test mode",
      "z": "In triage, we decided that this relaxation seemed like a reasonable to do.\n \n \n \n (Unrelated to this issue: @fmassa was wondering if there wasn't a reason why BatchNorm didn't just take a sync argument. @mrshenli will investigate.)",
      "y": "If it is just for local test, you can initialize the process group using `world_size=1`."
   },
   {
      "null": 523,
      "x": "RuntimeError: expected backend CPU and dtype Double but got backend CPU and dtype Float - in torch normalize function",
      "z": "This has been fixed in the master version of torchvision",
      "y": "Fixed in the master version of torchvision."
   },
   {
      "null": 524,
      "x": "Batched Triu And Tril Incorrect for Some Inputs",
      "z": "I'm able to reproduce the issue on CPU. Working on fixing it.",
      "y": "Fix behavior of `torch.triu` / `torch.tril` on certain unsqueezed tensors that lead to uninitialized values on CPU."
   },
   {
      "null": 525,
      "x": "Drop 'add extra samples to make it evenly divisible' constraint in DistributedSampler",
      "z": "I tried doing what you proposed in `maskrcnn-benchmark`, but it used to give deadlocks (at least with an older version of PyTorch and DDP), precisely because some GPUs would compute a different number of forward operations.\n \n \n \n This might have been fixed since then, but I believe what we need to have is a set of evaluation metrics which work on distributed settings (like the one I have in `torchvision/references/detection` for COCO).",
      "y": "Have a set of evaluation metrics which work on distributed settings (like the one I have in `torchvision/references/detection` for COCO)."
   },
   {
      "null": 526,
      "x": "New VSX support breaks compilation if using g++ v7 (ppc64le)",
      "z": "for example. I believe for vec_signed it should be sufficient defining them this way:\n \n ```\n \n #if !defined(vec_signed)\n \n \n \n  vint32 vec_signed(const vfloat32& vec_in) {\n \n  vint32 vec_out;\n \n  __asm__(\"xvcvspsxws %x0,%x1\" : \"=wa\"(vec_out) : \"wf\"(vec_in));\n \n  return vec_out;\n \n }\n \n \n \n  vint64 vec_signed(const vfloat64& vec_in) {\n \n  vint64 vec_out;\n \n  __asm__(\"xvcvdpsxds %x0,%x1\" : \"=wa\"(vec_out) : \"wd\"(vec_in));\n \n  return vec_out;\n \n }\n \n #endif\n \n ```\n \n ```\n \n wa - any vsx register\n \n wf - vsx float\n \n wd - vsx double\n \n %x - vsx modifier\n \n mostly asm operations follow this:\n \n \n \n  asm_op out_reg, input_reg\n \n __asm__( \"asm_op %x0, %x1\" : \"=wa\"(output): \"wa\"(input));\n \n ```\n \n [godbolt example for vec_signed](https://godbolt.org/z/n4MjbP)",
      "y": "Inline in `vsx_helpers.h` is simply missing the `return vec_out;`"
   },
   {
      "null": 527,
      "x": "CUDNN_STATUS_EXECUTION_FAILED when using AMP",
      "z": "I ran your code for 7h on an RTX2080Ti using the PyTorch `1.7.1` conda binary with CUDA10.2 and unfortunately cannot reproduce this issue.\n \n Have you had a chance to run it on bare metal without mps?",
      "y": "Faulty kernel might be in cudnn7.6.5."
   },
   {
      "null": 528,
      "x": "distributions.Independent does not correctly update distribution's support",
      "z": "Actually I'm just about to push a fix, as part of an unrelated effort...",
      "y": "Fix a number of inconsistencies in torch.distributions.constraints as used for parameters and supports of probability distributions.\n \n 1. Add a `constraints.independent` and replaces `real_vector` with `independent(real, 1)`. (this pattern has long been used in Pyro)\n 2. Add an `.event_dim` attribute to all constraints.\n 3. Test that `constraint.check(data)` has the correct shape. (Previously the shapes were incorrect).\n 4. Add machinery to set `static .is_discrete` and `.event_dim` for `constraints.dependent`.\n 5. Fix constraints for a number of distributions."
   },
   {
      "null": 529,
      "x": "torch.nn.parallel.scatter_gather.gather cannot handle namedtuple as output",
      "z": "cc @rohan-varma Since this seems like an enhancement on top of https://github.com/pytorch/pytorch/pull/44220.",
      "y": "Fix torch.nn.parallel.scatter_gather.gather to handle NamedTuples and handle moving output to CPU."
   },
   {
      "null": 530,
      "x": "Make it easier to run mypy locally",
      "z": "> @rgommers Good point, sorry for my unclear wording; what I meant to say was that that wiki page doesn't have any instructions for editor integration like the Flake8 page does. Should I add VS Code instructions to that page, or to the [Lint as you type](https://github.com/pytorch/pytorch/wiki/Lint-as-you-type) page, or neither?\n \n \n \n I don't have much of a preference here. There's so many editors that I typically don't add editor-specific info to development guides, but if you think it's useful I'd say please go ahead and add them I'd say. The type annotation page looks like the more appropriate page.\n \n \n \n > @rgommers Those timings are surprising, since I am using an SSD (relatively recent 16\" MacBook Pro). Are there any other possible causes? I have the repository checked out with all submodules (recursively); does that affect the timing?\n \n \n \n Perhaps build differences - I usually build with `USE_DISTRIBUTED=0`, `USE_QNNPACK=0`, `USE_MKLDNN=0`, `USE_FBGEMM=0`, `USE_NNPACK=0`. It would be surprising to me if that would be it, but if it is then maybe it's fixable.\n \n \n \n > I do agree that the page should be linked from `CONTRIBUTING.md`, so I just opened #50540 to do that.\n \n \n \n Thanks!",
      "y": "Build with `USE_DISTRIBUTED=0`, `USE_QNNPACK=0`, `USE_MKLDNN=0`, `USE_FBGEMM=0`, `USE_NNPACK=0`."
   },
   {
      "null": 531,
      "x": "rpc memory leak",
      "z": "Looks like the memory leak is happening due to the timeoutMap_: https://github.com/pytorch/pytorch/blob/master/torch/csrc/distributed/rpc/tensorpipe_agent.cpp#L727. We add an entry to timeoutMap_ on RPC send, but once we receive a response we never remove it from the map. The entire timeout duration needs to expire before we actually end up removing it from the map.",
      "y": "Add an entry to timeoutMap_ on RPC send, but once we receive a response we never remove it from the map. The entire timeout duration needs to expire before we actually end up removing it from the map."
   },
   {
      "null": 532,
      "x": "When I was compiling, these errors appeared, what should I do",
      "z": "Can you please share the full build log (please `rm -rf build` folder before running `setup.py` again)",
      "y": "Switch to 1.7."
   },
   {
      "null": 533,
      "x": "The computation of weighted CrossEntropyLoss is not reasonable",
      "z": "Yes. I can work on it. Thank you.",
      "y": "Enable min & max for Float16 & BFloat16."
   },
   {
      "null": 534,
      "x": "`ATen/cpu/vec256/` should be a header only library for all but quantized types",
      "z": "Also, removing triage review since there are already a PR that addresses the issue.",
      "y": "Make Vec256 header only library."
   },
   {
      "null": 535,
      "x": "torch.linalg.qr ignores zero batched dimensions",
      "z": "probably it's obvious, but it's worth noting that `torch.qr` exhibits the same behavior (not surprising, since one is implemented on top of the other):\n \n ```pycon\n \n >>> import torch\n \n >>> A = torch.randn(shape)\n \n >>> Q, R = torch.qr(A)\n \n >>> Q.shape\n \n torch.Size([4, 4])\n \n >>> R.shape\n \n torch.Size([4, 5])\n \n ```",
      "y": "Fix MAGMA qr for empty batched inputs."
   },
   {
      "null": 536,
      "x": "Learning rate scheduler in C++ API",
      "z": "Sry about the delay, I was sick during the weekend and probably need rest today as well. I will go over this code no later then tomorrow and update here. Then we can start an official PR to the pytorch repo.\n \n \n \n @jamesbut @ssbotelh \n \n pytorch 1.8 deadline is already passed. I am targeting to include this into pytorch 1.9. But welcome to use it immediately once it is landed into pytorch master repo.",
      "y": "Implement ReduceLROnPlateau and StepLR."
   },
   {
      "null": 537,
      "x": "Add a `vectorize` flag to torch.autograd.functional.{jacobian, hessian}",
      "z": "> One question: do we need to do special handling when the `create_graph=True` flag is set?\n \n \n \n Other than setting create_graph=True when we call autograd.grad, no, I don't think so.",
      "y": "Set `create_graph=True` when we call autograd.grad."
   },
   {
      "null": 538,
      "x": "Add batched grad checking to Opinfo",
      "z": "Initial support added in #50818",
      "y": "1. add new `check_batched_grad=True` and `check_batched_gradgrad=True`\n attributes to OpInfo. These are `True` by default because we expect most\n operators to support batched gradient computation.\n 2. If `check_batched_grad=True`, then `test_fn_grad` invokes gradcheck\n with `check_batched_grad=True`.\n 3. If `check_batched_gradgrad=True`, then `test_fn_gradgradgrad` invokes\n `gradgradcheck` with `check_batched_grad=True`."
   },
   {
      "null": 539,
      "x": "C++ API: `at::empty(size)` modifies `size` ArrayRef in-place when it owns the ArrayRef",
      "z": "In particular, the reference may not even be valid to dereference after assignment. I think ASAN should complain if, for example, you resize from dim() greater than 5 to, say, dim() 3.",
      "y": "Output from `t.sizes()` is `IntArrayRef` which is a reference type, so shouldn't really have expected this to work. Re-assigning the tensor shouldn't provide any guarantees about whether or not the reference returned from t.sizes() gets modified."
   },
   {
      "null": 0,
      "x": "Don't support torch.Size() in the script",
      "z": "BTW, the main reason why we created torch.Size was to be able to use the legacy torch.Tensor constructor, which was overloaded on size / data.\nNow that we are mainly moving towards having a data constructor torch.tensor and a size constructor torch.empty, I believe we could deprecate at some point torch.Size, and once (if) we drop support for torch.Tensor altogether, we can probably just make tensor.size() return a tuple as well.",
      "y": "By the way, this is the major reason we built torch. To be able to use the legacy torch, you needed to be of a certain size. The tensor function Object() { [native code] } was overloaded in terms of size and data.\nNow that we have a data function Object() { [native code] } torch.tensor and a size function Object() { [native code] } torch, we are mostly heading towards having a data function Object() { [native code] } torch.tensor and a size function Object() { [native code] } torch.\nI believe we could deprecate torch at some point.\nSize, and when (if) we eliminate torch support.\nWe can probably just make tensors as a whole."
   },
   {
      "null": 1,
      "x": "opening nn page is very slow",
      "z": "For example, see https://pytorch.org/docs/stable/nn.html#torch.nn.functional.normalize\n\nTwo reasons:\n\nIt renders a lot of mathjax.\nWe put both modules and functionals on the same html.\nCan we speed it up possibly with some refactoring?",
      "y": "https://pytorch.org/docs/stable/nn.html#torch.nn.functional.normalize as an example.\nThere are two factors at play:\nA lot of mathjax is rendered.\nOn the same html, we included both modules and functionals.\nIs it possible to refactor it to make it go faster?"
   },
   {
      "null": 2,
      "x": "Add document page for torch.distributed.rpc/autograd",
      "z": "For 2, I think since that rpc and dist autograd interact a lot together and a user would probably use both of them if they were using one, they should live in the same page",
      "y": "For 2, I believe that since rpc and dist autograd interact so much and a user would almost certainly use both if they only used one, they should be on the same page."
   },
   {
      "null": 3,
      "x": "[jit] support self._modules in TorchScript",
      "z": "_modules itself shouldn't be exposed, but the accessor methods (e.g. modules(), named_children(), named_modules(), children() should be\n\nmost of these are iterators unfortunately.. Maybe we should override them in ScriptModule and return list instead.",
      "y": "The accessor methods (e.g. modules(), named children(), named modules(), children()) should be provided, but not _modules itself.\n\nUnfortunately, the majority of these are iterators. Perhaps we should override them in ScriptModule and instead return a list."
   },
   {
      "null": 4,
      "x": "Proposal to change the offsets for the EmbeddingBag operator in PyTorch",
      "z": "Yeah, we need a new flag in the frontend to have this new change. And this new change is not just for convenience. To lower this op to accelerator, we usually create a larger buffer and zero pad. For example, we want to look up indices [0, 1, 2, 3], but since we allocate the indices as max size of 8, the input to accelerator becomes [0, 1, 2, 3, 0, 0, 0, 0]. Since 0 is a valid lookup index, we don't know where we are going to end just from current last offset. One argument is that we can pad something like -1 to have [0, 1, 2, 3, -1, -1, -1, -1]. But padding -1 is probably not as efficient and we have to add more branching logic to check whether we hit -1 or not.",
      "y": "To implement this new change, we'll need a new flag in the frontend. This new move isn't merely for the sake of convenience. We normally add a larger buffer and zero pad to reduce this op to accelerator. For example, we want to look up the indices [0, 1, 2, 3], but because the indices have a maximum size of 8, the input to the accelerator is [0, 1, 2, 3, 0, 0, 0, 0]. We don't use 0 because it's a valid lookup index."
   },
   {
      "null": 5,
      "x": "register_forward_hooks (and register_backward_hooks) support for TorchScript.",
      "z": "Thanks for the feature request! We plan to implement hooks in the near future, but I don't have an exact timeline yet. I'll update this ",
      "y": "Thank you for requesting a feature!\nWe intend to implement hooks in the near future, but I don't know when that will be.\nI'll keep this updated. "
   },
   {
      "null": 6,
      "x": "PyTorch deadlocks when freeing memory in embedded Pybind interpreter",
      "z": "I was talking about GIL deadlocks and I noticed a few bugs in the original example that lead to the deadlock (or crashes in some case). Additionally, pybind11 v.2.3.0 (used in the repro) had a [critical bug](https://github.com/pybind/pybind11/issues/1364) that would lead to crashes even when the example is fixed. You'll need to use at least v.2.4.0 (I tested with v2.5.0).\n\nHere is my fixed version of main.cpp:\n\n```c++\n\n#include <pybind11/embed.h>\n\n#include <thread>\n\n\n\nnamespace py = pybind11;\n\n\n\nint main() {\n\n    py::scoped_interpreter guard{};\n\n    auto threading = py::module::import(\"threading\"); // (1)\n\n    auto thread = std::thread([]() {\n\n      py::gil_scoped_acquire acquire; // (2)\n\n      auto hello = py::module::import(\"hello\");\n\n      py::object a =  hello.attr(\"A\")();\n\n      a.attr(\"run\")();\n\n    });\n\n    py::gil_scoped_release release;  // (3)\n\n    thread.join();\n\n}\n\n1) Python 3.7 expects that the threading module is initialized before additional threads are created. Otherwise you get an error `assert tlock.locked()` on exit.\n\n2) The C++ thread must acquire the GIL before calling into Python.\n\n3) The main thread should release the GIL before calling the blocking operation `thread.join()`. Otherwise (2) will never be able to acquire the GIL.\n\nThese three changes (combined with the updated pybind11) fix the deadlocks in the example below",
      "y": "I was discussing GIL deadlocks when I realised a couple flaws in the original example that resulted in the deadlock (or crashes in some case).\nAdditionally, even after the example was patched, pybind11 v.2.3.0 (used in the repro) had a [major bug](https://github.com/pybind/pybind11/issues/1364) that caused crashes.\nYou'll need at least version 2.4.0. (I tested with v2.5.0).\n\n\n\nHere is my fixed version of main.cpp:\n\n```c++\n\n#include <pybind11/embed.h>\n\n#include <thread>\n\n\n\nnamespace py = pybind11;\n\n\n\nint main() {\n\n    py::scoped_interpreter guard{};\n\n    auto threading = py::module::import(\"threading\"); // (1)\n\n    auto thread = std::thread([]() {\n\n      py::gil_scoped_acquire acquire; // (2)\n\n      auto hello = py::module::import(\"hello\");\n\n      py::object a =  hello.attr(\"A\")();\n\n      a.attr(\"run\")();\n\n    });\n\n    py::gil_scoped_release release;  // (3)\n\n    thread.join();\n\n}"
   },
   {
      "null": 7,
      "x": "initializedContextIds_ field in DistEngine grows without bound",
      "z": " The \"high priority\" label means it goes to triage review (i.e. is the right person working on this issue right now). I doubt that was your intention here.\n\nIs there a better label for per-(module|topic|queue) prioritization? ",
      "y": "The designation of \"high priority\" denotes that it will be reviewed by a triage team (i.e. is the right person working on this issue right now).\nThat wasn't your aim, I'm sure.\n\n\nIs there a better term for prioritisation by (module|topic|queue)? "
   },
   {
      "null": 8,
      "x": "[Feature request] Provide binaries for Python 3.8",
      "z": "Python is upgraded to 3.8 in arch linux based distros recently, I suppose many arch or manjaro users are waiting for 3.8 binaries.",
      "y": "Python 3.8 was recently upgraded on arch linux based distros, and I'm sure many arch or manjaro users are eagerly awaiting 3.8 binaries. "
   },
   {
      "null": 9,
      "x": "magma functionality isn't working on K80 GPU with official binaries",
      "z": "the torch.solve thing does work!",
      "y": "The torch.solve method actually works! "
   },
   {
      "null": 10,
      "x": null,
      "z": "mypy error for TorchScript moduleOne *other* alternative is to replace the current:     def forward(self, *input: Any, **kwargs: Any) -> T_co: ...  # type: ignore\n\nwith the more permissive:\n\n    forward: Callable[..., T_co]\n\nThe current signature basically will always fail to type check, because mypy enforces that the subclass method's input types must be \"wider\" than their superclass method's input types (i.e. they can vary contravariantly). And nothing is wider than Any.\n\nThis change would ensure mypy doesn't complain to you about forward() typing, which resolves the problem nicely. I'll put a PR when I get a sec.",
      "y": "A *different* option is to replace the current:      def forward(self, *input: Any, **kwargs: Any) -> T_co: ...  # type: ignore\n\nwith the more permissive:\n\n    forward: Callable[..., T_co]\n\nBecause mypy requires that the subclass method's input types be \"wider\" than their superclass method's input types, the existing signature will always fail to type check (i.e. they can vary contravariantly).\nAnd nothing is more expansive than Any.\n\n\nThis change would ensure mypy doesn't complain to you about forward() typing, which resolves the problem nicely. I'll put a PR when I get a sec. "
   },
   {
      "null": 11,
      "x": "Indexed assignment of quantized Tensors yields unexpected results",
      "z": "copy_and_clobber_?",
      "y": "copy and clobber_? "
   },
   {
      "null": 12,
      "x": "RPC tests are flaky",
      "z": "Turning this issue into a tracking issue, let's continue to post new examples of flakiness here to help ",
      "y": "To assist turn this into a tracked issue, let's keep posting additional cases of flakiness here. "
   },
   {
      "null": 13,
      "x": "Windows CI build is flaky: cannot delete workspace... because it is being used by another process",
      "z": "The plan on record is to move these CI jobs to CircleCI and then we kill the Jenkins job.",
      "y": "On paper, the plan is to migrate these CI jobs to CircleCI, then kill the Jenkins job."
   },
   {
      "null": 14,
      "x": "Gradle build is flaky: \"could not get resource\"",
      "z": "Working on it.",
      "y": "I'm working on it right now. "
   },
   {
      "null": 15,
      "x": "PytorchStreamWriter failed opening archive.",
      "z": "tried change the saving path worked for me",
      "y": "Changed the save route and it worked for me. "
   },
   {
      "null": 16,
      "x": "Blank value for _GLIBCXX_USE_CXX11_ABI when compiling with Homebrew's GCC (on macOS)",
      "z": "I'm not sure we support building with gcc on mac os, we generally recommend using clang on mac. (see instructions at https://github.com/pytorch/pytorch)",
      "y": "We don't think we support building with gcc on Mac OS X; instead, we propose clang.\n(See https://github.com/pytorch/pytorch for instructions) "
   },
   {
      "null": 17,
      "x": "CPU memory leak when using torch.no_grad()",
      "z": "Cross post from the forum: https://discuss.pytorch.org/t/memory-leak-if-use-torch-no-grad/97484\n",
      "y": "https://discuss.pytorch.org/t/memory-leak-if-use-torch-no-grad/97484"
   },
   {
      "null": 18,
      "x": "test_torch.py/test_inverse_cuda causes illegal memory access on some platforms",
      "z": "The PR has been merged, so it should solve the problem you saw on 10.1.105. If you have verified that and don't see a crash elsewhere, feel free to close it. Thanks! ",
      "y": "The PR has been merged, therefore it should fix the issue you were experiencing on 10.1.105.\nIf you've double-checked that and don't see any other crashes, feel free to close it.\nThanks! "
   },
   {
      "null": 19,
      "x": "Why doesn't amp improve training speed",
      "z": "See\n\nhttps://pytorch.org/tutorials/recipes/recipes/amp_recipe.html#speedup-with-amp-is-minor\n\nhttps://pytorch.org/tutorials/recipes/recipes/tuning_guide.html\n\nfor general and amp guidance.\n\n\n\nIf your model/batch size are small and achieve low device utilization, amp won't help, and may hurt.  Also, if your GPU is not Tensor Core enabled (in other words if it's Pascal or earlier) amp isn't expected to deliver much speedup.  What model GPU are you using?\n\n\n\nUNet has 3D convolutions (right?) so it's also possible you're hitting bad cudnn heuristics.  `torch.backends.cudnn.benchmark = True` at the top of your script may help.  If you find a particular layer/conv shape that performs poorly with FP16 even after `benchmark = True` I will report it to cudnn devs.\n\n",
      "y": "See\n\n\nhttps://pytorch.org/tutorials/recipes/recipes/amp recipe.html#speedup-with-amp-is-minor\n\n\nhttps://pytorch.org/tutorials/recipes/recipes/tuning guide.html\n\n\nfor general and amplification advice\n\n\n\n\nAmp won't assist, and may hurt, if your model/batch size is tiny and you obtain poor device use.\nAlso, amp isn't expected if your GPU isn't Tensor Core enabled (in other words, if it's Pascal or before). "
   },
   {
      "null": 20,
      "x": "torch.autograd.profiler.load_nvprof() crash",
      "z": "PR  #45004 fixes the issue, will pick relevant parts of the fix into 1.7",
      "y": "PR #45004 resolves the issue; relevant parts of the patch will be incorporated into 1.7."
   },
   {
      "null": 21,
      "x": "Wrong signature for torch.max_pool(1d, 2d, 3d) in get_testing_overrides()",
      "z": "I believe one was to be created, but I don't know if it has been yet. But if it isnot, `module: numpy` is what we have",
      "y": "\nI assume one was supposed to be made, but I'm not sure if it has yet.\nHowever, if it isn't, we have module: numpy. "
   },
   {
      "null": 22,
      "x": "JIT Profiling executor is not fusing Dropout while legacy executor does",
      "z": "Just to motivate the problem, in case you don't have the context, the dropout operator is heavily used in Transformer/Bert NLP networks and being able to fuse to dropout is an important perf opportunity in those networks.",
      "y": "To give you a sense of the difficulty, the dropout operator is widely utilised in Transformer/Bert NLP networks, and being able to fuse to dropout is a significant efficiency opportunity in those networks. "
   },
   {
      "null": 23,
      "x": "Tensors with complex dtypes fail to be cloned contiguously",
      "z": "Hi,\nYes this design difference is done on purpose.\n\nThe plan we have to solve this issue is to build on top of https://github.com/pytorch/pytorch/issues/49171 so that the user can give the weights as proper inputs to the function and thus work well with the function autograd API.",
      "y": "Hi,\n\nYes, the design contrast is deliberate.\n\nThe goal we have to fix this problem is to build on top of https://github.com/pytorch/issues/49171 so that the user may submit the weights as suitable inputs to the function and therefore interact nicely with the autograd API. "
   },
   {
      "null": 24,
      "x": "No documentation for `torch.sgn`",
      "z": "It is actually documented but it's missing an torch.rst entry so it doesn't appear in the built docs. See here:\nhttps://github.com/pytorch/pytorch/blob/68a6e4637903dba279c60daae5cff24e191ff9b4/torch/_torch_docs.py#L7517\n\n\n\n",
      "y": "It is documented, but the torch.rst entry is missing, therefore it does not appear in the built docs.\nHere's an example:\n\nhttps://github.com/pytorch/pytorch/blob/68a6e4637903dba279c60daae5cff24e191ff9b4/torch/ torch docs.py#L7517 "
   },
   {
      "null": 25,
      "x": "libtorch_cuda.so is missing fast kernels from libcudnn_static.a, therefore statically linked cuDNN could be much slower than ",
      "z": "So, does this issue explain why NGC containers have been consistently faster than the official conda builds for a number of PyTorch versions now? NGC == dynamic link, conda/pip = static w/ this issue? This has pretty significant impact if that is the case.\n\n\n\nI ran some benchmarks trying to figure out what was happening as I've kept bumping into it with new releases... https://gist.github.com/rwightman/bb59f9e245162cee0e38bd66bd8cd77f",
      "y": "Is this the reason why NGC containers have consistently outperformed official conda builds for a number of PyTorch versions?\nWith this issue, NGC == dynamic link, conda/pip = static.\nIf that is the case, it has a huge influence.\n\nI performed some benchmarks to see what was going on because I kept running into it with fresh releases...\nhttps://gist.github.com/rwightm "
   },
   {
      "null": 26,
      "x": " Broken formatting for the function signature of `torch.randperm`",
      "z": "We would accept a PR to fix this (unless it turns out that there's a reason why this isn't documented)",
      "y": "We'd accept a PR to address this (unless there's a reason why this isn't documented). "
   },
   {
      "null": 27,
      "x": "Broken formatting on `torch.repeat_interleave`",
      "z": "We would accept a PR to fix this.",
      "y": "We'd be happy to take a PR to address this. "
   },
   {
      "null": 28,
      "x": "Add torch.linalg.vector_norm and torch.linalg.matrix_norm functions",
      "z": "All vector norms and frobenius norm are implemented in the same way, via reduction. For matrix norm indeed splitting into separate functions might make sense, but even with that I'd leave  +/-inf and +/-one matrix norm in the same function, they share a lot of implementation. That leaves nuclear norm and +/-2-norms that could also live together because they both rely on first computing svd. Question: why is frobenius norm implemented as \n\n```\n\n    if (self.is_complex()){\n\n      result_ = at::sqrt(at::sum(at::real(self.conj() * self), dim_, keepdim));\n\n    } else {\n\n      result_ = at::sqrt(at::sum((self * self), dim_, keepdim));\n\n    }\n\n``` \n\n? This is super inefficient compared to vector norm, and produces the same result.    ",
      "y": "The reduction method is used to implement all vector norms and frobenius norms.\nSplitting matrix norm into separate functions makes sensible, however I'd still keep +/-inf and +/-one matrix norm in the same function because they share a lot of implementation.\nThat leaves nuclear norm and +/-2-norms, which can coexist because they both employ svd as the first computation step. What is the purpose of Frobenius norm? ```\n\n    if (self.is_complex()){\n\n      result_ = at::sqrt(at::sum(at::real(self.conj() * self), dim_, keepdim));\n\n    } else {\n\n      result_ = at::sqrt(at::sum((self * self), dim_, keepdim));\n\n    }\n\n``` \n\n? This is super inefficient compared to vector norm, and produces the same result.    "
   },
   {
      "null": 29,
      "x": "Let loss_func(empty_inputs, reduction=\"mean\") return 0",
      "z": "I do agree that mean should return `nan` for empty Tensors and it is consistent with numpy:\n",
      "y": "I agree that for empty Tensors, mean should return nan, and this is compatible with numpy: "
   },
   {
      "null": 30,
      "x": "Can you add higher order derivative support for torch's embedding function? ",
      "z": "There are known issues in building PyTorch with CUDA11.2 we are facing now regarding the functionality as well as performance. We are actively working with the CUDA team to isolate and fix this issues. Until we have resolutions we discourage users to use CUDA 11.2 with PyTorch.",
      "y": "There are known concerns with creating PyTorch with CUDA11.2 that we are currently experiencing in terms of functionality and performance.\nWe're working closely with the CUDA team to isolate and resolve these issues.\nUsers should avoid using CUDA 11.2 with PyTorch until we have resolutions. "
   },
   {
      "null": 31,
      "x": "Discuss options for implementing einsum_path",
      "z": "I've never seen `einsum_path` used much, I would not add it as a separate function. Adding only the `optimize` keyword makes ",
      "y": "I've never seen einsum path used very often, so I wouldn't include it as a separate function.\nThe keyword \"optimise\" is all that is needed. "
   },
   {
      "null": 32,
      "x": "Docs build failures are difficult to identify",
      "z": "The easiest thing to do would be to improve the error message at the end of the build step, either in the `Makefile` or in the circleCI script that calls `make html` to state \"if this step fails, look back in the log for WARNING: (in capital letters)\". A more advanced solution would be to do something like\n\n```\n\nset -o pipefail\n\nmake html | tee /tmp/doc_build.log\n\nif [ $? -ne 0 ]; then \n\n    echo \"doc build error detected:\"; \n\n    grep WARNING /tmp/doc_build.log;\n\nfi\n\n```\n\n\n\nI wonder if there is a way to write a test to make sure it works as advertised",
      "y": "The simplest solution is to change the error message at the conclusion of the build step, either in the Makefile or in the circleCI script that calls make html, to say \"if this step fails, look back in the log for WARNING: (in capital letters)\".\nSomething like this would be a more advanced solution.\n"
   },
   {
      "null": 33,
      "x": "Categorical logits argument is treated as log probabilities",
      "z": "Thanks for the clarification. I never really thought about the inconsistency between binary and multinomial logits and that tripped me up.\n\nPerhaps the documentation could be clearer on this point as logits refer to different things in `torch.logit` and the `logits` argument to `Categorical`.",
      "y": "Thank you for clarifying.\nThe discrepancy between binary and multinomial logits had never occurred to me before, and it had tripped me up.\n\n\nBecause logits relate to different objects in torch.logit and the logits parameter to Categorical, perhaps the documentation could be clearer on this point. "
   },
   {
      "null": 34,
      "x": "`Expected self.scalar_type() == ScalarType::Float to be true, but got false.` when doing quantization aware training?\n",
      "z": "Thanks for the clarification. I never really thought about the inconsistency between binary and multinomial logits and that tripped me up.\n\nPerhaps the documentation could be clearer on this point as logits refer to different things in `torch.logit` and the `logits` argument to `Categorical`.",
      "y": "logits relate to different objects in torch.logit and the logits parameter to Categorical, perhaps the documentation could be clearer on this point"
   },
   {
      "null": 35,
      "x": "Casting from long to int may cause wrong result",
      "z": "The code still will have an issue. You will need to use an intermediate Tensor variable like\n```\nauto int_t = t.to(at::kInt);\n\nauto p = int_t.data_ptr<int>();",
      "y": "There will still be a problem with the code. You'll need to utilise an intermediary Tensor variable like\nauto int_t = t.to(at::kInt);\nauto p = int t.data ptrint>();"
   },
   {
      "null": 0,
      "x": "torch.cuda.device not working but torch.cuda.set_device works",
      "z": "Hi,\n\n`torch.cuda.device()` is a context manager:\n```\ntorch.cuda.set_device(0)\n# On device 0\nwith torch.cuda.device(1):\n print(\"Inside device is 1\") \n # On device 1\nprint(\"Outside is still 0\")\n# On device 0\n```",
      "y": "`torch.cuda.device()` is a context manager: ``` torch.cuda.set_device(0) # On device 0 with torch.cuda.device(1): print(\"Inside device is 1\") # On device 1 print(\"Outside is still 0\") # On device 0 ```"
   },
   {
      "null": 1,
      "x": "Have torch.manual_seed seed all GPUs as well",
      "z": "Yes I think it's enough to add these two lines.\n\nIt's because there are no CUDA generators. There's only a single cuRAND state per device, and it's embedded in to THCState",
      "y": "Yes I think it's enough to add these two lines. It's because there are no CUDA generators. There's only a single cuRAND state per device, and it's embedded in to THCState"
   },
   {
      "null": 2,
      "x": "ImportError: libmkl_intel_lp64.so: cannot open shared object file",
      "z": "Try a clean re-install. \n```bash\nrm -rf build\nrm -rf torch/lib/build\n```\nThen install again",
      "y": "Try a clean re-install. \n```bash\nrm -rf build\nrm -rf torch/lib/build\n```"
   },
   {
      "null": 3,
      "x": "Variable.clone() does not clone to the same device",
      "z": "It's not expected. `clone()` should operate within a single device",
      "y": "It's not expected. `clone()` should operate within a single device"
   },
   {
      "null": 4,
      "x": "How to get raw pointer from tensors?",
      "z": "@SuperShinyEyes data_ptr for a GPU Tensor will point to a pointer to GPU memory. You cannot operate on that pointer directly, and have to give it to a CUDA kernel",
      "y": "for a GPU Tensor will point to a pointer to GPU memory. You cannot operate on that pointer directly, and have to give it to a CUDA kernel"
   },
   {
      "null": 5,
      "x": "Pytorch Freezes System",
      "z": "it's not typical at all, there's something weird going on, but it's not the GPU memory.\n\nUsually freezing indicates two things:\n- you are running out of CPU memory and you are hitting disk swap\n- a lot of hardware (say PCI-e or faulty GPU) / disk errors are being generated and the kernel is coping up with it slowly.\n\nI wonder if your case is either of them.",
      "y": "it's not typical at all, there's something weird going on, but it's not the GPU memory. Usually freezing indicates two things: - you are running out of CPU memory and you are hitting disk swap - a lot of hardware (say PCI-e or faulty GPU) / disk errors are being generated and the kernel is coping up with it slowly. I wonder if your case is either of them."
   },
   {
      "null": 6,
      "x": "Ellipsis encoding fails when printing tensors",
      "z": "I ran into this just now. What fixed it for me is to set the environment variable `LANG=C.UTF-8 LC_ALL=C.UTF-8`. This is in a Docker container running Ubuntu.",
      "y": "What fixed it for me is to set the environment variable `LANG=C.UTF-8 LC_ALL=C.UTF-8`."
   },
   {
      "null": 7,
      "x": "[JIT] jit.trace does not support parameter.requires_grad?",
      "z": "Yes, `jit.trace` only records Tensor operations. Modifying attributes of tensor objects are not recorded by design.",
      "y": "Yes, `jit.trace` only records Tensor operations. Modifying attributes of tensor objects are not recorded by design."
   },
   {
      "null": 8,
      "x": "torch.meshgrid has no docstring if typing.TYPE_CHECKING is True",
      "z": "> (and on the standard Python interpreter) returns `None`.\n\nThis means that it's not a regression.\n\nYou're using PyTorch 1.7.0, for which `meshgrid` simply does not have a docstring: https://github.com/pytorch/pytorch/blob/1.7/torch/functional.py#L352\n\nThis is fixed in 1.8.0 and in master (see https://github.com/pytorch/pytorch/blob/master/torch/functional.py#L417), so I'll close the issue.",
      "y": "This is fixed in 1.8.0 and in master"
   },
   {
      "null": 9,
      "x": "ManyLinux v1.8 release .whl for AArch64 (Arm) does not work on CentOS 8",
      "z": "Hi @malfet,\nThat works for me if I build my own whls, and I've just tested the latest 1.8.1 release from https://download.pytorch.org/whl/torch_stable.html on RHEL and Ubuntu systems here and it appears to work fine out of the box, cheers.",
      "y": "That works for me if I build my own whls, and I've just tested the latest 1.8.1 release from https://download.pytorch.org/whl/torch_stable.html on RHEL and Ubuntu systems here and it appears to work fine out of the box,"
   },
   {
      "null": 10,
      "x": "Add complex autograd support for torch.symeig",
      "z": "Just adding `'symeig'` to `GRADIENT_IMPLEMENTED_FOR_COMPLEX` of in `tools/autograd/gen_variable_type.py` resolves this issue.\nhttps://github.com/pytorch/pytorch/blob/master/tools/autograd/gen_variable_type.py#L73-L95",
      "y": "Just adding `'symeig'` to `GRADIENT_IMPLEMENTED_FOR_COMPLEX` of in `tools/autograd/gen_variable_type.py` resolves this issue"
   },
   {
      "null": 11,
      "x": "Pytorch install via Pip Error",
      "z": "verified this is fixed in 1.8.1\n`pip install torch==1.8.1+cu102 torchvision==0.9.1+cu102 torchaudio===0.8.1 -f https://download.pytorch.org/whl/torch_stable.html`\n\n![image](https://user-images.githubusercontent.com/32868157/112998017-367eb300-91a0-11eb-9ce4-b1aa031e6bcc.png)\n\nclose the issue.",
      "y": "verified this is fixed in 1.8.1 `pip install torch==1.8.1+cu102 torchvision==0.9.1+cu102 torchaudio===0.8.1"
   },
   {
      "null": 12,
      "x": "Exception in thread pool task: !completed() INTERNAL ASSERT FAILED",
      "z": "It looks like the actual crash is fixed in nightlies and we've added a couple PRs to improve error handling, so I'll go ahead and close this one out. Feel free to reopen if there are additional questions/issues.",
      "y": "It looks like the actual crash is fixed in nightlies and we've added a couple PRs to improve error handling,"
   },
   {
      "null": 13,
      "x": "[linear-algebra][discussion][proposal] Move linear algebra functions to torch.linalg, at::native::linalg",
      "z": "`torch.Tensor` does document these methods, but there isn't a formal organization (or a side-bar).\nI'm saying that if we want to improve discoverability, we probably want to do either of those.\nHowever, that will break the alphabetical-orderness of the current method list",
      "y": "`torch.Tensor` does document these methods, but there isn't a formal organization (or a side-bar)."
   },
   {
      "null": 14,
      "x": "Distributed Training shut down on second epoch",
      "z": "@wangdongxuking61 @mcarilli Thank you for your reply. The problem has been solved by building master.",
      "y": "This is fixed in 1.8.0 and in master"
   },
   {
      "null": 15,
      "x": "MultivariateNormal and potrf is slow on gpu and seems to have some memory leak",
      "z": "Thanks for the fix",
      "y": "This is fixed in 1.8.0 and in master"
   },
   {
      "null": 16,
      "x": "Quantization Model Support",
      "z": "this feature is planned for 1.1, we will have a proposal listed out soon.",
      "y": "This is fixed in 1.8.0 and in master"
   },
   {
      "null": 17,
      "x": "ImportError: cannot import name 'caffe2_pb2' (Ubuntu 18.04)",
      "z": "Don't start python from inside the pytorch directory, cd somewhere else and try again.",
      "y": "Don't start python from inside the pytorch directory, cd somewhere else and try again."
   },
   {
      "null": 18,
      "x": "RuntimeError \"sizes must be non-negative\" (0.4.1)",
      "z": "We didn't have proper empty tensor support, so neither behavior you saw for 0.4.1 and the one you saw for 0.4.0 is desired. Now on master (and in next release) we have proper empty tensor support. So you will get a proper [3, 4, 2, 0] shaped tensor soon!",
      "y": "This is fixed in 1.8.0 and in master"
   },
   {
      "null": 19,
      "x": "Unintuitive error message when loading state into incompatibly-sized model",
      "z": "This has been fixed since then.",
      "y": "This is fixed in 1.8.0 and in master"
   },
   {
      "null": 20,
      "x": "[JIT] Tracer throws runtime exception for torch.normal",
      "z": "I'm fixing this right now.",
      "y": "This is fixed in 1.8.0 and in master"
   },
   {
      "null": 21,
      "x": "RuntimeError: the derivative for 'target' is not implemented. When I use F.smooth_l1_loss(x, y, reduce=False)",
      "z": "Are you sure that the target is the second argument and not the first? You feed (GT, PRED) but it should be (PRED, GT)...",
      "y": "You feed (GT, PRED) but it should be (PRED, GT)..."
   },
   {
      "null": 22,
      "x": "nn.BatchNorm1d failed on GPU",
      "z": "Testing this on master gives this more descriptive error message:\n![image](https://user-images.githubusercontent.com/5652049/33382285-57f1cfc6-d4ee-11e7-930c-7ae29d5bf32e.png)\n\nThe problem is that you should change the `nn.BatchNorm1d` module to live on the GPU because it has weights that are initialized on the cpu:\n\n```\nimport torch\nfrom torch.autograd import Variable\na = Variable(torch.randn(2,5).cuda(), requires_grad=True)\nbatchnorm = torch.nn.BatchNorm1d(5).cuda()\ny = batchnorm(a)\n```",
      "y": "The problem is that you should change the `nn.BatchNorm1d` module to live on the GPU because it has weights that are initialized on the cpu: ``` import torch from torch.autograd import Variable a = Variable(torch.randn(2,5).cuda(), requires_grad=True) batchnorm = torch.nn.BatchNorm1d(5).cuda() y = batchnorm(a) ```"
   },
   {
      "null": 23,
      "x": "Aten compile error",
      "z": "If you have an existing version of PyTorch installed, this problem may be another manifestation of https://github.com/pytorch/pytorch/issues/3669, i.e. old headers are picked up. A workaround for https://github.com/pytorch/pytorch/issues/3669 is removing the old headers from system include path.",
      "y": "A workaround is removing the old headers from system include path."
   },
   {
      "null": 24,
      "x": "`backward` hangs in multiprocess after single-process",
      "z": "@adamlerer workaround is to add this right after `import torch.multiprocessing as mp`:\n\n```python\nif __name__ == \"__main__\":\n mp.set_start_method(\"spawn\")\n```\n\nEDIT: prefer \"spawn\" over \"forkserver\" (by @colesbury)",
      "y": "workaround is to add this right after `import torch.multiprocessing as mp`: ```python if __name__ == \"__main__\": mp.set_start_method(\"spawn\") ```"
   },
   {
      "null": 25,
      "x": "Out of memory with higher-order gradients involving batchnorm2d",
      "z": "Thanks, I can't reproduce this now in branch v0.3.",
      "y": "This is fixed in 0.3.0 and in master"
   },
   {
      "null": 26,
      "x": "0.2_4 release notes inconsistent with documentation and actual behavior of reduce functions",
      "z": "The release notes had a typo, sorry. It should've been: \n> For example torch.sum(torch.randn(10, 20), dim=0) returns a 1D Tensor.\n\nI've fixed them now.\n\nAs you see from the documentation, there are two prototypes of `torch.sum`, one with the `dim` argument and one without. the one with the `dim` argument is a global sum of the Tensor.",
      "y": "This is fixed in 1.8.0 and in master"
   },
   {
      "null": 27,
      "x": "RuntimeError: cublas runtime error : library not initialized",
      "z": "`sudo rm -rf ~/.nv` works.",
      "y": "`sudo rm -rf ~/.nv` works."
   },
   {
      "null": 28,
      "x": "Feature request: expm1",
      "z": "`torch.expm1` has been merged into master. I think this issue can be closed now.",
      "y": "This is fixed in master"
   },
   {
      "null": 29,
      "x": "How to use conda to update pytorch to 0.4 version",
      "z": "Hi,\n\n0.3 has been released now for conda and contains this feature.\nThe main website contains the informations to install 0.3 with conda.",
      "y": "The main website contains the informations to install 0.3 with conda."
   },
   {
      "null": 30,
      "x": "NVIDIA memory not deallocated after interupt",
      "z": "Hi,\n\nYou can run `lsof /dev/nvidia0` to list all processes using the GPU. One of them is taking some memory, kill it.",
      "y": "You can run `lsof /dev/nvidia0` to list all processes using the GPU. One of them is taking some memory, kill it."
   },
   {
      "null": 31,
      "x": "Error in building 0.3.0 in macOS High Sierra",
      "z": "Sure, you can download the Command Line Tools at https://download.developer.apple.com/Developer_Tools/Command_Line_Tools_macOS_10.12_for_Xcode_8.2/Command_Line_Tools_macOS_10.12_for_Xcode_8.2.dmg . I think that you need to switch to Xcode 8.2 first, and then install the package.",
      "y": "you can download the Command Line Tools at https://download.developer.apple.com/Developer_Tools/Command_Line_Tools_macOS_10.12_for_Xcode_8.2/Command_Line_Tools_macOS_10.12_for_Xcode_8.2.dmg . I think that you need to switch to Xcode 8.2 first, and then install the package."
   },
   {
      "null": 32,
      "x": "Test without backward the model will run out of memory",
      "z": "Same things apply for 0.4. For `volatile`, use `torch.no_grad()`",
      "y": "Same things apply for 0.4. For `volatile`, use `torch.no_grad()`"
   },
   {
      "null": 33,
      "x": "FAILED Build: __AVX2__ is defined (via e.g. -mavx2) \" \"but CAFFE2_PERF_WITH_AVX2 is not defined.",
      "z": "Found a better solution. It seems that the errors are not to do with build threads.\n\nIt turns out the errors were caused by cmake using Ninja instead of Make, if Ninja is installed. Adding USE_NINJA=OFF seems to fix it",
      "y": "It turns out the errors were caused by cmake using Ninja instead of Make, if Ninja is installed. Adding USE_NINJA=OFF seems to fix it"
   },
   {
      "null": 34,
      "x": "Segmentation fault (core dumped) in C++ API for centos",
      "z": "I think your GCC version is < 4.9 in Centos, which is ABI-incompatible with PyTorch, see https://github.com/pytorch/pytorch/issues/6987\n\nCould you try updating your GCC to be 4.9 or higher, following instructions from https://gist.github.com/goldsborough/d466f43e8ffc948ff92de7486c5216d6 for example?",
      "y": "try updating your GCC to be 4.9 or higher,"
   },
   {
      "null": 35,
      "x": "Non-deterministic behavior in pytorch even with seeds set",
      "z": "I can't speak about achieving determinism on GPU, but on CPU you also have to ensure that only one thread is being used, to avoid the accumulation of numerical errors resulting from performing the same operations in a slightly different order. You can use `torch.set_num_threads(1)` for this.",
      "y": "I can't speak about achieving determinism on GPU, but on CPU you also have to ensure that only one thread is being used, to avoid the accumulation of numerical errors resulting from performing the same operations in a slightly different order. You can use `torch.set_num_threads(1)` for this."
   },
   {
      "null": 36,
      "x": "torch.nn.utils.rnn.pack_padded_sequence not working in multi-GPU environments",
      "z": "You need to pass `device_ids` argument when you are wrapping your model in `DistributedDataParallel` so that each process is using only one GPU, as described in @soumith's link (Important notices, item 4).",
      "y": "You need to pass `device_ids` argument when you are wrapping your model in `DistributedDataParallel` so that each process is using only one GPU"
   },
   {
      "null": 37,
      "x": "export onnx model and load by caffe2 error",
      "z": "@cswwp onnx_caffe2 is out of date. It is merged to caffe2. We have a PR to update the tutorial: \nhttps://github.com/pytorch/tutorials/pull/348\n\nAlso here is a tutorial, which should work: https://github.com/onnx/tutorials/blob/master/tutorials/OnnxCaffe2Import.ipynb",
      "y": "onnx_caffe2 is out of date. It is merged to caffe2. Also here is a tutorial, which should work: https://github.com/onnx/tutorials/blob/master/tutorials/OnnxCaffe2Import.ipynb"
   },
   {
      "null": 38,
      "x": "Memory Error in pip install of torch 1.2.0 on Linux",
      "z": "> \n> .. `pip install --no-cache-dir install torchvision` seems to have gotten around the issue.\n\nI guess there is a typo in your command. The command which worked for me: \n\n`pip --no-cache-dir install torchvision`",
      "y": "`pip install --no-cache-dir install torchvision` seems to have gotten around the issue. I guess there is a typo in your command. The command which worked for me: `pip --no-cache-dir install torchvision`"
   },
   {
      "null": 39,
      "x": "ImportError: libcudart.so.9.0: cannot open shared object file: No such file or directory",
      "z": "Sorry, I didn't see the post carefully. Would you please try whether the following command solves your problem? `pip3 install torch===1.2.0 torchvision===0.4.0 -f https://download.pytorch.org/whl/torch_stable.html`",
      "y": "Would you please try whether the following command solves your problem? `pip3 install torch===1.2.0 torchvision===0.4.0 -f https://download.pytorch.org/whl/torch_stable.html`"
   },
   {
      "null": 40,
      "x": "torch.load issue on loading file created by torch.save",
      "z": "Thanks for the concise repro! This does indeed seem to be a bug, there is a fix in #25279.",
      "y": "This is fixed in 1.8.0 and in master"
   },
   {
      "null": 41,
      "x": "Trouble installing PyTorch for CUDA 9.0",
      "z": "> How do I install it in this case?\n\n1. Open the link [https://download.pytorch.org/whl/cu90/torch_stable.html](https://download.pytorch.org/whl/cu90/torch_stable.html) in a browser\n2. Go into the source \n3. Download the appropriate version. For `PyTorch 1.1.0`, `CUDA 9.0` and `Python 3.6` I have downloaded `torch-1.1.0-cp36-cp36m-linux_x86_64.whl`, If you have other requirements then download appropriate `.whl` file\n4. Install via `pip install [downloaded .whl file]`.",
      "y": "> How do I install it in this case?\n\n1. Open the link [https://download.pytorch.org/whl/cu90/torch_stable.html](https://download.pytorch.org/whl/cu90/torch_stable.html) in a browser\n2. Go into the source \n3. Download the appropriate version. For `PyTorch 1.1.0`, `CUDA 9.0` and `Python 3.6` I have downloaded `torch-1.1.0-cp36-cp36m-linux_x86_64.whl`, If you have other requirements then download appropriate `.whl` file\n4. Install via `pip install [downloaded .whl file]`."
   },
   {
      "null": 42,
      "x": "problem with mkldnn and march=native",
      "z": "@branfosj, we solve this problem in #25757, thanks!",
      "y": "This is fixed in 1.8.0 and in master"
   },
   {
      "null": 43,
      "x": "Unknown Error at torch/lib/THC/THCGeneral.c:66",
      "z": "cuda error unknown happens for many weird reasons.\nHere's one try to fix it:\n\n```\n$ sudo python\n>>> import torch\n>>> a = torch.randn().cuda()\n```\nThen exit.\n\nThis might initialize the device drivers properly. Sometimes, the nvidia device files are not properly created under /dev/ and sudo helps.",
      "y": "cuda error unknown happens for many weird reasons.\nHere's one try to fix it:\n\n```\n$ sudo python\n>>> import torch\n>>> a = torch.randn().cuda()\n```\nThen exit.\n\nThis might initialize the device drivers properly. Sometimes, the nvidia device files are not properly created under /dev/ and sudo helps."
   },
   {
      "null": 44,
      "x": "LSTM memory leak?",
      "z": "It was probably the autograd refactor that removed Variables from the graph (they were replaced with AccumulateGrad nodes). The leak was likely a reference cycle",
      "y": "It was probably the autograd refactor that removed Variables from the graph (they were replaced with AccumulateGrad nodes). The leak was likely a reference cycle"
   },
   {
      "null": 45,
      "x": "How to specify the cuda PATH in pytorch?",
      "z": "Try running `CUDA_HOME=/path/to/cuda-version python setup.py install`",
      "y": "Try running `CUDA_HOME=/path/to/cuda-version python setup.py install`"
   },
   {
      "null": 46,
      "x": "Confusing error msg when padding is set to float in nn.Conv1d",
      "z": "To clarify, I think that it might be helpful if the RuntimeError is updated to include the type of the incorrect tuple, so its clear that the type is expected to be an int.\n```python\nimport torch\nimport torch.nn as nn\nfrom torch.autograd import Variable\ninput = Variable(torch.randn(1, 1, 10))\noutput = nn.Conv1d(1, 1, 3, padding=1)(input) # fine\noutput = nn.Conv1d(1, 1, 3, padding=1.0)(input) # error\n```",
      "y": "I think that it might be helpful if the RuntimeError is updated to include the type of the incorrect tuple, so its clear that the type is expected to be an int.\n```python\nimport torch\nimport torch.nn as nn\nfrom torch.autograd import Variable\ninput = Variable(torch.randn(1, 1, 10))\noutput = nn.Conv1d(1, 1, 3, padding=1)(input) # fine\noutput = nn.Conv1d(1, 1, 3, padding=1.0)(input) # error\n```"
   },
   {
      "null": 47,
      "x": "Implement toCFloat() for Variables and numel() = 1 Tensors",
      "z": "this is now implemented.",
      "y": "this is now implemented."
   },
   {
      "null": 48,
      "x": "cuda 9.0 not found but detected 6.1",
      "z": "CUDA architecture is something different than the driver and toolkit version (you can think of it as version of your hardware - it will be the same no matter what's the driver). 6.1 are the Pascal GPUs. You're all good.",
      "y": "CUDA architecture is something different than the driver and toolkit version (you can think of it as version of your hardware - it will be the same no matter what's the driver). 6.1 are the Pascal GPUs. You're all good."
   },
   {
      "null": 49,
      "x": "Why the Dropout2d and BatchNorm2d's model.eval() result is poor",
      "z": "decrease momentum value in BatchNorm layer to something small like 0.0001",
      "y": "decrease momentum value in BatchNorm layer to something small like 0.0001"
   },
   {
      "null": 50,
      "x": "2 processes cannot use same GPU",
      "z": "It happens because your GPUs are in `EXCLUSIVE_PROCESS` mode, so the CUDA driver will forbid two processes from using the same GPU. You should be able to change that by running `nvidia-smi -g <GPU number> -c 0`.",
      "y": "It happens because your GPUs are in `EXCLUSIVE_PROCESS` mode, so the CUDA driver will forbid two processes from using the same GPU. You should be able to change that by running `nvidia-smi -g <GPU number> -c 0`."
   },
   {
      "null": 51,
      "x": "how could i get old version of libtorch , thanks",
      "z": "For example, \nhttps://download.pytorch.org/libtorch/cu101/libtorch-shared-with-deps-1.4.0.zip",
      "y": "https://download.pytorch.org/libtorch/cu101/libtorch-shared-with-deps-1.4.0.zip"
   },
   {
      "null": 52,
      "x": "Export to ONNX fails using F.interpolate with mode='area'",
      "z": "@omasaht - `area` mode is not explicitly supported in ONNX spec, and therefore, not supported in the exporter.",
      "y": "area` mode is not explicitly supported in ONNX spec, and therefore, not supported in the exporter."
   },
   {
      "null": 53,
      "x": "Performance regression for inference from pytorch 1.4.0 to >= 1.5.0",
      "z": "Thanks @ezyang. I tried with the nightly build (`1.7.0.dev20200705+cpu`) and I indeed see performance similar to 1.4.0. I am going to close the issue.",
      "y": "I tried with the nightly build (`1.7.0.dev20200705+cpu`) and I indeed see performance similar to 1.4.0"
   },
   {
      "null": 54,
      "x": "Error installing 0.3.0 from Anaconda on MacOS 10.13.1",
      "z": "Update conda first with `conda update conda` and try again",
      "y": "Update conda first with `conda update conda` and try again"
   },
   {
      "null": 55,
      "x": "Segmentation Fault when importing Torch",
      "z": "Append '/usr/lib/nvidia-384' or whichever nvidia driver version is being used to the LD_LIBRARY_PATH Environment variable. This worked out for me! Here is a reference - https://stackoverflow.com/questions/42678439/importerror-libnvidia-fatbinaryloader-so-375-39-cannot-open-shared-object-file",
      "y": "Append '/usr/lib/nvidia-384' or whichever nvidia driver version is being used to the LD_LIBRARY_PATH Environment variable."
   },
   {
      "null": 56,
      "x": "How to only padding the bottom when use the Conv2D ?",
      "z": "use the [functional padding method](http://pytorch.org/docs/master/nn.html#torch.nn.functional.pad)",
      "y": "use the [functional padding method](http://pytorch.org/docs/master/nn.html#torch.nn.functional.pad)"
   },
   {
      "null": 57,
      "x": "how should I cite PyTorch in the paper?",
      "z": "For now you could cite our NIPS 2017 workshop paper that discusses just the autodiff engine of PyTorch:\n\n```\n@article{paszke2017automatic,\n title={Automatic differentiation in PyTorch},\n author={Paszke, Adam and Gross, Sam and Chintala, Soumith and Chanan, Gregory and Yang, Edward and DeVito, Zachary and Lin, Zeming and Desmaison, Alban and Antiga, Luca and Lerer, Adam},\n year={2017}\n}\n```\n\nThe paper is located here: https://openreview.net/forum?id=BJJsrmfCZ\n\nOnce we release a full paper (with more authors), I will update this thread with a new citation.",
      "y": "For now you could cite our NIPS 2017 workshop paper that discusses just the autodiff engine of PyTorch:\n\n```\n@article{paszke2017automatic,\n title={Automatic differentiation in PyTorch},\n author={Paszke, Adam and Gross, Sam and Chintala, Soumith and Chanan, Gregory and Yang, Edward and DeVito, Zachary and Lin, Zeming and Desmaison, Alban and Antiga, Luca and Lerer, Adam},\n year={2017}\n}\n```\n\nThe paper is located here: https://openreview.net/forum?id=BJJsrmfCZ"
   },
   {
      "null": 58,
      "x": "non-cudnn LSTM and GRU biases have wrong shapes",
      "z": "Fix now included in https://github.com/pytorch/pytorch/pull/1683",
      "y": "Fix now included in https://github.com/pytorch/pytorch/pull/1683"
   },
   {
      "null": 59,
      "x": "Is there any API to visualize the architecture of model?",
      "z": "There's [@szagoruyko script](https://github.com/szagoruyko/functional-zoo/blob/master/visualize.py) that produces a `dot` file",
      "y": "There's (https://github.com/szagoruyko/functional-zoo/blob/master/visualize.py) that produces a `dot` file"
   },
   {
      "null": 60,
      "x": "Add a torch.matmul function and document broadcast behavior of it and delegated functions",
      "z": "Yes, `matmul` is present in master and in v0.2",
      "y": "Yes, `matmul` is present in master and in v0.2"
   },
   {
      "null": 61,
      "x": "Add SELU (Scaled ELU)",
      "z": "It looks like it can be implemented somewhat efficiently in one line:\n```python\nimport torch.nn.functional as F\ndef selu(x):\n alpha = 1.6732632423543772848170429916717\n scale = 1.0507009873554804934193349852946\n return scale * F.elu(x, alpha)\n```\nNote that pytorch `elu` has the `alpha` parameter built-in, which makes it easier to implement than in TF. For an overview of what the `alpha` parameter do, you can have a [look here](https://github.com/pytorch/pytorch/blob/master/torch/lib/THNN/generic/ELU.c#L23).",
      "y": "It looks like it can be implemented somewhat efficiently in one line: ```python import torch.nn.functional as F def selu(x): alpha = 1.6732632423543772848170429916717 scale = 1.0507009873554804934193349852946 return scale * F.elu(x, alpha) ```"
   },
   {
      "null": 62,
      "x": "What to do if CUDA doesn't work",
      "z": "> where it used to work earlier and it didn't all of a sudden. Before trying any of the solutions I restarted my computer, and it worked fine.\n\nThis usually happens when the nvidia driver gets updated.",
      "y": "where it used to work earlier and it didn't all of a sudden. Before trying any of the solutions I restarted my computer, and it worked fine. This usually happens when the nvidia driver gets updated."
   },
   {
      "null": 63,
      "x": "Missing bernoulli_ on torch.cuda.ByteTensor for nn.AlphaDropout",
      "z": "So I think we should really remove `bernoulli_` as it's very ambiguous. In this case, I think we should use `torch.bernoulli(p, out=input.data.new().byte())` (until we add `dtype`)",
      "y": "we should really remove `bernoulli_` as it's very ambiguous. In this case, I think we should use `torch.bernoulli(p, out=input.data.new().byte())` (until we add `dtype`)"
   },
   {
      "null": 64,
      "x": "RuntimeError: invalid multinomial distribution (encountering probability entry < 0)",
      "z": "When this error happens, probs_2d equals to `tensor([[nan, nan, nan, nan, nan, nan, nan]])`",
      "y": "When this error happens, probs_2d equals to `tensor([[nan, nan, nan, nan, nan, nan, nan]])`"
   },
   {
      "null": 65,
      "x": "Exception using optimize_for_mobile on retinanet from torchvision",
      "z": "Fixed on master with: https://github.com/pytorch/pytorch/pull/46285",
      "y": "Fixed on master"
   },
   {
      "null": 66,
      "x": "The return of torch.inverse contains nan sometime",
      "z": "You can still use multi-stream if you properly register all the tensors to the correct streams",
      "y": "You can still use multi-stream if you properly register all the tensors to the correct streams"
   },
   {
      "null": 67,
      "x": "[jit] create dict or list from zip",
      "z": "fixed with https://github.com/pytorch/pytorch/pull/42382",
      "y": "Fixed on master"
   },
   {
      "null": 68,
      "x": "CUDAExtension: nvcc does not pick right gcc by default",
      "z": "It seems like that this issue has been fixed by https://github.com/pytorch/pytorch/commit/4134b7abfa60a659de27736704c62277ecf291d2#diff-258d5272916c8cf5ac89aad77f7c114b585848826f6bbaeee91fe62a4391ee5e",
      "y": "Fixed on master"
   },
   {
      "null": 69,
      "x": "Update how to build PyTorch with CUDA Windows instructions",
      "z": "@mszhanyi I think we should just say that we support cuda >= 10.1 and vs >= 2019.",
      "y": "I think we should just say that we support cuda >= 10.1 and vs >= 2019."
   },
   {
      "null": 70,
      "x": "Unable to create TorchScript compatible CUDA extension",
      "z": "@wanchaol I already have a fix https://github.com/pytorch/pytorch/pull/47492 and a test https://github.com/pytorch/pytorch/pull/47524",
      "y": "Fixed on master"
   },
   {
      "null": 71,
      "x": "Migrate `set_` from the TH to Aten (CUDA)",
      "z": "https://github.com/pytorch/pytorch/pull/34403",
      "y": "Fixed in Master"
   },
   {
      "null": 72,
      "x": "Support cast of single-element tensors to numbers",
      "z": "Yup \n\n```python\n>>> import numpy as np\n>>> zero_dim = np.array(3)\n>>> scalar = np.float(3)\n>>> single_elem = np.array([[[3]]])\n>>> float(zero_dim), float(scalar), float(single_elem)\n(3.0, 3.0, 3.0)\n>>> empty = np.array([], dtype=np.float32)\n>>> float(empty)\nTraceback (most recent call last):\n File \"<stdin>\", line 1, in <module>\nTypeError: only length-1 arrays can be converted to Python scalars\n\n```",
      "y": "Yup \n\n```python\n>>> import numpy as np\n>>> zero_dim = np.array(3)\n>>> scalar = np.float(3)\n>>> single_elem = np.array([[[3]]])\n>>> float(zero_dim), float(scalar), float(single_elem)\n(3.0, 3.0, 3.0)\n>>> empty = np.array([], dtype=np.float32)\n>>> float(empty)\nTraceback (most recent call last):\n File \"<stdin>\", line 1, in <module>\nTypeError: only length-1 arrays can be converted to Python scalars\n\n```"
   },
   {
      "null": 73,
      "x": "CUDNN_STATUS_NOT_INITIALIZED",
      "z": "pytorch ships it's own cudnn",
      "y": "pytorch ships it's own cudnn"
   },
   {
      "null": 74,
      "x": "Tensor flattening on broadcast doesn't handle heterogeneous types",
      "z": "fixed in master. will be in next release.",
      "y": "fixed in master"
   },
   {
      "null": 75,
      "x": "Anaconda installation CUDA requested, cpuonly obtained",
      "z": "In case someone still runs into this: try setting `conda config --set channel_priority strict`.\n\nA similar case was reported on Slack. After some investigation, I could trigger the cpu-only install by first installing 1.5.0 and then downgrading to 1.4.0 without having `channel_priority strict` set. \n\nThis will install the `cuda10.1` package:\n```\nconda create -n pytorch14\nconda activate pytorch14\nconda install pytorch==1.4.0 -c pytorch\n```\n\nStarting again in a clean env:\n```\nconda create -n pytorch14\nconda activate pytorch14\nconda install pytorch==1.5.0 -c pytorch # installs cuda10.1 version\nconda install pytorch==1.4.0 -c pytorch # wants to install cpu_only, say \"no\"\n\nconda config --set channel_priority strict\nconda install pytorch==1.4.0 -c pytorch # installs cuda10.1 version\n```",
      "y": "In case someone still runs into this: try setting `conda config --set channel_priority strict`.\n\nA similar case was reported on Slack. After some investigation, I could trigger the cpu-only install by first installing 1.5.0 and then downgrading to 1.4.0 without having `channel_priority strict` set. \n\nThis will install the `cuda10.1` package:\n```\nconda create -n pytorch14\nconda activate pytorch14\nconda install pytorch==1.4.0 -c pytorch\n```\n\nStarting again in a clean env:\n```\nconda create -n pytorch14\nconda activate pytorch14\nconda install pytorch==1.5.0 -c pytorch # installs cuda10.1 version\nconda install pytorch==1.4.0 -c pytorch # wants to install cpu_only, say \"no\"\n\nconda config --set channel_priority strict\nconda install pytorch==1.4.0 -c pytorch # installs cuda10.1 version\n```"
   },
   {
      "null": 76,
      "x": "how to install pytorch on AMD GPU",
      "z": "What about `python -m pip install torch==1.4.0+cpu torchvision==0.5.0+cpu -f https://download.pytorch.org/whl/torch_stable.html`?",
      "y": "`python -m pip install torch==1.4.0+cpu torchvision==0.5.0+cpu -f https://download.pytorch.org/whl/torch_stable.html`?"
   },
   {
      "null": 77,
      "x": "AttributeError: module 'torch.nn.utils' has no attribute 'prune'",
      "z": "you need to `import torch.nn.utils.prune` not only `import torch`",
      "y": "you need to `import torch.nn.utils.prune` not only `import torch`"
   },
   {
      "null": 78,
      "x": "Flaky test test_cos_scalar_cpu TestAutogradDeviceTypeCPU on MacOS",
      "z": "@albanD and I agreed that we should just zero initialize `tmp_values` in this case. Since the `count != size()` case is only used for the ragged end it's fine if we add a few more instructions here, since it's not in the tight loop.",
      "y": "we should just zero initialize `tmp_values` in this case. Since the `count != size()` case is only used for the ragged end it's fine if we add a few more instructions here, since it's not in the tight loop."
   },
   {
      "null": 79,
      "x": "Numerical problems with torch.nn.functional.kl_div",
      "z": "Ok, it seems I know where the issue is. The issue is with `log(softmax)`. Computing `log(softmax)` in a straightforward fashion is not numerically stable, we can rescale a numerator and a denominator by any exponent prior to taking `log` and take into account that we actually deal with ratios. `softmax/log_softmax` perform this rescaling by premultiplying with `exp{max prob}` and deals with subtractions instead of divisions...\nSo, probabilities might have some structure to which the KL-divergence implementation is oblivious. It could be useful to accept `target` in both the original space and the log-space if the direct computation of `log(target)` is not optimal.",
      "y": "The issue is with `log(softmax)`. Computing `log(softmax)` in a straightforward fashion is not numerically stable, we can rescale a numerator and a denominator by any exponent prior to taking `log` and take into account that we actually deal with ratios. `softmax/log_softmax` perform this rescaling by premultiplying with `exp{max prob}` and deals with subtractions instead of divisions... So, probabilities might have some structure to which the KL-divergence implementation is oblivious. It could be useful to accept `target` in both the original space and the log-space if the direct computation of `log(target)` is not optimal."
   },
   {
      "null": 80,
      "x": "Instantiating `torch.distributions.Categorical` with all-zero long-dtype probabilities crashes Python",
      "z": "It's the divide-by-0 behavior. `torch.zeros(1,dtype=torch.float) / 0` gives `nan`, `torch.zeros(1,dtype=torch.long) / 0` crashes.",
      "y": "It's the divide-by-0 behavior. `torch.zeros(1,dtype=torch.float) / 0` gives `nan`, `torch.zeros(1,dtype=torch.long) / 0` crashes."
   },
   {
      "null": 81,
      "x": "RuntimeError: cuDNN error: CUDNN_STATUS_NOT_SUPPORTED. This error may appear if you passed in a non-contiguous input",
      "z": "It happens to me as well with \npytorch=1.5.0\npy3.8\ncuda10.1.243\ncudnn7.6.3_0\n\nIt happens when I reach a batch normalization layer with a huge batch size, but when I decrease the batch size the error is gone. It is probably a memory issue that happens when a batch is too big.",
      "y": "It happens when I reach a batch normalization layer with a huge batch size, but when I decrease the batch size the error is gone. It is probably a memory issue that happens when a batch is too big."
   },
   {
      "null": 82,
      "x": "Input and output tensors to `torch._C.Node`",
      "z": "You can use `.inputs()` and `.outputs()` to get the input and output values, but there are no tensors since you're inspecting a representation of a program that's not running yet. Those APIs are internal, so we won't be writing documentation for them just yet.",
      "y": "You can use `.inputs()` and `.outputs()` to get the input and output values, but there are no tensors since you're inspecting a representation of a program that's not running yet. Those APIs are internal, so we won't be writing documentation for them just yet."
   },
   {
      "null": 83,
      "x": "RuntimeError: CUDA out of memory. Tried to allocate 12.50 MiB (GPU 0; 10.92 GiB total capacity; 8.57 MiB already allocated; 9.28 GiB free; 4.68 MiB cached)",
      "z": "It is because of mini-batch of data does not fit on to GPU memory. Just decrease the batch size. When I set batch size = 256 for cifar10 dataset I got the same error; Then I set the batch size = 128, it is solved.",
      "y": "It is because of mini-batch of data does not fit on to GPU memory. Just decrease the batch size. When I set batch size = 256 for cifar10 dataset I got the same error; Then I set the batch size = 128, it is solved."
   },
   {
      "null": 84,
      "x": "tensor multinomial dependent on shape for some reason",
      "z": "For me, it was \n```\nimport torch\nimport numpy as np\nfoo3 = torch.from_numpy(np.array([[[0.25, 0.25, 0.25, 0.25]]]))\ntorch.multinomial(foo3, 10, True)\n# tensor([[0, 0, 0, 0, 0, 0, 0, 0, 0, 0]])\n```",
      "y": "For me, it was \n```\nimport torch\nimport numpy as np\nfoo3 = torch.from_numpy(np.array([[[0.25, 0.25, 0.25, 0.25]]]))\ntorch.multinomial(foo3, 10, True)\n# tensor([[0, 0, 0, 0, 0, 0, 0, 0, 0, 0]])\n```"
   },
   {
      "null": 85,
      "x": "Implement target derivative for smooth L1 loss",
      "z": "In the meantime, you can implement `smooth_l1_loss` via the following:\n```python\ndef smooth_l1_loss(input, target, beta=1, size_average=True):\n \"\"\"\n very similar to the smooth_l1_loss from pytorch, but with\n the extra beta parameter\n \"\"\"\n n = torch.abs(input - target)\n cond = n < beta\n loss = torch.where(cond, 0.5 * n ** 2 / beta, n - 0.5 * beta)\n if size_average:\n return loss.mean()\n return loss.sum()\n```",
      "y": "In the meantime, you can implement `smooth_l1_loss` via the following:\n```python\ndef smooth_l1_loss(input, target, beta=1, size_average=True):\n \"\"\"\n very similar to the smooth_l1_loss from pytorch, but with\n the extra beta parameter\n \"\"\"\n n = torch.abs(input - target)\n cond = n < beta\n loss = torch.where(cond, 0.5 * n ** 2 / beta, n - 0.5 * beta)\n if size_average:\n return loss.mean()\n return loss.sum()\n```"
   },
   {
      "null": 86,
      "x": "When can PyTorch support for RTX series GPU?",
      "z": "closing this issue, because PyTorch now works and ships with CUDA10.",
      "y": "PyTorch works and ships with CUDA10."
   },
   {
      "null": 87,
      "x": "Naming conflict of `test_api` in Windows release builds for wheels",
      "z": "@JerrikEph It is actually mkldnn that is out of sync. So running the following script should help.\n```bash\ngit submodule sync\ngit submodule update --init --recursive\n```",
      "y": "@JerrikEph It is actually mkldnn that is out of sync. So running the following script should help.\n```bash\ngit submodule sync\ngit submodule update --init --recursive\n```"
   },
   {
      "null": 88,
      "x": "Lack of Square Function",
      "z": "In the one hand, we have a `sqrt` function, so adding a `square` function makes sense. In the other hand, I'd pretty much prefer avoid bloating the API with \"redundant\" functions (and we get the same behavior with less typing by just doing `a ** 2`).\nBut I'll let @soumith decide on that.",
      "y": "In the one hand, we have a `sqrt` function, so adding a `square` function makes sense. In the other hand, I'd pretty much prefer avoid bloating the API with \"redundant\" functions (and we get the same behavior with less typing by just doing `a ** 2`)."
   },
   {
      "null": 89,
      "x": "Combine Variable and Tensor APIs (Perform autograd directly on torch.Tensor)",
      "z": "This is done. (Still some clean-up to do)",
      "y": "Fixed in Master"
   },
   {
      "null": 90,
      "x": "How to convert to older version of pytorch v0.1.12?",
      "z": "`conda install pytorch=0.1.12 cuda80 -c soumith`\nWe'll update the docs soon to give links to older versions.",
      "y": "`conda install pytorch=0.1.12 cuda80 -c soumith`\n"
   },
   {
      "null": 91,
      "x": "add documentation / links for old binaries",
      "z": "you can find the linux wheels\n- http://download.pytorch.org/whl/cu80/torch-0.1.12.post2-cp27-none-linux_x86_64.whl (cuda 8)\n- http://download.pytorch.org/whl/cu75/torch-0.1.12.post2-cp27-none-linux_x86_64.whl (cuda 7.5)\n\nand the mac wheel: \n- http://download.pytorch.org/whl/torch-0.1.12.post2-cp27-none-macosx_10_7_x86_64.whl",
      "y": "you can find the linux wheels\n- http://download.pytorch.org/whl/cu80/torch-0.1.12.post2-cp27-none-linux_x86_64.whl (cuda 8)\n- http://download.pytorch.org/whl/cu75/torch-0.1.12.post2-cp27-none-linux_x86_64.whl (cuda 7.5)\n\nand the mac wheel: \n- http://download.pytorch.org/whl/torch-0.1.12.post2-cp27-none-macosx_10_7_x86_64.whl"
   },
   {
      "null": 92,
      "x": "datasets.ImageFolder error \"in __getattr__ raise AttributeError(name) AttributeError: __exit__\"",
      "z": "I had the same issue, and upgrading the Pillow library resolved it.\nTry this:\n`pip install --upgrade Pillow `",
      "y": "I had the same issue, and upgrading the Pillow library resolved it.\nTry this:\n`pip install --upgrade Pillow `"
   },
   {
      "null": 93,
      "x": "RuntimeError: cuda runtime error (46) : all CUDA-capable devices are busy or unavailable at /opt/conda/conda-bld/pytorch_1502004572321/work/torch/lib/THC/generic/THCStorage.cu:66",
      "z": "Restart the program with os.environ[\"CUDA_VISIBLE_DEVICES\"] = '0'",
      "y": "Restart the program with os.environ[\"CUDA_VISIBLE_DEVICES\"] = '0'"
   },
   {
      "null": 94,
      "x": "CondaError: CondaHTTPError: HTTP 000 CONNECTION FAILED for url <https://conda.anaconda.org/soumith/linux-64/pytorch-0.2.0-py35hb1547bd_4cu80.tar.bz2>",
      "z": "hmmm, could you try the `pip` based install (see instructions on pytorch.org ). Maybe conda.anaconda.org is blocked in your network.\nSince there is not much we can do at pytorch end, i am closing the issue.",
      "y": "could you try the `pip` based install (see instructions on pytorch.org ). Maybe conda.anaconda.org is blocked in your network."
   },
   {
      "null": 95,
      "x": "Same ADD operation, Tensor has a different result from Numpy",
      "z": "> @siyuhsu did you close this issue on purpose?\n\nThis problem is solved by upgrading PyTorch version. Only Pytorch `1.3.0` has this problem. Please see https://github.com/pytorch/pytorch/issues/54650#issuecomment-806389825",
      "y": "This problem is solved by upgrading PyTorch version."
   },
   {
      "null": 96,
      "x": "`@torch.jit.ignore` and `@property`",
      "z": "`@ignore` = cannot run in TorchScript and is executed in Python if called, disables export\n`@unused` = cannot run in TorchScript nor Python and throws an error if called",
      "y": "`@ignore` = cannot run in TorchScript and is executed in Python if called, disables export\n`@unused` = cannot run in TorchScript nor Python and throws an error if called"
   },
   {
      "null": 97,
      "x": "torch.linalg.svd out of memory",
      "z": "The code in LinearAlgebraUtils.h for svd is wrong it incorrectly initializes large matrix and then narrows it if full_matrices=False. The whole svd code was not refactored during recent linalg updates, only linalg_svd wrapper of the old code was added.",
      "y": "The code in LinearAlgebraUtils.h for svd is wrong it incorrectly initializes large matrix and then narrows it if full_matrices=False. The whole svd code was not refactored during recent linalg updates, only linalg_svd wrapper of the old code was added."
   },
   {
      "null": 98,
      "x": "Please verify ONNX v1.10.0 Release Candidate",
      "z": "Tests passed locally with onnx submodule at a57bc99daa6ddeef2ad535f8f78d1847f57216f0, which I guess is two commits behind RC2, but the 2 missing commits seem not problematic.\nI ran:\n`python test/onnx/test_pytorch_onnx_onnxruntime.py`",
      "y": "Tests passed locally with onnx submodule at a57bc99daa6ddeef2ad535f8f78d1847f57216f0, which I guess is two commits behind RC2, but the 2 missing commits seem not problematic.\nI ran:\n`python test/onnx/test_pytorch_onnx_onnxruntime.py`"
   },
   {
      "null": 99,
      "x": "Unable to install 1.2",
      "z": "Pytorch 1.2 just went live on conda for Windows",
      "y": "Pytorch 1.2 just went live on conda for Windows"
   },
   {
      "null": 100,
      "x": "problem with torch.util.tensorboard add_graph()",
      "z": "I too am getting a graph page that is empty. I did flush and close the SummaryWriter.\nAttaching screenshot including Chrome's console which shows an error that may be related.\n\nNote:\n* I do see the textual graph being dumped to the command line console and it seems correct there.\n\n**Configuration:**\n* PyTorch 1.2.0\n* TensoBoard 1.14.0\n* Python 3.5.2\n\n![image](https://user-images.githubusercontent.com/345348/63727390-07120280-c815-11e9-9439-e9ee0827ebd4.png)",
      "y": "I too am getting a graph page that is empty. I did flush and close the SummaryWriter.\nAttaching screenshot including Chrome's console which shows an error that may be related.\n\nNote:\n* I do see the textual graph being dumped to the command line console and it seems correct there.\n\n**Configuration:**\n* PyTorch 1.2.0\n* TensoBoard 1.14.0\n* Python 3.5.2\n\n![image](https://user-images.githubusercontent.com/345348/63727390-07120280-c815-11e9-9439-e9ee0827ebd4.png)"
   },
   {
      "null": 101,
      "x": "Implement torch.uniform",
      "z": "I think this already exists?\n\n```python\na = torch.tensor([0., 10.])\nb = torch.tensor([1., 11.])\ntorch.distributions.Uniform(a, b).sample()\n>>> tensor([ 0.8583, 10.0226])\n```",
      "y": "\n```python\na = torch.tensor([0., 10.])\nb = torch.tensor([1., 11.])\ntorch.distributions.Uniform(a, b).sample()\n>>> tensor([ 0.8583, 10.0226])\n```"
   },
   {
      "null": 102,
      "x": "How to install torchaudio on Mac M1 ARM?",
      "z": "Hi, this issue is being tracked on the torchaudio repository, https://github.com/pytorch/audio/issues/1573, please add input there.",
      "y": "Hi, this issue is being tracked on the torchaudio repository, https://github.com/pytorch/audio/issues/1573"
   },
   {
      "null": 103,
      "x": "Pytorch 1.2.0 RuntimeError: code is too big",
      "z": "The fix is released in [Intel MKL-DNN v0.20.2](https://github.com/intel/mkl-dnn/releases/tag/v0.20.2). \n\n@gujinghui, @jgong5, could you please help with instruction on building Pytorch with updated Intel MKL-DNN?",
      "y": "The fix is released in [Intel MKL-DNN v0.20.2](https://github.com/intel/mkl-dnn/releases/tag/v0.20.2)."
   },
   {
      "null": 104,
      "x": "Very Slow Moving Tensor to CUDA device (CUDA 10.1 with PyTorch 1.3)",
      "z": "This issue is now fixed with newly updated binaries.\nUninstalling and reinstalling PyTorch from Anaconda will fix it.",
      "y": "This issue is now fixed with newly updated binaries.\nUninstalling and reinstalling PyTorch from Anaconda will fix it."
   },
   {
      "null": 105,
      "x": "Indexed, in-place multiplication segfaults/drops values",
      "z": "Yes, it has been fixed.",
      "y": "it has been fixed."
   },
   {
      "null": 106,
      "x": "Install from source on Centos7 doesn't work",
      "z": "Have you tried the following?\n* `python setup.py clean --all` before compiling\n* creating a new conda environment to compile in, using `conda create -n pytorch python=3.6` and `conda activate pytorch`\n* compiling without CUDA: `NO_CUDA=1 python setup.py install`\n* making sure you are on the lastest master commit",
      "y": "Have you tried the following?\n* `python setup.py clean --all` before compiling\n* creating a new conda environment to compile in, using `conda create -n pytorch python=3.6` and `conda activate pytorch`\n* compiling without CUDA: `NO_CUDA=1 python setup.py install`\n* making sure you are on the lastest master commit"
   },
   {
      "null": 107,
      "x": "Cholesky Error for positive definite matrix",
      "z": "FYI, Cholesky algorithms work independent of condition number, as long as the input is symmetric and positive definite.",
      "y": "Cholesky algorithms work independent of condition number, as long as the input is symmetric and positive definite."
   },
   {
      "null": 108,
      "x": "libtorch cannot find CUDA",
      "z": "I found that if I set CUDA_HOME in already opened terminal, then cmake fails to find CUDA. We should add the export lines in .bashrc",
      "y": " if I set CUDA_HOME in already opened terminal, then cmake fails to find CUDA. We should add the export lines in .bashrc"
   },
   {
      "null": 109,
      "x": "Does tensors got from torch.distributed.all_gather in order?",
      "z": "> To answer your question, though: yes, that is correct. But double check the API for `all_gather`, since you don't get back a single tensor, but a list of tensors. If you want to combine them into a single tensor, check out [`torch.cat`](https://pytorch.org/docs/stable/torch.html#torch.cat).\n\nHi, thank you for answering. But in our expeiments, it looks like returned in random order. Is it a bug?",
      "y": "yes, that is correct. But double check the API for `all_gather`, since you don't get back a single tensor, but a list of tensors. If you want to combine them into a single tensor, check out [`torch.cat`](https://pytorch.org/docs/stable/torch.html#torch.cat)."
   },
   {
      "null": 110,
      "x": "What is the torchvision version for pytorch-nightly? Use 0.3.0 to report errors",
      "z": "It should be `torchvision-nightly` that matches with `pytorch-nightly`, but they are not provided currently.",
      "y": "It should be `torchvision-nightly` that matches with `pytorch-nightly`, but they are not provided currently."
   },
   {
      "null": 111,
      "x": "running into error installing from source",
      "z": "What cudnn version are you using? Try cudnn v6.",
      "y": "Try cudnn v6."
   },
   {
      "null": 112,
      "x": "padding_idx doesn't work.",
      "z": "Remove this line: e.weight = nn.Parameter(torch.rand(3, 2))\nFrom nn.Embedding source, you could see:\n```\n if self.padding_idx is not None:\n self.weight.data[self.padding_idx].fill_(0)\n```",
      "y": "Remove this line: e.weight = nn.Parameter(torch.rand(3, 2))\nFrom nn.Embedding source, you could see:\n```\n if self.padding_idx is not None:\n self.weight.data[self.padding_idx].fill_(0)\n```"
   },
   {
      "null": 113,
      "x": "Undefined symbol \"_state\"",
      "z": "Are you sure this bug only appears on machines without CUDA? That's strange because my machine does have CUDA 8 (as proof, I have tensorflow-gpu installed and running). I think my cuda files live in `/usr/local/cuda/lib` and not `/usr/local/cuda/lib64`, but I looked through the default linker arguments and it looks like we include both as -L options?\n\nAlso, I started using `MACOSX_DEPLOYMENT_TARGET=10.11` instead of `10.9` because I realized that was the wrong version, but it didn't fix anything.\n\nAlso, I ran `install_name_tool -add_rpath /usr/local/cuda/lib ~/Desktop/pytorch/torch/lib/tmp_install/lib/libTHPP.1.dylib` and this seemed to solve the error listed in my edit.",
      "y": null
   },
   {
      "null": 114,
      "x": "pack_padded_sequence output is not as expected",
      "z": "This is expected. The (unusual) format of a `PackedSequence` object is the way it is because of the API of cuDNN's RNN/LSTM functions, which expect a time-sequence of variable-length batches rather than a batch-sequence of variable-length samples. If your samples are `a`, `bc`, and `defg` the `PackedSequence` will contain the packed tensor `abdcefg` and a list of lengths `[3, 2, 1, 1]`.",
      "y": null
   },
   {
      "null": 115,
      "x": "batch matrix-vector multiplication (bmv)",
      "z": "according to broadcasting rules, batch2 wont auto-expand because the expansion is to not prepend dimensions but to append in this case.\n\nI believe you can do:\n`batch2 = batch2.unsqueeze(2)`",
      "y": null
   },
   {
      "null": 116,
      "x": "nonzero doesn't squeeze dimension",
      "z": "\ud83d\udc4d for making `nonzero()` compatible with numpy. Current nonzero() output cannot be used for indexing e.g. `z[x.nonzero()] = 2.0` would be convenient. AFAIK it cannot be used with index_select or index_fill as well.",
      "y": null
   },
   {
      "null": 117,
      "x": "numerical stability for logSigmoid",
      "z": "Using function below seems to be a better choice.\n\ndef log_sigmoid(x):\n return torch.clamp(x, max=0)-torch.log(torch.exp(-torch.abs(x))+1)\n\nUpdate: The gradient at x=0 for function above is not the same as log sigmoid. I just add an additional term to eliminate the difference.\ndef log_sigmoid(x):\n return torch.clamp(x, max=0) - torch.log(torch.exp(-torch.abs(x)) + 1) + 0.5 * torch.clamp(x, min=0, max=0)",
      "y": null
   },
   {
      "null": 118,
      "x": "OpenCV get Stuck in Transform when used in DataLoader",
      "z": "Getting hit by this too on Ubuntu. On `cv2.warpPerspective`. What is weird about this is that the 1st epoch runs fine but the second epoch get stuck as soon as an item get transformed with `cv2.warpPerspective` from the `getitem()` Dataloader. There must be some kind of bad deadlock somewhere.\n\nI ended up using: `multiprocessing.set_start_method('spawn', force=True)`",
      "y": null
   },
   {
      "null": 119,
      "x": "ModuleNotFoundError: No module named 'torch.autograd'",
      "z": "pip install torchvision \n\nTry This and Be happy ... Its working",
      "y": null
   },
   {
      "null": 120,
      "x": "torch.std giving incorrect results",
      "z": "They are correct. The difference lies in [Bessel's correction](https://en.wikipedia.org/wiki/Bessel%27s_correction).",
      "y": null
   },
   {
      "null": 121,
      "x": "torch.manual_seed raises an error in forked processes",
      "z": "No. It will raise an error after fork (stopping you from reusing the same CUDA context, which is UB anyway), and will seed a process-local generator if you use spawn",
      "y": null
   },
   {
      "null": 122,
      "x": "segfault in python multithreaded setting",
      "z": "My code had almost the same structure as @Louis-Tian's example, and I was able to get around it by putting a lock where I instantiate my module in each thread. Working code below (pay attention to the lock)\n\n```python\nimport torch\nimport threading\nimport torch.functional as f\nfrom concurrent.futures import ThreadPoolExecutor as ThreadPool\n\n\ndef build(cuda=False):\n nn = torch.nn.Sequential(\n torch.nn.Linear(1024, 1024),\n torch.nn.Linear(1024, 1)\n )\n\n return nn.cuda() if cuda else nn\n\ndef train(nn, X, y, epoch=100):\n X = torch.autograd.Variable(X)\n y = torch.autograd.Variable(y)\n optim = torch.optim.SGD(nn.parameters(), lr=0.1)\n for i in range(epoch):\n yhat = nn(X)\n loss = ((yhat - y) ** 2).mean()\n loss.backward()\n optim.step()\n\ndef data(cuda=False):\n X = torch.zeros(10, 1024)\n y = torch.zeros((10, 1))\n return (X.cuda(), y.cuda()) if cuda else (X, y)\n\ndef cpu_run(lock):\n with lock:\n nn = build(cuda=False)\n d = data(cuda=False)\n train(nn, *d)\n\ndef thread_cpu_run():\n pool = ThreadPool()\n lock = threading.Lock()\n threads = pool.map(cpu_run, [lock for _ in range(5)])\n\n return list(threads)\n\nthread_cpu_run()\n```",
      "y": null
   },
   {
      "null": 123,
      "x": "share_memory_ needs a check for empty tensor",
      "z": "I think the new release should be out by next week.",
      "y": null
   },
   {
      "null": 124,
      "x": "get error when load checkpoint and finetune with optimizer, RuntimeError: expected backend CPU and dtype Float but got backend CUDA and dtype Float",
      "z": "~~Experiencing the same issue, would love to know how to solve this please~~\n\nEdit: I found a solution after reading this thread:\nhttps://discuss.pytorch.org/t/code-that-loads-sgd-fails-to-load-adam-state-to-gpu/61783\nThe solution marked in this thread is the key to solving this issue.\n\nI ended up moving the model to the device just before loading the state of the optimizer:\nSomething like this:\n```\nself.model.load_state_dict(checkpoint['model_state_dict'])\nself.model.to(device)\nself.optimizer.load_state_dict(checkpoint['optimizer_state_dict'])\nself.scheduler.load_state_dict(checkpoint['scheduler_state_dict'])\n```",
      "y": null
   },
   {
      "null": 125,
      "x": "Legacy autograd function with non-static forward method is deprecated and will be removed in 1.3 and UserWarning: Legacy autograd function object was called twice. You will probably get incorrect gradients from this computation, as the saved tensors from the second invocation will clobber the saved tensors from the first invocation. Please consider rewriting your autograd function in the modern style; for information on the new format, please see: https://pytorch.org/docs/stable/notes/extending.html#extending-torch-autograd 626",
      "z": "i have the same problem. did you solve it?\n\nLegacy autograd function with non-static forward method is deprecated. Please use new-style autograd function with static forward method. (Example: https://pytorch.org/docs/stable/autograd.html#torch.autograd.Function)",
      "y": null
   },
   {
      "null": 126,
      "x": "Missing \u2018get_trace_graph\u2019 function in Torch1.4",
      "z": "Yes, `get_trace_graph` was an internal function that was intended for use by ONNX export only. As an alternative, please get the graph like: \n```\ntraced = torch.jit.trace(model, inputs)\ntraced_graph = traced.graph\n```\nLet me know if you're having any issues!",
      "y": null
   },
   {
      "null": 127,
      "x": "CUDA for Pytorch",
      "z": "PyTorch packages doesn't rely on the global CUDA installation. For conda packages, it relies on a conda package called `cudatoolkit`. For wheels packages, the CUDA libraries are installed locally.",
      "y": null
   },
   {
      "null": 128,
      "x": "Turn on exact_dtype by default on test_sparse.py",
      "z": "(This task is allocated to a current bootcamper, please reach out to me before claiming it.)",
      "y": null
   },
   {
      "null": 129,
      "x": "Any reason to keep AT_WARN?",
      "z": "Ye, we should rename all of them and delete `AT_WARN` entirely. The problem is we don't have a Werror build so people keep finger macroing the old name when they add warnings. So we have to rip the bandaid off entirely to keep it fixed.",
      "y": null
   },
   {
      "null": 130,
      "x": "I could male libtorch dll ,lib from soruce in Windows , but build error happened in build for mnist.cpp",
      "z": "BTW, if you are using a GPU version of LibTorch, you need to link against `c10.lib`, `c10_cuda.lib`, `torch_cpu.lib` and `torch_cuda.lib` at the same time.",
      "y": null
   },
   {
      "null": 131,
      "x": "tensor._copy should not copy when src and dst point to the same memory",
      "z": "Use of `.data` is discouraged. As @albanD suggests, if you don't want some operations tracked by autograd, you should wrap them in `no_grad()` context manager. Once you do that, `x.copy_(y)` passes `is_same()` check and no memcopy is invoked.",
      "y": null
   },
   {
      "null": 132,
      "x": "Different gradients with batch norm on GPU and CPU",
      "z": "The last suggestion produces errors < 1e-2, which is acceptable. Thanks for your help!",
      "y": null
   },
   {
      "null": 133,
      "x": "Didn't find engine for operation quantized::conv_prepack NoQEngine (operator () at ..\\aten\\src\\ATen\\native\\quantized\\cpu\\qconv_prepack.cpp:264) (no backtrace available)",
      "z": "@uchihaltachi If you are on Windows, then you'll need to update to 1.5.0.",
      "y": null
   },
   {
      "null": 134,
      "x": "RuntimeError: Could not run 'aten::thnn_conv2d_forward' with arguments from the 'QuantizedCPUTensorId' backend. 'aten::thnn_conv2d_forward' is only available for these backends: [CPUTensorId, VariableTensorId].",
      "z": "I faced the same issue, when I run:\n```python\nmodel.eval()\nmodel.qconfig = torch.quantization.get_default_qconfig('fbgemm')\nmodel.fuse_model()\nmodel_backbone_prep = torch.quantization.prepare(model)\nsample_input = torch.rand(1, 3, 224, 224, device='cpu')\nmodel_backbone_prep(sample_input)\nq_model = torch.quantization.convert(model_backbone_prep)\nq_model(sample_input)\n```\n\nthis is the error:\n```\nRuntimeError: Could not run 'aten::thnn_conv2d_forward' with arguments from the 'QuantizedCPU' backend. This could be because the operator doesn't exist for this backend, or was omitted during the selective/custom build process (if using custom build). If you are a Facebook employee using PyTorch on mobile, please visit https://fburl.com/ptmfixes for possible resolutions. 'aten::thnn_conv2d_forward' is only available for these backends: [CPU, CUDA, BackendSelect, Named, AutogradOther, AutogradCPU, AutogradCUDA, AutogradXLA, AutogradNestedTensor, AutogradPrivateUse1, AutogradPrivateUse2, AutogradPrivateUse3, Tracer, Autocast, Batched, VmapMode].\n```\n\n`model` is a [resnet101](https://github.com/facebookresearch/detectron2/blob/2e589a1b38f957172a41fd895889afc5b7755323/detectron2/modeling/backbone/resnet.py), set on CPU device.\n\nPyTorch version 1.8\nCUDA 11\npython 3.8\nOS: ubuntu \n\n@jerryzh168 What do you suggest?",
      "y": null
   },
   {
      "null": 135,
      "x": "Could you please add a torch setting like torch.set_checkpoint_enabled(True)",
      "z": "Thank you for the suggestion. We would be willing to accept a PR enabling this feature",
      "y": null
   },
   {
      "null": 136,
      "x": "DISABLED test_keypoint_rcnn (__main__.TestONNXRuntime_opset11)",
      "z": "@eellison - are there are any other ONNX tests besides this one that is being disabled because of your interpolate PR?",
      "y": null
   },
   {
      "null": 137,
      "x": "ImportError: cannot import name 'GoogLeNetOutputs' from 'torchvision.models.googlenet'",
      "z": "> @ninjakx I am getting the same error. How to resolve that?\n\nif you dont use quantizaton, delete this import can make it work",
      "y": null
   },
   {
      "null": 138,
      "x": "torch.distributions.Categorical.sample uses unnecessary huge amount of memory",
      "z": "@jjabo `[500,B]` is correct in your example since `sample_shape` is by convention left of `batch_shape`. From a [tutorial](https://pyro.ai/examples/tensor_shapes.html#Distributions-shapes:-batch_shape-and-event_shape):\n```\n | iid | independent | dependent\n------+--------------+-------------+------------\nshape = sample_shape + batch_shape + event_shape\n```",
      "y": null
   },
   {
      "null": 139,
      "x": "[FR] More consistent matrix norm for torch.norm",
      "z": "It does seem like this may merit a BC-breaking change. Some things we should check before doing so: (1) when was this consistency introduced, (2) is the inconsistent behavior tested for in our test suite, or is this actually totally untested.\n\nThis seems like a case where we can easily give a deprecation warning, since the forward compatible way of batching (specify an axis explicitly) works today,",
      "y": null
   },
   {
      "null": 140,
      "x": "Pylint Error `torch.tensor is not callable`",
      "z": "This error is still popping out even with \n```\n[TYPECHECK]\ngenerated-members=numpy.*,torch.*\n```\nin the `.pylintrc` file",
      "y": null
   },
   {
      "null": 141,
      "x": "cpu usage is too high on the main thread after pytorch version 1.1 (and 1.2) (not data loader workers )",
      "z": "Pytorch 1.1 and above utilize more CPU threads than Pytorch 1.0.1. If you want to return to previous behavior (slower code, using less cores) run your code with `OMP_NUM_THREADS=1` or any other suitable value.",
      "y": null
   },
   {
      "null": 142,
      "x": "Transformer Encoder Layer with src_key_padding makes NaN",
      "z": "This is not an issue related to `nn.Transformer` or `nn.MultiheadAttention`.\n\nAfter the key_padding_mask filter layer, `attn_output_weights` is passed to `softmax` and here is the problem. In your case, you are fully padding the last two batches (see `y`). This results in two vectors fully filled with `-inf` in `attn_output_weights`. If a tensor fully filled with `-inf` is passed to softmax, softmax will return `nan` as a result. Will check with the team and see if this behavior is expected.\n\nA quick solution to your case. If you want to fully pad some batches, you could simply remove them, right?",
      "y": null
   },
   {
      "null": 143,
      "x": "ONNX Exporter Supporting ROIAlign",
      "z": "This will enable the export for RoiAlign until the torchvision devs add the required symbolic function.\nMind the `opset_version=10` in`onnx.export(...)`!\n\n\n```python\nimport torch\nimport torch.nn as nn\nimport torchvision as tv\nimport onnxruntime as ot\nimport numpy as np\n\nfrom functools import wraps\nfrom torchvision.ops.roi_align import _RoIAlignFunction\n\ndef add_method(cls):\n def decorator(func):\n @wraps(func)\n def wrapper(*args, **kwargs):\n return func(*args, **kwargs)\n setattr(cls, func.__name__, wrapper)\n # Note we are not binding func, but wrapper which accepts self but does exactly the same as func\n return func # returning func means func can still be used normally\n return decorator\n\n@add_method(_RoIAlignFunction)\ndef symbolic(g, input, roi, output_size, spatial_scale, sampling_ratio):\n import torch.onnx.symbolic_helper as sym_help\n\n sampling_ratio = sampling_ratio if sampling_ratio > 0 else 0\n\n rois = sym_help._slice_helper(g, roi, axes=[1], starts=[1], ends=[5])\n index = g.op(\"Constant\", value_t=torch.tensor(0, dtype=torch.int64))\n batch_indices = g.op(\"Gather\", roi, index, axis_i=1)\n batch_indices = g.op(\"Cast\", batch_indices, to_i=sym_help.cast_pytorch_to_onnx[\"Long\"])\n\n return g.op(\"RoiAlign\", input, rois, batch_indices, mode_s=\"avg\", output_height_i=output_size[0],\n output_width_i=output_size[1], sampling_ratio_i=sampling_ratio, spatial_scale_f=spatial_scale)\n\n### Testing\n\nclass Model(nn.Module):\n def __init__(self, output_size=(14, 14)):\n super(Model, self).__init__()\n\n self.output_size = output_size\n\n def forward(self, feature_map, bbx_pd):\n\n output = tv.ops.roi_align(feature_map, bbx_pd, self.output_size)\n\n return output\n\n\nmodel = Model()\n\nbatch_size = 2\na = 10\n\nfeature_map = torch.randn(batch_size, 20, 64, 64)\nbbx_pd = torch.randint(10, 15, (a, 5)).float()\nbbx_pd[:, 0] = torch.randint(0, batch_size, (bbx_pd.size(0),))\n\n\ntorch.onnx.export(model, (feature_map, bbx_pd), 'model.onnx', input_names=[\"feature_map\", \"bbx_pd\"], verbose=True, opset_version=10)\n\n\n### Inference\n\nosess = ot.InferenceSession(\"model.onnx\")\n\nbatch_size = 2\na = 10\n\nfeature_map = np.random.randn(batch_size, 20, 64, 64).astype(np.float32)\nbbx_pd = np.random.randint(10, 15, (a, 5)).astype(np.float32)\nbbx_pd[:, 0] = np.random.randint(0, batch_size, (a,))\n\nret = osess.run(None, {\"feature_map\": feature_map, \"bbx_pd\": bbx_pd})\n```",
      "y": null
   },
   {
      "null": 144,
      "x": "Can pytorch 1.2 support cuda 9.0?",
      "z": "I answered your question [here](https://discuss.pytorch.org/t/can-pytorch1-0-0-in-win10-with-cuda9-0-use-tensorboard/65747). Please don't post things in various closed issues. Thank you.",
      "y": null
   },
   {
      "null": 145,
      "x": "in-place operations over ranges with overlap can lead to unexpected results.",
      "z": "This is related to #8212",
      "y": null
   },
   {
      "null": 146,
      "x": "Let new and existing backends easily register for testing",
      "z": "(Written with @ailzhang)\n\n\"High-level\" proposal (additional details forthcoming), is to let the following be a maximally expressive command:\n\n`pytorch/test: python test_nn.py cuda -base cpu`\n\nThis command:\n\n- Runs the tests specified in test_nn.py with the \"cuda backend\"\n- It excludes tests that the \"cuda backend\" has stop listed\n- It excludes comparison/precision tests that the cpu has stop listed\n- It compares the results of the \"cuda backend\" to those of the \"cpu backend\"\n\nAnd this UX has the following caveats/options:\n\n- If `python test_nn.py` is run then Pytorch will run a series of test commands on test_nn.py, like `python test_nn.py cpu` and `python test_nn.py cuda` covering all applicable test backends.\n- If the option -base is not specified it is assumed to be cpu.\n- If the tested implementation and the implementation to compare with are the same then comparison/precision tests may not be run.\n\nTo implement this UX each test backend will have a file, like test_cuda.py, that contains:\n\n- An initialization function\n- A function for converting tensors to the appropriate type (e.g. `.cuda()`)\n- Functions for querying which tests should not be run\n- (Optionally) Tests and test plans\n\nThis will allow users to invoke all tests for a backend by running `test_<backend>.py`, and run specific tests by either doing `python test_<X>.py <backend>` or `python test_<backend>.py Test<Backend>.<X>` .\n\nA drawback of this approach is that test backends must explicitly exclude tests they cannot run. If operator coverage is spotty then new tests are likely to break those backends. In the future it may be preferable for the test infrastructure to catch classes of errors, like not implemented errors, and report tests where they occur as expected failures.\n\nIn the future it's also interesting to consider adding more control over datatypes and precision.",
      "y": null
   },
   {
      "null": 147,
      "x": "Run time error with backward() after upgrading to pytorch 1.2.0",
      "z": "I think the problem is that PyTorch 1.2 is saving `bb` for backward, while PyTorch 1.1 didn't need to. Autograd's version-count checker sounds like it is working correctly. The culprit is this line in derivatives.yaml:\n\nhttps://github.com/pytorch/pytorch/blob/6100205eb8a88090581a905e595e7d1d2a8c232d/tools/autograd/derivatives.yaml#L389-L391\n\nthe `index` function saves `self` for backward because it is present here. However, we don't need to save the entire tensor; in PyTorch 1.1, we only saved the geometry of `self`. (zeros_like(self) in the below snippet is special-cased in our code gen to only save the geometry of the tensor).\nhttps://github.com/pytorch/pytorch/blob/0b868b19063645afed59d6d49aff1e43d1665b88/tools/autograd/derivatives.yaml#L383-L385\n\nIndexBackward shouldn't need to rely on self's data, so changing it to take `zeros_like(self)` should fix the problem. I'll test a fix soon.",
      "y": null
   },
   {
      "null": 148,
      "x": "[Windows] Including Windows.h before LibTorch headers causes compilation errors",
      "z": "I tried and this issue occurred, but it can be easily resolved if you define `NOMINMAX` before including `Windows.h`.",
      "y": null
   },
   {
      "null": 149,
      "x": "Issue with maxInt in utils.py",
      "z": "it's in torch/text, not torch/torch\n[https://github.com/pytorch/text/pull/584](#584)",
      "y": null
   },
   {
      "null": 150,
      "x": "SobolEngine should use random seed by default when scrambling",
      "z": "@Balandat sounds very reasonable. I actually hadn't thought of this at the time. I would be wary of the tests, because they assume the above behavior.",
      "y": null
   },
   {
      "null": 151,
      "x": "libtorch_python.so: undefined symbol: _PyThreadState_UncheckedGet",
      "z": "3.5.0 has a known ABI bug (3.5.1 accidentally changed ABI). you need atleast 3.5.1 for a few Python package binaries to work correctly, including pytorch",
      "y": null
   },
   {
      "null": 152,
      "x": "Implement RAdam optimizer ?",
      "z": "I haven't contributed to pytorch before but I have been using it for almost a year. I'd like to take up this issue if possible. Do you think this issue is doable for a first-timer?",
      "y": null
   },
   {
      "null": 153,
      "x": "Lazy conjugated tensor is not numpy interoperable",
      "z": "> I don't think so - before `a.conj()` would _also_ make a copy, so `a.conj().numpy()` would not share storage with `a`.\n\n`a.conj()` is a result of the conjugate operation and it is not expected that `a.conj().numpy()` would share the storage of `a` (unless `a` is real and then `conj()` would be no-op anyway).\nInstead, it is expected that `b.numpy()` would share the storage with `b` where `b = a.conj()`. With the new lazy conjugate feature, `b` shares the storage with `a`, so the storage-sharing promise of `.numpy()` cannot be fulfilled in one or another way.\n\nBtw, `a.conj().conj().numpy()` would share the storage with `a` (assuming that `a` does not have the conjugate bit set).",
      "y": null
   },
   {
      "null": 154,
      "x": "add `--init_method` to `torch.distributed.launch`",
      "z": "Hey Stas,\n\nObviously my first suggestion would be to run your tests sequentially, but I assume for one reason or another this is not possible in your case. Having said that, be aware that we plan to deprecate `torch.distributed.launch` in the near future. With PyTorch v1.9 we upstreamed TorchElastic to PyTorch and introduced a new `torch.distributed.run` script that also supports elasticity. Please take a quick look at its [documentation](https://pytorch.org/docs/1.9.0/elastic/quickstart.html) before reading the rest of my comment since some of my suggestions require familiarity with the new `torch.distributed.run` script.\n\nComing back to your problem, there are four potential solutions that I can think of:\n\n1) #59719 will most likely address your concern in a very similar way you proposed here. It will allow you to specify a `FileStore` as a \"rendezvous endpoint\" in `torch.distributed.run`. Unfortunately this PR couldn't make the v1.9 cut, so won't be included in the official v1.9 release. However once merged you will be able to run your script similar to:\n\n```bash\npython -m torch.distributed.run\n --nnodes=<world_size>\n --nproc_per_node=<workers_per_node>\n --rdzv_id=<job_id>\n --rdzv_backend=c10d\n --rdzv_endpoint=<path_to_your_temp_file>\n --rdzv_conf=\"store_type=file\"\n <your_script>.py (--arg1 ... train script args...)\n```\n\n2) Another option you will have with v1.9 is to use an etcd server/cluster as your rendezvous endpoint. etcd is pretty lightweight and can easily be installed and run locally with a simple shell script just before you execute your test suite. Similar to the first option, the way you will run your script will be similar to:\n\n```bash\npython -m torch.distributed.run\n --nnodes=<world_size>\n --nproc_per_node=<workers_per_node>\n --rdzv_id=<job_id>\n --rdzv_backend=etcd-v2\n --rdzv_endpoint=<hostname_of_etcd_server>\n <your_script>.py (--arg1 ... train script args...)\n```\n\nSince all tests will share the same etcd server, there is no such thing as a port conflict in this case.\n\n3) If you are using PyTorch v1.8 or earlier (and don't want to migrate to v1.9 soon), you can leverage TorchElastic. The second option was in fact upstreamed from TorchElastic to PyTorch. TorchElastic comes with a launcher script that is very similar to `torch.distributed.run` and mostly expects the same arguments. You can find its documentation [here](https://pytorch.org/elastic/0.2.2/quickstart.html).\n\n4) If none of these suggestions work for you, yet another option might be to reserve a range of port numbers that is large enough (e.g. from 3000 to 3999) and during test time assign them sequentially to the tests to be executed. A naive bash implementation would like this:\n\n```bash\nfor (( port = 300; port < 3999; port++ )); do\n # Run in background.\n python -m torch_distributed_run --master_port=$port ... &\ndone\n\n# Wait for all tests to finish.\nwait\n```",
      "y": null
   },
   {
      "null": 155,
      "x": "`OMP_NUM_THREADS` doesn't work as intended in some cases",
      "z": "I checked behaivior of `Openmp` and `MKL`\n I run all scripts on a machine with 10 physical cores.\n \n ```python\n import omp_thread_count\n omp_thread_count.get_thread_count()\n ```\n \n Running this script with `OMP_NUM_THREADS=100 python filename.py` outputs `100` which is expected behavior.\n \n ```python\n import mkl\n mkl.get_max_threads()\n ```\n \n Running this script with `MKL_NUM_THREADS=100 python filename.py` outputs `10` which is not expected behavior.\n \n \n ```python\n import mkl\n mkl.set_num_threads(100)\n mkl.get_max_threads()\n ```\n \n Running this script with `python filename.py` outputs `10` which is not expected behavior.\n \n The above behaviors show that the `OpenMP` works well and `MKL` doesn't work.\n \n```python\nimport mkl\nmkl.set_num_threads(20)\nmkl.set_dynamic(False)\nmkl.get_max_threads()\n```\n\nRunning this script with `python filename.py` outputs `20` which is expected behavior.\n\n\n```python\nimport mkl\nmkl.set_num_threads(20)\nmkl.get_max_threads()\n```\n\nRunning this script with `MKL_DYNAMIC=FALSE MKL_NUM_THREADS=20 python filename.py` outputs `20` which is expected behavior.\n\n```python\nimport torch\nprint(torch.get_num_threads())\n```\n\n\nRunning this script with `MKL_DYNAMIC=FALSE MKL_NUM_THREADS=20 python filename.py` outputs `20` which is expected behavior.\n'MKL_DYNAMIC=FALSE OMP_NUM_THREADS=20 python filename.py` also works well.\n\nThis is because `mkl.set_dynamic(False)` is called in `torch.set_num_threads`.\n\nhttps://github.com/pytorch/pytorch/blob/cac9ae1506feabfc87d37a208b3d39ed46c59483/aten/src/ATen/ParallelOpenMP.cpp#L44-L65\n\nDoes your question solve?",
      "y": null
   },
   {
      "null": 156,
      "x": "Compilation issue with CUDA 92",
      "z": "Please note, that CUDA-9.2 is not officially supported in trunk, so any fixes are likely to regress",
      "y": null
   },
   {
      "null": 157,
      "x": "DISABLED test_backward_rref_nested (__main__.ProcessGroupDistAutogradTestWithSpawn)",
      "z": "> Is TensorPipeDdpComparisonTestWithSpawn.test_ddp_dist_autograd_local_vs_remote also flaky?\n\nYep, that is tracked in #59792",
      "y": null
   },
   {
      "null": 158,
      "x": "Annoying warning with nn.MaxPool2d",
      "z": "Having same issue, agreed that it would be nice if it made it into `1.9.1`.",
      "y": null
   },
   {
      "null": 159,
      "x": "Warning with torch::nn::init::orthogonal_ with LibTorch 1.9.0",
      "z": "just noting the equivalent in python doesn't trigger.",
      "y": null
   },
   {
      "null": 160,
      "x": "torch.load issue on different folder",
      "z": "not sure it's a JIT issue, but unlikely we'll be able to help without a reproduction.",
      "y": null
   },
   {
      "null": 161,
      "x": "Source of `torch.testing.assert_close` reaches a dead end",
      "z": "Fixed by https://github.com/pytorch/pytorch.github.io/commit/5a2028452f93fdc5e44a65e761e03ed658048336 and https://github.com/pytorch/pytorch.github.io/commit/82eaa25adc292d81414e2bd33e95466c52e78dea",
      "y": null
   },
   {
      "null": 162,
      "x": "torch.package: Module did not match against any action pattern. Extern, mock, or intern it.",
      "z": "Hi! @tchaton we do have docs explaining what intern/extern/mocking are in our unreleased [master documentation](https://pytorch.org/docs/master/package.html). The information you need should be there!\n\n@muellerzr There is an example of defining `__reduce_package__` for custom classes in the documentation as well. The problem you are hitting is due to the fact that `torch.package` is unable to package code in the `__main__` module. You'll need to move your code for `Flatten` to another file/module.",
      "y": null
   },
   {
      "null": 163,
      "x": "[C++] grad_input may have different memory_format from grad_output in structured kernels",
      "z": "The problem is most likely in the META function that handles the allocation of grad_input. Note that after fixing META function (probably in similar way that MaxPooling operates today) cpu implementation will also need to be changed to be able to handle channels-last grad_input (just by copying whatever format cpu implementation produces to correctly strided grad_input).",
      "y": null
   },
   {
      "null": 164,
      "x": "Docs of torch.Tensor.permute refer to docs of torch.permute for which do not exist",
      "z": "@Haydnspass please feel free to submit a fix for this if you are up for it",
      "y": null
   },
   {
      "null": 165,
      "x": "On CPU, vectorized float tensor addition might be slower than unvectorized float tensor addition",
      "z": "It sounds like the cost is allocation/kernel zero-filling. Your C++ program is adding into the same array every iteration. If you want that in PyTorch use, `torch.add(x, y, out=out)`.\n\nAlternatively, you can try using jemalloc as your allocator. It tends to hold onto larger allocations internally, so you don't pay the kernel zero-fill cost for every add call. (You can use `LD_PRELOAD` to run with jemalloc; it doesn't require any changes to your program).",
      "y": null
   },
   {
      "null": 166,
      "x": "[torch.profiler] non-context manager use",
      "z": "we will add non-context manager support in next release.",
      "y": null
   },
   {
      "null": 167,
      "x": "Multiple failures in distributed/elastic tests",
      "z": "Those are tests that are moved from the standalone TorchElastic repo as part of the upstreaming work we have done in PyTorch v1.9. Both authors (Aliaksandr and Kiuk) as well as the PyTorch Distributed team are aware of those local run failures. We already have an internal backlog item (T90265589) to fix them in the near future.",
      "y": null
   },
   {
      "null": 168,
      "x": "make torch.all and torch.any support all dtypes",
      "z": "This is a great idea and we would accept a PR fixing this. It'd be nice to have\n\n- function versions `torch.all` and `torch.any` documented\n- tensor methods documented as not just applying to bool tensors",
      "y": null
   },
   {
      "null": 169,
      "x": "fx: unable to symbolically trace BatchNorm2d due to control flow",
      "z": "I see, thanks, makes sense. The use case we are working with is hitting this because a custom child of `nn.BatchNorm` is calling `super().forward(x)`:\n\n```\nclass BatchNorm2d(torch.nn.BatchNorm2d):\n def forward(self, x):\n if x.numel() > 0:\n return super(BatchNorm2d, self).forward(x)\n # get output shape\n output_shape = x.shape\n return _NewEmptyTensorOp.apply(x, output_shape)\n```\n\nsince we chatted offline that we can likely remove the `x.numel() > 0` check, this behavior can be deleted as well. I'll follow up on this specific case.",
      "y": null
   },
   {
      "null": 170,
      "x": "fx: assertions based on tensor shapes are not symbolically traceable",
      "z": "In addition to `torch.assert` we could also explore doing an AST rewrite on `forward` methods that are being traced to automatically convert `assert` statements to callsites to that. Don't think it would be too difficult and would open the door to more friendly workarounds in the future",
      "y": null
   },
   {
      "null": 171,
      "x": "`emit_nvtx` example in docs seems to be broken.",
      "z": "I think the error here is that:\n```\nwith torch.autograd.profiler.profile():\n```\nshould be\n```\nwith torch.cuda.profiler.profile():\n```\n\nI've made that mistake as well and it seems confusing to have `emit_nvtx` in the `autograd.profiler` package, but enable its use via the `cuda.profiler` package.\n\nThe example in the docs is correct, but it's easy to not notice and type the wrong thing.",
      "y": null
   },
   {
      "null": 172,
      "x": "libtorch load trace module failed in Qt5",
      "z": "@glaringlee \nSorry for the late reply. \nToday I downloaded the newest version of Pytorch and Libtorch, and the problem disappeared. \nI think it is because Pytorch is too old. \n\nThank you for your helping.",
      "y": null
   },
   {
      "null": 173,
      "x": "fx: unable to symbolically trace model with torch.zeroes(Proxy)",
      "z": "Oh, just use `new_zeros` instead of `zeros_like`, which takes a shape too.",
      "y": null
   },
   {
      "null": 174,
      "x": "a new type of nn.Module param for large transient data (that doesn't get saved)",
      "z": "Done: https://github.com/pytorch/pytorch/issues/45012\n\nThank you again, @mruberry! You helped me discover a new feature today!",
      "y": null
   },
   {
      "null": 175,
      "x": "Inconsistent result from FBGEMM and QNNPACK quantization backends",
      "z": "I dug deeper into FBGEMM implementation and found out that in this example I get a saturation in `vpmaddubsw` (`211 * 71 + 171 * 107 == 33278`). \n\nThe issue can be closed, since this is a quirk of the implementation, but IMO this should be documented, since it's a bit obscure and hard to trace. (:",
      "y": null
   },
   {
      "null": 176,
      "x": "The support for 3080 or 3090",
      "z": "I can't take credit for this, or remember where I found it, but what worked for me was\n```\npip uninstall torch\npip install torch==1.7.0+cu110 torchvision==0.8.1+cu110 torchaudio===0.7.0 -f https://download.pytorch.org/whl/torch_stable.html\n```",
      "y": null
   },
   {
      "null": 177,
      "x": "GeForce RTX 3080 with CUDA capability sm_86 is not compatible with the current PyTorch installation.",
      "z": "I am currently using Windows10, python3.7, cuda11\uff0cand install pytorch1.7 using the following command:\n_**pip install --pre torch torchvision -f https://download.pytorch.org/whl/nightly/cu110/torch_nightly.html**_\nThe warning still exists, but the program can run normally and achieve the expected effect on the RTX3080 GPU.",
      "y": null
   },
   {
      "null": 178,
      "x": "Whether create_graph=true or false, torch.autograd.grad returns different gradients.",
      "z": "I have noticed the same problem and it would be great if someone could have a look",
      "y": null
   },
   {
      "null": 179,
      "x": "Error in test suite: an illegal memory access was encountered",
      "z": "We are trying to setup a node with a K80 GPU to reproduce this issue (waiting for our cluster team, if that would be possible today).\nFaster workaround could be to setup a machine with a Titan Z, which might be able to reproduce it.",
      "y": null
   },
   {
      "null": 180,
      "x": "[JIT] Support for CUDA streams in TorchScript",
      "z": "This was previously blocked on support for `with`-statements in TorchScript, but that is available now. @SplitInfinity, do you have any thoughts on timeline here? Could be a good ramp up task for someone",
      "y": null
   },
   {
      "null": 181,
      "x": "TestTensorExprFuser.test_unused is failing in release/1.6 branch.",
      "z": "cc @malfet @seemethere",
      "y": null
   },
   {
      "null": 182,
      "x": "torch.distributed NCCL backend does not support bitwise reduction ops",
      "z": "Sounds good, @thinking-tower I think the first step for now can just be to throw a descriptive error if the operation is not supported (not in the `ncclOp` map) and probably update our distributed documentation indicating that BAND, BOR, and BXOR is not supported with the NCCL backend currently. \n\nIn the longer term, we can discuss with NCCL and see if there is demand for these ops from users.",
      "y": null
   },
   {
      "null": 183,
      "x": "ConvTranspose1d extremely slow on GPU (T4), even slower than CPU",
      "z": "Double post from [here](https://discuss.pytorch.org/t/convtranspose1d-extremely-slow-on-gpu-t4-slower-than-cpu/88977/3?u=ptrblck).\nCopying the post:\n> It seems a bad kernel is selected in the default setup by cudnn and you can use torch.backends.cudnn.benchmark = True to use the cudnn benchmark mode to select the fastest kernel.\nIn this mode the first iteration will be slower, as multiple algorithms will be executed to select the fastest one.\nAfter setting torch.backends.cudnn.benchmark = True at the beginning of the script, I\u2019ll get ~11.44ms.\nThe fast kernel seems to be already selected in the upcoming cudnn8 release.",
      "y": null
   },
   {
      "null": 184,
      "x": "Searchsorted differentiable?",
      "z": "And of course for anyone led here by google, while looking for a quick solution.\nI applied @albanD's suggestion of detaching the input to unique. So the line:\n\n for c in det[:, -1].unique():\n\nbecame\n\n for c in det[:, -1].detach().unique():\n\nAnd all is fine...",
      "y": null
   },
   {
      "null": 185,
      "x": "LowRankMultivariateNormal creates Illegal Memory Access in magma_spotrf_batched",
      "z": "LowRankMultivariateNormal with n = 1024 calls batched cholesky with sizes (1024, 1024, 10, 10). While calling LowRankMultivariateNormal twice with these dimensions causes illegal memory access, directly calling batched cholesky twice with sizes (1024, 1024, 10, 10) does not. However, directly calling batched cholesky with sizes (512, 512, 10, 10) once does cause illegal memory access. To me this indicates that the code is likely accessing off-bounds for all dimensions but it only causes an error for certain memory allocations as opposed to there being specific sizes that cause the code to take another broken path.",
      "y": null
   },
   {
      "null": 186,
      "x": "Is it planning to support nn.Embeddings quantization?",
      "z": "We currently do support quantization of `nn.Embedding` and `nn.EmbeddingBag`. Please try with pytorch nightly to get the relevant changes. \nEmbedding quantization is supported using the eager mode static api (i.e `prepare` and `convert`). The qconfig for the embedding layers need to be set to `float_qparams_weight_only_qconfig`.\n\nAn example of how to enable quantization on embedding layers can be found here https://github.com/pytorch/pytorch/blob/master/test/quantization/test_quantize.py#L521",
      "y": null
   },
   {
      "null": 187,
      "x": "Memory corruption and crash in quantized convolution with small inputs",
      "z": "https://github.com/pytorch/pytorch/pull/41814 is the merge PR in PyTorch.",
      "y": null
   },
   {
      "null": 188,
      "x": "[RFC] torch.distributed.app (role-based + higher-level RPC APIs)",
      "z": "Thank you for your and @mrshenli 's precious time!\n\nI think we can add some more functions to role-based rpc, since the importance of \"role\" is (1) reschedule a failed role unit dynamically (2) grouping agents with similar properties together. The first point is meaningless currently because torchelastic choose to \"brutally\" tear down all processes, the second point is undermined because currently `rpc_aync`, `rpc_sync` are still point to point, and \"rpc.rpc_async_role\" is more or less a more beautiful refinement of \"for loop\" + \"P2P\" without changing the nature. I will formally define my proposal of a real \"rpc group\" and \"service\" in the next RFC along with the definition of roles in this RFC!",
      "y": null
   },
   {
      "null": 189,
      "x": "Replace blacklist/whitelist throughout the PyTorch codebase",
      "z": "Hi, Is this still open for contributing?\nIf yes, can someone help me understand what the requirement is and how to submit a successful PR? if someone could help me with this, this could be my first step into open source.\nThank you.",
      "y": null
   },
   {
      "null": 190,
      "x": "[JIT] Support Dict comprehension",
      "z": "instead of writing a forloop manually, one interesting workaround is to use list comprehension: `x = dict([(i,i) for i in range(2)])`",
      "y": null
   },
   {
      "null": 191,
      "x": "received 0 items of ancdata when appending dic elements to a list",
      "z": "this is a pretty small repro script, so i'm tentatively marking this high prio for investigation",
      "y": null
   },
   {
      "null": 192,
      "x": "test_nn failures on power",
      "z": "@albanD this might be related to the locking problems you're trying to fix, perhaps they are more noticeable on POWER",
      "y": null
   },
   {
      "null": 193,
      "x": "Adam implementation different from paper",
      "z": "> [fed5ca1](https://github.com/pytorch/pytorch/commit/fed5ca192c98619947cca2fb9491ac9624b787d2) actually fixed the divergence from this more recent [paper](https://arxiv.org/pdf/1711.05101.pdf). And there Algorithm 1 is highlighting the difference between SGD and SGDW. But line 12 of Algorithm 2 is the place we should look at. There `epsilon` is added to `sqrt(v_hat)` in which `v_hat` is the `exp_avg_sq` scaled by `bias_correction2`\n\nThen I think the current implementation conforms with the paper. \nThe issue is that the [documentation](https://pytorch.org/docs/stable/optim.html#torch.optim.Adam) makes it seem like this implements the Adam proposed in the [original paper](https://arxiv.org/abs/1412.6980).",
      "y": null
   },
   {
      "null": 194,
      "x": "Update SobolEngine direction numbers",
      "z": "@Balandat I think it's totally fine to have it embedded in .so file and let OS take care of lazy loading the data section. \nAs long as literals are kept in C-style arrays it is almost zero runtime overhead to load it.\nPlease include the python script in your PR that downloads/converts https://web.maths.unsw.edu.au/~fkuo/sobol/new-joe-kuo-6.21201 into https://github.com/pytorch/pytorch/blob/master/aten/src/ATen/native/SobolEngineOpsUtils.cpp",
      "y": null
   },
   {
      "null": 195,
      "x": "Unflatten module for nn.Sequential",
      "z": "Right so it will take two arguments, the dimension to unflatten and the new shape this dimension should take. Matching the unflatten API of `tensor.unflatten()`.\nSounds good !",
      "y": null
   },
   {
      "null": 196,
      "x": "Dataloader in DistributedDataParallel hangs",
      "z": "spawning 32 processes is a nontrivial task..",
      "y": null
   },
   {
      "null": 197,
      "x": "RuntimeError: nvrtc: error: invalid value for --gpu-architecture (-arch)",
      "z": "@gjghks I won't talk about it in details since I haven't done that before. Just giving you some insights.",
      "y": null
   },
   {
      "null": 198,
      "x": "Incorrect info about overriding torch tensors in version 1.7.0",
      "z": "Yes, this was done in #37091, specifically in https://github.com/pytorch/pytorch/pull/37091/commits/b422a1bfdb16c5f86ea31367b3c2c73ec92454b1, in the context of there being various utility methods there that were already well-tested and needed to be exposed, such as, for example, `is_tensor_method_or_property`. I can't seem to find the conversation, maybe it happened in a meeting.",
      "y": null
   },
   {
      "null": 199,
      "x": "Build from source: CMake not using compilers installed by Anaconda, fails with stl_pair.h errors",
      "z": "Note that\n```\nconda install -c anaconda gcc_linux-64 gxx_linux-64 -y\n```\ngives gcc 7. I suggest using\n```\nconda install -c conda-forge gcc_linux-64 gxx_linux-64 -y\n```\nthat will install gcc 9. In fact, the subsequent `conda install ..` will upgrade gcc 7 to gcc 9 anyway.\n\nPytorch requires C++14, so one must apply:\n```\nexport CXXFLAGS=\"`echo $CXXFLAGS | sed 's/-std=c++17/-std=c++14/'`\"\n```\n\n[Perhaps OT] Locally, when trying to reproduce this issue, I noticed that `python` refers to python in conda base environment rather than the python in the given pytorch conda environment. This leads to other issues (cannot import `typing_extensions`) when running `python setup.py develop`. So, make sure that the correct python is picked up.\n\nNext, I got the following build failure [perhaps also OT]:\n```\n../third_party/tensorpipe/tensorpipe/common/shm_segment.cc: In function 'std::tuple<tensorpipe::Error, tensorpipe::Fd> tensorpipe::{anonymous}::openTmpfileInDevShm()':\n../third_party/tensorpipe/tensorpipe/common/shm_segment.cc:79:15: error: 'O_TMPFILE' was not declared in this scope\n 79 | int flags = O_TMPFILE | O_EXCL | O_RDWR | O_CLOEXEC;\n | ^~~~~~~~~\n```\nTo overcome this as an unrelated build issue, I set `USE_TENSORPIPE=0` and restarted the build from a clean state (`git clean -xddf; git submodule foreach --recursive git clean -xfdd`).\n\nEDIT: To resolve the `libcusparse.so.11` linker error, define:\n```\nexport LDFLAGS=\"${LDFLAGS} -Wl,-rpath-link,${CUDA_HOME}/lib64 -L${CUDA_HOME}/lib64\"\n```\n\nNow the build will succeed.\n\nIn summary, when using conda environment, make sure that C++14 is effective and specify CUDA installation path in `LDFLAGS`:\n```\nexport CXXFLAGS=\"`echo $CXXFLAGS | sed 's/-std=c++17/-std=c++14/'`\"\nexport LDFLAGS=\"${LDFLAGS} -Wl,-rpath-link,${CUDA_HOME}/lib64 -L${CUDA_HOME}/lib64\"\n```",
      "y": null
   },
   {
      "null": 200,
      "x": "Error in test_quantize - missing FBGEMM implementation",
      "z": "Thanks for flagging, yes these tests should have the skipIfNoFBGEMM decorator on them. I'm surprised they passed CI, I thoguht we do run tests there without FBGEMM enabled. \nI'll fix this and and a `TORCH_CHECK` in the non-fbgemm path",
      "y": null
   },
   {
      "null": 201,
      "x": "Core dump when checking that basic CNN works (Python 3.9)",
      "z": "The range of commits from my initial testing to when this problem presented itself is this: https://github.com/pytorch/pytorch/compare/b6a2444eff6fd8d92de1d7c3501aa892376d54e1...ae5c2febb912066f1a8dec8b54451c09195b2c6d",
      "y": null
   },
   {
      "null": 202,
      "x": "Incorrect result of `fmod` and `remainder` operator.",
      "z": "Making fmod and remainder have type promotion consistent with NumPy and the rest of PyTorch sounds great.",
      "y": null
   },
   {
      "null": 203,
      "x": "Support `torch.linalg.norm` for complex tensors on both CPU and CUDA",
      "z": "@mthrok, `ord=3` for matrix inputs is explicitly not something we support, since numpy does not support it. So that won't work for real number inputs either. However, we can consider adding it--I don't think anything should necessarily stop us from adding features on top of numpy-compatible functions if they're useful.\n\n```\n>>> numpy.linalg.norm(numpy.random.random((10, 10)), ord=3)\n...\nValueError: Invalid norm order for matrices.\n```",
      "y": null
   },
   {
      "null": 204,
      "x": "I found such a strange thing. Add code anywhere in the definition model that is not associated with the current model, such as torch.nn.Linear () will affect the model",
      "z": "Each layer creation will randomly initialize the parameters (if available) and will thus call into the pseudorandom number generator. If your training is sensitive to different seeds, these additional layer creations can change the training results significantly and you should see the same effect by removing the layer creation and just changing the seed at the beginning of your script.\n\nPS: the forum is a better place for questions like that.",
      "y": null
   },
   {
      "null": 205,
      "x": "Linker error when building from source with CUDA 11.0",
      "z": "The ld on your cluster looks pretty old, any way of upgrading it?",
      "y": null
   },
   {
      "null": 206,
      "x": "INTERNAL ASSERT FAILED for `S.unique().shape`",
      "z": "I'm also facing this issue with PyTorch1.7. Simple code to reproduce the bug:\n```py\nx1 = torch.rand(10, 3, requires_grad=True)\ntorch.unique(x1, dim=0)\n```\nThis raises the following error:\n```\nRuntimeError: isDifferentiableType(variable.scalar_type()) INTERNAL ASSERT FAILED at \"..\\\\torch/csrc/autograd/functions/utils.h\":64, please report a bug to PyTorch.\n```",
      "y": null
   },
   {
      "null": 207,
      "x": "Tensor iteration changes behavior in 1.7 with no hint",
      "z": "I met this error, too. \nOne solution is using indices, which returns **SelectBackward** tensors instead of **UnbindBackward** tensors.\n```python\nimport torch\nb = torch.rand(2, 5, 5, requires_grad=True)\n# for bb in b:\nfor i in range(len(b)):\n bb=b[i]\n bb[:, 0].clamp_(min=0, max=0.3)\n bb[:, 1].clamp_(min=0, max=0.3)\n```\n\n\n\nFurther, if you are interested in my problem, I simplify my code and reproduce the error below (works in previous versions but fails in 1.7)\n```python\nimport torch\n\nx = torch.tensor([[0.5, 0.2, 0.3, 0.8]], requires_grad=True)\nw = torch.tensor([[0.2, 0.5, 0.1, 0.5]], requires_grad=True)\ny_true = torch.tensor([1])\n\n# In 1.7, `list(w)` returns list of UnbindBackward tensors, which further leads to inplace operation error.\n# In previous versions, `list(w)` returns list of SelectBackward tensors and it works.\nweight_list = list(w)\nprint('weight_list',weight_list) \n\nfor i in range(2):\n l2_norm = torch.norm(weight_list[0], 2)\n y = w.mm(x.T)\n\n # mse with l2 regularization\n loss = pow(y - y_true, 2) + 0.5 * pow(l2_norm, 2)\n loss.backward()\n\n # simulate optimizer.step()\n with torch.no_grad():\n w.add_(w.grad, alpha=1e-3)\n\n```\n\noutput:\n**torch 1.7:** \n```\nweight_list [tensor([0.2000, 0.5000, 0.1000, 0.5000], grad_fn=<UnbindBackward>)]\n...\nRuntimeError: Output 0 of UnbindBackward is a view and its base or another view of its base has been modified inplace. This view is the output of a function that returns multiple views. Such functions do not allow the output views to be modified inplace. You should replace the inplace operation by an out-of-place one.\n\nProcess finished with exit code 1\n```\n\n**previous versions:** \n```\nweight_list [tensor([0.2000, 0.5000, 0.1000, 0.5000], grad_fn=<SelectBackward>)]\n\nProcess finished with exit code 0\n```\n\nDetails can be seen in https://github.com/shenweichen/DeepCTR-Torch/issues/130. My solution is to remove operations like `list(w)` operation for <class 'torch.nn.parameter.Parameter'> in https://github.com/shenweichen/DeepCTR-Torch/commit/a9402a474481fd186cb8428d6df9aadc2b3c69dd. Besides, if it's necessary to so such operation, using indices works:\n```python\n# weight_list = list(w)\nweight_list = [w[i] for i in range(len(w))]\n```",
      "y": null
   },
   {
      "null": 208,
      "x": "Using model.eval() with batchnorm gives high error",
      "z": "It seems like a common issue in pytorch forum while no one is answering people concerns and experience.",
      "y": null
   },
   {
      "null": 209,
      "x": "packaging question, setup.py egg_info relies on submodule",
      "z": "Hopefully within a month",
      "y": null
   },
   {
      "null": 210,
      "x": "[feature proposal] An effortless way of loading pre-trained embeddings",
      "z": "I have a helper function for this in my Python utils, but native support would be great:\n\n```Python\nfrom torch import nn, Tensor\n\n\ndef emb_layer(keyed_vectors, trainable=False):\n \"\"\"Create an Embedding layer from the supplied gensim keyed_vectors.\"\"\"\n emb_weights = Tensor(keyed_vectors.syn0)\n emb = nn.Embedding(*emb_weights.shape)\n emb.weight = nn.Parameter(emb_weights)\n emb.weight.requires_grad = trainable\n return emb\n```",
      "y": null
   },
   {
      "null": 211,
      "x": "Can't Resize_() IntTensor",
      "z": "Please use the [forum](https://discuss.pytorch.org/) to ask questions.\n\nIn your case, I guess the `targets` comes from numpy? Since `Tensor`s share content with numpy arrays, they cannot be resized, you have to `.clone()` them before.",
      "y": null
   },
   {
      "null": 212,
      "x": "[docs] Missing docs online for F.softplus",
      "z": "Last action item on this issue is writing the [functional.softplus](http://pytorch.org/docs/master/nn.html?highlight=softplus#torch.nn.functional.softplus) docs.",
      "y": null
   },
   {
      "null": 213,
      "x": "release 0.3.1 checklist",
      "z": "May I suggest #4524 and #4558 ?",
      "y": null
   },
   {
      "null": 214,
      "x": "ModuleNotFoundError: No module named 'torch'",
      "z": "@vishwakftw for PackageNotFoundError I tried this:\n$conda update conda\n$conda install mkl=2018\nwhich I found [here](https://discuss.pytorch.org/t/issues-installing-pytorch-for-os-x-with-conda/11496)\nand it works now\nThanks vishwakftw",
      "y": null
   },
   {
      "null": 215,
      "x": "Inconsistency between advanced indexing of Tensors and Variables",
      "z": "Ok, thanks for the quick reply. From the documentation it seemed that this is already the case, and thus the report...",
      "y": null
   },
   {
      "null": 216,
      "x": "remove cuda capability 3.0 and 5.0 from binaries",
      "z": "Could you please add binary for 5.0 GPUs? Maybe the size won't get larger than 500mb. \n\nThere are lots of pc running on old gpus, and it's hard to build binaries.",
      "y": null
   },
   {
      "null": 217,
      "x": "Using CUDNN gives bad results.",
      "z": "It also might be interaction between weight norm and parameter flattening for cudnn. Can you try your network without weight norm and see if with cudnn and without cudnn produce similar results?",
      "y": null
   },
   {
      "null": 218,
      "x": "Make UndefinedTensor's data_ptr() return nullptr",
      "z": "Maybe make it `data_ptr_opt()`?",
      "y": null
   },
   {
      "null": 219,
      "x": "CUDNN_LIBRARY selection",
      "z": "I think #3251 and #5042 are quite possibly related.\n\nEDIT: also #5084",
      "y": null
   },
   {
      "null": 220,
      "x": "\"CUDA error: out of memory\" for the last Batch",
      "z": "I have encountered from the same problem.Maybe the last irregular batch will use more memory(I don't know the reason).At last,I found that DataLoader has the option **drop_last** that can drop out the last irregular batch,I used it to solve the problem.",
      "y": null
   },
   {
      "null": 221,
      "x": "DataParallel module leaks memory",
      "z": "@khanrc Turns out I was running python 3. On python 2.7 there are indeed reference cycles being created with a large number of objects. Am investigating.\n\nedit: As far as I can tell, the objects with reference cycles that are being garbage collected are created internally by python 2.7 by not by python 3. Those objects also do not take up a lot of memory. You can use [gc.set_threshold](https://docs.python.org/2/library/gc.html#gc.set_threshold) to control how frequently the python gc runs, or use Python 3 if the objects being created by Python 2.7 bother you.\n\nI don't think this is a pytorch issue; there's definitely no memory leak, because garbage collection is a normal python thing. Let me know what you think, @khanrc.",
      "y": null
   },
   {
      "null": 222,
      "x": "Support torch.abs for ByteTensor, CharTensor",
      "z": "@hameerabbasi happy to help :). The current `abs` functions exist in ATen and are implemented (partially) as call-throughs to the TH functions that @fmassa listed above (i.e. https://github.com/pytorch/pytorch/blob/1aa90192eae306e4375c68f12f46eb329042e07d/aten/src/ATen/native/cuda/CUDAUnaryOps.cpp#L53 and https://github.com/pytorch/pytorch/blob/1aa90192eae306e4375c68f12f46eb329042e07d/aten/src/ATen/native/UnaryOps.cpp#L154).\n\nYou can perhaps use TensorIterator to implement it. See the implementation of `threshold` for inspiration: https://github.com/pytorch/pytorch/blob/1aa90192eae306e4375c68f12f46eb329042e07d/aten/src/ATen/native/Activation.cpp#L53-L81",
      "y": null
   },
   {
      "null": 223,
      "x": "Some operations after `.copy_()` results in error in backward",
      "z": "1. We should change the generated function to only unpack variables that are needed. \n2. It would be nice to change VariableType.cpp to only save variables that are needed (as @albanD suggests). This probably depends on (1).\n\nTo expand on (1):\n\nMulBackward0 looks like:\n\n```c++\nvariable_list MulBackward0::apply(variable_list&& grads) {\n IndexRangeGenerator gen;\n auto self_ix = gen.range(1);\n auto other_ix = gen.range(1);\n variable_list grad_inputs(gen.size());\n auto& grad = grads[0];\n auto self = self_.unpack();\n auto other = other_.unpack();\n if (should_compute_output({ other_ix })) {\n auto grad_result = grad * self;\n copy_range(grad_inputs, other_ix, grad_result);\n }\n if (should_compute_output({ self_ix })) {\n auto grad_result = grad * other;\n copy_range(grad_inputs, self_ix, grad_result);\n }\n return grad_inputs;\n}\n```\n\nRoughly, the `auto self = self_.unpack();` and ` auto other = other_.unpack();` should be moved inside the if should_compute_output statements. In practice, it will be a bit more complicated because some variables are used to compute multiple derivatives.",
      "y": null
   },
   {
      "null": 224,
      "x": "Incorrect NaN gradient from distribution.Normal.log_prob when using subset",
      "z": "I think this is an autograd issue and not specific to distributions. The following code example supports my hypothesis:\n\n```python\nimport torch\na = torch.randn(4)\na[-1] = float('nan')\na.requires_grad_() # tensor([-1.6708, 0.6398, -0.9565, nan], requires_grad=True)\nb = torch.randn(4) # tensor([ 0.0499, -0.1655, 0.9131, 1.4798])\nc = (a - b)**2 # tensor([2.9609, 0.6485, 3.4954, nan], grad_fn=<PowBackward0>)\nl = c[:-1].sum()\nl.backward()\na.grad # tensor([-3.4414, 1.6106, -3.7392, nan])\n```\n\nIf I understand correctly, I think the last item of `a.grad` should be 0.",
      "y": null
   },
   {
      "null": 225,
      "x": "Build simple c++ example-cpp using Libtorch fails on arm with undefined reference to c10::Error::Error",
      "z": "I managed to successfully compile it on Jetson TX2 following these steps:\n\n1.Build PyTorch from source. `python3 setup.py build`\n\n2.No need to download LibTorch from the official website, which is the x86_64 build.\n\n3.build the example-cpp using compiled tmp_install directory: `cmake -DCMAKE_PREFIX_PATH=/absolute/path/to/pytorch/torch/lib/tmp_install ..`",
      "y": null
   },
   {
      "null": 226,
      "x": "The font size of equations in tutorial is too small",
      "z": "@brsoff It's much better now:\n![image](https://user-images.githubusercontent.com/1032377/50029350-78c85a00-ffc0-11e8-9476-6431abf918a0.png)",
      "y": null
   },
   {
      "null": 227,
      "x": "Pytorch 1.0.0 with Serverless and AWS Lambda timeout; Error in cpuinfo",
      "z": "Created an example project here showing how to deploy using [Serverless Application Model (SAM)](https://github.com/awslabs/serverless-application-model) which deploys your AWS Lambda functions + API GW easily here - https://github.com/mattmcclean/sam-pytorch-example",
      "y": null
   },
   {
      "null": 228,
      "x": "Problem \"torch.jit.trace()\"ing pretrained Inception_v3 model",
      "z": "@ShahriarSS you are giving an input with batch-size of 1 in training mode.\n\nEither of the two things will work:\n\n1. batch size of > 1:\n\n```\nexample = torch.rand(2, 3, 299, 299)\n```\n\n2. Put the model in eval mode:\n\n```\nmodel = torchvision.models.inception_v3(pretrained=True)\nmodel.eval()\n```\n\nHowever, that being said, I am seeing the following fail:\n\n```\nimport torch\nimport torchvision\n\nmodel = torchvision.models.inception_v3(pretrained=True)\nmodel.eval()\n\nexample = torch.randn(2, 3, 299, 299)\ntraced_script_module = torch.jit.trace(model, example)\n```\n\n```\n/scratch/soumith/pytorch/torch/jit/__init__.py:642: TracerWarning: Trace had nondeterministic nodes. Nodes:\n %x : Float(2, 2048, 1, 1) = aten::dropout(%input.266, %5133, %5134), scope: Inception3\nThis may cause errors in trace checking. To disable trace checking, pass check_trace=False to torch.jit.trace()\n _check_trace([example_inputs], func, executor_options, module, check_tolerance, _force_outplace)\n/scratch/soumith/pytorch/torch/jit/__init__.py:642: TracerWarning: Output nr 1. of the traced function does not match the corresponding output of the Python function. Detailed error:\nNot within tolerance rtol=1e-05 atol=1e-05 at input[0, 664] (3.1920981407165527 vs. -0.4343012273311615) and 1999 other locations (100.00%)\n _check_trace([example_inputs], func, executor_options, module, check_tolerance, _force_outplace)\n/scratch/soumith/pytorch/torch/jit/__init__.py:642: TracerWarning: Output nr 2. of the traced function does not match the corresponding output of the Python function. Detailed error:\nNot within tolerance rtol=1e-05 atol=1e-05 at input[0, 507] (-0.2636445462703705 vs. -1.134244441986084) and 1999 other locations (100.00%)\n _check_trace([example_inputs], func, executor_options, module, check_tolerance, _force_outplace)\n```\n\nConfirmed by @zdevito that this is a bug in the tracer.",
      "y": null
   },
   {
      "null": 229,
      "x": "Can't open x64.obj while installing pytorch with win10",
      "z": "Please remove `CUDNN_INCLUDE_DIR` and `CUDNN_LIBRARY` by using the following commands.\n```cmd\nset CUDNN_INCLUDE_DIR=\nset CUDNN_LIBRARY=\n```",
      "y": null
   },
   {
      "null": 230,
      "x": "torch.tril and torch.triu produce incorrect results with device='cuda'",
      "z": "@zou3519 Yes, they should. I am facing some issues w.r.t. the implementation and have opened a PR (#15257), so it would be nice if someone could help resolve them.",
      "y": null
   },
   {
      "null": 231,
      "x": "backward pass slow when using Conv2d",
      "z": "The best way to see what's going on with Intel MKL-DNN is to run the script with `MKLDNN_VERBOSE=1`.",
      "y": null
   },
   {
      "null": 232,
      "x": "Pytorch1.3 can't be compiled successfully with Python3.8.0",
      "z": "This is a relevant issue: https://bugzilla.redhat.com/show_bug.cgi?id=1718837 and the quick fix is to replace `nullptr /* tp_print */` with `0 /* tp_vectorcall_offset */`.",
      "y": null
   },
   {
      "null": 233,
      "x": "PyTorch1.3 cannot find kernel to dispatch when runing a Quantization Model",
      "z": "It works. Thanks!\n(For whom it may concern: The input data type should be `torch.quint8` in my case.)",
      "y": null
   },
   {
      "null": 234,
      "x": "torch.var fails to pass \"dim\" as tuple ints",
      "z": "your pytorch is too old so you are looking at docs of the wrong version. the multidim support is added after 1.0.1.",
      "y": null
   },
   {
      "null": 235,
      "x": "Store and retry futures that are created when nodes cleanup DistAutogradContext",
      "z": "Assigning to @osalpekar since he is going to look into retrying internal RPC messages across the board.",
      "y": null
   },
   {
      "null": 236,
      "x": "Implement destructor for ProcessGroupAgent that cleans up resources and doesn't wait",
      "z": "@rohan-varma The underlying problem is that the recv-from-any is not interruptible.\n\nInstead of hacking around this it should be addressed at the root. Detaching a thread means we'll invariably run into cleanup issues later on (either with the operation unblocking and accessing bad memory, or with zombie processes, or...). I created https://github.com/facebookincubator/gloo/issues/227 to track.",
      "y": null
   },
   {
      "null": 237,
      "x": "ROCm CI can fail test_sync_params_with_buffers",
      "z": "this has been disabled in https://github.com/pytorch/pytorch/commit/0b243e9c4c94bc611f99df02a15fac9b332e5432",
      "y": null
   },
   {
      "null": 238,
      "x": "RuntimeError: CUDA out of memory.",
      "z": "I think you should use `watch nvidia-smi` to monitor the GPU usage.",
      "y": null
   },
   {
      "null": 239,
      "x": "`default_qconfig` doesn't exist as an attribute, so it is better to label it as ``default_qconfig``",
      "z": "> what do you mean?\n\nIn the documentation we use `` :attr:`~torch.quantization.default_qconfig` ``, which is supposed to create a link to the documentation on that attribute. The problem is that `default_qconfig` is not an attribute (just a global variable), and the documentation will fail to create a link.\n\nInstead we should use ``` ``default_qconfig`` ```",
      "y": null
   },
   {
      "null": 240,
      "x": "Master process finishes and leave workers hanging when master and worker has different gpu numbers",
      "z": "IIUC, this line might be the problem: https://github.com/matthewygf/torch_interference/blob/master/image_classifier.py#L248. In Distributed Data Parallel, each worker needs to process exactly the same number of batches. Looking at your code it seems like the two processes on N3 are processing more number of batches and hence N1 finishes first and N3 is left hanging.",
      "y": null
   },
   {
      "null": 241,
      "x": "[ONNX] Export Pad(11) opset11",
      "z": "@dashesy this is being added in https://github.com/pytorch/pytorch/pull/28225",
      "y": null
   },
   {
      "null": 242,
      "x": "Can not load model",
      "z": "I tried your model (`gcn2_640x480.pt`), I'm running into a different error (both in Python `torch.jit.load` and C++ `torch::load`) on master (on Linux).\n\nFor\n```python\nimport torch\ntorch.jit.load('gcn2_640x480.pt')\n```\n\nI get\n\n```\nTraceback (most recent call last):\n File \"<stdin>\", line 1, in <module>\n File \"/scratch/davidriazati/dev/pytorch/torch/jit/__init__.py\", line 239, in load\n cpp_module = torch._C.import_ir_module(cu, f, map_location, _extra_files)\nRuntimeError: \nArguments for call are not valid.\nThe following operator variants are available:\n \n aten::grid_sampler(Tensor input, Tensor grid, int interpolation_mode, int padding_mode, bool align_corners) -> (Tensor):\n Argument align_corners not provided.\n\nThe original call is:\nat code/gcn2_480x640.py:67:22\n _25 = torch.slice(vu, 0, 0, 9223372036854775807, 1)\n _26 = torch.select(_25, 1, 0)\n _27 = torch.slice(vu, 0, 0, 9223372036854775807, 1)\n _28 = torch.select(_27, 1, 1)\n _29 = torch.to(_26, dtype=4, layout=0, device=torch.device(\"cuda:0\"), non_blocking=False, copy=False)\n _30 = torch.to(_28, dtype=4, layout=0, device=torch.device(\"cuda:0\"), non_blocking=False, copy=False)\n _31 = torch.unsqueeze(torch.index(CONSTANTS.c1, [_29, _30]), 1)\n ref_points = torch.cat([_24, _31], 1)\n grid = torch.view(ref_points, [1, 1, -1, 2])\n _32 = torch.squeeze(torch.grid_sampler(input, grid, 0, 0))\n ~~~~~~~~~~~~~~~~~~ <--- HERE\n desc_1 = torch.t(_32)\n desc_2 = torch.to(torch.gt(desc_1, 0), 6, False, False)\n _33 = ops.prim.NumToTensor(torch.size(desc_2, 0))\n desc_3 = torch.view(desc_2, [int(_33), 32, 8])\n desc_4 = torch.mul(desc_3, CONSTANTS.c2)\n desc = torch.sum(desc_4, [2], False)\n _34 = (_17, torch.to(desc, 0, False, False))\n return _34\n```\n\nUnzipping the `gcn2_640x480.pt` file and fixing that line in `./gcn2_480x640/code/gcn2_480x640.py` to be `_32 = torch.squeeze(torch.grid_sampler(input, grid, 0, 0, False))` makes the model load successfully, but that seems like a bug on our part (either the operator was changed or is not being serialized correctly). Could you try to load it using the nightly libtorch package? And does it load correctly in Python (via `torch.jit.load`)?",
      "y": null
   },
   {
      "null": 243,
      "x": "Module 'torch.nn' has no attribute 'backends'",
      "z": "@kamrankausar Try backend*s* with a s.",
      "y": null
   },
   {
      "null": 244,
      "x": "Adding a .fit() method to nn.Module",
      "z": "Please have a look at http://github.com/pytorch/ignite or https://skorch.readthedocs.io/en/stable/ or https://github.com/williamFalcon/pytorch-lightning which give you roughly what you are looking for. Based on what pietern said, we're unlikely to do this in core.",
      "y": null
   },
   {
      "null": 245,
      "x": "Torch.log2 + Torch.round() different and strange behaviour",
      "z": "I don't see the issue here?\n\nIn the first case, the parenthesis don't change anything. So all three are equal.\n\nIn the second case, you write `2^(log2(0.123)) == log2(2^0.123) == 0.123`\n\nAnd the last one `round(log2(2^0.123)) == 0`",
      "y": null
   },
   {
      "null": 246,
      "x": "DDP should set grad to None for (globally) unused params",
      "z": "@pritamdamania87 \n\nFor example, SGD optimizer does weight decay and momentum if grad is zero however skipping for None grad:\nhttps://github.com/pytorch/pytorch/blob/master/torch/optim/sgd.py#L89",
      "y": null
   },
   {
      "null": 247,
      "x": "RRef.to_here() and local_value() should return future instead of blocking",
      "z": "#27943 and #28025 should close this? @mrshenli",
      "y": null
   },
   {
      "null": 248,
      "x": "`CUDNN_STATUS_BAD_PARAM` reported during traced model execution",
      "z": "> in this case, it's just that the small input is too short for the 2nd convolution to work with (due to no padding).\n\nIn the current releases (cudnn7.6.5 and cudnn8.0.5) and a source build I get a proper error message and no cudnn failure:\n```python\nRuntimeError: Calculated padded input size per channel: (2). Kernel size: (3). Kernel size can't be greater than actual input size\n```",
      "y": null
   },
   {
      "null": 0,
      "x": "torch.utils.data.dataloader doesn't support multiprocessing with multiple workers",
      "z": "It\u00e2\u20ac\u2122s impossible to set your subprocess as daemonic and also ask to use multiprocessing workers. You should either not set your subprocess as daemonic or use num_workers=0. This isn\u00e2\u20ac\u2122t a bug at all. \n",
      "y": "It's impossible to set your subprocess as daemonic and also ask to use multiprocessing workers. You should either not set your subprocess as daemonic or use num_workers=0."
   },
   {
      "null": 1,
      "x": "Trouble building for ROCm",
      "z": "Closing ticket. Cannot reproduce with Ubuntu 20.04, ROCm 3.9, and reproduction step of USE_ROCM=1 USE_LMDB=1 USE_OPENCV=1 MAX_JOBS=8 python3 setup.py install --user.\n\nAlso note that our ROCm PyTorch CI jobs have been successful for a long while now, for example most of the ROCm 3.x release series.\n\nPlease reopen if you continue to face issues with ROCm PyTorch builds. Thanks.",
      "y": "Note that our ROCm PyTorch CI jobs have been successful for a long while now, for example most of the ROCm 3.x release series. Please check latest docs."
   },
   {
      "null": 0,
      "x": "Conda install gives 1.0.0 with CUDA 9.0",
      "z": "@LLNLanLeN upgrade your NVIDIA driver to latest, and that will likely fix the issue. cudatoolkit 10.1 requires much newer driver than 10.0",
      "y": "upgrade your NVIDIA driver to latest"
   },
   {
      "null": 1,
      "x": "[Feature Request]: Batchwise torch.lstsq",
      "z": "If you are only looking to compute the `solution` output from [`torch.lstsq`](https://pytorch.org/docs/stable/generated/torch.lstsq.html), but batch-wise, you could make use of [`torch.pinverse`](https://pytorch.org/docs/stable/generated/torch.pinverse.html) which is implemented to support batch-wise computation.\n\n> More about the relationship between the Moore-Penrose pseudo-inverse and ordinary least-squares can be found here: https://en.wikipedia.org/wiki/Moore%E2%80%93Penrose_inverse#Linear_least-squares\n\nSo that, if the take the example from the [`torch.lstsq`](https://pytorch.org/docs/stable/generated/torch.lstsq.html) documentation:\n\n# No batch support\n\n```python\nimport torch\n\nA = torch.tensor([[1., 1, 1],\n                  [2, 3, 4],\n                  [3, 5, 2],\n                  [4, 2, 5],\n                  [5, 4, 3]])\n\nB = torch.tensor([[-10., -3],\n                  [ 12, 14],\n                  [ 14, 12],\n                  [ 16, 16],\n                  [ 18, 16]])\n\nX, _ = torch.lstsq(B, A)\n\nX[:A.shape[1]]  # According to the doc: \"The first n rows of X contains the solution.\"\n\n# tensor([[2.0000, 1.0000],\n#         [1.0000, 1.0000],\n#         [1.0000, 2.0000]])\n```\n\n# With batch support\n\n```python\ndef batch_lstsq(\n    input: torch.Tensor,  # matrix B of shape (batch * m * k) \n    A: torch.Tensor  # matrix A of shape (batch * m * n) \n):\n   \n    X = torch.bmm(\n        torch.pinverse(A),\n        input\n    )\n \n    return X\n```\n\n## Tests\n\n- with the same tensors as the ones defined above, but repeated\n\n```python\nn_batch = 3\n\nA_batch = torch.cat(n_batch * [A.unsqueeze(0)])\nB_batch = torch.cat(n_batch * [B.unsqueeze(0)])\n\nsolution = batch_lstsq(B_batch, A_batch)\n\nfor i in range(len(solution)):    \n    \n    X, _ = torch.lstsq(B_batch[i], A_batch[i])\n    X = X[:A_batch[i].shape[1]]\n    \n    assert torch.allclose(solution[i], X[:A.shape[1]])\n```\n\n- with random tensors, works for both (m >= n) and (m < n)\n\n```python\nn_batch = 3\nm = 10  # 2\nn = 3\nk = 2\n\nA_batch = torch.rand(n_batch, m, n)\nB_batch = torch.rand(n_batch, m, k)\n\nsolution = batch_lstsq(B_batch, A_batch)\n\nfor i in range(len(solution)):    \n    \n    X, _ = torch.lstsq(B_batch[i], A_batch[i])\n    X = X[:A_batch[i].shape[1]]\n    \n    assert torch.allclose(solution[i], X[:A.shape[1]])\n```\n\n## Wall clock performance test\n\n- Setup\n```\nn_batch = 1000\n\nA_batch = torch.cat(n_batch * [A.unsqueeze(0)])\nB_batch = torch.cat(n_batch * [B.unsqueeze(0)])\n\nm = A_batch.shape[2]\n```\n\n- with a for loop as suggested by @Electric-Turtle \n\n```\n%%timeit\nX = torch.stack([torch.lstsq(B_batch[i], A_batch[i])[0][:m] for i in range(n_batch)],dim=0)\n\n# on my machine\n# 16.7 ms \u00b1 320 \u00b5s per loop (mean \u00b1 std. dev. of 7 runs, 100 loops each)\n```\n\n- when using batch-wise `torch.pinverse`\n```\n%%timeit\nX = batch_lstsq(B_batch, A_batch)\n\n# on my machine\n# 3.02 ms \u00b1 60.7 \u00b5s per loop (mean \u00b1 std. dev. of 7 runs, 100 loops each)\n```\n\n\n\n\n\n",
      "y": "you could make use of [`torch.pinverse`](https://pytorch.org/docs/stable/generated/torch.pinverse.html) which is implemented to support batch-wise computation.\n\n> More about the relationship between the Moore-Penrose pseudo-inverse and ordinary least-squares can be found here: https://en.wikipedia.org/wiki/Moore%E2%80%93Penrose_inverse#Linear_least-squares"
   },
   {
      "null": 2,
      "x": "[Android] Unknown builtin op: aten::mul",
      "z": "> Oh man, I went through the bother of reproducing the error by creating a simple app, then added `torch::autograd::AutoGradMode guard(false);` and that fixed everything (also in my real app)!\n\nGlad to know it helps. Did it fix the \"_adaptive_avg_pool2d_backward\" error or not?\n\nIn case you don't know, we also created a sample android app that you can play with: https://github.com/pytorch/android-demo-app\n\nMore information can be found at: https://pytorch.org/mobile/home/",
      "y": "add `torch::autograd::AutoGradMode guard(false);` "
   },
   {
      "null": 3,
      "x": "quantizationed model cannot inference with cuda?",
      "z": "That is correct, we don't support cuda currently.",
      "y": "That is correct, we don't support cuda currently."
   },
   {
      "null": 4,
      "x": "Can not use .cuda() function to load the model into GPU using Pytorch 1.3",
      "z": "@sayakpaul as the error message says, upgrade your CUDA driver, you installed the CUDA 10.1 compatible pytorch package which is the default.",
      "y": "upgrade your CUDA driver, you installed the CUDA 10.1 compatible pytorch package which is the default."
   },
   {
      "null": 5,
      "x": "Try torch.nn.quantized.functional.conv2d failed",
      "z": "Finally, it works and thanks for your help!\n```\nqF.conv2d(q_inputs, q_filters, bias)\n```\nWhat's more, the data type of convolutional kernals seems to be quint8, and data type of intput tensor is qint8.",
      "y": "```\nqF.conv2d(q_inputs, q_filters, bias)\n```\nthe data type of convolutional kernals seems to be quint8, and data type of intput tensor is qint8."
   },
   {
      "null": 6,
      "x": "Support dictionary outputs in TorchScript tracer",
      "z": "@mohammedayub44 this is a feature after 1.5, you have to install master or nightly builds to get this feature, or wait for 1.6 :)",
      "y": "this is a feature after 1.5, you have to install master or nightly builds to get this feature, or wait for 1.6"
   },
   {
      "null": 7,
      "x": "[feature request] Batched (n-1)-D to n-D matrix_diag",
      "z": "@chrelli check my comment in https://github.com/pytorch/pytorch/issues/5198#issuecomment-425069863\nHere is a proposed implementation\n```python\ndef matrix_diag(diagonal):\n    N = diagonal.shape[-1]\n    shape = diagonal.shape[:-1] + (N, N)\n    device, dtype = diagonal.device, diagonal.dtype\n    result = torch.zeros(shape, dtype=dtype, device=device)\n    indices = torch.arange(result.numel(), device=device).reshape(shape)\n    indices = indices.diagonal(dim1=-2, dim2=-1)\n    result.view(-1)[indices] = diagonal\n    return result\n```",
      "y": "Here is a proposed implementation\n```python\ndef matrix_diag(diagonal):\n    N = diagonal.shape[-1]\n    shape = diagonal.shape[:-1] + (N, N)\n    device, dtype = diagonal.device, diagonal.dtype\n    result = torch.zeros(shape, dtype=dtype, device=device)\n    indices = torch.arange(result.numel(), device=device).reshape(shape)\n    indices = indices.diagonal(dim1=-2, dim2=-1)\n    result.view(-1)[indices] = diagonal\n    return result\n```"
   },
   {
      "null": 8,
      "x": "[caffe2] VS2017 compiler error for ATen",
      "z": "I hit the same problem and updating to VS 2017 15.9.0 also fixed it for me. I'm not trying to build with CUDA, but have CUDA 10.0 installed.",
      "y": "updating to VS 2017 15.9.0 fixes it. I'm not trying to build with CUDA, but have CUDA 10.0 installed."
   },
   {
      "null": 9,
      "x": "GPU hangs after killing the program using DistributedDataparallel Model",
      "z": "hi, i  meet the same issue, my solution is :\ntry\n`ps aux|grep python` \nand then \n`kill -9 PID`\nbe sure that kill process **in order**",
      "y": "`ps aux|grep python` \nand then \n`kill -9 PID`\nbe sure that kill process **in order**"
   },
   {
      "null": 10,
      "x": "Error building a custom PyTorch CUDA extension with 1.0 on macOS High Sierra",
      "z": "We deprecated and removed FFI two days ago. Please try to build our new C++ extension API: https://pytorch.org/tutorials/advanced/cpp_extension.html",
      "y": "Please try to build our new C++ extension API: https://pytorch.org/tutorials/advanced/cpp_extension.html"
   },
   {
      "null": 11,
      "x": "get_cudnn_version in collect_env is flaky",
      "z": "That's because LD_LIBRARY_PATH is not the only path used for lookup.",
      "y": "That's because LD_LIBRARY_PATH is not the only path used for lookup."
   },
   {
      "null": 12,
      "x": "Slowdown in distributions log_prob methods",
      "z": "Yes some of my benchmarks showed that the main slowdown in `log_prob`was in `pow` but that slowdown doesn't scale with input shapes, which might be related to some tensor overhead we added after 0.4. \nSorry didn't have time to get back to this last week, will do this week. ",
      "y": "Yes some of my benchmarks showed that the main slowdown in `log_prob`was in `pow`"
   },
   {
      "null": 13,
      "x": "[Feature Request]Synchronized batch norm",
      "z": "If anyone happens to be struggling to get the `DistributedDataParallel` to use `SyncBatchnorm`, take a look at this small step-by-step a colleague of mine wrote: [github/dougsouza/pytorch-sync-batchnorm-example](https://github.com/dougsouza/pytorch-sync-batchnorm-example). Hope it helps. :)",
      "y": " take a look at this: [github/dougsouza/pytorch-sync-batchnorm-example](https://github.com/dougsouza/pytorch-sync-batchnorm-example)."
   },
   {
      "null": 14,
      "x": "Non Deterministic Behaviour even after cudnn.deterministic = True and cudnn.benchmark=False",
      "z": "I don't think there is much to do, and FGSA is intentionally discontinuous (due to the sign). What you could do is determine the digit where it is unstable empirically and round that away before taking the sign.\n\nThat said, the reason is likely that the backward uses the cuda function `atomicAdd`, which is non-deterministic. For a factor of two, it would seem that\n```python\nclass MyUpsample2(nn.Module):\n    def forward(self, x):\n        return x[:, :, :, None, :, None].expand(-1, -1, -1, 2, -1, 2).reshape(x.size(0), x.size(1), x.size(2)*2, x.size(3)*2)\n```\nmakes this deterministic.\n\nI'll probably add a note to https://github.com/pytorch/pytorch/blob/master/docs/source/notes/randomness.rst about atomic add (Users include: index_add, scatter_add, bincount, lossctc in forward, embedding/embedding_bag, all sorts of pooling, padding, sampling in backward, possibly also sparse to dense or coalescing, but I didn't check).\n",
      "y": "```python\nclass MyUpsample2(nn.Module):\n    def forward(self, x):\n        return x[:, :, :, None, :, None].expand(-1, -1, -1, 2, -1, 2).reshape(x.size(0), x.size(1), x.size(2)*2, x.size(3)*2)\n```\nmakes this deterministic.\n"
   },
   {
      "null": 15,
      "x": "PackagesNotFoundError: unable to install torchvision in anaconda prompt on windows 10",
      "z": "Use `pip install torchvision` instead. It is not yet available in Anaconda Cloud.",
      "y": "Use `pip install torchvision` instead. It is not yet available in Anaconda Cloud."
   },
   {
      "null": 16,
      "x": "Weight decay modifies grad for SGD but not for Adam",
      "z": "no adam modifies it in place see:\n\nhttps://github.com/pytorch/pytorch/blob/master/torch/optim/adam.py#L90\n",
      "y": "no adam modifies it in place see:\n\nhttps://github.com/pytorch/pytorch/blob/master/torch/optim/adam.py#L90\n"
   },
   {
      "null": 17,
      "x": "Bug in masked_fill_ for non contiguous tensors",
      "z": "> `expanded_tensor * scalar` now copies it into a non contiguous tensor on linux\n\nGood to know! We're relying on it triggering a `.contiguous()` in a number of places in Pyro.",
      "y": "> `expanded_tensor * scalar` now copies it into a non contiguous tensor on linux"
   },
   {
      "null": 18,
      "x": "[Conv3D][cudnn] CUDNN_STATUS_NOT_INITIALIZED error when using batch size > 1 with conv layer",
      "z": "@zou3519  Thank you for the reply. Unfortunately due to a non disclosure agreement, I am not allowed to upload code or parts of the code. But I found the solution. I was using a custom collate function for dataloader and one of the variables was initialized wrong. I was copying a tensor (index wise) to another, based on the indices in the wrongly initialized variable right before the conv3d layer. So it was more like an \"out of range\" error or something and I was confused by the CUDNN_STATUS_NOT_INITIALIZED error statement. ",
      "y": "It is more like an \"out of range\" error or something and confused by the CUDNN_STATUS_NOT_INITIALIZED error statement. "
   },
   {
      "null": 19,
      "x": "Documentation not clear on torch.expand() alternatives when performing torch.autograd.gradcheck",
      "z": "The note only talks about input though. It should have nothing to do with how your function is implemented. All you need to make sure is that the input used in gradcheck don't have overlapping storage.",
      "y": "All you need to make sure is that the input used in gradcheck don't have overlapping storage."
   },
   {
      "null": 20,
      "x": "Build failed when linking bin/test_parallel with multiple openmp functions defined",
      "z": "I managed to complete the build+install process with a quick and dirty workaround:\n\n> find . -type f -name 'build.make' -exec sed -i 's:^.*\\: /usr/lib/x86_64-linux-gnu/libiomp5.so::g' {} \\;\n> find . -type f -name 'link.txt' -exec sed -i 's:/usr/lib/x86_64-linux-gnu/libiomp5.so::g' {} \\;\n\nI think I've run these commands mainly in `build/caffe2/`, `build/test_api/` and `build/test_jit/` one can simply run them in `build` and everything should build fine.\n",
      "y": "\n> find . -type f -name 'build.make' -exec sed -i 's:^.*\\: /usr/lib/x86_64-linux-gnu/libiomp5.so::g' {} \\;\n> find . -type f -name 'link.txt' -exec sed -i 's:/usr/lib/x86_64-linux-gnu/libiomp5.so::g' {} \\;\n\nI think I've run these commands mainly in `build/caffe2/`, `build/test_api/` and `build/test_jit/` one can simply run them in `build` and everything should build fine.\n"
   },
   {
      "null": 21,
      "x": "CTCLoss CUDA backward throws setStorage: storage size mismatch error",
      "z": "The canonical way is to build torchvision from source, it's really easy (much more so than PyTorch itself).\n",
      "y": "The canonical way is to build torchvision from source, it's really easy (much more so than PyTorch itself).\n"
   },
   {
      "null": 22,
      "x": "In-place operation on differentiable view leaks memory",
      "z": "This is a ref-cycle in cpp that goes as `x -> CopySlices -> AccumulateGrad -> x`.\nThis only happens because `x` here is a leaf Tensor.\nNote that after doing such thing, trying to run backward will fail with \"leaf variable has been moved into the graph interior\" so we should fail earlier before the cycle is created.\nThis can be done by making [this check](https://github.com/pytorch/pytorch/blob/6249d7302b7277864ed0ade93f58d88ee0cd3aa8/torch/csrc/autograd/VariableTypeUtils.h#L44) detect views of leafs in addition to only leafs.",
      "y": "This is a ref-cycle in cpp that goes as `x -> CopySlices -> AccumulateGrad -> x`.\nThis only happens because `x` here is a leaf Tensor.\nNote that after doing such thing, trying to run backward will fail with \"leaf variable has been moved into the graph interior\" so we should fail earlier before the cycle is created."
   },
   {
      "null": 23,
      "x": "Converting from IValue to double gives INTERNAL ASSERT failure",
      "z": "The `.to...` methods don't do any conversion, you'll have to cast the IValue to a tensor `.toTensor()` then get the value out. The `toDouble()` method only works if the `IValue` contained is a C++ `double` type",
      "y": "The `.to...` methods don't do any conversion, you'll have to cast the IValue to a tensor `.toTensor()` then get the value out. The `toDouble()` method only works if the `IValue` contained is a C++ `double` type"
   },
   {
      "null": 24,
      "x": "cmake error build failed",
      "z": "Two options available: \n1. unset `CMAKE_GENERATOR_TOOLSET_VERSION` \n```cmd\nset CMAKE_GENERATOR_TOOLSET_VERSION=\n```\n2. activate the env.\n```cmd\nset DISTUTILS_USE_SDK=1\nfor /f \"usebackq tokens=*\" %i in (`\"%ProgramFiles(x86)%\\Microsoft Visual Studio\\Installer\\vswhere.exe\" -version [15^,16^) -products * -latest -property installationPath`) do call \"%i\\VC\\Auxiliary\\Build\\vcvarsall.bat\" x64 -vcvars_ver=%CMAKE_GENERATOR_TOOLSET_VERSION%\n```\nIt is clearly written in the doc through the previous link. If it is still broken, then please check your VS installation.",
      "y": "Two options available: \n1. unset `CMAKE_GENERATOR_TOOLSET_VERSION` \n```cmd\nset CMAKE_GENERATOR_TOOLSET_VERSION=\n```\n2. activate the env.\n```cmd\nset DISTUTILS_USE_SDK=1\nfor /f \"usebackq tokens=*\" %i in (`\"%ProgramFiles(x86)%\\Microsoft Visual Studio\\Installer\\vswhere.exe\" -version [15^,16^) -products * -latest -property installationPath`) do call \"%i\\VC\\Auxiliary\\Build\\vcvarsall.bat\" x64 -vcvars_ver=%CMAKE_GENERATOR_TOOLSET_VERSION%\n```\nIt is clearly written in the doc through the previous link. If it is still broken, then please check your VS installation."
   },
   {
      "null": 25,
      "x": "Multiprocessing and tensor problem",
      "z": "To be more precise it still happens if I use the no grad function, hence no backprop involved. So, the problem is returning a pytorch tensor, I think. If I return the pytorch tensor as a list using the related tolist() method everything is fine. I don't know if this is expected or not.",
      "y": "To be more precise it still happens if I use the no grad function, hence no backprop involved. So, the problem is returning a pytorch tensor, I think. If I return the pytorch tensor as a list using the related tolist() method everything is fine. I don't know if this is expected or not."
   },
   {
      "null": 26,
      "x": "Bug in saving indexed torch tensors makes it much slower than numpy.",
      "z": "this is not a bug, it's actually by design.\n\nwhen you index individual elements into a Tensor, then you are holding a view into the original Tensor.\n\n```\na = torch.randn(10, 20)\nb = a[0]\nb.add_(10) # changes `a`\n```\n\nIf you actually do `torch.save([a, b], 'foo.pt')` then PyTorch makes sure that this \"view\" property is actually preserved across serialization / deserialization.\nSo, \n\n```\na = torch.randn(10, 20)\nb = a[0]\nb.add_(10) # changes `a`\n\ntorch.save((a, b), 'foo.pt')\na, b = torch.save('foo.pt')\n\nb.add_(10) # still changes `a`\n```\n\nSo, what you are seeing is that even when you are indexing, the whole original Storage is saved.\n\nYou can make pytorch only save the particular element with `torch.save(a[0].clone()`",
      "y": "this is not a bug, it's actually by design.\n\nwhen you index individual elements into a Tensor, then you are holding a view into the original Tensor.\n\n```\na = torch.randn(10, 20)\nb = a[0]\nb.add_(10) # changes `a`\n```\n\nIf you actually do `torch.save([a, b], 'foo.pt')` then PyTorch makes sure that this \"view\" property is actually preserved across serialization / deserialization."
   },
   {
      "null": 27,
      "x": "Unexpected output size for Maxpool",
      "z": "\nThe implementation should allow for the output size to be 0 if the kernel is larger than input.\n\nFor example, if ** i=2, p=0, d=1, s=2, k=3**, the kernel is 1 pixel larger than the input.\n\nThe formula says output size =  **(2 - 3 )/ 2 + 1** =  **-1/2 + 1**\n\nHowever, in  the C++ implementation above, because of truncation (and not flooring) we get output size = 1. In Python we get 0.\n\nIf k=4, then the C++ behavior becomes correct again, and we get output size = 0. It seems like a corner case in the C++ implementation. The  floor function seems correct to use, versus rounding towards zero.\n",
      "y": "\nThe implementation should allow for the output size to be 0 if the kernel is larger than input.\n\nFor example, if ** i=2, p=0, d=1, s=2, k=3**, the kernel is 1 pixel larger than the input.\n\nThe formula says output size =  **(2 - 3 )/ 2 + 1** =  **-1/2 + 1**\n\nHowever, in  the C++ implementation above, because of truncation (and not flooring) we get output size = 1. In Python we get 0.\n\nIf k=4, then the C++ behavior becomes correct again, and we get output size = 0. It seems like a corner case in the C++ implementation. The  floor function seems correct to use, versus rounding towards zero.\n"
   },
   {
      "null": 28,
      "x": "base_lrs in torch.optim.lr_scheduler.CyclicLR gets overriden by parent class if parameter groups have 'initial_lr' set",
      "z": "~~Schedulers are not currently chainable #13022, and so switching from one to another overrides the past. This should be considered for #24352.~~",
      "y": "Schedulers are not currently chainable "
   },
   {
      "null": 29,
      "x": "Implement covariance_matrix for Independent distributions",
      "z": "@fehiepsi showed me this trick for batch-compatible `torch.diag()` (I think that's what you're asking for, @justindomke )\n```py\ndef batch_diag(batched_variance):\n    batch_shape = batched_variance.shape[:-1]\n    event_size = batched_variance.size(-1)\n    cov = batched_variance.new_zeros(batch_shape + (event_size * event_size,))\n    cov[..., ::1 + event_size] = batched_variance\n    return cov.reshape(batch_shape + (event_size, event_size))\n```",
      "y": "```py\ndef batch_diag(batched_variance):\n    batch_shape = batched_variance.shape[:-1]\n    event_size = batched_variance.size(-1)\n    cov = batched_variance.new_zeros(batch_shape + (event_size * event_size,))\n    cov[..., ::1 + event_size] = batched_variance\n    return cov.reshape(batch_shape + (event_size, event_size))\n```"
   },
   {
      "null": 30,
      "x": "About CMAKE_PREFIX_PATH",
      "z": "OK I fixed that.\n\n`CMAKE_PREFIX_PATH` should be a folder containing `TorchConfig.cmake` which is `torch/share/cmake/Torch` \n\nmahmood@m2000:build$ cmake -DCMAKE_PREFIX_PATH=/home/mahmood/pytorch/torch/share/cmake/Torch/ ..\n-- The C compiler identification is GNU 7.4.0\n-- The CXX compiler identification is GNU 7.4.0\n-- Check for working C compiler: /usr/bin/cc\n-- Check for working C compiler: /usr/bin/cc -- works\n-- Detecting C compiler ABI info\n-- Detecting C compiler ABI info - done\n-- Detecting C compile features\n-- Detecting C compile features - done\n-- Check for working CXX compiler: /usr/bin/c++\n-- Check for working CXX compiler: /usr/bin/c++ -- works\n-- Detecting CXX compiler ABI info\n-- Detecting CXX compiler ABI info - done\n-- Detecting CXX compile features\n-- Detecting CXX compile features - done\n-- Looking for pthread.h\n-- Looking for pthread.h - found\n-- Looking for pthread_create\n-- Looking for pthread_create - not found\n-- Looking for pthread_create in pthreads\n-- Looking for pthread_create in pthreads - not found\n-- Looking for pthread_create in pthread\n-- Looking for pthread_create in pthread - found\n-- Found Threads: TRUE\n-- Found CUDA: /usr/local/cuda-10.0 (found version \"10.0\")\n-- Caffe2: CUDA detected: 10.0\n-- Caffe2: CUDA nvcc is: /usr/local/cuda-10.0/bin/nvcc\n-- Caffe2: CUDA toolkit directory: /usr/local/cuda-10.0\n-- Caffe2: Header version is: 10.0\n-- Found CUDNN: /usr/local/cuda-10.0/include\n-- Found cuDNN: v7.4.2  (include: /usr/local/cuda-10.0/include, library: /usr/local/cuda-10.0/lib64/libcudnn.so)\n-- Autodetected CUDA architecture(s):  5.2\n-- Added CUDA NVCC flags for: -gencode;arch=compute_52,code=sm_52\n-- Found torch: /home/mahmood/pytorch/torch/lib/libtorch.so\n-- Downloading MNIST dataset\n-- Configuring done\n-- Generating done\n-- Build files have been written to: /home/mahmood/pytorch/examples/cpp/dcgan/build\n```",
      "y": "`CMAKE_PREFIX_PATH` should be a folder containing `TorchConfig.cmake` which is `torch/share/cmake/Torch` "
   },
   {
      "null": 31,
      "x": "ReduceLROnPlateau parent class is not _LRScheduler",
      "z": "`ReduceLROnPlateau` overrides every `_LRScheduler` function. We never had need to have `_LRScheduler` as parent. \n\nPlus, as already mentioned, usage of any internal API names is a terrible idea. \n\nI will close the issue as non-bug. However feel free to send us PR which will introduce good method of checking if object is Scheduler",
      "y": "`ReduceLROnPlateau` overrides every `_LRScheduler` function. We never had need to have `_LRScheduler` as parent. "
   },
   {
      "null": 32,
      "x": "\"Trying to resize storage that is not resizable\" when calling pin_memory() on some zero-dimensional tensors",
      "z": "@qbx2 This bug seems to be fixed on master, could you try installing PyTorch via `pip install torch_nightly -f https://download.pytorch.org/whl/nightly/cu100/torch_nightly.html` and then run the code again?",
      "y": "This bug seems to be fixed on master, could you try installing PyTorch via `pip install torch_nightly -f https://download.pytorch.org/whl/nightly/cu100/torch_nightly.html` and then run the code again?"
   },
   {
      "null": 33,
      "x": "Need GPU implementation of dirichlet_grad (originally: Reparameterized gradient on GPU for beta / Dirichlet)",
      "z": "I find it surprising that there is no implementation of the Dirichlet gradient because there is already an implementation of the Gamma gradient. The standard way to generate a Beta variable is to generate two Gamma variables (see for instance [Wikipedia](https://en.wikipedia.org/wiki/Beta_distribution#Generating_beta-distributed_random_variates)) and compute their fractional ratio. For instance, the following fix on the code of @vmoens seems to work:\n\n    a,b = torch.ones(3,4,1,5,requires_grad=True,device='cuda'),torch.ones(3,4,1,5,requires_grad=True,device='cuda')\n    s1 = torch.distributions.Gamma(a,1).rsample(torch.Size((10,)))\n    s2 = torch.distributions.Gamma(b,1).rsample(torch.Size((10,)))\n    s = s1 / (s1+s2)\n    torch.sum(s).backward()\n    a.grad\n\nSame goes for sampling Dirichlet variables, one actually samples independent Gamma variables. So, right now users should be able to use this fix, but it seems to me that it should not be difficult to implement native support for Beta gradients with CUDA because all the functions are already there.\n\n@ezyang and @vishwakftw I posted a similar comment on related issue https://github.com/pytorch/pytorch/issues/11030.",
      "y": "I find it surprising that there is no implementation of the Dirichlet gradient because there is already an implementation of the Gamma gradient. The standard way to generate a Beta variable is to generate two Gamma variables (see for instance [Wikipedia](https://en.wikipedia.org/wiki/Beta_distribution#Generating_beta-distributed_random_variates)) and compute their fractional ratio. For instance, the following fix on the code of @vmoens seems to work:\n\n    a,b = torch.ones(3,4,1,5,requires_grad=True,device='cuda'),torch.ones(3,4,1,5,requires_grad=True,device='cuda')\n    s1 = torch.distributions.Gamma(a,1).rsample(torch.Size((10,)))\n    s2 = torch.distributions.Gamma(b,1).rsample(torch.Size((10,)))\n    s = s1 / (s1+s2)\n    torch.sum(s).backward()\n    a.grad\n"
   },
   {
      "null": 34,
      "x": "Seems there's memory issue in pytorch 1.0.0",
      "z": "I don't know what exactly would be the issue, but I experienced something somewhat similar, which I actually managed to pinpoint. The reason for my memory increase was using DataLoader with custom Dataset (subclassing from torch.utils.data.Dataset) where I was storing loading data in a dict instead of a simple list to accommodate shuffling. However, the indices going to method `__getitem__` (generated by DataLoader) were not simple integers as I anticipated (and as the doc would suggest, https://pytorch.org/docs/stable/data.html#torch.utils.data.Dataset) but 0-D dimensional integer tensors. And since these don't have the `__hash__` depending on a value, rather on memory location, the dict would grow and grow and grow, because it didn't know there was something in it already. Hope I made it somewhat clear. To illustrate it more:\n\n    class LiDARDataset(torch.utils.data.Dataset):\n        def __init__(self, folder, keep_ram=False):\n            self.names = sorted(glob.glob(os.path.join(folder, '*.npy')))\n            self.keep_ram = keep_ram\n            if self.keep_ram:\n                self.loaded = dict()\n            else:\n                self.loaded = weakref.WeakValueDictionary()\n\n        def __len__(self):\n            return len(self.names)\n\n        def __getitem__(self, key):\n            if not isinstance(key, int): # without these two lines, it would grow\n                key = key.item() \n            result = self.loaded.get(key) # because the result would always be None\n            if result is None:\n                result = self.transform(np.load(self.names[key]), key)\n                self.loaded[key] = result\n            return result\n\nSo, check you don't have any dictionary, which is indexed by torch.tensor, because that is what is changed since 0.4.1. Or you don't rely on hashing of torch.tensor by its value rather than the memory location",
      "y": " check you don't have any dictionary, which is indexed by torch.tensor, because that is what is changed since 0.4.1. Or you don't rely on hashing of torch.tensor by its value rather than the memory location"
   },
   {
      "null": 35,
      "x": "torch::save causes serialization error on mnist example",
      "z": "You need to wrap the torch::nn::Module object (i.e., model) with torch::nn::ModuleHolder",
      "y": "You need to wrap the torch::nn::Module object (i.e., model) with torch::nn::ModuleHolder"
   },
   {
      "null": 36,
      "x": "Implementation of ISTFT",
      "z": "@SsnL @vincentqb Seems that ISTFT has been recently contributed to torchaudio: https://pytorch.org/audio/functional.html#istft\n\nIt would be great if it was relocated to the core",
      "y": "Seems that ISTFT has been recently contributed to torchaudio: https://pytorch.org/audio/functional.html#istft"
   },
   {
      "null": 37,
      "x": "Dropout on integer tensor types dies with SIGFPE",
      "z": "@mruberry Think this issue should be closed since the signal is no longer raised.\n\n~~Instead we get this errors:~~\n\n~~```RuntimeError: Integer division of tensors using div or / is no longer supported, and in a future release div will perform true division as in Python 3. Use true_divide or floor_divide (// in Python) instead.```~~\n\n~~in both cases (the one with Dropout and the second one from @colesbury).~~ \n\nRan the code with the nightly build.\n\nFor the first example this error is raised:\n`RuntimeError: result type Float can't be cast to the desired output type Long`\n\nand for the second one (the one from @colesbury) there is no error, instead we get: \n`tensor([inf, nan, nan, nan, inf])`\n\nI think now it's more clear to the user what's the problem.",
      "y": "@mruberry Think this issue should be closed since the signal is no longer raised.\n\n~~Instead we get this errors:~~\n\n~~```RuntimeError: Integer division of tensors using div or / is no longer supported, and in a future release div will perform true division as in Python 3. Use true_divide or floor_divide (// in Python) instead.```~~\n\n~~in both cases (the one with Dropout and the second one from @colesbury).~~ \n\nRan the code with the nightly build.\n\nFor the first example this error is raised:\n`RuntimeError: result type Float can't be cast to the desired output type Long`\n\nand for the second one (the one from @colesbury) there is no error, instead we get: \n`tensor([inf, nan, nan, nan, inf])`\n\nI think now it's more clear to the user what's the problem."
   },
   {
      "null": 38,
      "x": "THCudaCheck FAIL file=/pytorch/aten/src/THC/THCGeneral.cpp line=405 error=11 : invalid argument",
      "z": "I have successfully solved this problem by updating the cuda8.0 to cuda10.0.",
      "y": "I have successfully solved this problem by updating the cuda8.0 to cuda10.0."
   },
   {
      "null": 39,
      "x": "Memory leak during backprop() in PyTorch 1.0.0",
      "z": "tl;dr After investigation I determined that the reinforcement learning tutorial doesn't leak memory on pytorch 1.2. However, we are committed to making sure there are no more memory leaks so please create new issues with individual new examples of memory leaks and we will investigate with high priority.\n\nI tried to repro this on my mac. I don't have a linux box with a monitor (which is a requirement to run the tutorial) but I suspect that the result should be the same.\n\nMy conclusion was that our reinforcement learning tutorial does NOT leak memory. However, it does grow in memory consumption. The reason why it grows in memory consumption is due to the model storing replay memory: https://github.com/pytorch/tutorials/blob/94cb6a3635b9b5ccb8a59c5346addba6901cd15d/intermediate_source/reinforcement_q_learning.py#L336\n\nOnce the replay memory limit is hit, the RAM consumption stays constant. This can be verified by setting the replay number to something low (i.e. 10) and watching RAM.\n\nI know that this wasn't completely satisfactory because I'm sure that some of you have encountered memory leaks while training in PyTorch, but we are committed to fixing them so please submit additional bug reports about memory leaks for your individual examples and we can investigate them separately. \n\n",
      "y": "tl;dr After investigation I determined that the reinforcement learning tutorial doesn't leak memory on pytorch 1.2. However, we are committed to making sure there are no more memory leaks so please create new issues with individual new examples of memory leaks and we will investigate with high priority.\n\nI tried to repro this on my mac. I don't have a linux box with a monitor (which is a requirement to run the tutorial) but I suspect that the result should be the same.\n\nMy conclusion was that our reinforcement learning tutorial does NOT leak memory. However, it does grow in memory consumption. The reason why it grows in memory consumption is due to the model storing replay memory: https://github.com/pytorch/tutorials/blob/94cb6a3635b9b5ccb8a59c5346addba6901cd15d/intermediate_source/reinforcement_q_learning.py#L336\n\nOnce the replay memory limit is hit, the RAM consumption stays constant. This can be verified by setting the replay number to something low (i.e. 10) and watching RAM.\n\nI know that this wasn't completely satisfactory because I'm sure that some of you have encountered memory leaks while training in PyTorch, but we are committed to fixing them so please submit additional bug reports about memory leaks for your individual examples and we can investigate them separately. \n\n"
   },
   {
      "null": 40,
      "x": "Debugging feature for \"modified by an inplace operation\" errors",
      "z": "> which points directly to `b.exp_()`, and indeed, if you change that line to be `b.exp()`, it all works fine.\n\nTo clarify for other readers, the anomaly detection will *not* necessarily point you at the inplace operation that caused the failure. Instead, it will point you at the operation that could not compute its gradient in the backward pass. The inplace operation to blame may occur anywhere after that, modifying one of the tensors that participated in the line found by the anomaly detection.\n\nExample:\n```\nx = torch.rand(10, 20, requires_grad=True)\ny = torch.rand(10)\nz = (x / y[:, np.newaxis])  # anomaly detection will point here\nc = y.abs_()  # but the problem is here\nz.sum().backward()\n```\nThe last line will cause a `RuntimeError`. With anomaly detection enabled, it will point at the line performing the division, but the inplace operation came later.",
      "y": "> which points directly to `b.exp_()`, and indeed, if you change that line to be `b.exp()`, it all works fine.\n\nTo clarify for other readers, the anomaly detection will *not* necessarily point you at the inplace operation that caused the failure. Instead, it will point you at the operation that could not compute its gradient in the backward pass. The inplace operation to blame may occur anywhere after that, modifying one of the tensors that participated in the line found by the anomaly detection.\n\nExample:\n```\nx = torch.rand(10, 20, requires_grad=True)\ny = torch.rand(10)\nz = (x / y[:, np.newaxis])  # anomaly detection will point here\nc = y.abs_()  # but the problem is here\nz.sum().backward()\n```\nThe last line will cause a `RuntimeError`. With anomaly detection enabled, it will point at the line performing the division, but the inplace operation came later."
   },
   {
      "null": 41,
      "x": "Keyboard interrupt and saving the last state of a model",
      "z": "This is all you need to implement this. I don't see why would this have to be part of the library.\n```python\ntry:\n    # training code here\nexcept KeyboardInterrupt:\n    # save model here\n```",
      "y": "This is all you need to implement this. I don't see why would this have to be part of the library.\n```python\ntry:\n    # training code here\nexcept KeyboardInterrupt:\n    # save model here\n```"
   },
   {
      "null": 42,
      "x": "DataLoader freezes randomly when num_workers > 0 (Multiple threads train models on different GPUs in separate threads)",
      "z": "I have solved the problem by using processes instead of threads. So in my main python file where I generate all possible hyperparameter combinations and then train each combination on a different GPU, I do not create a new thread and train a model in each thread. Instead, I fork new processes (one for each GPU) and create a `multiprocessing.Queue` that is passed to each subprocess. This queue contains hyperparameters to be trained. Each process is working on the same queue and training models on a dedicated GPU. \n\nIn my tests, this works even with num_workers=32. There was no freezing for 3 hours now. Usually it freezed much earlier (after a few minutes), so I hope that it is really fixed.\n\nSince it is working with `Process` and not with `Thread`, I assume that there is a bug in PyTorch's `DataLoader` class that causes the freezes when multiple `DataLoader`s are used in different threads. This problem does not occur with processes, because they have their private memory.",
      "y": "I have solved the problem by using processes instead of threads. So in my main python file where I generate all possible hyperparameter combinations and then train each combination on a different GPU, I do not create a new thread and train a model in each thread. Instead, I fork new processes (one for each GPU) and create a `multiprocessing.Queue` that is passed to each subprocess. This queue contains hyperparameters to be trained. Each process is working on the same queue and training models on a dedicated GPU. \n\nIn my tests, this works even with num_workers=32. There was no freezing for 3 hours now. Usually it freezed much earlier (after a few minutes), so I hope that it is really fixed.\n\nSince it is working with `Process` and not with `Thread`, I assume that there is a bug in PyTorch's `DataLoader` class that causes the freezes when multiple `DataLoader`s are used in different threads. This problem does not occur with processes, because they have their private memory."
   },
   {
      "null": 43,
      "x": "[JIT] Support Meta programming on If self.training to conditionally NOT compile training only code",
      "z": "Hi @sidazhang, thanks for reporting this. Actually `self.training` is already supported in JIT, training and inference subgraph should be both exists in the generated IR, and will depend on the actual value of self.training to execute the corresponding if branch. So we don't need to meta program it. \n\nThe problem or workaround on your example is that, you should also annotate the function `training_code_only` with `@torch.jit.script_method`, because although we can run it without making it a script_method, we cannot serialize a python function call if you want to save the graph. \n\nEdited: if `training_code_only` is a TorchScript incompatible logic and cannot be rewrite to make it compatible, can you describe what is actually not compatible so that we could fix it instead? A simple for loop like the above can be simply rewrite to make it compatible. We don't want to add additional meta programming conditions as those logics are kinda hacks to get around some special syntax, also in many cases we could not know the actual value of `self.training` until runtime, so meta programming it might not be a generic solution. ",
      "y": "Hi @sidazhang, thanks for reporting this. Actually `self.training` is already supported in JIT, training and inference subgraph should be both exists in the generated IR, and will depend on the actual value of self.training to execute the corresponding if branch. So we don't need to meta program it. \n\nThe problem or workaround on your example is that, you should also annotate the function `training_code_only` with `@torch.jit.script_method`, because although we can run it without making it a script_method, we cannot serialize a python function call if you want to save the graph. \n\nEdited: if `training_code_only` is a TorchScript incompatible logic and cannot be rewrite to make it compatible, can you describe what is actually not compatible so that we could fix it instead? A simple for loop like the above can be simply rewrite to make it compatible. We don't want to add additional meta programming conditions as those logics are kinda hacks to get around some special syntax, also in many cases we could not know the actual value of `self.training` until runtime, so meta programming it might not be a generic solution. "
   },
   {
      "null": 44,
      "x": "[QUESTION] Should _dummy_type use name instead of storage_name?",
      "z": "Are you sure the 50% failure rate has anything to do with your change? That's a Caffe2 test and it shouldn't be exercising `torch/cuda/__init__.py` at all",
      "y": "Are you sure the 50% failure rate has anything to do with your change? That's a Caffe2 test and it shouldn't be exercising `torch/cuda/__init__.py` at all"
   },
   {
      "null": 45,
      "x": "Non-monotonic execution times for increasing kernel sizes",
      "z": "Maybe cudnn is probably selecting different algorithms.\nCould you try setting `torch.backends.cudnn.benchmark = True` in the beginning of the script and compare the values again?",
      "y": "Maybe cudnn is probably selecting different algorithms.\nCould you try setting `torch.backends.cudnn.benchmark = True` in the beginning of the script and compare the values again?"
   },
   {
      "null": 46,
      "x": "How to load a model trained on GPU 0 (cuda: 0) to GPU 1 (cuda:1) for inference?",
      "z": "In all likelihood your `device` value is nonsense. Print it out and see what the problem is.",
      "y": "In all likelihood your `device` value is nonsense. Print it out and see what the problem is."
   },
   {
      "null": 47,
      "x": "Feature request : Profiler",
      "z": "MXnet\u2019s profiler is almost certainly based on (or at least inspired by) the PyTorch profiler https://pytorch.org/docs/stable/autograd.html#torch.autograd.profiler.profile",
      "y": "MXnet\u2019s profiler is almost certainly based on (or at least inspired by) the PyTorch profiler https://pytorch.org/docs/stable/autograd.html#torch.autograd.profiler.profile"
   },
   {
      "null": 48,
      "x": "LibTorch Windows binaries appear to not be built with CuDNN",
      "z": "Okay, I know the reason. The PATHs of cuDNN is not passed through arguments like in setup.py when building libtorch.",
      "y": "Okay, I know the reason. The PATHs of cuDNN is not passed through arguments like in setup.py when building libtorch."
   },
   {
      "null": 49,
      "x": "RuntimeError: Expected a 'N2at13CUDAGeneratorE' but found 'PN2at9GeneratorE'",
      "z": "Note that the error: `RuntimeError: Expected a 'N2at13CUDAGeneratorE' but found 'PN2at9GeneratorE'` originated from this line. https://github.com/pytorch/pytorch/blob/master/aten/src/ATen/CheckGenerator.h#L15. Before my PR, the generated dispatch code utilized this function: https://github.com/pytorch/pytorch/blob/master/aten/src/ATen/function_wrapper.py#L327, and put it on all random distribution function dispatch code. For CUDA side,`THGenerator*` is set to null: https://github.com/pytorch/pytorch/blob/master/aten/src/ATen/function_wrapper.py#L1247. Hence, with the THGenerator* being null, and the `check_generator` function resolving to `check_generator<CUDAGenerator>(null, &globalContext().defaultGenerator(at::CUDA))`, we were getting that error. In my PR, `check_generator` (now called `check_generator_with_default`) is removed from `function_wrapper.py` and is currently only being called for the CPU functions and hence \"solves\" the bug. \n\nThe torch.manual_seed returning a CPU Generator is still there and I agree with @ezyang that it's weird. IMO, torch.manual_seed should return nothing, like how it's in torch.cuda.manual_seed. That is, [Py_RETURN_NONE](https://github.com/pytorch/pytorch/blob/53bb739b675a0dd2af2fcb2b02e37f64a432bd2f/torch/csrc/cuda/Module.cpp#L165) vs [(PyObject*)self](https://github.com/pytorch/pytorch/blob/53bb739b675a0dd2af2fcb2b02e37f64a432bd2f/torch/csrc/Generator.cpp#L104)",
      "y": "Note that the error: `RuntimeError: Expected a 'N2at13CUDAGeneratorE' but found 'PN2at9GeneratorE'` originated from this line. https://github.com/pytorch/pytorch/blob/master/aten/src/ATen/CheckGenerator.h#L15. Before my PR, the generated dispatch code utilized this function: https://github.com/pytorch/pytorch/blob/master/aten/src/ATen/function_wrapper.py#L327, and put it on all random distribution function dispatch code. For CUDA side,`THGenerator*` is set to null: https://github.com/pytorch/pytorch/blob/master/aten/src/ATen/function_wrapper.py#L1247. Hence, with the THGenerator* being null, and the `check_generator` function resolving to `check_generator<CUDAGenerator>(null, &globalContext().defaultGenerator(at::CUDA))`, we were getting that error. In my PR, `check_generator` (now called `check_generator_with_default`) is removed from `function_wrapper.py` and is currently only being called for the CPU functions and hence \"solves\" the bug. \n\nThe torch.manual_seed returning a CPU Generator is still there and I agree with @ezyang that it's weird. IMO, torch.manual_seed should return nothing, like how it's in torch.cuda.manual_seed. That is, [Py_RETURN_NONE](https://github.com/pytorch/pytorch/blob/53bb739b675a0dd2af2fcb2b02e37f64a432bd2f/torch/csrc/cuda/Module.cpp#L165) vs [(PyObject*)self](https://github.com/pytorch/pytorch/blob/53bb739b675a0dd2af2fcb2b02e37f64a432bd2f/torch/csrc/Generator.cpp#L104)"
   },
   {
      "null": 50,
      "x": ".size() vs .shape, which one should be used?",
      "z": ".size() method returns total elements in a dataframe , for eg shape of a tensor might be (10,3) , here total elements in tensor would be returned by .size() = 10X3 = 30 elements!!",
      "y": ".size() method returns total elements in a dataframe , for eg shape of a tensor might be (10,3) , here total elements in tensor would be returned by .size() = 10X3 = 30 elements!!"
   },
   {
      "null": 51,
      "x": "[feature request] ATen Documentation and Tutorial",
      "z": "See https://pytorch.org/cppdocs/\nSee https://pytorch.org/cppdocs/notes/tensor_creation.html",
      "y": "See https://pytorch.org/cppdocs/\nSee https://pytorch.org/cppdocs/notes/tensor_creation.html"
   },
   {
      "null": 52,
      "x": "RuntimeError: dimension specified as 0 but tensor has no dimensions",
      "z": "Did you try `z = z.unsqueeze(0)`? \n\nThe reference is [here](https://github.com/hunkim/PyTorchZeroToAll/issues/24).\n",
      "y": "Did you try `z = z.unsqueeze(0)`? \n\nThe reference is [here](https://github.com/hunkim/PyTorchZeroToAll/issues/24).\n"
   },
   {
      "null": 53,
      "x": "NCCL Error 1 when using torch.nn.DataParallel",
      "z": "I can help -  @aleksod \nLooking at their official build guide: https://github.com/NVIDIA/nccl\n\nyou need to clone a different repo for the tests:\n`git clone https://github.com/NVIDIA/nccl-tests.git`\nThen, in order to build the tests, enter that repo and run:\nCUDA_HOME=[path to your cuda main install dir] NCCL_HOME=[path to your nccl build dir] make\n\nfor example\nCUDA_HOME=/usr/local/cuda-10.0 NCCL_HOME=/path/to/nccl/build make\n\nthen, an example test from their official doc is:\n`./build/all_reduce_perf -b 8 -e 256M -f 2 -g <ngpus>`\n\nif you get missing .so libraries errors, you can use LD_LIBRARY_PATH to run the test. for example:\n\n`LD_LIBRARY_PATH=/usr/local/cuda-10.0/lib64:/some/path/to/nccl/nccl/build/lib:$LD_LIBRARY_PATH ./build/all_reduce_perf -b 8 -e 256M -f 2 -g 2`\n\n\n\n\n",
      "y": "I can help -  @aleksod \nLooking at their official build guide: https://github.com/NVIDIA/nccl\n\nyou need to clone a different repo for the tests:\n`git clone https://github.com/NVIDIA/nccl-tests.git`\nThen, in order to build the tests, enter that repo and run:\nCUDA_HOME=[path to your cuda main install dir] NCCL_HOME=[path to your nccl build dir] make\n\nfor example\nCUDA_HOME=/usr/local/cuda-10.0 NCCL_HOME=/path/to/nccl/build make\n\nthen, an example test from their official doc is:\n`./build/all_reduce_perf -b 8 -e 256M -f 2 -g <ngpus>`\n\nif you get missing .so libraries errors, you can use LD_LIBRARY_PATH to run the test. for example:\n\n`LD_LIBRARY_PATH=/usr/local/cuda-10.0/lib64:/some/path/to/nccl/nccl/build/lib:$LD_LIBRARY_PATH ./build/all_reduce_perf -b 8 -e 256M -f 2 -g 2`\n\n\n\n\n"
   },
   {
      "null": 54,
      "x": "dimension out of range (expected to be in range of [-1, 0], but got 1)",
      "z": "ok, i figured it out. So the key point which i didn't understand from the documentation was that the TARGET should be just one entry saying which class it belongs to Eg:[2] instead of a one hot vector like [0,0,1]. I mean, frankly, I  imagined the input and target being of similar shape (which is more intuitive). i.e Now that the input is a vector of [0,0,1] i imagined the TARGET also should be in same shape. Anyway, am glad it worked out. I would love to have a better worded documentation though, imho. \n\n Below is the code/shapes without dimension error.\n\n```\nlen(pred_y):\ntorch.Size([1, 3])\ntensor([[ 0.0000,  0.0000,  0.1527]])\nlen(x):\ntorch.Size([1])\ntensor([ 2])\nloss_training = loss_fn(pred_y, x)\n\n\n```",
      "y": "ok, i figured it out. So the key point which i didn't understand from the documentation was that the TARGET should be just one entry saying which class it belongs to Eg:[2] instead of a one hot vector like [0,0,1]. I mean, frankly, I  imagined the input and target being of similar shape (which is more intuitive). i.e Now that the input is a vector of [0,0,1] i imagined the TARGET also should be in same shape. Anyway, am glad it worked out. I would love to have a better worded documentation though, imho. \n\n Below is the code/shapes without dimension error.\n\n```\nlen(pred_y):\ntorch.Size([1, 3])\ntensor([[ 0.0000,  0.0000,  0.1527]])\nlen(x):\ntorch.Size([1])\ntensor([ 2])\nloss_training = loss_fn(pred_y, x)\n\n\n```"
   },
   {
      "null": 55,
      "x": "Incorrect error message for advanced indexing cuda tensor",
      "z": "This has been fixed in #5583. Now one can index tensors with both CPU and CUDA tensors so this particular error message doesn't apply anymore :)",
      "y": "This has been fixed in #5583. Now one can index tensors with both CPU and CUDA tensors so this particular error message doesn't apply anymore :)"
   },
   {
      "null": 56,
      "x": "\"Reduce Failed to Synchronise\" in F.binary_cross_entropy ",
      "z": "> See also: #2209\n> \n> BCELoss accepts only inputs that have all elements in range [0; 1] but this condition doesn't hold in your case\n\nI got [the same error](https://discuss.pytorch.org/t/cuda-out-of-memory-when-optimizer-step/55942?u=shirui-japina)\n\nand I tried to use `nn.BCELoss()` like:\n\n```\noptimizer = optim.SGD(model.parameters(), lr=0.0001)\ncriterion = nn.BCELoss()\n```\n\n_loop epoch train part:_\n\n```\nprediction = model(batch_input)\nloss = criterion(torch.sigmoid(prediction), label)\n\noptimizer.zero_grad()\nloss.backward()\noptimizer.step()\n```\n\nThen I solved the problem. Thank you for your comment!\n(But I don't know why optim.Adam() can't work well. It still errors: CUDA out of memory.)\n",
      "y": "> See also: #2209\n> \n> BCELoss accepts only inputs that have all elements in range [0; 1] but this condition doesn't hold in your case\n\nI got [the same error](https://discuss.pytorch.org/t/cuda-out-of-memory-when-optimizer-step/55942?u=shirui-japina)\n\nand I tried to use `nn.BCELoss()` like:\n\n```\noptimizer = optim.SGD(model.parameters(), lr=0.0001)\ncriterion = nn.BCELoss()\n```\n\n_loop epoch train part:_\n\n```\nprediction = model(batch_input)\nloss = criterion(torch.sigmoid(prediction), label)\n\noptimizer.zero_grad()\nloss.backward()\noptimizer.step()\n```\n\nThen I solved the problem. Thank you for your comment!\n(But I don't know why optim.Adam() can't work well. It still errors: CUDA out of memory.)\n"
   },
   {
      "null": 57,
      "x": "Device-side Assert in `THCReduceAll.cuh:339`",
      "z": "You have out-of-bound indices when you create the one-hot vector. The error will be raised on the responsible line if you use `CUDA_LAUNCH_BLOCKING=1`",
      "y": "You have out-of-bound indices when you create the one-hot vector. The error will be raised on the responsible line if you use `CUDA_LAUNCH_BLOCKING=1`"
   },
   {
      "null": 58,
      "x": "Import Error : no module  named torch",
      "z": "Add the path by: export PATH=~/anaconda3/bin:$PATH  \nbefore opening the python.",
      "y": "Add the path by: export PATH=~/anaconda3/bin:$PATH  \nbefore opening the python."
   },
   {
      "null": 59,
      "x": "torch.autograd.Function memory leak",
      "z": "Yes. The leaks happen only if you hold on to outputs via a mechanism different than `save_for_backward`",
      "y": "Yes. The leaks happen only if you hold on to outputs via a mechanism different than `save_for_backward`"
   },
   {
      "null": 60,
      "x": "Missing torch.* docs",
      "z": "#5443 addresses all but `torch.default_generator`. :)",
      "y": "#5443 addresses all but `torch.default_generator`. :)"
   },
   {
      "null": 61,
      "x": "Add Scale Factor To SGD",
      "z": "1. because it's so much more common than rescaling gradients by a constant that doesn't depend on the iteration\n2. it needs to be performed on weights and not gradients, and this requires an extra `no_grad` block\n3. ideally it wouldn't be part of the optimizer, but we're stuck with it for now because of backward compatibility",
      "y": "1. because it's so much more common than rescaling gradients by a constant that doesn't depend on the iteration\n2. it needs to be performed on weights and not gradients, and this requires an extra `no_grad` block\n3. ideally it wouldn't be part of the optimizer, but we're stuck with it for now because of backward compatibility"
   },
   {
      "null": 62,
      "x": "[bug] Expected GLOO_USE_IBVERBS to be defined",
      "z": "Looks like the reproduced failure was due to a staled gloo library.  After starting everything from fresh, everything works fine. So as I said, @alexholdenmiller, could you try building from scratch by git clone --recursive the latest PyTorch master. \n\nOr if you don't want to clone a new pytorch, at least, please make sure torch/lib/gloo folder's git status is clean.  Also in Pytorch folder, please do git submodule update to get gloo lib updated.\n\nIf this works, please close this issue.",
      "y": "Looks like the reproduced failure was due to a staled gloo library.  After starting everything from fresh, everything works fine. So as I said, @alexholdenmiller, could you try building from scratch by git clone --recursive the latest PyTorch master. \n\nOr if you don't want to clone a new pytorch, at least, please make sure torch/lib/gloo folder's git status is clean.  Also in Pytorch folder, please do git submodule update to get gloo lib updated.\n\nIf this works, please close this issue."
   },
   {
      "null": 63,
      "x": "[bug]  assert len(modules) == len(inputs) when use torch.distributed to compute last batch",
      "z": "As a quick fix you can pass `drop_last=True` when creating the data loader, but it's a bug that we'll need to fix",
      "y": "As a quick fix you can pass `drop_last=True` when creating the data loader, but it's a bug that we'll need to fix"
   },
   {
      "null": 64,
      "x": "[bug?] Problem with load_state_dict after installing latest pytorch by source",
      "z": "The `running_*` are disabled by default for InstanceNorm* layers after #4922 . You can add `track_running_stats=True` to the InstanceNorm* layer constructors.",
      "y": "The `running_*` are disabled by default for InstanceNorm* layers after #4922 . You can add `track_running_stats=True` to the InstanceNorm* layer constructors."
   },
   {
      "null": 65,
      "x": "when pytorch should add Windows support.",
      "z": "Starting from the next release. Should be out within weeks",
      "y": "Starting from the next release. Should be out within weeks"
   },
   {
      "null": 66,
      "x": "In IPython torch.dot always returns 0",
      "z": "please do:\n\n```\npip uninstall -y numpy\nconda install -y numpy \n```\n\nThe pip version of numpy is linked against OpenBLAS, which has a `dot` that conflicts in ABI with MKL's `dot`",
      "y": "please do:\n\n```\npip uninstall -y numpy\nconda install -y numpy \n```\n\nThe pip version of numpy is linked against OpenBLAS, which has a `dot` that conflicts in ABI with MKL's `dot`"
   },
   {
      "null": 67,
      "x": "Feature request: named dimensions",
      "z": "See `xarray` for prior work https://github.com/pydata/xarray",
      "y": "See `xarray` for prior work https://github.com/pydata/xarray"
   },
   {
      "null": 68,
      "x": "[feature request] torch.argmax / torch.argmin",
      "z": "@thecortex note that you can currently obtain the argmax as the second return value of `torch.max` when a dimension is specified. Same for argmin",
      "y": "@thecortex note that you can currently obtain the argmax as the second return value of `torch.max` when a dimension is specified. Same for argmin"
   },
   {
      "null": 69,
      "x": "Pytorch 0.4.0 documentation typo for batchnorm's momentum parameter",
      "z": "closed via https://github.com/pytorch/pytorch/pull/5450/",
      "y": "closed via https://github.com/pytorch/pytorch/pull/5450/"
   },
   {
      "null": 70,
      "x": "Can not restart the training to obtain the same results",
      "z": "Many things can matter, e.g. random number generator, cudnn non-derministic algorithms, dataloader worker task scheduling, optimizer state, etc.",
      "y": "Many things can matter, e.g. random number generator, cudnn non-derministic algorithms, dataloader worker task scheduling, optimizer state, etc."
   },
   {
      "null": 71,
      "x": "[feature request] Add C++ test framework for pure C++ autograd+jit tests",
      "z": "The Catch2 devs recommend to just vendor Catch2 in repositories\nhttps://github.com/catchorg/Catch2/blob/master/docs/build-systems.md#cmake\n\nThe alternative, also mentioned on that page, is to use CMake's ability to git clone dependencies\u00a0using the ExternalProject machinery, but I honestly wouldn't go there.",
      "y": "The Catch2 devs recommend to just vendor Catch2 in repositories\nhttps://github.com/catchorg/Catch2/blob/master/docs/build-systems.md#cmake\n\nThe alternative, also mentioned on that page, is to use CMake's ability to git clone dependencies\u00a0using the ExternalProject machinery, but I honestly wouldn't go there."
   },
   {
      "null": 72,
      "x": "Handle None gradients in nn.utils.clip_grad_norm",
      "z": "What this says is `p.grad` is None. It's possible that `p` (whatever it is) wasn't used in the gradient computation, or there was no backwards pass applied.",
      "y": "What this says is `p.grad` is None. It's possible that `p` (whatever it is) wasn't used in the gradient computation, or there was no backwards pass applied."
   },
   {
      "null": 73,
      "x": "Variables are behaving strangely for indexing",
      "z": "One problem is the slice parsing code didn't properly handle the error return. That caused the weird error messages.\n\nThe other problem is that `Variable` doesn't implement `__index__` so you can't use Variables as slice indices.",
      "y": "One problem is the slice parsing code didn't properly handle the error return. That caused the weird error messages.\n\nThe other problem is that `Variable` doesn't implement `__index__` so you can't use Variables as slice indices."
   },
   {
      "null": 74,
      "x": "Softmax using multiple threads inhibiting parallel execution of forward passes (?)",
      "z": "Using `torch.set_num_threads(1)` solves the problem.\n\nI figure the problem was a deadlock arising as a result of creating a huge number of threads:\n\nI create a number of threads where each thread should execute a series of forward passes. However, when the thread encounters the softmax, it spawns a number of child threads. As every thread did this, a huge number of threads was being spawned in total, which caused a deadlock. \n\nIt is funny though that this occurs only on Linux and not Mac OS.",
      "y": "Using `torch.set_num_threads(1)` solves the problem.\n\nI figure the problem was a deadlock arising as a result of creating a huge number of threads:\n\nI create a number of threads where each thread should execute a series of forward passes. However, when the thread encounters the softmax, it spawns a number of child threads. As every thread did this, a huge number of threads was being spawned in total, which caused a deadlock. \n\nIt is funny though that this occurs only on Linux and not Mac OS."
   },
   {
      "null": 75,
      "x": "0.3.0 availability on conda",
      "z": "conda install pytorch=0.3.0 torchvision -c pytorch\n\nI test this command is right to install pytorch3.0.0 in MacOS!",
      "y": "conda install pytorch=0.3.0 torchvision -c pytorch\n\nI test this command is right to install pytorch3.0.0 in MacOS!"
   },
   {
      "null": 76,
      "x": "Only nn.Parameters defined directly within nn.Module are listed in module.parameters()",
      "z": "There are good reason why we don't want to traverse arbitrary Python data structures, with performance being the most important consideration. That's why we have `nn.ParameterList` and `nn.ModuleList`. `nn.Module()` can also be used as a dictionary-like object for both parameters and submodules.",
      "y": "There are good reason why we don't want to traverse arbitrary Python data structures, with performance being the most important consideration. That's why we have `nn.ParameterList` and `nn.ModuleList`. `nn.Module()` can also be used as a dictionary-like object for both parameters and submodules."
   },
   {
      "null": 77,
      "x": "Issues about symeig and svd on GPU",
      "z": "Do you mean svd? We use magma bindings to many linalg functions. Magma doesn't implement everything in GPU so it sometimes still makes some lapack calls.",
      "y": "Do you mean svd? We use magma bindings to many linalg functions. Magma doesn't implement everything in GPU so it sometimes still makes some lapack calls."
   },
   {
      "null": 78,
      "x": "ModuleNotFoundError: No module named 'torch.version'",
      "z": "It looks like you are. Change the directory and you should be good.",
      "y": "It looks like you are. Change the directory and you should be good."
   },
   {
      "null": 79,
      "x": "the derivative for 'svd' is not implemented",
      "z": "the derivative for torch.svd is implemented in the master branch. It hasn't yet been incorporated into a release yet. If you would like to use this feature immediately, you can install pytorch from source: https://github.com/pytorch/pytorch#from-source",
      "y": "the derivative for torch.svd is implemented in the master branch. It hasn't yet been incorporated into a release yet. If you would like to use this feature immediately, you can install pytorch from source: https://github.com/pytorch/pytorch#from-source"
   },
   {
      "null": 80,
      "x": "backward(create_graph=True) should raise a warning for potential memory leak",
      "z": "My 2 cents. I've done higher-order differentiation many times. I've always used `autograd.grad` instead of `.backward` or `autograd.backward` because \n1. `autograd.grad` doesn't accumulate gradient with previous values, which can lead to obscure errors.\n2. it is almost always clearer in code than accessing `.grad` of leaves.\n3. it avoids ref cycle.\n\nIn fact, most DL uses of higher-order gradients I've seen use `autograd.grad`. So could we add a warning (either in doc or in code) to `backward(create_graph)` and advocate `autograd.grad` instead?",
      "y": "My 2 cents. I've done higher-order differentiation many times. I've always used `autograd.grad` instead of `.backward` or `autograd.backward` because \n1. `autograd.grad` doesn't accumulate gradient with previous values, which can lead to obscure errors.\n2. it is almost always clearer in code than accessing `.grad` of leaves.\n3. it avoids ref cycle.\n\nIn fact, most DL uses of higher-order gradients I've seen use `autograd.grad`. So could we add a warning (either in doc or in code) to `backward(create_graph)` and advocate `autograd.grad` instead?"
   },
   {
      "null": 81,
      "x": "Bug in inplace operation after expansion",
      "z": "Yes, and that's the expected behavior. Don't do in-place on expanded tensors.",
      "y": "Yes, and that's the expected behavior. Don't do in-place on expanded tensors."
   },
   {
      "null": 82,
      "x": "[Feature request] Gradient of cholesky_inverse, cholesky_solve",
      "z": "I use the following workarounds for lack of batching in `cholesky_inverse` and lack of gradients in `cholesky_solve`. I'm guessing it should be easy to move this logic into PyTorch.\n```py\ndef cholesky_solve(b, u):\n    \"Like :func:`torch.cholesky_solve` but supports gradients.\"\n    if not b.requires_grad and not u.requires_grad:\n        return b.cholesky_solve(u)\n    x = b.triangular_solve(u, upper=False).solution\n    return x.triangular_solve(u, upper=False, transpose=True).solution\n\ndef cholesky_inverse(u):\n    \"Like :func:`torch.cholesky_inverse` but supports batching and gradients.\"\n    if u.dim() == 2 and not u.requires_grad:\n        return u.cholesky_inverse()\n    return cholesky_solve(torch.eye(u.size(-1)).expand(u.size()), u)\n```",
      "y": "I use the following workarounds for lack of batching in `cholesky_inverse` and lack of gradients in `cholesky_solve`. I'm guessing it should be easy to move this logic into PyTorch.\n```py\ndef cholesky_solve(b, u):\n    \"Like :func:`torch.cholesky_solve` but supports gradients.\"\n    if not b.requires_grad and not u.requires_grad:\n        return b.cholesky_solve(u)\n    x = b.triangular_solve(u, upper=False).solution\n    return x.triangular_solve(u, upper=False, transpose=True).solution\n\ndef cholesky_inverse(u):\n    \"Like :func:`torch.cholesky_inverse` but supports batching and gradients.\"\n    if u.dim() == 2 and not u.requires_grad:\n        return u.cholesky_inverse()\n    return cholesky_solve(torch.eye(u.size(-1)).expand(u.size()), u)\n```"
   },
   {
      "null": 83,
      "x": "No module named torchvision",
      "z": "If you followed the instructions here: http://pytorch.org/ and install with conda it will install both torch and torchvision together.",
      "y": "If you followed the instructions here: http://pytorch.org/ and install with conda it will install both torch and torchvision together."
   },
   {
      "null": 84,
      "x": "Failed to load model",
      "z": "@apaszke I solved it by using the recommended way instead, it's my fault : )\n```python\ntorch.save(the_model.state_dict(), PATH)\n\nthe_model = TheModelClass(*args, **kwargs)\nthe_model.load_state_dict(torch.load(PATH))\n```",
      "y": "@apaszke I solved it by using the recommended way instead, it's my fault : )\n```python\ntorch.save(the_model.state_dict(), PATH)\n\nthe_model = TheModelClass(*args, **kwargs)\nthe_model.load_state_dict(torch.load(PATH))\n```"
   },
   {
      "null": 85,
      "x": "Serialization of tensors with pickle.dumps seems to be inconsistent, leading to inconsistent redis cache hit/miss",
      "z": "Can you provide a minimum example that reproduces this behavior and that indicate that this is a PyTorch problem?\n\nFrom your description, it might imply that `pickle.dumps(tensor)` is not always the same. But I couldn't reproduce this locally.\n\nIt could also be that the algorithm that Redis uses to cache large objects is not perfect and is subject to missing it a few times. Hard to say without a repro.",
      "y": "Can you provide a minimum example that reproduces this behavior and that indicate that this is a PyTorch problem?\n\nFrom your description, it might imply that `pickle.dumps(tensor)` is not always the same. But I couldn't reproduce this locally.\n\nIt could also be that the algorithm that Redis uses to cache large objects is not perfect and is subject to missing it a few times. Hard to say without a repro."
   },
   {
      "null": 86,
      "x": "Grad is None after using view",
      "z": "So this isn't a bug per se, but it is definitely a source of confusion. The issue with the above code is that the gradient information is attached to the initial tensor before the `view`, but not the viewed tensor. Performing the initialization and view operation before assigning the tensor to the variable results in losing the access to the gradient information. Splitting out the view works fine. It would be useful to call this out in the docs (maybe I missed this).\n```\nX0 = torch.tensor([0.25, 0.75], requires_grad=True,)\nX_view = X0.view(2, 1, 1)\nprint(f\"X_view.shape: {X_view.shape}\")\nX_view.sum().backward()\nprint(f\"X_view.grad: {X_view.grad}\")\nprint(f\"X_view.grad is None: {X_view.grad is None}\")\nprint(f\"X0.grad: {X0.grad}\")\n```\nOutput:\n```\nX_view.shape: torch.Size([2, 1, 1])\nX_view.grad: None\nX_view.grad is None: True\nX0.grad: tensor([1., 1.])\n```\n",
      "y": "So this isn't a bug per se, but it is definitely a source of confusion. The issue with the above code is that the gradient information is attached to the initial tensor before the `view`, but not the viewed tensor. Performing the initialization and view operation before assigning the tensor to the variable results in losing the access to the gradient information. Splitting out the view works fine. It would be useful to call this out in the docs (maybe I missed this).\n```\nX0 = torch.tensor([0.25, 0.75], requires_grad=True,)\nX_view = X0.view(2, 1, 1)\nprint(f\"X_view.shape: {X_view.shape}\")\nX_view.sum().backward()\nprint(f\"X_view.grad: {X_view.grad}\")\nprint(f\"X_view.grad is None: {X_view.grad is None}\")\nprint(f\"X0.grad: {X0.grad}\")\n```\nOutput:\n```\nX_view.shape: torch.Size([2, 1, 1])\nX_view.grad: None\nX_view.grad is None: True\nX0.grad: tensor([1., 1.])\n```\n"
   },
   {
      "null": 87,
      "x": "Could Pytorch C++ API load tensor from gpu memory directly?",
      "z": "Example code that converts a cv::cuda::GpuMat of type 32F to a torch::Tensor:\n\n```C++\n    void deleter(void *arg){};\n    torch::Tensor matToTensor(const cv::cuda::GpuMat &image)\n    {\n        std::vector<int64_t> dims = {image.rows, image.cols, image.channels()};\n        long long step = image.step / sizeof(float);\n        std::vector<int64_t> strides = {step, image.channels(), 1};\n        return torch::from_blob(image.data, dims, strides, deleter, torch::kCUDA)\n    }\n```\n\nIf you want to send in bytes, you would update step size and add torch::kByte/kChar to options.\n\nOnly tricky part for your GL image may be step size otherwise the code should look almost identical.",
      "y": "Example code that converts a cv::cuda::GpuMat of type 32F to a torch::Tensor:\n\n```C++\n    void deleter(void *arg){};\n    torch::Tensor matToTensor(const cv::cuda::GpuMat &image)\n    {\n        std::vector<int64_t> dims = {image.rows, image.cols, image.channels()};\n        long long step = image.step / sizeof(float);\n        std::vector<int64_t> strides = {step, image.channels(), 1};\n        return torch::from_blob(image.data, dims, strides, deleter, torch::kCUDA)\n    }\n```\n\nIf you want to send in bytes, you would update step size and add torch::kByte/kChar to options.\n\nOnly tricky part for your GL image may be step size otherwise the code should look almost identical."
   },
   {
      "null": 88,
      "x": "[Caffe2] Compile error: onnxTensorDescriptorV1 has no member named quantizationParams",
      "z": "Should be fixed by https://github.com/pytorch/pytorch/pull/19793",
      "y": "Should be fixed by https://github.com/pytorch/pytorch/pull/19793"
   },
   {
      "null": 89,
      "x": "AverageUnpooling layer for PyTorch (Proposal)",
      "z": "Isn't `AverageUnpooling` very similar to `F.interpolate`?",
      "y": "Isn't `AverageUnpooling` very similar to `F.interpolate`?"
   },
   {
      "null": 90,
      "x": "Why nn.Sequential can't handle multiple input?",
      "z": "@soumith Hi, I changed `nn.Sequential` to this\n```python\nclass mySequential(nn.Sequential):\n    def forward(self, *input):\n        for module in self._modules.values():\n            input = module(*input)\n        return input\n```\nAnd it could handle multiple inputs/outputs only need the number of outputs from the previous layer equals the number of inputs from the next layer.\n```python\nclass n_to_n(nn.Module):\n    def __init__(self):\n        super().__init__()\n        self.conv1 = nn.Conv2d(3, 3, 1, 1, bias=False)\n        self.conv2 = nn.Conv2d(3, 3, 1, 1, bias=False)\n\n    def forward(self, x1, x2):\n        y1 = self.conv1(x1)\n        y2 = self.conv2(x2)\n        return y1, y2\n\n\nclass n_to_one(nn.Module):\n    def __init__(self):\n        super().__init__()\n        self.conv1 = nn.Conv2d(3, 3, 1, 1, bias=False)\n        self.conv2 = nn.Conv2d(3, 3, 1, 1, bias=False)\n\n    def forward(self, x1, x2):\n        y1 = self.conv1(x1)\n        y2 = self.conv2(x2)\n        return y1 + y2\n\n\nclass one_to_n(nn.Module):\n    def __init__(self):\n        super().__init__()\n        self.conv1 = nn.Conv2d(3, 3, 1, 1, bias=False)\n        self.conv2 = nn.Conv2d(3, 3, 1, 1, bias=False)\n\n    def forward(self, x):\n        y1 = self.conv1(x)\n        y2 = self.conv2(x)\n        return y1, y2\n\nseq = mySequential(one_to_n(), n_to_n(), n_to_one()).cuda()\ntd = torch.rand(1, 3, 32, 32).cuda()\n\nout = seq(td)\nprint(out.size())\n```\n```shell\ntorch.Size([1, 3, 32, 32])\n```\nWhat do you think?",
      "y": "@soumith Hi, I changed `nn.Sequential` to this\n```python\nclass mySequential(nn.Sequential):\n    def forward(self, *input):\n        for module in self._modules.values():\n            input = module(*input)\n        return input\n```\nAnd it could handle multiple inputs/outputs only need the number of outputs from the previous layer equals the number of inputs from the next layer.\n```python\nclass n_to_n(nn.Module):\n    def __init__(self):\n        super().__init__()\n        self.conv1 = nn.Conv2d(3, 3, 1, 1, bias=False)\n        self.conv2 = nn.Conv2d(3, 3, 1, 1, bias=False)\n\n    def forward(self, x1, x2):\n        y1 = self.conv1(x1)\n        y2 = self.conv2(x2)\n        return y1, y2\n\n\nclass n_to_one(nn.Module):\n    def __init__(self):\n        super().__init__()\n        self.conv1 = nn.Conv2d(3, 3, 1, 1, bias=False)\n        self.conv2 = nn.Conv2d(3, 3, 1, 1, bias=False)\n\n    def forward(self, x1, x2):\n        y1 = self.conv1(x1)\n        y2 = self.conv2(x2)\n        return y1 + y2\n\n\nclass one_to_n(nn.Module):\n    def __init__(self):\n        super().__init__()\n        self.conv1 = nn.Conv2d(3, 3, 1, 1, bias=False)\n        self.conv2 = nn.Conv2d(3, 3, 1, 1, bias=False)\n\n    def forward(self, x):\n        y1 = self.conv1(x)\n        y2 = self.conv2(x)\n        return y1, y2\n\nseq = mySequential(one_to_n(), n_to_n(), n_to_one()).cuda()\ntd = torch.rand(1, 3, 32, 32).cuda()\n\nout = seq(td)\nprint(out.size())\n```\n```shell\ntorch.Size([1, 3, 32, 32])\n```\nWhat do you think?"
   },
   {
      "null": 91,
      "x": "CTCLoss reduction=\u2019none\u2018 and calculate average in hand is not equal to 'mean'",
      "z": "@carry-xz this is expected, see the implementation of `CTCLoss` in https://github.com/pytorch/pytorch/blob/39b885cbbfc8c115069d49f5a6d27ea622bd05dc/aten/src/ATen/native/LossCTC.cpp#L366-L371\n\nFrom [the documentation](https://pytorch.org/docs/stable/nn.html#torch.nn.CTCLoss)\n> \u2018mean\u2019: the output losses will be divided by the target lengths and then the mean over the batch is taken",
      "y": "@carry-xz this is expected, see the implementation of `CTCLoss` in https://github.com/pytorch/pytorch/blob/39b885cbbfc8c115069d49f5a6d27ea622bd05dc/aten/src/ATen/native/LossCTC.cpp#L366-L371\n\nFrom [the documentation](https://pytorch.org/docs/stable/nn.html#torch.nn.CTCLoss)\n> \u2018mean\u2019: the output losses will be divided by the target lengths and then the mean over the batch is taken"
   },
   {
      "null": 92,
      "x": "Using DistributedDataParallel through NCCL throws RuntimeError",
      "z": "@Ze-Yang Pytorch DDP has a serious flaw which is not documented: the computational graphs for all the nodes must be the same or you will get this error or training sometimes just hangs at 100% CPU (even when training on GPU) on backward() call. You can debug your problem by visualising the computational graphs on each node and see if they are the same. If not, make them (or fix the pytorch source).\n\nAn example: if your loss function contains an IF condition with additional computation, and one node goes into IF and other does not, this will be enough to crash DDP training. Instead of IF try to solve the same thing some other way and/or trick pytorch to make the graph look the same on each node. Of course, there might be other causes for different autograd graph too.\n\nMy case was this. Buggy code (crashed):\n```\nif positive_class.shape[0] > 0:\n   loss += torch.somefunction(positive_class)\n```\nFix:\n```\n# concat with tensor that has no effect on loss calculation, but positive_class always has content\npositive_class = torch.cat(positive_class, dummy)\nloss += torch.somefunction(positive_class)\n```\n\nI don't know if your problem is similar, I do not see any obvious divergence of graphs here, but it's worth to look at graphs if you are stuck :)",
      "y": "@Ze-Yang Pytorch DDP has a serious flaw which is not documented: the computational graphs for all the nodes must be the same or you will get this error or training sometimes just hangs at 100% CPU (even when training on GPU) on backward() call. You can debug your problem by visualising the computational graphs on each node and see if they are the same. If not, make them (or fix the pytorch source).\n\nAn example: if your loss function contains an IF condition with additional computation, and one node goes into IF and other does not, this will be enough to crash DDP training. Instead of IF try to solve the same thing some other way and/or trick pytorch to make the graph look the same on each node. Of course, there might be other causes for different autograd graph too.\n\nMy case was this. Buggy code (crashed):\n```\nif positive_class.shape[0] > 0:\n   loss += torch.somefunction(positive_class)\n```\nFix:\n```\n# concat with tensor that has no effect on loss calculation, but positive_class always has content\npositive_class = torch.cat(positive_class, dummy)\nloss += torch.somefunction(positive_class)\n```\n\nI don't know if your problem is similar, I do not see any obvious divergence of graphs here, but it's worth to look at graphs if you are stuck :)"
   },
   {
      "null": 93,
      "x": "At training time, pytorch batchnorm use biased batch var to normalize input, but running var is updated by unbiased batch var",
      "z": "Finally,I find the problem.\nAt training time, pytorch batchnorm use biased batch var to normalize input, but running var is updated by unbiased batch var.So after model convergence and switch to eval model,the running var gives unbiased prediction, but this is inconsistency with train mode with biased prediction.\nChange the torch.var unbiased option to True or False for seeing the result.\n```\nimport torch\nimport torch.nn as nn\nfrom torch.nn import Parameter, init\n\nclass MyBatchNorm(nn.Module):\n    _version = 2\n    __constants__ = ['track_running_stats', 'momentum', 'eps', 'weight', 'bias',\n                     'running_mean', 'running_var', 'num_batches_tracked']\n\n    def __init__(self, num_features, eps=1e-5, momentum=0.1, affine=True,\n                 track_running_stats=True):\n        super(MyBatchNorm, self).__init__()\n        self.num_features = num_features\n        self.eps = eps\n        self.momentum = momentum\n        self.affine = affine\n        self.track_running_stats = track_running_stats\n        if self.affine:\n            self.weight = Parameter(torch.Tensor(num_features))\n            self.bias = Parameter(torch.Tensor(num_features))\n        else:\n            self.register_parameter('weight', None)\n            self.register_parameter('bias', None)\n        if self.track_running_stats:\n            self.register_buffer('running_mean', torch.zeros(num_features))\n            self.register_buffer('running_var', torch.ones(num_features))\n            self.register_buffer('num_batches_tracked', torch.tensor(0, dtype=torch.long))\n        else:\n            self.register_parameter('running_mean', None)\n            self.register_parameter('running_var', None)\n            self.register_parameter('num_batches_tracked', None)\n        self.reset_parameters()\n\n    def reset_running_stats(self):\n        if self.track_running_stats:\n            self.running_mean.zero_()\n            self.running_var.fill_(1)\n            self.num_batches_tracked.zero_()\n\n    def reset_parameters(self):\n        self.reset_running_stats()\n        if self.affine:\n            init.uniform_(self.weight)\n            init.zeros_(self.bias)\n        \n    def forward(self, input):        \n        input_size = input.size()\n        input = input.transpose(1,0)\n        input = input.view(input.size(0), -1)\n\n        if self.training:\n            mean = input.mean(dim=1)\n            var = torch.var(input,dim=1, unbiased=True)\n            self.running_mean[:] = (1. - self.momentum) * self.running_mean + self.momentum * mean\n            self.running_var[:] = (1. - self.momentum) * self.running_var + self.momentum * var\n        else:\n            mean = self.running_mean\n            var = self.running_var\n\n        input = input - mean.view(-1,1)\n        input = input / (torch.sqrt(var+self.eps).view(-1,1))\n       \n        input = self.weight.view(-1, 1) * input + self.bias.view(-1, 1)\n        input = input.transpose(1,0)\n        input = input.view(*input_size)\n        return input\n\n    def extra_repr(self):\n        return '{num_features}, eps={eps}, momentum={momentum}, affine={affine}, ' \\\n               'track_running_stats={track_running_stats}'.format(**self.__dict__)\n\n    def _load_from_state_dict(self, state_dict, prefix, local_metadata, strict,\n                              missing_keys, unexpected_keys, error_msgs):\n        version = local_metadata.get('version', None)\n\n        if (version is None or version < 2) and self.track_running_stats:\n            # at version 2: added num_batches_tracked buffer\n            #               this should have a default value of 0\n            num_batches_tracked_key = prefix + 'num_batches_tracked'\n            if num_batches_tracked_key not in state_dict:\n                state_dict[num_batches_tracked_key] = torch.tensor(0, dtype=torch.long)\n\n        super(MyBatchNorm, self)._load_from_state_dict(\n            state_dict, prefix, local_metadata, strict,\n            missing_keys, unexpected_keys, error_msgs)\n\ndef test_batch_norm():\n    momentum = 1.0\n    torch.manual_seed(1234)\n\n    batch_norm = MyBatchNorm(3,eps=1e-5,momentum=momentum,affine=True,track_running_stats=True)\n    n1 = batch_norm\n    torch.save(n1.state_dict(),'n1.pth')\n    \n    torch_batch_norm = nn.BatchNorm1d(3,eps=1e-5,momentum=momentum,affine=True,track_running_stats=True)\n    n2 = torch_batch_norm\n    n2.load_state_dict(torch.load('n1.pth'))\n    \n    x = torch.FloatTensor([[1,2,3], [3,4,0], [3,3,1]])\n    y = torch.FloatTensor([[2], [3], [1]])\n    criterion = nn.MSELoss()\n\n    x = x.cuda()\n    y = y.cuda()\n    batch_norm.cuda()\n    torch_batch_norm.cuda()\n\n    print('Switch to eval mode.')\n    batch_norm.eval()\n    torch_batch_norm.eval()\n    out1 = n1(x)\n    out2 = n2(x)\n    eval1 = (torch.abs(out2-out1).sum().item() < 1e-4)\n\n    print('Swtich to train mode.')\n    batch_norm.train()\n    torch_batch_norm.train()\n\n    out1 = n1(x)\n    out2 = n2(x)\n    train2 = (torch.abs(out2-out1).sum().item() < 1e-4)\n\n    print('Switch to eval mode.')\n    n1.eval()\n    n2.eval()\n    print('MyBatchNorm:')\n    print('running_mean:',batch_norm.running_mean.cpu().numpy())\n    print('running_var:',batch_norm.running_var.cpu().numpy())\n    print('weight:',batch_norm.weight.data.cpu().numpy())\n    print('bias:',batch_norm.bias.data.cpu().numpy())\n    print()\n    \n    print('TorchBatchNorm:')\n    print('running_mean:',torch_batch_norm.running_mean.cpu().numpy())\n    print('running_var:',torch_batch_norm.running_var.cpu().numpy())\n    print('weight:',torch_batch_norm.weight.data.cpu().numpy())\n    print('bias:',torch_batch_norm.bias.data.cpu().numpy())\n    out1 = n1(x)\n    out2 = n2(x)\n    eval3 = (torch.abs(out2-out1).sum().item() < 1e-4)\n    print('eval1,train2,eval3:',eval1,train2,eval3)\n    assert eval1 and train2 and eval3\n\nif __name__ == '__main__':\n    test_batch_norm()\n```",
      "y": "Finally,I find the problem.\nAt training time, pytorch batchnorm use biased batch var to normalize input, but running var is updated by unbiased batch var.So after model convergence and switch to eval model,the running var gives unbiased prediction, but this is inconsistency with train mode with biased prediction.\nChange the torch.var unbiased option to True or False for seeing the result.\n```\nimport torch\nimport torch.nn as nn\nfrom torch.nn import Parameter, init\n\nclass MyBatchNorm(nn.Module):\n    _version = 2\n    __constants__ = ['track_running_stats', 'momentum', 'eps', 'weight', 'bias',\n                     'running_mean', 'running_var', 'num_batches_tracked']\n\n    def __init__(self, num_features, eps=1e-5, momentum=0.1, affine=True,\n                 track_running_stats=True):\n        super(MyBatchNorm, self).__init__()\n        self.num_features = num_features\n        self.eps = eps\n        self.momentum = momentum\n        self.affine = affine\n        self.track_running_stats = track_running_stats\n        if self.affine:\n            self.weight = Parameter(torch.Tensor(num_features))\n            self.bias = Parameter(torch.Tensor(num_features))\n        else:\n            self.register_parameter('weight', None)\n            self.register_parameter('bias', None)\n        if self.track_running_stats:\n            self.register_buffer('running_mean', torch.zeros(num_features))\n            self.register_buffer('running_var', torch.ones(num_features))\n            self.register_buffer('num_batches_tracked', torch.tensor(0, dtype=torch.long))\n        else:\n            self.register_parameter('running_mean', None)\n            self.register_parameter('running_var', None)\n            self.register_parameter('num_batches_tracked', None)\n        self.reset_parameters()\n\n    def reset_running_stats(self):\n        if self.track_running_stats:\n            self.running_mean.zero_()\n            self.running_var.fill_(1)\n            self.num_batches_tracked.zero_()\n\n    def reset_parameters(self):\n        self.reset_running_stats()\n        if self.affine:\n            init.uniform_(self.weight)\n            init.zeros_(self.bias)\n        \n    def forward(self, input):        \n        input_size = input.size()\n        input = input.transpose(1,0)\n        input = input.view(input.size(0), -1)\n\n        if self.training:\n            mean = input.mean(dim=1)\n            var = torch.var(input,dim=1, unbiased=True)\n            self.running_mean[:] = (1. - self.momentum) * self.running_mean + self.momentum * mean\n            self.running_var[:] = (1. - self.momentum) * self.running_var + self.momentum * var\n        else:\n            mean = self.running_mean\n            var = self.running_var\n\n        input = input - mean.view(-1,1)\n        input = input / (torch.sqrt(var+self.eps).view(-1,1))\n       \n        input = self.weight.view(-1, 1) * input + self.bias.view(-1, 1)\n        input = input.transpose(1,0)\n        input = input.view(*input_size)\n        return input\n\n    def extra_repr(self):\n        return '{num_features}, eps={eps}, momentum={momentum}, affine={affine}, ' \\\n               'track_running_stats={track_running_stats}'.format(**self.__dict__)\n\n    def _load_from_state_dict(self, state_dict, prefix, local_metadata, strict,\n                              missing_keys, unexpected_keys, error_msgs):\n        version = local_metadata.get('version', None)\n\n        if (version is None or version < 2) and self.track_running_stats:\n            # at version 2: added num_batches_tracked buffer\n            #               this should have a default value of 0\n            num_batches_tracked_key = prefix + 'num_batches_tracked'\n            if num_batches_tracked_key not in state_dict:\n                state_dict[num_batches_tracked_key] = torch.tensor(0, dtype=torch.long)\n\n        super(MyBatchNorm, self)._load_from_state_dict(\n            state_dict, prefix, local_metadata, strict,\n            missing_keys, unexpected_keys, error_msgs)\n\ndef test_batch_norm():\n    momentum = 1.0\n    torch.manual_seed(1234)\n\n    batch_norm = MyBatchNorm(3,eps=1e-5,momentum=momentum,affine=True,track_running_stats=True)\n    n1 = batch_norm\n    torch.save(n1.state_dict(),'n1.pth')\n    \n    torch_batch_norm = nn.BatchNorm1d(3,eps=1e-5,momentum=momentum,affine=True,track_running_stats=True)\n    n2 = torch_batch_norm\n    n2.load_state_dict(torch.load('n1.pth'))\n    \n    x = torch.FloatTensor([[1,2,3], [3,4,0], [3,3,1]])\n    y = torch.FloatTensor([[2], [3], [1]])\n    criterion = nn.MSELoss()\n\n    x = x.cuda()\n    y = y.cuda()\n    batch_norm.cuda()\n    torch_batch_norm.cuda()\n\n    print('Switch to eval mode.')\n    batch_norm.eval()\n    torch_batch_norm.eval()\n    out1 = n1(x)\n    out2 = n2(x)\n    eval1 = (torch.abs(out2-out1).sum().item() < 1e-4)\n\n    print('Swtich to train mode.')\n    batch_norm.train()\n    torch_batch_norm.train()\n\n    out1 = n1(x)\n    out2 = n2(x)\n    train2 = (torch.abs(out2-out1).sum().item() < 1e-4)\n\n    print('Switch to eval mode.')\n    n1.eval()\n    n2.eval()\n    print('MyBatchNorm:')\n    print('running_mean:',batch_norm.running_mean.cpu().numpy())\n    print('running_var:',batch_norm.running_var.cpu().numpy())\n    print('weight:',batch_norm.weight.data.cpu().numpy())\n    print('bias:',batch_norm.bias.data.cpu().numpy())\n    print()\n    \n    print('TorchBatchNorm:')\n    print('running_mean:',torch_batch_norm.running_mean.cpu().numpy())\n    print('running_var:',torch_batch_norm.running_var.cpu().numpy())\n    print('weight:',torch_batch_norm.weight.data.cpu().numpy())\n    print('bias:',torch_batch_norm.bias.data.cpu().numpy())\n    out1 = n1(x)\n    out2 = n2(x)\n    eval3 = (torch.abs(out2-out1).sum().item() < 1e-4)\n    print('eval1,train2,eval3:',eval1,train2,eval3)\n    assert eval1 and train2 and eval3\n\nif __name__ == '__main__':\n    test_batch_norm()\n```"
   },
   {
      "null": 94,
      "x": "Is CudnnRNN thread-safe?",
      "z": "can you reproduce this on 1.1?  A number of thread-safe fixes have gone in since 0.4.1.  I'm going to close for now, please reopen if you can reproduce this on 1.1.",
      "y": "can you reproduce this on 1.1?  A number of thread-safe fixes have gone in since 0.4.1.  I'm going to close for now, please reopen if you can reproduce this on 1.1."
   },
   {
      "null": 95,
      "x": "ProcessGroupNCCL.cpp:260, unhandled cuda error, when using 2 nodes with 4 GPUs each",
      "z": "@SerailHydra Hi, I finally solved it with setting NCCL_SOCKET_IFNAME=ib0 and NCCL_IB_DISABLE=1, where ib0 is my ip interface.\n\nHope it will help!",
      "y": "@SerailHydra Hi, I finally solved it with setting NCCL_SOCKET_IFNAME=ib0 and NCCL_IB_DISABLE=1, where ib0 is my ip interface.\n\nHope it will help!"
   },
   {
      "null": 96,
      "x": "DataLoader for Large Corpus File",
      "z": "This is definitely a valid feature request. In fact, I implemented something called `IterableDataset` that will be used as an iterable (e.g., generator, data stream) in PyTorch. It is currently being reviewed at #19228 .",
      "y": "This is definitely a valid feature request. In fact, I implemented something called `IterableDataset` that will be used as an iterable (e.g., generator, data stream) in PyTorch. It is currently being reviewed at #19228 ."
   },
   {
      "null": 97,
      "x": "[jit] Slice assignment is completely elided in Onnx graph",
      "z": "This is an expected behavior, and appropriate warning have sent out. So close the issue.",
      "y": "This is an expected behavior, and appropriate warning have sent out. So close the issue."
   },
   {
      "null": 98,
      "x": "torch.nn.LogSoftmax.__repr__() does not include dim argument",
      "z": "@SdgJlbl  reminder that you can override `extra_repr` instead of  `__repr__`.",
      "y": "@SdgJlbl  reminder that you can override `extra_repr` instead of  `__repr__`."
   },
   {
      "null": 99,
      "x": "cuda-runtime error(4) on PyTorch 0.4.1",
      "z": "> Is there some solution can solve it without upgrade?\n\nI was able to solve this problem on 0.4.1 by reinstalling NVIDIA drivers. ",
      "y": "> Is there some solution can solve it without upgrade?\n\nI was able to solve this problem on 0.4.1 by reinstalling NVIDIA drivers. "
   },
   {
      "null": 100,
      "x": "[ONNX] The shape of PReLU weight is wrong",
      "z": "> @ezyang Thanks! Could you please give directions on how to fix it? The weight [here](https://github.com/pytorch/pytorch/blob/master/torch/onnx/symbolic_opset9.py#L396) has the type `prim::Param`. I haven't found a way to modify it.\n\nYou can try inserting ```onnx::unsqueeze``` for the weight\n```\nweight = g.op(\"Unsqueeze\", axes_i=[1, 2])\nreturn g.op(\"PRelu\", self, weight)\n```\nIf the weight is ```prim::Param``` or Constant, this unsqueeze will be optimized away if ```do_constant_folding``` is turned on while exporting. \n\nEdit: you'll also need to construct the ```axes_i``` parameter for ```Unsqueeze``` based on actual rank of ```self``` tensor, like here\nhttps://github.com/pytorch/pytorch/blob/0f58d20fe43e89138ffcbbf64fb48569539f2e4e/torch/onnx/symbolic_opset9.py#L316",
      "y": "> @ezyang Thanks! Could you please give directions on how to fix it? The weight [here](https://github.com/pytorch/pytorch/blob/master/torch/onnx/symbolic_opset9.py#L396) has the type `prim::Param`. I haven't found a way to modify it.\n\nYou can try inserting ```onnx::unsqueeze``` for the weight\n```\nweight = g.op(\"Unsqueeze\", axes_i=[1, 2])\nreturn g.op(\"PRelu\", self, weight)\n```\nIf the weight is ```prim::Param``` or Constant, this unsqueeze will be optimized away if ```do_constant_folding``` is turned on while exporting. \n\nEdit: you'll also need to construct the ```axes_i``` parameter for ```Unsqueeze``` based on actual rank of ```self``` tensor, like here\nhttps://github.com/pytorch/pytorch/blob/0f58d20fe43e89138ffcbbf64fb48569539f2e4e/torch/onnx/symbolic_opset9.py#L316"
   },
   {
      "null": 101,
      "x": "Make DDP failure recoverable",
      "z": "## Trying Solution 2\n\n#21534 seems addressed the problem but in quite a dirty way. A better solution might need to satisfy the following requirements:\n\n1. As mentioned by @pietern, the hook deletion function should be implemented in `torch/csrc/autograd/function.h`, as it owns the data. \n2. We should not slow down existing use cases of `add_post_hook` and `post_hooks()`.\n\nI initially thought about using an `OrderedDict` to store named hooks, as what we did for params, buffers, and children in `nn/Module.h`, but that would violate the second requirement.\n\n~Another possibility is that, instead of using the default deleter, we create a special deleter for the hook unique ptr in DDP, e.g., `ReducerHookDeleter`, that wraps the default deleter.  The `add_post_hook` and `post_hooks()` APIs would stay the same, then we add one `delete_post_hook<DeleterType>()` API to `torch/csrc/autograd/function.h`, which loops over all post hooks, and only delete the ones with matching deleter type, i.e., `ReducerHookDeleter`. This would be slow, but is OK, as we only need this on failures, where timeout delay will dominate. Any comments?~ Should be able to directly check hook pointer type.",
      "y": "## Trying Solution 2\n\n#21534 seems addressed the problem but in quite a dirty way. A better solution might need to satisfy the following requirements:\n\n1. As mentioned by @pietern, the hook deletion function should be implemented in `torch/csrc/autograd/function.h`, as it owns the data. \n2. We should not slow down existing use cases of `add_post_hook` and `post_hooks()`.\n\nI initially thought about using an `OrderedDict` to store named hooks, as what we did for params, buffers, and children in `nn/Module.h`, but that would violate the second requirement.\n\n~Another possibility is that, instead of using the default deleter, we create a special deleter for the hook unique ptr in DDP, e.g., `ReducerHookDeleter`, that wraps the default deleter.  The `add_post_hook` and `post_hooks()` APIs would stay the same, then we add one `delete_post_hook<DeleterType>()` API to `torch/csrc/autograd/function.h`, which loops over all post hooks, and only delete the ones with matching deleter type, i.e., `ReducerHookDeleter`. This would be slow, but is OK, as we only need this on failures, where timeout delay will dominate. Any comments?~ Should be able to directly check hook pointer type."
   },
   {
      "null": 102,
      "x": "pytorch 1.1.0 fails to load on windows (python3.6, 3.7)",
      "z": "I extracted the build scripts from your log, that is, the code below.\n```cmd\nset PYTHON=Python36\nset ARCH=-x64 \nset PYTORCH=1.1.0-cp36-cp36m\n\ngit clone -q --depth=25 --branch=travis-pytorch-avy https://github.com/jvesely/PsyNeuLink.git C:\\projects\\psyneulink-wuxsn\ncd C:\\projects\\psyneulink-wuxsn\ngit checkout -qf ae5a4dcbe1f72bf83c199ce8e95b21828b90219d\n\nchoco upgrade graphviz.portable -y\npip --version\npip install --user -U pip\npip --version\npip install --user -U certifi \"numpy<1.16\"\npip install --user git+https://github.com/benureau/leabra.git@master\nif not \"%PYTORCH%\" == \"\" pip install --user http://download.pytorch.org/whl/cpu/torch-%PYTORCH%-win_amd64.whl\nif \"%PYTORCH%\" == \"\" (findstr /V torch < dev_requirements.txt > tmp_req && move /Y tmp_req dev_requirements.txt)\npip install --user -e .[dev]\npytest --junit-xml=tests_out.xml -n auto --strict-markers tests/ %EXTRA_ARGS%\ncurl -X POST -F \"file=@tests_out.xml\" https://ci.appveyor.com/api/testresults/junit/%APPVEYOR_JOB_ID%\n```\nAnd then I tried to reproduce it locally, so I started to contrust the env according to your commands and did the smoke testing after that. And I found that it is throwing out an error.\n```cmd\nPython 3.6.2 (v3.6.2:5fd33b5, Jul  8 2017, 04:57:36) [MSC v.1900 64 bit (AMD64)] on win32\nType \"help\", \"copyright\", \"credits\" or \"license\" for more information.\n>>> import torch\nTraceback (most recent call last):\n  File \"<stdin>\", line 1, in <module>\n  File \"C:\\Python36\\lib\\site-packages\\torch\\__init__.py\", line 79, in <module>\n    from torch._C import *\nModuleNotFoundError: No module named 'numpy.core._multiarray_umath'\n>>> quit()\n```\nAfter that, I tried to upgrade numpy from 3.15 to 3.16, and it worked. I also tried to downgrade torch to 1.0.1 and it also worked. So you could actually remove the version constraint on numpy and try again.",
      "y": "I extracted the build scripts from your log, that is, the code below.\n```cmd\nset PYTHON=Python36\nset ARCH=-x64 \nset PYTORCH=1.1.0-cp36-cp36m\n\ngit clone -q --depth=25 --branch=travis-pytorch-avy https://github.com/jvesely/PsyNeuLink.git C:\\projects\\psyneulink-wuxsn\ncd C:\\projects\\psyneulink-wuxsn\ngit checkout -qf ae5a4dcbe1f72bf83c199ce8e95b21828b90219d\n\nchoco upgrade graphviz.portable -y\npip --version\npip install --user -U pip\npip --version\npip install --user -U certifi \"numpy<1.16\"\npip install --user git+https://github.com/benureau/leabra.git@master\nif not \"%PYTORCH%\" == \"\" pip install --user http://download.pytorch.org/whl/cpu/torch-%PYTORCH%-win_amd64.whl\nif \"%PYTORCH%\" == \"\" (findstr /V torch < dev_requirements.txt > tmp_req && move /Y tmp_req dev_requirements.txt)\npip install --user -e .[dev]\npytest --junit-xml=tests_out.xml -n auto --strict-markers tests/ %EXTRA_ARGS%\ncurl -X POST -F \"file=@tests_out.xml\" https://ci.appveyor.com/api/testresults/junit/%APPVEYOR_JOB_ID%\n```\nAnd then I tried to reproduce it locally, so I started to contrust the env according to your commands and did the smoke testing after that. And I found that it is throwing out an error.\n```cmd\nPython 3.6.2 (v3.6.2:5fd33b5, Jul  8 2017, 04:57:36) [MSC v.1900 64 bit (AMD64)] on win32\nType \"help\", \"copyright\", \"credits\" or \"license\" for more information.\n>>> import torch\nTraceback (most recent call last):\n  File \"<stdin>\", line 1, in <module>\n  File \"C:\\Python36\\lib\\site-packages\\torch\\__init__.py\", line 79, in <module>\n    from torch._C import *\nModuleNotFoundError: No module named 'numpy.core._multiarray_umath'\n>>> quit()\n```\nAfter that, I tried to upgrade numpy from 3.15 to 3.16, and it worked. I also tried to downgrade torch to 1.0.1 and it also worked. So you could actually remove the version constraint on numpy and try again."
   },
   {
      "null": 103,
      "x": "Why module->eval() doesn't work in C++",
      "z": "nvm, got it to work in annotation method as well.. Just had to add `model.eval();` after loading the model in C++. ",
      "y": "nvm, got it to work in annotation method as well.. Just had to add `model.eval();` after loading the model in C++. "
   },
   {
      "null": 104,
      "x": "Support sublist arguments for torch.einsum",
      "z": "Similar to numpy functionality `torch.einsum(([[0,1],[0,2],[0,3],[0,4]],[1,2,3,4]), comp_list)` can be implemented to transform first parameter to string and then call already implemented `torch.einsum('..', comp_list)`\n\nOne way to do this:\n1. implement additional `einsum` in  `ATen/native/Linear.cpp`\n```\nTensor einsum(Tensor lhs_t, Tensor rhs_t, TensorList tensors) {\n      // convert lhs_t and rhs_t to eqn\n     std::string eqn =  ... ;\n     return at::einsum(eqn, tensors);\n}\n``` \n2. add definition to `native_funcions.yaml`\n```\n- func: einsum.Tensor(Tensor lhs_t, Tensor rhs_t, Tensor[] tensors) -> Tensor\n  use_c10_dispatcher: unboxed_only\n```\n3. `torch/functional.py`  dispatch call in `def einsum(equation, *operands):` by checking if the first parameter is a string \n\nAnother:\n \n1. just transform the first argument to string directly in `torch/functional.py`, in that case, there won't be any cpp implementation for that. \n\nI have the following questions :\n1. Which approach suits best to PyTorch?\n2. Using only-python support we can treat python `ellipsis` , but I don't see how ellipsis can be supported in cpp implementation of subscripts.\n3. If it should be a cpp version, which types for subscripts arrays are better to use?  ",
      "y": "Similar to numpy functionality `torch.einsum(([[0,1],[0,2],[0,3],[0,4]],[1,2,3,4]), comp_list)` can be implemented to transform first parameter to string and then call already implemented `torch.einsum('..', comp_list)`\n\nOne way to do this:\n1. implement additional `einsum` in  `ATen/native/Linear.cpp`\n```\nTensor einsum(Tensor lhs_t, Tensor rhs_t, TensorList tensors) {\n      // convert lhs_t and rhs_t to eqn\n     std::string eqn =  ... ;\n     return at::einsum(eqn, tensors);\n}\n``` \n2. add definition to `native_funcions.yaml`\n```\n- func: einsum.Tensor(Tensor lhs_t, Tensor rhs_t, Tensor[] tensors) -> Tensor\n  use_c10_dispatcher: unboxed_only\n```\n3. `torch/functional.py`  dispatch call in `def einsum(equation, *operands):` by checking if the first parameter is a string \n\nAnother:\n \n1. just transform the first argument to string directly in `torch/functional.py`, in that case, there won't be any cpp implementation for that. \n\nI have the following questions :\n1. Which approach suits best to PyTorch?\n2. Using only-python support we can treat python `ellipsis` , but I don't see how ellipsis can be supported in cpp implementation of subscripts.\n3. If it should be a cpp version, which types for subscripts arrays are better to use?  "
   },
   {
      "null": 105,
      "x": "NCCL process groups don't support `.group_ranks()`",
      "z": "In that case you should be able to use `torch.distributed.get_world_size(group=pg)` directly.\n\nThe `pg` is still to be considered an abstract object. In reality it's an instance of the C++ `c10d::ProcessGroup` class, and you can also call `pg.size` to get the same information. This is not considered public API so please prefer going through the `torch.distributed` function instead.",
      "y": "In that case you should be able to use `torch.distributed.get_world_size(group=pg)` directly.\n\nThe `pg` is still to be considered an abstract object. In reality it's an instance of the C++ `c10d::ProcessGroup` class, and you can also call `pg.size` to get the same information. This is not considered public API so please prefer going through the `torch.distributed` function instead."
   },
   {
      "null": 106,
      "x": "JIT RuntimeError: isTensor() ASSERT FAILED",
      "z": "@wanchaol thanks for providing a solution, but I don't feel quite convinced about it. I just see one thing here: from the maintenance side of the library if we want to provide jit ready functions to be traced, does it mean that we have to wrap every single function to a ScriptModule, cast all to tensors and so ? Also from user side, in case we don't provide such wrapping could be really a pain to the user and convert the whole thing to the worst user friendly lib ever.\n\nA \"quick\" fix we figured out would be converting the non-tensors parameters from all signatures to tensor, meaning that this will break all our current api and its backward compatibility. We have also been tracking this issue: https://github.com/pytorch/pytorch/issues/20939 Do you think that this will make things easier from jit perspective side ?",
      "y": "@wanchaol thanks for providing a solution, but I don't feel quite convinced about it. I just see one thing here: from the maintenance side of the library if we want to provide jit ready functions to be traced, does it mean that we have to wrap every single function to a ScriptModule, cast all to tensors and so ? Also from user side, in case we don't provide such wrapping could be really a pain to the user and convert the whole thing to the worst user friendly lib ever.\n\nA \"quick\" fix we figured out would be converting the non-tensors parameters from all signatures to tensor, meaning that this will break all our current api and its backward compatibility. We have also been tracking this issue: https://github.com/pytorch/pytorch/issues/20939 Do you think that this will make things easier from jit perspective side ?"
   },
   {
      "null": 107,
      "x": "MKLDNN convolution leaks memory",
      "z": "Thanks for reporting this issue. We've been able to identify the issue and come up with a workaround as below. A formal fix will be submitted soon. \n\nA simple workaround in mkldnn src/common/stream.cpp.\nhttps://github.com/intel/mkl-dnn/blob/rls-v0.18/src/common/stream.cpp\n\n```\ndiff --git a/src/common/stream.cpp b/src/common/stream.cpp\nindex 054fbb9..be11cf0 100644\n--- a/src/common/stream.cpp\n+++ b/src/common/stream.cpp\n@@ -46,7 +46,9 @@ status_t stream_t::submit(const nstl::vector<primitive_t *> &prims,\n\n     const size_t start = stream_.size();\n     stream_.insert(stream_.end(), prims.begin(), prims.end());\n-    return submit_impl(start, stream_.size(), error_prim);\n+    auto res = submit_impl(start, stream_.size(), error_prim);\n+    stream_.clear();\n+    return res;\n }\n\n bool stream_t::closed() const { return true; }\n```\n\n",
      "y": "Thanks for reporting this issue. We've been able to identify the issue and come up with a workaround as below. A formal fix will be submitted soon. \n\nA simple workaround in mkldnn src/common/stream.cpp.\nhttps://github.com/intel/mkl-dnn/blob/rls-v0.18/src/common/stream.cpp\n\n```\ndiff --git a/src/common/stream.cpp b/src/common/stream.cpp\nindex 054fbb9..be11cf0 100644\n--- a/src/common/stream.cpp\n+++ b/src/common/stream.cpp\n@@ -46,7 +46,9 @@ status_t stream_t::submit(const nstl::vector<primitive_t *> &prims,\n\n     const size_t start = stream_.size();\n     stream_.insert(stream_.end(), prims.begin(), prims.end());\n-    return submit_impl(start, stream_.size(), error_prim);\n+    auto res = submit_impl(start, stream_.size(), error_prim);\n+    stream_.clear();\n+    return res;\n }\n\n bool stream_t::closed() const { return true; }\n```\n\n"
   },
   {
      "null": 108,
      "x": "How to disable MKL-DNN 64-bit compilation?",
      "z": "It may be caused by the use of `sudo` here, because some configuration of sudo would drop or only keep some current environment variables. Normally you shouldn't run the build command with `sudo`; but if you have to do so, you can try `sudo -E` to force `sudo` to preserve environment variables.",
      "y": "It may be caused by the use of `sudo` here, because some configuration of sudo would drop or only keep some current environment variables. Normally you shouldn't run the build command with `sudo`; but if you have to do so, you can try `sudo -E` to force `sudo` to preserve environment variables."
   },
   {
      "null": 109,
      "x": "Using Ninja instead of Make results in inability to find correct headers when building from source on Linux",
      "z": "Yes it seems to all work normally with USE_NINJA=OFF so far",
      "y": "Yes it seems to all work normally with USE_NINJA=OFF so far"
   },
   {
      "null": 110,
      "x": "dist.new_group() failed. BUG or I misunderstood something?",
      "z": "Hey @lecoan, the doc says:\n\n> This function requires that all processes in the main group (i.e. all processes that are part of the distributed job) enter this function, even if they are not going to be members of the group. \n\nIn the code snippet above, you have:\n\n```python\n    start = 0\n    while rank not in perm[start: start + 2]:\n        start += 2\n    group = perm.tolist()[start: start + 2]\n    pg = dist.new_group(group, timeout=timedelta(seconds=30))\n```\n\nSo all processes will enter `dist.new_group` but with different `group` ranks. As a result, all of them will enter `_new_process_group_helper` together with a smaller world size and duplicated group ranks. \n\nhttps://github.com/pytorch/pytorch/blob/590619ab8c2d237d4e0b55c8cc3552932afe7da5/torch/distributed/distributed_c10d.py#L1471-L1486\n\nCan you try making multiple `dist.new_group` calls on all processes, one call per new group. Say if you would like to create two new groups [0, 1] and [2, 3] out of 4 processes, then each process should do sth like the following, even if they are not in the group.\n\n```python\ndis.new_group([0, 1], ...)\ndis.new_group([2, 3], ...)\n```",
      "y": "Hey @lecoan, the doc says:\n\n> This function requires that all processes in the main group (i.e. all processes that are part of the distributed job) enter this function, even if they are not going to be members of the group. \n\nIn the code snippet above, you have:\n\n```python\n    start = 0\n    while rank not in perm[start: start + 2]:\n        start += 2\n    group = perm.tolist()[start: start + 2]\n    pg = dist.new_group(group, timeout=timedelta(seconds=30))\n```\n\nSo all processes will enter `dist.new_group` but with different `group` ranks. As a result, all of them will enter `_new_process_group_helper` together with a smaller world size and duplicated group ranks. \n\nhttps://github.com/pytorch/pytorch/blob/590619ab8c2d237d4e0b55c8cc3552932afe7da5/torch/distributed/distributed_c10d.py#L1471-L1486\n\nCan you try making multiple `dist.new_group` calls on all processes, one call per new group. Say if you would like to create two new groups [0, 1] and [2, 3] out of 4 processes, then each process should do sth like the following, even if they are not in the group.\n\n```python\ndis.new_group([0, 1], ...)\ndis.new_group([2, 3], ...)\n```"
   },
   {
      "null": 111,
      "x": "No type hints on nn.Parameter",
      "z": "In general, you should replicate the module import structure of the original py files, which definitionally don't have a cycle. It's possible we messed up some imports in the pyi files; in that case, you'd have to fix it.",
      "y": "In general, you should replicate the module import structure of the original py files, which definitionally don't have a cycle. It's possible we messed up some imports in the pyi files; in that case, you'd have to fix it."
   },
   {
      "null": 112,
      "x": "[jit] Can't script .type()",
      "z": "Thanks for the report! We should fix this but as a workaround until then you can use `x.to(torch.int8)`",
      "y": "Thanks for the report! We should fix this but as a workaround until then you can use `x.to(torch.int8)`"
   },
   {
      "null": 113,
      "x": "torch.distributed.gather(): the type of gather_list parameter must be list[Tensor]?",
      "z": "This works for me:\n\n```python\nimport torch\nimport torch.distributed as dist\n\ndist.init_process_group(\"gloo\")\n\ntensor = torch.tensor([dist.get_rank()], dtype=torch.int32)\nif dist.get_rank() == 0:\n    output = [tensor.clone() for _ in range(dist.get_world_size())]\n    dist.gather(tensor=tensor, gather_list=output, dst=0)\n    print(output)\nelse:\n    dist.gather(tensor=tensor, gather_list=[], dst=0)\n```\n\nThat said, we can improve this such that the non-dst doesn't have to specify `gather_list`.",
      "y": "This works for me:\n\n```python\nimport torch\nimport torch.distributed as dist\n\ndist.init_process_group(\"gloo\")\n\ntensor = torch.tensor([dist.get_rank()], dtype=torch.int32)\nif dist.get_rank() == 0:\n    output = [tensor.clone() for _ in range(dist.get_world_size())]\n    dist.gather(tensor=tensor, gather_list=output, dst=0)\n    print(output)\nelse:\n    dist.gather(tensor=tensor, gather_list=[], dst=0)\n```\n\nThat said, we can improve this such that the non-dst doesn't have to specify `gather_list`."
   },
   {
      "null": 114,
      "x": "[jit] torch.jit.script range() input type raises isInt() when used with int tensor values",
      "z": "You can find more info on https://pytorch.org/ with the Quick Start selector (change it to \"Preview (Nightly)\" at the top), but this should work for your env:\n\n```bash\npip install --pre torch torchvision -f https://download.pytorch.org/whl/nightly/cu92/torch_nightly.html\n```\n",
      "y": "You can find more info on https://pytorch.org/ with the Quick Start selector (change it to \"Preview (Nightly)\" at the top), but this should work for your env:\n\n```bash\npip install --pre torch torchvision -f https://download.pytorch.org/whl/nightly/cu92/torch_nightly.html\n```\n"
   },
   {
      "null": 115,
      "x": "RuntimeError: CUDA error: device-side assert triggered - yesterday it worked",
      "z": "I met the same error, then I set 'device = torch.device(\"cpu\")' to run codes on cpu, the error statements were much more clear.\nAnyone has a similar error can have a try.",
      "y": "I met the same error, then I set 'device = torch.device(\"cpu\")' to run codes on cpu, the error statements were much more clear.\nAnyone has a similar error can have a try."
   },
   {
      "null": 116,
      "x": "nn.functional.conv2d is a factor 5 slower when using specific weight tensor on CPU.",
      "z": "It seems your performances gets a hit by handling denormal values.\nTry to set [torch.set_flush_denormal(True)](https://pytorch.org/docs/stable/torch.html#torch.set_flush_denormal) and profile the code again.",
      "y": "It seems your performances gets a hit by handling denormal values.\nTry to set [torch.set_flush_denormal(True)](https://pytorch.org/docs/stable/torch.html#torch.set_flush_denormal) and profile the code again."
   },
   {
      "null": 117,
      "x": "interfaces of many schedulers in lr_scheduler.py are missing in lr_scheduler.pyi",
      "z": "I checked the lr_scheduler.pyi file and only found the missing type checking for two classes. I have submitted a PR #23934 . If you find other files in which there is a missing type check, I am willing to help.",
      "y": "I checked the lr_scheduler.pyi file and only found the missing type checking for two classes. I have submitted a PR #23934 . If you find other files in which there is a missing type check, I am willing to help."
   },
   {
      "null": 118,
      "x": "Memory leak when using itertools.cycle",
      "z": "Ok upon further investigation it seems that `itertools.cycle` attempts to save all outputs in order to re-cycle through them. Replacing it with something like this:\n\n```python\ndef cycle(iterable):\n    iterator = iter(iterable)\n    while True:\n        try:\n            yield next(iterator)\n        except StopIteration:\n            iterator = iter(iterable)\n```\n\nsolves the issue. Is there a canonical way of creating an infinite iterator in pytorch? It is unfortunate that `itertools.cycle` is implemented in this way, although I suppose it makes sense in the average case where the amount of data is very low.\n\nI will close this issue since it's not a pytorch issue.",
      "y": "Ok upon further investigation it seems that `itertools.cycle` attempts to save all outputs in order to re-cycle through them. Replacing it with something like this:\n\n```python\ndef cycle(iterable):\n    iterator = iter(iterable)\n    while True:\n        try:\n            yield next(iterator)\n        except StopIteration:\n            iterator = iter(iterable)\n```\n\nsolves the issue. Is there a canonical way of creating an infinite iterator in pytorch? It is unfortunate that `itertools.cycle` is implemented in this way, although I suppose it makes sense in the average case where the amount of data is very low.\n\nI will close this issue since it's not a pytorch issue."
   },
   {
      "null": 119,
      "x": "count_nonzero",
      "z": "We have added torch.count_nonzero, which I believe addresses this issue. See: https://pytorch.org/docs/master/generated/torch.count_nonzero.html?highlight=count_nonzero#torch.count_nonzero.",
      "y": "We have added torch.count_nonzero, which I believe addresses this issue. See: https://pytorch.org/docs/master/generated/torch.count_nonzero.html?highlight=count_nonzero#torch.count_nonzero."
   },
   {
      "null": 120,
      "x": "New _batch_mahalanobis slower than in previous commit",
      "z": "Yes, it is undesirable. For such shapes of A, and b, I would like to make `b` have shape `(10, 1000)` and apply triangular solve for A and new b (so no broadcasting is triggered).\n\nBtw, the regression here is caused by a performance issue of triangular_solve in GPU with batch_size=1. Consider\n```\nimport torch\nx = torch.eye(2).cuda()\ny = torch.eye(2).cuda()\nbx = x.reshape(1, 2, 2)\nby = y.reshape(1, 2, 2)\n\n%%timeit\ntorch.cuda.synchronize()\ntorch.triangular_solve(x, y)\n\n%%timeit\ntorch.cuda.synchronize()\ntorch.triangular_solve(bx, by)\n```\nwhich will return\n```\n45.4 \u00b5s \u00b1 318 ns per loop (mean \u00b1 std. dev. of 7 runs, 10000 loops each)\n```\nfor the first case and\n```\n243 \u00b5s \u00b1 3.81 \u00b5s per loop (mean \u00b1 std. dev. of 7 runs, 1000 loops each)\n```\nfor the second case.\n\nI guess we can fix it in `triangular_solve` implementation. Or I can add an if/else check to squeeze the input when `batch_shape=(1,)`. What do you think? Fixing it in `triangular_solve` seems more reasonable to me.",
      "y": "Yes, it is undesirable. For such shapes of A, and b, I would like to make `b` have shape `(10, 1000)` and apply triangular solve for A and new b (so no broadcasting is triggered).\n\nBtw, the regression here is caused by a performance issue of triangular_solve in GPU with batch_size=1. Consider\n```\nimport torch\nx = torch.eye(2).cuda()\ny = torch.eye(2).cuda()\nbx = x.reshape(1, 2, 2)\nby = y.reshape(1, 2, 2)\n\n%%timeit\ntorch.cuda.synchronize()\ntorch.triangular_solve(x, y)\n\n%%timeit\ntorch.cuda.synchronize()\ntorch.triangular_solve(bx, by)\n```\nwhich will return\n```\n45.4 \u00b5s \u00b1 318 ns per loop (mean \u00b1 std. dev. of 7 runs, 10000 loops each)\n```\nfor the first case and\n```\n243 \u00b5s \u00b1 3.81 \u00b5s per loop (mean \u00b1 std. dev. of 7 runs, 1000 loops each)\n```\nfor the second case.\n\nI guess we can fix it in `triangular_solve` implementation. Or I can add an if/else check to squeeze the input when `batch_shape=(1,)`. What do you think? Fixing it in `triangular_solve` seems more reasonable to me."
   },
   {
      "null": 121,
      "x": "Quantized Linear does not work for bias=False",
      "z": "The issue is that we don't have the direct test coverage for the `from_float` function with `bias = NULL`. The current unit test with \"indirect\" test coverage is to test `convert` and `quantize` function from here:\nhttps://github.com/pytorch/pytorch/blob/master/test/test_quantization.py#L21\n\nPreviously we have fixed the general `bias = nullptr` here: https://github.com/pytorch/pytorch/pull/22403",
      "y": "The issue is that we don't have the direct test coverage for the `from_float` function with `bias = NULL`. The current unit test with \"indirect\" test coverage is to test `convert` and `quantize` function from here:\nhttps://github.com/pytorch/pytorch/blob/master/test/test_quantization.py#L21\n\nPreviously we have fixed the general `bias = nullptr` here: https://github.com/pytorch/pytorch/pull/22403"
   },
   {
      "null": 122,
      "x": "forward_packed operator in LSTM not supported by jit scriptmodule",
      "z": "I think this is expected for your example, the hidden state is supposed to be of `Optional[Tuple[Tensor, Tensor]]` instead of just `Tensor`. Your example fails in eager mode as well\n\n```\nRuntimeError: Expected hidden[0] size (1, 1, 512), got (1, 512)\n```\n\nYour example runs fine if you use `h1` for the hidden state instead. Closing since this looks like user error, feel free to re-open if you are still running into errors.",
      "y": "I think this is expected for your example, the hidden state is supposed to be of `Optional[Tuple[Tensor, Tensor]]` instead of just `Tensor`. Your example fails in eager mode as well\n\n```\nRuntimeError: Expected hidden[0] size (1, 1, 512), got (1, 512)\n```\n\nYour example runs fine if you use `h1` for the hidden state instead. Closing since this looks like user error, feel free to re-open if you are still running into errors."
   },
   {
      "null": 123,
      "x": "Getting gradient of element of tensor wrt the element itself",
      "z": "```\nprint(torch.autograd.grad(b[0][0], b[0], allow_unused=True)\n```\n\nThis doesn't do what you want because the autograd graph for `b[0][0]` is not related to `b[0]` (no CSE happens here.) The second code is correct and you should do it that way.",
      "y": "```\nprint(torch.autograd.grad(b[0][0], b[0], allow_unused=True)\n```\n\nThis doesn't do what you want because the autograd graph for `b[0][0]` is not related to `b[0]` (no CSE happens here.) The second code is correct and you should do it that way."
   },
   {
      "null": 124,
      "x": "[naming] Promote _LRScheduler to LRScheduler",
      "z": "Since this could encourage people to experiment with the learning rate scheduler class, I'm ok if @vadimkantorov wants to open a pull request promoting `_LRScheduler` to `LRScheduler`, and keeping the old name for backward compatibility, potentially with a deprecation warning. ",
      "y": "Since this could encourage people to experiment with the learning rate scheduler class, I'm ok if @vadimkantorov wants to open a pull request promoting `_LRScheduler` to `LRScheduler`, and keeping the old name for backward compatibility, potentially with a deprecation warning. "
   },
   {
      "null": 125,
      "x": "[docs] Update and momentum formulas in SGD docs",
      "z": "IMO, this is a no brainer. I'll put up a PR.\n\nShould it be\n`v_{t+1} = p_{t}*v_{t} + g_{t+1}`? (Added a subscript for `p`)",
      "y": "IMO, this is a no brainer. I'll put up a PR.\n\nShould it be\n`v_{t+1} = p_{t}*v_{t} + g_{t+1}`? (Added a subscript for `p`)"
   },
   {
      "null": 126,
      "x": "torch.jit.trace() does not work without check_trace =False",
      "z": "> hoe to slove it ?\nJust don't pass the same tensor 3 times as the same var. See the next my comment.\n\n",
      "y": "> hoe to slove it ?\nJust don't pass the same tensor 3 times as the same var. See the next my comment.\n\n"
   },
   {
      "null": 127,
      "x": "TracerWarning: Output nr 1. of the traced function does not match the corresponding output of the Python function. Detailed error: Not within tolerance",
      "z": "I solved the problem by adding model.eval() and deleting the model.cuda(), aka, map the model to cpu",
      "y": "I solved the problem by adding model.eval() and deleting the model.cuda(), aka, map the model to cpu"
   },
   {
      "null": 128,
      "x": "[JIT] Making JIT work with Pyro's VAE example",
      "z": "> Do the examples do a lot of operations with scalar tensors?\n\nThe arguments passed to the traced functions are not scalars in many cases, but there may be other places within the traced function call (e.g. in distributions or our own internals) that have scalar operations. Is there a way to get a more detailed JIT log to localize where the issue might be?",
      "y": "> Do the examples do a lot of operations with scalar tensors?\n\nThe arguments passed to the traced functions are not scalars in many cases, but there may be other places within the traced function call (e.g. in distributions or our own internals) that have scalar operations. Is there a way to get a more detailed JIT log to localize where the issue might be?"
   },
   {
      "null": 129,
      "x": "[feature request][pytorch] finfo as in numpy and finfo for default dtype",
      "z": "If you don't care about where the code lives: https://github.com/pytorch/pytorch/blob/ddf187c198f8e249e78351ba94e773bf9d21de3a/torch/distributions/utils.py#L20",
      "y": "If you don't care about where the code lives: https://github.com/pytorch/pytorch/blob/ddf187c198f8e249e78351ba94e773bf9d21de3a/torch/distributions/utils.py#L20"
   },
   {
      "null": 130,
      "x": "Performance improvement on sparse CUDA coalesce()",
      "z": "Closing this because when nnz is large enough, CUDA kernel actually performance reasonably well:\n```\n>>> from random import *\n>>> n = 100000\n>>> I = torch.tensor([[randint(0, 99) for _ in range(3)] for _ in range(n)])\n>>> V = torch.randn(n)\n>>> size = torch.Size([1000, 1000, 1000])\n>>> S = torch.sparse_coo_tensor(I.t(), V, size)\n\n>>> %timeit S.coalesce()\n10 loops, best of 3: 30.7 ms per loop\n\n>>> S = torch.sparse_coo_tensor(I.t(), V.cuda(), size)\n>>> %timeit torch.cuda.synchronize(); S.coalesce(); torch.cuda.synchronize();\n100 loops, best of 3: 9.59 ms per loop\n```",
      "y": "Closing this because when nnz is large enough, CUDA kernel actually performance reasonably well:\n```\n>>> from random import *\n>>> n = 100000\n>>> I = torch.tensor([[randint(0, 99) for _ in range(3)] for _ in range(n)])\n>>> V = torch.randn(n)\n>>> size = torch.Size([1000, 1000, 1000])\n>>> S = torch.sparse_coo_tensor(I.t(), V, size)\n\n>>> %timeit S.coalesce()\n10 loops, best of 3: 30.7 ms per loop\n\n>>> S = torch.sparse_coo_tensor(I.t(), V.cuda(), size)\n>>> %timeit torch.cuda.synchronize(); S.coalesce(); torch.cuda.synchronize();\n100 loops, best of 3: 9.59 ms per loop\n```"
   },
   {
      "null": 131,
      "x": "autograd's elu_backward usage seems not correct",
      "z": "Closed it thinking I made a mistake but actually indeed seems wrong.\n\nThis line is on ATen's Type.h:\n`virtual Tensor elu_backward(const Tensor & grad_output, Scalar alpha, Scalar scale, const Tensor & output) const;`\n\n`elu_forward` declaration in derivatives.yaml seems wrong too, no input_scale in ATen",
      "y": "Closed it thinking I made a mistake but actually indeed seems wrong.\n\nThis line is on ATen's Type.h:\n`virtual Tensor elu_backward(const Tensor & grad_output, Scalar alpha, Scalar scale, const Tensor & output) const;`\n\n`elu_forward` declaration in derivatives.yaml seems wrong too, no input_scale in ATen"
   },
   {
      "null": 132,
      "x": "Parameters in dict not registered",
      "z": "I believe you have to use [`register_parameter`](https://pytorch.org/docs/stable/nn.html#torch.nn.Module.register_parameter) for this purpose.",
      "y": "I believe you have to use [`register_parameter`](https://pytorch.org/docs/stable/nn.html#torch.nn.Module.register_parameter) for this purpose."
   },
   {
      "null": 133,
      "x": "Cuda runtime error : the launch timed out and was terminated",
      "z": "Problem cause: https://devtalk.nvidia.com/default/topic/1043126/linux/xid-8-in-various-cuda-deep-learning-applications-for-nvidia-gtx-1080-ti/",
      "y": "Problem cause: https://devtalk.nvidia.com/default/topic/1043126/linux/xid-8-in-various-cuda-deep-learning-applications-for-nvidia-gtx-1080-ti/"
   },
   {
      "null": 134,
      "x": "[Outdated documentation] Previous Versions Installation with Conda",
      "z": "pointing `previous versions` to use `soumith` channel was a mistake. We switch from `soumith` to `pytorch` channel (i think in 0.3.1), but forgot to update the page. I just fixed the link via https://github.com/pytorch/pytorch.github.io/commit/398a3b8f6eb53ac3928cc544018e46744afcf5fa\n\nPyTorch Windows support officially released from v0.4.0, and hence we only have 0.4.0 available from the `pytorch` channel since then. For unofficial / previous releases, see `peterjc123`'s channel: https://anaconda.org/peterjc123\n\nClosed via https://github.com/pytorch/pytorch.github.io/commit/398a3b8f6eb53ac3928cc544018e46744afcf5fa\n",
      "y": "pointing `previous versions` to use `soumith` channel was a mistake. We switch from `soumith` to `pytorch` channel (i think in 0.3.1), but forgot to update the page. I just fixed the link via https://github.com/pytorch/pytorch.github.io/commit/398a3b8f6eb53ac3928cc544018e46744afcf5fa\n\nPyTorch Windows support officially released from v0.4.0, and hence we only have 0.4.0 available from the `pytorch` channel since then. For unofficial / previous releases, see `peterjc123`'s channel: https://anaconda.org/peterjc123\n\nClosed via https://github.com/pytorch/pytorch.github.io/commit/398a3b8f6eb53ac3928cc544018e46744afcf5fa\n"
   },
   {
      "null": 135,
      "x": "flip a Tensor",
      "z": "Here's @dmarnerides code but with cuda support\n\n```py\n# https://github.com/pytorch/pytorch/issues/229\ndef flip(x, dim):\n    dim = x.dim() + dim if dim < 0 else dim\n    inds = tuple(slice(None, None) if i != dim\n             else x.new(torch.arange(x.size(i)-1, -1, -1).tolist()).long()\n             for i in range(x.dim()))\n    return x[inds]\n\n# Code to test it with cpu\na = torch.Tensor([range(1, 25)]).view(1, 2, 3, 4)\nprint(a)\nprint(flip(a, 0)) # Or -4\nprint(flip(a, 1)) # Or -3\nprint(flip(a, 2)) # Or -2\nprint(flip(a, 3)) # Or -1\n\n# Code to test it with cuda\na = torch.Tensor([range(1, 25)]).view(1, 2, 3, 4).cuda()\nprint(a)\nprint(flip(a, 0)) # Or -4\nprint(flip(a, 1)) # Or -3\nprint(flip(a, 2)) # Or -2\nprint(flip(a, 3)) # Or -1\n```",
      "y": "Here's @dmarnerides code but with cuda support\n\n```py\n# https://github.com/pytorch/pytorch/issues/229\ndef flip(x, dim):\n    dim = x.dim() + dim if dim < 0 else dim\n    inds = tuple(slice(None, None) if i != dim\n             else x.new(torch.arange(x.size(i)-1, -1, -1).tolist()).long()\n             for i in range(x.dim()))\n    return x[inds]\n\n# Code to test it with cpu\na = torch.Tensor([range(1, 25)]).view(1, 2, 3, 4)\nprint(a)\nprint(flip(a, 0)) # Or -4\nprint(flip(a, 1)) # Or -3\nprint(flip(a, 2)) # Or -2\nprint(flip(a, 3)) # Or -1\n\n# Code to test it with cuda\na = torch.Tensor([range(1, 25)]).view(1, 2, 3, 4).cuda()\nprint(a)\nprint(flip(a, 0)) # Or -4\nprint(flip(a, 1)) # Or -3\nprint(flip(a, 2)) # Or -2\nprint(flip(a, 3)) # Or -1\n```"
   },
   {
      "null": 136,
      "x": "PyTorch goes distributed",
      "z": "Shubho here from SVAIL @ Baidu\n\nOne long-term thing to keep in mind is interfacing with a job scheduler - SLURM is pretty standard. I think using `salloc` with `pytorch_exec` should be fairly easy.\n\nThe framework that I helped architect at Baidu follows a peer-to-peer model - all workers are peers - no master and no slave - each peer has access to 1 GPU - and has a synchronous view of the parameter space that is completely replicated on each worker. Workers use MPI to communicate - and the MPI byte-transport layer deals with IBVerb and CUDA shared mem transport (if GPUs are on the same PCI-E root complex and many are). This peer-to-peer architecture is relatively simple and scales really well - at least to 256 GPUs and possibly more (at this point people started losing friends for hogging the cluster). It can also sustain about 70% of InfiniBand's peak bandwidth (for FDR Infiniband) but I had to reimplement the MPI collective operations since they are not optimized. This simple architecture has served us well for anything we train - various recurrent nets with or without attention, wavenet and I don't see why convnets should be an issue. This setup is however not fault tolerant - if a peer dies - the training comes to a halt - and SLURM will time it out and reschedule and this is fairly painless since we save checkpoints and it restarts from the last checkpoint. People rarely notice these failures in practice.\n\nPossibly `torch.distributed` is more full featured than this... I haven't started looking at the code yet.. but will soon...\n\nHappy to test on our cluster since we have FDR InfiniBand backplane with OpenMPI and SLURM and also contribute our learnings and code",
      "y": "Shubho here from SVAIL @ Baidu\n\nOne long-term thing to keep in mind is interfacing with a job scheduler - SLURM is pretty standard. I think using `salloc` with `pytorch_exec` should be fairly easy.\n\nThe framework that I helped architect at Baidu follows a peer-to-peer model - all workers are peers - no master and no slave - each peer has access to 1 GPU - and has a synchronous view of the parameter space that is completely replicated on each worker. Workers use MPI to communicate - and the MPI byte-transport layer deals with IBVerb and CUDA shared mem transport (if GPUs are on the same PCI-E root complex and many are). This peer-to-peer architecture is relatively simple and scales really well - at least to 256 GPUs and possibly more (at this point people started losing friends for hogging the cluster). It can also sustain about 70% of InfiniBand's peak bandwidth (for FDR Infiniband) but I had to reimplement the MPI collective operations since they are not optimized. This simple architecture has served us well for anything we train - various recurrent nets with or without attention, wavenet and I don't see why convnets should be an issue. This setup is however not fault tolerant - if a peer dies - the training comes to a halt - and SLURM will time it out and reschedule and this is fairly painless since we save checkpoints and it restarts from the last checkpoint. People rarely notice these failures in practice.\n\nPossibly `torch.distributed` is more full featured than this... I haven't started looking at the code yet.. but will soon...\n\nHappy to test on our cluster since we have FDR InfiniBand backplane with OpenMPI and SLURM and also contribute our learnings and code"
   },
   {
      "null": 137,
      "x": "UnboundLocalError when importing torch.cuda",
      "z": "In pytorch you can import torch.cuda always. cuda is lazy loaded only when actually first used",
      "y": "In pytorch you can import torch.cuda always. cuda is lazy loaded only when actually first used"
   },
   {
      "null": 138,
      "x": "ubuntu 16.04, CUDA8 and pytorch have compile issues, investigate",
      "z": "Hi @soumith \n\nI had this problem. Mine was because of the thrust library in Cuda.  (my version was 8.0.27)\n\nI updated to 8.0.44 (recent version) and it was solved. \n\n",
      "y": "Hi @soumith \n\nI had this problem. Mine was because of the thrust library in Cuda.  (my version was 8.0.27)\n\nI updated to 8.0.44 (recent version) and it was solved. \n\n"
   },
   {
      "null": 139,
      "x": "define default GPU device",
      "z": "Yeah I think `set_device` would be better. It has much clearer semantics. With `set_default_device` it might sometimes work inside a nested function, and sometimes silently have no effect.\n\n@colesbury I also don't like this very much, but as @soumith says, I think it's useful for notebooks. However, we should clearly discourage its usage in the docs, except for these situations.",
      "y": "Yeah I think `set_device` would be better. It has much clearer semantics. With `set_default_device` it might sometimes work inside a nested function, and sometimes silently have no effect.\n\n@colesbury I also don't like this very much, but as @soumith says, I think it's useful for notebooks. However, we should clearly discourage its usage in the docs, except for these situations."
   },
   {
      "null": 140,
      "x": "allow forward / backward hooks to rewrite outputs and gradients",
      "z": "You can overcame the need of the `input`/`output` by using upvalues to the function, and keeping record of them in list/dictionaries.",
      "y": "You can overcame the need of the `input`/`output` by using upvalues to the function, and keeping record of them in list/dictionaries."
   },
   {
      "null": 141,
      "x": "auto-wrap tensors as inputs to autograd",
      "z": "It's pretty much all about making the flag viral or not. The output of the function doesn't require grad if **all** inputs don't require it. This allows you to efficiently use pretrained models and never backprop through them. The output is volatile if **any** of the inputs is volatile. This is convenient for inference because you don't need to modify your parameters, but simply create a volatile input.\n\nAnother difference is that at the moment variables with requires_grad=False are still constructing the graph, while volatile ones don't (creator is None for all of them)",
      "y": "It's pretty much all about making the flag viral or not. The output of the function doesn't require grad if **all** inputs don't require it. This allows you to efficiently use pretrained models and never backprop through them. The output is volatile if **any** of the inputs is volatile. This is convenient for inference because you don't need to modify your parameters, but simply create a volatile input.\n\nAnother difference is that at the moment variables with requires_grad=False are still constructing the graph, while volatile ones don't (creator is None for all of them)"
   },
   {
      "null": 142,
      "x": "squeeze dimension after mean / sum",
      "z": "I've been working on broadcasting, and it probably makes sense to introduce broadcasting and squeeze dimension together (i.e. \"Broadcasting itself can be added sooner as it doesn't need any thought on backward-compatibility\" isn't quite true).\n\nFor example, consider test_nn.test_InstanceNorm1d:\n```\n        output = IN(input_var)\n        \n        input_reshaped = input_var.transpose(1, 0).contiguous().view(c, -1)\n        mean = input_reshaped.mean(1)\n\n        # do some calculation based on mean.data - IN.running_mean\n```\n\nHere, mean.data is (4,1) (because the dimension is not squeezed) and In.running_mean is (4).  The broadcast causes the result of the calculation to be (4,4) instead of (4,1), which changes the calculation (e.g. imagine a sum).  If mean is changed to squeeze the dimension, this calculation \"just works\" as written.  Clearly there are cases where this won't hold, but it seems like introducing them together may cause less disruption.",
      "y": "I've been working on broadcasting, and it probably makes sense to introduce broadcasting and squeeze dimension together (i.e. \"Broadcasting itself can be added sooner as it doesn't need any thought on backward-compatibility\" isn't quite true).\n\nFor example, consider test_nn.test_InstanceNorm1d:\n```\n        output = IN(input_var)\n        \n        input_reshaped = input_var.transpose(1, 0).contiguous().view(c, -1)\n        mean = input_reshaped.mean(1)\n\n        # do some calculation based on mean.data - IN.running_mean\n```\n\nHere, mean.data is (4,1) (because the dimension is not squeezed) and In.running_mean is (4).  The broadcast causes the result of the calculation to be (4,4) instead of (4,1), which changes the calculation (e.g. imagine a sum).  If mean is changed to squeeze the dimension, this calculation \"just works\" as written.  Clearly there are cases where this won't hold, but it seems like introducing them together may cause less disruption."
   },
   {
      "null": 143,
      "x": "gradient clip for optimizer",
      "z": "For those reading this thread, note that clip_grad_norm is now deprecated for clip_grad_norm_",
      "y": "For those reading this thread, note that clip_grad_norm is now deprecated for clip_grad_norm_"
   },
   {
      "null": 144,
      "x": "Segmentation fault when dividing by zero with integer tensors",
      "z": "integer division by zero getting a FP exception is something we cannot avoid, it's a hardware exception. Numpy wraps it's integer division code to literally check the denominators with conditionals and instead generates fpectl exceptions. I dont think this can be a hi-pri issue for us just looking at the amount of work involved in refactoring TH.\n\nYour second issue looks more like a bug.",
      "y": "integer division by zero getting a FP exception is something we cannot avoid, it's a hardware exception. Numpy wraps it's integer division code to literally check the denominators with conditionals and instead generates fpectl exceptions. I dont think this can be a hi-pri issue for us just looking at the amount of work involved in refactoring TH.\n\nYour second issue looks more like a bug."
   },
   {
      "null": 145,
      "x": "Softmax2d and LogSoftmax don't work for 2d and higher",
      "z": "It looks correct to me. `Softmax2d` does a softmax over channels, in each point of the 3rd and 4th dimension independently. You have only a single channel, so all values get normalized to ones.",
      "y": "It looks correct to me. `Softmax2d` does a softmax over channels, in each point of the 3rd and 4th dimension independently. You have only a single channel, so all values get normalized to ones."
   },
   {
      "null": 146,
      "x": "CUDA initialization fails fatally in multiprocessing",
      "z": "Seems that multithreading works in this example. I will first try this line and see how it works. \n\nThanks all for the help! ",
      "y": "Seems that multithreading works in this example. I will first try this line and see how it works. \n\nThanks all for the help! "
   },
   {
      "null": 147,
      "x": "cudnn backend needs contiguity checks to report better error messages",
      "z": "Ok, so it appears that cuDNN conv modules could give invalid results, but that could happen only if one had non-contiguous weights. **As long as you sticked to nn modules this bug did not affect you in any way.** For RNN modules, we haven't seen any issues like these with conv, but we'll go over the code to make sure there are no more errors like these.\n\nAnother other error is that sometimes non-contuguous inputs gave `CUDNN_STATUS_INTERNAL_ERROR` for some unknown reason. But it was harmless in a sense, that it would quietly mess up your results.\n\n@ngimel do you know of any other places where we should be more careful with contiguity checks? any hints on why non-contig inputs can give internal errors?",
      "y": "Ok, so it appears that cuDNN conv modules could give invalid results, but that could happen only if one had non-contiguous weights. **As long as you sticked to nn modules this bug did not affect you in any way.** For RNN modules, we haven't seen any issues like these with conv, but we'll go over the code to make sure there are no more errors like these.\n\nAnother other error is that sometimes non-contuguous inputs gave `CUDNN_STATUS_INTERNAL_ERROR` for some unknown reason. But it was harmless in a sense, that it would quietly mess up your results.\n\n@ngimel do you know of any other places where we should be more careful with contiguity checks? any hints on why non-contig inputs can give internal errors?"
   },
   {
      "null": 148,
      "x": "nn.Sequential should have an add_module(module) instead of add_module(name, module)",
      "z": "If you want to just change the stride/padding/dilation, you can just directly change the [property](https://github.com/pytorch/pytorch/blob/master/torch/nn/modules/conv.py#L22) on the `ConvNd` object.\n\nIf you want to reuse the pretrained weight, just do `new_layer.weight = orig_layer.weight` and then use the `new_layer` when constructing the new `Sequential`.",
      "y": "If you want to just change the stride/padding/dilation, you can just directly change the [property](https://github.com/pytorch/pytorch/blob/master/torch/nn/modules/conv.py#L22) on the `ConvNd` object.\n\nIf you want to reuse the pretrained weight, just do `new_layer.weight = orig_layer.weight` and then use the `new_layer` when constructing the new `Sequential`."
   },
   {
      "null": 149,
      "x": "Some function don't implement \"out=result\" convention",
      "z": "`out` has to be a `ByteTensor`.",
      "y": "`out` has to be a `ByteTensor`."
   },
   {
      "null": 150,
      "x": "THNN unnecessary .zero() calls on gradients",
      "z": "this is now fixed in master: https://github.com/pytorch/pytorch/blob/master/torch/csrc/autograd/functions/batch_normalization.cpp#L119",
      "y": "this is now fixed in master: https://github.com/pytorch/pytorch/blob/master/torch/csrc/autograd/functions/batch_normalization.cpp#L119"
   },
   {
      "null": 151,
      "x": "binaries compiled against newer numpy, but executed with older numpy error out",
      "z": "upgrade your numpy",
      "y": "upgrade your numpy"
   },
   {
      "null": 152,
      "x": "simpler explode and join",
      "z": "Just tried `torch.stack`, it's great, thanks!\n\nRe #289, numpy-like broadcast would be awesome (even if it was an explicit broadcast, like `t.broadcast(dim, num_repeat)`).",
      "y": "Just tried `torch.stack`, it's great, thanks!\n\nRe #289, numpy-like broadcast would be awesome (even if it was an explicit broadcast, like `t.broadcast(dim, num_repeat)`)."
   },
   {
      "null": 153,
      "x": "How to package pytorch with the file build from source.",
      "z": "Use `python setup.py bdist_wheel` to build a wheel file. The wheel file will be in the `dist` sub-directory.",
      "y": "Use `python setup.py bdist_wheel` to build a wheel file. The wheel file will be in the `dist` sub-directory."
   },
   {
      "null": 154,
      "x": "Missing tests for most torch.*(out=...) tensor operators",
      "z": "I think https://github.com/pytorch/pytorch/pull/53259 addresses this issue. Let me know if you agree @antocuni. If it doesn't, let's maybe open a new issue that explains what else should be done.",
      "y": "I think https://github.com/pytorch/pytorch/pull/53259 addresses this issue. Let me know if you agree @antocuni. If it doesn't, let's maybe open a new issue that explains what else should be done."
   },
   {
      "null": 155,
      "x": "BatchSampler & PEP 479",
      "z": "try making the list [1,2,3] into an iterator and try again. That iterator would raise StopIteration at the end and it would be converted to RuntimeError in this parent iterator according to the PEP.",
      "y": "try making the list [1,2,3] into an iterator and try again. That iterator would raise StopIteration at the end and it would be converted to RuntimeError in this parent iterator according to the PEP."
   },
   {
      "null": 156,
      "x": "`CatTransform` should work with `event_dim > 0` transforms",
      "z": "@vikigenius hmm I think I see your point now, and I think my previous comment was wrong. Does this look right to you:\n1. The concatenation `dim` can be either a batch dim or an event dim.\n2. If `dim` is a batch dim then the current logic is correct, i.e. concatenate the log-det-jacobians:\n    ```py\n    logdeghacs = []\n    for trans, length in zip(self.transforms, self.lengths):\n        ...\n        logdetjacs.append(trans.log_abs_det_jacobian(...))\n    return torch.cat(logdetjacs, dim=self.dim)\n    ```\n3. If `dim` is an event dim then we need new logic to instead *add* the component log-det-jacobians:\n    ```py\n    ...\n    return sum(logdetjacs)\n    ```\n\nIf this sounds correct, then I think there's a pretty simple fix:\n\n<details>\n\n```diff\ndiff --git a/torch/distributions/transforms.py b/torch/distributions/transforms.py\nindex 09e00d55e8..eddce21c25 100644\n--- a/torch/distributions/transforms.py\n+++ b/torch/distributions/transforms.py\n@@ -633,6 +633,7 @@ class CatTransform(Transform):\n     \"\"\"\n     def __init__(self, tseq, dim=0, lengths=None, cache_size=0):\n         assert all(isinstance(t, Transform) for t in tseq)\n+        assert len({t.event_dim for t in tseq}) == 1\n         if cache_size:\n             tseq = [t.with_cache(cache_size) for t in tseq]\n         super(CatTransform, self).__init__(cache_size=cache_size)\n@@ -686,7 +687,15 @@ class CatTransform(Transform):\n             yslice = y.narrow(self.dim, start, length)\n             logdetjacs.append(trans.log_abs_det_jacobian(xslice, yslice))\n             start = start + length  # avoid += for jit compat\n-        return torch.cat(logdetjacs, dim=self.dim)\n+        dim = self.event_dim + (self.dim if self.dim < 0 else self.dim - x.dim())\n+        if dim < 0:  # concatenate along a batch_dim\n+            return torch.cat(logdetjacs, dim=dim)\n+        else:  # concatenate along an event_dim\n+            return sum(logdetjacs)\n+\n+    @property\n+    def event_dim(self):\n+        return self.transforms[0].event_dim\n\n     @property\n     def bijective(self):\n```\n\n</details>",
      "y": "@vikigenius hmm I think I see your point now, and I think my previous comment was wrong. Does this look right to you:\n1. The concatenation `dim` can be either a batch dim or an event dim.\n2. If `dim` is a batch dim then the current logic is correct, i.e. concatenate the log-det-jacobians:\n    ```py\n    logdeghacs = []\n    for trans, length in zip(self.transforms, self.lengths):\n        ...\n        logdetjacs.append(trans.log_abs_det_jacobian(...))\n    return torch.cat(logdetjacs, dim=self.dim)\n    ```\n3. If `dim` is an event dim then we need new logic to instead *add* the component log-det-jacobians:\n    ```py\n    ...\n    return sum(logdetjacs)\n    ```\n\nIf this sounds correct, then I think there's a pretty simple fix:\n\n<details>\n\n```diff\ndiff --git a/torch/distributions/transforms.py b/torch/distributions/transforms.py\nindex 09e00d55e8..eddce21c25 100644\n--- a/torch/distributions/transforms.py\n+++ b/torch/distributions/transforms.py\n@@ -633,6 +633,7 @@ class CatTransform(Transform):\n     \"\"\"\n     def __init__(self, tseq, dim=0, lengths=None, cache_size=0):\n         assert all(isinstance(t, Transform) for t in tseq)\n+        assert len({t.event_dim for t in tseq}) == 1\n         if cache_size:\n             tseq = [t.with_cache(cache_size) for t in tseq]\n         super(CatTransform, self).__init__(cache_size=cache_size)\n@@ -686,7 +687,15 @@ class CatTransform(Transform):\n             yslice = y.narrow(self.dim, start, length)\n             logdetjacs.append(trans.log_abs_det_jacobian(xslice, yslice))\n             start = start + length  # avoid += for jit compat\n-        return torch.cat(logdetjacs, dim=self.dim)\n+        dim = self.event_dim + (self.dim if self.dim < 0 else self.dim - x.dim())\n+        if dim < 0:  # concatenate along a batch_dim\n+            return torch.cat(logdetjacs, dim=dim)\n+        else:  # concatenate along an event_dim\n+            return sum(logdetjacs)\n+\n+    @property\n+    def event_dim(self):\n+        return self.transforms[0].event_dim\n\n     @property\n     def bijective(self):\n```\n\n</details>"
   },
   {
      "null": 157,
      "x": "Build-from-source Failed on Mac OS",
      "z": "Hi,\n\nThis is simple to fix. All you have to do is to change the `%ld` to `%lld` at `torch/csrc/jit/codegen/cuda/kernel_cache.cpp:29` and `torch/csrc/jit/codegen/cuda/kernel_cache.cpp:31`. Can you make that change and try to build?",
      "y": "Hi,\n\nThis is simple to fix. All you have to do is to change the `%ld` to `%lld` at `torch/csrc/jit/codegen/cuda/kernel_cache.cpp:29` and `torch/csrc/jit/codegen/cuda/kernel_cache.cpp:31`. Can you make that change and try to build?"
   },
   {
      "null": 158,
      "x": "[Windows CUDA Build] Unknown option '-Xcompiler /w -w'",
      "z": "Seems it's similar but not exactly that issue (and that's already fixed, probably)\n\nFound 2 ways around:\n1. Turn off cuda separable compilation.\n2. msbuild is probably OK. But it's way too slow (no concurrency for nvcc, will run for hours) so I didn't do a full test.",
      "y": "Seems it's similar but not exactly that issue (and that's already fixed, probably)\n\nFound 2 ways around:\n1. Turn off cuda separable compilation.\n2. msbuild is probably OK. But it's way too slow (no concurrency for nvcc, will run for hours) so I didn't do a full test."
   },
   {
      "null": 159,
      "x": "Implementation of 2d bicubic grid sampler",
      "z": "Answer to myself. If the forward is implemented, the numerical method could be used to get the ground truth of backward gradient. i.e. forward a small amount dh, then get the output dH. dH/dh is the partial differential for the specific pixel output and pixel input",
      "y": "Answer to myself. If the forward is implemented, the numerical method could be used to get the ground truth of backward gradient. i.e. forward a small amount dh, then get the output dH. dH/dh is the partial differential for the specific pixel output and pixel input"
   },
   {
      "null": 160,
      "x": "Something goes wrong with pytorch build from source,",
      "z": "Hi,\n\nWe use github issues only for bugs or feature requests.\nPlease use the forum to ask questions: https://discuss.pytorch.org/\n\nIn your case it looks like some issues with the GLOO detection. Are you planning on using distributed? If not you can try to set `USE_DISTRIBUTED=0`.",
      "y": "Hi,\n\nWe use github issues only for bugs or feature requests.\nPlease use the forum to ask questions: https://discuss.pytorch.org/\n\nIn your case it looks like some issues with the GLOO detection. Are you planning on using distributed? If not you can try to set `USE_DISTRIBUTED=0`."
   },
   {
      "null": 161,
      "x": "OneCycleLR Scheduler does not have argument for verbose",
      "z": "Fixed by #41580",
      "y": "Fixed by #41580"
   },
   {
      "null": 162,
      "x": "rewrite the torch.sparse main doc page",
      "z": "For me it was super-confusing multitude of different matrix multiply ops:\ntorch.mm, torch.matmul torch.sparse.mm, torch.sparse.FloatTensor.spmm, torch.sparse.FloatTensor.hspmm, torch.sparse.FloatTensor.sspmm\n\nMy practical usecase was sketching: dense-sparse multiply with +1/-1 sparse matrix (requires no gradient to the sparse tensor): https://gist.github.com/vadimkantorov/d9b56f9b85f1f4ce59ffecf893a1581a#file-compact_bilinear_pooling-py-L15 ",
      "y": "For me it was super-confusing multitude of different matrix multiply ops:\ntorch.mm, torch.matmul torch.sparse.mm, torch.sparse.FloatTensor.spmm, torch.sparse.FloatTensor.hspmm, torch.sparse.FloatTensor.sspmm\n\nMy practical usecase was sketching: dense-sparse multiply with +1/-1 sparse matrix (requires no gradient to the sparse tensor): https://gist.github.com/vadimkantorov/d9b56f9b85f1f4ce59ffecf893a1581a#file-compact_bilinear_pooling-py-L15 "
   },
   {
      "null": 163,
      "x": "[feature request] ONNX export for torch.std_mean / torch.var_mean",
      "z": "We recently merged a PR (https://github.com/pytorch/pytorch/pull/45678) adding support for `var_mean` and `std_mean`. Please try it out and see if we can close this item.",
      "y": "We recently merged a PR (https://github.com/pytorch/pytorch/pull/45678) adding support for `var_mean` and `std_mean`. Please try it out and see if we can close this item."
   },
   {
      "null": 164,
      "x": "[JIT] RuntimeError:  ill formed octal specifier for List[str]",
      "z": "Thanks for the help @SplitInfinity. I can confirm that 1.7 solved the problem :D",
      "y": "Thanks for the help @SplitInfinity. I can confirm that 1.7 solved the problem :D"
   },
   {
      "null": 165,
      "x": "torch.utils.data.random_split crashes without an error message with non CPU Generator object",
      "z": "> > @Kae1101\n> > I got the same problem 2 day ago in colab, my code also ran find before. I think the problem happen when your DataLoader for training set attribute shuffle=True, you can try with your test DataLoader, which shuffle attribute set to False, the problem won't happen.\n> > I find the way to make my code run again, hope it will help you\n> > add attribute generator to your DataLoader and set it like this:\n> > train_loader = DataLoader(trainset,batch_size=batch_size,shuffle=True,num_workers=0,pin_memory=False, generator=torch.Generator(device='cuda'))\n> > Hope it gonna help :)))\n> \n> @NLQVan\n> Thanks for your solution~But I already fixed the problem by commenting out the following command:\n> \n> # torch.set_default_tensor_type(torch.cuda.FloatTensor)\n> does dataiter.next() return cpu.floatTensor by default? If it does, I think that is why the error was reported... ...\n> \n> But I still confused because I ran the same code may be more than 50 times a week before 2021/06/19 and didn't get any errors like this.\n\nThanks. I was having the same issue and downgrading to 1.8.1 (March release) from 1.9.0 (June release, current latest) fixes this. ",
      "y": "> > @Kae1101\n> > I got the same problem 2 day ago in colab, my code also ran find before. I think the problem happen when your DataLoader for training set attribute shuffle=True, you can try with your test DataLoader, which shuffle attribute set to False, the problem won't happen.\n> > I find the way to make my code run again, hope it will help you\n> > add attribute generator to your DataLoader and set it like this:\n> > train_loader = DataLoader(trainset,batch_size=batch_size,shuffle=True,num_workers=0,pin_memory=False, generator=torch.Generator(device='cuda'))\n> > Hope it gonna help :)))\n> \n> @NLQVan\n> Thanks for your solution~But I already fixed the problem by commenting out the following command:\n> \n> # torch.set_default_tensor_type(torch.cuda.FloatTensor)\n> does dataiter.next() return cpu.floatTensor by default? If it does, I think that is why the error was reported... ...\n> \n> But I still confused because I ran the same code may be more than 50 times a week before 2021/06/19 and didn't get any errors like this.\n\nThanks. I was having the same issue and downgrading to 1.8.1 (March release) from 1.9.0 (June release, current latest) fixes this. "
   },
   {
      "null": 166,
      "x": "Memory surges when loading models",
      "z": "Came across this while training on a GPU with very limited memory. Training worked fine but upon restoring from a previous checkpoint, got OOM on the GPU. Thanks so much for the hint to \"load the checkpoint to cpu first and then move onto GPU\" and the note in the torch.load() function! This easily worked for me:\n```\n# Bad gives OOM on GPU\n# params = torch.load(model_save_path, map_location=lambda storage, loc: storage)\n# Good. Load to cpu first, then to GPU\nparams = torch.load(model_save_path, map_location='cpu')\nmodel.load_state_dict(params['state_dict'])\nmodel = model.to(device)\n```\nSubsequently, I ran into them same problem with the optimizer, where I got OOM while the GPU tensors where copied. The same strategy was successful here too, namely\n```\n# Bad. Gives OOM\n# optimizer.load_state_dict(torch.load(model_save_path + '.optim')\n# Good. Works.\noptimizer.load_state_dict(torch.load(model_save_path + '.optim', map_location='cpu'))\noptimizer_to(optimizer,device)\n```\nHere, optimizer_to is the code snippet from @0phoff posted in #8741 \n```\ndef optimizer_to(optim, device):\n    for param in optim.state.values():\n        # Not sure there are any global tensors in the state dict\n        if isinstance(param, torch.Tensor):\n            param.data = param.data.to(device)\n            if param._grad is not None:\n                param._grad.data = param._grad.data.to(device)\n        elif isinstance(param, dict):\n            for subparam in param.values():\n                if isinstance(subparam, torch.Tensor):\n                    subparam.data = subparam.data.to(device)\n                    if subparam._grad is not None:\n                        subparam._grad.data = subparam._grad.data.to(device)\n```\n(This issue came up prominently on google when searching my error, so just would like to share how one can solve.)",
      "y": "Came across this while training on a GPU with very limited memory. Training worked fine but upon restoring from a previous checkpoint, got OOM on the GPU. Thanks so much for the hint to \"load the checkpoint to cpu first and then move onto GPU\" and the note in the torch.load() function! This easily worked for me:\n```\n# Bad gives OOM on GPU\n# params = torch.load(model_save_path, map_location=lambda storage, loc: storage)\n# Good. Load to cpu first, then to GPU\nparams = torch.load(model_save_path, map_location='cpu')\nmodel.load_state_dict(params['state_dict'])\nmodel = model.to(device)\n```\nSubsequently, I ran into them same problem with the optimizer, where I got OOM while the GPU tensors where copied. The same strategy was successful here too, namely\n```\n# Bad. Gives OOM\n# optimizer.load_state_dict(torch.load(model_save_path + '.optim')\n# Good. Works.\noptimizer.load_state_dict(torch.load(model_save_path + '.optim', map_location='cpu'))\noptimizer_to(optimizer,device)\n```\nHere, optimizer_to is the code snippet from @0phoff posted in #8741 \n```\ndef optimizer_to(optim, device):\n    for param in optim.state.values():\n        # Not sure there are any global tensors in the state dict\n        if isinstance(param, torch.Tensor):\n            param.data = param.data.to(device)\n            if param._grad is not None:\n                param._grad.data = param._grad.data.to(device)\n        elif isinstance(param, dict):\n            for subparam in param.values():\n                if isinstance(subparam, torch.Tensor):\n                    subparam.data = subparam.data.to(device)\n                    if subparam._grad is not None:\n                        subparam._grad.data = subparam._grad.data.to(device)\n```\n(This issue came up prominently on google when searching my error, so just would like to share how one can solve.)"
   },
   {
      "null": 167,
      "x": "[feature request] `select_index` for sparse tensors",
      "z": "Thanks. I have solved the problem. I just need to get one-hot embedding for the diag sparse matrix. So I do not need to select the specific row. Instead, I directly generate the one-hot embeddings in the training process rather than select from the sparse matrix.",
      "y": "Thanks. I have solved the problem. I just need to get one-hot embedding for the diag sparse matrix. So I do not need to select the specific row. Instead, I directly generate the one-hot embeddings in the training process rather than select from the sparse matrix."
   },
   {
      "null": 168,
      "x": "grad of output with respect to inputs  functions on cpu but not gpu",
      "z": "Your code had a bug, I fixed it for you:\n\n```\ndef test_gpu():\n    mod = testModule().cuda()\n    t = torch.ones([1, 10], requires_grad=True, device=\"cuda:0\")\n    output = mod(t)\n    output[0].backward()\n    test = t.grad\n```",
      "y": "Your code had a bug, I fixed it for you:\n\n```\ndef test_gpu():\n    mod = testModule().cuda()\n    t = torch.ones([1, 10], requires_grad=True, device=\"cuda:0\")\n    output = mod(t)\n    output[0].backward()\n    test = t.grad\n```"
   },
   {
      "null": 169,
      "x": "Caffe2 install failure",
      "z": "Serendipitously, I was able to eventually find the solution to my problem. I will post my solution here, in case someone else stumbles upon this same problem, then I'll close this issue. \n\n1. Someone else had a similar issue on the caffe2 repo: https://github.com/caffe2/caffe2/issues/2487 and the solution was to re-install the eigen3 library from the master branch at the [github mirror](https://github.com/eigenteam/eigen-git-mirror.git). So that solved this particular issue.\n2. Next, I had another problem with cuda/cudnn somehow (that I did not screenshot) but it was similar to [this problem](https://discuss.pytorch.org/t/solved-source-compile-error/9490). Incredibly, as this link suggests, the solution was to change a line in the file: /usr/include/cudnn.h from\n```c++\n#include \u201cdriver_types.h\u201d\n```\nto\n```c++\n#include <driver_types.h>\n```\nSo that it looks in the correct location for the header file `driver_types.h`...Now everything is installed properly and hopefully this helps someone else down the road.",
      "y": "Serendipitously, I was able to eventually find the solution to my problem. I will post my solution here, in case someone else stumbles upon this same problem, then I'll close this issue. \n\n1. Someone else had a similar issue on the caffe2 repo: https://github.com/caffe2/caffe2/issues/2487 and the solution was to re-install the eigen3 library from the master branch at the [github mirror](https://github.com/eigenteam/eigen-git-mirror.git). So that solved this particular issue.\n2. Next, I had another problem with cuda/cudnn somehow (that I did not screenshot) but it was similar to [this problem](https://discuss.pytorch.org/t/solved-source-compile-error/9490). Incredibly, as this link suggests, the solution was to change a line in the file: /usr/include/cudnn.h from\n```c++\n#include \u201cdriver_types.h\u201d\n```\nto\n```c++\n#include <driver_types.h>\n```\nSo that it looks in the correct location for the header file `driver_types.h`...Now everything is installed properly and hopefully this helps someone else down the road."
   },
   {
      "null": 170,
      "x": "THD refactoring",
      "z": "Don't think we need this issue from now on.",
      "y": "Don't think we need this issue from now on."
   },
   {
      "null": 171,
      "x": "[Caffe2] Are you still maintaining Caffe2 docker?",
      "z": "@jgong5 @pjh5\nI think it's better to update the Caffe2 images. Because the Detectron Dockerfile is based on Caffe2 base. \nIt's inconvenient to build our own Caffe2 while I just want to use Detectron.",
      "y": "@jgong5 @pjh5\nI think it's better to update the Caffe2 images. Because the Detectron Dockerfile is based on Caffe2 base. \nIt's inconvenient to build our own Caffe2 while I just want to use Detectron."
   },
   {
      "null": 172,
      "x": "[Feature Request] nn.Module should also get a `device` attribute",
      "z": "That\u2019s not possible. Modules can hold parameters of different types on different devices, and so it\u2019s not always possible to unambiguously determine the device.",
      "y": "That\u2019s not possible. Modules can hold parameters of different types on different devices, and so it\u2019s not always possible to unambiguously determine the device."
   },
   {
      "null": 173,
      "x": "magma in pytorch",
      "z": "because we statically link magma, the magma package is not needed at runtime.",
      "y": "because we statically link magma, the magma package is not needed at runtime."
   },
   {
      "null": 174,
      "x": "Cannot deepcopy torch.(int/float/...)*",
      "z": "That's because `numpy.float32` isn't a numpy dtype:\n```\ntype(numpy.float32)\n<class 'type'>\n\n\n>>> type(numpy.array(0).dtype)\n<class 'numpy.dtype'>\n\n>>> isinstance(numpy.array(0).dtype, type)\nFalse\n```",
      "y": "That's because `numpy.float32` isn't a numpy dtype:\n```\ntype(numpy.float32)\n<class 'type'>\n\n\n>>> type(numpy.array(0).dtype)\n<class 'numpy.dtype'>\n\n>>> isinstance(numpy.array(0).dtype, type)\nFalse\n```"
   },
   {
      "null": 175,
      "x": "Multiprocessing runtime error freeze_support() in Windows 64 bit",
      "z": "Read the windows [doc](https://pytorch.org/docs/stable/notes/windows.html#multiprocessing-error-without-if-clause-protection) please.",
      "y": "Read the windows [doc](https://pytorch.org/docs/stable/notes/windows.html#multiprocessing-error-without-if-clause-protection) please."
   },
   {
      "null": 176,
      "x": "AttributeError: module 'torch.nn' has no attribute 'BCEWithLogitsLoss'",
      "z": "That is too old. You should upgrade to the latest version of pytorch (which is 0.4). Installation instructions can be found at our [main website](https://pytorch.org/). Hope that helps!\n\nIf you don't want to upgrade you can probably copy and paste the BCELossWithLogits code and make your own Loss function.",
      "y": "That is too old. You should upgrade to the latest version of pytorch (which is 0.4). Installation instructions can be found at our [main website](https://pytorch.org/). Hope that helps!\n\nIf you don't want to upgrade you can probably copy and paste the BCELossWithLogits code and make your own Loss function."
   },
   {
      "null": 177,
      "x": "[PyTorch] KeyError: 'momentum' while using SGD optimizer",
      "z": "Because I loaded checkpoints from an Adam optimizer.",
      "y": "Because I loaded checkpoints from an Adam optimizer."
   },
   {
      "null": 178,
      "x": "Drop support for magma v1 (compilation with it is broken right now)",
      "z": "Remove all the `MAGMA_V2` ifdefs",
      "y": "Remove all the `MAGMA_V2` ifdefs"
   },
   {
      "null": 179,
      "x": "Can't get attribute 'Net' on <module '__main__' from 'D:/demo/cnn/test1.py'>",
      "z": "You should save & load the statedict instead. :)\nSee https://pytorch.org/docs/master/notes/serialization.html",
      "y": "You should save & load the statedict instead. :)\nSee https://pytorch.org/docs/master/notes/serialization.html"
   },
   {
      "null": 180,
      "x": "Failure to install caffe2 builded from source",
      "z": "Closing this issue due to age and because it is now recommended to use PyTorch, not Caffe2. If this is still relevant please file a new issue. ",
      "y": "Closing this issue due to age and because it is now recommended to use PyTorch, not Caffe2. If this is still relevant please file a new issue. "
   },
   {
      "null": 181,
      "x": "Compiling Pytorch 0.4 from source for Tegra (arm processor) fails",
      "z": "I'm closing this since the original problem was solved with commit `fb5cc630f6f4cbdebde08e0e82a9679431afa9d2 `.\n\n@juanmed, if you're running into build issues on Tegra can you open up a new issue with the relevant details (build logs, etc.)? Although, to be honest, I'm not sure how much effort we will put into to supporting PyTorch 0.4. ",
      "y": "I'm closing this since the original problem was solved with commit `fb5cc630f6f4cbdebde08e0e82a9679431afa9d2 `.\n\n@juanmed, if you're running into build issues on Tegra can you open up a new issue with the relevant details (build logs, etc.)? Although, to be honest, I'm not sure how much effort we will put into to supporting PyTorch 0.4. "
   },
   {
      "null": 182,
      "x": "\"Parameters of a model after .cuda() will be different objects with those before the call.\" is wrong.",
      "z": "> If you need to move a model to GPU via .cuda(), please do so before constructing optimizers for it. Parameters of a model after .cuda() will be different objects with those before the call.\n>\n> In general, you should make sure that optimized parameters live in consistent locations when optimizers are constructed and used.\n\nStarting from https://github.com/pytorch/pytorch/pull/21613, the new behavior we will have in future releases is consistent with this warning, which is that parameters of a model after dtype/device conversion functions such as `.cuda()`/`.cpu()`/`.to()`/`.float()`/`.double()` will be different objects with those before the call (you can enable this new behavior by setting `torch.__future__.set_overwrite_module_params_on_conversion(True)`. Hence we strongly recommend converting the model to a different device / dtype **before** constructing optimizers for it.",
      "y": "> If you need to move a model to GPU via .cuda(), please do so before constructing optimizers for it. Parameters of a model after .cuda() will be different objects with those before the call.\n>\n> In general, you should make sure that optimized parameters live in consistent locations when optimizers are constructed and used.\n\nStarting from https://github.com/pytorch/pytorch/pull/21613, the new behavior we will have in future releases is consistent with this warning, which is that parameters of a model after dtype/device conversion functions such as `.cuda()`/`.cpu()`/`.to()`/`.float()`/`.double()` will be different objects with those before the call (you can enable this new behavior by setting `torch.__future__.set_overwrite_module_params_on_conversion(True)`. Hence we strongly recommend converting the model to a different device / dtype **before** constructing optimizers for it."
   },
   {
      "null": 183,
      "x": "[PyTorch] [ONNX] Peephole optimizer transpose fusion broken",
      "z": "Try this on for size https://github.com/pytorch/pytorch/pull/7872\n",
      "y": "Try this on for size https://github.com/pytorch/pytorch/pull/7872\n"
   },
   {
      "null": 184,
      "x": "[PyTorch] torch.stft is slow on cpu compared to numpy",
      "z": "@Rikorose Now the master has a `stft` with a new signature consistent with `librosa` and using `fft`.",
      "y": "@Rikorose Now the master has a `stft` with a new signature consistent with `librosa` and using `fft`."
   },
   {
      "null": 185,
      "x": "Cannot import onnx_caffe2.backend",
      "z": "I finally resolved it. Seems like I need to use the protobuf that caffe2 depends on instead of the one from conda-forge (they are both 3.5.2 though), to do this just install caffe2 before onnx. And I have to build onnx from source.",
      "y": "I finally resolved it. Seems like I need to use the protobuf that caffe2 depends on instead of the one from conda-forge (they are both 3.5.2 though), to do this just install caffe2 before onnx. And I have to build onnx from source."
   },
   {
      "null": 186,
      "x": "Serious perf drop on CPU",
      "z": "@mingfeima - Thank you for looking into this further and the update! \n\nI looked into [removing tbb and replacing it with openmp](https://github.com/pytorch/pytorch/pull/8255) and then running the benchmark. I get much better performance (i.e. am able to close the perf gap) when doing so. You could give this a try as well if you like. We generally prefer tbb for many reasons, but might need to (temporarily) go ahead with this if it turns out to be the main source for this and then reintroduce it as part of a larger effort.\n\nGenerally we can't expect the user to set or know about things like `KMP_BLOCKTIME` or \n `KMP_AFFINITY`, so we'd either need to set them statically or change our setup so that they are not necessary.\n\nEDIT: @mingfeima - I assume this is mkldnn through your channel? I've also seen some issues where it won't set the rpath to link against openmp and mklml\n\n```\n/usr/bin/ld: warning: libmklml_intel.so, needed by //private/home/cpuhrsch/miniconda2/lib/libmkldnn.so.0, not found (try using -rpath or -rpath-link)\n/usr/bin/ld: warning: libiomp5.so, needed by //private/home/cpuhrsch/miniconda2/lib/libmkldnn.so.0, not found (try using -rpath or -rpath-link)\n```\n\nIt can be fixed by setting my LD_LIBRARY_PATH to point to `/private/home/cpuhrsch/miniconda2/lib`, but I'm wondering if this is something you could adjust as part of your conda package in general? To give you more context, I'm building a binary against libcaffe2.so within a repository outside of pytorch, which was build using mkldnn using [this cmake setup](https://github.com/pytorch/benchmark/blob/f8fb533f5c62f0766eef2e67d372e94b8be7312f/timing/cpp/CMakeLists.txt).",
      "y": "@mingfeima - Thank you for looking into this further and the update! \n\nI looked into [removing tbb and replacing it with openmp](https://github.com/pytorch/pytorch/pull/8255) and then running the benchmark. I get much better performance (i.e. am able to close the perf gap) when doing so. You could give this a try as well if you like. We generally prefer tbb for many reasons, but might need to (temporarily) go ahead with this if it turns out to be the main source for this and then reintroduce it as part of a larger effort.\n\nGenerally we can't expect the user to set or know about things like `KMP_BLOCKTIME` or \n `KMP_AFFINITY`, so we'd either need to set them statically or change our setup so that they are not necessary.\n\nEDIT: @mingfeima - I assume this is mkldnn through your channel? I've also seen some issues where it won't set the rpath to link against openmp and mklml\n\n```\n/usr/bin/ld: warning: libmklml_intel.so, needed by //private/home/cpuhrsch/miniconda2/lib/libmkldnn.so.0, not found (try using -rpath or -rpath-link)\n/usr/bin/ld: warning: libiomp5.so, needed by //private/home/cpuhrsch/miniconda2/lib/libmkldnn.so.0, not found (try using -rpath or -rpath-link)\n```\n\nIt can be fixed by setting my LD_LIBRARY_PATH to point to `/private/home/cpuhrsch/miniconda2/lib`, but I'm wondering if this is something you could adjust as part of your conda package in general? To give you more context, I'm building a binary against libcaffe2.so within a repository outside of pytorch, which was build using mkldnn using [this cmake setup](https://github.com/pytorch/benchmark/blob/f8fb533f5c62f0766eef2e67d372e94b8be7312f/timing/cpp/CMakeLists.txt)."
   },
   {
      "null": 187,
      "x": "How to implement the internal zero padding for the feature map?",
      "z": "This works:\n```py\n>>> import torch.nn.functional as F\n>>>\n>>> def pad_within(x, stride=2):\n...   w = x.new_zeros(stride, stride)\n...   w[0, 0] = 1\n...   return F.conv_transpose2d(x, w.expand(x.size(1), 1, stride, stride), stride=stride, groups=x.size(1))\n...\n>>> x = torch.arange(8, dtype=torch.float).view(2, 4).expand(1, 3, 2, 4)\n>>> x\ntensor([[[[ 0.,  1.,  2.,  3.],\n          [ 4.,  5.,  6.,  7.]],\n\n         [[ 0.,  1.,  2.,  3.],\n          [ 4.,  5.,  6.,  7.]],\n\n         [[ 0.,  1.,  2.,  3.],\n          [ 4.,  5.,  6.,  7.]]]])\n>>> pad_within(x)\ntensor([[[[ 0.,  0.,  1.,  0.,  2.,  0.,  3.,  0.],\n          [ 0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.],\n          [ 4.,  0.,  5.,  0.,  6.,  0.,  7.,  0.],\n          [ 0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.]],\n\n         [[ 0.,  0.,  1.,  0.,  2.,  0.,  3.,  0.],\n          [ 0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.],\n          [ 4.,  0.,  5.,  0.,  6.,  0.,  7.,  0.],\n          [ 0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.]],\n\n         [[ 0.,  0.,  1.,  0.,  2.,  0.,  3.,  0.],\n          [ 0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.],\n          [ 4.,  0.,  5.,  0.,  6.,  0.,  7.,  0.],\n          [ 0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.]]]])\n\n```\n\nyou can wrap this in a module to avoid recomputing the weight every time.",
      "y": "This works:\n```py\n>>> import torch.nn.functional as F\n>>>\n>>> def pad_within(x, stride=2):\n...   w = x.new_zeros(stride, stride)\n...   w[0, 0] = 1\n...   return F.conv_transpose2d(x, w.expand(x.size(1), 1, stride, stride), stride=stride, groups=x.size(1))\n...\n>>> x = torch.arange(8, dtype=torch.float).view(2, 4).expand(1, 3, 2, 4)\n>>> x\ntensor([[[[ 0.,  1.,  2.,  3.],\n          [ 4.,  5.,  6.,  7.]],\n\n         [[ 0.,  1.,  2.,  3.],\n          [ 4.,  5.,  6.,  7.]],\n\n         [[ 0.,  1.,  2.,  3.],\n          [ 4.,  5.,  6.,  7.]]]])\n>>> pad_within(x)\ntensor([[[[ 0.,  0.,  1.,  0.,  2.,  0.,  3.,  0.],\n          [ 0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.],\n          [ 4.,  0.,  5.,  0.,  6.,  0.,  7.,  0.],\n          [ 0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.]],\n\n         [[ 0.,  0.,  1.,  0.,  2.,  0.,  3.,  0.],\n          [ 0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.],\n          [ 4.,  0.,  5.,  0.,  6.,  0.,  7.,  0.],\n          [ 0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.]],\n\n         [[ 0.,  0.,  1.,  0.,  2.,  0.,  3.,  0.],\n          [ 0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.],\n          [ 4.,  0.,  5.,  0.,  6.,  0.,  7.,  0.],\n          [ 0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.]]]])\n\n```\n\nyou can wrap this in a module to avoid recomputing the weight every time."
   },
   {
      "null": 188,
      "x": "Some confusion about the grad of torch.sign()",
      "z": "You should call `x_binary.retain_grad()`",
      "y": "You should call `x_binary.retain_grad()`"
   },
   {
      "null": 189,
      "x": "[feature request] Official CUDA support for macOS through eGPU",
      "z": "We do support macOS with CUDA GPUs. However atm you have to build from source.",
      "y": "We do support macOS with CUDA GPUs. However atm you have to build from source."
   },
   {
      "null": 190,
      "x": "Pytorch Model Summary",
      "z": "So something along the lines of:\n\n```\nfrom prettytable import PrettyTable\n\ndef count_parameters(model):\n    table = PrettyTable([\"Modules\", \"Parameters\"])\n    total_params = 0\n    for name, parameter in model.named_parameters():\n        if parameter.requires_grad:\n            param = parameter.numel()\n            table.add_row([name, param])\n            total_params+=param\n    print(table)\n    return f\"{total_params:,}\"\n    \ncount_parameters(net)\n```\n\nwhich outputs:\n\n```\n+---------------------+------------+\n|       Modules       | Parameters |\n+---------------------+------------+\n|  features.0.weight  |    1728    |\n|   features.0.bias   |     64     |\n|  features.3.weight  |   73728    |\n|   features.3.bias   |    128     |\n|  features.6.weight  |   294912   |\n|   features.6.bias   |    256     |\n|  features.8.weight  |   589824   |\n|   features.8.bias   |    256     |\n|  features.11.weight |  1179648   |\n|   features.11.bias  |    512     |\n|  features.13.weight |  2359296   |\n|   features.13.bias  |    512     |\n|  features.16.weight |  2359296   |\n|   features.16.bias  |    512     |\n|  features.18.weight |  2359296   |\n|   features.18.bias  |    512     |\n| classifier.0.weight | 102760448  |\n|  classifier.0.bias  |    4096    |\n| classifier.3.weight |  16777216  |\n|  classifier.3.bias  |    4096    |\n| classifier.6.weight |  4096000   |\n|  classifier.6.bias  |    1000    |\n+---------------------+------------+\n```\n\ncould be done.\n\nWell, this doesn't give shape after each layer.",
      "y": "So something along the lines of:\n\n```\nfrom prettytable import PrettyTable\n\ndef count_parameters(model):\n    table = PrettyTable([\"Modules\", \"Parameters\"])\n    total_params = 0\n    for name, parameter in model.named_parameters():\n        if parameter.requires_grad:\n            param = parameter.numel()\n            table.add_row([name, param])\n            total_params+=param\n    print(table)\n    return f\"{total_params:,}\"\n    \ncount_parameters(net)\n```\n\nwhich outputs:\n\n```\n+---------------------+------------+\n|       Modules       | Parameters |\n+---------------------+------------+\n|  features.0.weight  |    1728    |\n|   features.0.bias   |     64     |\n|  features.3.weight  |   73728    |\n|   features.3.bias   |    128     |\n|  features.6.weight  |   294912   |\n|   features.6.bias   |    256     |\n|  features.8.weight  |   589824   |\n|   features.8.bias   |    256     |\n|  features.11.weight |  1179648   |\n|   features.11.bias  |    512     |\n|  features.13.weight |  2359296   |\n|   features.13.bias  |    512     |\n|  features.16.weight |  2359296   |\n|   features.16.bias  |    512     |\n|  features.18.weight |  2359296   |\n|   features.18.bias  |    512     |\n| classifier.0.weight | 102760448  |\n|  classifier.0.bias  |    4096    |\n| classifier.3.weight |  16777216  |\n|  classifier.3.bias  |    4096    |\n| classifier.6.weight |  4096000   |\n|  classifier.6.bias  |    1000    |\n+---------------------+------------+\n```\n\ncould be done.\n\nWell, this doesn't give shape after each layer."
   },
   {
      "null": 191,
      "x": "register_full_backward_hook does not consistently fire",
      "z": "The temporary fix you can use is to make the input require gradients.\n\n",
      "y": "The temporary fix you can use is to make the input require gradients.\n\n"
   },
   {
      "null": 192,
      "x": "[docs] Rendering type hint issues at torchvision",
      "z": "Hello @vadimkantorov \nThis was issued due to Sphinx 3.x. This has been fixed over torchvision master by downgrading Sphinx to 2.4.4\nDocumentations look are fine now over the webpages for master as well as stable!\n\n@vadimkantorov rendering is not unified, for each domain library text, vision and audio docs are built and deployed over GitHub pages from their respective repos!\n\nFeel free to close this as it is fixed!",
      "y": "Hello @vadimkantorov \nThis was issued due to Sphinx 3.x. This has been fixed over torchvision master by downgrading Sphinx to 2.4.4\nDocumentations look are fine now over the webpages for master as well as stable!\n\n@vadimkantorov rendering is not unified, for each domain library text, vision and audio docs are built and deployed over GitHub pages from their respective repos!\n\nFeel free to close this as it is fixed!"
   },
   {
      "null": 193,
      "x": "Fail to LOADING A TORCHSCRIPT MODEL IN C++",
      "z": "> Here is the other one: https://download.pytorch.org/libtorch/cpu/libtorch-shared-with-deps-1.8.1%2Bcpu.zip. Can you try using this? I used this one and the tutorial is working as expected.\n\nThank you very much! I have succeed to build the example use the libtorch: \n downloaded from `https://download.pytorch.org/libtorch/cpu/libtorch-shared-with-deps-1.8.1%2Bcpu.zip`.",
      "y": "> Here is the other one: https://download.pytorch.org/libtorch/cpu/libtorch-shared-with-deps-1.8.1%2Bcpu.zip. Can you try using this? I used this one and the tutorial is working as expected.\n\nThank you very much! I have succeed to build the example use the libtorch: \n downloaded from `https://download.pytorch.org/libtorch/cpu/libtorch-shared-with-deps-1.8.1%2Bcpu.zip`."
   },
   {
      "null": 194,
      "x": "Create a context manager to enable InferenceMode in python frontend",
      "z": "You can already use `torch.is_grad_enabled()` (or the corresponding c++ API) to see if you are in a context where you might end up creating order + 1 derivative.",
      "y": "You can already use `torch.is_grad_enabled()` (or the corresponding c++ API) to see if you are in a context where you might end up creating order + 1 derivative."
   },
   {
      "null": 195,
      "x": "Cleaner mechanism in the source-code to check if multiple tensors are on the same device",
      "z": "We should avoid wrapping Tensor arguments in TensorArg for perf reasons (this bumps ref count), and all the mentioned functions act on TensorArgs. ",
      "y": "We should avoid wrapping Tensor arguments in TensorArg for perf reasons (this bumps ref count), and all the mentioned functions act on TensorArgs. "
   },
   {
      "null": 196,
      "x": "Incorrect example output in sparse_csr_tensor doc-string",
      "z": "We would definitely accept PRs correcting examples to reflect the current version of PyTorch!",
      "y": "We would definitely accept PRs correcting examples to reflect the current version of PyTorch!"
   },
   {
      "null": 197,
      "x": "sparse_csr_tensor segfaults when crow_indices or col_indices are non-tensors",
      "z": "This issue waqs solved here: https://github.com/pytorch/pytorch/pull/59010",
      "y": "This issue waqs solved here: https://github.com/pytorch/pytorch/pull/59010"
   },
   {
      "null": 198,
      "x": "default_pg_timeout in torch/testing/_internal/distributed/distributed_test.py is not sufficient to change system-wide timeouts",
      "z": "@ezyang You are running into the RPC timeout which is different from the process group timeout. The default timeout for RPC is set here: https://github.com/pytorch/pytorch/blob/d83ae5d1b74df1ad6e2b80652392ce0c2b31f3f3/torch/csrc/distributed/rpc/init.cpp#L80. \n\nYou can also modify the rpc timeout by specifying it in rpc_backen_options.rpc_timeout as part of the init_rpc call: https://github.com/pytorch/pytorch/blob/master/torch/testing/_internal/dist_utils.py#L83",
      "y": "@ezyang You are running into the RPC timeout which is different from the process group timeout. The default timeout for RPC is set here: https://github.com/pytorch/pytorch/blob/d83ae5d1b74df1ad6e2b80652392ce0c2b31f3f3/torch/csrc/distributed/rpc/init.cpp#L80. \n\nYou can also modify the rpc timeout by specifying it in rpc_backen_options.rpc_timeout as part of the init_rpc call: https://github.com/pytorch/pytorch/blob/master/torch/testing/_internal/dist_utils.py#L83"
   },
   {
      "null": 199,
      "x": "quick-check didn't capture error until PR landed",
      "z": "Just checked; the reason is because the Facebook-internal diff [D27774396](https://www.internalfb.com/diff/D27774396) did not match the GitHub pull request.\n\nYou can also see this by comparing the PR diff...\n\n- https://github.com/pytorch/pytorch/pull/55992/files#diff-e7602003c853555c44d805885670cf3500aefe9b8cfec7f8571e2d11f57fd56dR36\n\n... to the commit that landed to `master`:\n\n- https://github.com/pytorch/pytorch/commit/0df239e55082ab947e07161468b61f324eb6bed5#diff-e7602003c853555c44d805885670cf3500aefe9b8cfec7f8571e2d11f57fd56dR36",
      "y": "Just checked; the reason is because the Facebook-internal diff [D27774396](https://www.internalfb.com/diff/D27774396) did not match the GitHub pull request.\n\nYou can also see this by comparing the PR diff...\n\n- https://github.com/pytorch/pytorch/pull/55992/files#diff-e7602003c853555c44d805885670cf3500aefe9b8cfec7f8571e2d11f57fd56dR36\n\n... to the commit that landed to `master`:\n\n- https://github.com/pytorch/pytorch/commit/0df239e55082ab947e07161468b61f324eb6bed5#diff-e7602003c853555c44d805885670cf3500aefe9b8cfec7f8571e2d11f57fd56dR36"
   },
   {
      "null": 200,
      "x": "Element-wise max of two Tensors computes the wrong gradient in case of equality",
      "z": "Looks like the [`torch.amax()`]() docs are aware of this behavior:\n```\namax/amin evenly distributes gradient between equal values, while max(dim)/min(dim) propagates gradient only to a single index in the source tensor.\n```",
      "y": "Looks like the [`torch.amax()`]() docs are aware of this behavior:\n```\namax/amin evenly distributes gradient between equal values, while max(dim)/min(dim) propagates gradient only to a single index in the source tensor.\n```"
   },
   {
      "null": 201,
      "x": "[NNC] Vectorization caused wrong results",
      "z": "Fixed by #59423",
      "y": "Fixed by #59423"
   },
   {
      "null": 202,
      "x": "torch.gather behavior changed from 1.5.1 to master",
      "z": "Fixed by #41672.",
      "y": "Fixed by #41672."
   },
   {
      "null": 203,
      "x": "Is STFT in torchaudio consistent with librosa?",
      "z": "As far as I know, `stft` was not part of `torchaudio`. (@vincentqb can clarify on this one)\n\n`istft` was originally `torchaudio`, but recently moved to `pytorch`. \n\nSee https://github.com/pytorch/pytorch/issues/3775 for the development around FT.\n\nWe check that `istft` is inverse of `torch.stft`, (see [the tests here](https://github.com/pytorch/audio/blob/master/test/functional_cpu_test.py) the same tests have been ported to PyTorch main repo.) but we do not check the parity against `librosa`.\n\n@LordOfLuck\n\n> consistent with librosa implementations?\n\nIf you can give the configuration you are considering to use, we can provide you better insight. In general, I do not expect PyTorch's `stft` to match `librosa` for all the possible combination of parameters. (at least I did not see a test torch check the parity of `stft`).",
      "y": "As far as I know, `stft` was not part of `torchaudio`. (@vincentqb can clarify on this one)\n\n`istft` was originally `torchaudio`, but recently moved to `pytorch`. \n\nSee https://github.com/pytorch/pytorch/issues/3775 for the development around FT.\n\nWe check that `istft` is inverse of `torch.stft`, (see [the tests here](https://github.com/pytorch/audio/blob/master/test/functional_cpu_test.py) the same tests have been ported to PyTorch main repo.) but we do not check the parity against `librosa`.\n\n@LordOfLuck\n\n> consistent with librosa implementations?\n\nIf you can give the configuration you are considering to use, we can provide you better insight. In general, I do not expect PyTorch's `stft` to match `librosa` for all the possible combination of parameters. (at least I did not see a test torch check the parity of `stft`)."
   },
   {
      "null": 204,
      "x": "quantization.fuse_modules fails with Conv1d and BatchNorm1d",
      "z": "Here (https://github.com/pytorch/pytorch/blob/0c77bd7c0bbd4d6e50a5f3ce7b4debbee85d7963/torch/quantization/fuse_modules.py#L106) is a list of modules which can be fused.  LeakyRELU fusion does not work because we don't have a fusion implemented for it.",
      "y": "Here (https://github.com/pytorch/pytorch/blob/0c77bd7c0bbd4d6e50a5f3ce7b4debbee85d7963/torch/quantization/fuse_modules.py#L106) is a list of modules which can be fused.  LeakyRELU fusion does not work because we don't have a fusion implemented for it."
   },
   {
      "null": 205,
      "x": "weird bug of torchscript: it thinks my python bool is a tensor but it's not",
      "z": "I think this is working as intended (other than the confusing error expr range issue).\n\nEMPTY_FLOAT is a mutable tensor outside of the scripted function, TorchScript wouldn't know how to deal with it. Think about the case where you save this model on disk and another program loads it back. This second program doesn't have access to the EMPTY_FLOAT object in the first program. Therefore TorchScript is being conservative here.\n\nWith that being said, I assume your intention is actually to use the value of EMPTY_TENSOR as constant. In that case, I would suggest adding EMPTY_TENSOR as another functional argument.\n\n@suo, any better suggestions?",
      "y": "I think this is working as intended (other than the confusing error expr range issue).\n\nEMPTY_FLOAT is a mutable tensor outside of the scripted function, TorchScript wouldn't know how to deal with it. Think about the case where you save this model on disk and another program loads it back. This second program doesn't have access to the EMPTY_FLOAT object in the first program. Therefore TorchScript is being conservative here.\n\nWith that being said, I assume your intention is actually to use the value of EMPTY_TENSOR as constant. In that case, I would suggest adding EMPTY_TENSOR as another functional argument.\n\n@suo, any better suggestions?"
   },
   {
      "null": 206,
      "x": "Add SpectralOps CPU implementation for ARM/PowerPC processors (where MKL is not available)",
      "z": "Thanks @malfet. I was able to compute fft on ARM by using CUDA device on waveform:\n```\nimport torchaudio\nimport torch\n\nwaveform, sample_rate = torchaudio.load('test.wav')\nwaveform = waveform.to(\"cuda:0\")\n\nspectrogram = torchaudio.transforms.Spectrogram(sample_rate).to(\"cuda:0\")(waveform)\n```",
      "y": "Thanks @malfet. I was able to compute fft on ARM by using CUDA device on waveform:\n```\nimport torchaudio\nimport torch\n\nwaveform, sample_rate = torchaudio.load('test.wav')\nwaveform = waveform.to(\"cuda:0\")\n\nspectrogram = torchaudio.transforms.Spectrogram(sample_rate).to(\"cuda:0\")(waveform)\n```"
   },
   {
      "null": 207,
      "x": "cudnn8 version check fails",
      "z": "These lines has already been changed to\n```cmake\n  # Get cuDNN version\n  if(EXISTS ${CUDNN_INCLUDE_PATH}/cudnn_version.h)\n    file(READ ${CUDNN_INCLUDE_PATH}/cudnn_version.h CUDNN_HEADER_CONTENTS)\n  else()\n    file(READ ${CUDNN_INCLUDE_PATH}/cudnn.h CUDNN_HEADER_CONTENTS)\n  endif()\n```\nplease update to latest master branch",
      "y": "These lines has already been changed to\n```cmake\n  # Get cuDNN version\n  if(EXISTS ${CUDNN_INCLUDE_PATH}/cudnn_version.h)\n    file(READ ${CUDNN_INCLUDE_PATH}/cudnn_version.h CUDNN_HEADER_CONTENTS)\n  else()\n    file(READ ${CUDNN_INCLUDE_PATH}/cudnn.h CUDNN_HEADER_CONTENTS)\n  endif()\n```\nplease update to latest master branch"
   },
   {
      "null": 208,
      "x": "AssertionError: Torch not compiled with CUDA enabled - DETECTRON CPU/LINUX TRAINING ERROR",
      "z": "@svideloc have you reported this to detectron2?  This looks like a detectron2 issue to me, not a pytorch issue.\n\nIf you find that that is not the case, feel free to reopen this issue.",
      "y": "@svideloc have you reported this to detectron2?  This looks like a detectron2 issue to me, not a pytorch issue.\n\nIf you find that that is not the case, feel free to reopen this issue."
   },
   {
      "null": 209,
      "x": "torch.distributed and RPC cannot both be initialized with the same host:port pair.",
      "z": "@froody and @blefaudeux brought up a good point that, it might be too much hurdle for users to initialize twice (c10d and RPC), it might be easier to use if we support initializing an RPC gang using an initialized ProcessGroup. \n\nI recall we discussed the possibility to have a new `init_rpc` API that takes a Store instead of `rank`, `world_size`, `init_method` etc. Can this be one step further for solving this issue? E.g., we let `ProcessGroup` expose an API to return its store, and then init RPC using that Store. \n\nThere are different options to implement this behavior.\n\nOption 1: As the c10d gang is stable with fixed ranks, the derived RPC gang can also stay that way and share the same rank/id with the ProcessGroup instance. \nOption 2: Let the RPC stay decoupled from the c10d ProcessGroup, and still allows dynamic join/leave. In this case, we cannot match the rank/id between ProcessGroup and RPC agents any more. \n\n",
      "y": "@froody and @blefaudeux brought up a good point that, it might be too much hurdle for users to initialize twice (c10d and RPC), it might be easier to use if we support initializing an RPC gang using an initialized ProcessGroup. \n\nI recall we discussed the possibility to have a new `init_rpc` API that takes a Store instead of `rank`, `world_size`, `init_method` etc. Can this be one step further for solving this issue? E.g., we let `ProcessGroup` expose an API to return its store, and then init RPC using that Store. \n\nThere are different options to implement this behavior.\n\nOption 1: As the c10d gang is stable with fixed ranks, the derived RPC gang can also stay that way and share the same rank/id with the ProcessGroup instance. \nOption 2: Let the RPC stay decoupled from the c10d ProcessGroup, and still allows dynamic join/leave. In this case, we cannot match the rank/id between ProcessGroup and RPC agents any more. \n\n"
   },
   {
      "null": 210,
      "x": "whitelist keyword to quantization.prepare is implemented incorrectly",
      "z": "the default argument is changed to None in https://github.com/pytorch/pytorch/pull/42576",
      "y": "the default argument is changed to None in https://github.com/pytorch/pytorch/pull/42576"
   },
   {
      "null": 211,
      "x": "Feature request: torch.isclose should set default atol and rtol based on the dtype of the tensors it's given",
      "z": "Basically if you do any arithmetic on `float32`, it will rarely hit that `1e-8` atol tolerance, so the defaults are almost useless for the default type (unlike `np.isclose` where the default type is usually float64). ",
      "y": "Basically if you do any arithmetic on `float32`, it will rarely hit that `1e-8` atol tolerance, so the defaults are almost useless for the default type (unlike `np.isclose` where the default type is usually float64). "
   },
   {
      "null": 212,
      "x": "torch.nn.functional.grid_sample segfaults on large inputs",
      "z": "@erdmann Thank you for your perspective, it's fascinating! \nIt's true that as resolution goes up, images become bigger. But in practical terms \n1) GPU memory is usually not big enough to hold multiple large tensors \n2) Even if grid_sample was fixed to properly handle 64-bit indices on the GPU, typically you'd want to call a convolution next, and it won't work because of cudnn limitation. \nThat said, binary size increase from fixing grid_sample will probably be pretty small, so if we can, we should just do it. ",
      "y": "@erdmann Thank you for your perspective, it's fascinating! \nIt's true that as resolution goes up, images become bigger. But in practical terms \n1) GPU memory is usually not big enough to hold multiple large tensors \n2) Even if grid_sample was fixed to properly handle 64-bit indices on the GPU, typically you'd want to call a convolution next, and it won't work because of cudnn limitation. \nThat said, binary size increase from fixing grid_sample will probably be pretty small, so if we can, we should just do it. "
   },
   {
      "null": 213,
      "x": "CUDA not found when using latest pre-built version (Libtorch 1.5.1  - CUDA 10.1 - Cudnn7.6.4 - VS2017)",
      "z": "Please add the linker option `-INCLUDE:?warp_size@cuda@at@@YAHXZ`.",
      "y": "Please add the linker option `-INCLUDE:?warp_size@cuda@at@@YAHXZ`."
   },
   {
      "null": 214,
      "x": "Parameter `timeout` in torch.distributed.init_process_group cannot work",
      "z": "@worsecoder the timeout does not include initial socket connection establishment. After initializing process group, the time out will be applied to NCCL collective APIs such as all reduce as etc. \n\nFor your need, do you want to try to use torchelastic for failure handling?",
      "y": "@worsecoder the timeout does not include initial socket connection establishment. After initializing process group, the time out will be applied to NCCL collective APIs such as all reduce as etc. \n\nFor your need, do you want to try to use torchelastic for failure handling?"
   },
   {
      "null": 215,
      "x": "Provide a convenient way for the user to reset the grad",
      "z": "Having both zero_grad() and reset_grad() might be kind of confusing to the end users. \n\nAre zero-ing all gradients always equivalent to setting all gradients to None? Wonder if there's any case where zero_grad() is preferred over reset_grad().\n\nIf the two are not equivalent, and have pros and cons, we probably should mention those pros and cons in the documentations to articulate the difference in order to prevent confusion. We should probably have some default recommendation (e.g. do we think reset_grad() will be better in most cases, which looks like the case, because most references I found are just calling .backward() right after calling zero_grad()).",
      "y": "Having both zero_grad() and reset_grad() might be kind of confusing to the end users. \n\nAre zero-ing all gradients always equivalent to setting all gradients to None? Wonder if there's any case where zero_grad() is preferred over reset_grad().\n\nIf the two are not equivalent, and have pros and cons, we probably should mention those pros and cons in the documentations to articulate the difference in order to prevent confusion. We should probably have some default recommendation (e.g. do we think reset_grad() will be better in most cases, which looks like the case, because most references I found are just calling .backward() right after calling zero_grad())."
   },
   {
      "null": 216,
      "x": "Replace blacklist/whitelist in caffe2/opt/tvm_transformer.cc",
      "z": "Simply replacing the names does not help. Assistance is needed.",
      "y": "Simply replacing the names does not help. Assistance is needed."
   },
   {
      "null": 217,
      "x": "Behavior of torch.mean (and std) compared to numpy mean (std)",
      "z": "A WIP implementation can be found in https://github.com/pytorch/pytorch/pull/2116\nBut it needs someone to complete it.",
      "y": "A WIP implementation can be found in https://github.com/pytorch/pytorch/pull/2116\nBut it needs someone to complete it."
   },
   {
      "null": 218,
      "x": "Easy way of creating your own custom cuda kernels",
      "z": "we also provide examples of easily interfacing with CUDA code in https://github.com/pytorch/extension-ffi where you dont have to write additional bindings",
      "y": "we also provide examples of easily interfacing with CUDA code in https://github.com/pytorch/extension-ffi where you dont have to write additional bindings"
   },
   {
      "null": 219,
      "x": "HalfTensor Training Needs non-Stateless Method in F.Linear",
      "z": "If it has an mm method then we should enable `torch.mm` for it too. It should be a one line change in cwrap.",
      "y": "If it has an mm method then we should enable `torch.mm` for it too. It should be a one line change in cwrap."
   },
   {
      "null": 220,
      "x": "Just one cpu core in use, until I use numpy...",
      "z": "Sometimes mkl caps the number of threads below your system's max. See mkl's cap via `mkl.get_max_threads()`. To increase the cap to your actual max, use `export MKL_DYNAMIC=FALSE` in bash before running Python. Find your actual max via `echo $(nproc)` in bash.\n\nIntel's documentation:\nhttps://software.intel.com/en-us/mkl-linux-developer-guide-mkl-dynamic\n\nI came across this solution in this issue:\nhttps://github.com/ContinuumIO/mkl-service/issues/2\n",
      "y": "Sometimes mkl caps the number of threads below your system's max. See mkl's cap via `mkl.get_max_threads()`. To increase the cap to your actual max, use `export MKL_DYNAMIC=FALSE` in bash before running Python. Find your actual max via `echo $(nproc)` in bash.\n\nIntel's documentation:\nhttps://software.intel.com/en-us/mkl-linux-developer-guide-mkl-dynamic\n\nI came across this solution in this issue:\nhttps://github.com/ContinuumIO/mkl-service/issues/2\n"
   },
   {
      "null": 221,
      "x": "Support Caffe2 export/pure C(++) inference mode",
      "z": "If I were to suggest something, I think I'd convert all the weights to numpy and dump them in a well established format like HDF5. These should be easily readable from C.\n\nOur main idea is to add a model exporter, that could dump it to a Caffe2 graph, as that framework is very well optimized for production usage. And even though we don't plan to put a great amount of work on packaging production-ready models trained in PyTorch right now, there are some projects in the community that are aiming to do that, and we're going to provide them with support. You can find them in the forum link that @fmassa posted.",
      "y": "If I were to suggest something, I think I'd convert all the weights to numpy and dump them in a well established format like HDF5. These should be easily readable from C.\n\nOur main idea is to add a model exporter, that could dump it to a Caffe2 graph, as that framework is very well optimized for production usage. And even though we don't plan to put a great amount of work on packaging production-ready models trained in PyTorch right now, there are some projects in the community that are aiming to do that, and we're going to provide them with support. You can find them in the forum link that @fmassa posted."
   },
   {
      "null": 222,
      "x": "GPU torch.multinomial produces an out-of-bounds index",
      "z": "In case this is helpful to anyone, a possible temporary workaround is to use\n```\n_, sample = torch.max(log_dist - torch.log(-torch.log(torch.rand(*log_dist.size()).cuda())), 1)\n```\nwhere `log_dist` is batchwise log probabilities (e.g. output of `F.log_softmax`).",
      "y": "In case this is helpful to anyone, a possible temporary workaround is to use\n```\n_, sample = torch.max(log_dist - torch.log(-torch.log(torch.rand(*log_dist.size()).cuda())), 1)\n```\nwhere `log_dist` is batchwise log probabilities (e.g. output of `F.log_softmax`)."
   },
   {
      "null": 223,
      "x": "Flexible state dict loading for model (or optimizer)",
      "z": "If you have partial state_dict, which is missing some keys you can do the following:\n\n```python\nstate = model.state_dict()\nstate.update(partial)\nmodel.load_state_dict(state)\n```",
      "y": "If you have partial state_dict, which is missing some keys you can do the following:\n\n```python\nstate = model.state_dict()\nstate.update(partial)\nmodel.load_state_dict(state)\n```"
   },
   {
      "null": 224,
      "x": "Support int16 numpy conversions",
      "z": "The error message is quite self explanatory. PyTorch doesn't support `int8` tensors at the moment.",
      "y": "The error message is quite self explanatory. PyTorch doesn't support `int8` tensors at the moment."
   },
   {
      "null": 225,
      "x": "The command '/bin/sh -c' returned a non-zero code: 2 during docker image",
      "z": "As you can see [here](https://github.com/pytorch/pytorch/blob/f2d72ba10fabe6f78e67246031c8b1da48e7ddf1/Dockerfile#L13-L19) the WHOLE command following `RUN` has to be a single line. You should add `\\` when necessary to avoid problem.\nYour sample:\n```\nRUN\nchmod +x ~/miniconda.sh && \n~/miniconda.sh -b -p /opt/conda && \\\nrm ~/miniconda.sh && \n/opt/conda/bin/conda install conda-build && \n/opt/conda/bin/conda create -y --name pytorch-py35 python=3.5.2 numpy scipy ipython mkl&& \n/opt/conda/bin/conda clean -ya\n```\nshould be\n```\nRUN chmod +x ~/miniconda.sh && \\\n~/miniconda.sh -b -p /opt/conda && \\\nrm ~/miniconda.sh && \\\n/opt/conda/bin/conda install conda-build && \\\n/opt/conda/bin/conda create -y --name pytorch-py35 python=3.5.2 numpy scipy ipython mkl && \\\n/opt/conda/bin/conda clean -ya\n```",
      "y": "As you can see [here](https://github.com/pytorch/pytorch/blob/f2d72ba10fabe6f78e67246031c8b1da48e7ddf1/Dockerfile#L13-L19) the WHOLE command following `RUN` has to be a single line. You should add `\\` when necessary to avoid problem.\nYour sample:\n```\nRUN\nchmod +x ~/miniconda.sh && \n~/miniconda.sh -b -p /opt/conda && \\\nrm ~/miniconda.sh && \n/opt/conda/bin/conda install conda-build && \n/opt/conda/bin/conda create -y --name pytorch-py35 python=3.5.2 numpy scipy ipython mkl&& \n/opt/conda/bin/conda clean -ya\n```\nshould be\n```\nRUN chmod +x ~/miniconda.sh && \\\n~/miniconda.sh -b -p /opt/conda && \\\nrm ~/miniconda.sh && \\\n/opt/conda/bin/conda install conda-build && \\\n/opt/conda/bin/conda create -y --name pytorch-py35 python=3.5.2 numpy scipy ipython mkl && \\\n/opt/conda/bin/conda clean -ya\n```"
   },
   {
      "null": 226,
      "x": "old lua torch model to pytorch, nn.JoinTable(1,3) error",
      "z": "Because the forward looks like that `def forward(self, input):` so it expects two arguments, but it gets self, Z and Y from you. `nn.Sequential` only accepts a single input.",
      "y": "Because the forward looks like that `def forward(self, input):` so it expects two arguments, but it gets self, Z and Y from you. `nn.Sequential` only accepts a single input."
   },
   {
      "null": 227,
      "x": "torch ModuleNotFoundError in ipython",
      "z": "1. try to uninstall your installed pytorch.\n2. conda install -c peterjc123 pytorch-cpu\n3. on your conda type python\n4. import torch ",
      "y": "1. try to uninstall your installed pytorch.\n2. conda install -c peterjc123 pytorch-cpu\n3. on your conda type python\n4. import torch "
   },
   {
      "null": 228,
      "x": "Device memory not released",
      "z": "I've seen this before. If the main process segfaults, then the background data loading processes might still be running and holding the cuda context.\n\nRun:\n`killall python` (or `killall python3`)",
      "y": "I've seen this before. If the main process segfaults, then the background data loading processes might still be running and holding the cuda context.\n\nRun:\n`killall python` (or `killall python3`)"
   },
   {
      "null": 229,
      "x": "Manually unrolling cuDNN RNN OOM",
      "z": "First one is not using cudnn, second one is. Both are unrolled in the same way. You can switch between them by commenting/uncommenting the following line in the script:\n```\ntorch.backends.cudnn.enabled=False\n```",
      "y": "First one is not using cudnn, second one is. Both are unrolled in the same way. You can switch between them by commenting/uncommenting the following line in the script:\n```\ntorch.backends.cudnn.enabled=False\n```"
   },
   {
      "null": 230,
      "x": "padding for nn.AvgPool3d?",
      "z": "there isn't a particular reason, we haven't implemented it yet in our C backend.",
      "y": "there isn't a particular reason, we haven't implemented it yet in our C backend."
   },
   {
      "null": 231,
      "x": "Compiling error of gloo when installing pytorch from source",
      "z": "I just added `#define SPEED_UNKNOWN -1` to the file `torch/lib/gloo/gloo/common/linux.cc`.  It appears that this is suppose to be defined by one of the headers from the linux kernel, but I am guessing isn't defined for older versions.",
      "y": "I just added `#define SPEED_UNKNOWN -1` to the file `torch/lib/gloo/gloo/common/linux.cc`.  It appears that this is suppose to be defined by one of the headers from the linux kernel, but I am guessing isn't defined for older versions."
   },
   {
      "null": 232,
      "x": "RuntimeError: DataLoader worker (pid 23616) is killed by signal: Terminated.",
      "z": "I run into the same problem as @shirishr. It was simply because of not enough memory, you may add more memory / swap space to solve the problem. ",
      "y": "I run into the same problem as @shirishr. It was simply because of not enough memory, you may add more memory / swap space to solve the problem. "
   },
   {
      "null": 233,
      "x": "from torch._C import *  (ImportError: DLL load failed: The specified module could not be found.",
      "z": "I had the same issue and it was caused by the directory torch which is generated in the same directory by compiling the source. The solution for me was simply changing the directory before open python.\n  ",
      "y": "I had the same issue and it was caused by the directory torch which is generated in the same directory by compiling the source. The solution for me was simply changing the directory before open python.\n  "
   },
   {
      "null": 234,
      "x": "nn.BatchNorm1d fails with batch size 1 on the new PyTorch 0.3",
      "z": "Like the error message says, you can't use feature-wise batch normalization if you only have 1 element per-feature.\n\nhttps://arxiv.org/abs/1502.03167",
      "y": "Like the error message says, you can't use feature-wise batch normalization if you only have 1 element per-feature.\n\nhttps://arxiv.org/abs/1502.03167"
   },
   {
      "null": 235,
      "x": "NVIDIA driver too old error",
      "z": "I solved it by downgrading the pytorch. That is less cumbersome compared to updating the driver. \nI followed `https://pytorch.org/get-started/previous-versions/` for compatible pytorch version with cuda toolkit",
      "y": "I solved it by downgrading the pytorch. That is less cumbersome compared to updating the driver. \nI followed `https://pytorch.org/get-started/previous-versions/` for compatible pytorch version with cuda toolkit"
   },
   {
      "null": 236,
      "x": "Feature request: torch.bincount",
      "z": "@sunshineatnoon, how about:\n```python\n>>> import torch\n>>> help(torch.bincount)\n```\n```rst\nHelp on built-in function bincount:\n\nbincount(...)\n    bincount(self, weights=None, minlength=0) -> Tensor\n\n    Count the frequency of each value in an array of non-negative ints.\n\n    The number of bins (size 1) is one larger than the largest value in\n    :attr:`input`. If :attr:`minlength` is specified, the number of bins is at least\n    :attr:`minlength`. If ``n`` is the value at position ``i``,\n    :math:`out[n] += weights[i]` if :attr:`weights` is specified else\n    :math:`out[n] += 1`.\n\n    Arguments:\n        input (Tensor): 1-d int tensor\n        weights (Tensor): optional, weight for each value in the input tensor.\n            Should be of same size as input tensor.\n        minlength (int): optional, min number of bins. Should be non-negative.\n\n    Shape:\n        output (Tensor): ``Size([max(input) + 1])``\n\n    Example::\n\n        >>> input = torch.randint(0, 8, (5,), dtype=torch.int64)\n        >>> weights = torch.linspace(0, 1, steps=5)\n        >>> input, weights\n        (tensor([4, 3, 6, 3, 4]),\n         tensor([ 0.0000,  0.2500,  0.5000,  0.7500,  1.0000])\n\n        >>> torch.bincount(input)\n        tensor([0, 0, 0, 2, 2, 0, 1])\n\n        >>> input.bincount(weights)\n        tensor([0.0000, 0.0000, 0.0000, 1.0000, 1.0000, 0.0000, 0.5000])\n```",
      "y": "@sunshineatnoon, how about:\n```python\n>>> import torch\n>>> help(torch.bincount)\n```\n```rst\nHelp on built-in function bincount:\n\nbincount(...)\n    bincount(self, weights=None, minlength=0) -> Tensor\n\n    Count the frequency of each value in an array of non-negative ints.\n\n    The number of bins (size 1) is one larger than the largest value in\n    :attr:`input`. If :attr:`minlength` is specified, the number of bins is at least\n    :attr:`minlength`. If ``n`` is the value at position ``i``,\n    :math:`out[n] += weights[i]` if :attr:`weights` is specified else\n    :math:`out[n] += 1`.\n\n    Arguments:\n        input (Tensor): 1-d int tensor\n        weights (Tensor): optional, weight for each value in the input tensor.\n            Should be of same size as input tensor.\n        minlength (int): optional, min number of bins. Should be non-negative.\n\n    Shape:\n        output (Tensor): ``Size([max(input) + 1])``\n\n    Example::\n\n        >>> input = torch.randint(0, 8, (5,), dtype=torch.int64)\n        >>> weights = torch.linspace(0, 1, steps=5)\n        >>> input, weights\n        (tensor([4, 3, 6, 3, 4]),\n         tensor([ 0.0000,  0.2500,  0.5000,  0.7500,  1.0000])\n\n        >>> torch.bincount(input)\n        tensor([0, 0, 0, 2, 2, 0, 1])\n\n        >>> input.bincount(weights)\n        tensor([0.0000, 0.0000, 0.0000, 1.0000, 1.0000, 0.0000, 0.5000])\n```"
   },
   {
      "null": 237,
      "x": "[feature request] Weight norm option for RNN cells",
      "z": "You can already achieve it now. For instance,\n```\n>>> rnn = nn.RNN(10,10,2)  # build an RNN\n>>> nn.utils.weight_norm(rnn, 'weight_hh_l0')  # apply weight_norm to any particular weight you want\n```",
      "y": "You can already achieve it now. For instance,\n```\n>>> rnn = nn.RNN(10,10,2)  # build an RNN\n>>> nn.utils.weight_norm(rnn, 'weight_hh_l0')  # apply weight_norm to any particular weight you want\n```"
   },
   {
      "null": 238,
      "x": "[Feature request] PackedSequence with length = 0",
      "z": "I don't think that's a good idea. If you know it can happen in your use case just filter out the empty sequences before you call this function. It would make it more convenient for you, but may silently accept inputs that are incorrect for other uses.",
      "y": "I don't think that's a good idea. If you know it can happen in your use case just filter out the empty sequences before you call this function. It would make it more convenient for you, but may silently accept inputs that are incorrect for other uses."
   },
   {
      "null": 239,
      "x": "Cannot install pytorch cuda 8.0 using conda",
      "z": "do you already have the cuda90 feature installed, for some reason...\n\nTry first doing:\n\n`conda uninstall cuda90`",
      "y": "do you already have the cuda90 feature installed, for some reason...\n\nTry first doing:\n\n`conda uninstall cuda90`"
   },
   {
      "null": 240,
      "x": "Warning on infinite acos gradients?",
      "z": "A lot of our ops work this way, and adding these checks would cause massive slowdowns. Thanks for the suggestion, but we can't afford that.",
      "y": "A lot of our ops work this way, and adding these checks would cause massive slowdowns. Thanks for the suggestion, but we can't afford that."
   },
   {
      "null": 241,
      "x": "Autogradpp issue masterthread",
      "z": "Regarding `tensor.max()` I think a better way would be to auto-generate \"named-tuples\" in C++ (i.e. you can still do `std::get<0>(tensor.max())` OR `tensor.max().values` (we'd generate a struct with two fields that also supports the `get` interface)",
      "y": "Regarding `tensor.max()` I think a better way would be to auto-generate \"named-tuples\" in C++ (i.e. you can still do `std::get<0>(tensor.max())` OR `tensor.max().values` (we'd generate a struct with two fields that also supports the `get` interface)"
   },
   {
      "null": 242,
      "x": "`torch.normal` accepts Variables but does not propagate gradients",
      "z": "if you are on master, you can do `x = torch.distributions.Normal(mu, sigma).rsample()`",
      "y": "if you are on master, you can do `x = torch.distributions.Normal(mu, sigma).rsample()`"
   },
   {
      "null": 243,
      "x": "`torch.jit.freeze`'d models cannot be moved to GPU with `.to()`",
      "z": "@Linux-cpp-lisp this is currently expected behavior. We need to be able to inline the attributes as constants in order to do anything useful in optimizing them. There is also nothing preventing the user from having device-specific logic we also bake in.\n\n```\n    def forward(self, x):\n       if self.twos.device.is_cuda():\n              ....\n```\nModels might also contain some CPU & some GPU compute. However, as you've shown, there are many models where it is completely valid to remap devices after freezing. \n\nCan I ask what the specific use case is ?\n\n\n",
      "y": "@Linux-cpp-lisp this is currently expected behavior. We need to be able to inline the attributes as constants in order to do anything useful in optimizing them. There is also nothing preventing the user from having device-specific logic we also bake in.\n\n```\n    def forward(self, x):\n       if self.twos.device.is_cuda():\n              ....\n```\nModels might also contain some CPU & some GPU compute. However, as you've shown, there are many models where it is completely valid to remap devices after freezing. \n\nCan I ask what the specific use case is ?\n\n\n"
   },
   {
      "null": 244,
      "x": "gradgradcheck fails if the function does not depend on the input",
      "z": "Still fails.\n\n    Traceback (most recent call last):\n      File \"<stdin>\", line 1, in <module>\n      File \"/home/mfkasim/anaconda3/envs/torchdev/lib/python3.8/site-packages/torch/autograd/gradcheck.py\", line 1212, in gradgradcheck\n        return gradcheck(\n      File \"/home/mfkasim/anaconda3/envs/torchdev/lib/python3.8/site-packages/torch/autograd/gradcheck.py\", line 1088, in gradcheck\n        return _gradcheck_helper(**args)\n      File \"/home/mfkasim/anaconda3/envs/torchdev/lib/python3.8/site-packages/torch/autograd/gradcheck.py\", line 1096, in _gradcheck_helper\n        func_out = func(*tupled_inputs)\n      File \"/home/mfkasim/anaconda3/envs/torchdev/lib/python3.8/site-packages/torch/autograd/gradcheck.py\", line 1209, in new_func\n        grad_inputs = torch.autograd.grad(outputs, input_args, grad_outputs, create_graph=True)\n      File \"/home/mfkasim/anaconda3/envs/torchdev/lib/python3.8/site-packages/torch/autograd/__init__.py\", line 226, in grad\n        return Variable._execution_engine.run_backward(\n    RuntimeError: One of the differentiated Tensors appears to not have been used in the graph. Set allow_unused=True if this is the desired behavior.\n\nThe problem is this line:\nhttps://github.com/pytorch/pytorch/blob/807bea1c4e6fdf896570f1fac83897cf42231f49/torch/autograd/gradcheck.py#L1209\n\nProbably the fix is as simple as put `allow_unused=True`",
      "y": "Still fails.\n\n    Traceback (most recent call last):\n      File \"<stdin>\", line 1, in <module>\n      File \"/home/mfkasim/anaconda3/envs/torchdev/lib/python3.8/site-packages/torch/autograd/gradcheck.py\", line 1212, in gradgradcheck\n        return gradcheck(\n      File \"/home/mfkasim/anaconda3/envs/torchdev/lib/python3.8/site-packages/torch/autograd/gradcheck.py\", line 1088, in gradcheck\n        return _gradcheck_helper(**args)\n      File \"/home/mfkasim/anaconda3/envs/torchdev/lib/python3.8/site-packages/torch/autograd/gradcheck.py\", line 1096, in _gradcheck_helper\n        func_out = func(*tupled_inputs)\n      File \"/home/mfkasim/anaconda3/envs/torchdev/lib/python3.8/site-packages/torch/autograd/gradcheck.py\", line 1209, in new_func\n        grad_inputs = torch.autograd.grad(outputs, input_args, grad_outputs, create_graph=True)\n      File \"/home/mfkasim/anaconda3/envs/torchdev/lib/python3.8/site-packages/torch/autograd/__init__.py\", line 226, in grad\n        return Variable._execution_engine.run_backward(\n    RuntimeError: One of the differentiated Tensors appears to not have been used in the graph. Set allow_unused=True if this is the desired behavior.\n\nThe problem is this line:\nhttps://github.com/pytorch/pytorch/blob/807bea1c4e6fdf896570f1fac83897cf42231f49/torch/autograd/gradcheck.py#L1209\n\nProbably the fix is as simple as put `allow_unused=True`"
   },
   {
      "null": 245,
      "x": "With & without -O2 compilation optimization level, AVX512 Complex multiplication & division results aren't equal",
      "z": "@quickwritereader, this is so embarrassing!!! I had wrongly assumed that `CPU_CAPABILITY_DEFAULT` is always defined, unless a user uses the environment variable `ATEN_CPU_CAPABILITY` to set it to a particular value! That's why I hadn't bothered to change that if clause because it seemed redundant to do so.\n\nSo, the issue was just that the testing for AVX512 was being done against the default implementation? \ud83e\udd23 \nThanks to you, all tests pass now! \ud83d\ude04 ",
      "y": "@quickwritereader, this is so embarrassing!!! I had wrongly assumed that `CPU_CAPABILITY_DEFAULT` is always defined, unless a user uses the environment variable `ATEN_CPU_CAPABILITY` to set it to a particular value! That's why I hadn't bothered to change that if clause because it seemed redundant to do so.\n\nSo, the issue was just that the testing for AVX512 was being done against the default implementation? \ud83e\udd23 \nThanks to you, all tests pass now! \ud83d\ude04 "
   },
   {
      "null": 246,
      "x": "Linking statically with CUPTI using gold linker disrupts exception handling",
      "z": "The CMake file change at https://github.com/pytorch/pytorch/commit/c71459602785bcc9f0f93a880c642ff61299d3d5#diff-12e8125164bbfc7556b1781a8ed516e333cc0bf058acb7197f7415be44606c72R1859-R1863 now actually honors `USE_CUPTI_SO`. This was previously not the case which meant building PyTorch \"correctly\" i.e. with dynamic cupti, was impossible and the issue unavoidable (we patched that for our builds)\n\nTo me linking to static cupti is the bug which is now fixed: Define USE_CUPTI_SO=1 and all works (testing this right now)\n\nBut yes the (likely) faulty cupti static library is not fixed by this but I think this is rather a cupti bug than a pytorch one. Not sure here, so please reopen if you still want to track this bug here.\nJust from my side I'm happy if USE_CUPTI_SO is now working.",
      "y": "The CMake file change at https://github.com/pytorch/pytorch/commit/c71459602785bcc9f0f93a880c642ff61299d3d5#diff-12e8125164bbfc7556b1781a8ed516e333cc0bf058acb7197f7415be44606c72R1859-R1863 now actually honors `USE_CUPTI_SO`. This was previously not the case which meant building PyTorch \"correctly\" i.e. with dynamic cupti, was impossible and the issue unavoidable (we patched that for our builds)\n\nTo me linking to static cupti is the bug which is now fixed: Define USE_CUPTI_SO=1 and all works (testing this right now)\n\nBut yes the (likely) faulty cupti static library is not fixed by this but I think this is rather a cupti bug than a pytorch one. Not sure here, so please reopen if you still want to track this bug here.\nJust from my side I'm happy if USE_CUPTI_SO is now working."
   },
   {
      "null": 247,
      "x": "Retiring ONNX Optimizer",
      "z": "Discussed this issue during triage review: let's disable optimizer support as it is no longer supported.",
      "y": "Discussed this issue during triage review: let's disable optimizer support as it is no longer supported."
   },
   {
      "null": 248,
      "x": "Cannot pass script remote module object to over the RPC",
      "z": "@SciPioneer I wonder if we can employ a method similar to https://github.com/pytorch/pytorch/blob/master/torch/distributed/rpc/internal.py#L110-L112, although I wonder why it isn't already supported out of the box. I think `RecursiveScriptModule` is an instance of `torch.jit.ScriptModule` so not sure why this codepath isn't hit. ",
      "y": "@SciPioneer I wonder if we can employ a method similar to https://github.com/pytorch/pytorch/blob/master/torch/distributed/rpc/internal.py#L110-L112, although I wonder why it isn't already supported out of the box. I think `RecursiveScriptModule` is an instance of `torch.jit.ScriptModule` so not sure why this codepath isn't hit. "
   },
   {
      "null": 249,
      "x": "torch.cat should not do type promotion when one input is empty tensor",
      "z": "NumPy type promotes in this case and I don't think it makes sense to complicate the type promotion rules even further:\n```\n>>> np.concatenate([np.zeros(5, dtype=np.long), np.array([])]).dtype\ndtype('float64')\n```",
      "y": "NumPy type promotes in this case and I don't think it makes sense to complicate the type promotion rules even further:\n```\n>>> np.concatenate([np.zeros(5, dtype=np.long), np.array([])]).dtype\ndtype('float64')\n```"
   },
   {
      "null": 250,
      "x": "A bug on the document interpreter in `torch.autograd.profiler` help page",
      "z": "Fixed on master https://pytorch.org/docs/master/generated/torch.autograd.profiler.profile.key_averages.html?highlight=key_averages#torch.autograd.profiler.profile.key_averages",
      "y": "Fixed on master https://pytorch.org/docs/master/generated/torch.autograd.profiler.profile.key_averages.html?highlight=key_averages#torch.autograd.profiler.profile.key_averages"
   },
   {
      "null": 251,
      "x": "`nan_to_num` produces incorrect output for `BFloat16` on CUDA",
      "z": "Ok, so it looks like `nan_to_num` is actually doing the correct thing, and it's the compare with reference that goes wrong. That's good to know. ",
      "y": "Ok, so it looks like `nan_to_num` is actually doing the correct thing, and it's the compare with reference that goes wrong. That's good to know. "
   },
   {
      "null": 252,
      "x": "torch.cat fails with torch.jit.script and torch.cuda.amp.autocast",
      "z": "At a wild guess: this patch should fix it:\n\n```\ndiff --git a/aten/src/ATen/autocast_mode.cpp b/aten/src/ATen/autocast_mode.cpp\nindex 7d85211e9c..7809b7d323 100644\n--- a/aten/src/ATen/autocast_mode.cpp\n+++ b/aten/src/ATen/autocast_mode.cpp\n@@ -487,9 +487,9 @@ TORCH_LIBRARY_IMPL(aten, Autocast, m) {\n   KERNEL_UNBOXED_ONLY(ADD_NS(tensordot), \"tensordot\", Tensor (const Tensor &, const Tensor &, IntArrayRef, IntArrayRef), promote)\n   KERNEL_UNBOXED_ONLY(ADD_NS(dot), \"dot\", Tensor (const Tensor &, const Tensor &), promote)\n   KERNEL(ADD_NS(equal), \"equal\", bool (const Tensor &, const Tensor &), promote)\n-  KERNEL_UNBOXED_ONLY(ADD_NS(cat), \"cat\", Tensor (TensorList, int64_t), promote)\n-  KERNEL_UNBOXED_ONLY(ADD_NS(cat), \"cat.names\", Tensor (TensorList, Dimname), promote)\n-  KERNEL_UNBOXED_ONLY(ADD_NS(_cat), \"_cat\", Tensor (TensorList, int64_t), promote)\n+  KERNEL(ADD_NS(cat), \"cat\", Tensor (TensorList, int64_t), promote)\n+  KERNEL(ADD_NS(cat), \"cat.names\", Tensor (TensorList, Dimname), promote)\n+  KERNEL(ADD_NS(_cat), \"_cat\", Tensor (TensorList, int64_t), promote)\n   KERNEL_UNBOXED_ONLY(ADD_NS(stack), \"stack\", Tensor (TensorList, int64_t), promote)\n \n   m.impl_UNBOXED(\"binary_cross_entropy\", &at::autocast::binary_cross_entropy_banned);\n```\n\n@smessmer not sure if we should just rush \"get rid of unboxed only\" or start adding some checks to reject incorrect invocations of unboxed only.",
      "y": "At a wild guess: this patch should fix it:\n\n```\ndiff --git a/aten/src/ATen/autocast_mode.cpp b/aten/src/ATen/autocast_mode.cpp\nindex 7d85211e9c..7809b7d323 100644\n--- a/aten/src/ATen/autocast_mode.cpp\n+++ b/aten/src/ATen/autocast_mode.cpp\n@@ -487,9 +487,9 @@ TORCH_LIBRARY_IMPL(aten, Autocast, m) {\n   KERNEL_UNBOXED_ONLY(ADD_NS(tensordot), \"tensordot\", Tensor (const Tensor &, const Tensor &, IntArrayRef, IntArrayRef), promote)\n   KERNEL_UNBOXED_ONLY(ADD_NS(dot), \"dot\", Tensor (const Tensor &, const Tensor &), promote)\n   KERNEL(ADD_NS(equal), \"equal\", bool (const Tensor &, const Tensor &), promote)\n-  KERNEL_UNBOXED_ONLY(ADD_NS(cat), \"cat\", Tensor (TensorList, int64_t), promote)\n-  KERNEL_UNBOXED_ONLY(ADD_NS(cat), \"cat.names\", Tensor (TensorList, Dimname), promote)\n-  KERNEL_UNBOXED_ONLY(ADD_NS(_cat), \"_cat\", Tensor (TensorList, int64_t), promote)\n+  KERNEL(ADD_NS(cat), \"cat\", Tensor (TensorList, int64_t), promote)\n+  KERNEL(ADD_NS(cat), \"cat.names\", Tensor (TensorList, Dimname), promote)\n+  KERNEL(ADD_NS(_cat), \"_cat\", Tensor (TensorList, int64_t), promote)\n   KERNEL_UNBOXED_ONLY(ADD_NS(stack), \"stack\", Tensor (TensorList, int64_t), promote)\n \n   m.impl_UNBOXED(\"binary_cross_entropy\", &at::autocast::binary_cross_entropy_banned);\n```\n\n@smessmer not sure if we should just rush \"get rid of unboxed only\" or start adding some checks to reject incorrect invocations of unboxed only."
   },
   {
      "null": 253,
      "x": "[JIT] lack of type support in tensor indexing.",
      "z": "Hi @chenbohua3, thanks for the detailed issue and succinct repro! This was recently added in https://github.com/pytorch/pytorch/pull/38378 and works on master, so I'm closing. ",
      "y": "Hi @chenbohua3, thanks for the detailed issue and succinct repro! This was recently added in https://github.com/pytorch/pytorch/pull/38378 and works on master, so I'm closing. "
   },
   {
      "null": 254,
      "x": "One PyTorch Upsample op balloons into over 20 ONNX operations",
      "z": "This is expected since by default, interpolate recomputes the given scale_factor using input size. If you try torch.nn.Functional with recompute_scale_factor = False, you'll see a single Resize node in the graph.\nThe default behavior of interpolate is going to change to set recompute_scale_factor = False by default,  as part of the PR: https://github.com/pytorch/pytorch/pull/38362",
      "y": "This is expected since by default, interpolate recomputes the given scale_factor using input size. If you try torch.nn.Functional with recompute_scale_factor = False, you'll see a single Resize node in the graph.\nThe default behavior of interpolate is going to change to set recompute_scale_factor = False by default,  as part of the PR: https://github.com/pytorch/pytorch/pull/38362"
   },
   {
      "null": 255,
      "x": "Unrecognized attribute: min for operator Clip",
      "z": "seems fixed and can't repro with `pytorch 1.6`",
      "y": "seems fixed and can't repro with `pytorch 1.6`"
   },
   {
      "null": 256,
      "x": "``ToTensor()`` Exception for ``num_workers`` in ``DataLoader`` when ``torch.set_default_tensor_type(torch.cuda.FloatTensor)``",
      "z": "the root cause is that `fork` inherits the current process state, including default tensor type, while `spawn` doesn't. this is a fundamental difference between the two start methods, and not really pytorch/torchvision specific. ",
      "y": "the root cause is that `fork` inherits the current process state, including default tensor type, while `spawn` doesn't. this is a fundamental difference between the two start methods, and not really pytorch/torchvision specific. "
   },
   {
      "null": 257,
      "x": "`verbose` unused in `torch.backends.cudnn`",
      "z": "I can fix it by removing this parameter.",
      "y": "I can fix it by removing this parameter."
   },
   {
      "null": 258,
      "x": "RuntimeError: one of the variables needed for gradient computation has been modified by an inplace operation?",
      "z": "Hi,\n\nThis happens because the `opt_D.step()` modifies the parameters of your discriminator inplace. But these parameters are required to compute the gradient for the generator. Hence the error.\nWe fixed the inplace detection for the optimizers in 1.5, this is why it works in 1.4.\n\nYou should re-organize your code to only do the `steps()` after all the gradients have been computed or make sure you don't modify parameters that are required.\nSomething like that should work.\n```\nfor step in range(10000):\n    artist_paintings = artist_works()  # real painting from artist\n    G_ideas = torch.randn(BATCH_SIZE, N_IDEAS)  # random ideas\n    G_paintings = G(G_ideas)  # fake painting from G (random ideas)\n\n    prob_artist1 = D(G_paintings)  # G tries to fool D\n\n    G_loss = torch.mean(torch.log(1. - prob_artist1))\n    opt_G.zero_grad()\n    G_loss.backward()\n    opt_G.step()\n\n    prob_artist0 = D(artist_paintings)  # D try to increase this prob\n    # detach here to make sure we don't backprop in G that was already changed.\n    prob_artist1 = D(G_paintings.detach())  # D try to reduce this prob\n\n    D_loss = - torch.mean(torch.log(prob_artist0) + torch.log(1. - prob_artist1))\n    opt_D.zero_grad()\n    D_loss.backward(retain_graph=True)  # reusing computational graph\n    opt_D.step()\n```\n\nIn the future, I would recommend to ask these questions on the forum: https://discuss.pytorch.org/\nWe keep github issues for bug and features only. And more people look at the forum so you will get a faster answer.",
      "y": "Hi,\n\nThis happens because the `opt_D.step()` modifies the parameters of your discriminator inplace. But these parameters are required to compute the gradient for the generator. Hence the error.\nWe fixed the inplace detection for the optimizers in 1.5, this is why it works in 1.4.\n\nYou should re-organize your code to only do the `steps()` after all the gradients have been computed or make sure you don't modify parameters that are required.\nSomething like that should work.\n```\nfor step in range(10000):\n    artist_paintings = artist_works()  # real painting from artist\n    G_ideas = torch.randn(BATCH_SIZE, N_IDEAS)  # random ideas\n    G_paintings = G(G_ideas)  # fake painting from G (random ideas)\n\n    prob_artist1 = D(G_paintings)  # G tries to fool D\n\n    G_loss = torch.mean(torch.log(1. - prob_artist1))\n    opt_G.zero_grad()\n    G_loss.backward()\n    opt_G.step()\n\n    prob_artist0 = D(artist_paintings)  # D try to increase this prob\n    # detach here to make sure we don't backprop in G that was already changed.\n    prob_artist1 = D(G_paintings.detach())  # D try to reduce this prob\n\n    D_loss = - torch.mean(torch.log(prob_artist0) + torch.log(1. - prob_artist1))\n    opt_D.zero_grad()\n    D_loss.backward(retain_graph=True)  # reusing computational graph\n    opt_D.step()\n```\n\nIn the future, I would recommend to ask these questions on the forum: https://discuss.pytorch.org/\nWe keep github issues for bug and features only. And more people look at the forum so you will get a faster answer."
   },
   {
      "null": 259,
      "x": "[JIT] Expected integer literal for index:",
      "z": "@yangsenius changing the forward to look like this:\ndef forward(self, x: List[torch.Tensor]):\nfixed that error for me. Now, I am stuck on this:\n```\nExpected a default value of type Tensor on parameter \"mask\".:\n  File \"/usr/local/lib/python3.8/dist-packages/torchvision/ops/deform_conv.py\", line 152\n    def forward(self, input: Tensor, offset: Tensor, mask: Tensor = None) -> Tensor:\n    ~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n        \"\"\"\n        ~~~\n        Args:\n        ~~~~~\n            input (Tensor[batch_size, in_channels, in_height, in_width]): input tensor\n            ~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n            offset (Tensor[batch_size, 2 * offset_groups * kernel_height * kernel_width,\n            ~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n                out_height, out_width]): offsets to be applied for each position in the\n                ~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n                convolution kernel.\n                ~~~~~~~~~~~~~~~~~~~\n            mask (Tensor[batch_size, offset_groups * kernel_height * kernel_width,\n            ~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n                out_height, out_width]): masks to be applied for each position in the\n                ~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n                convolution kernel.\n                ~~~~~~~~~~~~~~~~~~~\n        \"\"\"\n        ~~~\n        return deform_conv2d(input, offset, self.weight, self.bias, stride=self.stride,\n        ~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n                             padding=self.padding, dilation=self.dilation, mask=mask)\n                             ~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~ <--- HERE\n```\n\nNote: I didn't need to change anything to trace but I got size issues.\nThis is all to fix scripting.",
      "y": "@yangsenius changing the forward to look like this:\ndef forward(self, x: List[torch.Tensor]):\nfixed that error for me. Now, I am stuck on this:\n```\nExpected a default value of type Tensor on parameter \"mask\".:\n  File \"/usr/local/lib/python3.8/dist-packages/torchvision/ops/deform_conv.py\", line 152\n    def forward(self, input: Tensor, offset: Tensor, mask: Tensor = None) -> Tensor:\n    ~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n        \"\"\"\n        ~~~\n        Args:\n        ~~~~~\n            input (Tensor[batch_size, in_channels, in_height, in_width]): input tensor\n            ~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n            offset (Tensor[batch_size, 2 * offset_groups * kernel_height * kernel_width,\n            ~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n                out_height, out_width]): offsets to be applied for each position in the\n                ~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n                convolution kernel.\n                ~~~~~~~~~~~~~~~~~~~\n            mask (Tensor[batch_size, offset_groups * kernel_height * kernel_width,\n            ~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n                out_height, out_width]): masks to be applied for each position in the\n                ~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n                convolution kernel.\n                ~~~~~~~~~~~~~~~~~~~\n        \"\"\"\n        ~~~\n        return deform_conv2d(input, offset, self.weight, self.bias, stride=self.stride,\n        ~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n                             padding=self.padding, dilation=self.dilation, mask=mask)\n                             ~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~ <--- HERE\n```\n\nNote: I didn't need to change anything to trace but I got size issues.\nThis is all to fix scripting."
   },
   {
      "null": 260,
      "x": "[FR] Add space as delimiter in TORCH_CHECK and other macros",
      "z": "Agree with @ShawnZhong, in most cases error strings already contain the necessary spaces, those that don't can be fixed at the call site, it does not make sense to silently insert spaces in the TORCH_CHECK macro itself. ",
      "y": "Agree with @ShawnZhong, in most cases error strings already contain the necessary spaces, those that don't can be fixed at the call site, it does not make sense to silently insert spaces in the TORCH_CHECK macro itself. "
   },
   {
      "null": 261,
      "x": "torch.distributed support on MacOS is missing",
      "z": "For Windows support, please check this RFC (#42095)\n\nHey @neggert, yes PyTorch + Gloo works on MacOS, but you will need to compile from source using the following steps:\n\n0. follow the readme in https://github.com/pytorch/pytorch to setup conda and dependencies\n1. then conda install libuv and pkg-config\n2. then run `time env MACOSX_DEPLOYMENT_TARGET=10.9 CC=clang CXX=clang++ BUILD_CAFFE2_OPS=0 USE_CUDA=0 USE_MKLDNN=0 USE_DISTRIBUTED=1 python setup.py develop`",
      "y": "For Windows support, please check this RFC (#42095)\n\nHey @neggert, yes PyTorch + Gloo works on MacOS, but you will need to compile from source using the following steps:\n\n0. follow the readme in https://github.com/pytorch/pytorch to setup conda and dependencies\n1. then conda install libuv and pkg-config\n2. then run `time env MACOSX_DEPLOYMENT_TARGET=10.9 CC=clang CXX=clang++ BUILD_CAFFE2_OPS=0 USE_CUDA=0 USE_MKLDNN=0 USE_DISTRIBUTED=1 python setup.py develop`"
   },
   {
      "null": 262,
      "x": "Installing PyTorch 1.1 into cloned conda env that contained PyTorch 1.0 gives \"Getting \"module 'torch._C' has no attribute 'BoolStorageBase'\" with PyTorch 1.1\"",
      "z": "This seems to be similar issue to https://github.com/pytorch/pytorch/issues/12031 . It looks like you must do `conda uninstall pytorch -y` before `conda install pytorch` or you end up in unrecoverable bad state",
      "y": "This seems to be similar issue to https://github.com/pytorch/pytorch/issues/12031 . It looks like you must do `conda uninstall pytorch -y` before `conda install pytorch` or you end up in unrecoverable bad state"
   },
   {
      "null": 263,
      "x": "DLL error on Windows 10",
      "z": "Let me conclude it for you:\n1. Install WDK https://docs.microsoft.com/en-us/windows-hardware/drivers/download-the-wdk\n2. Execute these commands:\n```powershell\ngflags /i ${YOUR EXECUTABLE} +sls # e.g. python.exe # Turn on loader snaps\ncdb ${YOUR COMMAND} # e.g. python -c \"import torch\"\n# Keep typing in `g` and `Enter` until the end. Use `q` and `Enter` to exit.\ngflags /i ${YOUR EXECUTABLE} -sls # e.g. python.exe # Turn off loader snaps\n```",
      "y": "Let me conclude it for you:\n1. Install WDK https://docs.microsoft.com/en-us/windows-hardware/drivers/download-the-wdk\n2. Execute these commands:\n```powershell\ngflags /i ${YOUR EXECUTABLE} +sls # e.g. python.exe # Turn on loader snaps\ncdb ${YOUR COMMAND} # e.g. python -c \"import torch\"\n# Keep typing in `g` and `Enter` until the end. Use `q` and `Enter` to exit.\ngflags /i ${YOUR EXECUTABLE} -sls # e.g. python.exe # Turn off loader snaps\n```"
   },
   {
      "null": 264,
      "x": "Non blocking tensor copy to GPU not working from torch 1.0",
      "z": "You are not measuring the effect of non_blocking because you are explicitly synchronizing inside your measurement.  You end up measuring some complicated interaction between copy overhead and copy time.\n\nnon_blocking=True doesn't make the copy faster. It just allows the copy_ call to return before the copy is completed. If you call `torch.cuda.synchronize()` immediately after a copy you've added back the synchronization you just tried to remove.\n\nAnyways, `non_blocking=True` works in PyTorch 1.0 (from CPU->CUDA). You can see it by adding a GPU delay before your copy:\n\n```python\nimport time\nimport torch\n\nDELAY = 100000000 \nx = torch.randn((1024, 1024), pin_memory=True)\n\ntorch.cuda.synchronize()\nstart = time.time()\ntorch.cuda._sleep(DELAY)\nx.cuda(non_blocking=True)\nend = time.time()\n\nprint('non_blocking=True', (end - start)*1000.)  # ~7 ms on my GPU\n\ntorch.cuda.synchronize()\nstart = time.time()\ntorch.cuda._sleep(DELAY)\nx.cuda(non_blocking=False)\nend = time.time()\n\n\nprint('non_blocking=False', (end - start)*1000.)  # ~77 ms on my GPU\n```",
      "y": "You are not measuring the effect of non_blocking because you are explicitly synchronizing inside your measurement.  You end up measuring some complicated interaction between copy overhead and copy time.\n\nnon_blocking=True doesn't make the copy faster. It just allows the copy_ call to return before the copy is completed. If you call `torch.cuda.synchronize()` immediately after a copy you've added back the synchronization you just tried to remove.\n\nAnyways, `non_blocking=True` works in PyTorch 1.0 (from CPU->CUDA). You can see it by adding a GPU delay before your copy:\n\n```python\nimport time\nimport torch\n\nDELAY = 100000000 \nx = torch.randn((1024, 1024), pin_memory=True)\n\ntorch.cuda.synchronize()\nstart = time.time()\ntorch.cuda._sleep(DELAY)\nx.cuda(non_blocking=True)\nend = time.time()\n\nprint('non_blocking=True', (end - start)*1000.)  # ~7 ms on my GPU\n\ntorch.cuda.synchronize()\nstart = time.time()\ntorch.cuda._sleep(DELAY)\nx.cuda(non_blocking=False)\nend = time.time()\n\n\nprint('non_blocking=False', (end - start)*1000.)  # ~77 ms on my GPU\n```"
   },
   {
      "null": 265,
      "x": "The result of  gloo all_gather error",
      "z": "@qijianan777 confirm that I can reproduce, and this is indeed a bug in `ProcessGroupGloo`. More specifically, when you do `chunk` on `dim=0`, the result tensors share the same underlying storage and are contiguous, but with different offset. So, when we do [`flat`](https://github.com/pytorch/pytorch/blob/c5845c44821bb3e9f8847544c122cca42aaeba6d/torch/lib/c10d/ProcessGroupGloo.cpp#L842) the tensors here, it will do nothing. Later, when retrieving [data pointer](https://github.com/pytorch/pytorch/blob/c5845c44821bb3e9f8847544c122cca42aaeba6d/torch/lib/c10d/ProcessGroupGloo.cpp#L110) of the tensors, it will return the same ptr value (this is the bug). As a result, both processes are reading the first 2 elements. \n\nThanks for reporting, I will add a fix for it. ",
      "y": "@qijianan777 confirm that I can reproduce, and this is indeed a bug in `ProcessGroupGloo`. More specifically, when you do `chunk` on `dim=0`, the result tensors share the same underlying storage and are contiguous, but with different offset. So, when we do [`flat`](https://github.com/pytorch/pytorch/blob/c5845c44821bb3e9f8847544c122cca42aaeba6d/torch/lib/c10d/ProcessGroupGloo.cpp#L842) the tensors here, it will do nothing. Later, when retrieving [data pointer](https://github.com/pytorch/pytorch/blob/c5845c44821bb3e9f8847544c122cca42aaeba6d/torch/lib/c10d/ProcessGroupGloo.cpp#L110) of the tensors, it will return the same ptr value (this is the bug). As a result, both processes are reading the first 2 elements. \n\nThanks for reporting, I will add a fix for it. "
   },
   {
      "null": 266,
      "x": "Error downloading MNIST dataset",
      "z": "I solved my own problem too...\nthere was a mismatch of torchvision I installed using pip...it was 0.2.2 with torch 1.8.0...\nI then built and installed torchvision 0.9.0 from source and it works correctly",
      "y": "I solved my own problem too...\nthere was a mismatch of torchvision I installed using pip...it was 0.2.2 with torch 1.8.0...\nI then built and installed torchvision 0.9.0 from source and it works correctly"
   },
   {
      "null": 267,
      "x": "\"derivative for _thnn_fused_lstm_cell_backward is not implemented\" while using GPU",
      "z": "Apparently, double backward on gpu has been broken for some time. What's happening is in the forward _thnn_fused_lstm_cell is called https://github.com/pytorch/pytorch/blob/master/aten/src/ATen/native/RNN.cpp#L259-L265, and it has its derivative specified in derivatives.yaml, it's _thnn_fused_lstm_cell_backward. _thnn_fused_lstm_cell_backward does not have derivative, and throws an error. \nThe easiest fix would be to determine that there would be double backward and not use fused cell in forward, but I don't think there's a way to tell in forward if there will be double backward or not. So it looks like it's necessary to define differentiable backward functions for rnn cells, similarly to how it's done for weight norm, and fall back to them if `GradMode::is_enabled()`.",
      "y": "Apparently, double backward on gpu has been broken for some time. What's happening is in the forward _thnn_fused_lstm_cell is called https://github.com/pytorch/pytorch/blob/master/aten/src/ATen/native/RNN.cpp#L259-L265, and it has its derivative specified in derivatives.yaml, it's _thnn_fused_lstm_cell_backward. _thnn_fused_lstm_cell_backward does not have derivative, and throws an error. \nThe easiest fix would be to determine that there would be double backward and not use fused cell in forward, but I don't think there's a way to tell in forward if there will be double backward or not. So it looks like it's necessary to define differentiable backward functions for rnn cells, similarly to how it's done for weight norm, and fall back to them if `GradMode::is_enabled()`."
   },
   {
      "null": 268,
      "x": "Implementing GELU activation",
      "z": "This is fairly simple (see below for what would need to be added to `functional.py`), but seeing as it is common enough now could be worthwhile. If you're able to submit a PR with this (including a Module that wraps this and tests as necessary) then that would be useful.\n\n```py\ndef gelu(x):\n  return 0.5 * x * (1 + torch.tanh(math.sqrt(math.pi / 2) * (x + 0.044715 * x ** 3)))\n```",
      "y": "This is fairly simple (see below for what would need to be added to `functional.py`), but seeing as it is common enough now could be worthwhile. If you're able to submit a PR with this (including a Module that wraps this and tests as necessary) then that would be useful.\n\n```py\ndef gelu(x):\n  return 0.5 * x * (1 + torch.tanh(math.sqrt(math.pi / 2) * (x + 0.044715 * x ** 3)))\n```"
   },
   {
      "null": 269,
      "x": "tensorboard not updating",
      "z": "My current workaround:\n\nwhile true; do\n        timeout -sHUP 1m tensorboard --logdir=runs;\ndone",
      "y": "My current workaround:\n\nwhile true; do\n        timeout -sHUP 1m tensorboard --logdir=runs;\ndone"
   },
   {
      "null": 270,
      "x": "RuntimeError: Creating MTGP constants failed",
      "z": "This is now solved because curandStateMtgp constants are never created as a result of https://github.com/pytorch/pytorch/pull/20886. Closing this issue.",
      "y": "This is now solved because curandStateMtgp constants are never created as a result of https://github.com/pytorch/pytorch/pull/20886. Closing this issue."
   },
   {
      "null": 271,
      "x": "Install only a specific version via pip",
      "z": "this issue can now be closed, because we use local version identifiers. \nFor example, CPU-only version is:\n```\npip3 install torch==1.2.0+cpu torchvision==0.4.0+cpu -f https://download.pytorch.org/whl/torch_stable.html\n```\n\nSo, in `requirements.txt`, having a constraint such as `1.2.0+cpu` should be good.\n\nWe dont ship the variant wheels via PyPI though, and have no plans to do so (PyPI doesn't allow local version identifiers to be uploaded live)",
      "y": "this issue can now be closed, because we use local version identifiers. \nFor example, CPU-only version is:\n```\npip3 install torch==1.2.0+cpu torchvision==0.4.0+cpu -f https://download.pytorch.org/whl/torch_stable.html\n```\n\nSo, in `requirements.txt`, having a constraint such as `1.2.0+cpu` should be good.\n\nWe dont ship the variant wheels via PyPI though, and have no plans to do so (PyPI doesn't allow local version identifiers to be uploaded live)"
   },
   {
      "null": 272,
      "x": "MobileNetV2 export to ONNX fails",
      "z": "look into the mobilenet.py in torchvision, change the forward function:\n    def forward(self, x):\n        x = self.features(x)\n        # x = x.mean([2, 3])   # this line will result in bug\n        x = x.mean(3).mean(2)\n        x = self.classifier(x)\n        return x\n\n",
      "y": "look into the mobilenet.py in torchvision, change the forward function:\n    def forward(self, x):\n        x = self.features(x)\n        # x = x.mean([2, 3])   # this line will result in bug\n        x = x.mean(3).mean(2)\n        x = self.classifier(x)\n        return x\n\n"
   },
   {
      "null": 273,
      "x": "nn.CTCLoss RuntimeError on GPU",
      "z": "Perfect! I compile torchvision from source and it works well. Thanks you @t-vi",
      "y": "Perfect! I compile torchvision from source and it works well. Thanks you @t-vi"
   },
   {
      "null": 274,
      "x": "StepLR, MultiStepLR, ExponentialLR and CosineAnnealingLR scheduler wrong lr value",
      "z": "Thanks for the example. As mentioned in #26423, `get_lr` should be replaced by `get_last_lr`, see below.\n\n```python\nimport torch\nprint(\"pytorch version\",torch.__version__) \nimport torch.nn as nn\nmodel = nn.Linear(1, 1) # 'Net' is a simple MLP\noptimizer = torch.optim.SGD(model.parameters(), lr=0.1)\nschedular = torch.optim.lr_scheduler.MultiStepLR(optimizer, milestones = [4,7], gamma=0.1)\n\nprint('Initial LR : {0:.8f}'.format(schedular.get_lr()[0]))\nfor e in range(8):\n  optimizer.step()\n  schedular.step()\n  print('Epoch {0}, LR: {1:.8f}, opt LR {2:.8f}'.format(e, schedular.get_last_lr()[0],\n          optimizer.param_groups[0]['lr']))\n```\n\nSince #26423 has been merged, I will close this issue. Please feel free to re-open if the issue persists.",
      "y": "Thanks for the example. As mentioned in #26423, `get_lr` should be replaced by `get_last_lr`, see below.\n\n```python\nimport torch\nprint(\"pytorch version\",torch.__version__) \nimport torch.nn as nn\nmodel = nn.Linear(1, 1) # 'Net' is a simple MLP\noptimizer = torch.optim.SGD(model.parameters(), lr=0.1)\nschedular = torch.optim.lr_scheduler.MultiStepLR(optimizer, milestones = [4,7], gamma=0.1)\n\nprint('Initial LR : {0:.8f}'.format(schedular.get_lr()[0]))\nfor e in range(8):\n  optimizer.step()\n  schedular.step()\n  print('Epoch {0}, LR: {1:.8f}, opt LR {2:.8f}'.format(e, schedular.get_last_lr()[0],\n          optimizer.param_groups[0]['lr']))\n```\n\nSince #26423 has been merged, I will close this issue. Please feel free to re-open if the issue persists."
   },
   {
      "null": 275,
      "x": "Couple hundred MB are taken just by initializing cuda",
      "z": "> why it's need so many memory on the GPU?\n\nIt's used by the CUDA driver. I think it's used to store the CUDA kernels and PyTorch has a lot of CUDA kernels.\n\n> can i release it?\n\nNo (other than quitting the process)\n\nThe standard practice is to **not** access the same GPU from multiple processes. i.e. use one process per GPU. ",
      "y": "> why it's need so many memory on the GPU?\n\nIt's used by the CUDA driver. I think it's used to store the CUDA kernels and PyTorch has a lot of CUDA kernels.\n\n> can i release it?\n\nNo (other than quitting the process)\n\nThe standard practice is to **not** access the same GPU from multiple processes. i.e. use one process per GPU. "
   },
   {
      "null": 276,
      "x": "Saving state_dicts should capture shared state",
      "z": "I don't see anything we should change in PyTorch. `load_state_dict` doesn't change the structure of the Module and **should not** change the structure.\n\nIf you create your first version with:\n\n```\na = A()\nb = B(a=a)\nc = C(a=a, b=b)\n```\n\nThen your second version of C needs to be created with the same structure:\n\n```\na2 = A()\nb2 = B(a=a2)\nc2 = C(a=a2, b=b2)\n```\n\nIf you instead do:\n\n```\nc2 = C()\n```\n\nYou've create a different structure and shouldn't expect it to work the same as your first incarnation.\n",
      "y": "I don't see anything we should change in PyTorch. `load_state_dict` doesn't change the structure of the Module and **should not** change the structure.\n\nIf you create your first version with:\n\n```\na = A()\nb = B(a=a)\nc = C(a=a, b=b)\n```\n\nThen your second version of C needs to be created with the same structure:\n\n```\na2 = A()\nb2 = B(a=a2)\nc2 = C(a=a2, b=b2)\n```\n\nIf you instead do:\n\n```\nc2 = C()\n```\n\nYou've create a different structure and shouldn't expect it to work the same as your first incarnation.\n"
   },
   {
      "null": 277,
      "x": "Multiple GPU, Batch Normalization - RuntimeError: the derivative for 'running_mean' is not implemented",
      "z": "Low chance this applies to your situation, but I got this error when I used a parameter with `requires_grad` set to `True` as the `running_mean` in the `nn.functional.batch_norm` function. Hopefully this helps someone else who finds themselves in the same situation.",
      "y": "Low chance this applies to your situation, but I got this error when I used a parameter with `requires_grad` set to `True` as the `running_mean` in the `nn.functional.batch_norm` function. Hopefully this helps someone else who finds themselves in the same situation."
   },
   {
      "null": 278,
      "x": "[Feature request] Get cell state from the last layer for each t when using LSTM",
      "z": "considering that using custom RNNs is the only way this is possible (because nn.LSTM's backing CUDNN LSTM does not allow extraction of intermediate states), I am closing this request with @zou3519 's answer above as the way forward.",
      "y": "considering that using custom RNNs is the only way this is possible (because nn.LSTM's backing CUDNN LSTM does not allow extraction of intermediate states), I am closing this request with @zou3519 's answer above as the way forward."
   },
   {
      "null": 279,
      "x": "Compiling pytorch on MacOSX 10.13.5 with support for CUDA GeForceGT 750M",
      "z": "https://docs.nvidia.com/cuda/cuda-installation-guide-mac-os-x/index.html\n\nYou need to download Command_Line_Tools_macOS_10.13_for_Xcode_9.2.dmg from https://developer.apple.com/download/more/ and install then execute `sudo xcode-select --switch /Library/Developer/CommandLineTools`",
      "y": "https://docs.nvidia.com/cuda/cuda-installation-guide-mac-os-x/index.html\n\nYou need to download Command_Line_Tools_macOS_10.13_for_Xcode_9.2.dmg from https://developer.apple.com/download/more/ and install then execute `sudo xcode-select --switch /Library/Developer/CommandLineTools`"
   },
   {
      "null": 280,
      "x": "RuntimeError: DataLoader worker is killed by signal: Killed.",
      "z": "It is very likely there was a out-of-memory(OOM) in your system so the data worker got killed by the system. Try to use `dmesg -T` to see the detailed reason.",
      "y": "It is very likely there was a out-of-memory(OOM) in your system so the data worker got killed by the system. Try to use `dmesg -T` to see the detailed reason."
   },
   {
      "null": 281,
      "x": "[Feature Request] tensordot",
      "z": "I'd do tensordot based on the (private) sumproduct_pair function used by einsum. In fact, if we want this, I'd be glad to send a PR.\nMedium-term, I'd like to incorporate the optimizations into einsum itself. (Numpy, did, too.)",
      "y": "I'd do tensordot based on the (private) sumproduct_pair function used by einsum. In fact, if we want this, I'd be glad to send a PR.\nMedium-term, I'd like to incorporate the optimizations into einsum itself. (Numpy, did, too.)"
   },
   {
      "null": 282,
      "x": "[proposal] out= doesn't resize storage",
      "z": "We can create memory pool and enforce using it for memory operations. This will allows us to control who can resize and who can't",
      "y": "We can create memory pool and enforce using it for memory operations. This will allows us to control who can resize and who can't"
   },
   {
      "null": 283,
      "x": "[jit] gen_jit_dispatch generates duplicate \"descriptor\"s",
      "z": "We no longer use descriptors, so this is fixed.",
      "y": "We no longer use descriptors, so this is fixed."
   },
   {
      "null": 284,
      "x": "[jit] cannot trace tensor factory methods",
      "z": "This is kind of expected. There are two main problems at play here:\n\n1. RNG calls are not functional - they mutate the state of the global PRNG, and therefore we probably shouldn't rearrange them. This however requires us to add the \"world token\" to inhibit optimizations.\n2. We never trace constructors. If we want to do this, we probably need to make the tracing a thread-local property, instead of auto-detecting it as a per-Variable property.",
      "y": "This is kind of expected. There are two main problems at play here:\n\n1. RNG calls are not functional - they mutate the state of the global PRNG, and therefore we probably shouldn't rearrange them. This however requires us to add the \"world token\" to inhibit optimizations.\n2. We never trace constructors. If we want to do this, we probably need to make the tracing a thread-local property, instead of auto-detecting it as a per-Variable property."
   },
   {
      "null": 285,
      "x": "ImportError: DLL load failed: The specified module could not be found",
      "z": "Downgrade sqlite3 in Anaconda is the solution, as it was the last version I couldn't upgrade it, and no option for install or uninstall is available. after the downgrade it worked fine",
      "y": "Downgrade sqlite3 in Anaconda is the solution, as it was the last version I couldn't upgrade it, and no option for install or uninstall is available. after the downgrade it worked fine"
   },
   {
      "null": 286,
      "x": "PackedSequence's sorted_indices is not put on cuda when to('cuda') is called.",
      "z": "Issue is still present on `pytorch==1.3.1`\n\nTo fix, replace the following:\n`X = X.to(device)`\nWith this:\n`X = X.to(device=device)`\nProvided that \"X\" is a packed sequence.",
      "y": "Issue is still present on `pytorch==1.3.1`\n\nTo fix, replace the following:\n`X = X.to(device)`\nWith this:\n`X = X.to(device=device)`\nProvided that \"X\" is a packed sequence."
   },
   {
      "null": 287,
      "x": "Schema not found for node torch::eye",
      "z": "Are you able to extract a smaller repro?\n\n`import geoopt` works fine on master, and re-producing [the code](https://github.com/geoopt/geoopt/blob/master/geoopt/linalg/_expm.py#L29) causing the failure runs fine as well\n\n```python\n@torch.jit.script\ndef test(A):\n    return torch.eye(A.shape[1], dtype=A.dtype, device=A.device)\n```",
      "y": "Are you able to extract a smaller repro?\n\n`import geoopt` works fine on master, and re-producing [the code](https://github.com/geoopt/geoopt/blob/master/geoopt/linalg/_expm.py#L29) causing the failure runs fine as well\n\n```python\n@torch.jit.script\ndef test(A):\n    return torch.eye(A.shape[1], dtype=A.dtype, device=A.device)\n```"
   },
   {
      "null": 288,
      "x": "element-wise multiplication out of memory",
      "z": "You need ~12.6 GB of memory for your example (20 *75 * 1024* 1024 * 4 bytes * 2 = 12.58 GB). The last 2 is because you need space for `g` and `res`. If you don't care about keeping the value of `g` do:\n\n```python\nimport torch\n\ng = torch.rand([20, 75, 1024, 1024])\nw = torch.rand([1024, 1024])\ng *= w\nres = g\n```\n\nThat will require ~6.3 GB of memory.\n\nAlso, element-wise multiplication is just `g * w` in general. You can use einsum if you want, but it's not necessary.\n\nYou're Tensorflow example isn't actually running anything (you'd need a `session.run()` call) so it doesn't require any memory for storing tensor data.",
      "y": "You need ~12.6 GB of memory for your example (20 *75 * 1024* 1024 * 4 bytes * 2 = 12.58 GB). The last 2 is because you need space for `g` and `res`. If you don't care about keeping the value of `g` do:\n\n```python\nimport torch\n\ng = torch.rand([20, 75, 1024, 1024])\nw = torch.rand([1024, 1024])\ng *= w\nres = g\n```\n\nThat will require ~6.3 GB of memory.\n\nAlso, element-wise multiplication is just `g * w` in general. You can use einsum if you want, but it's not necessary.\n\nYou're Tensorflow example isn't actually running anything (you'd need a `session.run()` call) so it doesn't require any memory for storing tensor data."
   },
   {
      "null": 289,
      "x": "torch::tensor(std::vector) does not work properly in Microsoft Visual Studio Windows",
      "z": "`int64_t` works fine.",
      "y": "`int64_t` works fine."
   },
   {
      "null": 290,
      "x": "unable to load istream by using torch::jit::load(istream)",
      "z": "@lantiga\n I have solved it \uff0cthanks\n\n1. read pt model to char buffer by using ifstream\n\n2.change buffer to istream  \nstrstreambuf  buf(pModelData,length);\nstd::istream in(&buf);\n\n3\u3001load istream using torch::jit::load\n ",
      "y": "@lantiga\n I have solved it \uff0cthanks\n\n1. read pt model to char buffer by using ifstream\n\n2.change buffer to istream  \nstrstreambuf  buf(pModelData,length);\nstd::istream in(&buf);\n\n3\u3001load istream using torch::jit::load\n "
   },
   {
      "null": 291,
      "x": "autodiff for user script functions aka torch.jit.script for autograd.Function",
      "z": "I think the fix should be to allow users to provide derivatives.yaml for the extensions, and generate the autograd tracking code that Tom had to write manually if derivatives.yaml is provided. That would make extensions differentiable without having to wrap them in the custom autograd function, and improve UX in python eager mode and in C++. It will also reduce the need for custom autograd functions - it looks like in 99% {?) of cases custom autograd functions are needed to make extensions differentiable. ",
      "y": "I think the fix should be to allow users to provide derivatives.yaml for the extensions, and generate the autograd tracking code that Tom had to write manually if derivatives.yaml is provided. That would make extensions differentiable without having to wrap them in the custom autograd function, and improve UX in python eager mode and in C++. It will also reduce the need for custom autograd functions - it looks like in 99% {?) of cases custom autograd functions are needed to make extensions differentiable. "
   },
   {
      "null": 292,
      "x": "Build with MKLDNN broken",
      "z": "@mdreammao\nPls try to apply the patch in #22910 , then run \"git submodule update --init --recursive\".\n\nThanks.",
      "y": "@mdreammao\nPls try to apply the patch in #22910 , then run \"git submodule update --init --recursive\".\n\nThanks."
   },
   {
      "null": 293,
      "x": "Auto-differentiating torch.cdist is broken on GPU",
      "z": "Interesting. After instantiating a new Python kernel, the first time I run the above code, I receive the \"Backward is not reentrant\" error message. Subsequent reruns of the same code (with new and identical `X` tensor and same Python kernel) result in the original error message and the analytical gradients are very different each call.\n\nNotebook: \n[cdist_issue_repeated.ipynb.txt](https://github.com/pytorch/pytorch/files/3341208/cdist_issue_repeated.ipynb.txt)\n",
      "y": "Interesting. After instantiating a new Python kernel, the first time I run the above code, I receive the \"Backward is not reentrant\" error message. Subsequent reruns of the same code (with new and identical `X` tensor and same Python kernel) result in the original error message and the analytical gradients are very different each call.\n\nNotebook: \n[cdist_issue_repeated.ipynb.txt](https://github.com/pytorch/pytorch/files/3341208/cdist_issue_repeated.ipynb.txt)\n"
   },
   {
      "null": 294,
      "x": "conversion  to non-scalar type  torch::jit::load(\"model.pt\")",
      "z": "It looks like you are using a nightly build. We recently changed the output type of load. This should work:\n\n```\n torch::jit::script::Module module = torch::jit::load(\"model.pt\");\n```\n\nTutorials/documentation are still for the 1.1 release. It will be updated before we release 1.2.",
      "y": "It looks like you are using a nightly build. We recently changed the output type of load. This should work:\n\n```\n torch::jit::script::Module module = torch::jit::load(\"model.pt\");\n```\n\nTutorials/documentation are still for the 1.1 release. It will be updated before we release 1.2."
   },
   {
      "null": 295,
      "x": "Dependency issues with torch.utils.tensorboard: \"No module named past\" and \"No module named 'PIL'\"",
      "z": "Note that the right fix for the `past.stringbase` is really to stop using the deprecated API (which is not that hard too...)",
      "y": "Note that the right fix for the `past.stringbase` is really to stop using the deprecated API (which is not that hard too...)"
   },
   {
      "null": 296,
      "x": "Cuda required when loading a TorchScript with map_location='cpu'",
      "z": "It looks like this model came from a trace, is that correct? When exporting a model for use on CPU, we recommend that you switch it to CPU mode _before_ tracing it and exporting it. Otherwise it will hard-code details of using the GPU into the model, as was done here (`torch.to(CONSTANTS.c0, torch.device(\"cuda:0\"), 6, False, False)`). Telling the parameters to map to the CPU will not change traced details like the `to` call which copies the Tensor to the GPU).",
      "y": "It looks like this model came from a trace, is that correct? When exporting a model for use on CPU, we recommend that you switch it to CPU mode _before_ tracing it and exporting it. Otherwise it will hard-code details of using the GPU into the model, as was done here (`torch.to(CONSTANTS.c0, torch.device(\"cuda:0\"), 6, False, False)`). Telling the parameters to map to the CPU will not change traced details like the `to` call which copies the Tensor to the GPU)."
   },
   {
      "null": 297,
      "x": "runtime error (7) : too many resources requested for launch at pytorch/aten/src/THC/THCTensorSort.cu:62",
      "z": "The main thing I'm aware of is that the switchover from bitonic sort to thrust as a fallback is different for 64 bit dtypes. These have been lowered for sort but that has not been applied to topk's sorting.\nThe natural questions would be\n- What is the tensor dtype and size and what are dim and k used ins topk?\n- Does the non-sorting topk work for that?\n- Does sort work for the non-sorted values?\n\n(The last two also might offer a workaround until it is fixed in PyTorch).\n",
      "y": "The main thing I'm aware of is that the switchover from bitonic sort to thrust as a fallback is different for 64 bit dtypes. These have been lowered for sort but that has not been applied to topk's sorting.\nThe natural questions would be\n- What is the tensor dtype and size and what are dim and k used ins topk?\n- Does the non-sorting topk work for that?\n- Does sort work for the non-sorted values?\n\n(The last two also might offer a workaround until it is fixed in PyTorch).\n"
   },
   {
      "null": 298,
      "x": "[Build Error]undefined reference to `__cudaPushCallConfiguration'",
      "z": "This probably means that the inconsistent version of NVCC compile and your conda CUDAToolKit package\n\n```\n# Check the NVCC compile version(e.g.)\n/usr/cuda-9.2/bin/nvcc --version\n# Check the CUDAToolKit version(e.g.)\n~/anaconda3/bin/conda list | grep cuda\n\n# If you need to update your CUDAToolKit\n~/anaconda3/bin/conda install -c anaconda cudatoolkit==9.2\n```\nBoth of them should have the same version. For example, if NVCC==9.2 and CUDAToolKit==9.2, this will be fine while when NVCC==9.2 but CUDAToolKit==9, it fails.",
      "y": "This probably means that the inconsistent version of NVCC compile and your conda CUDAToolKit package\n\n```\n# Check the NVCC compile version(e.g.)\n/usr/cuda-9.2/bin/nvcc --version\n# Check the CUDAToolKit version(e.g.)\n~/anaconda3/bin/conda list | grep cuda\n\n# If you need to update your CUDAToolKit\n~/anaconda3/bin/conda install -c anaconda cudatoolkit==9.2\n```\nBoth of them should have the same version. For example, if NVCC==9.2 and CUDAToolKit==9.2, this will be fine while when NVCC==9.2 but CUDAToolKit==9, it fails."
   },
   {
      "null": 299,
      "x": "forward function can not insert pdb.trace",
      "z": "Hi @DanlanChen,\n\nWe do not have deep integration of pdb into our runtime. To debug a ScriptModule, you can do a few things:\n\n1) Remove the `@torch.jit.script_method` decorator from the method and debug the method as Python code\n2) Create a free python function, pass in values you're interested to inspect into that function, and within the function call `pdb`. Example:\n\n```\nimport torch\n\ndef debug_fn(a, b, c):\n    print(a, b, c)\n    import pdb; pdb.set_trace()\n\nclass FooMod(torch.jit.ScriptModule):\n    @torch.jit.script_method\n    def forward(self, x, y):\n        z = x + y\n        debug_fn(x, y, z)\n        return z\n\nfm = FooMod()\nfm(torch.rand(3), torch.rand(3))\n```\n\nHope this helps with your debugging!",
      "y": "Hi @DanlanChen,\n\nWe do not have deep integration of pdb into our runtime. To debug a ScriptModule, you can do a few things:\n\n1) Remove the `@torch.jit.script_method` decorator from the method and debug the method as Python code\n2) Create a free python function, pass in values you're interested to inspect into that function, and within the function call `pdb`. Example:\n\n```\nimport torch\n\ndef debug_fn(a, b, c):\n    print(a, b, c)\n    import pdb; pdb.set_trace()\n\nclass FooMod(torch.jit.ScriptModule):\n    @torch.jit.script_method\n    def forward(self, x, y):\n        z = x + y\n        debug_fn(x, y, z)\n        return z\n\nfm = FooMod()\nfm(torch.rand(3), torch.rand(3))\n```\n\nHope this helps with your debugging!"
   },
   {
      "null": 300,
      "x": "zip not allowed in forward function in torch.jit.ScriptModule",
      "z": "As a workaround, you can do the following:\n\n```\nmod_list = []\nfor mod1, mod2 in zip(module1, module2):\n    mod_list.append(nn.Sequential(mod1, mod2))\nself.module = nn.ModuleList(mod_list)\n```\n",
      "y": "As a workaround, you can do the following:\n\n```\nmod_list = []\nfor mod1, mod2 in zip(module1, module2):\n    mod_list.append(nn.Sequential(mod1, mod2))\nself.module = nn.ModuleList(mod_list)\n```\n"
   },
   {
      "null": 301,
      "x": "Cannot build libtorch: SLEEF does not allow in-source builds",
      "z": "Sleef doesn't support nesting. I could fix it similarly to https://ceres-solver-review.googlesource.com/c/ceres-solver/+/9780:\n\n- Using sleef_[SOURCE/BINARY]_DIR (which are defined by CMake when\n  project(sleef) is called, in favour of CMAKE_[SOURCE/BINARY]_DIR\n  enables sleef to be nested within (and built by) a larger CMake\n  project (which also contains other projects).\n- CMAKE_[SOURCE/BINARY]_DIR always refers to the top-level source\n  and binary directories (i.e. the first encountered), as a result if\n  sleef is a nested project within a larger project, these would not\n  correctly identify the source/binary directories for sleef (as they\n  would refer to the root project in which sleef is nested).\n- Using sleef_[SOURCE/BINARY]_DIR should ensure that sleef always uses\n  the correct source/binary directories, irrespective of whether sleef\n  is nested or not.\n\nPatch for `aten/src/ATen/CMakeLists.txt`:\n------------------------------------------------------\n```\ndiff --git a/aten/src/ATen/CMakeLists.txt b/aten/src/ATen/CMakeLists.txt\nindex 07b3c106b..6133c583a 100644\n--- a/aten/src/ATen/CMakeLists.txt\n+++ b/aten/src/ATen/CMakeLists.txt\n@@ -172,10 +172,13 @@ if(NOT MSVC AND NOT EMSCRIPTEN)\n   set(BUILD_DFT OFF CACHE BOOL \"Don't build sleef DFT lib\" FORCE)\n   set(BUILD_GNUABI_LIBS OFF CACHE BOOL \"Don't build sleef gnuabi libs\" FORCE)\n   set(BUILD_TESTS OFF CACHE BOOL \"Don't build sleef tests\" FORCE)\n+  set(sleef_SOURCE_DIR \"${CMAKE_CURRENT_SOURCE_DIR}/../../../third_party/sleef\")\n+  set(sleef_BINARY_DIR \"${CMAKE_BINARY_DIR}/sleef\")\n   add_subdirectory(\"${CMAKE_CURRENT_SOURCE_DIR}/../../../third_party/sleef\" ${CMAKE_BINARY_DIR}/sleef)\n   set_property(TARGET sleef PROPERTY FOLDER \"dependencies\")\n-  list(APPEND ATen_THIRD_PARTY_INCLUDE ${CMAKE_BINARY_DIR}/include)\n-  link_directories(${CMAKE_BINARY_DIR}/sleef/lib)\n+  list(APPEND ATen_THIRD_PARTY_INCLUDE ${sleef_BINARY_DIR}/include)\n+  link_directories(${sleef_BINARY_DIR}/lib)\n   list(APPEND ATen_CPU_DEPENDENCY_LIBS sleef)\n \n   set(CMAKE_C_FLAGS_DEBUG ${OLD_CMAKE_C_FLAGS_DEBUG})\n```\n\nPatch for cmake files in sleef:\n---------------------------------------\n[sleef_patch.txt](https://github.com/pytorch/pytorch/files/2979917/sleef_patch.txt)\n",
      "y": "Sleef doesn't support nesting. I could fix it similarly to https://ceres-solver-review.googlesource.com/c/ceres-solver/+/9780:\n\n- Using sleef_[SOURCE/BINARY]_DIR (which are defined by CMake when\n  project(sleef) is called, in favour of CMAKE_[SOURCE/BINARY]_DIR\n  enables sleef to be nested within (and built by) a larger CMake\n  project (which also contains other projects).\n- CMAKE_[SOURCE/BINARY]_DIR always refers to the top-level source\n  and binary directories (i.e. the first encountered), as a result if\n  sleef is a nested project within a larger project, these would not\n  correctly identify the source/binary directories for sleef (as they\n  would refer to the root project in which sleef is nested).\n- Using sleef_[SOURCE/BINARY]_DIR should ensure that sleef always uses\n  the correct source/binary directories, irrespective of whether sleef\n  is nested or not.\n\nPatch for `aten/src/ATen/CMakeLists.txt`:\n------------------------------------------------------\n```\ndiff --git a/aten/src/ATen/CMakeLists.txt b/aten/src/ATen/CMakeLists.txt\nindex 07b3c106b..6133c583a 100644\n--- a/aten/src/ATen/CMakeLists.txt\n+++ b/aten/src/ATen/CMakeLists.txt\n@@ -172,10 +172,13 @@ if(NOT MSVC AND NOT EMSCRIPTEN)\n   set(BUILD_DFT OFF CACHE BOOL \"Don't build sleef DFT lib\" FORCE)\n   set(BUILD_GNUABI_LIBS OFF CACHE BOOL \"Don't build sleef gnuabi libs\" FORCE)\n   set(BUILD_TESTS OFF CACHE BOOL \"Don't build sleef tests\" FORCE)\n+  set(sleef_SOURCE_DIR \"${CMAKE_CURRENT_SOURCE_DIR}/../../../third_party/sleef\")\n+  set(sleef_BINARY_DIR \"${CMAKE_BINARY_DIR}/sleef\")\n   add_subdirectory(\"${CMAKE_CURRENT_SOURCE_DIR}/../../../third_party/sleef\" ${CMAKE_BINARY_DIR}/sleef)\n   set_property(TARGET sleef PROPERTY FOLDER \"dependencies\")\n-  list(APPEND ATen_THIRD_PARTY_INCLUDE ${CMAKE_BINARY_DIR}/include)\n-  link_directories(${CMAKE_BINARY_DIR}/sleef/lib)\n+  list(APPEND ATen_THIRD_PARTY_INCLUDE ${sleef_BINARY_DIR}/include)\n+  link_directories(${sleef_BINARY_DIR}/lib)\n   list(APPEND ATen_CPU_DEPENDENCY_LIBS sleef)\n \n   set(CMAKE_C_FLAGS_DEBUG ${OLD_CMAKE_C_FLAGS_DEBUG})\n```\n\nPatch for cmake files in sleef:\n---------------------------------------\n[sleef_patch.txt](https://github.com/pytorch/pytorch/files/2979917/sleef_patch.txt)\n"
   },
   {
      "null": 302,
      "x": "Confusing behavior with *= operator with torch.expand",
      "z": "This is a duplicate of https://github.com/pytorch/pytorch/issues/957\n\nI agree this is confusing and has bitten a lot of users in the past, we should discuss about how to circumvent this (maybe by adding a `WRITABLE = False` flag to expanded tensors?)",
      "y": "This is a duplicate of https://github.com/pytorch/pytorch/issues/957\n\nI agree this is confusing and has bitten a lot of users in the past, we should discuss about how to circumvent this (maybe by adding a `WRITABLE = False` flag to expanded tensors?)"
   },
   {
      "null": 303,
      "x": "from torch._C import * ImportError: DLL load failed: The specified module could not be found.",
      "z": "> the same issue today\n> i've tried python 3.6.x and 3.7.1, didn't work.\n\nfirst, install python3.6.7.(other version such as 3.6.0 and 3.6.8 didn't work)\nand then ,` pip install --upgrade numpy  and pip install intel-openmp` \nit works for me\n",
      "y": "> the same issue today\n> i've tried python 3.6.x and 3.7.1, didn't work.\n\nfirst, install python3.6.7.(other version such as 3.6.0 and 3.6.8 didn't work)\nand then ,` pip install --upgrade numpy  and pip install intel-openmp` \nit works for me\n"
   },
   {
      "null": 304,
      "x": "multivariate_normal.log_prob is slow",
      "z": "@t-vi Thanks for notifying me! I think that it is my fault when trying to use `trtrs` instead of `inverse`. The slowdown seems lie at [this line](https://github.com/pytorch/pytorch/blob/master/torch/distributions/multivariate_normal.py#L43) where we expand `bL` batch shape to match the later part of `bx` batch shape, then do triangle solve with \"expanded\" bL. If we use `inverse`, then after taking the inverse of `bL`, we just simply do `matmul` with `bx` to get the result. So it will be much faster in the above example.\n\nThe current implementation works well when `x` has additional batch shapes instead. For example, the following version will be fast because we won't expand bL.\n```\nsigma = torch.eye(8).expand(6, 1, 8, 8).contiguous().requires_grad_()\nx_repeat = torch.randn(8000, 6, 1, 8)\n```\n\nI'll sketch out a solution which I have in mind for further discussion.",
      "y": "@t-vi Thanks for notifying me! I think that it is my fault when trying to use `trtrs` instead of `inverse`. The slowdown seems lie at [this line](https://github.com/pytorch/pytorch/blob/master/torch/distributions/multivariate_normal.py#L43) where we expand `bL` batch shape to match the later part of `bx` batch shape, then do triangle solve with \"expanded\" bL. If we use `inverse`, then after taking the inverse of `bL`, we just simply do `matmul` with `bx` to get the result. So it will be much faster in the above example.\n\nThe current implementation works well when `x` has additional batch shapes instead. For example, the following version will be fast because we won't expand bL.\n```\nsigma = torch.eye(8).expand(6, 1, 8, 8).contiguous().requires_grad_()\nx_repeat = torch.randn(8000, 6, 1, 8)\n```\n\nI'll sketch out a solution which I have in mind for further discussion."
   },
   {
      "null": 305,
      "x": "Training on 360 sequences, validating on 0 sequences. python3: symbol lookup error: /home/ankitakulkarni/anaconda3/lib/python3.6/site-packages/torch/lib/libtorch_python.so: undefined symbol: PySlice_Unpack",
      "z": "Solved ...Update the version of python from 3.6.0 to 3.6.2 \nThanks!!",
      "y": "Solved ...Update the version of python from 3.6.0 to 3.6.2 \nThanks!!"
   },
   {
      "null": 306,
      "x": "Windows pytorch CUDA 10 error",
      "z": "FWIW, I fixed this by reinstalling torch on a fresh installation of miniconda3 (the one available on their website right now). However, when I do conda update --all, the problem comes back. This means the problem has to do with installing the most recent version of one of these packages: cryptography, pyopenssl, python, setuptools. I've managed to get around this for now by installing torch on a clean new environment.",
      "y": "FWIW, I fixed this by reinstalling torch on a fresh installation of miniconda3 (the one available on their website right now). However, when I do conda update --all, the problem comes back. This means the problem has to do with installing the most recent version of one of these packages: cryptography, pyopenssl, python, setuptools. I've managed to get around this for now by installing torch on a clean new environment."
   },
   {
      "null": 307,
      "x": "Broken indexing?",
      "z": "Resolved by upgrading to >= Python 3.6.1",
      "y": "Resolved by upgrading to >= Python 3.6.1"
   },
   {
      "null": 308,
      "x": "\"invalid parameter combination for AltiVec intrinsic\" error with ppc64le, g++ v7.4",
      "z": "Ahh!  I see, I over-corrected.  \nSo I put back vec_sldw on line 297, and the compile completed without error.  Whew!\nGranted, this was a test of the one file that had failed rather than a full build, but it does look good.",
      "y": "Ahh!  I see, I over-corrected.  \nSo I put back vec_sldw on line 297, and the compile completed without error.  Whew!\nGranted, this was a test of the one file that had failed rather than a full build, but it does look good."
   },
   {
      "null": 309,
      "x": "torch.linalg.cond return dtype inconsistent with doc and np.linalg.cond",
      "z": "`torch.linalg.cond` was changed to always return a real-valued tensor in https://github.com/pytorch/pytorch/pull/48284. We've discussed that with @kurtamohler and @mruberry yesterday and came to the conclusion it's the right thing to be divergent from NumPy in this case. This behavior should be fixed in NumPy and the issue was filed https://github.com/numpy/numpy/issues/18304.\n\nWhen normal `torch.linalg.cond` is called it should return the real number always (float32 for complex64 inputs, float64 for complex128 inputs). When the out variant is called float and complex tensors should be allowed according to the description of the correct \"out=\" behavior https://github.com/pytorch/pytorch/wiki/Developer-FAQ#how-does-out-work-in-pytorch.\n",
      "y": "`torch.linalg.cond` was changed to always return a real-valued tensor in https://github.com/pytorch/pytorch/pull/48284. We've discussed that with @kurtamohler and @mruberry yesterday and came to the conclusion it's the right thing to be divergent from NumPy in this case. This behavior should be fixed in NumPy and the issue was filed https://github.com/numpy/numpy/issues/18304.\n\nWhen normal `torch.linalg.cond` is called it should return the real number always (float32 for complex64 inputs, float64 for complex128 inputs). When the out variant is called float and complex tensors should be allowed according to the description of the correct \"out=\" behavior https://github.com/pytorch/pytorch/wiki/Developer-FAQ#how-does-out-work-in-pytorch.\n"
   },
   {
      "null": 310,
      "x": "Is `torch.multiprocessing.spawn` compatible with `DataLoader`?",
      "z": "hey @PetrochukM!\n\nI think this is less a PyTorch issue and something the Lightning team should investigate. \n\nI'll hijack this issue for now to make sure my thoughts are documented!\n\nForgive me since I wasn't there when the original DDP docs for Lightning were made and this caveat was introduced, but from working with DDP in a spawn setting vs using `Popen` like `torch.distributed.launch`, the main issue with spawn is the need to serialize all state objects. This becomes an issue when using functions defined at runtime like so:\n\nhttps://gist.github.com/SeanNaren/f4a99235fc736a438637c31ee94f082f\n\nTo fix you just need to move `TestModel` outside of the main script. I also tested with multiple workers and it seems fine. The message in the Lightning Docs may be outdated or incorrect, and if thats the case we should update it ASAP!",
      "y": "hey @PetrochukM!\n\nI think this is less a PyTorch issue and something the Lightning team should investigate. \n\nI'll hijack this issue for now to make sure my thoughts are documented!\n\nForgive me since I wasn't there when the original DDP docs for Lightning were made and this caveat was introduced, but from working with DDP in a spawn setting vs using `Popen` like `torch.distributed.launch`, the main issue with spawn is the need to serialize all state objects. This becomes an issue when using functions defined at runtime like so:\n\nhttps://gist.github.com/SeanNaren/f4a99235fc736a438637c31ee94f082f\n\nTo fix you just need to move `TestModel` outside of the main script. I also tested with multiple workers and it seems fine. The message in the Lightning Docs may be outdated or incorrect, and if thats the case we should update it ASAP!"
   },
   {
      "null": 311,
      "x": "Using LAPACK on Pi4 (64Bit Raspberry Pi OS RAM 8GB)",
      "z": "I've got a way to work out: \n1. https://github.com/ljk53/pytorch-rpi download torch-1.7.0a0-cp38-cp38-linux_aarch64.whl and pip3 install\n2. apt install libopenblas-dev\n3. success!",
      "y": "I've got a way to work out: \n1. https://github.com/ljk53/pytorch-rpi download torch-1.7.0a0-cp38-cp38-linux_aarch64.whl and pip3 install\n2. apt install libopenblas-dev\n3. success!"
   },
   {
      "null": 312,
      "x": "Quantized LeakyReLu Bug: Input tensor size determines the output values",
      "z": "Hi @Krosus , thanks again for the report.  After some internal discussion we decided that the discrepancy is acceptable, we are getting higher performance in the vectorized path, and the discrepancy will only be present if we are comparing tensors of different sizes.  Please let us know if this is blocking you, happy to help brainstorm workarounds.",
      "y": "Hi @Krosus , thanks again for the report.  After some internal discussion we decided that the discrepancy is acceptable, we are getting higher performance in the vectorized path, and the discrepancy will only be present if we are comparing tensors of different sizes.  Please let us know if this is blocking you, happy to help brainstorm workarounds."
   },
   {
      "null": 313,
      "x": "aten::normal_ not handled as a special op in RemoveTensorMutation pass.",
      "z": "The easiest fix is to probably just implement `normal`",
      "y": "The easiest fix is to probably just implement `normal`"
   },
   {
      "null": 314,
      "x": "test_variant_consistency_jit tests fail on CPU for min & max when dtype is bfloat16 & dim argument is passed",
      "z": "@imaginary-person Thanks for the update! Yes there are known problems with the BFloat16 operator in JIT, it's not fully supported. We have an issue #48978 (I see you've commented on it already) to figure out a way to clean up the test. You should be good to skip the BFloat16 dtype for the test_variant_consistency_jit tests without worry ",
      "y": "@imaginary-person Thanks for the update! Yes there are known problems with the BFloat16 operator in JIT, it's not fully supported. We have an issue #48978 (I see you've commented on it already) to figure out a way to clean up the test. You should be good to skip the BFloat16 dtype for the test_variant_consistency_jit tests without worry "
   },
   {
      "null": 315,
      "x": "[collect_env] Unable to collect CUDA version anymore",
      "z": "I encountered the same problem some time ago. Please see the description in https://github.com/k2-fsa/k2/pull/584\n\n- For CUDA 11.0\n```\n$ nvcc --version\nnvcc: NVIDIA (R) Cuda compiler driver\nCopyright (c) 2005-2020 NVIDIA Corporation\nBuilt on Thu_Jun_11_22:26:38_PDT_2020\nCuda compilation tools, release 11.0, V11.0.194\nBuild cuda_11.0_bu.TC445_37.28540450_0\n```\n\n- For CUDA 10.1\n```\n$ nvcc --version\nnvcc: NVIDIA (R) Cuda compiler driver\nCopyright (c) 2005-2019 NVIDIA Corporation\nBuilt on Sun_Jul_28_19:07:16_PDT_2019\nCuda compilation tools, release 10.1, V10.1.243\n```\n\nThere is an extra line `Build cuda_11.0_bu.TC445_37.28540450_0` for CUDA 11.0, which cannot be handled\nby `torch.utils.collect_env.get_running_cuda_version` since it uses a pattern `r\"V(.*)$\"` without `re.MULTILINE`.\nTherefore, `torch.utils.collect_env.get_running_cuda_version` returns `None` for CUDA 11.0",
      "y": "I encountered the same problem some time ago. Please see the description in https://github.com/k2-fsa/k2/pull/584\n\n- For CUDA 11.0\n```\n$ nvcc --version\nnvcc: NVIDIA (R) Cuda compiler driver\nCopyright (c) 2005-2020 NVIDIA Corporation\nBuilt on Thu_Jun_11_22:26:38_PDT_2020\nCuda compilation tools, release 11.0, V11.0.194\nBuild cuda_11.0_bu.TC445_37.28540450_0\n```\n\n- For CUDA 10.1\n```\n$ nvcc --version\nnvcc: NVIDIA (R) Cuda compiler driver\nCopyright (c) 2005-2019 NVIDIA Corporation\nBuilt on Sun_Jul_28_19:07:16_PDT_2019\nCuda compilation tools, release 10.1, V10.1.243\n```\n\nThere is an extra line `Build cuda_11.0_bu.TC445_37.28540450_0` for CUDA 11.0, which cannot be handled\nby `torch.utils.collect_env.get_running_cuda_version` since it uses a pattern `r\"V(.*)$\"` without `re.MULTILINE`.\nTherefore, `torch.utils.collect_env.get_running_cuda_version` returns `None` for CUDA 11.0"
   },
   {
      "null": 316,
      "x": "M1 release and nightly binaries",
      "z": "The compilation should be working just fine now. Note that some tests are failing though (for various reasons, not only M1): https://github.com/pytorch/pytorch/issues/56779\n\nYou can also get binaries from nightly by just following the instructions here: https://pytorch.org/get-started/locally/ (an x86 python will pull the x86 package and an arm64 python will pull the arm64 package).",
      "y": "The compilation should be working just fine now. Note that some tests are failing though (for various reasons, not only M1): https://github.com/pytorch/pytorch/issues/56779\n\nYou can also get binaries from nightly by just following the instructions here: https://pytorch.org/get-started/locally/ (an x86 python will pull the x86 package and an arm64 python will pull the arm64 package)."
   },
   {
      "null": 317,
      "x": "[FR] Initialize module on specified device",
      "z": "This might be related to the meta-learning paradigm, i.e. to use one network to predict parameters of another network. There are libraries made for this (https://github.com/tristandeleu/pytorch-meta/blob/master/torchmeta/modules/linear.py). It seems to be another potential use case of this feature.\n\nWith this feature and #53144 to allow creating a module on null/meta device, it seems we'll be able to do the following to easily achieve meta learning, reusing nn.Conv2d without having to create custom MetaConv2d:\n\n```python\ndef __init__():\n   self.meta_conv = Conv2d(..., device='meta')\n   self.parameter_predictor = PredictorNet(num_param=sum(k.numel() for k in self.meta_conv.parameters()))\n\ndef forward(x):\n  params = self.parameter_predictor(x)\n  with temporary_set_params(self.meta_conv, params):\n      return self.meta_conv(x)\n```",
      "y": "This might be related to the meta-learning paradigm, i.e. to use one network to predict parameters of another network. There are libraries made for this (https://github.com/tristandeleu/pytorch-meta/blob/master/torchmeta/modules/linear.py). It seems to be another potential use case of this feature.\n\nWith this feature and #53144 to allow creating a module on null/meta device, it seems we'll be able to do the following to easily achieve meta learning, reusing nn.Conv2d without having to create custom MetaConv2d:\n\n```python\ndef __init__():\n   self.meta_conv = Conv2d(..., device='meta')\n   self.parameter_predictor = PredictorNet(num_param=sum(k.numel() for k in self.meta_conv.parameters()))\n\ndef forward(x):\n  params = self.parameter_predictor(x)\n  with temporary_set_params(self.meta_conv, params):\n      return self.meta_conv(x)\n```"
   },
   {
      "null": 318,
      "x": "Please fix all the related links format from http to https",
      "z": "closed via https://github.com/pytorch/pytorch.github.io/pull/128",
      "y": "closed via https://github.com/pytorch/pytorch.github.io/pull/128"
   },
   {
      "null": 319,
      "x": "ImportError: libcurand.so.9.0: cannot open shared object file: No such file or directory",
      "z": "@ezyang  thanks for your help. I think it is because that I used to install old caffe2.  The old caffe2 `libcaffe2.so`, `libcaffe2_detectron_ops_gpu.so`,` libcaffe2_gpu.so`, `libcaffe2_module_test_dynamic.so`, `libcaffe2_observers.so` is in `/usr/local/lib`, but now new installed caffe2 they all in `pytorch/build/lib`. I delete all in  `/usr/local/lib`. Now it seems OK. do you think my approach is correct? thanks!",
      "y": "@ezyang  thanks for your help. I think it is because that I used to install old caffe2.  The old caffe2 `libcaffe2.so`, `libcaffe2_detectron_ops_gpu.so`,` libcaffe2_gpu.so`, `libcaffe2_module_test_dynamic.so`, `libcaffe2_observers.so` is in `/usr/local/lib`, but now new installed caffe2 they all in `pytorch/build/lib`. I delete all in  `/usr/local/lib`. Now it seems OK. do you think my approach is correct? thanks!"
   },
   {
      "null": 320,
      "x": "Exception in Thread: ValueError: signal number 32 out of range",
      "z": "I have solved the problem by updating Python3.5 to Python3.7",
      "y": "I have solved the problem by updating Python3.5 to Python3.7"
   },
   {
      "null": 321,
      "x": "[JIT] torch.tensor doesn't trace devices correctly",
      "z": "I have a tentative fix, that, unless someone explains me that this is a bad idea, I'll make into a PR.\nThe basic idea is to use a bit more tensor methods (in particular `Tensor.to(...)`) instead of doing this manually.\n\n```\niff --git a/torch/csrc/utils/tensor_new.cpp b/torch/csrc/utils/tensor_new.cpp\nindex f9d6ffc62..d8c939232 100644\n--- a/torch/csrc/utils/tensor_new.cpp\n+++ b/torch/csrc/utils/tensor_new.cpp\n@@ -244,7 +244,9 @@ Tensor internal_new_from_data(\n       (char*)tensor.data_ptr(), tensor.sizes(), tensor.strides(), 0,\n       scalarType, tensor.type().elementSizeInBytes(), data);\n   const auto& type_to_use = type_inference ? type.toScalarType(scalarType) : type;\n-  return new_with_type_conversion(type_to_use, tensor, device_index);\n+  auto device = device_opt.has_value() ? *device_opt : tensor.device();\n+  return tensor.to(device, type_to_use.scalarType(), /*blocking=*/false, /*copy=*/false);\n+  //return new_with_type_conversion(type_to_use, tensor, device_index);\n }\n \n Tensor new_from_data_copy(\n```\n",
      "y": "I have a tentative fix, that, unless someone explains me that this is a bad idea, I'll make into a PR.\nThe basic idea is to use a bit more tensor methods (in particular `Tensor.to(...)`) instead of doing this manually.\n\n```\niff --git a/torch/csrc/utils/tensor_new.cpp b/torch/csrc/utils/tensor_new.cpp\nindex f9d6ffc62..d8c939232 100644\n--- a/torch/csrc/utils/tensor_new.cpp\n+++ b/torch/csrc/utils/tensor_new.cpp\n@@ -244,7 +244,9 @@ Tensor internal_new_from_data(\n       (char*)tensor.data_ptr(), tensor.sizes(), tensor.strides(), 0,\n       scalarType, tensor.type().elementSizeInBytes(), data);\n   const auto& type_to_use = type_inference ? type.toScalarType(scalarType) : type;\n-  return new_with_type_conversion(type_to_use, tensor, device_index);\n+  auto device = device_opt.has_value() ? *device_opt : tensor.device();\n+  return tensor.to(device, type_to_use.scalarType(), /*blocking=*/false, /*copy=*/false);\n+  //return new_with_type_conversion(type_to_use, tensor, device_index);\n }\n \n Tensor new_from_data_copy(\n```\n"
   },
   {
      "null": 322,
      "x": "gradient difference between single GPU and multi-GPU DataParallel",
      "z": "the order of doing operations accumulates floating point errors in different ways. This is the reason for this difference, and it's expected.",
      "y": "the order of doing operations accumulates floating point errors in different ways. This is the reason for this difference, and it's expected."
   },
   {
      "null": 323,
      "x": "UserWarning: ONNX export failed on ATen operator _argmax because torch.onnx.symbolic._argmax does not exist",
      "z": "If I add the following function into torch/onnx/symbolic.py\n\n@parse_args('v', 'i', 'i')\ndef _argmax(g, self, dim, keepdim=None):\n    return g.op(\"ArgMax\", self, axis_i=dim, keepdim_i=keepdim)\n\n\nThen the export is able to proceed without error.\n",
      "y": "If I add the following function into torch/onnx/symbolic.py\n\n@parse_args('v', 'i', 'i')\ndef _argmax(g, self, dim, keepdim=None):\n    return g.op(\"ArgMax\", self, axis_i=dim, keepdim_i=keepdim)\n\n\nThen the export is able to proceed without error.\n"
   },
   {
      "null": 324,
      "x": "cmake error about rocrand",
      "z": "@NIEYALI I see. So to do ROCm build rocblas is a required dependency, so you need to install that. If you don't want to do ROCm build (but just happens you have partially installed ROCm before in your system), I have put up #14261 to allow disabling ROCm build with USE_ROCM=0 environment variable.",
      "y": "@NIEYALI I see. So to do ROCm build rocblas is a required dependency, so you need to install that. If you don't want to do ROCm build (but just happens you have partially installed ROCm before in your system), I have put up #14261 to allow disabling ROCm build with USE_ROCM=0 environment variable."
   },
   {
      "null": 325,
      "x": "[Aten] at::randint doesn't return a variable",
      "z": "The variable in the case of `at::normal` is because of `torch::zeros`. Functions in the `torch` namespace are by default `Variable`s.",
      "y": "The variable in the case of `at::normal` is because of `torch::zeros`. Functions in the `torch` namespace are by default `Variable`s."
   },
   {
      "null": 326,
      "x": "libtorch latest-deps cuasing an error: PyTorch script module file is too old",
      "z": "Nevermind -- re-exported model with new nightly & it worked fine.",
      "y": "Nevermind -- re-exported model with new nightly & it worked fine."
   },
   {
      "null": 327,
      "x": "Unable to pickle torch dtype objects in Python 3.5",
      "z": "Actually torch dtype object is already serializable. Closing....\n```\nIn [6]: b = copy.deepcopy(a)\n\nIn [7]: id(b)\nOut[7]: 139818768678472\n\nIn [8]: id(a)\nOut[8]: 139818768678472\n\nIn [9]: import pickle\n\nIn [10]: with open('/tmp/a', 'wb') as f:\n    ...:     pickle.dump(torch.float32, f)\n    ...:\n```\n",
      "y": "Actually torch dtype object is already serializable. Closing....\n```\nIn [6]: b = copy.deepcopy(a)\n\nIn [7]: id(b)\nOut[7]: 139818768678472\n\nIn [8]: id(a)\nOut[8]: 139818768678472\n\nIn [9]: import pickle\n\nIn [10]: with open('/tmp/a', 'wb') as f:\n    ...:     pickle.dump(torch.float32, f)\n    ...:\n```\n"
   },
   {
      "null": 328,
      "x": "do pytorch c++ jit trace run model need more gpu memory than python env of the same model?",
      "z": "It is possible, if you model has a bunch of in-place ops like relu_. We are adding mutability to JIT so this will be fixed eventually. cc @zou3519  ",
      "y": "It is possible, if you model has a bunch of in-place ops like relu_. We are adding mutability to JIT so this will be fixed eventually. cc @zou3519  "
   },
   {
      "null": 329,
      "x": "the device of tensor can not be change",
      "z": "> As I said in your previous issue `.to` returns new tensors, so you need to re-assign results of these two lines\n> \n> ```\n>         lp07.to(device)\n>         lp14.to(device)\n> ```\nthanks for answering my question. i figure out it by modify my code:\n`\n            lp07 = lp07.to(device)\n            lp14 = lp14.to(device)\n`\nbut i don't know why should i do this. In pytorch 0.3.0, the tensor just use:\n`tensor.cuda()`",
      "y": "> As I said in your previous issue `.to` returns new tensors, so you need to re-assign results of these two lines\n> \n> ```\n>         lp07.to(device)\n>         lp14.to(device)\n> ```\nthanks for answering my question. i figure out it by modify my code:\n`\n            lp07 = lp07.to(device)\n            lp14 = lp14.to(device)\n`\nbut i don't know why should i do this. In pytorch 0.3.0, the tensor just use:\n`tensor.cuda()`"
   },
   {
      "null": 330,
      "x": "torch.full and torch.randint are inconsistent in arg order",
      "z": "I think the intention was probably to keep PyTorch's syntax aligned with NumPy's syntax, because this is how NumPy does it too.",
      "y": "I think the intention was probably to keep PyTorch's syntax aligned with NumPy's syntax, because this is how NumPy does it too."
   },
   {
      "null": 331,
      "x": "JIT pickler should support both little endian and big endian systems",
      "z": "Endianness is also part of the pickle format itself and should be handled on loading / saving (the binary output from a big endian or little endian system should be the same), so if there are issues with endianness in the pickler that should be considered a bug in our implementation",
      "y": "Endianness is also part of the pickle format itself and should be handled on loading / saving (the binary output from a big endian or little endian system should be the same), so if there are issues with endianness in the pickler that should be considered a bug in our implementation"
   },
   {
      "null": 332,
      "x": "Improve the way RPC unit tests are skipped for windows",
      "z": "I think `@unittest.skipIf(not dist.is_available())` would be much cleaner and we should probably add it to the test class rather than functions. Regarding the try-catch, why do we need that in the test file? The imports don't throw at the moment.",
      "y": "I think `@unittest.skipIf(not dist.is_available())` would be much cleaner and we should probably add it to the test class rather than functions. Regarding the try-catch, why do we need that in the test file? The imports don't throw at the moment."
   },
   {
      "null": 333,
      "x": "Build pytorch from source in osx",
      "z": "I've installed Pytorch from source on Mac OS successfully. You can take a look at my tutorial: https://zhaoyu.li/post/install-pytorch-on-mac-with-nvidia-gpu/",
      "y": "I've installed Pytorch from source on Mac OS successfully. You can take a look at my tutorial: https://zhaoyu.li/post/install-pytorch-on-mac-with-nvidia-gpu/"
   },
   {
      "null": 334,
      "x": "Python crash during backward",
      "z": "Resolved in latest builds.",
      "y": "Resolved in latest builds."
   },
   {
      "null": 335,
      "x": "Upsampling is not implemented for 1D (temporal) inputs",
      "z": "i think i know this.\n\n```\npip uninstall torch\npip uninstall torch\npython setup.py clean\npython setup.py build develop\n```",
      "y": "i think i know this.\n\n```\npip uninstall torch\npip uninstall torch\npython setup.py clean\npython setup.py build develop\n```"
   },
   {
      "null": 336,
      "x": "With backward method of CUDA Variables, grad is None if gradient is supplied",
      "z": "try this:\n```\n_x = Variable(torch.ones(10), requires_grad=True)\nx = _x.cuda()\ny = x * Variable(torch.linspace(1, 10, 10), requires_grad=False).cuda()\ny.backward(torch.ones(10).cuda())\nprint(x.grad)\nprint(_x.grad)\n```\nThis is because x is an intermediate node.\nAlternative is:\n```\nx = Variable(torch.ones(10).cuda(), requires_grad=True)\ny = x * Variable(torch.linspace(1, 10, 10), requires_grad=False).cuda()\ny.backward(torch.ones(10).cuda())\nprint(x.grad)\n```",
      "y": "try this:\n```\n_x = Variable(torch.ones(10), requires_grad=True)\nx = _x.cuda()\ny = x * Variable(torch.linspace(1, 10, 10), requires_grad=False).cuda()\ny.backward(torch.ones(10).cuda())\nprint(x.grad)\nprint(_x.grad)\n```\nThis is because x is an intermediate node.\nAlternative is:\n```\nx = Variable(torch.ones(10).cuda(), requires_grad=True)\ny = x * Variable(torch.linspace(1, 10, 10), requires_grad=False).cuda()\ny.backward(torch.ones(10).cuda())\nprint(x.grad)\n```"
   },
   {
      "null": 337,
      "x": "from torch._C import * ImportError: numpy.core.multiarray failed to import",
      "z": "$ pip install numpy -I \nhas worked on me, I'll close the issue. But I don't know why does $ pip install numpy --upgrade not work.\n(py35) user@user-ASUS:~$ pip install numpy -I  \nCollecting numpy\n  Using cached numpy-1.13.1-cp35-cp35m-manylinux1_x86_64.whl\nInstalling collected packages: numpy\nSuccessfully installed numpy-1.13.1\n\n(py35) user@user-ASUS:~$ python\nPython 3.5.3 |Continuum Analytics, Inc.| (default, Mar  6 2017, 11:58:13) \n[GCC 4.4.7 20120313 (Red Hat 4.4.7-1)] on linux\nType \"help\", \"copyright\", \"credits\" or \"license\" for more information.\n>>> import torch\n>>> x = torch.Tensor(2,3)\n>>> x\n\n 1.6534e+05  4.5673e-41  1.3032e-37\n 0.0000e+00  4.4842e-44  0.0000e+00\n[torch.FloatTensor of size 2x3]\n",
      "y": "$ pip install numpy -I \nhas worked on me, I'll close the issue. But I don't know why does $ pip install numpy --upgrade not work.\n(py35) user@user-ASUS:~$ pip install numpy -I  \nCollecting numpy\n  Using cached numpy-1.13.1-cp35-cp35m-manylinux1_x86_64.whl\nInstalling collected packages: numpy\nSuccessfully installed numpy-1.13.1\n\n(py35) user@user-ASUS:~$ python\nPython 3.5.3 |Continuum Analytics, Inc.| (default, Mar  6 2017, 11:58:13) \n[GCC 4.4.7 20120313 (Red Hat 4.4.7-1)] on linux\nType \"help\", \"copyright\", \"credits\" or \"license\" for more information.\n>>> import torch\n>>> x = torch.Tensor(2,3)\n>>> x\n\n 1.6534e+05  4.5673e-41  1.3032e-37\n 0.0000e+00  4.4842e-44  0.0000e+00\n[torch.FloatTensor of size 2x3]\n"
   },
   {
      "null": 338,
      "x": "tensor.rand() and uniform_() returns numbers from [0,1] (right-hand inclusive)",
      "z": "Here's a small test, reproducing device- and host-side generation, as implemented in TH/THC:\n```\n#include <curand_globals.h>\n#include <limits.h>\n#include <iostream>\n#include <iomanip>\n\nint main(){\n   uint x = UINT_MAX;\n   double d = x * CURAND_2POW32_INV_DOUBLE + (CURAND_2POW32_INV_DOUBLE/2.0f);\n   std::cout << std::setprecision(14) << \"double max \" << d << \"\\n\";\n   float f = x * CURAND_2POW32_INV + (CURAND_2POW32_INV/2.0f);\n   std::cout << \"float max \" << f << \"\\n\";\n//TH backend\n   double dc = UINT_MAX * (1.0/4294967296.0);   \n   std::cout << std::setprecision(14) << \"double max cpu \" << dc << \"\\n\"; \n   float fc = (float)(UINT_MAX * (1.0/4294967296.0));\n   std::cout << std::setprecision(14) << \"float max cpu \" << fc << \"\\n\"; \n  \n}\n```\nOnly cpu-side double generation is doing what is promised: [0, 1). cpu-side for floats of course rounds max to 1, thus generating [0 1]\ngpu-side for floats generates (0,1], as promised by curand manual, but that's not what torch manual specifies. \ngpu-side for doubles has a bug, thus generates (0,1), contrary to what's promised in the manual. \nReplacing x by (1-x) is expected to fail because of the different density of fp numbers near 0 and near 1 (thus, when 1-x is calculated for a small non-zero x, result will be rounded to 1, so (0,1] range will essentially be mapped to [0 1] range). \n",
      "y": "Here's a small test, reproducing device- and host-side generation, as implemented in TH/THC:\n```\n#include <curand_globals.h>\n#include <limits.h>\n#include <iostream>\n#include <iomanip>\n\nint main(){\n   uint x = UINT_MAX;\n   double d = x * CURAND_2POW32_INV_DOUBLE + (CURAND_2POW32_INV_DOUBLE/2.0f);\n   std::cout << std::setprecision(14) << \"double max \" << d << \"\\n\";\n   float f = x * CURAND_2POW32_INV + (CURAND_2POW32_INV/2.0f);\n   std::cout << \"float max \" << f << \"\\n\";\n//TH backend\n   double dc = UINT_MAX * (1.0/4294967296.0);   \n   std::cout << std::setprecision(14) << \"double max cpu \" << dc << \"\\n\"; \n   float fc = (float)(UINT_MAX * (1.0/4294967296.0));\n   std::cout << std::setprecision(14) << \"float max cpu \" << fc << \"\\n\"; \n  \n}\n```\nOnly cpu-side double generation is doing what is promised: [0, 1). cpu-side for floats of course rounds max to 1, thus generating [0 1]\ngpu-side for floats generates (0,1], as promised by curand manual, but that's not what torch manual specifies. \ngpu-side for doubles has a bug, thus generates (0,1), contrary to what's promised in the manual. \nReplacing x by (1-x) is expected to fail because of the different density of fp numbers near 0 and near 1 (thus, when 1-x is calculated for a small non-zero x, result will be rounded to 1, so (0,1] range will essentially be mapped to [0 1] range). \n"
   },
   {
      "null": 339,
      "x": "tensor[...,None] adds unit axis to wrong dimension (inconsistent with numpy)",
      "z": "fixed in master. soon to be in nightlies that we're building.",
      "y": "fixed in master. soon to be in nightlies that we're building."
   },
   {
      "null": 340,
      "x": "How to extract middle layer features",
      "z": "pytorch is built around programs, not graphs. copying/modifying a forward function and returning the particular layer you want is the right way to do things. If you want to return a middle layer more conveniently, write your model `forward` to take a `name` string as well, and return that `name` layer.",
      "y": "pytorch is built around programs, not graphs. copying/modifying a forward function and returning the particular layer you want is the right way to do things. If you want to return a middle layer more conveniently, write your model `forward` to take a `name` string as well, and return that `name` layer."
   },
   {
      "null": 341,
      "x": "topk cudaerror traceback...",
      "z": "This is not related to topk itself, but because the operation is pretty intense / taking too long on your GPU.\nThe NVIDIA driver realizes that the operation is taking too long, and **because an active display is also attached to this GPU**, it kicks out the CUDA kernel.\n\nThis thread gives more context: https://devtalk.nvidia.com/default/topic/483643/cuda-the-launch-timed-out-and-was-terminated/\n\nThere's not a whole lot we can do from the pytorch end.\n\nWhat GPU do you have? Is it an option to not turn on the display on the GPU (maybe use this GPU purely over SSH). Is it an option to run your code on a second gpu (if you have two GPUs)",
      "y": "This is not related to topk itself, but because the operation is pretty intense / taking too long on your GPU.\nThe NVIDIA driver realizes that the operation is taking too long, and **because an active display is also attached to this GPU**, it kicks out the CUDA kernel.\n\nThis thread gives more context: https://devtalk.nvidia.com/default/topic/483643/cuda-the-launch-timed-out-and-was-terminated/\n\nThere's not a whole lot we can do from the pytorch end.\n\nWhat GPU do you have? Is it an option to not turn on the display on the GPU (maybe use this GPU purely over SSH). Is it an option to run your code on a second gpu (if you have two GPUs)"
   },
   {
      "null": 342,
      "x": "Variable input size training is slow",
      "z": "do you set `cudnn.benchmark=True` anywhere in your code? that is probably the culprit.",
      "y": "do you set `cudnn.benchmark=True` anywhere in your code? that is probably the culprit."
   },
   {
      "null": 343,
      "x": "Can't import saved pytorch model",
      "z": "Update:\n\nI finally figure it out the way to make it right.\nsolution for those who want to use SDN pox controller:\nadd the \"hparamDict\" definition before boot() function in the ~/any/pox/path/pox.py\n\nI am wondering if there is a way to avoid this? I mean avoiding copy the definition source code when load model. I save my model with this method \"torch.save(model.state_dict())\".\nThe attribute missing is actually a dict class I define for hyperparameters.\n\n=========================================================\nHi, guys. any solutions?\n\nI got the same problem. In fact, I tried to use pytorch with SDN controller POX. I can not load model because of the similar problem. \nSaying \"'module' object has no attribute 'hparamDict'\".\n\nI am very frustrated. I import and add the code of definition just right before torch.load(*). If this can not be sovled I have to reimplement all my experiments in tensorflow and give up pytorch, which is quite painful.\nThe command I use is \"./pox.py pytorch_model\"\n",
      "y": "Update:\n\nI finally figure it out the way to make it right.\nsolution for those who want to use SDN pox controller:\nadd the \"hparamDict\" definition before boot() function in the ~/any/pox/path/pox.py\n\nI am wondering if there is a way to avoid this? I mean avoiding copy the definition source code when load model. I save my model with this method \"torch.save(model.state_dict())\".\nThe attribute missing is actually a dict class I define for hyperparameters.\n\n=========================================================\nHi, guys. any solutions?\n\nI got the same problem. In fact, I tried to use pytorch with SDN controller POX. I can not load model because of the similar problem. \nSaying \"'module' object has no attribute 'hparamDict'\".\n\nI am very frustrated. I import and add the code of definition just right before torch.load(*). If this can not be sovled I have to reimplement all my experiments in tensorflow and give up pytorch, which is quite painful.\nThe command I use is \"./pox.py pytorch_model\"\n"
   },
   {
      "null": 344,
      "x": "LSTM architecture has met a explosive growth in the training process",
      "z": "I had this problem just earlier today and I was implementing an RNN as well.  it went away  when i properly detatched my hidden states using h = h.detach() ; c = c.detach(). The model was backpropagating the hidden state further than it should and it was causing massive memory usage. Not sure if thats your problem but I cant see detach or .'repackage_hidden' in your code so that could be it.",
      "y": "I had this problem just earlier today and I was implementing an RNN as well.  it went away  when i properly detatched my hidden states using h = h.detach() ; c = c.detach(). The model was backpropagating the hidden state further than it should and it was causing massive memory usage. Not sure if thats your problem but I cant see detach or .'repackage_hidden' in your code so that could be it."
   },
   {
      "null": 345,
      "x": "TypeError: Type torch.LongTensor doesn't implement stateless method mean",
      "z": "We dont implement `mean` for LongTensor.\nYou can do: `y=torch.mean(x.float())`",
      "y": "We dont implement `mean` for LongTensor.\nYou can do: `y=torch.mean(x.float())`"
   },
   {
      "null": 346,
      "x": "None Grad with Custom Loss",
      "z": "This line seems the one which interferes with the gradient:\n```\n            score_final = score_final * (score[..., i] <= 0).float()\n```",
      "y": "This line seems the one which interferes with the gradient:\n```\n            score_final = score_final * (score[..., i] <= 0).float()\n```"
   },
   {
      "null": 347,
      "x": "[request] Encode/decode variables",
      "z": "I can see two solutions to this issue:\n\n1. Having `encode` and `decode` functions on class Variable. Every time the variable is computed/stored these functions will be called.\n2. Allowing `register_hook` to accept a keyword argument `check`, which will default to `True`. if `check == True`, [python_hook.cpp#L147] will be run.\n\n[python_hook.cpp#L147]:https://github.com/pytorch/pytorch/blob/1290e586fbc3d6266423f3417723d6620267054b/torch/csrc/autograd/python_hook.cpp#L147",
      "y": "I can see two solutions to this issue:\n\n1. Having `encode` and `decode` functions on class Variable. Every time the variable is computed/stored these functions will be called.\n2. Allowing `register_hook` to accept a keyword argument `check`, which will default to `True`. if `check == True`, [python_hook.cpp#L147] will be run.\n\n[python_hook.cpp#L147]:https://github.com/pytorch/pytorch/blob/1290e586fbc3d6266423f3417723d6620267054b/torch/csrc/autograd/python_hook.cpp#L147"
   },
   {
      "null": 348,
      "x": "no module named reduction",
      "z": "Just in case anyone else stumbles into this issue like I just did:\n\nYou will see this issue if your current working directory is `site-packages/torch`, in which case any `import multiprocessing` will import torch's own conveniently named `multiprocessing` package, and not the default one.\n\n(I saw this, because I was diagnosing LD_LIBRARY_PATH issues with my wheel build of PyTorch 1.0 preview + CUDA 10.)",
      "y": "Just in case anyone else stumbles into this issue like I just did:\n\nYou will see this issue if your current working directory is `site-packages/torch`, in which case any `import multiprocessing` will import torch's own conveniently named `multiprocessing` package, and not the default one.\n\n(I saw this, because I was diagnosing LD_LIBRARY_PATH issues with my wheel build of PyTorch 1.0 preview + CUDA 10.)"
   },
   {
      "null": 349,
      "x": "Constructing a ParameterDict raises a warning",
      "z": "Thanks, I am ignoring it now with:\n\n```\nimport warnings\nwarnings.filterwarnings(\"ignore\", message=\"Setting attributes on ParameterDict is not supported.\")\n```\n\nI will wait for the new release!!",
      "y": "Thanks, I am ignoring it now with:\n\n```\nimport warnings\nwarnings.filterwarnings(\"ignore\", message=\"Setting attributes on ParameterDict is not supported.\")\n```\n\nI will wait for the new release!!"
   },
   {
      "null": 350,
      "x": "flake8 errors are not shown by github actions",
      "z": "Good catch, thanks! I'll go ahead and fix that, sorry for introducing the regression",
      "y": "Good catch, thanks! I'll go ahead and fix that, sorry for introducing the regression"
   },
   {
      "null": 351,
      "x": "RuntimeError: \"mul_cuda\" not implemented for 'Bool'",
      "z": "Maybe just changing, includeBool to `true` will work. https://github.com/pytorch/pytorch/blob/cd26d027b3357cd913412ee13060cdbb28fc178a/aten/src/ATen/native/cuda/BinaryMulDivKernel.cu#L68\n",
      "y": "Maybe just changing, includeBool to `true` will work. https://github.com/pytorch/pytorch/blob/cd26d027b3357cd913412ee13060cdbb28fc178a/aten/src/ATen/native/cuda/BinaryMulDivKernel.cu#L68\n"
   },
   {
      "null": 352,
      "x": "[POLL][RFC] Can we retire Single-Process Multi-Device Mode from DistributedDataParallel?",
      "z": "The ranking model distributed setting (where the many negatives benefits) has been de-prioritized in ParlAI due to advancements in generative models. It's acceptable for me to lose that functionality now.",
      "y": "The ranking model distributed setting (where the many negatives benefits) has been de-prioritized in ParlAI due to advancements in generative models. It's acceptable for me to lose that functionality now."
   },
   {
      "null": 353,
      "x": "Convolution operations are extremely slow on RTX 30 series GPU",
      "z": "> The binaries use cudnn8.0.3, which doesn't ship with tuned heuristics for 3090 and cudnn8.0.5 will provide them.\n> Until then performance regressions on these devices are unfortunately expected.\n\nVery thanks for your help.\ud83d\ude03\nSo the solution may be: I should compile PyTorch from source once when cuDNN 8.0.5 is available\uff1f",
      "y": "> The binaries use cudnn8.0.3, which doesn't ship with tuned heuristics for 3090 and cudnn8.0.5 will provide them.\n> Until then performance regressions on these devices are unfortunately expected.\n\nVery thanks for your help.\ud83d\ude03\nSo the solution may be: I should compile PyTorch from source once when cuDNN 8.0.5 is available\uff1f"
   },
   {
      "null": 354,
      "x": "torch.arange numerics are different after 1.7 update on CPU",
      "z": "I tried using intrinsics to see if it would avoid FMA, and it didn't work at first--but then I found a way to get it working. First, I changed from this\n\n```\n  template<typename step_t>\n  static Vec256<double> arange_index_offset(double base = 0., step_t step = static_cast<step_t>(1), int64_t index_offset = 0) {\n    return Vec256<double>(\n        base + step * index_offset,\n        base + step * (index_offset + 1),\n        base + step * (index_offset + 2),\n        base + step * (index_offset + 3));\n  }\n```\n\nto this:\n```\n  template<typename step_t>\n  static Vec256<double> arange_index_offset(double base = 0., step_t step = static_cast<step_t>(1), int64_t index_offset = 0) {\n    Vec256<double> base_v(base, base, base, base);\n    Vec256<double> step_v(step, step, step, step);\n    Vec256<double> index_v(index_offset, index_offset + 1, index_offset + 2, index_offset + 3); \n    return _mm256_add_pd(base_v, _mm256_mul_pd(step_v, index_v));\n  }\n```\n\nEvidently the `_mm256_add_pd` and `_mm256_mul_pd` calls get combined such that I'm effectively calling `_mm256_fmadd_pd`, so this didn't fix the problem. But then I wondered, what would happen if I just used the multiply intrinsic and then used regular add operations? Like this:\n\n```\n  template<typename step_t>\n  static Vec256<double> arange_index_offset(double base = 0., step_t step = static_cast<step_t>(1), int64_t index_offset = 0) {\n    Vec256<double> index_v(index_offset, index_offset + 1, index_offset + 2, index_offset + 3); \n    Vec256<double> step_v(step, step, step, step);\n    Vec256<double> tmp = _mm256_mul_pd(step_v, index_v);\n    return Vec256<double>(\n      base + tmp.values[0],\n      base + tmp.values[1],\n      base + tmp.values[2],\n      base + tmp.values[3]);\n  }\n```\n\nThis worked! Since I'm using an intrinsic only for the multiply and not the add, the two operations don't get combined, and now we get the proper behavior for the example we've been looking at:\n\n```\n$ ATEN_CPU_CAPABILITY=avx2 python\n>>> import torch\n>>> torch.arange(-5, 5, 1.4, device='cpu', dtype=torch.float64).floor()\ntensor([-5., -4., -3., -1.,  0.,  2.,  3.,  4.], dtype=torch.float64)\n>>> torch.arange(-5, 5, 1.4, device='cpu', dtype=torch.float64).storage()\n -5.0\n -3.6\n -2.2\n -0.8000000000000007\n 0.5999999999999996\n 2.0      <---- NOTE: this is exactly 2 now, and not 1.99999... like before\n 3.3999999999999986\n 4.799999999999999\n[torch.DoubleStorage of size 8]\n```\n\nThis result is exactly the same for `ATEN_CPU_CAPABILITY=avx2`, `ATEN_CPU_CAPABILITY=avx`, and `ATEN_CPU_CAPABILITY=default`. And it agrees with the Pytorch 1.6 result as well.\n\nI'll admit that it's a bit of an odd solution, but it does work. Is it alright if we go ahead with this?",
      "y": "I tried using intrinsics to see if it would avoid FMA, and it didn't work at first--but then I found a way to get it working. First, I changed from this\n\n```\n  template<typename step_t>\n  static Vec256<double> arange_index_offset(double base = 0., step_t step = static_cast<step_t>(1), int64_t index_offset = 0) {\n    return Vec256<double>(\n        base + step * index_offset,\n        base + step * (index_offset + 1),\n        base + step * (index_offset + 2),\n        base + step * (index_offset + 3));\n  }\n```\n\nto this:\n```\n  template<typename step_t>\n  static Vec256<double> arange_index_offset(double base = 0., step_t step = static_cast<step_t>(1), int64_t index_offset = 0) {\n    Vec256<double> base_v(base, base, base, base);\n    Vec256<double> step_v(step, step, step, step);\n    Vec256<double> index_v(index_offset, index_offset + 1, index_offset + 2, index_offset + 3); \n    return _mm256_add_pd(base_v, _mm256_mul_pd(step_v, index_v));\n  }\n```\n\nEvidently the `_mm256_add_pd` and `_mm256_mul_pd` calls get combined such that I'm effectively calling `_mm256_fmadd_pd`, so this didn't fix the problem. But then I wondered, what would happen if I just used the multiply intrinsic and then used regular add operations? Like this:\n\n```\n  template<typename step_t>\n  static Vec256<double> arange_index_offset(double base = 0., step_t step = static_cast<step_t>(1), int64_t index_offset = 0) {\n    Vec256<double> index_v(index_offset, index_offset + 1, index_offset + 2, index_offset + 3); \n    Vec256<double> step_v(step, step, step, step);\n    Vec256<double> tmp = _mm256_mul_pd(step_v, index_v);\n    return Vec256<double>(\n      base + tmp.values[0],\n      base + tmp.values[1],\n      base + tmp.values[2],\n      base + tmp.values[3]);\n  }\n```\n\nThis worked! Since I'm using an intrinsic only for the multiply and not the add, the two operations don't get combined, and now we get the proper behavior for the example we've been looking at:\n\n```\n$ ATEN_CPU_CAPABILITY=avx2 python\n>>> import torch\n>>> torch.arange(-5, 5, 1.4, device='cpu', dtype=torch.float64).floor()\ntensor([-5., -4., -3., -1.,  0.,  2.,  3.,  4.], dtype=torch.float64)\n>>> torch.arange(-5, 5, 1.4, device='cpu', dtype=torch.float64).storage()\n -5.0\n -3.6\n -2.2\n -0.8000000000000007\n 0.5999999999999996\n 2.0      <---- NOTE: this is exactly 2 now, and not 1.99999... like before\n 3.3999999999999986\n 4.799999999999999\n[torch.DoubleStorage of size 8]\n```\n\nThis result is exactly the same for `ATEN_CPU_CAPABILITY=avx2`, `ATEN_CPU_CAPABILITY=avx`, and `ATEN_CPU_CAPABILITY=default`. And it agrees with the Pytorch 1.6 result as well.\n\nI'll admit that it's a bit of an odd solution, but it does work. Is it alright if we go ahead with this?"
   },
   {
      "null": 355,
      "x": "`F.grid_sample` fails to dispatch correctly when args are of different subclasses",
      "z": "Many thanks for these suggestions @hameerabbasi !\n\nI'm happy to go the wrapping approach, although iirc i'd been warned off it before as being hard to get full compatibility\n\nI've tried this\n\n```\nclass TensorBase:\n    def __init__(self, data, metadata=None, **kwargs):\n        self._fa_tensor = tensor(data)\n        for k,v in kwargs.items(): setattr(self, k, v)\n    def __torch_function__(self, func, types, args=(), kwargs=None):\n        if kwargs is None: kwargs = {}\n        args = [getattr(a,'_fa_tensor',a) for a in args]\n        ret = func(*args, **kwargs)\n        return TensorBase(ret, **self.__dict__)\n    def __getattr__(self, k): return getattr(self._fa_tensor, k)\n    def __getitem__(self, k): return self._fa_tensor[k]\n```\n\nBut I'm not sure how to support stuff like `+` with an int, without doing something a bit hacky",
      "y": "Many thanks for these suggestions @hameerabbasi !\n\nI'm happy to go the wrapping approach, although iirc i'd been warned off it before as being hard to get full compatibility\n\nI've tried this\n\n```\nclass TensorBase:\n    def __init__(self, data, metadata=None, **kwargs):\n        self._fa_tensor = tensor(data)\n        for k,v in kwargs.items(): setattr(self, k, v)\n    def __torch_function__(self, func, types, args=(), kwargs=None):\n        if kwargs is None: kwargs = {}\n        args = [getattr(a,'_fa_tensor',a) for a in args]\n        ret = func(*args, **kwargs)\n        return TensorBase(ret, **self.__dict__)\n    def __getattr__(self, k): return getattr(self._fa_tensor, k)\n    def __getitem__(self, k): return self._fa_tensor[k]\n```\n\nBut I'm not sure how to support stuff like `+` with an int, without doing something a bit hacky"
   },
   {
      "null": 356,
      "x": "torch.fft does not give the same result as torch.stft",
      "z": "The difference is because `torch.fft`'s second argument isn't the transform axis, instead it's `signal_ndim` or the number of dimensions to transform. So, `torch.fft(x_torch, 3)` is actually equivalent to `scipy.fft.fftn(x_scipy, axes=(-1, -2, -3))`. For a more `numpy`-like interface use the new [`torch.fft.fft`](https://pytorch.org/docs/master/fft.html#torch.fft.fft) function, or if you're stuck with PyTorch 1.6 you can use `transpose(axis, -2)` to move the desired transform axis into the right place:\n\n```\nS_torch = torch.fft(x_torch.transpose(1, -2), signal_ndim=1).transpose(1, -2)\n```\nWhich results in:\n![image](https://user-images.githubusercontent.com/13238737/97711956-abe13d00-1ab5-11eb-9910-7fa99580663f.png)",
      "y": "The difference is because `torch.fft`'s second argument isn't the transform axis, instead it's `signal_ndim` or the number of dimensions to transform. So, `torch.fft(x_torch, 3)` is actually equivalent to `scipy.fft.fftn(x_scipy, axes=(-1, -2, -3))`. For a more `numpy`-like interface use the new [`torch.fft.fft`](https://pytorch.org/docs/master/fft.html#torch.fft.fft) function, or if you're stuck with PyTorch 1.6 you can use `transpose(axis, -2)` to move the desired transform axis into the right place:\n\n```\nS_torch = torch.fft(x_torch.transpose(1, -2), signal_ndim=1).transpose(1, -2)\n```\nWhich results in:\n![image](https://user-images.githubusercontent.com/13238737/97711956-abe13d00-1ab5-11eb-9910-7fa99580663f.png)"
   },
   {
      "null": 357,
      "x": "Legacy tensor ctor returns uninitialized tensor when input and output device differ",
      "z": "> When the input is a 0-D or 1-size tensor, behavior depends on the dtype: int32: uninitialized tensor\n\nwhat is happening here is that the constructor is interpreting the Tensor as an IntArrayRef because a 1-element integer tensor passed PyLong_Check.",
      "y": "> When the input is a 0-D or 1-size tensor, behavior depends on the dtype: int32: uninitialized tensor\n\nwhat is happening here is that the constructor is interpreting the Tensor as an IntArrayRef because a 1-element integer tensor passed PyLong_Check."
   },
   {
      "null": 358,
      "x": "Independent Distribution Wrapper Disguises Negative StdDev in Underlying Normal Distribution",
      "z": "Hi @decodyng, I think the best we can guarantee in the `torch.distributions` library is to correctly catch errors *when validation is enabled*. I believe this error would have been caught earlier if you had initially called\n```py\ntorch.distributions.Distribution.set_default_validate_args(True)\n```\nIn fact we recently [enabled validation by default](https://github.com/pyro-ppl/pyro/pull/2701) in Pyro (a downstream library). If this seems useful we could consider enabling validation by default also in PyTorch. What's your opinion?",
      "y": "Hi @decodyng, I think the best we can guarantee in the `torch.distributions` library is to correctly catch errors *when validation is enabled*. I believe this error would have been caught earlier if you had initially called\n```py\ntorch.distributions.Distribution.set_default_validate_args(True)\n```\nIn fact we recently [enabled validation by default](https://github.com/pyro-ppl/pyro/pull/2701) in Pyro (a downstream library). If this seems useful we could consider enabling validation by default also in PyTorch. What's your opinion?"
   },
   {
      "null": 359,
      "x": "torch.trace type promotion behavior is different on CPU vs CUDA",
      "z": "1.6.0:\n```\n>>> import torch\n>>> torch.__version__\n'1.6.0'\n>>> x = torch.ones(5, 5, dtype=torch.uint8)\n>>> x = x.cuda()\n>>> x.trace()\ntensor(5, device='cuda:0')\n>>> x.trace().dtype\ntorch.int64\n```\n\n1.5.1:\n```\n>>> import torch\n>>> torch.__version__\n'1.5.1'\n>>> x = torch.ones(5, 5, dtype=torch.uint8)\n>>> x = x.cuda()\n>>> x.trace()\ntensor(5, device='cuda:0', dtype=torch.uint8)\n```",
      "y": "1.6.0:\n```\n>>> import torch\n>>> torch.__version__\n'1.6.0'\n>>> x = torch.ones(5, 5, dtype=torch.uint8)\n>>> x = x.cuda()\n>>> x.trace()\ntensor(5, device='cuda:0')\n>>> x.trace().dtype\ntorch.int64\n```\n\n1.5.1:\n```\n>>> import torch\n>>> torch.__version__\n'1.5.1'\n>>> x = torch.ones(5, 5, dtype=torch.uint8)\n>>> x = x.cuda()\n>>> x.trace()\ntensor(5, device='cuda:0', dtype=torch.uint8)\n```"
   },
   {
      "null": 360,
      "x": "Jit Error with CUDA and FP16 -- identifier \"aten_add_flat__1\" is undefined",
      "z": "The fix for this wasn't cherry picked into 1.7.1.  It should be in nightly tho.",
      "y": "The fix for this wasn't cherry picked into 1.7.1.  It should be in nightly tho."
   },
   {
      "null": 361,
      "x": "Distributed weight updates",
      "z": "@jianjiandandande \n\nAfter calling `loss.backward()`, can you try the following?\n\n```python\nfor name, param in m.named_parameters():\n  if not param.grad:\n    print(f\"detected unused parameter: {name}\")\n```\n\nThis should tell you what parameters are not used. \n\nBTW, any reason for not using [`DistributedDataParallel`](https://pytorch.org/docs/stable/notes/ddp.html) and set `find_unused_parameters=True`? \n",
      "y": "@jianjiandandande \n\nAfter calling `loss.backward()`, can you try the following?\n\n```python\nfor name, param in m.named_parameters():\n  if not param.grad:\n    print(f\"detected unused parameter: {name}\")\n```\n\nThis should tell you what parameters are not used. \n\nBTW, any reason for not using [`DistributedDataParallel`](https://pytorch.org/docs/stable/notes/ddp.html) and set `find_unused_parameters=True`? \n"
   },
   {
      "null": 362,
      "x": "How to add PyTorch to requirements.txt",
      "z": "Looks like this works:\n\n```\n--find-links https://download.pytorch.org/whl/torch_stable.html\ntorch==1.3.1+cpu\n```\n\nThanks for the help!",
      "y": "Looks like this works:\n\n```\n--find-links https://download.pytorch.org/whl/torch_stable.html\ntorch==1.3.1+cpu\n```\n\nThanks for the help!"
   },
   {
      "null": 363,
      "x": "Use non-system cuda path",
      "z": "@mahmoodn I assume you are building from source right? \nTry setting `CUDA_HOME` env?",
      "y": "@mahmoodn I assume you are building from source right? \nTry setting `CUDA_HOME` env?"
   },
   {
      "null": 364,
      "x": "list of registered buffers does not move to cuda",
      "z": "@xonobo \nSo if you do `print(list(a.buffers()))` you'll see your buffer are on CUDA device. \n`a.params` as an attribute will be copied when `register_buffer` and `params` won't move along with `.to()`. But the buffer does. \nPlease let us know if this doesn't solve your question. Thanks!",
      "y": "@xonobo \nSo if you do `print(list(a.buffers()))` you'll see your buffer are on CUDA device. \n`a.params` as an attribute will be copied when `register_buffer` and `params` won't move along with `.to()`. But the buffer does. \nPlease let us know if this doesn't solve your question. Thanks!"
   },
   {
      "null": 365,
      "x": "Memory leak with Conv1d on CPU",
      "z": "Wow, indeed, `LRU_CACHE_CAPACITY=1` (from https://github.com/pytorch/pytorch/issues/27971) solves my issue!\n\nThanks a lot for pointing out, @ezyang!",
      "y": "Wow, indeed, `LRU_CACHE_CAPACITY=1` (from https://github.com/pytorch/pytorch/issues/27971) solves my issue!\n\nThanks a lot for pointing out, @ezyang!"
   },
   {
      "null": 366,
      "x": "nn.Transformer.generate_square_subsequent_mask does not behave as expected",
      "z": "It's an old issue related to type promotion https://github.com/pytorch/pytorch/pull/28231 and has been fixed by v.1.3.1. Please update your pytorch with the latest binary package or master branch. Feel free to re-open the issue if you still have questions.",
      "y": "It's an old issue related to type promotion https://github.com/pytorch/pytorch/pull/28231 and has been fixed by v.1.3.1. Please update your pytorch with the latest binary package or master branch. Feel free to re-open the issue if you still have questions."
   },
   {
      "null": 367,
      "x": "Vanilla Resnet50 Not Computing on iOS via Libtorch (Pytorch Mobile)",
      "z": "Hi Hussain, \n\nAs we've discussed in the PyTorch forum, this is a known issue, and we've fixed it - #29885  .If you want to try out the fix, feel free to pull the latest code from master and re-compile the static libraries from source code. Note that the fix will be available in the next release of Cocoapods.",
      "y": "Hi Hussain, \n\nAs we've discussed in the PyTorch forum, this is a known issue, and we've fixed it - #29885  .If you want to try out the fix, feel free to pull the latest code from master and re-compile the static libraries from source code. Note that the fix will be available in the next release of Cocoapods."
   },
   {
      "null": 368,
      "x": "Error when loading model with traced `to` call",
      "z": "@sysuzyq This looks like a forward-compatibility issue, i.e. the version of PyTorch you use to save your model is newer than the libtorch version. Can you check if this works if these versions match?",
      "y": "@sysuzyq This looks like a forward-compatibility issue, i.e. the version of PyTorch you use to save your model is newer than the libtorch version. Can you check if this works if these versions match?"
   },
   {
      "null": 369,
      "x": "died with <Signals.SIGSEGV: 11>",
      "z": "Try Python 3.6.",
      "y": "Try Python 3.6."
   },
   {
      "null": 370,
      "x": "Memory leak when evaluating model on CPU with dynamic size tensor input.",
      "z": "os.environ['LRU_CACHE_CAPACITY'] = '1' also work for me, THANKS!",
      "y": "os.environ['LRU_CACHE_CAPACITY'] = '1' also work for me, THANKS!"
   },
   {
      "null": 371,
      "x": "torch.nn.parallel.DistributedDataParallel is slow on backpropagation",
      "z": "A difference between Apex and PyTorch DDP that comes to mind here is that Apex figures out the order in which gradients are produced at runtime, whereas PyTorch DDP still assumes that gradients are produced in reverse order how they are defined in the `nn.Module`. If the model you link to defines some first encoders / MLPs / etc as the last parameters in the module, you would see head-of-line blocking where all reductions will be sequenced after the final gradients, effectively removing all opportunity for overlapping reduction with gradient computation.",
      "y": "A difference between Apex and PyTorch DDP that comes to mind here is that Apex figures out the order in which gradients are produced at runtime, whereas PyTorch DDP still assumes that gradients are produced in reverse order how they are defined in the `nn.Module`. If the model you link to defines some first encoders / MLPs / etc as the last parameters in the module, you would see head-of-line blocking where all reductions will be sequenced after the final gradients, effectively removing all opportunity for overlapping reduction with gradient computation."
   },
   {
      "null": 372,
      "x": "Illegal instruction : 4",
      "z": "YESSSSSS !!\nThanks a lot.\n\nHere it is :\n\n(base) iMac27:miniconda3 xxxxxx$ **conda install --update-all pytorch-nightly torchvision -c pytorch**\nCollecting package metadata (current_repodata.json): done\nSolving environment: done\n\n## Package Plan ##\n\n  environment location: /Users/xxxxxx/miniconda3\n\n  added / updated specs:\n    - pytorch-nightly\n    - torchvision\n\n\nThe following packages will be downloaded:\n\n    package                    |            build\n    ---------------------------|-----------------\n    **pytorch-nightly-1.0.0.dev20190328**|          py3.7_0        45.3 MB  pytorch\n    tqdm-4.38.0                |             py_0          51 KB\n    ------------------------------------------------------------\n                                           Total:        45.4 MB\n\nThe following NEW packages will be INSTALLED:\n\n  pytorch-nightly    pytorch/osx-64::pytorch-nightly-1.0.0.dev20190328-py3.7_0\n\nThe following packages will be UPDATED:\n\n  tqdm                                          4.36.1-py_0 --> 4.38.0-py_0\n\n\nProceed ([y]/n)? y\n\n\nDownloading and Extracting Packages\npytorch-nightly-1.0. | 45.3 MB   | #################################################################################### | 100% \ntqdm-4.38.0          | 51 KB     | #################################################################################### | 100% \nPreparing transaction: done\nVerifying transaction: done\nExecuting transaction: done\n(base) iMac27:miniconda3 xxxxxx$ python3\nPython 3.7.5 (default, Oct 25 2019, 10:52:18) \n[Clang 4.0.1 (tags/RELEASE_401/final)] :: Anaconda, Inc. on darwin\nType \"help\", \"copyright\", \"credits\" or \"license\" for more information.\n>>> import torch\n>>> \n(base) iMac27:miniconda3 xxxxxx$ \n",
      "y": "YESSSSSS !!\nThanks a lot.\n\nHere it is :\n\n(base) iMac27:miniconda3 xxxxxx$ **conda install --update-all pytorch-nightly torchvision -c pytorch**\nCollecting package metadata (current_repodata.json): done\nSolving environment: done\n\n## Package Plan ##\n\n  environment location: /Users/xxxxxx/miniconda3\n\n  added / updated specs:\n    - pytorch-nightly\n    - torchvision\n\n\nThe following packages will be downloaded:\n\n    package                    |            build\n    ---------------------------|-----------------\n    **pytorch-nightly-1.0.0.dev20190328**|          py3.7_0        45.3 MB  pytorch\n    tqdm-4.38.0                |             py_0          51 KB\n    ------------------------------------------------------------\n                                           Total:        45.4 MB\n\nThe following NEW packages will be INSTALLED:\n\n  pytorch-nightly    pytorch/osx-64::pytorch-nightly-1.0.0.dev20190328-py3.7_0\n\nThe following packages will be UPDATED:\n\n  tqdm                                          4.36.1-py_0 --> 4.38.0-py_0\n\n\nProceed ([y]/n)? y\n\n\nDownloading and Extracting Packages\npytorch-nightly-1.0. | 45.3 MB   | #################################################################################### | 100% \ntqdm-4.38.0          | 51 KB     | #################################################################################### | 100% \nPreparing transaction: done\nVerifying transaction: done\nExecuting transaction: done\n(base) iMac27:miniconda3 xxxxxx$ python3\nPython 3.7.5 (default, Oct 25 2019, 10:52:18) \n[Clang 4.0.1 (tags/RELEASE_401/final)] :: Anaconda, Inc. on darwin\nType \"help\", \"copyright\", \"credits\" or \"license\" for more information.\n>>> import torch\n>>> \n(base) iMac27:miniconda3 xxxxxx$ \n"
   },
   {
      "null": 373,
      "x": "ONNX export failed: Couldn't export operator aten::upsample_bilinear2d",
      "z": "Fixed after adding \n\ntorch.onnx.export(..,opset_version=11)\n",
      "y": "Fixed after adding \n\ntorch.onnx.export(..,opset_version=11)\n"
   },
   {
      "null": 374,
      "x": "Add SWA to PyTorch mainline",
      "z": "In general, we focus on including methods that the community uses as a standard, or else the code maintenance problem balloons up for us. We do show discretion based on what the paper shows as evidence, for example BatchNorm was included within weeks of it's publish date (in Torch). In terms of rejected methods, we've rejected newly minted papers such as Swish (#3260, #3182), [Yellowfin](https://github.com/pytorch/pytorch/issues/1960) and many others that haven't become standardized in the community (like LSTM/Transformer/BatchNorm).\n\nIn this case, since at least a year has passed following the addition in contrib, the github repo and the paper have gathered momentum, I do agree with bringing SWA in pytorch mainline. Would you like to open a pull request with the relevant code, algorithm and tests?",
      "y": "In general, we focus on including methods that the community uses as a standard, or else the code maintenance problem balloons up for us. We do show discretion based on what the paper shows as evidence, for example BatchNorm was included within weeks of it's publish date (in Torch). In terms of rejected methods, we've rejected newly minted papers such as Swish (#3260, #3182), [Yellowfin](https://github.com/pytorch/pytorch/issues/1960) and many others that haven't become standardized in the community (like LSTM/Transformer/BatchNorm).\n\nIn this case, since at least a year has passed following the addition in contrib, the github repo and the paper have gathered momentum, I do agree with bringing SWA in pytorch mainline. Would you like to open a pull request with the relevant code, algorithm and tests?"
   },
   {
      "null": 375,
      "x": "The Bug of onnx model format exported by torch.onnx.export",
      "z": "For exporting a model to ONNX, you should not set the export type to OperatorExportTypes.ONNX_ATEN.\nIs there a specific reason you're using a different export type?\n\nTo try out the repro code, I added the code below to read the image:\n\n```\ninput_img = osp.join(\n            osp.dirname(__file__), '../tests/data/color.jpg')\noriginal_image = mmcv.imread(input_img)\noriginal_image = torch.from_numpy(original_image)\n```\n\nWith this code, I two errors in exporter regarding missing symbolics:\n1- Unsupported: ONNX export of roi_align with aligned=True\n2-  Exporting the operator new_empty to ONNX opset version 12 is not supported\n\nCan you please confirm if you see a similar behavior with pytorch 1.6 and detectron2 build from source?\n\nAlso, is there a reason you're exporting this model to external data format?\nRunning ONNX checker on models in external data format might be a bit different.\nI tried bypassing the issue with symbolics (1- set aligned=False, and inserted a symbolic for new_empty),\nand then tried export and checker with: onnx.checker.check_model(\"mask_rcnn.onnx\")\nWhich seems to work and pass checker successfully.",
      "y": "For exporting a model to ONNX, you should not set the export type to OperatorExportTypes.ONNX_ATEN.\nIs there a specific reason you're using a different export type?\n\nTo try out the repro code, I added the code below to read the image:\n\n```\ninput_img = osp.join(\n            osp.dirname(__file__), '../tests/data/color.jpg')\noriginal_image = mmcv.imread(input_img)\noriginal_image = torch.from_numpy(original_image)\n```\n\nWith this code, I two errors in exporter regarding missing symbolics:\n1- Unsupported: ONNX export of roi_align with aligned=True\n2-  Exporting the operator new_empty to ONNX opset version 12 is not supported\n\nCan you please confirm if you see a similar behavior with pytorch 1.6 and detectron2 build from source?\n\nAlso, is there a reason you're exporting this model to external data format?\nRunning ONNX checker on models in external data format might be a bit different.\nI tried bypassing the issue with symbolics (1- set aligned=False, and inserted a symbolic for new_empty),\nand then tried export and checker with: onnx.checker.check_model(\"mask_rcnn.onnx\")\nWhich seems to work and pass checker successfully."
   },
   {
      "null": 376,
      "x": "How to use and debug mixed-precision in 1.6.0 ?",
      "z": "You should check if Tensor cores are used at all, there are some dimension requirements on batch size and number of hidden units. With apex it was done using for example: https://github.com/NVIDIA/apex/tree/master/apex/pyprof. \n\nBtw, while being advertised,  mixed precision is not quite there yet, e.g. RNN modules/cells don't work at all see:\n\n1. https://github.com/pytorch/pytorch/issues/42605\n2. https://github.com/pytorch/pytorch/issues/36428\n\nAlso you mentioned data parallelism and I'm not sure to which parallelism model are you referring to, I think it doesn't work with DataParallel, it only works with DDP (I'm not 100% sure for this, there were some problems, maybe it is fixed now).\n\nFinally, here you have some pointers for debugging at the end of slides: https://nvlabs.github.io/iccv2019-mixed-precision-tutorial/files/dusan_stosic_intro_to_mixed_precision_training.pdf.",
      "y": "You should check if Tensor cores are used at all, there are some dimension requirements on batch size and number of hidden units. With apex it was done using for example: https://github.com/NVIDIA/apex/tree/master/apex/pyprof. \n\nBtw, while being advertised,  mixed precision is not quite there yet, e.g. RNN modules/cells don't work at all see:\n\n1. https://github.com/pytorch/pytorch/issues/42605\n2. https://github.com/pytorch/pytorch/issues/36428\n\nAlso you mentioned data parallelism and I'm not sure to which parallelism model are you referring to, I think it doesn't work with DataParallel, it only works with DDP (I'm not 100% sure for this, there were some problems, maybe it is fixed now).\n\nFinally, here you have some pointers for debugging at the end of slides: https://nvlabs.github.io/iccv2019-mixed-precision-tutorial/files/dusan_stosic_intro_to_mixed_precision_training.pdf."
   },
   {
      "null": 377,
      "x": "after updating to pytorch 1.6 mypy does not recognise the tensor attributes ndim, nonzero and T ",
      "z": "`ndim` was fixed yesterday (gh-42908), `T` was already present in master for longer, `nonzero` is still missing.",
      "y": "`ndim` was fixed yesterday (gh-42908), `T` was already present in master for longer, `nonzero` is still missing."
   },
   {
      "null": 378,
      "x": "How to use torch.utils.checkpoint and DistributedDataParallel together",
      "z": "Hey @devilztt if you are manually synchronizing gradients, then you don't need DDP anymore. \n\n```python\ninit_process_group(...)\nmodel = MyModel(...)\nmodel(inputs).sum().backward()\nworks = []\nfor p in model.parameters():\n    # to speed it up, you can also organize grads to larger buckets to make allreduce more efficient\n    works.append(dist.all_reduce(p.grad, async_op=True))\nfor work in works:\n    work.wait()\n...\n```",
      "y": "Hey @devilztt if you are manually synchronizing gradients, then you don't need DDP anymore. \n\n```python\ninit_process_group(...)\nmodel = MyModel(...)\nmodel(inputs).sum().backward()\nworks = []\nfor p in model.parameters():\n    # to speed it up, you can also organize grads to larger buckets to make allreduce more efficient\n    works.append(dist.all_reduce(p.grad, async_op=True))\nfor work in works:\n    work.wait()\n...\n```"
   },
   {
      "null": 379,
      "x": "complex32 seems to be doing very very weird things on CPU",
      "z": "Currently there is almost no support for `torch.complex32`, so yeah we should disable it.",
      "y": "Currently there is almost no support for `torch.complex32`, so yeah we should disable it."
   },
   {
      "null": 380,
      "x": "[jit] TorchScript does not work with Python coverage package",
      "z": "Hi @janeyx99, is this resolved by your recent code coverage enhancement for TorchScript? ",
      "y": "Hi @janeyx99, is this resolved by your recent code coverage enhancement for TorchScript? "
   },
   {
      "null": 381,
      "x": "Trying to get pytorch working for the first time",
      "z": "Looks like you've installed pytorch with cuda 7.5, and it is trying to jit the code for you 1080 card. When you set device to 1, you are running on 610, which has compute capability 3.0 and most likely not supported. Try installing cuda 8 version of pytorch (conda install pytorch torchvision cuda80 -c soumith)",
      "y": "Looks like you've installed pytorch with cuda 7.5, and it is trying to jit the code for you 1080 card. When you set device to 1, you are running on 610, which has compute capability 3.0 and most likely not supported. Try installing cuda 8 version of pytorch (conda install pytorch torchvision cuda80 -c soumith)"
   },
   {
      "null": 382,
      "x": "Feature Request: Add Pixel Unshuffle",
      "z": "The implementation from the topic starter is quite efficient already. I haven't seen it being used too often and usually, it's only used once at the beginning of the network. So benefits from adding it to the core are questionable. If you need class you could copy-paste this:\n```python\nclass SpaceToDepth(nn.Module):\n    def __init__(self, block_size=4):\n        super().__init__()\n        assert block_size in {2, 4}, \"Space2Depth only supports blocks size = 4 or 2\"\n        self.block_size = block_size\n\n    def forward(self, x):\n        N, C, H, W = x.size()\n        S = self.block_size\n        x = x.view(N, C, H // S, S, W // S, S)  # (N, C, H//bs, bs, W//bs, bs)\n        x = x.permute(0, 3, 5, 1, 2, 4).contiguous()  # (N, bs, bs, C, H//bs, W//bs)\n        x = x.view(N, C * S * S, H // S, W // S)  # (N, C*bs^2, H//bs, W//bs)\n        return x\n\n    def extra_repr(self):\n        return f\"block_size={self.block_size}\"\n```",
      "y": "The implementation from the topic starter is quite efficient already. I haven't seen it being used too often and usually, it's only used once at the beginning of the network. So benefits from adding it to the core are questionable. If you need class you could copy-paste this:\n```python\nclass SpaceToDepth(nn.Module):\n    def __init__(self, block_size=4):\n        super().__init__()\n        assert block_size in {2, 4}, \"Space2Depth only supports blocks size = 4 or 2\"\n        self.block_size = block_size\n\n    def forward(self, x):\n        N, C, H, W = x.size()\n        S = self.block_size\n        x = x.view(N, C, H // S, S, W // S, S)  # (N, C, H//bs, bs, W//bs, bs)\n        x = x.permute(0, 3, 5, 1, 2, 4).contiguous()  # (N, bs, bs, C, H//bs, W//bs)\n        x = x.view(N, C * S * S, H // S, W // S)  # (N, C*bs^2, H//bs, W//bs)\n        return x\n\n    def extra_repr(self):\n        return f\"block_size={self.block_size}\"\n```"
   },
   {
      "null": 383,
      "x": "Timeout option for parallel DataLoader",
      "z": "We have switched to using `mp.Queue` (not `SimpleQueue`) now. I tested the above code on master. Interestingly, it makes the worker segfault. It works with `num_workers=0`.\n\nI adapted as following to make it run:\n```\nimport torch.utils.data\n\nclass Dataset(object):\n  def __len__(self):\n    return 100\n\n  def __getitem__(self, i):\n    return list(range(100000))\n\n\nclass Sampler(torch.utils.data.Sampler):\n  def __iter__(self):\n    return (range(100000) for batch_ind in range(100))\n\n  def __len__(self):\n    return 100\n\nd = torch.utils.data.DataLoader(dataset = Dataset(), sampler = Sampler(None), num_workers = 0)\nfor i, x in enumerate(d):\n    print(i)\n    \n```",
      "y": "We have switched to using `mp.Queue` (not `SimpleQueue`) now. I tested the above code on master. Interestingly, it makes the worker segfault. It works with `num_workers=0`.\n\nI adapted as following to make it run:\n```\nimport torch.utils.data\n\nclass Dataset(object):\n  def __len__(self):\n    return 100\n\n  def __getitem__(self, i):\n    return list(range(100000))\n\n\nclass Sampler(torch.utils.data.Sampler):\n  def __iter__(self):\n    return (range(100000) for batch_ind in range(100))\n\n  def __len__(self):\n    return 100\n\nd = torch.utils.data.DataLoader(dataset = Dataset(), sampler = Sampler(None), num_workers = 0)\nfor i, x in enumerate(d):\n    print(i)\n    \n```"
   },
   {
      "null": 384,
      "x": "numpy like tensor.all and tensor.any",
      "z": "As a note, `any` and `all` exist on ByteTensors, but do not appear in online documentation.",
      "y": "As a note, `any` and `all` exist on ByteTensors, but do not appear in online documentation."
   },
   {
      "null": 385,
      "x": "Bad error message when concatting different type Tensor(Variable)",
      "z": "FYI, the same misleading message happens when trying to concat a Variable with a Tensor.\n\n> TypeError: cat received an invalid combination of arguments - got (list), but expected one of:\n>  * (sequence[torch.cuda.FloatTensor] seq)\n>       didn't match because some of the arguments have invalid types: (list)\n>  * (sequence[torch.cuda.FloatTensor] seq, int dim)\n> ",
      "y": "FYI, the same misleading message happens when trying to concat a Variable with a Tensor.\n\n> TypeError: cat received an invalid combination of arguments - got (list), but expected one of:\n>  * (sequence[torch.cuda.FloatTensor] seq)\n>       didn't match because some of the arguments have invalid types: (list)\n>  * (sequence[torch.cuda.FloatTensor] seq, int dim)\n> "
   },
   {
      "null": 386,
      "x": "Feature request: nn.View",
      "z": "You could define a module like this:\n```\nclass View(nn.Module):\n       def __init__(self):\n            super(View, self).__init__()\n   \n        def forward(self, x):\n            return x.view(-1) \n```\n\nThis flattens the input but similarly you could provide a size object.\n\nNow you can use this as part of the model.",
      "y": "You could define a module like this:\n```\nclass View(nn.Module):\n       def __init__(self):\n            super(View, self).__init__()\n   \n        def forward(self, x):\n            return x.view(-1) \n```\n\nThis flattens the input but similarly you could provide a size object.\n\nNow you can use this as part of the model."
   },
   {
      "null": 387,
      "x": "function expand_as() works incorrectly on latest Pytorch 0.2.0_1",
      "z": "Replace sum(1) with sum(1, keepdim=True)\n\nThis is caused by the change of sum in 0.2.0.",
      "y": "Replace sum(1) with sum(1, keepdim=True)\n\nThis is caused by the change of sum in 0.2.0."
   },
   {
      "null": 388,
      "x": "PyTorch 0.2.0_1 Freezes at nn.Conv2d()",
      "z": "Just to Update:\n\nIt works fine if we use 'spawn' start method.\n\nUpdated Snippet:\n\n```\nimport torch\nimport torch.nn as nn\nimport torch.nn.functional as F\nfrom torch.autograd import Variable\nimport torch.multiprocessing as mp\n\n\nclass Net(nn.Module):\n    def __init__(self, input_size):\n        super(Net, self).__init__()\n        self.conv1 = nn.Conv2d(input_size, 32, 3, stride=2, padding=1)\n\n    def forward(self, input):\n        print('Before Conv1 call')\n        x = F.elu(self.conv1(input))\n        print('After Conv1 call')\n        return x\n\n\ndef train():\n    net = Net(1)\n    net(Variable(torch.randn(1, 80, 80).unsqueeze(0)))\n    print('Passed!')\n\n\nif __name__ == '__main__':\n    mp.set_start_method('spawn')\n\n    # directly calling the method works\n    train()\n\n    # Works fine as well with 'spawn'\n    p = mp.Process(target=train, args=())\n    p.start()\n    p.join()\n```\nOutput:\n\n```\nBefore Conv1 call\nAfter Conv1 call\nPassed!\nBefore Conv1 call\nAfter Conv1 call\nPassed!\n```\n",
      "y": "Just to Update:\n\nIt works fine if we use 'spawn' start method.\n\nUpdated Snippet:\n\n```\nimport torch\nimport torch.nn as nn\nimport torch.nn.functional as F\nfrom torch.autograd import Variable\nimport torch.multiprocessing as mp\n\n\nclass Net(nn.Module):\n    def __init__(self, input_size):\n        super(Net, self).__init__()\n        self.conv1 = nn.Conv2d(input_size, 32, 3, stride=2, padding=1)\n\n    def forward(self, input):\n        print('Before Conv1 call')\n        x = F.elu(self.conv1(input))\n        print('After Conv1 call')\n        return x\n\n\ndef train():\n    net = Net(1)\n    net(Variable(torch.randn(1, 80, 80).unsqueeze(0)))\n    print('Passed!')\n\n\nif __name__ == '__main__':\n    mp.set_start_method('spawn')\n\n    # directly calling the method works\n    train()\n\n    # Works fine as well with 'spawn'\n    p = mp.Process(target=train, args=())\n    p.start()\n    p.join()\n```\nOutput:\n\n```\nBefore Conv1 call\nAfter Conv1 call\nPassed!\nBefore Conv1 call\nAfter Conv1 call\nPassed!\n```\n"
   },
   {
      "null": 389,
      "x": "Advanced Indexing doesn't work with uniform",
      "z": "This is the expected behavior. Advanced Indexing always returns a copy of the indexed Tensor unless you perform assignment. From NumPy:\n\n> Advanced indexing always returns a copy of the data.",
      "y": "This is the expected behavior. Advanced Indexing always returns a copy of the indexed Tensor unless you perform assignment. From NumPy:\n\n> Advanced indexing always returns a copy of the data."
   },
   {
      "null": 390,
      "x": "Segfault (free() on invalid pointer)",
      "z": "Ok. So here's an ***_incredibly_*** hacky temorary workaround for Jupyter notebooks:\n\nFind the file called `ipykernel_launcher.py`  (mine is at `~/.local/lib/python3.6/site-packages/ipykernel_launcher.py` for example)\n\nNow, just after `import sys` insert a `import torch`. This gets rid of the notebook crashes, but comes at a \nslight \"cost\" that all the notebooks you start has torch already loaded into memory.\n\nJust remember to remove this hack after this issue is fixed :P ;)\n",
      "y": "Ok. So here's an ***_incredibly_*** hacky temorary workaround for Jupyter notebooks:\n\nFind the file called `ipykernel_launcher.py`  (mine is at `~/.local/lib/python3.6/site-packages/ipykernel_launcher.py` for example)\n\nNow, just after `import sys` insert a `import torch`. This gets rid of the notebook crashes, but comes at a \nslight \"cost\" that all the notebooks you start has torch already loaded into memory.\n\nJust remember to remove this hack after this issue is fixed :P ;)\n"
   },
   {
      "null": 391,
      "x": "undefined symbol in master",
      "z": "you can compile with `NO_DISTRIBUTED=1 python setup.py install` to avoid this. This might be a consequence of gloo not compiling for pre-3.x cards.",
      "y": "you can compile with `NO_DISTRIBUTED=1 python setup.py install` to avoid this. This might be a consequence of gloo not compiling for pre-3.x cards."
   },
   {
      "null": 392,
      "x": "weight_norm assertion error when using bias=False and using cuda",
      "z": "This issue is the same issue as raised in https://github.com/pytorch/pytorch/issues/2343 and is due to the way in which RNNs now flatten their weights when used on the GPU (otherwise it's a null op). This also breaks code for the [weight dropped LSTM](https://github.com/salesforce/awd-lstm-lm/blob/master/weight_drop.py).\n\nI realized that weight norm would have the same issue so searched hoping for an elegant solution, but alas :)\n\nI've fixed it by setting `rnn.flatten_parameters = lambda *args, **kwargs: None`, which results in a warning (below) but otherwise running code (except see caveat re: lambda).\n`UserWarning: RNN module weights are not part of single contiguous chunk of memory. This means they need to be compacted at every call, possibly greately increasing memory usage. To compact weights again call flatten_parameters().`\n(Just realized the error has a typo in \"greatly\" but oh well ;))\n\nYou could (and others have) converted the RNN to CUDA early but that would break other parts of my code. Killing the `flatten_parameters` function seems the most consistently backward compatible option. Only issue is you need to create an empty function (i.e. not use lambda) as otherwise you get pickling issues (`AttributeError: Can't pickle local object 'WeightDrop._setup.<locals>.<lambda>'`) on model save.\n\n@apaszke, have you got any insights as to a more elegant solution either temporarily or longer form? I know `flatten_parameters` is already a complicated and low level beast.",
      "y": "This issue is the same issue as raised in https://github.com/pytorch/pytorch/issues/2343 and is due to the way in which RNNs now flatten their weights when used on the GPU (otherwise it's a null op). This also breaks code for the [weight dropped LSTM](https://github.com/salesforce/awd-lstm-lm/blob/master/weight_drop.py).\n\nI realized that weight norm would have the same issue so searched hoping for an elegant solution, but alas :)\n\nI've fixed it by setting `rnn.flatten_parameters = lambda *args, **kwargs: None`, which results in a warning (below) but otherwise running code (except see caveat re: lambda).\n`UserWarning: RNN module weights are not part of single contiguous chunk of memory. This means they need to be compacted at every call, possibly greately increasing memory usage. To compact weights again call flatten_parameters().`\n(Just realized the error has a typo in \"greatly\" but oh well ;))\n\nYou could (and others have) converted the RNN to CUDA early but that would break other parts of my code. Killing the `flatten_parameters` function seems the most consistently backward compatible option. Only issue is you need to create an empty function (i.e. not use lambda) as otherwise you get pickling issues (`AttributeError: Can't pickle local object 'WeightDrop._setup.<locals>.<lambda>'`) on model save.\n\n@apaszke, have you got any insights as to a more elegant solution either temporarily or longer form? I know `flatten_parameters` is already a complicated and low level beast."
   },
   {
      "null": 393,
      "x": "CUDA error (3): initialization error (multiprocessing)",
      "z": "OK so I narrowed it down to `torch.manual_seed` of all things. Here is a minimal script reproducing the issue.\n\n```python\nimport torch\nimport torch.nn as nn\nimport torch.multiprocessing as mp\nimport torch.nn.functional as F\nfrom torch.autograd import Variable\n\n\ndef task(pid, model):\n    x = Variable(torch.rand(64, 10))\n    y = model(x)\n    t = y.clone() * 0.99\n    loss = F.smooth_l1_loss(y, t)\n\n    # here it breaks\n    loss.backward()\n\n    print(\"Process %d finished\" % pid)\n\n\nif __name__ == \"__main__\":\n\n    # comment manual_seed and the CUDA initialization error is gone.\n    torch.manual_seed(23)\n\n    net = nn.Linear(10, 4)\n    net.share_memory()\n\n    processes = []\n    for pid in range(8):\n        p = mp.Process(target=task, args=(pid, net))\n        p.start()\n\n    for p in processes:\n        p.join()\n\n    print(\"Done.\")\n```\n\nedit: this can be solved by setting `mp.set_start_method('spawn')` before setting the rng seed which in turn calls cuda. Although I am not sure it is ideal.",
      "y": "OK so I narrowed it down to `torch.manual_seed` of all things. Here is a minimal script reproducing the issue.\n\n```python\nimport torch\nimport torch.nn as nn\nimport torch.multiprocessing as mp\nimport torch.nn.functional as F\nfrom torch.autograd import Variable\n\n\ndef task(pid, model):\n    x = Variable(torch.rand(64, 10))\n    y = model(x)\n    t = y.clone() * 0.99\n    loss = F.smooth_l1_loss(y, t)\n\n    # here it breaks\n    loss.backward()\n\n    print(\"Process %d finished\" % pid)\n\n\nif __name__ == \"__main__\":\n\n    # comment manual_seed and the CUDA initialization error is gone.\n    torch.manual_seed(23)\n\n    net = nn.Linear(10, 4)\n    net.share_memory()\n\n    processes = []\n    for pid in range(8):\n        p = mp.Process(target=task, args=(pid, net))\n        p.start()\n\n    for p in processes:\n        p.join()\n\n    print(\"Done.\")\n```\n\nedit: this can be solved by setting `mp.set_start_method('spawn')` before setting the rng seed which in turn calls cuda. Although I am not sure it is ideal."
   },
   {
      "null": 394,
      "x": "Runtime error when trying to swap first two axes of a four dimensional Tensor with torch.transpose.",
      "z": "This is a printing issue... We fixed it on master. Sorry about it.",
      "y": "This is a printing issue... We fixed it on master. Sorry about it."
   },
   {
      "null": 395,
      "x": "Transposed Convolution output shape does not take into account dilation",
      "z": "I just noticed this as well because I use the formula provided in by the docs in my [OutputShapeFor](https://github.com/Erotemic/netharn/blob/master/netharn/output_shape_for.py) class.\n\nConsidering the code\n\n```python.\n        module = nn.ConvTranspose2d(1, 1, kernel_size=3, stride=1, dilation=2)\n        input_shape = (1, 1, 10, 10)\n        module(torch.rand(*input_shape)).shape\n```\n\nAccording  to the docs the output shape should be (1, 1, 12, 12), but in reality the output shape is (1, 1, 14, 14).\n\nModifying the formula as suggested by @Coolnesss does result in the correct answer, and the changes matches my intuition of how dilation increases the effective kernel size. ",
      "y": "I just noticed this as well because I use the formula provided in by the docs in my [OutputShapeFor](https://github.com/Erotemic/netharn/blob/master/netharn/output_shape_for.py) class.\n\nConsidering the code\n\n```python.\n        module = nn.ConvTranspose2d(1, 1, kernel_size=3, stride=1, dilation=2)\n        input_shape = (1, 1, 10, 10)\n        module(torch.rand(*input_shape)).shape\n```\n\nAccording  to the docs the output shape should be (1, 1, 12, 12), but in reality the output shape is (1, 1, 14, 14).\n\nModifying the formula as suggested by @Coolnesss does result in the correct answer, and the changes matches my intuition of how dilation increases the effective kernel size. "
   },
   {
      "null": 396,
      "x": "Caffe2: ONNX building with lite proto failure",
      "z": "Hi @bddppq\nyes, by applying #14140 and #14150 I am able to successfully build Caffe2 with protobuf-lite for Android.\nThank you very much for helping!",
      "y": "Hi @bddppq\nyes, by applying #14140 and #14150 I am able to successfully build Caffe2 with protobuf-lite for Android.\nThank you very much for helping!"
   },
   {
      "null": 397,
      "x": "[Caffe2] ONNX Caffe2Backend.prepare() initializes input as float64",
      "z": "Due to this bug, I was getting error in a different form. \nI was not using CUDA, but was trying to export the model using -\n```\nfrom caffe2.python.predictor import mobile_exporter\n\nmobile_exporter.Export(prepared_backend.workspace, prepared_backend.predict_net, prepared_backend.predict_net.external_input) \n```\nThis was throwing `KeyError: dtype('float64')` from [here](https://github.com/pytorch/pytorch/blob/d55b25a633b7e2e6122becf6dbdf0528df6e8b13/caffe2/python/predictor/mobile_exporter.py#L41).\n\nThanks for @laggui for the fix given above. I was circumventing this in different way.",
      "y": "Due to this bug, I was getting error in a different form. \nI was not using CUDA, but was trying to export the model using -\n```\nfrom caffe2.python.predictor import mobile_exporter\n\nmobile_exporter.Export(prepared_backend.workspace, prepared_backend.predict_net, prepared_backend.predict_net.external_input) \n```\nThis was throwing `KeyError: dtype('float64')` from [here](https://github.com/pytorch/pytorch/blob/d55b25a633b7e2e6122becf6dbdf0528df6e8b13/caffe2/python/predictor/mobile_exporter.py#L41).\n\nThanks for @laggui for the fix given above. I was circumventing this in different way."
   },
   {
      "null": 398,
      "x": "FP16 results in \"Floating point exception\"",
      "z": "Unfortunately 9.2 nightlies are built with cudnn 7.1.4 that has a known fpe bug. The solution here would be to build nightlies with more recent cudnn versions.",
      "y": "Unfortunately 9.2 nightlies are built with cudnn 7.1.4 that has a known fpe bug. The solution here would be to build nightlies with more recent cudnn versions."
   },
   {
      "null": 399,
      "x": "torch.argmin behaves differently on CPU and GPU",
      "z": "@carefree0910 argmin returns the index of the minimum value in the dimension. It doesn't have a guarantee to return the index of the **first** minimum. The GPU result is also correct from what `argmin` is supposed to return",
      "y": "@carefree0910 argmin returns the index of the minimum value in the dimension. It doesn't have a guarantee to return the index of the **first** minimum. The GPU result is also correct from what `argmin` is supposed to return"
   },
   {
      "null": 400,
      "x": "Library not loaded: libmklml.dylib use c++ front",
      "z": "@yf225 apparently this is fixed in latest MKL-DNN upgrade. It is pending land https://github.com/pytorch/pytorch/pull/22910",
      "y": "@yf225 apparently this is fixed in latest MKL-DNN upgrade. It is pending land https://github.com/pytorch/pytorch/pull/22910"
   },
   {
      "null": 401,
      "x": "Batched SVD using cuSolver",
      "z": "Closing this since the feature is available on master. The current implementation uses sequential MAGMA calls in a for-loop.",
      "y": "Closing this since the feature is available on master. The current implementation uses sequential MAGMA calls in a for-loop."
   },
   {
      "null": 402,
      "x": "cross_entropy - class weights is a bit unclear",
      "z": "I believe `elementwise_mean` has been removed from `reduction` options now. https://pytorch.org/docs/stable/nn.html?highlight=cross_entropy#torch.nn.CrossEntropyLoss\nClosing, please feel free to reopen if you have other questions. Thanks!",
      "y": "I believe `elementwise_mean` has been removed from `reduction` options now. https://pytorch.org/docs/stable/nn.html?highlight=cross_entropy#torch.nn.CrossEntropyLoss\nClosing, please feel free to reopen if you have other questions. Thanks!"
   },
   {
      "null": 403,
      "x": "nn.parallel.DistributedDataParallel raise CUDA error",
      "z": "The problem is not dense vs. sparse, but cpu vs. cuda. Use `.to('cuda')` to put the network on gpu.",
      "y": "The problem is not dense vs. sparse, but cpu vs. cuda. Use `.to('cuda')` to put the network on gpu."
   },
   {
      "null": 404,
      "x": "\"log_softmax_lastdim_kernel_impl\" not implemented for 'torch.LongTensor'",
      "z": "Softmax doesn't work on a long tensor -- convert it to a float or double tensor first (via `tensor.float()`)",
      "y": "Softmax doesn't work on a long tensor -- convert it to a float or double tensor first (via `tensor.float()`)"
   },
   {
      "null": 405,
      "x": "RNN have CuDNN error: CUDNN_STATUS_SUCCESS with Tesla T4",
      "z": "Updating to the CUDA 10 compiled version of pytorch along with CUDA 10 runtime resolved this issue for me.",
      "y": "Updating to the CUDA 10 compiled version of pytorch along with CUDA 10 runtime resolved this issue for me."
   },
   {
      "null": 406,
      "x": "Crash when autograd function returns list instead of tuple",
      "z": "Now it's `TypeError: NewFunctionBackward.forward: expected Variable (got list) for return value 0` which is better, but if it said that a Tuple is expected instead of List or if it cast List to a Tuple automatically, it would be better",
      "y": "Now it's `TypeError: NewFunctionBackward.forward: expected Variable (got list) for return value 0` which is better, but if it said that a Tuple is expected instead of List or if it cast List to a Tuple automatically, it would be better"
   },
   {
      "null": 407,
      "x": "[feature request] Removing hooks from module",
      "z": "```python\nmodel = ...\nhandle = model.register_forward_hook(...)\nhandle.remove()\n# hook will no longer trigger\n```",
      "y": "```python\nmodel = ...\nhandle = model.register_forward_hook(...)\nhandle.remove()\n# hook will no longer trigger\n```"
   },
   {
      "null": 408,
      "x": "Unable to build from source",
      "z": "Searching issues for \"PRId64\" shows that this error popped up and has been fixed in various places in the pytorch codebase, e.g. https://github.com/pytorch/pytorch/issues/3571\n\nThe problem seems to be that this format specifier is not defined by default in C++ and/or old gcc versions. A hacky way to fix this is replacing `\"%\" PRId64 \"` with `\"%lld\"` in the two files currently causing this error on master. A better way is to find the proper place for addding\n\n```\n#define __STDC_FORMAT_MACROS\n#include <inttypes.h>\n```\nas described in the issue linked above.",
      "y": "Searching issues for \"PRId64\" shows that this error popped up and has been fixed in various places in the pytorch codebase, e.g. https://github.com/pytorch/pytorch/issues/3571\n\nThe problem seems to be that this format specifier is not defined by default in C++ and/or old gcc versions. A hacky way to fix this is replacing `\"%\" PRId64 \"` with `\"%lld\"` in the two files currently causing this error on master. A better way is to find the proper place for addding\n\n```\n#define __STDC_FORMAT_MACROS\n#include <inttypes.h>\n```\nas described in the issue linked above."
   },
   {
      "null": 409,
      "x": "Give a better error when we run out of shared memory, instead of \"RuntimeError: DataLoader worker (pid 13) is killed by signal: Bus error.\"",
      "z": "Do `nvidia-docker run -d --shm-size 50G -p 8888:8888 -p 6006:6006 -v ${PWD}:/notebook -v ${PWD}/data/:/notebook/data sachinruk/pytorch_gpu`. Main point is --shm-size ...\n\nIf anyone is wondering when I checked `docker stats`, it was showing that there is 59G available memory and I was using only 1G or so. So seems that you have to explicitly set `--shm-size`.",
      "y": "Do `nvidia-docker run -d --shm-size 50G -p 8888:8888 -p 6006:6006 -v ${PWD}:/notebook -v ${PWD}/data/:/notebook/data sachinruk/pytorch_gpu`. Main point is --shm-size ...\n\nIf anyone is wondering when I checked `docker stats`, it was showing that there is 59G available memory and I was using only 1G or so. So seems that you have to explicitly set `--shm-size`."
   },
   {
      "null": 410,
      "x": "RuntimeError: cuda runtime error (38)",
      "z": "Problem solved.\nI made a very stupid mistake.\nThere is a line in the head which is\nos.environ[\"CUDA_VISIBLE_DEVICES\"] = '3'.\nI did not notice it first time I ran the program and got another error. I revised it to os.environ[\"CUDA_VISIBLE_DEVICES\"] = '0' without restarting the kernel. Then l got this error.\n\nRestart the program with os.environ[\"CUDA_VISIBLE_DEVICES\"] = '0'can solve the problem.",
      "y": "Problem solved.\nI made a very stupid mistake.\nThere is a line in the head which is\nos.environ[\"CUDA_VISIBLE_DEVICES\"] = '3'.\nI did not notice it first time I ran the program and got another error. I revised it to os.environ[\"CUDA_VISIBLE_DEVICES\"] = '0' without restarting the kernel. Then l got this error.\n\nRestart the program with os.environ[\"CUDA_VISIBLE_DEVICES\"] = '0'can solve the problem."
   },
   {
      "null": 411,
      "x": "np.random generates the same random numbers for each data batch",
      "z": "This is because numpy doesn't properly handle RNG states when `fork` subprocesses. It's numpy's issue with multiprocessing tracked at https://github.com/numpy/numpy/issues/9248). But we do provide some workarounds, e.g. the `worker_init_fn` in DataLoader (see http://pytorch.org/docs/master/data.html#torch.utils.data.DataLoader). Or you can use other start methods like spawn.",
      "y": "This is because numpy doesn't properly handle RNG states when `fork` subprocesses. It's numpy's issue with multiprocessing tracked at https://github.com/numpy/numpy/issues/9248). But we do provide some workarounds, e.g. the `worker_init_fn` in DataLoader (see http://pytorch.org/docs/master/data.html#torch.utils.data.DataLoader). Or you can use other start methods like spawn."
   },
   {
      "null": 412,
      "x": "Potential bug when sampling from categorical distribution",
      "z": "On master the issue has been solved for the categorial distribution but not for the multinomial distribution as it seems: \n\n```\nimport torch.distributions as dis\nimport torch\nimport numpy as np\n\nG = 3\nD = 2\np_dG = torch.Tensor(G, D)\np_dG[:, 0] = torch.Tensor([0.1, 0.8, 0.1])\np_dG[:, 1] = torch.Tensor([0.1, 0.8, 0.1])\n\np_dg = p_dG[:, 0]\nz = p_dg.multinomial(250, replacement=True)\ntrue_z_np = z.numpy()\nv, c = np.unique(true_z_np, return_counts=True)\nprint(v)\nprint(c)\n```\nand also\n```\nz = torch.multinomial(p_dg, 250, replacement=True)\ntrue_z_np = z.numpy()\nv, c = np.unique(true_z_np, return_counts=True)\nprint(v)\nprint(c)\n```\n",
      "y": "On master the issue has been solved for the categorial distribution but not for the multinomial distribution as it seems: \n\n```\nimport torch.distributions as dis\nimport torch\nimport numpy as np\n\nG = 3\nD = 2\np_dG = torch.Tensor(G, D)\np_dG[:, 0] = torch.Tensor([0.1, 0.8, 0.1])\np_dG[:, 1] = torch.Tensor([0.1, 0.8, 0.1])\n\np_dg = p_dG[:, 0]\nz = p_dg.multinomial(250, replacement=True)\ntrue_z_np = z.numpy()\nv, c = np.unique(true_z_np, return_counts=True)\nprint(v)\nprint(c)\n```\nand also\n```\nz = torch.multinomial(p_dg, 250, replacement=True)\ntrue_z_np = z.numpy()\nv, c = np.unique(true_z_np, return_counts=True)\nprint(v)\nprint(c)\n```\n"
   },
   {
      "null": 413,
      "x": "Recent git pull breaks working pytorch build",
      "z": "after a git pull, do:\n\n```\ngit submodule update --init --recursive\n```\n\nor whatever it takes to update your submodules to their marked commits.\n\nif that doesn't work, just do a fresh git clone.",
      "y": "after a git pull, do:\n\n```\ngit submodule update --init --recursive\n```\n\nor whatever it takes to update your submodules to their marked commits.\n\nif that doesn't work, just do a fresh git clone."
   },
   {
      "null": 414,
      "x": "MKL Error when import torch after installing 0.3.0 on CentOS",
      "z": "Hi @malbergo,\nMy solution was quite hacky. \nI add `anaconda2/envs/myenv/lib` into LD_LIBRARY_PATH in my .bashrc.\n\nI still not quite like it because it should be automatically triggered when I call `source activate myenv`",
      "y": "Hi @malbergo,\nMy solution was quite hacky. \nI add `anaconda2/envs/myenv/lib` into LD_LIBRARY_PATH in my .bashrc.\n\nI still not quite like it because it should be automatically triggered when I call `source activate myenv`"
   },
   {
      "null": 415,
      "x": "Segmentation fault (core dumped) on LSTMCell on pytorch",
      "z": "Hi @saitarslanboun \nSome sizes are missing in the snippet you provided, so I cannot exactly replicate it and get the segfault error. However, I think I can see a few potential mistakes in your code. I think you might be mistaking the input specifications for `LSTM` and `LSTMCell`. Note that by default, `LSTM` takes as input a tensor of shape `(seq_len, batch, input_size)`, whereas `LSTMCell` takes as input a tensor of shape `(batch, input_size)`.\n\nIn your code snippet, you seem to have `x` where `x.size(1)` specifies the number of timesteps. If I assume that `x` is then of the form `(batch_size, seq_len, input_size)`, the first thing we might want to do is to transpose the first 2 dimensions, by `x.transpose_(0,1)`, to make things `seq_len` first, or `batch_size` second. This is only because recurrent layers like to have the timesteps in the first dimension.\n\nNow, ideally, with a tensor `x` in such a shape, you do not need to use either a for loop, or a `LSTMCell`, and can simply pass it to a `LSTM` module to get the desired output. You can find the documentation for LSTM [here](http://pytorch.org/docs/master/nn.html#torch.nn.LSTM). Your entire code will look like this:\n```\n# create lstmcell module\nlstm = nn.LSTM(embed_size * 2, hidden_size)\n\n# transpose from batch first to batch second for recurrent layers\nx = x.transpose(0, 1).contiguous()\noutput, (h_t, c_t) = lstm(x)\n```\n\nNow let us assume that for some reason, you must use a `LSTMCell` instead of a `LSTM`. In this case, if you go through the documentation for `LSTMCell` [here](http://pytorch.org/docs/master/nn.html#torch.nn.LSTMCell), you will see that the input needs to be passed 1 time step at a time. So, assuming your `x` is of the form `(batch_size, seq_len, input_size)`, your entire code will look like this:\n```\n# create lstmcell module\nlstm = nn.LSTMCell(embed_size * 2, hidden_size)\n\n# initialize h,c outside for loop\nh_t = Variable(x.data.new().resize_as_(x[:, 0, :].data).zero_())\nc_t = Variable(x.data.new().resize_as_(x[:, 0, :].data).zero_())\n\n# loop over time steps\nfor time_step in range(x.size(1)):\n    x_t = x[:, time_step, :]\n    (h_t, c_t) = lstm(x_t, (h_t, c_t))\n```\n\nFrom what I have understood from your snippet, there are a few potential mistakes:\n- Wrong sizes to initialize `hidden` and `cell`, which should be `(batch_size, input_size)` for `LSTMCell`\n- Two nested for loops, first over time steps, and second possibly over batch size, which is wrong\n\nLet me know if this helps you and/or if there are any details in your question that I have misunderstood.",
      "y": "Hi @saitarslanboun \nSome sizes are missing in the snippet you provided, so I cannot exactly replicate it and get the segfault error. However, I think I can see a few potential mistakes in your code. I think you might be mistaking the input specifications for `LSTM` and `LSTMCell`. Note that by default, `LSTM` takes as input a tensor of shape `(seq_len, batch, input_size)`, whereas `LSTMCell` takes as input a tensor of shape `(batch, input_size)`.\n\nIn your code snippet, you seem to have `x` where `x.size(1)` specifies the number of timesteps. If I assume that `x` is then of the form `(batch_size, seq_len, input_size)`, the first thing we might want to do is to transpose the first 2 dimensions, by `x.transpose_(0,1)`, to make things `seq_len` first, or `batch_size` second. This is only because recurrent layers like to have the timesteps in the first dimension.\n\nNow, ideally, with a tensor `x` in such a shape, you do not need to use either a for loop, or a `LSTMCell`, and can simply pass it to a `LSTM` module to get the desired output. You can find the documentation for LSTM [here](http://pytorch.org/docs/master/nn.html#torch.nn.LSTM). Your entire code will look like this:\n```\n# create lstmcell module\nlstm = nn.LSTM(embed_size * 2, hidden_size)\n\n# transpose from batch first to batch second for recurrent layers\nx = x.transpose(0, 1).contiguous()\noutput, (h_t, c_t) = lstm(x)\n```\n\nNow let us assume that for some reason, you must use a `LSTMCell` instead of a `LSTM`. In this case, if you go through the documentation for `LSTMCell` [here](http://pytorch.org/docs/master/nn.html#torch.nn.LSTMCell), you will see that the input needs to be passed 1 time step at a time. So, assuming your `x` is of the form `(batch_size, seq_len, input_size)`, your entire code will look like this:\n```\n# create lstmcell module\nlstm = nn.LSTMCell(embed_size * 2, hidden_size)\n\n# initialize h,c outside for loop\nh_t = Variable(x.data.new().resize_as_(x[:, 0, :].data).zero_())\nc_t = Variable(x.data.new().resize_as_(x[:, 0, :].data).zero_())\n\n# loop over time steps\nfor time_step in range(x.size(1)):\n    x_t = x[:, time_step, :]\n    (h_t, c_t) = lstm(x_t, (h_t, c_t))\n```\n\nFrom what I have understood from your snippet, there are a few potential mistakes:\n- Wrong sizes to initialize `hidden` and `cell`, which should be `(batch_size, input_size)` for `LSTMCell`\n- Two nested for loops, first over time steps, and second possibly over batch size, which is wrong\n\nLet me know if this helps you and/or if there are any details in your question that I have misunderstood."
   },
   {
      "null": 416,
      "x": "ImportError: libcudart.so.9.0: cannot open shared object file: No such file or directory",
      "z": "Ok, great. For now i fixed it with: \n> conda install -c anaconda cudatoolkit==9.0",
      "y": "Ok, great. For now i fixed it with: \n> conda install -c anaconda cudatoolkit==9.0"
   },
   {
      "null": 417,
      "x": "`Normal` distribution: Gaussian policy with zero gradient of mean head",
      "z": "Yep, you need to block gradient flow through sampling by calling `.sample()` instead of `.rsample()` (or detach it before calling `.logprob(.)`)",
      "y": "Yep, you need to block gradient flow through sampling by calling `.sample()` instead of `.rsample()` (or detach it before calling `.logprob(.)`)"
   },
   {
      "null": 418,
      "x": "Wrong torch.svd Calculation Result",
      "z": "Your NumPy is using Accererate, but PyTorch uses MKL. It is natural that they behave slightly differently. Given that the singular value is so small. I would just classify this as precision problem.",
      "y": "Your NumPy is using Accererate, but PyTorch uses MKL. It is natural that they behave slightly differently. Given that the singular value is so small. I would just classify this as precision problem."
   },
   {
      "null": 419,
      "x": "Issue with pytorch update (version 0.4.1) with cuda9.1.85",
      "z": "@darolt do `conda uninstall cuda90 cuda91 cuda92 pytorch -y`, and then re-run your command. That will fix it.",
      "y": "@darolt do `conda uninstall cuda90 cuda91 cuda92 pytorch -y`, and then re-run your command. That will fix it."
   },
   {
      "null": 420,
      "x": "Incorrect PROTOBUF version",
      "z": "Solved by doing\n```\nconda uninstall libprotobuf\n```",
      "y": "Solved by doing\n```\nconda uninstall libprotobuf\n```"
   },
   {
      "null": 421,
      "x": "implement dirichlet / beta GPU grad",
      "z": "Note that pyro's `Beta` **does** support GPU `rsample` and there are no issues there. So just need to get those changes ported in.",
      "y": "Note that pyro's `Beta` **does** support GPU `rsample` and there are no issues there. So just need to get those changes ported in."
   },
   {
      "null": 422,
      "x": "seg fault on import caffe2.python.onnx.backend",
      "z": "Yes this works. Thanks for the fix. \n```\n>>> import caffe2.python.onnx.backend as backend\n>>> import numpy as np\n```\n",
      "y": "Yes this works. Thanks for the fix. \n```\n>>> import caffe2.python.onnx.backend as backend\n>>> import numpy as np\n```\n"
   },
   {
      "null": 423,
      "x": "cufft errors after lots of plan generation",
      "z": "As of CUDA 10 release last week, the bug has been fixed in cuFFT and I have updated the note here: https://github.com/pytorch/pytorch/blob/master/aten/src/ATen/native/cuda/CuFFTPlanCache.h#L349. Also, I have added CUDA 10 guards to `CuFFTPlanCache.h` through this commit https://github.com/pytorch/pytorch/commit/ffbac7d0bb4e0772cab0054f79339478124ea9aa. Hence, if you compile PyTorch with CUDA 10, the cufft plan cache array should grow as intended without failing at the 1024th plan. I have tested this commit by building pytorch with CUDA 10 and running the test suite successfully.\n\nPlease let me know if there are any other queries regarding this issue. Otherwise, I think we are good to close this issue. ",
      "y": "As of CUDA 10 release last week, the bug has been fixed in cuFFT and I have updated the note here: https://github.com/pytorch/pytorch/blob/master/aten/src/ATen/native/cuda/CuFFTPlanCache.h#L349. Also, I have added CUDA 10 guards to `CuFFTPlanCache.h` through this commit https://github.com/pytorch/pytorch/commit/ffbac7d0bb4e0772cab0054f79339478124ea9aa. Hence, if you compile PyTorch with CUDA 10, the cufft plan cache array should grow as intended without failing at the 1024th plan. I have tested this commit by building pytorch with CUDA 10 and running the test suite successfully.\n\nPlease let me know if there are any other queries regarding this issue. Otherwise, I think we are good to close this issue. "
   },
   {
      "null": 424,
      "x": "Backward through sparse_coo_tensor",
      "z": "the gradient of sum is by definition dense, so a sparse representation would use even more memory. use `x.values().sum()` if you want sparse gradients.",
      "y": "the gradient of sum is by definition dense, so a sparse representation would use even more memory. use `x.values().sum()` if you want sparse gradients."
   },
   {
      "null": 425,
      "x": "Inexplicable `test_variant_consistency_{eager/jit}_index_select` failure  for `torch.bfloat16` and `torch.float16`",
      "z": "Apparently, index_select is not implemented for scalar half inputs in the cpu (non-scalar inputs are ok):\n```\nIn [26]: x=torch.tensor(2.2500, dtype=torch.float16)                                                                                                                                                           \n\nIn [27]: i=torch.tensor([0], dtype=torch.long)                                                                                                                                                                 \n\nIn [28]: torch.index_select(x,0,i)                                                                                                                                                                             \n---------------------------------------------------------------------------\nRuntimeError                              Traceback (most recent call last)\n<ipython-input-28-573672115355> in <module>\n----> 1 torch.index_select(x,0,i)\n\nRuntimeError: \"index_select\" not implemented for 'Half'\n\nIn [29]: torch.index_select(x.cuda(),0,i.cuda())                                                                                                                                                               \nOut[29]: tensor(2.2500, device='cuda:0', dtype=torch.float16)\n\nIn [30]: x=torch.randn(3,4, dtype=torch.float16)                                                                                                                                                               \n\nIn [31]: i=torch.tensor([0,2])                                                                                                                                                                                 \n\nIn [32]: torch.index_select(x,0,i)                                                                                                                                                                             \nOut[32]: \ntensor([[ 0.6143,  0.4473,  0.3953, -0.4465],\n        [-0.6519, -1.0537,  0.3137,  1.6221]], dtype=torch.float16)\n\nIn [33]: torch.index_select(x.cuda(), 0, i.cuda())                                                                                                                                                             \nOut[33]: \ntensor([[ 0.6143,  0.4473,  0.3953, -0.4465],\n        [-0.6519, -1.0537,  0.3137,  1.6221]], device='cuda:0',\n       dtype=torch.float16)\n\n```\n",
      "y": "Apparently, index_select is not implemented for scalar half inputs in the cpu (non-scalar inputs are ok):\n```\nIn [26]: x=torch.tensor(2.2500, dtype=torch.float16)                                                                                                                                                           \n\nIn [27]: i=torch.tensor([0], dtype=torch.long)                                                                                                                                                                 \n\nIn [28]: torch.index_select(x,0,i)                                                                                                                                                                             \n---------------------------------------------------------------------------\nRuntimeError                              Traceback (most recent call last)\n<ipython-input-28-573672115355> in <module>\n----> 1 torch.index_select(x,0,i)\n\nRuntimeError: \"index_select\" not implemented for 'Half'\n\nIn [29]: torch.index_select(x.cuda(),0,i.cuda())                                                                                                                                                               \nOut[29]: tensor(2.2500, device='cuda:0', dtype=torch.float16)\n\nIn [30]: x=torch.randn(3,4, dtype=torch.float16)                                                                                                                                                               \n\nIn [31]: i=torch.tensor([0,2])                                                                                                                                                                                 \n\nIn [32]: torch.index_select(x,0,i)                                                                                                                                                                             \nOut[32]: \ntensor([[ 0.6143,  0.4473,  0.3953, -0.4465],\n        [-0.6519, -1.0537,  0.3137,  1.6221]], dtype=torch.float16)\n\nIn [33]: torch.index_select(x.cuda(), 0, i.cuda())                                                                                                                                                             \nOut[33]: \ntensor([[ 0.6143,  0.4473,  0.3953, -0.4465],\n        [-0.6519, -1.0537,  0.3137,  1.6221]], device='cuda:0',\n       dtype=torch.float16)\n\n```\n"
   },
   {
      "null": 426,
      "x": "amp does not work with LayerNorm gradient checkpointing",
      "z": "Can you check if https://github.com/pytorch/pytorch/pull/49757 fixes the issue?  Diffs are 2 lines of python, should be quick to reinstall if you built from source.  If you installed from pip, alter your installed pytorch in place:\n```\n>>> import sys\n>>> import torch\n>>> sys.modules[\"torch\"]\n```\nand edit `torch/utils/checkpoint.py` with the PR's diffs at the path `sys.modules[\"torch\"]` points to.",
      "y": "Can you check if https://github.com/pytorch/pytorch/pull/49757 fixes the issue?  Diffs are 2 lines of python, should be quick to reinstall if you built from source.  If you installed from pip, alter your installed pytorch in place:\n```\n>>> import sys\n>>> import torch\n>>> sys.modules[\"torch\"]\n```\nand edit `torch/utils/checkpoint.py` with the PR's diffs at the path `sys.modules[\"torch\"]` points to."
   },
   {
      "null": 427,
      "x": "Printing should not have (bad) autograd side effects",
      "z": "It's unclear to me why `collect_next_edges` returns an empty set when grad mode is disabled. It's likely the unexpected semantics of this function are what caused the bug in the first place. Imo it should be the responsibility of this function to do exactly what it says, and should be a responsibility of the call-site to determine whether the function should be called at all, possibly conditional on whether grad mode is enabled. This make the semantics much more clear.\n\nIn fact, for the vast majority of `collect_next_edges` call-sites, including all generated ones, there _is_ a separate check to `GradMode::is_enabled`, usually through `compute_requires_grad`, that determines whether `collect_next_edges` needs to be called at all.\n\nIn the codebase, I found only 3 `collect_next_edges` call-sites that rely on an empty list of edges being returned when grad mode is disabled:\n* `unpack_input()` in `torch/csrc/autograd/python_function.cpp`\n* `apply()` in `torch/csrc/autograd/custom_function.h`\n* `wrap_outputs()` in `torch/csrc/autograd/functions/utils.cpp`\n\nOf course, the inevitable check for grad mode enabled still happens at some point.\n\nWith this in mind, I propose that we:\n1. Remove the grad enabled check / empty list return logic from `collect_next_edges`\n2. Update the 3 call-sites to manually construct an empty list instead of calling `collect_next_edges` when grad mode is disabled\n\nWhile I understand this proposed fix affects a broader cross-section of the codebase than the originally proposed fixes, I think this fix is better conceptually, and fixing the semantics of `collect_next_edges` makes future maintenance easier.\n\nIf this seems too dangerous, I have a branch ready to go with the proposed fix of temporarily enabling grad mode during `collect_next_edges` in `grad_fn()`.\n\nThoughts on this? @albanD @gchanan ",
      "y": "It's unclear to me why `collect_next_edges` returns an empty set when grad mode is disabled. It's likely the unexpected semantics of this function are what caused the bug in the first place. Imo it should be the responsibility of this function to do exactly what it says, and should be a responsibility of the call-site to determine whether the function should be called at all, possibly conditional on whether grad mode is enabled. This make the semantics much more clear.\n\nIn fact, for the vast majority of `collect_next_edges` call-sites, including all generated ones, there _is_ a separate check to `GradMode::is_enabled`, usually through `compute_requires_grad`, that determines whether `collect_next_edges` needs to be called at all.\n\nIn the codebase, I found only 3 `collect_next_edges` call-sites that rely on an empty list of edges being returned when grad mode is disabled:\n* `unpack_input()` in `torch/csrc/autograd/python_function.cpp`\n* `apply()` in `torch/csrc/autograd/custom_function.h`\n* `wrap_outputs()` in `torch/csrc/autograd/functions/utils.cpp`\n\nOf course, the inevitable check for grad mode enabled still happens at some point.\n\nWith this in mind, I propose that we:\n1. Remove the grad enabled check / empty list return logic from `collect_next_edges`\n2. Update the 3 call-sites to manually construct an empty list instead of calling `collect_next_edges` when grad mode is disabled\n\nWhile I understand this proposed fix affects a broader cross-section of the codebase than the originally proposed fixes, I think this fix is better conceptually, and fixing the semantics of `collect_next_edges` makes future maintenance easier.\n\nIf this seems too dangerous, I have a branch ready to go with the proposed fix of temporarily enabling grad mode during `collect_next_edges` in `grad_fn()`.\n\nThoughts on this? @albanD @gchanan "
   },
   {
      "null": 428,
      "x": "Different Dice accuracy  using DataParallel",
      "z": "It doesn't matter if it is DP or DDP. You need to use Sync BN if you need global norm.",
      "y": "It doesn't matter if it is DP or DDP. You need to use Sync BN if you need global norm."
   },
   {
      "null": 429,
      "x": "rfftn / irfftn is not functioning properly",
      "z": "This issue is from a 6 month old nightly release used in a non-standard environment. It wasn't reproducible at the time, and I can't reproduce it now. So, I think it's fair to close this.",
      "y": "This issue is from a 6 month old nightly release used in a non-standard environment. It wasn't reproducible at the time, and I can't reproduce it now. So, I think it's fair to close this."
   },
   {
      "null": 430,
      "x": "torch.autograd.jacobian returns tensors with all zeros",
      "z": "This is not a bug. It happens because in your script the function accepts `x` as an input but computes the sum of `tmp`. Running with `strict=True` would diagnose this error for you.",
      "y": "This is not a bug. It happens because in your script the function accepts `x` as an input but computes the sum of `tmp`. Running with `strict=True` would diagnose this error for you."
   },
   {
      "null": 431,
      "x": "[Bug] Sometimes gradient doesn't back-propagate after view",
      "z": "I don't think this is a bug. He is checking the gradient of a non-leaf variable.\nChange the name of the viewed variable to `x2`, and verify that the gradient of `x1` is good.",
      "y": "I don't think this is a bug. He is checking the gradient of a non-leaf variable.\nChange the name of the viewed variable to `x2`, and verify that the gradient of `x1` is good."
   },
   {
      "null": 432,
      "x": "pytorch 0.4.0 always allocates memory on GPU:0 when the model and data are on other GPU.",
      "z": "I believe this has been fixed in https://github.com/pytorch/pytorch/pull/7392 , and is available in pytorch master.\n\nCan you run your python script with\n```python\nCUDA_VISIBLE_DEVICES=1 python my_script.py\n```\nwhile you don't update pytorch?",
      "y": "I believe this has been fixed in https://github.com/pytorch/pytorch/pull/7392 , and is available in pytorch master.\n\nCan you run your python script with\n```python\nCUDA_VISIBLE_DEVICES=1 python my_script.py\n```\nwhile you don't update pytorch?"
   },
   {
      "null": 433,
      "x": "roi_crop (from Detectron.pytorch) building consistently fails",
      "z": "@phalexo You could try out to install `pytorch 0.4.0`, and insert `CFLAGS=\"-std=c99` before `sh make.sh`. ",
      "y": "@phalexo You could try out to install `pytorch 0.4.0`, and insert `CFLAGS=\"-std=c99` before `sh make.sh`. "
   },
   {
      "null": 434,
      "x": "Crash with SIGFPE due to unhandled cases in distributions.MultivariateNormal",
      "z": "There needs to be two changes in the code:\n1. `bvec.size(-1)` to `bmat.size(-1)` in\nhttps://github.com/pytorch/pytorch/blob/302408e6c225bdd0fe9c6af9108c95d10dfb6ce4/torch/distributions/multivariate_normal.py#L32\n\n2. `*shape` to `shape` in\nhttps://github.com/pytorch/pytorch/blob/302408e6c225bdd0fe9c6af9108c95d10dfb6ce4/torch/distributions/multivariate_normal.py#L173\n\nThe sample function works fine. The distributions test suite passes as well.\n\n```python\n>>> import torch\n>>> m = torch.distributions.MultivariateNormal(torch.tensor(0.1), torch.tensor(0.5) * torch.eye(1))\n>>> m.sample()\ntensor([-0.2011])\n```",
      "y": "There needs to be two changes in the code:\n1. `bvec.size(-1)` to `bmat.size(-1)` in\nhttps://github.com/pytorch/pytorch/blob/302408e6c225bdd0fe9c6af9108c95d10dfb6ce4/torch/distributions/multivariate_normal.py#L32\n\n2. `*shape` to `shape` in\nhttps://github.com/pytorch/pytorch/blob/302408e6c225bdd0fe9c6af9108c95d10dfb6ce4/torch/distributions/multivariate_normal.py#L173\n\nThe sample function works fine. The distributions test suite passes as well.\n\n```python\n>>> import torch\n>>> m = torch.distributions.MultivariateNormal(torch.tensor(0.1), torch.tensor(0.5) * torch.eye(1))\n>>> m.sample()\ntensor([-0.2011])\n```"
   }
]