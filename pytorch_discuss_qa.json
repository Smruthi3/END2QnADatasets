[{"x":"Hi!\nI implemented a model where one variable is defined inside the function nn.Parameter() in order to be used during the optimization process. However, when I request the list of variables defined in the model, that variable is not displayed.\nHere is the class, in this case, the variable I need to optimize is W_ij:\nclass CapsNet(nn.Module):\n\tdef __init__(self, r = 1):\n\t\tsuper(CapsNet, self).__init__()\n\t\tself.ReLUConv1 = nn.Sequential(\n\t\t\tnn.Conv2d(in_channels=1, out_channels=256, kernel_size=9, stride=1),\n\t\t\tnn.ReLU(inplace=True)\n\t\t)\n\t\tself.W_ij = nn.Parameter(torch.rand((32, 10, 6*6, 16, 8), requires_grad = True))\n\t\tself.PrimaryCaps = nn.ModuleList()\n\t\tself.r = r\n\t\tfor _ in range(32):\n\t\t\tself.PrimaryCaps.append(nn.Conv2d(in_channels=256, out_channels=8, kernel_size=9, stride=2))\n\t\tself.decoder = nn.Sequential(\n\t\t\tnn.Linear(16*10, 512),\n\t\t\tnn.ReLU(inplace=True),\n\t\t\tnn.Linear(512, 1024),\n\t\t\tnn.ReLU(inplace=True),\n\t\t\tnn.Linear(1024, 784),\n\t\t\tnn.Sigmoid()\n\t\t)\n\n\tdef forward(self)...\n\nAnd here is the list of variables displayed by the model parameters output:\nmodel = CapsNet()\nmodel.parameters\n\n<bound method Module.parameters of CapsNet(\n  (ReLUConv1): Sequential(\n\t(0): Conv2d(1, 256, kernel_size=(9, 9), stride=(1, 1))\n\t(1): ReLU(inplace)\n  )\n  (PrimaryCaps): ModuleList(\n\t(0): Conv2d(256, 8, kernel_size=(9, 9), stride=(2, 2))\n\t(1): Conv2d(256, 8, kernel_size=(9, 9), stride=(2, 2))\n\t(2): Conv2d(256, 8, kernel_size=(9, 9), stride=(2, 2))\n\t(3): Conv2d(256, 8, kernel_size=(9, 9), stride=(2, 2))\n\t(4): Conv2d(256, 8, kernel_size=(9, 9), stride=(2, 2))\n\t(5): Conv2d(256, 8, kernel_size=(9, 9), stride=(2, 2))\n\t(6): Conv2d(256, 8, kernel_size=(9, 9), stride=(2, 2))\n\t(7): Conv2d(256, 8, kernel_size=(9, 9), stride=(2, 2))\n\t(8): Conv2d(256, 8, kernel_size=(9, 9), stride=(2, 2))\n\t(9): Conv2d(256, 8, kernel_size=(9, 9), stride=(2, 2))\n\t(10): Conv2d(256, 8, kernel_size=(9, 9), stride=(2, 2))\n\t(11): Conv2d(256, 8, kernel_size=(9, 9), stride=(2, 2))\n\t(12): Conv2d(256, 8, kernel_size=(9, 9), stride=(2, 2))\n\t(13): Conv2d(256, 8, kernel_size=(9, 9), stride=(2, 2))\n\t(14): Conv2d(256, 8, kernel_size=(9, 9), stride=(2, 2))\n\t(15): Conv2d(256, 8, kernel_size=(9, 9), stride=(2, 2))\n\t(16): Conv2d(256, 8, kernel_size=(9, 9), stride=(2, 2))\n\t(17): Conv2d(256, 8, kernel_size=(9, 9), stride=(2, 2))\n\t(18): Conv2d(256, 8, kernel_size=(9, 9), stride=(2, 2))\n\t(19): Conv2d(256, 8, kernel_size=(9, 9), stride=(2, 2))\n\t(20): Conv2d(256, 8, kernel_size=(9, 9), stride=(2, 2))\n\t(21): Conv2d(256, 8, kernel_size=(9, 9), stride=(2, 2))\n\t(22): Conv2d(256, 8, kernel_size=(9, 9), stride=(2, 2))\n\t(23): Conv2d(256, 8, kernel_size=(9, 9), stride=(2, 2))\n\t(24): Conv2d(256, 8, kernel_size=(9, 9), stride=(2, 2))\n\t(25): Conv2d(256, 8, kernel_size=(9, 9), stride=(2, 2))\n\t(26): Conv2d(256, 8, kernel_size=(9, 9), stride=(2, 2))\n\t(27): Conv2d(256, 8, kernel_size=(9, 9), stride=(2, 2))\n\t(28): Conv2d(256, 8, kernel_size=(9, 9), stride=(2, 2))\n\t(29): Conv2d(256, 8, kernel_size=(9, 9), stride=(2, 2))\n\t(30): Conv2d(256, 8, kernel_size=(9, 9), stride=(2, 2))\n\t(31): Conv2d(256, 8, kernel_size=(9, 9), stride=(2, 2))\n  )\n  (decoder): Sequential(\n\t(0): Linear(in_features=160, out_features=512, bias=True)\n\t(1): ReLU(inplace)\n\t(2): Linear(in_features=512, out_features=1024, bias=True)\n\t(3): ReLU(inplace)\n\t(4): Linear(in_features=1024, out_features=784, bias=True)\n\t(5): Sigmoid()\n  )\n)>\n\nOnly the variables defined in the sequential functions are defined as parameters of my model, but not the variable W_ij. In this regards, I have two questions:\n\nHow I can include the variable W_ij as a member of the set of parameters to be updated in the optimization step. So far, the loss function does not change during that part and I believe this is the reason.\nDo I have to set  requires_grad = True in the definition of the variable despite it is included in the nn.Parameter function?\n\nAny suggestions?\nHenry","y":"Hi  and : Thanks for your help. I changed the optimizer function from Adam to SGD and the loss function is minimized.","z":"Could you try to print the parameters using:\n<code class=\"lang-python\">print(list(model.parameters()))\n<\/code>\nHi :\nHere is part of the output because is larger than the number of characters allowed:\n[Parameter containing:\ntensor([[[[[7.6739e-01, 5.7694e-01, 1.4690e-01,  ..., 8.4265e-01,\n\t\t\t1.0182e-02, 5.0916e-01],\n\t\t   [5.8372e-01, 7.2278e-01, 8.8761e-01,  ..., 3.6188e-01,\n\t\t\t9.8832e-02, 7.6833e-01],\n\t\t   [1.7700e-01, 4.6641e-01, 8.7544e-01,  ..., 1.2639e-01,\n\t\t\t4.7846e-01, 5.5806e-01],\n\t\t   ...,\n\t\t   [7.1373e-01, 4.6280e-01, 7.5365e-01,  ..., 4.9755e-01,\n\t\t\t8.8003e-01, 9.3670e-01],\n\t\t   [5.3323e-01, 4.0508e-01, 4.6126e-01,  ..., 3.7920e-01,\n\t\t\t7.5806e-01, 4.8837e-01],\n\t\t   [5.3375e-02, 2.3048e-01, 3.9690e-01,  ..., 1.2138e-01,\n\t\t\t4.3386e-01, 3.4856e-01]],\n\n\t\t  [[4.5299e-01, 3.3679e-01, 4.0874e-01,  ..., 7.7343e-01,\n\t\t\t9.1206e-01, 1.8913e-01],\n\t\t   [3.6609e-01, 3.6512e-01, 5.8318e-01,  ..., 5.7687e-01,\n\t\t\t6.6697e-01, 5.4229e-02],\n\t\t   [8.8892e-01, 4.5645e-02, 4.1808e-01,  ..., 3.7985e-01,\n\t\t\t3.3769e-01, 6.7484e-01],\n\t\t   ...,\n\t\t   [5.0250e-01, 1.1942e-01, 3.0485e-01,  ..., 4.9193e-01,\n\t\t\t6.6392e-01, 1.7286e-01],\n\t\t   [1.4054e-01, 5.4048e-01, 4.5034e-02,  ..., 5.4134e-01,\n\t\t\t1.1507e-01, 4.3528e-01],\n\t\t   [4.1365e-01, 3.7377e-01, 8.4924e-01,  ..., 5.0650e-01,\n\t\t\t6.7584e-01, 8.7766e-01]],\n\n\t\t  [[3.2468e-01, 1.3789e-01, 1.9836e-01,  ..., 6.3810e-01,\n\t\t\t3.7559e-01, 7.9397e-01],\n\t\t   [3.1314e-01, 9.2096e-01, 7.8847e-01,  ..., 6.8551e-01,\n\t\t\t8.1996e-01, 2.2501e-01],\n\t\t   [4.4605e-01, 4.3197e-01, 3.0152e-01,  ..., 1.6773e-01,\n\t\t\t2.6052e-01, 4.0138e-01],\n\t\t   ...,\n\t\t   [1.4110e-01, 3.6810e-01, 1.4151e-01,  ..., 4.0450e-01,\n\t\t\t8.0524e-01, 1.7194e-01],\n\t\t   [3.4931e-01, 6.4747e-01, 8.5577e-01,  ..., 7.8586e-01,\n\t\t\t1.7757e-01, 9.3789e-01],\n\t\t   [4.0471e-01, 9.2119e-01, 2.8132e-02,  ..., 9.2249e-01,\n\t\t\t8.8225e-01, 3.8468e-01]],\n\n\t\t  ...,\n\n\t\t  [[8.8568e-01, 7.2598e-01, 4.1293e-02,  ..., 1.0657e-01,\n\t\t\t8.1734e-01, 3.4054e-01],\n\t\t   [1.3737e-01, 6.8914e-01, 4.7814e-01,  ..., 2.0132e-02,\n\t\t\t4.1065e-01, 9.6494e-01],\n\t\t   [3.2537e-01, 1.7425e-02, 1.9855e-01,  ..., 2.5449e-01,\n\t\t\t6.8413e-01, 8.9184e-02],\n\t\t   ...,\n\t\t   [4.0885e-01, 8.8580e-01, 4.7947e-01,  ..., 8.4072e-01,\n\t\t\t1.7515e-01, 9.2421e-03],\n\t\t   [2.8516e-01, 3.0517e-01, 6.1949e-01,  ..., 9.6947e-01,\n\t\t\t3.4296e-01, 2.5695e-01],\n\t\t   [4.7322e-01, 2.8092e-02, 8.4044e-01,  ..., 8.8433e-01,\n\t\t\t1.0560e-01, 1.9199e-01]],\n\n\t\t  [[5.4768e-01, 1.4706e-01, 7.2479e-01,  ..., 5.7813e-01,\n\t\t\t9.4183e-01, 1.6523e-01],\n\t\t   [8.5458e-01, 2.9474e-01, 7.6239e-01,  ..., 8.4729e-01,\n\t\t\t9.4218e-01, 7.7710e-01],\n\t\t   [3.9862e-01, 4.3010e-01, 5.2872e-01,  ..., 5.6694e-01,\n\t\t\t3.3170e-01, 9.6968e-01],\n\t\t   ...,\n\t\t   [6.9340e-02, 6.8474e-01, 9.6966e-01,  ..., 9.2907e-01,\n\t\t\t4.6483e-01, 3.6082e-01],\n\t\t   [8.2517e-02, 9.0183e-01, 8.5331e-01,  ..., 5.2363e-01,\n\t\t\t7.5155e-01, 7.1173e-01],\n\t\t   [1.4189e-01, 9.0067e-01, 7.9025e-01,  ..., 9.7414e-01,\n\t\t\t1.3880e-01, 9.1656e-01]],\n\n\t\t  [[4.4559e-01, 7.4797e-01, 3.7663e-01,  ..., 4.4997e-01,\n\t\t\t7.5805e-01, 1.4281e-02],\n\t\t   [7.1061e-01, 1.3400e-01, 3.4829e-01,  ..., 4.0531e-01,\n\t\t\t7.6288e-01, 9.3623e-03],\n\t\t   [7.3082e-01, 1.6582e-01, 9.1881e-01,  ..., 2.0254e-02,\n\t\t\t5.2745e-01, 1.7543e-01],\n\t\t   ...,\n\t\t   [1.2445e-01, 6.4993e-01, 7.2403e-01,  ..., 1.0495e-01,\n\t\t\t8.2953e-01, 8.4761e-01],\n\t\t   [2.2911e-01, 1.7264e-01, 7.1898e-01,  ..., 1.3536e-01,\n\t\t\t2.9724e-01, 2.1592e-01],\n\t\t   [7.1262e-03, 3.1743e-01, 3.5177e-02,  ..., 9.6428e-01,\n\t\t\t2.6071e-01, 9.1934e-01]]],\n\n\n\t\t [[[4.1528e-01, 2.5934e-01, 7.9593e-01,  ..., 8.9152e-01,\n\t\t\t7.7328e-01, 5.6743e-01],\n\t\t   [7.9241e-01, 1.9825e-01, 5.7067e-01,  ..., 7.4108e-01,\n\t\t\t3.2185e-01, 4.3047e-01],\n\t\t   [8.9096e-01, 3.9115e-01, 6.8938e-01,  ..., 7.1494e-01,\n\t\t\t1.4202e-01, 7.4137e-01],\n\t\t   ...,\n\t\t   [7.9495e-01, 7.3813e-01, 2.4878e-01,  ..., 4.3278e-01,\n\t\t\t5.1972e-01, 5.1730e-01],\n\t\t   [2.9028e-01, 1.6429e-01, 5.8873e-01,  ..., 6.6769e-01,\n\t\t\t5.7929e-01, 3.6350e-01],\n\t\t   [6.8588e-01, 5.2247e-01, 1.9651e-01,  ..., 9.9136e-01,\n\t\t\t4.4651e-01, 6.3356e-01]],\n\n\t\t  [[2.0207e-01, 5.5784e-01, 4.7625e-01,  ..., 4.2619e-02,\n\t\t\t9.9390e-01, 8.2427e-01],\n\t\t   [1.3512e-01, 7.1366e-01, 3.0682e-01,  ..., 5.2905e-01,\n\t\t\t7.0548e-01, 3.7589e-01],\n\t\t   [3.9355e-01, 7.6408e-01, 2.8698e-01,  ..., 3.3699e-01,\n\t\t\t1.1217e-02, 7.8939e-01],\n\t\t   ...,\n\t\t   [1.7819e-01, 7.4309e-01, 6.5005e-01,  ..., 2.3578e-01,\n\t\t\t4.5845e-01, 6.0670e-01],\n\t\t   [5.8735e-02, 1.4817e-02, 2.4480e-01,  ..., 6.4879e-01,\n\t\t\t1.9488e-01, 6.4382e-01],\n\t\t   [7.9225e-01, 7.8734e-01, 5.3828e-02,  ..., 3.0421e-02,\n\t\t\t5.1285e-01, 5.9031e-02]],\n\n\t\t  [[5.7578e-01, 8.2437e-01, 6.7843e-01,  ..., 6.5092e-01,\n\t\t\t5.2705e-01, 7.0242e-02],\n\t\t   [3.6100e-01, 6.9821e-01, 6.8266e-01,  ..., 9.9500e-01,\n\t\t\t1.7203e-03, 3.6993e-01],\n\t\t   [2.3840e-01, 3.5518e-01, 5.6097e-02,  ..., 4.5848e-01,\n\t\t\t5.1774e-01, 5.7378e-01],\n\t\t   ...,\n\t\t   [5.9616e-01, 7.2055e-01, 5.1226e-01,  ..., 1.6518e-01,\n\t\t\t8.6783e-01, 2.9836e-01],\n\t\t   [8.3414e-01, 5.8019e-01, 6.2259e-01,  ..., 2.7784e-01,\n\t\t\t5.0720e-01, 5.6435e-01],\n\t\t   [5.9361e-01, 2.3667e-01, 9.4878e-01,  ..., 6.9894e-02,\n\t\t\t3.2900e-01, 5.0268e-01]],\n\n\t\t  ...,\n\n\t\t  [[5.9668e-01, 1.8945e-02, 7.3882e-01,  ..., 1.6845e-01,\n\t\t\t2.7940e-01, 3.1264e-01],\n\t\t   [8.7321e-02, 5.1667e-01, 1.5205e-02,  ..., 3.9717e-01,\n\t\t\t1.6481e-02, 7.3159e-01],\n\t\t   [8.0506e-01, 8.7750e-01, 5.5920e-01,  ..., 9.0319e-01,\n\t\t\t2.0946e-01, 1.1467e-01],\n\t\t   ...,\n\t\t   [2.8784e-01, 8.4264e-02, 9.8612e-01,  ..., 7.0642e-01,\n\t\t\t7.5766e-01, 5.6043e-01],\n\t\t   [3.5024e-01, 4.3340e-01, 5.8842e-01,  ..., 5.2658e-01,\n\t\t\t3.8618e-01, 7.4563e-01],\n\t\t   [9.5279e-01, 2.0545e-01, 3.0640e-01,  ..., 8.6450e-01,\n\t\t\t2.7790e-01, 3.3090e-01]],\n\n\t\t  [[3.2479e-01, 7.5468e-01, 5.2824e-01,  ..., 4.0495e-02,\n\t\t\t4.0978e-01, 8.5620e-01],\n\t\t   [4.3639e-01, 5.1542e-01, 4.6560e-01,  ..., 6.2835e-01,\n\t\t\t9.0295e-01, 3.5546e-01],\n\t\t   [4.6104e-01, 3.1599e-01, 3.6135e-02,  ..., 3.3267e-01,\n\t\t\t6.4670e-01, 8.5443e-01],\n\t\t   ...,\n\t\t   [6.3489e-01, 8.9880e-02, 5.4958e-02,  ..., 4.7443e-01,\n\t\t\t8.0634e-01, 8.3051e-01],\n\t\t   [4.0276e-01, 2.4590e-01, 2.0508e-01,  ..., 9.1851e-01,\n\t\t\t5.1149e-01, 4.3412e-01],\n\t\t   [1.5342e-03, 1.8887e-01, 2.1114e-01,  ..., 9.6045e-02,\n\t\t\t2.0662e-01, 4.6233e-01]],\n\n\t\t  [[7.3246e-01, 4.2454e-01, 6.3876e-01,  ..., 4.6388e-01,\n\t\t\t6.6292e-01, 7.6189e-01],\n\t\t   [9.1187e-01, 5.4363e-01, 5.4122e-01,  ..., 1.5614e-01,\n\t\t\t6.9637e-01, 1.4138e-01],\n\t\t   [1.7490e-01, 2.1351e-01, 8.4375e-01,  ..., 5.0843e-01,\n\t\t\t9.5231e-01, 2.9017e-01],\n\t\t   ...,\n\t\t   [9.4328e-01, 7.4659e-01, 9.0242e-01,  ..., 1.7258e-01,\n\t\t\t2.8283e-01, 8.7365e-01],\n\t\t   [3.9166e-01, 7.4339e-01, 1.7533e-01,  ..., 3.8704e-01,\n\t\t\t1.5566e-01, 6.7763e-02],\n\t\t   [7.5075e-01, 1.2519e-01, 8.7407e-02,  ..., 2.5569e-01,\n\t\t\t7.1487e-01, 5.2214e-01]]],\n\n\n\t\t [[[8.6069e-01, 3.0978e-01, 2.6947e-01,  ..., 8.8378e-01,\n\t\t\t2.0586e-01, 2.4953e-01],\n\t\t   [4.9243e-01, 5.7869e-01, 6.8052e-01,  ..., 6.4253e-01,\n\t\t\t3.3459e-01, 4.3123e-02],\n\t\t   [8.4683e-01, 2.0274e-01, 7.2146e-01,  ..., 5.8167e-01,\n\t\t\t3.4023e-01, 8.2120e-01],\n\t\t   ...,\n\t\t   [7.2076e-01, 5.7922e-01, 5.3045e-01,  ..., 4.4999e-01,\n\t\t\t3.1915e-01, 9.4792e-01],\n\t\t   [8.5810e-02, 9.8088e-01, 2.8822e-01,  ..., 2.4828e-01,\n\t\t\t5.9598e-01, 6.7687e-01],\n\t\t   [7.3960e-01, 1.3854e-01, 1.7701e-01,  ..., 4.7463e-01,\n\t\t\t8.7126e-03, 4.5565e-01]],\n\n\t\t  [[9.6817e-01, 3.5741e-01, 1.5915e-01,  ..., 2.2334e-01,\n\t\t\t2.2585e-01, 4.2289e-02],\n\t\t   [9.7334e-01, 9.7148e-01, 7.7071e-01,  ..., 6.4645e-01,\n\t\t\t4.1084e-01, 5.8376e-01],\n\t\t   [3.0449e-01, 4.0146e-01, 9.2943e-01,  ..., 8.4704e-01,\n\t\t\t6.9278e-01, 4.3482e-01],\n\t\t   ...,\n\t\t   [7.4013e-01, 6.5953e-01, 8.9109e-01,  ..., 8.8038e-01,\n\t\t\t9.1634e-02, 8.0954e-01],\n\t\t   [7.8204e-01, 3.8120e-01, 1.0622e-01,  ..., 6.0812e-01,\n\t\t\t3.3097e-02, 4.0971e-03],\n\t\t   [5.7029e-03, 2.9170e-01, 1.4529e-02,  ..., 5.3710e-01,\n\t\t\t9.9842e-03, 5.4186e-01]],\n\n\t\t  [[7.6594e-01, 9.5179e-01, 4.7323e-01,  ..., 1.4289e-01,\n\t\t\t3.7384e-01, 8.2052e-01],\n\t\t   [4.9726e-01, 5.7614e-01, 3.8949e-01,  ..., 5.6762e-01,\n\t\t\t8.2138e-01, 3.3427e-01],\n\t\t   [8.1521e-01, 8.3014e-01, 7.4531e-01,  ..., 4.6001e-01,\n\t\t\t3.4006e-01, 4.3338e-01],\n\t\t   ...,\n\t\t   [5.3341e-01, 4.2412e-01, 4.1900e-01,  ..., 9.3899e-01,\n\t\t\t5.7563e-02, 4.1517e-01],\n\t\t   [2.4708e-01, 3.8884e-01, 3.6465e-01,  ..., 4.6556e-01,\n\t\t\t5.2193e-02, 7.1592e-01],\n\t\t   [9.0153e-01, 5.5619e-02, 7.6606e-01,  ..., 5.6541e-01,\n\t\t\t2.7944e-01, 9.8611e-01]],\n\n\t\t  ...,\n\n\t\t  [[5.5920e-02, 5.4057e-02, 7.8622e-01,  ..., 7.3954e-01,\n\t\t\t5.8098e-01, 8.3185e-01],\n\t\t   [2.0571e-01, 6.4555e-01, 7.4586e-01,  ..., 8.1000e-01,\n\t\t\t8.9614e-01, 7.8818e-02],\n\t\t   [5.9937e-01, 2.9032e-01, 7.9013e-01,  ..., 8.8225e-01,\n\t\t\t5.3767e-01, 2.5079e-02],\n\t\t   ...,\n\t\t   [2.4588e-01, 9.7691e-01, 8.1896e-01,  ..., 5.9354e-01,\n\t\t\t3.5735e-01, 6.8635e-01],\n\t\t   [3.0577e-01, 8.1374e-01, 5.8728e-01,  ..., 4.0778e-01,\n\t\t\t1.0003e-01, 2.9440e-01],\n\t\t   [6.7664e-01, 1.3860e-01, 1.4878e-01,  ..., 9.8078e-02,\n\t\t\t6.8442e-02, 7.3116e-01]],\n\n\t\t  [[3.5372e-01, 1.7224e-01, 6.5888e-01,  ..., 6.5219e-02,\n\t\t\t9.8796e-01, 7.1238e-01],\n\t\t   [4.2807e-01, 2.1368e-01, 8.6156e-01,  ..., 9.3650e-01,\n\t\t\t8.0852e-01, 7.2590e-01],\n\t\t   [2.3365e-01, 7.5662e-01, 1.5440e-01,  ..., 3.9711e-02,\n\t\t\t7.2361e-01, 6.0657e-01],\n\t\t   ...,\n\t\t   [9.2539e-02, 9.3710e-01, 2.0238e-01,  ..., 1.1861e-01,\n\t\t\t1.0211e-01, 8.5345e-01],\n\t\t   [4.7541e-02, 5.3932e-01, 1.7160e-01,  ..., 6.5774e-01,\n\t\t\t5.3190e-01, 3.4805e-01],\n\t\t   [8.0904e-01, 7.6469e-01, 2.4000e-01,  ..., 3.4909e-01,\n\t\t\t6.9401e-01, 8.1628e-01]],\n\n\t\t  [[7.4898e-01, 8.6198e-01, 2.7199e-01,  ..., 1.7970e-01,\n\t\t\t4.8535e-01, 9.6696e-02],\n\t\t   [5.6890e-01, 2.5746e-01, 9.2011e-01,  ..., 8.0284e-01,\n\t\t\t9.1778e-01, 6.9542e-01],\n\t\t   [8.8634e-01, 3.3647e-01, 7.8304e-01,  ..., 9.5858e-01,\n\t\t\t8.6181e-01, 7.0420e-01],\n\t\t   ...,\n\t\t   [1.1258e-01, 2.9942e-01, 7.0951e-01,  ..., 6.7769e-01,\n\t\t\t7.7942e-02, 7.8483e-01],\n\t\t   [9.6725e-01, 1.1985e-01, 7.3592e-02,  ..., 4.0335e-01,\n\t\t\t7.1557e-01, 5.9206e-01],\n\t\t   [2.7142e-01, 1.0375e-01, 6.8905e-01,  ..., 9.1768e-01,\n\t\t\t7.1096e-01, 5.4644e-01]]],\n\n\n\t\t ...,\n\n\n\t\t [[[2.3049e-01, 2.2262e-01, 6.9831e-01,  ..., 3.4785e-01,\n\t\t\t1.9261e-01, 1.6485e-01],\n\t\t   [8.7370e-02, 4.0153e-01, 5.5550e-01,  ..., 8.7350e-01,\n\t\t\t6.5091e-01, 5.7027e-01],\n\t\t   [8.2581e-01, 8.0229e-01, 5.1905e-02,  ..., 8.4354e-03,\n\t\t\t5.5208e-01, 8.0015e-01],\n\t\t   ...,\n\t\t   [3.5276e-01, 1.7161e-02, 9.9382e-01,  ..., 5.7967e-01,\n\t\t\t6.4500e-01, 3.8972e-01],\n\t\t   [2.9985e-02, 4.9689e-01, 3.9898e-01,  ..., 9.8758e-01,\n\t\t\t4.6939e-01, 8.5264e-01],\n\t\t   [4.9803e-01, 8.2988e-01, 1.3325e-01,  ..., 3.8771e-02,\n\t\t\t5.5842e-01, 6.0082e-01]],\n\n\t\t  [[4.4922e-01, 1.4637e-01, 3.6825e-01,  ..., 3.8888e-01,\n\t\t\t9.9635e-01, 7.6231e-01],\n\t\t   [6.0588e-01, 5.4020e-02, 7.7364e-01,  ..., 7.0918e-01,\n\t\t\t3.1280e-01, 4.1706e-01],\n\t\t   [5.4832e-01, 5.0517e-01, 3.4220e-01,  ..., 7.1315e-01,\n\t\t\t4.6581e-01, 3.6415e-01],\n\t\t   ...,\n\t\t   [3.8192e-01, 4.6033e-01, 5.8473e-01,  ..., 6.5528e-01,\n\t\t\t2.8630e-01, 7.3145e-01],\n\t\t   [9.0302e-01, 9.5635e-01, 1.0854e-01,  ..., 1.2172e-01,\n\t\t\t8.8041e-01, 4.6316e-01],\n\t\t   [2.3110e-01, 3.6821e-01, 7.7418e-01,  ..., 6.3039e-01,\n\t\t\t9.4273e-01, 2.1212e-01]],\n\n\t\t  [[7.8398e-01, 7.0921e-01, 9.6487e-02,  ..., 8.9825e-01,\n\t\t\t5.1998e-01, 9.8542e-01],\n\t\t   [9.7818e-01, 5.7448e-01, 2.6835e-02,  ..., 7.5816e-01,\n\t\t\t3.2348e-01, 4.5573e-02],\n\t\t   [9.0839e-01, 1.1082e-01, 1.4642e-01,  ..., 8.3780e-01,\n\t\t\t8.9171e-01, 4.4667e-01],\n\t\t   ...,\n\t\t   [5.9776e-01, 9.3012e-01, 7.6937e-01,  ..., 3.2172e-01,\n\t\t\t3.7485e-01, 2.8772e-01],\n\t\t   [3.4350e-01, 6.0984e-01, 3.2810e-01,  ..., 4.6941e-01,\n\t\t\t3.6490e-01, 8.4864e-02],\n\t\t   [7.5752e-01, 5.6821e-01, 3.1447e-01,  ..., 8.8520e-01,\n\t\t\t9.6437e-01, 3.7760e-01]],\n\n\t\t  ...,\n\n\t\t  [[7.1025e-02, 7.6608e-01, 2.3979e-01,  ..., 8.0521e-02,\n\t\t\t4.2642e-01, 1.3974e-02],\n\t\t   [1.8102e-01, 4.2851e-01, 5.1516e-01,  ..., 7.2275e-01,\n\t\t\t3.9572e-01, 3.0240e-01],\n\t\t   [1.6883e-01, 1.5237e-01, 1.8105e-01,  ..., 8.7423e-01,\n\t\t\t6.8085e-01, 6.6449e-01],\n\t\t   ...,\n\t\t   [4.6055e-01, 9.7129e-01, 5.5012e-01,  ..., 1.2927e-01,\n\t\t\t1.9321e-01, 5.7518e-01],\n\t\t   [2.5797e-04, 3.2652e-01, 4.4737e-01,  ..., 8.0914e-01,\n\t\t\t5.6270e-01, 6.4787e-01],\n\t\t   [1.7741e-01, 5.4729e-01, 9.7928e-03,  ..., 3.7991e-01,\n\t\t\t7.8216e-01, 9.6301e-01]],\n\n\t\t  [[8.4330e-01, 2.1859e-01, 9.1057e-01,  ..., 1.6024e-01,\n\t\t\t6.5961e-01, 5.2498e-01],\n\t\t   [9.8151e-01, 3.6714e-01, 2.8921e-01,  ..., 8.2588e-01,\n\t\t\t3.7878e-02, 8.7757e-01],\n\t\t   [9.8103e-01, 5.2342e-01, 2.5119e-01,  ..., 5.1633e-01,\n\t\t\t1.6919e-01, 5.6062e-01],\n\t\t   ...,\n\t\t   [9.0493e-01, 8.4267e-01, 3.7056e-01,  ..., 6.0395e-01,\n\t\t\t6.1962e-01, 4.1659e-01],\n\t\t   [7.0052e-01, 8.7154e-01, 1.7824e-01,  ..., 5.4432e-02,\n\t\t\t5.0960e-01, 9.5224e-02],\n\t\t   [5.7251e-01, 8.0133e-01, 5.8870e-01,  ..., 3.0814e-01,\n\t\t\t7.8979e-01, 4.4498e-01]],\n\n\t\t  [[7.7022e-01, 9.3518e-01, 8.9363e-01,  ..., 6.5739e-01,\n\t\t\t5.6208e-01, 2.7751e-02],\n\t\t   [7.1668e-01, 8.3066e-02, 8.0086e-01,  ..., 3.1376e-01,\n\t\t\t8.2750e-01, 1.1812e-01],\n\t\t   [5.1732e-01, 8.6545e-01, 7.4015e-01,  ..., 2.9727e-02,\n\t\t\t9.6790e-01, 1.1910e-01],\n\t\t   ...,\n\t\t   [6.4426e-01, 6.2727e-01, 3.4956e-01,  ..., 8.2050e-01,\n\t\t\t2.2073e-01, 5.4220e-01],\n\t\t   [1.3593e-01, 1.4675e-01, 7.4674e-01,  ..., 8.3513e-01,\n\t\t\t1.7398e-01, 3.7785e-01],\n\t\t   [3.8345e-01, 4.9171e-01, 9.0600e-01,  ..., 1.4847e-01,\n\t\t\t9.7346e-01, 6.1814e-01]]],\n\n\n\t\t [[[4.1622e-01, 1.5042e-02, 6.2890e-01,  ..., 5.4780e-01,\n\t\t\t6.1426e-01, 9.4636e-01],\n\t\t   [8.7305e-01, 9.5013e-01, 2.2473e-01,  ..., 9.7041e-01,\n\t\t\t9.1911e-01, 3.5189e-01],\n\t\t   [6.5247e-01, 3.0404e-01, 3.0104e-01,  ..., 8.4302e-01,\n\t\t\t4.1354e-01, 3.5731e-01],\n\nI uploaded the output of the file in the following link in case you want to review the entire set of records\nhttps:\/\/github.com\/henrychacon\/CapsNet\/blob\/master\/parameters.txt\nHi , by the way, the version of Torch I am using it is: 1.0.1.post2\nThanks!\nHenry\nProbably it would be easier to just check the names, since the actual parameter values are quite big:\n<code class=\"lang-python\">for name, _ in model.named_parameters():\n    print(name)\n<\/code>\nCould you run this code and check if your parameter is there?\nHi :\nWhen I run the command:\nfor name, _ in model.named_parameters():\n    print(name)\n\nThe parameter W_ij is displayed, however, it is not optimized during the learning process.\n\nIs there any way that I explicitly can include that parameter into the optimization process?\nWhy the parameter is shown in the command named_parameter in the for loop only (not when I execute print(model.named_parameters)) and not in the model.parameters used as input of optimizer = Adam(model.parameters())?\n\nHere is the output:\nW_ij\nReLUConv1.0.weight\nReLUConv1.0.bias\nPrimaryCaps.0.weight\nPrimaryCaps.0.bias\nPrimaryCaps.1.weight\nPrimaryCaps.1.bias\nPrimaryCaps.2.weight\nPrimaryCaps.2.bias\nPrimaryCaps.3.weight\nPrimaryCaps.3.bias\nPrimaryCaps.4.weight\nPrimaryCaps.4.bias\nPrimaryCaps.5.weight\nPrimaryCaps.5.bias\nPrimaryCaps.6.weight\nPrimaryCaps.6.bias\nPrimaryCaps.7.weight\nPrimaryCaps.7.bias\nPrimaryCaps.8.weight\nPrimaryCaps.8.bias\nPrimaryCaps.9.weight\nPrimaryCaps.9.bias\nPrimaryCaps.10.weight\nPrimaryCaps.10.bias\nPrimaryCaps.11.weight\nPrimaryCaps.11.bias\nPrimaryCaps.12.weight\nPrimaryCaps.12.bias\nPrimaryCaps.13.weight\nPrimaryCaps.13.bias\nPrimaryCaps.14.weight\nPrimaryCaps.14.bias\nPrimaryCaps.15.weight\nPrimaryCaps.15.bias\nPrimaryCaps.16.weight\nPrimaryCaps.16.bias\nPrimaryCaps.17.weight\nPrimaryCaps.17.bias\nPrimaryCaps.18.weight\nPrimaryCaps.18.bias\nPrimaryCaps.19.weight\nPrimaryCaps.19.bias\nPrimaryCaps.20.weight\nPrimaryCaps.20.bias\nPrimaryCaps.21.weight\nPrimaryCaps.21.bias\nPrimaryCaps.22.weight\nPrimaryCaps.22.bias\nPrimaryCaps.23.weight\nPrimaryCaps.23.bias\nPrimaryCaps.24.weight\nPrimaryCaps.24.bias\nPrimaryCaps.25.weight\nPrimaryCaps.25.bias\nPrimaryCaps.26.weight\nPrimaryCaps.26.bias\nPrimaryCaps.27.weight\nPrimaryCaps.27.bias\nPrimaryCaps.28.weight\nPrimaryCaps.28.bias\nPrimaryCaps.29.weight\nPrimaryCaps.29.bias\nPrimaryCaps.30.weight\nPrimaryCaps.30.bias\nPrimaryCaps.31.weight\nPrimaryCaps.31.bias\ndecoder.0.weight\ndecoder.0.bias\ndecoder.2.weight\ndecoder.2.bias\ndecoder.4.weight\ndecoder.4.bias\nIf the parameter is shown in model.named_parameters(), it will also be in model.parameters(), since internally parameters() calls named_parameters() as shown here with link \"https:\/\/github.com\/pytorch\/pytorch\/blob\/db611b7caf0369ba628c5ed8bba42cc22aadfb20\/torch\/nn\/modules\/module.py#L815\".\nHow do you check, if the parameter has been optimized? Do you substract it from itself? If yes, you may want to do an additional clone on the previous version because if you don\u2019t clone it, the variable only holds a reference to this parameter , which means the difference will always be 0.\nHi , in my model, I have two loss functions, one is associated with a matrix parameter defined with nn.parameter() and the other with a sequential CNN. In the first one, the loss value is constant during the optimization process, while in the second one no.\nHi  and : Thanks for your help. I changed the optimizer function from Adam to SGD and the loss function is minimized."},{"x":"Values with the same index in a sparse tensor are accumulated when  use torch.cuda.sparse.FloatTensor(i, v, torch.Size([2,3])).to_dense().\nHow to only remain the max value when  convert the sparse tensor to an dense one.\nAny tricks to implement this?\nSpecifically,\nindices = torch.cuda.LongTensor([[0, 1, 0], [2, 0, 2]])\nvalues = torch.cuda.FloatTensor([3, 4, 5])\nt = torch.cuda.sparse.FloatTensor(i, v, torch.Size([2,3])).to_dense()\nthen\nt\u2019s values are [[ 0  0  5],[  4  0  0]] rather than [[ 0  0  8],[  4  0  0]].","y":"This is NOT what sparse tensors want to do! The sparse tensor represents [[ 0 0 8],[ 4 0 0]].\nThe operation you want is requested in #22378 with link \"https:\/\/github.com\/pytorch\/pytorch\/issues\/22378\", which also links to a great third party implementation of scatter_max with link \"https:\/\/pytorch-scatter.readthedocs.io\/en\/latest\/functions\/max.html\".\nBest regards\nThomas","z":"This is NOT what sparse tensors want to do! The sparse tensor represents [[ 0 0 8],[ 4 0 0]].\nThe operation you want is requested in #22378 with link \"https:\/\/github.com\/pytorch\/pytorch\/issues\/22378\", which also links to a great third party implementation of scatter_max with link \"https:\/\/pytorch-scatter.readthedocs.io\/en\/latest\/functions\/max.html\".\nBest regards\nThomas"},{"x":"I follow the tutorial and use the same code but it comes up a problem\n*** Error in `.\/example-app': free(): invalid pointer: 0x00007ffe733866b0 ***\nI run them in the centos, How can I solve this problem?","y":"I update the g++ from 4.8.5 to 4.9.2 and it works.","z":"Could you post the complete stack trace, if you have it?\nYou are apparently trying to free something which isn\u2019t pointing to a \u201cfreeable\u201d memory address.\nstack trace:stack trace.png1694\u00d7390 20.7 KB with link \"https:\/\/discuss.pytorch.org\/uploads\/default\/original\/2X\/2\/25865c544fdb1287e9e94e172cf88e0aadc67150.png\"\nBut I only run the same code by following the tutorial.\nI update the g++ from 4.8.5 to 4.9.2 and it works.\nI update the g++ too, but still have the same problem.\nWhat\u2019s your g++ version?\ngcc version 4.9.2 20150212 (Red Hat 4.9.2-6) (GCC)\nYou can go to this with link \"https:\/\/github.com\/pytorch\/pytorch\/issues\/21627\" to explore possible solutions.\nSorry, I re-compile the project and  no more free() problem.\nIt is another one \noperation failed in interpreter:\nop_version_set = 0\ndef forward(self,\ninput: Tensor) -> Tensor:\nif bool(torch.gt(torch.sum(input), 0)):\noutput = torch.mv(self.weight, input)\n~~~~~~~~ <\u2014 HERE\nelse:\noutput_2 = torch.add(self.weight, input, alpha=1)\noutput = output_2\nreturn output\nAbandon (core dumped)\nCould you please show me your C++ code?\nI use the tutorial code :\n#include <torch\/script.h> \/\/ One-stop header.\n#include \n#include \nint main(int argc, const char* argv[]) {\nif (argc != 2) {\nstd::cerr << \u201cusage: example-app \\n\u201d;\nreturn -1;\n}\n\/\/ Deserialize the ScriptModule from a file using torch::jit::load().\nstd::shared_ptr<a>torch::jit::script::Module<\/a> module = torch::jit::load(argv[1]);\nassert(module != nullptr);\nstd::cout << \u201cok\\n\u201d;\n\/ Create a vector of inputs.\nstd::vector<a>torch::jit::IValue<\/a> inputs;\ninputs.push_back(torch::ones({1, 3, 224, 224}));\n\/\/ Execute the model and turn its output into a tensor.\nat::Tensor output = module->forward(inputs).toTensor();\nstd::cout << output.slice(\/dim=\/1, \/start=\/0, \/end=\/5) << \u2018\\n\u2019;\n}\nAnd I use this code to create the \u201cmodel.pt\u201d\nimport torch\nimport torchvision\n<h1>An instance of your model.<\/h1>\nmodel = torchvision.models.resnet18()\n<h1>An example input you would normally provide to your model\u2019s forward() method.<\/h1>\nexample = torch.rand(1, 3, 224, 224)\nprint(example)\n<h1>Use torch.jit.trace to generate a torch.jit.ScriptModule via tracing.<\/h1>\ntraced_script_module = torch.jit.trace(model, example)\ntraced_script_module.save(\u201cmodel.pt\u201d)\nIt with link \"https:\/\/discuss.pytorch.org\/t\/loading-batchnorm1d-with-jit\/37552\" has the similar problem with you, you can see this for solution.\nThank you. I have the version 1.1.0 but still have the same problem with \u201ctorch.mv\u201d function :\nterminate called after throwing an instance of \u2018std::runtime_error\u2019\nwhat():\nmv: Expected 1-D argument vec, but got 4-D (check_1d at \/pytorch\/aten\/src\/ATen\/native\/LinearAlgebra.cpp:143)\nframe #0: std::function<std::string ()>::operator()() const + 0x11 (0x7f30c5a46441 in \/home\/pixur\/Documents\/Zaynab\/TestLibtorch\/libtorch\/lib\/libc10.so)\nI didn\u2019t mke any chnge in the code, I just follow the tutorial\nI used the wrong model.pt. It working now. Thank you!"},{"x":"Hello,\nI\u2019ve read the excellent static quantization tutorial, which worked really well with my MobileNet_v2  pretrained weights\n\npytorch.org with link \"https:\/\/pytorch.org\/tutorials\/advanced\/static_quantization_tutorial.html?highlight=quantization\"\n\n\n\n(beta) Static Quantization with Eager Mode in PyTorch \u2014 PyTorch Tutorials... with link \"https:\/\/pytorch.org\/tutorials\/advanced\/static_quantization_tutorial.html?highlight=quantization\"\n\n\n\n\n\nNow I want to do the same with Inception_v3 model. Thing is,  when I try to load the state dictionary I get the following error (full trace bellow), why and how do I fix that? Do I need to create a new class as they did in the tutorial?\n\nper_channel_quantized_model = torchvision.models.quantization.inception_v3(pretrained=True, aux_logits=False, quantize=True)\nper_channel_quantized_model.fc = nn.Linear(2048, 2)\nstate_dict = torch.load(float_model_file, map_location=torch.device(\u2018cpu\u2019))\nper_channel_quantized_model.load_state_dict(state_dict)\n\n\nFile \u201c\u201d, line 5, in \nper_channel_quantized_model.load_state_dict(state_dict)\nFile \u201c\/home\/nimrod\/anaconda3\/lib\/python3.7\/site-packages\/torch\/nn\/modules\/module.py\u201d, line 1030, in load_state_dict\nload(self)\nFile \u201c\/home\/nimrod\/anaconda3\/lib\/python3.7\/site-packages\/torch\/nn\/modules\/module.py\u201d, line 1028, in load\nload(child, prefix + name + \u2018.\u2019)\nFile \u201c\/home\/nimrod\/anaconda3\/lib\/python3.7\/site-packages\/torch\/nn\/modules\/module.py\u201d, line 1028, in load\nload(child, prefix + name + \u2018.\u2019)\nFile \u201c\/home\/nimrod\/anaconda3\/lib\/python3.7\/site-packages\/torch\/nn\/modules\/module.py\u201d, line 1025, in load\nstate_dict, prefix, local_metadata, True, missing_keys, unexpected_keys, error_msgs)\nFile \u201c\/home\/nimrod\/anaconda3\/lib\/python3.7\/site-packages\/torch\/nn\/quantized\/modules\/conv.py\u201d, line 120, in _load_from_state_dict\nstate_dict[prefix + \u2018weight\u2019], state_dict[prefix + \u2018bias\u2019])\nKeyError: \u2018Conv2d_1a_3x3.conv.bias\u2019\n","y":"Here is a high level of the differences:\n<code class=\"lang-auto\"># fp32\n# torchvision.models.inception_v3(pretrained=False, aux_logits=False)\n# state_dict keys of Conv2d_1a_3x3\n# conv\nConv2d_1a_3x3.conv.weight           \n# bn\nConv2d_1a_3x3.bn.weight             \nConv2d_1a_3x3.bn.bias               \nConv2d_1a_3x3.bn.running_mean       \nConv2d_1a_3x3.bn.running_var        \nConv2d_1a_3x3.bn.num_batches_tracked\n\n# ready for quantization but not quantized\n# mq = torchvision.models.quantization.inception_v3(pretrained=False, aux_logits=False, quantize=False)\n# state_dict keys of Conv2d_1a_3x3\n# conv\nConv2d_1a_3x3.conv.weight   \n# bn        \nConv2d_1a_3x3.bn.weight             \nConv2d_1a_3x3.bn.bias               \nConv2d_1a_3x3.bn.running_mean       \nConv2d_1a_3x3.bn.running_var        \nConv2d_1a_3x3.bn.num_batches_tracked\n\n# quantized and fused\n# mq = torchvision.models.quantization.inception_v3(pretrained=False, aux_logits=False, quantize=True)\n# state_dict keys of Conv2d_1a_3x3\n# conv, including quantization-specific scale+zp\nConv2d_1a_3x3.conv.weight    \nConv2d_1a_3x3.conv.bias      \nConv2d_1a_3x3.conv.scale     \nConv2d_1a_3x3.conv.zero_point\n# no bn, it was fused into the conv\n<\/code>\nYou could use the ready for quantization but not quantized model if you are running quantization yourself, the state_dict will be closer to fp32 version since it is before fusion.  Then you\u2019d have to go block by block and see if there are additional differences.","z":"Using eager mode quantization often changes the model hierarchy of the model, so it\u2019s possible that the module hierarchy of your quantized model no longer matches your fp32 state_dict. What is the origin of the fp32 state_dict, is it also from torchvision?\nIn practice, people usually fix this by either loading the weights before fusing the model, or by writing custom state dict mappers which modify the state keys according to the module hierarchy changes.\nYes, the original state dict is a fp32 state_dict, a torchvision model.\nI loaded the state_dict before fusing the model, I fuse it only afterward. So writing a state_dict mappers (or creating the model class \u201cfrom scratch\u201d as they did in the tutorial) is the only solution?\nCan you give an example of how I can create such a mapper?\n\n\n\n Nimrod_Daniel:\n\nper_channel_quantized_model = torchvision.models.quantization.inception_v3(pretrained=True, aux_logits=False, quantize=True)\n\n\n\nper_channel_quantized_model = torchvision.models.quantization.inception_v3(pretrained=True, aux_logits=False, quantize=True)\n\nthis line should already load a pretrained model.  To clarify, is the reason you are loading weights again is to populate the new fc layer, or something else?\nIt loads imagenet weights, but I need a different representation, not just the fc layer, meaning the weights of the whole net. You can ignore the pretrained value.\nMakes sense.  The torchvision.models.quantization.inception_v3(pretrained=True, aux_logits=False, quantize=True) line is torchvision\u2019s best effort to provide a pretrained model ready for quantization for use cases where the default fp32 pretrained weights are fine.  Unfortunately, if you need to load a different version of floating point weights, a mapping of the state dict is required.\nHere is a code snippet which does this for an unrelated model, but the principle is the same:\n<code class=\"lang-auto\">        def get_new_bn_key(old_bn_key):\n            # tries to adjust the key for conv-bn fusion, where\n            # root\n            #   - conv\n            #   - bn\n            #\n            # becomes\n            #\n            # root\n            #   - conv\n            #     - bn\n            return old_bn_key.replace(\".bn.\", \".conv.bn.\")\n\n        non_qat_to_qat_state_dict_map = {}\n        for key in original_state_dict.keys():\n            if key in new_state_dict.keys():\n                non_qat_to_qat_state_dict_map[key] = key\n            else:\n                maybe_new_bn_key = get_new_bn_key(key)\n                if maybe_new_bn_key in new_state_dict.keys():\n                    non_qat_to_qat_state_dict_map[key] = maybe_new_bn_key\n        ...\n        # when loading the state dict, use the mapping created above\n<\/code>\nWe are planning to release a tool soon (hopefully v1.8) to automate all of this, so this should get easier in the near future.\nYeah, I get the idea. You copy the keys if they with the same name, or change the name and then copy the keys in the new map. Afterward, you load the weights to the quantized model according to the new mapping.\nIf I look at the Inception and Quantized Inception\u2019s state_dict  (before fusing, of course) then they have completely different names, and also a different length of state_dict. How can I tackle that ?\nHere is a high level of the differences:\n<code class=\"lang-auto\"># fp32\n# torchvision.models.inception_v3(pretrained=False, aux_logits=False)\n# state_dict keys of Conv2d_1a_3x3\n# conv\nConv2d_1a_3x3.conv.weight           \n# bn\nConv2d_1a_3x3.bn.weight             \nConv2d_1a_3x3.bn.bias               \nConv2d_1a_3x3.bn.running_mean       \nConv2d_1a_3x3.bn.running_var        \nConv2d_1a_3x3.bn.num_batches_tracked\n\n# ready for quantization but not quantized\n# mq = torchvision.models.quantization.inception_v3(pretrained=False, aux_logits=False, quantize=False)\n# state_dict keys of Conv2d_1a_3x3\n# conv\nConv2d_1a_3x3.conv.weight   \n# bn        \nConv2d_1a_3x3.bn.weight             \nConv2d_1a_3x3.bn.bias               \nConv2d_1a_3x3.bn.running_mean       \nConv2d_1a_3x3.bn.running_var        \nConv2d_1a_3x3.bn.num_batches_tracked\n\n# quantized and fused\n# mq = torchvision.models.quantization.inception_v3(pretrained=False, aux_logits=False, quantize=True)\n# state_dict keys of Conv2d_1a_3x3\n# conv, including quantization-specific scale+zp\nConv2d_1a_3x3.conv.weight    \nConv2d_1a_3x3.conv.bias      \nConv2d_1a_3x3.conv.scale     \nConv2d_1a_3x3.conv.zero_point\n# no bn, it was fused into the conv\n<\/code>\nYou could use the ready for quantization but not quantized model if you are running quantization yourself, the state_dict will be closer to fp32 version since it is before fusion.  Then you\u2019d have to go block by block and see if there are additional differences.\n\n\n\n Nimrod_Daniel:\n\nthey have completely different names\n\n\nwhich names in particular are different?  It should be pretty similar.\nThe state_dicts have the same lengths, my mistake. That was quite odd and confusing, though the lengths make sense now. The naming differences shouldn\u2019t be much of a problem.\nI\u2019ll deal with that and then perform the fusion.\nIt could have been great if I could perform a QAT using CUDA instead of dealing with the conversion, hopefully a CUDA support will available soon.\nSo far the PTQ with optimized calibration works really well with my data,  I get almost no drop in AP:) , but maybe in slightly different cases it would be useful in order to avoid drop in AP.\nThanks.\n\n\n\n Nimrod_Daniel:\n\nIt could have been great if I could perform a QAT using CUDA instead of dealing with the conversion, hopefully a CUDA support will available soon.\n\n\ncalling prepare and running the QAT fine-tuning is supported on CUDA.  The only thing not supported is calling convert and running the quantized kernels.  Not sure if that is what you were referring to.\nGood to know. I thought that there\u2019s no CUDA support also for QAT. Thanks."},{"x":"I\u2019ve being using action.reinforce(reward) for policy gradient based training, but it seems like there\u2019s been a change recently and I get an error stating:\nFile \u201c\/opt\/conda\/envs\/pytorch-py35\/lib\/python3.5\/site-packages\/torch\/autograd\/variable.py\u201d, line 209, in reinforce\nif not isinstance(self.grad_fn, StochasticFunction):\nNameError: name \u2018StochasticFunction\u2019 is not defined\nI read on github with link \"https:\/\/github.com\/pytorch\/pytorch\/issues\/3340\" that .reinforce is being deprecated, and it\u2019s suggested to use torch.distributions.\nIs there a reason for this change? Reinforce seemed relatively simple and intuitive. It\u2019ll be great if the reinforce example from pytorch with link \"https:\/\/github.com\/pytorch\/examples\/blob\/master\/reinforcement_learning\/reinforce.py\" is updated to reflect this change.","y":"If you are on the 0.2 release, reinforce is still available. If you\u2019re on master and have torch.distributions instead, the RL examples should now be as follows: https:\/\/github.com\/pytorch\/examples\/pull\/249\ntorch.distributions is much more general and suitable for a larger range of tasks - building the equivalent of reinforce using this is relatively simple (and arguably cleaner as it can be used to create a normal loss function to backpropagate).","z":"Here\u2019s a good thread with link \"https:\/\/github.com\/pytorch\/pytorch\/pull\/3165\" on the reason for the change. I think it can be summarized in two points: support for multiple stochastic outputs is difficult, and improving performance with Variables.\nIf you are on the 0.2 release, reinforce is still available. If you\u2019re on master and have torch.distributions instead, the RL examples should now be as follows: https:\/\/github.com\/pytorch\/examples\/pull\/249\ntorch.distributions is much more general and suitable for a larger range of tasks - building the equivalent of reinforce using this is relatively simple (and arguably cleaner as it can be used to create a normal loss function to backpropagate).\nThis helps, thanks a lot!"},{"x":"Hi I am working with PyTorch 0.2.0_3. I want to use direct back propagation rather than action.reinforce(reward) as it is more flexible. To be careful, I tried to compare the difference between two ways of update. I initialize an actor-critic model that will be updated via .reinforce and a deep copy of it that will be updated via direct BP. The action is chosen by the first model and the loss for the second model is calculated based on the corresponding action.\nI compare the difference of updates by calculating the l1 norm of the difference between the parameters of the two models, and the difference is not zero. Furthermore, the difference grows as the number of updates increases. I am wondering if I am not doing the comparison properly or if this can be a concern. Thank you.\nThe following code is based on the actor-critic example provided in pytorch\/examples with link \"https:\/\/github.com\/pytorch\/examples\/blob\/master\/reinforcement_learning\/actor_critic.py\".\nimport argparse\nimport copy\nimport gym\nimport numpy as np\nfrom itertools import count\nfrom collections import namedtuple\n\nimport torch\nimport torch.nn as nn\nimport torch.nn.functional as F\nimport torch.optim as optim\nimport torch.autograd as autograd\nfrom torch.autograd import Variable\n\n\nparser = argparse.ArgumentParser(description='PyTorch actor-critic example')\nparser.add_argument('--gamma', type=float, default=0.99, metavar='G',\n                    help='discount factor (default: 0.99)')\nparser.add_argument('--seed', type=int, default=543, metavar='N',\n                    help='random seed (default: 1)')\nparser.add_argument('--check-interval', type=int, default=1, metavar='N',\n                    help='interval between checks of different ways of update')\nparser.add_argument('--check-times', type=int, default=10, metavar='N',\n                    help='total times to check the difference between two ways of update')\nargs = parser.parse_args()\n\n\nenv = gym.make('CartPole-v0')\nenv.seed(args.seed)\ntorch.manual_seed(args.seed) \n\n\nSavedInfo = namedtuple('SavedInfo', ['action', 'value', 'log_prob'])\n\n\nclass Policy(nn.Module):\n    def __init__(self):\n        super(Policy, self).__init__()\n        self.affine1 = nn.Linear(4, 128)\n        self.action_head = nn.Linear(128, 2)\n        self.value_head = nn.Linear(128, 1)\n\n        self.saved_info = []\n        self.rewards = []\n\n    def forward(self, x):\n        x = F.relu(self.affine1(x))\n        action_scores = self.action_head(x)\n        state_values = self.value_head(x)\n\n        return F.softmax(action_scores), state_values\n\n    def select_action(self, state_):\n        state_ = torch.from_numpy(state_).float().unsqueeze(0)\n        probs, state_value = self.forward(Variable(state_, requires_grad=False))\n        action = probs.multinomial()\n\n        return action, state_value, probs\n\n\npolicy_reinforce = Policy()\noptimizer_reinforce = optim.Adam(policy_reinforce.parameters(), lr=3e-2)\n\npolicy_bp = copy.deepcopy(policy_reinforce)\noptimizer_bp = optim.Adam(policy_bp.parameters(), lr=3e-2)\n\n\ndef finish_episode():\n    # r for values related to the model being updated via reinforce\n    # b for values related to the model being updated via direct BP\n    saved_info_r = policy_reinforce.saved_info\n    value_loss_r = 0\n    saved_info_b = policy_bp.saved_info\n    value_loss_b = 0\n    policy_loss_b = 0\n\n    R = 0\n    cum_returns = []\n\n    for r in policy_reinforce.rewards[::-1]:\n        R = r + args.gamma * R\n        cum_returns.insert(0, R)\n\n    cum_returns = torch.Tensor(cum_returns)\n    cum_returns = (cum_returns - cum_returns.mean()) \/ (cum_returns.std() + np.finfo(np.float32).eps)\n\n    for (action_r, value_r, log_prob_r), R in zip(saved_info_r, cum_returns):\n        adv_r = R - value_r.data[0, 0]\n        action_r.reinforce(adv_r)\n        value_loss_r += F.smooth_l1_loss(value_r, Variable(torch.Tensor([R])))\n\n    optimizer_reinforce.zero_grad()\n    final_nodes = [value_loss_r] + list(map(lambda p: p.action, saved_info_r))\n    gradients = [torch.ones(1)] + [None] * len(saved_info_r)\n    autograd.backward(final_nodes, gradients)\n    optimizer_reinforce.step()\n\n    for (_, value_b, log_prob_b), R in zip(saved_info_b, cum_returns):\n        adv_b = R - value_b.data[0, 0]\n        policy_loss_b -= log_prob_b * adv_b\n        value_loss_b += F.smooth_l1_loss(value_b, Variable(torch.Tensor([R])))\n\n    optimizer_bp.zero_grad()\n    total_loss_b = policy_loss_b + value_loss_b\n    total_loss_b.backward()\n    optimizer_bp.step()\n\n    del policy_reinforce.rewards[:]\n    del policy_reinforce.saved_info[:]\n    del policy_bp.rewards[:]\n    del policy_bp.saved_info[:]\n\n\ndef check_difference():\n    reinforce_parameters = list(policy_reinforce.parameters())\n    bp_parameters = list(policy_bp.parameters())\n    difference = 0\n\n    for i in range(len(reinforce_parameters)):\n        difference += (reinforce_parameters[i] - bp_parameters[i]).norm(1).data[0]\n\n    return difference\n\n\nrunning_reward = 10\ncheck_done = 0\ndifferences = []\nfor i_episode in count(1):\n    state = env.reset()\n    done = False\n\n    for t in range(10000): # Don't infinite loop while learning\n        action_r, state_value_r, probs_r = policy_reinforce.select_action(state)\n        policy_reinforce.saved_info.append(SavedInfo(action_r, state_value_r,\n                                                     torch.log(probs_r.gather(1, Variable(action_r.data)))))\n\n        _, state_value_b, probs_b = policy_bp.select_action(state)\n        policy_bp.saved_info.append(SavedInfo(None, state_value_b,\n                                              torch.log(probs_b.gather(1, Variable(action_r.data)))))\n\n        state, reward, done, _ = env.step(action_r.data[0, 0])\n\n        policy_reinforce.rewards.append(reward)\n        policy_bp.rewards.append(reward)\n\n        if done:\n            break\n\n    running_reward = running_reward * 0.99 + t * 0.01\n    finish_episode()\n\n    if i_episode % args.check_interval == 0:\n        check_done += 1\n        result = check_difference()\n        differences.append(result)\n\n    if check_done == args.check_times:\n        print(differences)\n        break","y":"I think I have figured it out what\u2019s going on. According to backward function for multinomial with link \"https:\/\/github.com\/pytorch\/pytorch\/blob\/v0.2.0\/torch\/autograd\/_functions\/stochastic.py#L31\", they used -r\/(1e-6+p) directly in the backward process.","z":"I think I have figured it out what\u2019s going on. According to backward function for multinomial with link \"https:\/\/github.com\/pytorch\/pytorch\/blob\/v0.2.0\/torch\/autograd\/_functions\/stochastic.py#L31\", they used -r\/(1e-6+p) directly in the backward process."},{"x":"Below is the Embedding model that is being trained on a list of words following skip-gram method for word2vec algorithm:\n<code class=\"lang-auto\">SkipGram(\n  (embed): Embedding(63641, 300)\n  (output): Linear(in_features=300, out_features=63641, bias=True)\n  (log_softmax): LogSoftmax(dim=1)\n)\n<\/code>\nAnd in the tutorial the below in\/out data is being passed (sample of batch size 8):\n<code class=\"lang-auto\">for inputs, targets in get_batches(train_words, 8):\n    steps += 1\n    inputs, targets = torch.LongTensor(inputs), torch.LongTensor(targets)\n    inputs, targets = inputs.to(device), targets.to(device)\n    print('input_shape:', inputs.shape, 'output_shape:', targets.shape)\n    break\n<\/code>\n<code class=\"lang-auto\">input_shape: torch.Size([36]) output_shape: torch.Size([36])\n<\/code>\nThe idea, of course, is to train the weights of the Embedding layer to approximate the context relations given as input.\nMy question here is, clearly the output shape of the model is different than what is being passed to the target and it still trains (!) so how is that possible? shouldn\u2019t I have to convert the targets to one-hot encoding format for the loss computation to work?\nTo be specific, the target is just 1 integer while the softmax layer is expected to get a one-hot encoded version of the target integer. Is pytorch internally managing this or am I missing something?\n[EDIT]\nModel definition:\n<code class=\"lang-auto\">class SkipGram(nn.Module):\n    def __init__(self, n_vocab, n_embed):\n        super().__init__()\n        \n        self.embed = nn.Embedding(n_vocab, n_embed)\n        self.output = nn.Linear(n_embed, n_vocab)\n        self.log_softmax = nn.LogSoftmax(dim=1)\n    \n    def forward(self, x):\n        x = self.embed(x)\n        scores = self.output(x)\n        log_ps = self.log_softmax(scores)\n        \n        return log_ps\n<\/code>\nTraining loop:\n<code class=\"lang-auto\"># check if GPU is available\ndevice = 'cuda' if torch.cuda.is_available() else 'cpu'\n\nembedding_dim=300 # you can change, if you want\n\nmodel = SkipGram(len(vocab_to_int), embedding_dim).to(device)\ncriterion = nn.NLLLoss()\noptimizer = optim.Adam(model.parameters(), lr=0.003)\n\nprint_every = 500\nsteps = 0\nepochs = 1\n\n# train for some number of epochs\nfor e in range(epochs):\n    \n    # get input and target batches\n    for inputs, targets in get_batches(train_words, 512):\n        steps += 1\n        inputs, targets = torch.LongTensor(inputs), torch.LongTensor(targets)\n        inputs, targets = inputs.to(device), targets.to(device)\n        \n        log_ps = model(inputs)\n        loss = criterion(log_ps, targets)\n        optimizer.zero_grad()\n        loss.backward()\n        optimizer.step()\n\n        if steps % print_every == 0:                  \n            # getting examples and similarities      \n            valid_examples, valid_similarities = cosine_similarity(model.embed, device=device)\n            _, closest_idxs = valid_similarities.topk(6) # topk highest similarities\n            \n            valid_examples, closest_idxs = valid_examples.to('cpu'), closest_idxs.to('cpu')\n            for ii, valid_idx in enumerate(valid_examples):\n                closest_words = [int_to_vocab[idx.item()] for idx in closest_idxs[ii]][1:]\n                print(int_to_vocab[valid_idx.item()] + \" | \" + ', '.join(closest_words))\n            print(\"...\")\n<\/code>","y":"\n\n\n makhan:\n\nMy question here is, clearly the output shape of the model is different than what is being passed to the target and it still trains (!) so how is that possible? shouldn\u2019t I have to convert the targets to one-hot encoding format for the loss computation to work?\nTo be specific, the target is just 1 integer while the softmax layer is expected to get a one-hot encoded version of the target integer. Is pytorch internally managing this or am I missing something?\n\n\nSo the answer to your questions are 1. No. Torch internally converts indexes to one-hot encoded vectors so you don\u2019t have to. 2. Yes. For example:\n<code class=\"lang-auto\">loss = NLLLoss()\ny = torch.LongTensor([2, 1])\ny_pred = torch.Tensor([[0.1, 0, 0.9], [0.0, 0.5, 0.5]])\nprint(loss(torch.log(y_pred), y))\n<\/code>\ny and y_pred does not have the same dimensions, but torch gets it. This is very convenient for single-class classification problems. Your problem is a single-class classification problem.","z":"I think you need to provide some more context.\nthanks, I\u2019ve updated the main post - please let me know if more info is needed.\nThe model definition is missing. Provide a minimum working example.\nI\u2019ve added the model definition and the training loop, hope that helps too. I can share the utility functions definitions  as well if you like but did not want to clutter the post. Please see batch function and sample output of the get_batch function:\n<code class=\"lang-auto\">def get_batches(words, batch_size, window_size=5):\n    ''' Create a generator of word batches as a tuple (inputs, targets) '''\n    \n    n_batches = len(words)\/\/batch_size\n    \n    # only full batches\n    words = words[:n_batches*batch_size]\n    \n    for idx in range(0, len(words), batch_size):\n        x, y = [], []\n        batch = words[idx:idx+batch_size]\n        for ii in range(len(batch)):\n            batch_x = batch[ii]\n            batch_y = get_target(batch, ii, window_size)\n            y.extend(batch_y)\n            x.extend([batch_x]*len(batch_y))\n        yield x, y\n<\/code>\n<code class=\"lang-auto\">int_text = [i for i in range(20)]\nx,y = next(get_batches(int_text, batch_size=4, window_size=5))\n\nprint('x\\n', x)\nprint('y\\n', y)\n<\/code>\n<code class=\"lang-auto\">x\n [0, 0, 0, 1, 1, 1, 2, 2, 2, 3, 3, 3]\ny\n [1, 2, 3, 0, 2, 3, 0, 1, 3, 0, 1, 2]\n\n<\/code>\nthank you for your interest.\n\n\n\n makhan:\n\nMy question here is, clearly the output shape of the model is different than what is being passed to the target and it still trains (!) so how is that possible? shouldn\u2019t I have to convert the targets to one-hot encoding format for the loss computation to work?\nTo be specific, the target is just 1 integer while the softmax layer is expected to get a one-hot encoded version of the target integer. Is pytorch internally managing this or am I missing something?\n\n\nSo the answer to your questions are 1. No. Torch internally converts indexes to one-hot encoded vectors so you don\u2019t have to. 2. Yes. For example:\n<code class=\"lang-auto\">loss = NLLLoss()\ny = torch.LongTensor([2, 1])\ny_pred = torch.Tensor([[0.1, 0, 0.9], [0.0, 0.5, 0.5]])\nprint(loss(torch.log(y_pred), y))\n<\/code>\ny and y_pred does not have the same dimensions, but torch gets it. This is very convenient for single-class classification problems. Your problem is a single-class classification problem."},{"x":"Hi everyone. I need to deploy ssd object detection model on android, but since torch ops does not available on android. I need to trace decode function (https:\/\/github.com\/amdegroot\/ssd.pytorch\/blob\/master\/layers\/functions\/detection.py) to decode detection result. but I do not manage to trace it. Does any one succeed to trace the decode part ?\nI tried to trace the function but I got the following error:\n\nCompiled from code export_decode_model.py(121): decode\nexport_decode_model.py(129): forward\n\/usr\/local\/lib\/python3.6\/dist-packages\/torch\/nn\/modules\/module.py(525): _slow_forward\n\/usr\/local\/lib\/python3.6\/dist-packages\/torch\/nn\/modules\/module.py(539): call\n\/usr\/local\/lib\/python3.6\/dist-packages\/torch\/jit\/init.py(997): trace_module\n\/usr\/local\/lib\/python3.6\/dist-packages\/torch\/jit\/init.py(858): trace\nexport_decode_model.py(142): \n\nThe above operation failed in interpreter, with the following stack trace:","y":"We don\u2019t have a recipe for custom ops yet.  Stay turned!","z":"We don\u2019t have a recipe for custom ops yet.  Stay turned!"},{"x":"I\u2019ve exported Pytorch model for android and trying to load Module I\u2019ve got the following error:\n\njava.lang.RuntimeException: Unable to start activity ComponentInfo{org.pytorch.demo\/org.pytorch.demo.vision.FrameProcessingActivity}: com.facebook.jni.CppException: empty not implemented for TensorTypeSet(VariableTensorId, CUDATensorId) (empty at aten\/src\/ATen\/Functions.h:3679)\n(no backtrace available)\n\n\nwhole stack trace is:\n\nE\/AndroidRuntime: FATAL EXCEPTION: main\nProcess: org.pytorch.demo, PID: 19189\njava.lang.RuntimeException: Unable to start activity ComponentInfo{org.pytorch.demo\/org.pytorch.demo.vision.FrameProcessingActivity}: com.facebook.jni.CppException: empty not implemented for TensorTypeSet(VariableTensorId, CUDATensorId) (empty at aten\/src\/ATen\/Functions.h:3679)\n(no backtrace available)\nat android.app.ActivityThread.performLaunchActivity(ActivityThread.java:2379)\nat android.app.ActivityThread.handleLaunchActivity(ActivityThread.java:2442)\nat android.app.ActivityThread.access$800(ActivityThread.java:156)\nat android.app.ActivityThread$H.handleMessage(ActivityThread.java:1351)\nat android.os.Handler.dispatchMessage(Handler.java:102)\nat android.os.Looper.loop(Looper.java:211)\nat android.app.ActivityThread.main(ActivityThread.java:5389)\nat java.lang.reflect.Method.invoke(Native Method)\nat java.lang.reflect.Method.invoke(Method.java:372)\nat com.android.internal.os.ZygoteInit$MethodAndArgsCaller.run(ZygoteInit.java:1020)\nat com.android.internal.os.ZygoteInit.main(ZygoteInit.java:815)\nCaused by: com.facebook.jni.CppException: empty not implemented for TensorTypeSet(VariableTensorId, CUDATensorId) (empty at aten\/src\/ATen\/Functions.h:3679)\n(no backtrace available)\nat org.pytorch.NativePeer.initHybrid(Native Method)\nat org.pytorch.NativePeer.(NativePeer.java:18)\nat org.pytorch.Module.load(Module.java:23)\nat org.pytorch.demo.models.BeautyDefiner.(BeautyDefiner.java:23)\nat org.pytorch.demo.vision.FrameProcessingActivity.onCreate(FrameProcessingActivity.java:83)\nat android.app.Activity.performCreate(Activity.java:5990)\nat android.app.Instrumentation.callActivityOnCreate(Instrumentation.java:1106)\nat android.app.ActivityThread.performLaunchActivity(ActivityThread.java:2332)\n\u2026 10 more\n\nHow to solve that error?","y":"The problem was that I ran the model on the gpu before exporting with jit.\nSo I move the model to cpu and exported the model again.\nAnd it worked.","z":"The problem was that I ran the model on the gpu before exporting with jit.\nSo I move the model to cpu and exported the model again.\nAnd it worked."},{"x":"Is that possible to find out the reason which caused lagging of UI like specific thread or method?\nCause I kind of put all the heavy work into another thread but I\u2019ve still getting a lagged UI.\nI\u2019m using  CameraX  where  ImageAnalysis  works in separate thread in which I do emotion detection via neural network. So when a neural network process an image in this not UI thread my UI thread also lags.\nIs that possible at all? Can the separate thread slow down the UI thread by executing heavy task?\nMaybe there\u2019s some plugin for Android Studio or something like that to solve the problem.\nI will be glad to any advice\u2026","y":"Hello .\nAt the moment pytorch android thread count is fixed by device and equal \u201cnumber of big cores of cpu\u201d(N) on device.\nAs you also have bg thread for image decoding + UI thread - at the moment of inference you have at least (N + 2) competing threads for N big cores (+other applications threads). I think that is the reason why you see UI thread slow downs.\nWe are thinking to expose control of number of threads to java api, in that case you can set for example singleThread mode, that should not affect too much UI thread responsiveness.\nFor more details I would recommend to use android systrace.\nIf you are building pytorch android from the source you can do deeper investigation with systrace if you build it with environment variable TRACE_ENABLED=1\n(https:\/\/github.com\/pytorch\/pytorch\/blob\/master\/android\/pytorch_android\/CMakeLists.txt#L7)\nIt controls logging of additional sections for systrace, in that case you will see operators sections.\nIf you need all debug symbols for tracing - you may check example of test_app in our repo, that has a script how to build it with all c++ debug symbols:\n<code class=\"lang-auto\">TRACE_ENABLED=1 sh android\/build_test_app.sh\n<\/code>","z":"Hello .\nAt the moment pytorch android thread count is fixed by device and equal \u201cnumber of big cores of cpu\u201d(N) on device.\nAs you also have bg thread for image decoding + UI thread - at the moment of inference you have at least (N + 2) competing threads for N big cores (+other applications threads). I think that is the reason why you see UI thread slow downs.\nWe are thinking to expose control of number of threads to java api, in that case you can set for example singleThread mode, that should not affect too much UI thread responsiveness.\nFor more details I would recommend to use android systrace.\nIf you are building pytorch android from the source you can do deeper investigation with systrace if you build it with environment variable TRACE_ENABLED=1\n(https:\/\/github.com\/pytorch\/pytorch\/blob\/master\/android\/pytorch_android\/CMakeLists.txt#L7)\nIt controls logging of additional sections for systrace, in that case you will see operators sections.\nIf you need all debug symbols for tracing - you may check example of test_app in our repo, that has a script how to build it with all c++ debug symbols:\n<code class=\"lang-auto\">TRACE_ENABLED=1 sh android\/build_test_app.sh\n<\/code>\nHello \nWe just exposed conrtol on global number of threads used by pytorch android, it was landed in master with link \"https:\/\/github.com\/pytorch\/pytorch\/commit\/62254430093fef5f2dece3825d95b25d443faf63\"\n<code class=\"lang-auto\">method org.pytorch.Module#setNumThreads(int numThreads)\n<\/code>\n(https:\/\/github.com\/pytorch\/pytorch\/blob\/master\/android\/pytorch_android\/src\/main\/java\/org\/pytorch\/Module.java#L57)\nThe latest android nightlies already include them: https:\/\/github.com\/pytorch\/pytorch\/tree\/master\/android#nightly (you might need gradle argument --refresh-dependencies if you already using them)\nFor your case when inference is slowing down UI thread - you can experiment with setting number of threads to 1 or 2, maybe depending on device num cores, smth like:\n<code class=\"lang-auto\">Module module = Module.load(moduleFileAbsoluteFilePath);\nmodule.setNumThreads(1);\n<\/code>\nI think that should help with UI thread responsiveness, but check if inference time with this setting is still acceptable for your solution.\nThis is new functionality, please report if you find any issues with it.\nUpdate:\nWe moved  setNumThreads  method to separate class  org.pytorch.PyTorchAndroid  as a static method.\n\n\ngithub.com with link \"https:\/\/github.com\/pytorch\/pytorch\/blob\/master\/android\/pytorch_android\/src\/main\/java\/org\/pytorch\/PyTorchAndroid.java#L33\"\n\n\npytorch\/pytorch\/blob\/master\/android\/pytorch_android\/src\/main\/java\/org\/pytorch\/PyTorchAndroid.java#L33 with link \"https:\/\/github.com\/pytorch\/pytorch\/blob\/master\/android\/pytorch_android\/src\/main\/java\/org\/pytorch\/PyTorchAndroid.java#L33\"\n<code class=\"lang-java\">\n  public static Module loadModuleFromAsset(final AssetManager assetManager, final String assetName) {\n    return new Module(new NativePeer(assetName, assetManager));\n  }\n\n\n  \/**\n   * Globally sets the number of threads used on native side. Attention: Has global effect, all\n   * modules use one thread pool with specified number of threads.\n   *\n   *  numThreads number of threads, must be positive number.\n   *\/\n  public static void setNumThreads(int numThreads) {\n    if (numThreads < 1) {\n      throw new IllegalArgumentException(\"Number of threads cannot be less than 1\");\n    }\n\n\n    nativeSetNumThreads(numThreads);\n  }\n\n\n  private static native void nativeSetNumThreads(int numThreads);\n}\n<\/code>\n\n\n\n\n\n"},{"x":"Hello there,\nRecently I worked a lot with mobile deployment of PyTorch models and figured out that  having debugged TorchScript model on PC sometimes isn\u2019t enough to run it without errors on Android.\nThe main question is how to debug mobile model? At least print Tensor shapes during runtime. As I noticed Python print statements won\u2019t produce outputs on mobile\nExcuse me for direct mention. Would you mind to clarify it ?","y":"Hello ,\nJust update about torchscript print and logcat:\nadb shell setprop log.redirect-stdio redirects only java System.out\ntorchscript print by default uses native stdout.\nIf you desperately need this print in android logcat, you may use this patch:\n\n\ngithub.com\/pytorch\/pytorch with link \"https:\/\/github.com\/pytorch\/pytorch\/commit\/a038b15a4d89dada18da55f6218058aaff85cc61\"\n\n\n\n\n\n\n\n\n[NFC][WIP] Android native stdout,stderr to logcat with link \"https:\/\/github.com\/pytorch\/pytorch\/commit\/a038b15a4d89dada18da55f6218058aaff85cc61\"\n\n\n\n        committed 06:57PM - 17 Dec 19 UTC\n\n\nIvanKobzarev with link \"https:\/\/github.com\/IvanKobzarev\"\n\n\n+33\n-0 with link \"https:\/\/github.com\/pytorch\/pytorch\/commit\/a038b15a4d89dada18da55f6218058aaff85cc61\"\n\n\n\n\n\n\n\n\n\n\nwhich I copy-pasted from https:\/\/codelab.wordpress.com\/2014\/11\/03\/how-to-use-standard-output-streams-for-logging-in-android-apps\/\nIf you build pytorch-android with this patch (gradle -p android assembleDebug and use result aar files directly with link \"https:\/\/github.com\/pytorch\/pytorch\/tree\/master\/android#building-pytorch-android-from-source\" ) after calling PyTorchAndroid.nativeStdOutErrToLogcat() you will see torchscript print in logcat.\nBut we are not going to merge this :), we think about custom handling of print on mobile and print it to logcat. But doing it might take some time.","z":"Helo \nYes, python \u201cprint\u201d does not show up in logcat, (I even tried it with  adb shell setprop log.redirect-stdio true on emulator)\nBy this moment our understanding is that model is debugged using python and we only run debugged model on mobile.\nBut we will think how we can expose print and maybe something else to logcat.\nHello ,\nJust update about torchscript print and logcat:\nadb shell setprop log.redirect-stdio redirects only java System.out\ntorchscript print by default uses native stdout.\nIf you desperately need this print in android logcat, you may use this patch:\n\n\ngithub.com\/pytorch\/pytorch with link \"https:\/\/github.com\/pytorch\/pytorch\/commit\/a038b15a4d89dada18da55f6218058aaff85cc61\"\n\n\n\n\n\n\n\n\n[NFC][WIP] Android native stdout,stderr to logcat with link \"https:\/\/github.com\/pytorch\/pytorch\/commit\/a038b15a4d89dada18da55f6218058aaff85cc61\"\n\n\n\n        committed 06:57PM - 17 Dec 19 UTC\n\n\nIvanKobzarev with link \"https:\/\/github.com\/IvanKobzarev\"\n\n\n+33\n-0 with link \"https:\/\/github.com\/pytorch\/pytorch\/commit\/a038b15a4d89dada18da55f6218058aaff85cc61\"\n\n\n\n\n\n\n\n\n\n\nwhich I copy-pasted from https:\/\/codelab.wordpress.com\/2014\/11\/03\/how-to-use-standard-output-streams-for-logging-in-android-apps\/\nIf you build pytorch-android with this patch (gradle -p android assembleDebug and use result aar files directly with link \"https:\/\/github.com\/pytorch\/pytorch\/tree\/master\/android#building-pytorch-android-from-source\" ) after calling PyTorchAndroid.nativeStdOutErrToLogcat() you will see torchscript print in logcat.\nBut we are not going to merge this :), we think about custom handling of print on mobile and print it to logcat. But doing it might take some time.\n\nThat will be very helpful thanks!\nBTW to call PyTorchAndroid.nativeStdOutErrToLogcat() should one import this function from some module, or PyTorchAndroid already is an upper level module?\nIt\u2019s in org.pytorch package\n<code class=\"lang-auto\">import org.pytorch.PyTorchAndroid;\n\n...\n    PyTorchAndroid.nativeStdOutErrToLogcat()\n...\n<\/code>\nImporting the whole class should work. It\u2019s packaged in pytorch_android*aar\nI have added example of its usage to this commit:\n\n\ngithub.com\/pytorch\/pytorch with link \"https:\/\/github.com\/pytorch\/pytorch\/commit\/62d94d10466991595d316c7e5ceebbb1e2f18210\"\n\n\n\n\n\n\n\n\n[NFC][WIP] Android native stdout,stderr to logcat with link \"https:\/\/github.com\/pytorch\/pytorch\/commit\/62d94d10466991595d316c7e5ceebbb1e2f18210\"\n\n\n\n        committed 06:57PM - 17 Dec 19 UTC\n\n\nIvanKobzarev with link \"https:\/\/github.com\/IvanKobzarev\"\n\n\n+34\n-0 with link \"https:\/\/github.com\/pytorch\/pytorch\/commit\/62d94d10466991595d316c7e5ceebbb1e2f18210\"\n\n\n\n\n\n\n\n\n\n\n(the branch is https:\/\/github.com\/pytorch\/pytorch\/compare\/ik_wip_android_stdouterr_to_logcat)\nWe already use org.pytorch.PyTorchAndroid in our testapp in repo for loading module from asset:\nhttps:\/\/github.com\/pytorch\/pytorch\/blob\/master\/android\/test_app\/app\/src\/main\/java\/org\/pytorch\/testapp\/MainActivity.java#L94"},{"x":"I saw at least two ways to export pytorch models to android device namely via transfering a model from Pytorch to Caffe2 and exporting a model with jit module.\nWhat\u2019s the difference of the ways?\nWhich of them in which situation is preferable?","y":"Using torch.jit.script or torch.jit.trace is preferable to using Caffe2.  The Caffe2 mobile engine is not being actively developed.","z":"Using torch.jit.script or torch.jit.trace is preferable to using Caffe2.  The Caffe2 mobile engine is not being actively developed."},{"x":"It seems that pytorch qat doesn\u2019t simulate bias quantization error during qat. And I found that qat.Conv2d only fake-quantize weight and activation. So pytorch\u2019s quantization strategy does not quantize the bias, right?","y":"We find modeling bias in qat is not very important since it doesn\u2019t affect accuracy too much. one workaround you can do is to remove bias from Conv and add the bias explicitly outside of conv, so that adding bias can be modeled with add.","z":"yes, we do not quantize bias. there have been some internal discussions on this before, the problem of quantizing bias is that it needs to be quantized with the quantization parameters of input and weight, but the input can come from dynamic paths e.g.:\n<code class=\"lang-python\">if x > 0:\n    y = myConv1(x)\nelse:\n    y = myConv2(x)\n  \nz = myConv3(y)\n<\/code>\nand we have no way of getting this information in eager mode. currently we pass in bias in fp32 and it will be quantized inside the quantized ops like quantized::conv2d with quantization parameters of input and weight: y = conv(x_q,w_q) + bias\/(w_scale*x_scale).\nHowever, for qat, I think currently we do not simulate this behavior, I\u2019m not sure how much impact this has though, we\u2019ll discuss about it, thanks for the question.\n\n\n\n jerryzh168:\n\ny = conv(x_q,w_q) + bias\/(w_scale*x_scale)\n\n\nSo, if I want to transfer the quantization aware trained network to my hardware, how exactly should i implement the bias part?\nshould I use the above formula to quantize it?\nright now the quantization for bias is not modeled in quantization aware training, so there might be a little bit of discrepancy between the qat model and the model after convert, but I think it won\u2019t matter too much.\nThank you for the response, Jerry.\nSo, what should I do with the bias parameter of the batch-norm module when I want to implement my quantized model on hardware? the final converted model (quantized) still has this parameter (in FP) in the quantized version of ConvBnReLU2d.\n\nWould bias be totally ignored when we recall the quantized model for some input X  (model.eval() )?\nor the intermediate feature values are temporarily converted to FP to apply bias to them and then are converted back to INT8\/INT32?\nor bias is also converted to INT8 with a simple choice of sale or zero-point without the influence of the qat part?\n\nbias is an input to quantized::conv2d op, it is applied in quantized::conv2d op itself, with this formula:\n\n\n\n jerryzh168:\n\ny = conv(x_q,w_q) + bias\/(w_scale*x_scale)\n\n\nthis is in int32. then we\u2019ll requantize y with output_scale and output_zero_point\ncc  could you link the fbgemm implementation  for conv?\nWe find modeling bias in qat is not very important since it doesn\u2019t affect accuracy too much. one workaround you can do is to remove bias from Conv and add the bias explicitly outside of conv, so that adding bias can be modeled with add.\nThanks for your reply, modeling bias with add op sounds good!"},{"x":"Error at\n<code class=\"lang-auto\">\/usr\/local\/lib\/python3.6\/dist-packages\/torch\/nn\/quantized\/modules\/functional_modules.py in add(self, x, y)\n     43     def add(self, x, y):\n     44         # type: (Tensor, Tensor) -> Tensor\n---> 45         r = torch.add(x, y)\n     46         r = self.activation_post_process(r)\n     47         return r\n<\/code>\nFull error message\n<code class=\"lang-auto\">RuntimeError: Could not run 'aten::add.Tensor' with arguments from the 'QuantizedCPU' backend. 'aten::add.Tensor' is only available for these backends: [CPU, MkldnnCPU, SparseCPU, Meta, BackendSelect, Named, AutogradOther, AutogradCPU, AutogradCUDA, AutogradXLA, AutogradPrivateUse1, AutogradPrivateUse2, AutogradPrivateUse3, Tracer, Autocast, Batched, VmapMode].\n\nCPU: registered at \/pytorch\/build\/aten\/src\/ATen\/CPUType.cpp:2136 [kernel]\nMkldnnCPU: registered at \/pytorch\/build\/aten\/src\/ATen\/MkldnnCPUType.cpp:144 [kernel]\nSparseCPU: registered at \/pytorch\/build\/aten\/src\/ATen\/SparseCPUType.cpp:239 [kernel]\nMeta: registered at \/pytorch\/aten\/src\/ATen\/native\/BinaryOps.cpp:1049 [kernel]\nBackendSelect: fallthrough registered at \/pytorch\/aten\/src\/ATen\/core\/BackendSelectFallbackKernel.cpp:3 [backend fallback]\nNamed: fallthrough registered at \/pytorch\/aten\/src\/ATen\/core\/NamedRegistrations.cpp:11 [kernel]\nAutogradOther: registered at \/pytorch\/torch\/csrc\/autograd\/generated\/VariableType_2.cpp:8041 [autograd kernel]\nAutogradCPU: registered at \/pytorch\/torch\/csrc\/autograd\/generated\/VariableType_2.cpp:8041 [autograd kernel]\nAutogradCUDA: registered at \/pytorch\/torch\/csrc\/autograd\/generated\/VariableType_2.cpp:8041 [autograd kernel]\nAutogradXLA: registered at \/pytorch\/torch\/csrc\/autograd\/generated\/VariableType_2.cpp:8041 [autograd kernel]\nAutogradPrivateUse1: registered at \/pytorch\/torch\/csrc\/autograd\/generated\/VariableType_2.cpp:8041 [autograd kernel]\nAutogradPrivateUse2: registered at \/pytorch\/torch\/csrc\/autograd\/generated\/VariableType_2.cpp:8041 [autograd kernel]\nAutogradPrivateUse3: registered at \/pytorch\/torch\/csrc\/autograd\/generated\/VariableType_2.cpp:8041 [autograd kernel]\nTracer: registered at \/pytorch\/torch\/csrc\/autograd\/generated\/TraceType_2.cpp:9726 [kernel]\nAutocast: fallthrough registered at \/pytorch\/aten\/src\/ATen\/autocast_mode.cpp:254 [backend fallback]\nBatched: registered at \/pytorch\/aten\/src\/ATen\/BatchingRegistrations.cpp:531 [kernel]\nVmapMode: fallthrough registered at \/pytorch\/aten\/src\/ATen\/VmapModeRegistrations.cpp:33 [backend fallback]\n<\/code>\nCode snippet\n<code class=\"lang-auto\">\n    def forward(self, x, x2=None, x3=None):\n        x_size = x.size()\n        resl = x\n        for i in range(len(self.pools_sizes)):\n\n            y = self.convs[i](self.pools[i](x))\n            q_add0 = FloatFunctional()\n#error is because of this line below\n            resl = q_add0.add(resl, nn.functional.interpolate(y, x_size[2:], mode='bilinear', align_corners=True))  #error is because of this line\n        resl = self.relu(resl)\n        if self.need_x2:\n            \n            resl = nn.functional.interpolate(resl, x2.size()[2:], mode='bilinear', align_corners=True)\n        resl = self.conv_sum(resl)\n        if self.need_fuse:\n            q_add1 = FloatFunctional()\n            q_add2 = FloatFunctional()\n            resl = self.conv_sum_c(q_add1.add(q_add2.add(resl, x2), x3))\n        return resl\n<\/code>\nI tried to do as mentioned in post with link \"https:\/\/discuss.pytorch.org\/t\/supported-quantized-tensor-operations\/71688\".\nIf I eval the quantized model the add operation is looks like\n<code class=\"lang-auto\">(conv_sum): QuantizedConv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), scale=2.668934655503108e-07, zero_point=66, padding=(1, 1), bias=False)\n\n(conv_sum_c): QuantizedConv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), scale=1.018745024339296e-06, zero_point=56, padding=(1, 1), bias=False)\n<\/code>\nIn case, I use torch.nn.quantized. QFunctional. The model will not be quantized. The error would something like from CPU backend to QuantizedCPU backend is not possible.\nAny idea! Why?\nInference code\n<code class=\"lang-auto\">normalize = transforms.Normalize(mean=[0.485, 0.456, 0.406],\n                                 std=[0.229, 0.224, 0.225])\n\nvaldir = '\/content\/test'\n\ndataset_test = torchvision.datasets.ImageFolder(\n    valdir,\n    transforms.Compose([\n        transforms.Resize(256),\n        transforms.CenterCrop(224),\n        transforms.ToTensor(),\n        normalize,\n    ]))\n\ntest_sampler = torch.utils.data.SequentialSampler(dataset_test)\n\ndata_loader_test = torch.utils.data.DataLoader(\n    dataset_test, batch_size=1,\n    sampler=test_sampler)\n\nmodel.load_state_dict(torch.load('\/content\/model.pth'))\nmodel.eval()\n\nwith torch.no_grad():\n    for image, target in data_loader_test:\n     \n        print(image.size()) #torch.Size([1, 3, 224, 224])\n        output = model(image)\n        print(output)\n<\/code>\nI think the inference code is fine. Problem is in add operation. Please give some ideas on how to solve this.","y":"Actually, I see what your problem is. You are building the model incorrectly: the FloatFunctional is a \u201cstateful\u201d layer that needs to be initialized in the model constructor. Otherwise, it will not be visible to the convert script. Here is how you can rewrite the model (just an example):\n<code class=\"lang-auto\">def __init__(self):\n        super().__init__()\n        # ... Any other definitions\n        self.q_add0 = FloatFunctional()\n        self.q_add1 = FloatFunctional()\n        self.q_add2 = FloatFunctional()\n        # ... Any other definitions\n\ndef forward(self, x, x2=None, x3=None):\n        x_size = x.size()\n        resl = x\n        for i in range(len(self.pools_sizes)):\n\n            y = self.convs[i](self.pools[i](x))\n            # q_add0 = FloatFunctional()\n#error is because of this line below\n            resl = self.q_add0.add(resl, nn.functional.interpolate(y, x_size[2:], mode='bilinear', align_corners=True))  #error is because of this line\n        resl = self.relu(resl)\n        if self.need_x2:\n            \n            resl = nn.functional.interpolate(resl, x2.size()[2:], mode='bilinear', align_corners=True)\n        resl = self.conv_sum(resl)\n        if self.need_fuse:\n            # q_add1 = FloatFunctional()\n            # q_add2 = FloatFunctional()\n            resl = self.conv_sum_c(self.q_add1.add(self.q_add2.add(resl, x2), x3))\n        return resl\n<\/code>","z":"How are you quantizing? Did you setup qconfigs for FloatFunctionals (q_add0, q_add1 and q_add2) as well? If you set up qconfigs for these, they will get converted to quantized::add and this op will work on quantized tensor.\nYea, it looks like the model was not converted to quantized version correctly. As  mentioned, make sure you have the qconfigs in all the layers that need to be quantized\nActually, I see what your problem is. You are building the model incorrectly: the FloatFunctional is a \u201cstateful\u201d layer that needs to be initialized in the model constructor. Otherwise, it will not be visible to the convert script. Here is how you can rewrite the model (just an example):\n<code class=\"lang-auto\">def __init__(self):\n        super().__init__()\n        # ... Any other definitions\n        self.q_add0 = FloatFunctional()\n        self.q_add1 = FloatFunctional()\n        self.q_add2 = FloatFunctional()\n        # ... Any other definitions\n\ndef forward(self, x, x2=None, x3=None):\n        x_size = x.size()\n        resl = x\n        for i in range(len(self.pools_sizes)):\n\n            y = self.convs[i](self.pools[i](x))\n            # q_add0 = FloatFunctional()\n#error is because of this line below\n            resl = self.q_add0.add(resl, nn.functional.interpolate(y, x_size[2:], mode='bilinear', align_corners=True))  #error is because of this line\n        resl = self.relu(resl)\n        if self.need_x2:\n            \n            resl = nn.functional.interpolate(resl, x2.size()[2:], mode='bilinear', align_corners=True)\n        resl = self.conv_sum(resl)\n        if self.need_fuse:\n            # q_add1 = FloatFunctional()\n            # q_add2 = FloatFunctional()\n            resl = self.conv_sum_c(self.q_add1.add(self.q_add2.add(resl, x2), x3))\n        return resl\n<\/code>\nok. Got it. Thank you for replying. It has solved my problem.\nOne last thing, do I have to create all summation operation unique in for loop?\nFor example,\n<code class=\"lang-auto\">        resl = self.q_add00.add(resl, z0)\n        resl = self.q_add01.add(resl, z1)   \n        resl = self.q_add02.add(resl, z2)\n<\/code>\nIf I do like above mentioned, that part of the model will look something like:\n<code class=\"lang-auto\">      (q_add00): QFunctional(\n        scale=1.027651309967041, zero_point=67\n        (activation_post_process): Identity()\n      )\n      (q_add01): QFunctional(\n        scale=1.0117942094802856, zero_point=68\n        (activation_post_process): Identity()\n      )\n      (q_add02): QFunctional(\n        scale=0.9806106686592102, zero_point=74\n        (activation_post_process): Identity()\n      )\n<\/code>\nI do not know what is wrong but the quantized model has 0 %  accuracy. That is why I am trying different approaches.\nQuantized aware training can be one solution. But, what are the other parameters\/procedures to check\/apply in Post-training static quantization for better accuracy for ResNet-50 backend models?\nI thank you in advance.\nThank you for replying.\nI am currently simply using quantize_model(model, 'fbgemm'). I have also tried taking down the quantize_model and executed it line by line. In both cases, QFunctional  had never appeared in the model. I did not realize it supposed to be a layer.\nI am sorry but what did you mean by,\n\n\n\n dskhudia:\n\nyou set up qconfigs for these\n\n\n\n\n\n parth15041995:\n\nquantize_model\n\n\nI am not sure what is this script? Is it from the tutorials? Either way, if you follow the steps as shown in the static quantization tutorial, it should convert your model to quantized version. As for the accuracy, it could be that low if you don\u2019t calibrate your model before quantizing it: https:\/\/pytorch.org\/tutorials\/advanced\/static_quantization_tutorial.html\nThank you for your reply. It has mostly solved my problem.\nI want to quantized available salient object detection models.\n\n\n\n Zafar:\n\nthis script\n\n\nis one of them."},{"x":"Hi all,\nI am trying the resnet50 model quantization with PyTorch and I tried these 3 lines of code :\nthe import, model=qn.resnet50(pretrain=true), and model.state_dict()), and why the coefficients being shown are all float values if this is the quarantined version of the model?\nNoticed this while trying to figure out how to save\/load the coefficients for a quantized model, and is there anything special you need to do convert model coefficients between float and int8.\nPlease let me know. Appreciate any help\/suggestions , Thanks!","y":"check out https:\/\/pytorch.org\/docs\/stable\/quantization.html#quantized-torch-tensor-operations, in partucular the \u201cint_repr\u201d function.  By default, if you print out a quantized tensor you will see the dequantized values that the tensor represents.  To see the raw int8 values, you can use x.int_repr().","z":"Can anyone help me with this please?\ncheck out https:\/\/pytorch.org\/docs\/stable\/quantization.html#quantized-torch-tensor-operations, in partucular the \u201cint_repr\u201d function.  By default, if you print out a quantized tensor you will see the dequantized values that the tensor represents.  To see the raw int8 values, you can use x.int_repr().\nThank you so much for your response,I will check it.Thanks again.\nHI,I tried this but it gives me this error:\nRuntimeError: Could not run \u2018aten::int_repr\u2019 with arguments from the \u2018CPUTensorId\u2019 backend. \u2018aten::int_repr\u2019 is only available for these backends: [QuantizedCPUTensorId, VariableTensorId]."},{"x":"Hi, I am trying to implement this with link \"https:\/\/arxiv.org\/pdf\/1811.02172.pdf\" paper.\nMy implementation of the paper is here with link \"https:\/\/github.com\/desiredeveloper\/npmtplus\/blob\/master\/main.py\" for any information about the architecture.\nI am calculating the loss manually by taking a negative log of the probability. (Probability of the target sequence is calculated by the equation defined in the paper)\nThe error comes after few backward passes :\n<code class=\"lang-auto\">Warning: Traceback of forward call that caused the error:\n  File \"main.py\", line 455, in <module>\n    train_loss = train(model, train_iterator, optimizer, criterion, CLIP)\n  File \"main.py\", line 381, in train\n    output = model(src, trg)\n  File \"\/home\/shreyansh\/anaconda3\/lib\/python3.7\/site-packages\/torch\/nn\/modules\/module.py\", line 532, in __call__\n    result = self.forward(*input, **kwargs)\n  File \"main.py\", line 346, in forward\n    output = self.decoder(trg, hidden, encoder_outputs)\n  File \"\/home\/shreyansh\/anaconda3\/lib\/python3.7\/site-packages\/torch\/nn\/modules\/module.py\", line 532, in __call__\n    result = self.forward(*input, **kwargs)\n  File \"main.py\", line 308, in forward\n    probabilities = self.stable_softmax(prediction)\n  File \"main.py\", line 250, in stable_softmax\n    numerator = torch.exp(z)\n (print_stack at \/pytorch\/torch\/csrc\/autograd\/python_anomaly_mode.cpp:57)\nTraceback (most recent call last):\n  File \"main.py\", line 455, in <module>\n    train_loss = train(model, train_iterator, optimizer, criterion, CLIP)\n  File \"main.py\", line 385, in train\n    loss.backward()\n  File \"\/home\/shreyansh\/anaconda3\/lib\/python3.7\/site-packages\/torch\/tensor.py\", line 195, in backward\n    torch.autograd.backward(self, gradient, retain_graph, create_graph)\n  File \"\/home\/shreyansh\/anaconda3\/lib\/python3.7\/site-packages\/torch\/autograd\/__init__.py\", line 99, in backward\n    allow_unreachable=True)  # allow_unreachable flag\nRuntimeError: Function 'ExpBackward' returned nan values in its 0th output.\n<\/code>\nThe error is coming while calculating gradients of softmax.\n<code class=\"lang-auto\">  def stable_softmax(self,x):\n    z = x - torch.max(x,dim=1,keepdim=True).values\n    numerator = torch.exp(z)\n    denominator = torch.sum(numerator, dim=1, keepdims=True)\n    softmax = numerator \/ denominator\n    return softmax\n<\/code>\nI am running it on GPU.\ntorch.isnan(x).any() gives False tensor.\ntorch.isnan(z).any() gives False tensor.\nSo, that implies there are no NaNs in the forward pass. I am unable to figure out how NaN is coming in the backward pass.","y":"Thanks  for the help, I finally found the issue.\n\nThese alpha here denotes probability after projection over target vocabulary, I was implementing this equation as is.\nAs you can see there is a summation over multiplication of probabilities due to this alpha values were underflowing.\nI wasn\u2019t aware of this logsumexp with link \"https:\/\/pytorch.org\/docs\/master\/generated\/torch.logsumexp.html\" trick that could have been used to avoid underflow of probabilities by computing all probabilities in log scale.\nIf anyone is interested, go through this link with link \"https:\/\/www.xarg.org\/2016\/06\/the-log-sum-exp-trick-in-machine-learning\/\" to understand the trick. The implementation can be found here - https:\/\/pytorch.org\/docs\/master\/generated\/torch.logsumexp.html","z":"Could you check the input to torch.exp and its output?\nMaybe you are passing large values to it, so that the result might create an Inf output, which might result in a NaN in the backward pass.\nThese values don\u2019t seem to be quite large, I am attaching the logs of max\/min values of input and output to torch.exp\n<code class=\"lang-auto\">Number of training examples: 12907\nNumber of validation examples: 5\nNumber of testing examples: 25\nUnique tokens in source (en) vocabulary: 2804\nUnique tokens in target (hi) vocabulary: 3501\nThe model has 214,411 trainable parameters\nBefore applying exponential\nmax 0.0 min -0.01930026337504387\nAfter applying exponential\nmax 1.0 min 0.9808847904205322\nBefore applying exponential\nmax 0.0 min -0.018911730498075485\nAfter applying exponential\nmax 1.0 min 0.9812659621238708\nBefore applying exponential\nmax 0.0 min -0.019006941467523575\nAfter applying exponential\nmax 1.0 min 0.9811725616455078\nBefore applying exponential\nmax 0.0 min -0.018911048769950867\nAfter applying exponential\nmax 1.0 min 0.9812666177749634\nBefore applying exponential\nmax 0.0 min -0.019007515162229538\nAfter applying exponential\nmax 1.0 min 0.9811719655990601\nBefore applying exponential\nmax 0.0 min -0.01864556595683098\nAfter applying exponential\nmax 1.0 min 0.9815271496772766\nBefore applying exponential\nmax 0.0 min -0.02009735256433487\nAfter applying exponential\nmax 1.0 min 0.9801031947135925\nBefore applying exponential\nmax 0.0 min -0.018839091062545776\nAfter applying exponential\nmax 1.0 min 0.9813372492790222\nBefore applying exponential\nmax 0.0 min -0.018604880198836327\nAfter applying exponential\nmax 1.0 min 0.9815670251846313\nBefore applying exponential\nmax 0.0 min -0.020035836845636368\nAfter applying exponential\nmax 1.0 min 0.9801635146141052\nBefore applying exponential\nmax 0.0 min -0.019339684396982193\nAfter applying exponential\nmax 1.0 min 0.980846107006073\nBefore applying exponential\nmax 0.0 min -0.020187072455883026\nAfter applying exponential\nmax 1.0 min 0.9800152778625488\nBefore applying exponential\nmax 0.0 min -0.01930253952741623\nAfter applying exponential\nmax 1.0 min 0.9808825254440308\nBefore applying exponential\nmax 0.0 min -0.018910178914666176\nAfter applying exponential\nmax 1.0 min 0.981267511844635\nBefore applying exponential\nmax 0.0 min -0.01948946714401245\nAfter applying exponential\nmax 1.0 min 0.9806991815567017\nBefore applying exponential\nmax 0.0 min -0.01900828257203102\nAfter applying exponential\nmax 1.0 min 0.981171190738678\nBefore applying exponential\nmax 0.0 min -0.020097119733691216\nAfter applying exponential\nmax 1.0 min 0.9801034331321716\nBefore applying exponential\nmax 0.0 min -0.01883871853351593\nAfter applying exponential\nmax 1.0 min 0.9813375473022461\nBefore applying exponential\nmax 0.0 min -0.01860380545258522\nAfter applying exponential\nmax 1.0 min 0.9815680980682373\nBefore applying exponential\nmax 0.0 min -0.020035073161125183\nAfter applying exponential\nmax 1.0 min 0.9801642894744873\nBefore applying exponential\nmax 0.0 min -0.019338484853506088\nAfter applying exponential\nmax 1.0 min 0.9808472394943237\nBefore applying exponential\nmax 0.0 min -0.020188473165035248\nAfter applying exponential\nmax 1.0 min 0.980013906955719\nBefore applying exponential\nmax 0.0 min -0.019300060346722603\nAfter applying exponential\nmax 1.0 min 0.9808849096298218\nBefore applying exponential\nmax 0.0 min -0.018910465762019157\nAfter applying exponential\nmax 1.0 min 0.9812671542167664\nBefore applying exponential\nmax 0.0 min -0.019487164914608\nAfter applying exponential\nmax 1.0 min 0.9807014465332031\nBefore applying exponential\nmax 0.0 min -0.01900639571249485\nAfter applying exponential\nmax 1.0 min 0.981173038482666\nBefore applying exponential\nmax 0.0 min -0.01883944869041443\nAfter applying exponential\nmax 1.0 min 0.9813368916511536\nBefore applying exponential\nmax 0.0 min -0.018602870404720306\nAfter applying exponential\nmax 1.0 min 0.9815691113471985\nBefore applying exponential\nmax 0.0 min -0.020035918802022934\nAfter applying exponential\nmax 1.0 min 0.9801634550094604\nBefore applying exponential\nmax 0.0 min -0.019340507686138153\nAfter applying exponential\nmax 1.0 min 0.9808453321456909\nBefore applying exponential\nmax 0.0 min -0.020190240815281868\nAfter applying exponential\nmax 1.0 min 0.9800121784210205\nBefore applying exponential\nmax 0.0 min -0.019304800778627396\nAfter applying exponential\nmax 1.0 min 0.9808803200721741\nBefore applying exponential\nmax 0.0 min -0.018909433856606483\nAfter applying exponential\nmax 1.0 min 0.9812682271003723\nBefore applying exponential\nmax 0.0 min -0.019489070400595665\nAfter applying exponential\nmax 1.0 min 0.9806995987892151\nBefore applying exponential\nmax 0.0 min -0.01900819130241871\nAfter applying exponential\nmax 1.0 min 0.9811713099479675\nBefore applying exponential\nmax 0.0 min -0.018603935837745667\nAfter applying exponential\nmax 1.0 min 0.9815680384635925\nBefore applying exponential\nmax 0.0 min -0.02003599889576435\nAfter applying exponential\nmax 1.0 min 0.9801633954048157\nBefore applying exponential\nmax 0.0 min -0.019340313971042633\nAfter applying exponential\nmax 1.0 min 0.9808454513549805\nBefore applying exponential\nmax 0.0 min -0.0201878622174263\nAfter applying exponential\nmax 1.0 min 0.9800145030021667\nBefore applying exponential\nmax 0.0 min -0.019302817061543465\nAfter applying exponential\nmax 1.0 min 0.9808822870254517\nBefore applying exponential\nmax 0.0 min -0.018910393118858337\nAfter applying exponential\nmax 1.0 min 0.9812672734260559\nBefore applying exponential\nmax 0.0 min -0.019486038014292717\nAfter applying exponential\nmax 1.0 min 0.9807025790214539\nBefore applying exponential\nmax 0.0 min -0.019006924703717232\nAfter applying exponential\nmax 1.0 min 0.9811725616455078\nBefore applying exponential\nmax 0.0 min -0.020037759095430374\nAfter applying exponential\nmax 1.0 min 0.9801616072654724\nBefore applying exponential\nmax 0.0 min -0.019339915364980698\nAfter applying exponential\nmax 1.0 min 0.9808458685874939\nBefore applying exponential\nmax 0.0 min -0.020188800990581512\nAfter applying exponential\nmax 1.0 min 0.9800136685371399\nBefore applying exponential\nmax 0.0 min -0.01930314116179943\nAfter applying exponential\nmax 1.0 min 0.980881929397583\nBefore applying exponential\nmax 0.0 min -0.018910465762019157\nAfter applying exponential\nmax 1.0 min 0.9812671542167664\nBefore applying exponential\nmax 0.0 min -0.019486617296934128\nAfter applying exponential\nmax 1.0 min 0.9807019233703613\nBefore applying exponential\nmax 0.0 min -0.019006475806236267\nAfter applying exponential\nmax 1.0 min 0.9811729788780212\nBefore applying exponential\nmax 0.0 min -0.01933939754962921\nAfter applying exponential\nmax 1.0 min 0.9808463454246521\nBefore applying exponential\nmax 0.0 min -0.020187366753816605\nAfter applying exponential\nmax 1.0 min 0.9800150394439697\nBefore applying exponential\nmax 0.0 min -0.019302254542708397\nAfter applying exponential\nmax 1.0 min 0.9808828234672546\nBefore applying exponential\nmax 0.0 min -0.018910422921180725\nAfter applying exponential\nmax 1.0 min 0.9812672734260559\nBefore applying exponential\nmax 0.0 min -0.0194871723651886\nAfter applying exponential\nmax 1.0 min 0.9807014465332031\nBefore applying exponential\nmax 0.0 min -0.01900804415345192\nAfter applying exponential\nmax 1.0 min 0.9811714291572571\nBefore applying exponential\nmax 0.0 min -0.02018953673541546\nAfter applying exponential\nmax 1.0 min 0.9800128936767578\nBefore applying exponential\nmax 0.0 min -0.01930280774831772\nAfter applying exponential\nmax 1.0 min 0.9808822870254517\nBefore applying exponential\nmax 0.0 min -0.0189106035977602\nAfter applying exponential\nmax 1.0 min 0.9812670946121216\nBefore applying exponential\nmax 0.0 min -0.01948753371834755\nAfter applying exponential\nmax 1.0 min 0.9807010293006897\nBefore applying exponential\nmax 0.0 min -0.019007209688425064\nAfter applying exponential\nmax 1.0 min 0.9811722040176392\nBefore applying exponential\nmax 0.0 min -0.019301380962133408\nAfter applying exponential\nmax 1.0 min 0.9808836579322815\nBefore applying exponential\nmax 0.0 min -0.018910270184278488\nAfter applying exponential\nmax 1.0 min 0.9812673926353455\nBefore applying exponential\nmax 0.0 min -0.019488558173179626\nAfter applying exponential\nmax 1.0 min 0.9807000756263733\nBefore applying exponential\nmax 0.0 min -0.01900819130241871\nAfter applying exponential\nmax 1.0 min 0.9811713099479675\nBefore applying exponential\nmax 0.0 min -0.018910503014922142\nAfter applying exponential\nmax 1.0 min 0.9812671542167664\nBefore applying exponential\nmax 0.0 min -0.019488057121634483\nAfter applying exponential\nmax 1.0 min 0.9807005524635315\nBefore applying exponential\nmax 0.0 min -0.019006900489330292\nAfter applying exponential\nmax 1.0 min 0.9811725616455078\nBefore applying exponential\nmax 0.0 min -0.019488198682665825\nAfter applying exponential\nmax 1.0 min 0.9807004332542419\nBefore applying exponential\nmax 0.0 min -0.01900586299598217\nAfter applying exponential\nmax 1.0 min 0.981173574924469\nBefore applying exponential\nmax 0.0 min -0.018645640462636948\nAfter applying exponential\nmax 1.0 min 0.9815270900726318\nBefore applying exponential\nmax 0.0 min -0.020098166540265083\nAfter applying exponential\nmax 1.0 min 0.9801024198532104\nBefore applying exponential\nmax 0.0 min -0.018839359283447266\nAfter applying exponential\nmax 1.0 min 0.9813370108604431\nBefore applying exponential\nmax 0.0 min -0.018604330718517303\nAfter applying exponential\nmax 1.0 min 0.9815676212310791\nBefore applying exponential\nmax 0.0 min -0.020036987960338593\nAfter applying exponential\nmax 1.0 min 0.9801623821258545\nBefore applying exponential\nmax 0.0 min -0.019339658319950104\nAfter applying exponential\nmax 1.0 min 0.980846107006073\nBefore applying exponential\nmax 0.0 min -0.02018718235194683\nAfter applying exponential\nmax 1.0 min 0.9800151586532593\nBefore applying exponential\nmax 0.0 min -0.019302602857351303\nAfter applying exponential\nmax 1.0 min 0.9808824062347412\nBefore applying exponential\nmax 0.0 min -0.01891184411942959\nAfter applying exponential\nmax 1.0 min 0.9812658429145813\nBefore applying exponential\nmax 0.0 min -0.019488999620079994\nAfter applying exponential\nmax 1.0 min 0.9806996583938599\nBefore applying exponential\nmax 0.0 min -0.01982579194009304\nAfter applying exponential\nmax 1.0 min 0.9803694486618042\nBefore applying exponential\nmax 0.0 min -0.019007962197065353\nAfter applying exponential\nmax 1.0 min 0.9811714887619019\nBefore applying exponential\nmax 0.0 min -0.02009640820324421\nAfter applying exponential\nmax 1.0 min 0.9801041483879089\nBefore applying exponential\nmax 0.0 min -0.01883961260318756\nAfter applying exponential\nmax 1.0 min 0.9813367128372192\nBefore applying exponential\nmax 0.0 min -0.01860237866640091\nAfter applying exponential\nmax 1.0 min 0.9815695881843567\nBefore applying exponential\nmax 0.0 min -0.02003667876124382\nAfter applying exponential\nmax 1.0 min 0.9801627397537231\nBefore applying exponential\nmax 0.0 min -0.0193393062800169\nAfter applying exponential\nmax 1.0 min 0.9808464646339417\nBefore applying exponential\nmax 0.0 min -0.020189685747027397\nAfter applying exponential\nmax 1.0 min 0.9800127744674683\nBefore applying exponential\nmax 0.0 min -0.01930147223174572\nAfter applying exponential\nmax 1.0 min 0.9808835387229919\nBefore applying exponential\nmax 0.0 min -0.0189093928784132\nAfter applying exponential\nmax 1.0 min 0.9812682271003723\nBefore applying exponential\nmax 0.0 min -0.01948663219809532\nAfter applying exponential\nmax 1.0 min 0.9807019233703613\nBefore applying exponential\nmax 0.0 min -0.019825227558612823\nAfter applying exponential\nmax 1.0 min 0.9803699851036072\nBefore applying exponential\nmax 0.0 min -0.01900714449584484\nAfter applying exponential\nmax 1.0 min 0.9811723232269287\nBefore applying exponential\nmax 0.0 min -0.018840152770280838\nAfter applying exponential\nmax 1.0 min 0.9813361763954163\nBefore applying exponential\nmax 0.0 min -0.018603524193167686\nAfter applying exponential\nmax 1.0 min 0.981568455696106\nBefore applying exponential\nmax 0.0 min -0.020036067813634872\nAfter applying exponential\nmax 1.0 min 0.9801632761955261\nBefore applying exponential\nmax 0.0 min -0.019339565187692642\nAfter applying exponential\nmax 1.0 min 0.9808462262153625\nBefore applying exponential\nmax 0.0 min -0.020190367475152016\nAfter applying exponential\nmax 1.0 min 0.9800121188163757\nBefore applying exponential\nmax 0.0 min -0.019302181899547577\nAfter applying exponential\nmax 1.0 min 0.9808829426765442\nBefore applying exponential\nmax 0.0 min -0.018910503014922142\nAfter applying exponential\nmax 1.0 min 0.9812671542167664\nBefore applying exponential\nmax 0.0 min -0.01948685571551323\nAfter applying exponential\nmax 1.0 min 0.9807018041610718\nBefore applying exponential\nmax 0.0 min -0.019825518131256104\nAfter applying exponential\nmax 1.0 min 0.9803696870803833\nBefore applying exponential\nmax 0.0 min -0.019008396193385124\nAfter applying exponential\nmax 1.0 min 0.9811710715293884\nBefore applying exponential\nmax 0.0 min -0.01860380545258522\nAfter applying exponential\nmax 1.0 min 0.9815680980682373\nBefore applying exponential\nmax 0.0 min -0.020034171640872955\nAfter applying exponential\nmax 1.0 min 0.9801651835441589\nBefore applying exponential\nmax 0.0 min -0.01933983899652958\nAfter applying exponential\nmax 1.0 min 0.9808459281921387\nBefore applying exponential\nmax 0.0 min -0.020188678056001663\nAfter applying exponential\nmax 1.0 min 0.9800137281417847\nBefore applying exponential\nmax 0.0 min -0.01930253580212593\nAfter applying exponential\nmax 1.0 min 0.9808825254440308\nBefore applying exponential\nmax 0.0 min -0.018911192193627357\nAfter applying exponential\nmax 1.0 min 0.981266438961029\nBefore applying exponential\nmax 0.0 min -0.019490612670779228\nAfter applying exponential\nmax 1.0 min 0.9806980490684509\nBefore applying exponential\nmax 0.0 min -0.019825948402285576\nAfter applying exponential\nmax 1.0 min 0.9803692698478699\nBefore applying exponential\nmax 0.0 min -0.01900610886514187\nAfter applying exponential\nmax 1.0 min 0.9811733365058899\nBefore applying exponential\nmax 0.0 min -0.02003747597336769\nAfter applying exponential\nmax 1.0 min 0.9801619052886963\nBefore applying exponential\nmax 0.0 min -0.019340286031365395\nAfter applying exponential\nmax 1.0 min 0.9808454513549805\nBefore applying exponential\nmax 0.0 min -0.02018577791750431\nAfter applying exponential\nmax 1.0 min 0.9800165295600891\nBefore applying exponential\nmax 0.0 min -0.019299039617180824\nAfter applying exponential\nmax 1.0 min 0.9808859825134277\nBefore applying exponential\nmax 0.0 min -0.018910575658082962\nAfter applying exponential\nmax 1.0 min 0.9812670946121216\nBefore applying exponential\nmax 0.0 min -0.019486989825963974\nAfter applying exponential\nmax 1.0 min 0.9807016253471375\nBefore applying exponential\nmax 0.0 min -0.01982567459344864\nAfter applying exponential\nmax 1.0 min 0.980369508266449\nBefore applying exponential\nmax 0.0 min -0.019006211310625076\nAfter applying exponential\nmax 1.0 min 0.9811732172966003\nBefore applying exponential\nmax 0.0 min -0.019340457394719124\nAfter applying exponential\nmax 1.0 min 0.9808453321456909\nBefore applying exponential\nmax 0.0 min -0.020189344882965088\nAfter applying exponential\nmax 1.0 min 0.9800130724906921\nBefore applying exponential\nmax 0.0 min -0.01930450275540352\nAfter applying exponential\nmax 1.0 min 0.9808805584907532\nBefore applying exponential\nmax 0.0 min -0.01891041174530983\nAfter applying exponential\nmax 1.0 min 0.9812672734260559\nBefore applying exponential\nmax 0.0 min -0.019486669450998306\nAfter applying exponential\nmax 1.0 min 0.9807019233703613\nBefore applying exponential\nmax 0.0 min -0.019827280193567276\nAfter applying exponential\nmax 1.0 min 0.9803679585456848\nBefore applying exponential\nmax 0.0 min -0.019005414098501205\nAfter applying exponential\nmax 1.0 min 0.9811739921569824\nBefore applying exponential\nmax 0.0 min -0.02018619142472744\nAfter applying exponential\nmax 1.0 min 0.9800161719322205\nBefore applying exponential\nmax 0.0 min -0.019303616136312485\nAfter applying exponential\nmax 1.0 min 0.9808814525604248\nBefore applying exponential\nmax 0.0 min -0.0189113337546587\nAfter applying exponential\nmax 1.0 min 0.9812663793563843\nBefore applying exponential\nmax 0.0 min -0.019487900659441948\nAfter applying exponential\nmax 1.0 min 0.9807007312774658\nBefore applying exponential\nmax 0.0 min -0.019827045500278473\nAfter applying exponential\nmax 1.0 min 0.9803681969642639\nBefore applying exponential\nmax 0.0 min -0.019005149602890015\nAfter applying exponential\nmax 1.0 min 0.9811742305755615\nBefore applying exponential\nmax 0.0 min -0.01930542290210724\nAfter applying exponential\nmax 1.0 min 0.9808796644210815\nBefore applying exponential\nmax 0.0 min -0.018911056220531464\nAfter applying exponential\nmax 1.0 min 0.9812666177749634\nBefore applying exponential\nmax 0.0 min -0.019487980753183365\nAfter applying exponential\nmax 1.0 min 0.980700671672821\nBefore applying exponential\nmax 0.0 min -0.019826088100671768\nAfter applying exponential\nmax 1.0 min 0.9803690910339355\nBefore applying exponential\nmax 0.0 min -0.019005481153726578\nAfter applying exponential\nmax 1.0 min 0.9811739325523376\nBefore applying exponential\nmax 0.0 min -0.018912117928266525\nAfter applying exponential\nmax 1.0 min 0.9812655448913574\nBefore applying exponential\nmax 0.0 min -0.019485829398036003\nAfter applying exponential\nmax 1.0 min 0.9807027578353882\nBefore applying exponential\nmax 0.0 min -0.019825410097837448\nAfter applying exponential\nmax 1.0 min 0.9803697466850281\nBefore applying exponential\nmax 0.0 min -0.019007179886102676\nAfter applying exponential\nmax 1.0 min 0.9811723232269287\nBefore applying exponential\nmax 0.0 min -0.019486064091324806\nAfter applying exponential\nmax 1.0 min 0.9807025194168091\nBefore applying exponential\nmax 0.0 min -0.019826460629701614\nAfter applying exponential\nmax 1.0 min 0.9803687930107117\nBefore applying exponential\nmax 0.0 min -0.019005997106432915\nAfter applying exponential\nmax 1.0 min 0.9811734557151794\nBefore applying exponential\nmax 0.0 min -0.01982627622783184\nAfter applying exponential\nmax 1.0 min 0.980368971824646\nloss is tensor(97.8701, device='cuda:0', grad_fn=<NegBackward>)\n<\/code>\nYou are right, these values look alright, but they also don\u2019t produce the NaN issue, right?\nCould you check the values for a couple of more iterations until you encounter the first NaN value?\nUnfortunately, the code breaks in this iteration itself during back propagation.\n<code class=\"lang-auto\">Before applying exponential\nmax 0.0 min -0.01982627622783184\nAfter applying exponential\nmax 1.0 min 0.980368971824646\nloss is tensor(97.8701, device='cuda:0', grad_fn=<NegBackward>)\nWarning: Traceback of forward call that caused the error:\n  File \"main.py\", line 468, in <module>\n    train_loss = train(model, train_iterator, optimizer, criterion, CLIP)\n  File \"main.py\", line 386, in train\n    output = model(src, trg)\n  File \"\/home\/shreyansh\/anaconda3\/lib\/python3.7\/site-packages\/torch\/nn\/modules\/module.py\", line 532, in __call__\n    result = self.forward(*input, **kwargs)\n  File \"main.py\", line 351, in forward\n    output = self.decoder(trg, hidden, encoder_outputs)\n  File \"\/home\/shreyansh\/anaconda3\/lib\/python3.7\/site-packages\/torch\/nn\/modules\/module.py\", line 532, in __call__\n    result = self.forward(*input, **kwargs)\n  File \"main.py\", line 313, in forward\n    probabilities = self.stable_softmax(prediction)\n  File \"main.py\", line 253, in stable_softmax\n    numerator = torch.exp(z)\n (print_stack at \/pytorch\/torch\/csrc\/autograd\/python_anomaly_mode.cpp:57)\nTraceback (most recent call last):\n  File \"main.py\", line 468, in <module>\n    train_loss = train(model, train_iterator, optimizer, criterion, CLIP)\n  File \"main.py\", line 394, in train\n    loss.backward()\n  File \"\/home\/shreyansh\/anaconda3\/lib\/python3.7\/site-packages\/torch\/tensor.py\", line 195, in backward\n    torch.autograd.backward(self, gradient, retain_graph, create_graph)\n  File \"\/home\/shreyansh\/anaconda3\/lib\/python3.7\/site-packages\/torch\/autograd\/__init__.py\", line 99, in backward\n    allow_unreachable=True)  # allow_unreachable flag\nRuntimeError: Function 'ExpBackward' returned nan values in its 0th output.\n<\/code>\nThanks for the update.\nCould you check the denominator as well and make sure that it\u2019s not too small so that the result might overflow?\nYes, sure. These are the max\/min values of the denominator before getting an error.\n<code class=\"lang-auto\">max 3466.501953125 min 3464.61669921875\nmax 3466.2783203125 min 3462.81103515625\nmax 3466.5009765625 min 3464.6171875\nmax 3466.274658203125 min 3462.81591796875\nmax 3464.37353515625 min 3463.282470703125\nmax 3466.50537109375 min 3464.61962890625\nmax 3464.37109375 min 3463.28759765625\nmax 3466.51123046875 min 3464.6162109375\nmax 3466.273193359375 min 3462.81494140625\nmax 3464.3701171875 min 3463.287109375\nmax 3466.9697265625 min 3462.463623046875\nmax 3466.506591796875 min 3464.61962890625\nmax 3464.37255859375 min 3463.28173828125\nmax 3466.96875 min 3462.469482421875\nmax 3466.50732421875 min 3464.61767578125\nmax 3466.97021484375 min 3462.467041015625\nmax 3466.507080078125 min 3464.619873046875\nmax 3466.27734375 min 3462.8134765625\nmax 3464.37109375 min 3463.281982421875\nmax 3466.966796875 min 3462.467529296875\nmax 3468.5478515625 min 3464.38232421875\nmax 3466.505859375 min 3464.6142578125\nmax 3464.3720703125 min 3463.286865234375\nmax 3466.9716796875 min 3462.46923828125\nmax 3468.5419921875 min 3464.3779296875\nmax 3466.51025390625 min 3464.61572265625\nmax 3466.9677734375 min 3462.47021484375\nmax 3468.54345703125 min 3464.37744140625\nmax 3466.50634765625 min 3464.61865234375\nmax 3468.544677734375 min 3464.38330078125\nmax 3466.50244140625 min 3464.6181640625\nmax 3466.27099609375 min 3462.8115234375\nmax 3464.37548828125 min 3463.283935546875\nmax 3466.9716796875 min 3462.466064453125\nmax 3468.54248046875 min 3464.383544921875\nmax 3464.380615234375 min 3463.459716796875\nmax 3466.50634765625 min 3464.6181640625\nmax 3464.3740234375 min 3463.28271484375\nmax 3466.970703125 min 3462.4658203125\nmax 3468.54541015625 min 3464.37841796875\nmax 3464.37890625 min 3463.457763671875\nmax 3466.5078125 min 3464.6123046875\nmax 3466.970703125 min 3462.46826171875\nmax 3468.541748046875 min 3464.382568359375\nmax 3464.381591796875 min 3463.4609375\nmax 3466.505859375 min 3464.61669921875\nmax 3468.5390625 min 3464.3798828125\nmax 3464.38330078125 min 3463.45654296875\nmax 3466.50537109375 min 3464.61669921875\nmax 3464.377197265625 min 3463.455078125\nmax 3466.50390625 min 3464.615234375\nmax 3466.273681640625 min 3462.81298828125\nmax 3464.375 min 3463.28759765625\nmax 3466.964599609375 min 3462.46630859375\nmax 3468.542724609375 min 3464.37841796875\nmax 3464.3818359375 min 3463.45703125\nmax 3466.3603515625 min 3464.38037109375\nmax 3466.50537109375 min 3464.6181640625\nmax 3464.376953125 min 3463.2841796875\nmax 3466.96728515625 min 3462.462890625\nmax 3468.547119140625 min 3464.383056640625\nmax 3464.37841796875 min 3463.4580078125\nmax 3466.356201171875 min 3464.379638671875\nmax 3466.503662109375 min 3464.61474609375\nmax 3466.97119140625 min 3462.468505859375\nmax 3468.54248046875 min 3464.38232421875\nmax 3464.38232421875 min 3463.4599609375\nmax 3466.36083984375 min 3464.37939453125\nmax 3466.501953125 min 3464.62109375\nmax 3468.546875 min 3464.38037109375\nmax 3464.380859375 min 3463.458984375\nmax 3466.359130859375 min 3464.38134765625\nmax 3466.505859375 min 3464.61767578125\nmax 3464.380859375 min 3463.4541015625\nmax 3466.3515625 min 3464.379150390625\nmax 3466.506103515625 min 3464.614990234375\nmax 3466.35595703125 min 3464.380859375\nmax 3466.503173828125 min 3464.6171875\nmax 3466.27001953125 min 3462.81787109375\nmax 3464.369873046875 min 3463.279052734375\nmax 3466.9697265625 min 3462.46630859375\nmax 3468.541259765625 min 3464.378662109375\nmax 3464.3818359375 min 3463.45703125\nmax 3466.35888671875 min 3464.3828125\nmax 3465.03955078125 min 3464.380615234375\nmax 3466.50341796875 min 3464.61572265625\nmax 3464.373046875 min 3463.2841796875\nmax 3466.967529296875 min 3462.471435546875\nmax 3468.5419921875 min 3464.383544921875\nmax 3464.383544921875 min 3463.459716796875\nmax 3466.35791015625 min 3464.38134765625\nmax 3465.0439453125 min 3464.376220703125\nmax 3466.50341796875 min 3464.61572265625\nmax 3466.969482421875 min 3462.46630859375\nmax 3468.545166015625 min 3464.3857421875\nmax 3464.37841796875 min 3463.458984375\nmax 3466.35595703125 min 3464.379150390625\nmax 3465.04296875 min 3464.3798828125\nmax 3466.5078125 min 3464.61572265625\nmax 3468.539794921875 min 3464.3779296875\nmax 3464.376708984375 min 3463.46044921875\nmax 3466.359375 min 3464.378662109375\nmax 3465.0478515625 min 3464.376708984375\nmax 3466.50830078125 min 3464.61669921875\nmax 3464.377685546875 min 3463.4560546875\nmax 3466.3583984375 min 3464.381591796875\nmax 3465.04541015625 min 3464.3818359375\nmax 3466.50537109375 min 3464.6162109375\nmax 3466.352783203125 min 3464.381591796875\nmax 3465.0400390625 min 3464.379638671875\nmax 3466.505615234375 min 3464.6142578125\nmax 3465.03955078125 min 3464.38330078125\nmax 3466.5087890625 min 3464.6181640625\nmax 3466.27587890625 min 3462.8134765625\nmax 3464.370361328125 min 3463.28271484375\nmax 3466.96826171875 min 3462.46728515625\nmax 3468.545654296875 min 3464.382568359375\nmax 3464.380859375 min 3463.45654296875\nmax 3466.36083984375 min 3464.37939453125\nmax 3465.04931640625 min 3464.383056640625\nmax 3464.953125 min 3464.38037109375\nmax 3466.5029296875 min 3464.61767578125\nmax 3464.37109375 min 3463.28271484375\nmax 3466.9697265625 min 3462.4658203125\nmax 3468.545654296875 min 3464.38037109375\nmax 3464.383056640625 min 3463.455810546875\nmax 3466.3564453125 min 3464.37841796875\nmax 3465.0380859375 min 3464.3818359375\nmax 3464.95654296875 min 3464.376708984375\nmax 3466.50634765625 min 3464.614501953125\nmax 3466.96728515625 min 3462.470947265625\nmax 3468.54150390625 min 3464.3779296875\nmax 3464.383056640625 min 3463.4609375\nmax 3466.358154296875 min 3464.37939453125\nmax 3465.039306640625 min 3464.376953125\nmax 3464.9580078125 min 3464.38427734375\nmax 3466.49951171875 min 3464.61376953125\nmax 3468.54345703125 min 3464.3837890625\nmax 3464.381591796875 min 3463.45751953125\nmax 3466.358154296875 min 3464.3828125\nmax 3465.03857421875 min 3464.3798828125\nmax 3464.95361328125 min 3464.381103515625\nmax 3466.5009765625 min 3464.61572265625\nmax 3464.382568359375 min 3463.45263671875\nmax 3466.357666015625 min 3464.383056640625\nmax 3465.04296875 min 3464.376953125\nmax 3464.9599609375 min 3464.379150390625\nmax 3466.50732421875 min 3464.61572265625\nmax 3466.360107421875 min 3464.382568359375\nmax 3465.04443359375 min 3464.378662109375\nmax 3464.95947265625 min 3464.380859375\nmax 3466.506103515625 min 3464.6171875\nmax 3465.043701171875 min 3464.38134765625\nmax 3464.95849609375 min 3464.380126953125\nmax 3466.50341796875 min 3464.6171875\nmax 3464.95849609375 min 3464.383544921875\nmax 3466.50439453125 min 3464.61181640625\nmax 3466.27490234375 min 3462.81494140625\nmax 3464.37158203125 min 3463.2861328125\nmax 3466.9716796875 min 3462.463623046875\nmax 3468.54638671875 min 3464.376708984375\nmax 3464.38232421875 min 3463.4560546875\nmax 3466.359375 min 3464.38330078125\nmax 3465.0458984375 min 3464.3837890625\nmax 3464.957763671875 min 3464.3779296875\nmax 3466.45751953125 min 3464.38427734375\nmax 3466.5 min 3464.6181640625\nmax 3464.3759765625 min 3463.285400390625\nmax 3466.97021484375 min 3462.466552734375\nmax 3468.541015625 min 3464.37548828125\nmax 3464.3828125 min 3463.4560546875\nmax 3466.357177734375 min 3464.378173828125\nmax 3465.0439453125 min 3464.3837890625\nmax 3464.955322265625 min 3464.385498046875\nmax 3466.4580078125 min 3464.3779296875\nmax 3466.5029296875 min 3464.6181640625\nmax 3466.9716796875 min 3462.46337890625\nmax 3468.54296875 min 3464.380859375\nmax 3464.380126953125 min 3463.455078125\nmax 3466.3623046875 min 3464.384033203125\nmax 3465.044921875 min 3464.37890625\nmax 3464.95947265625 min 3464.380859375\nmax 3466.45556640625 min 3464.385009765625\nmax 3466.5048828125 min 3464.61572265625\nmax 3468.544921875 min 3464.384765625\nmax 3464.380859375 min 3463.45849609375\nmax 3466.35986328125 min 3464.376708984375\nmax 3465.04443359375 min 3464.378662109375\nmax 3464.953125 min 3464.379150390625\nmax 3466.45849609375 min 3464.3828125\nmax 3466.5048828125 min 3464.61767578125\nmax 3464.379150390625 min 3463.45947265625\nmax 3466.357421875 min 3464.3828125\nmax 3465.045654296875 min 3464.378173828125\nmax 3464.9609375 min 3464.380126953125\nmax 3466.4560546875 min 3464.380859375\nmax 3466.5009765625 min 3464.615234375\nmax 3466.36279296875 min 3464.377685546875\nmax 3465.045654296875 min 3464.3818359375\nmax 3464.95654296875 min 3464.385986328125\nmax 3466.45947265625 min 3464.38232421875\nmax 3466.506591796875 min 3464.617431640625\nmax 3465.04248046875 min 3464.385498046875\nmax 3464.9619140625 min 3464.382080078125\nmax 3466.45703125 min 3464.381103515625\nmax 3466.50830078125 min 3464.613525390625\nmax 3464.959716796875 min 3464.379638671875\nmax 3466.456298828125 min 3464.37646484375\nmax 3466.504638671875 min 3464.61669921875\nmax 3466.45556640625 min 3464.383056640625\nmax 3466.50341796875 min 3464.618408203125\nmax 3466.274658203125 min 3462.815185546875\nmax 3464.371826171875 min 3463.285888671875\nmax 3466.9697265625 min 3462.462646484375\nmax 3468.54248046875 min 3464.377197265625\nmax 3464.37646484375 min 3463.456787109375\nmax 3466.3583984375 min 3464.37939453125\nmax 3465.04443359375 min 3464.37890625\nmax 3464.95556640625 min 3464.378173828125\nmax 3466.45849609375 min 3464.38330078125\nmax 3465.380126953125 min 3464.380859375\nmax 3466.500244140625 min 3464.61767578125\nmax 3464.3740234375 min 3463.285888671875\nmax 3466.97412109375 min 3462.466552734375\nmax 3468.547119140625 min 3464.37646484375\nmax 3464.3818359375 min 3463.45947265625\nmax 3466.361083984375 min 3464.380126953125\nmax 3465.04345703125 min 3464.3818359375\nmax 3464.96142578125 min 3464.377685546875\nmax 3466.458251953125 min 3464.377685546875\nmax 3465.383544921875 min 3464.376953125\nmax 3466.50634765625 min 3464.61376953125\nmax 3466.97119140625 min 3462.464111328125\nmax 3468.543212890625 min 3464.3837890625\nmax 3464.37939453125 min 3463.4580078125\nmax 3466.35791015625 min 3464.3818359375\nmax 3465.039794921875 min 3464.382568359375\nmax 3464.952880859375 min 3464.381591796875\nmax 3466.461181640625 min 3464.385498046875\nmax 3465.382080078125 min 3464.38330078125\nmax 3466.501220703125 min 3464.614501953125\nmax 3468.54296875 min 3464.381591796875\nmax 3464.3798828125 min 3463.45654296875\nmax 3466.35693359375 min 3464.380126953125\nmax 3465.04345703125 min 3464.382080078125\nmax 3464.955078125 min 3464.37890625\nmax 3466.459228515625 min 3464.380859375\nmax 3465.386474609375 min 3464.38134765625\nmax 3466.505859375 min 3464.6162109375\nmax 3464.38427734375 min 3463.45458984375\nmax 3466.35888671875 min 3464.38427734375\nmax 3465.0419921875 min 3464.376708984375\nmax 3464.955078125 min 3464.376220703125\nmax 3466.458251953125 min 3464.37890625\nmax 3465.384033203125 min 3464.380126953125\nmax 3466.50537109375 min 3464.61767578125\nmax 3466.358642578125 min 3464.378662109375\nmax 3465.04443359375 min 3464.380615234375\nmax 3464.9580078125 min 3464.38330078125\nmax 3466.45849609375 min 3464.37890625\nmax 3465.38427734375 min 3464.3759765625\nmax 3466.5009765625 min 3464.6162109375\nmax 3465.039306640625 min 3464.3828125\nmax 3464.95654296875 min 3464.379150390625\nmax 3466.45654296875 min 3464.378662109375\nmax 3465.383544921875 min 3464.383544921875\nmax 3466.503662109375 min 3464.619140625\nmax 3464.96044921875 min 3464.3837890625\nmax 3466.45849609375 min 3464.382080078125\nmax 3465.381103515625 min 3464.384765625\nmax 3466.501220703125 min 3464.6171875\nmax 3466.459716796875 min 3464.380126953125\nmax 3465.38232421875 min 3464.38232421875\nmax 3466.50244140625 min 3464.61767578125\nmax 3465.3828125 min 3464.377197265625\nmax 3466.5078125 min 3464.614013671875\nmax 3466.27490234375 min 3462.81884765625\nmax 3464.3720703125 min 3463.287353515625\nmax 3466.97265625 min 3462.46435546875\nmax 3468.542236328125 min 3464.3818359375\nmax 3464.3798828125 min 3463.45654296875\nmax 3466.35986328125 min 3464.38037109375\nmax 3465.044677734375 min 3464.377685546875\nmax 3464.956787109375 min 3464.376708984375\nmax 3466.454345703125 min 3464.3828125\nmax 3465.382568359375 min 3464.379638671875\nmax 3464.5341796875 min 3464.37744140625\nmax 3466.501953125 min 3464.61767578125\nmax 3464.373046875 min 3463.284423828125\nmax 3466.9697265625 min 3462.462646484375\nmax 3468.543701171875 min 3464.38525390625\nmax 3464.378173828125 min 3463.45654296875\nmax 3466.359130859375 min 3464.382080078125\nmax 3465.03955078125 min 3464.382568359375\nmax 3464.957275390625 min 3464.38134765625\nmax 3466.459716796875 min 3464.377197265625\nmax 3465.38623046875 min 3464.380615234375\nmax 3464.535400390625 min 3464.378173828125\nmax 3466.50341796875 min 3464.613525390625\nmax 3466.970703125 min 3462.4658203125\nmax 3468.54296875 min 3464.3837890625\nmax 3464.384033203125 min 3463.45654296875\nmax 3466.3583984375 min 3464.380859375\nmax 3465.0400390625 min 3464.379150390625\nmax 3464.9580078125 min 3464.383056640625\nmax 3466.458984375 min 3464.37841796875\nmax 3465.386474609375 min 3464.38037109375\nmax 3464.535888671875 min 3464.3798828125\nmax 3466.5009765625 min 3464.6162109375\nmax 3468.545654296875 min 3464.383056640625\nmax 3464.3818359375 min 3463.46142578125\nmax 3466.35693359375 min 3464.382568359375\nmax 3465.041015625 min 3464.38037109375\nmax 3464.95751953125 min 3464.3828125\nmax 3466.456298828125 min 3464.384521484375\nmax 3465.37841796875 min 3464.37890625\nmax 3464.534423828125 min 3464.38525390625\nmax 3466.5068359375 min 3464.6162109375\nmax 3464.379150390625 min 3463.455810546875\nmax 3466.3583984375 min 3464.38134765625\nmax 3465.0478515625 min 3464.379150390625\nmax 3464.962890625 min 3464.3818359375\nmax 3466.457275390625 min 3464.3818359375\nmax 3465.385009765625 min 3464.376953125\nmax 3464.5341796875 min 3464.3779296875\nmax 3466.50634765625 min 3464.6201171875\nmax 3466.35791015625 min 3464.38037109375\nmax 3465.0419921875 min 3464.385498046875\nmax 3464.95458984375 min 3464.380859375\nmax 3466.45849609375 min 3464.380126953125\nmax 3465.38525390625 min 3464.38525390625\nmax 3464.531982421875 min 3464.3759765625\nmax 3466.5087890625 min 3464.6171875\nmax 3465.045654296875 min 3464.3876953125\nmax 3464.95361328125 min 3464.384033203125\nmax 3466.4560546875 min 3464.38037109375\nmax 3465.3828125 min 3464.381591796875\nmax 3464.531005859375 min 3464.37890625\nmax 3466.50830078125 min 3464.61474609375\nmax 3464.9521484375 min 3464.37646484375\nmax 3466.45751953125 min 3464.38134765625\nmax 3465.383056640625 min 3464.382568359375\nmax 3464.534423828125 min 3464.38427734375\nmax 3466.507080078125 min 3464.61767578125\nmax 3466.45361328125 min 3464.383056640625\nmax 3465.387939453125 min 3464.38623046875\nmax 3464.534423828125 min 3464.380615234375\nmax 3466.502685546875 min 3464.6171875\nmax 3465.38623046875 min 3464.3818359375\nmax 3464.5322265625 min 3464.380859375\nmax 3466.506103515625 min 3464.61474609375\nmax 3464.533447265625 min 3464.3759765625\nloss is tensor(97.8701, device='cuda:0', grad_fn=<NegBackward>)\nWarning: Traceback of forward call that caused the error:\n  File \"main.py\", line 465, in <module>\n    train_loss = train(model, train_iterator, optimizer, criterion, CLIP)\n  File \"main.py\", line 383, in train\n    output = model(src, trg)\n  File \"\/home\/shreyansh\/anaconda3\/lib\/python3.7\/site-packages\/torch\/nn\/modules\/module.py\", line 532, in __call__\n    result = self.forward(*input, **kwargs)\n  File \"main.py\", line 348, in forward\n    output = self.decoder(trg, hidden, encoder_outputs)\n  File \"\/home\/shreyansh\/anaconda3\/lib\/python3.7\/site-packages\/torch\/nn\/modules\/module.py\", line 532, in __call__\n    result = self.forward(*input, **kwargs)\n  File \"main.py\", line 310, in forward\n    probabilities = self.stable_softmax(prediction)\n  File \"main.py\", line 251, in stable_softmax\n    numerator = torch.exp(z)\n (print_stack at \/pytorch\/torch\/csrc\/autograd\/python_anomaly_mode.cpp:57)\nTraceback (most recent call last):\n  File \"main.py\", line 465, in <module>\n    train_loss = train(model, train_iterator, optimizer, criterion, CLIP)\n  File \"main.py\", line 391, in train\n    loss.backward()\n  File \"\/home\/shreyansh\/anaconda3\/lib\/python3.7\/site-packages\/torch\/tensor.py\", line 195, in backward\n    torch.autograd.backward(self, gradient, retain_graph, create_graph)\n  File \"\/home\/shreyansh\/anaconda3\/lib\/python3.7\/site-packages\/torch\/autograd\/__init__.py\", line 99, in backward\n    allow_unreachable=True)  # allow_unreachable flag\nRuntimeError: Function 'ExpBackward' returned nan values in its 0th output.\n<\/code>\nThanks for the update. Still unsure, where this might be coming from.\nCould you check all parameters and gradients for invalid values?\nYou could use torch.isfinite(tensor) to check for valid values.\nAlright, I have printed this and the output seems fine(all tensors are valid).\n<code class=\"lang-auto\">    for param in model.parameters():\n      print(\"param.data\",torch.isfinite(param.data).all())\n      print(\"param.grad.data\",torch.isfinite(param.grad.data).all(),\"\\n\")\n<\/code>\nPlease tell me if you mean to see something else.\n<code class=\"lang-auto\">After optimization step\n\nparam.data tensor(True, device='cuda:0')\nparam.grad.data tensor(True, device='cuda:0') \n\nparam.data tensor(True, device='cuda:0')\nparam.grad.data tensor(True, device='cuda:0') \n\nparam.data tensor(True, device='cuda:0')\nparam.grad.data tensor(True, device='cuda:0') \n\nparam.data tensor(True, device='cuda:0')\nparam.grad.data tensor(True, device='cuda:0') \n\nparam.data tensor(True, device='cuda:0')\nparam.grad.data tensor(True, device='cuda:0') \n\nparam.data tensor(True, device='cuda:0')\nparam.grad.data tensor(True, device='cuda:0') \n\nparam.data tensor(True, device='cuda:0')\nparam.grad.data tensor(True, device='cuda:0') \n\nparam.data tensor(True, device='cuda:0')\nparam.grad.data tensor(True, device='cuda:0') \n\nparam.data tensor(True, device='cuda:0')\nparam.grad.data tensor(True, device='cuda:0') \n\nparam.data tensor(True, device='cuda:0')\nparam.grad.data tensor(True, device='cuda:0') \n\nparam.data tensor(True, device='cuda:0')\nparam.grad.data tensor(True, device='cuda:0') \n\nparam.data tensor(True, device='cuda:0')\nparam.grad.data tensor(True, device='cuda:0') \n\nparam.data tensor(True, device='cuda:0')\nparam.grad.data tensor(True, device='cuda:0') \n\nparam.data tensor(True, device='cuda:0')\nparam.grad.data tensor(True, device='cuda:0') \n\nparam.data tensor(True, device='cuda:0')\nparam.grad.data tensor(True, device='cuda:0') \n\nparam.data tensor(True, device='cuda:0')\nparam.grad.data tensor(True, device='cuda:0') \n\nparam.data tensor(True, device='cuda:0')\nparam.grad.data tensor(True, device='cuda:0') \n\nparam.data tensor(True, device='cuda:0')\nparam.grad.data tensor(True, device='cuda:0') \n\nparam.data tensor(True, device='cuda:0')\nparam.grad.data tensor(True, device='cuda:0') \n\nparam.data tensor(True, device='cuda:0')\nparam.grad.data tensor(True, device='cuda:0') \n\nparam.data tensor(True, device='cuda:0')\nparam.grad.data tensor(True, device='cuda:0') \n\nparam.data tensor(True, device='cuda:0')\nparam.grad.data tensor(True, device='cuda:0') \n\nparam.data tensor(True, device='cuda:0')\nparam.grad.data tensor(True, device='cuda:0') \n\nparam.data tensor(True, device='cuda:0')\nparam.grad.data tensor(True, device='cuda:0') \n\nparam.data tensor(True, device='cuda:0')\nparam.grad.data tensor(True, device='cuda:0') \n\nparam.data tensor(True, device='cuda:0')\nparam.grad.data tensor(True, device='cuda:0') \n\nparam.data tensor(True, device='cuda:0')\nparam.grad.data tensor(True, device='cuda:0') \n\nparam.data tensor(True, device='cuda:0')\nparam.grad.data tensor(True, device='cuda:0') \n\nparam.data tensor(True, device='cuda:0')\nparam.grad.data tensor(True, device='cuda:0') \n\nparam.data tensor(True, device='cuda:0')\nparam.grad.data tensor(True, device='cuda:0') \n\nparam.data tensor(True, device='cuda:0')\nparam.grad.data tensor(True, device='cuda:0') \n\nparam.data tensor(True, device='cuda:0')\nparam.grad.data tensor(True, device='cuda:0') \n\nparam.data tensor(True, device='cuda:0')\nparam.grad.data tensor(True, device='cuda:0') \n\nparam.data tensor(True, device='cuda:0')\nparam.grad.data tensor(True, device='cuda:0') \n\nparam.data tensor(True, device='cuda:0')\nparam.grad.data tensor(True, device='cuda:0') \n\nparam.data tensor(True, device='cuda:0')\nparam.grad.data tensor(True, device='cuda:0') \n\nparam.data tensor(True, device='cuda:0')\nparam.grad.data tensor(True, device='cuda:0') \n\nparam.data tensor(True, device='cuda:0')\nparam.grad.data tensor(True, device='cuda:0') \n\nparam.data tensor(True, device='cuda:0')\nparam.grad.data tensor(True, device='cuda:0') \n\nparam.data tensor(True, device='cuda:0')\nparam.grad.data tensor(True, device='cuda:0') \n\nparam.data tensor(True, device='cuda:0')\nparam.grad.data tensor(True, device='cuda:0') \n\nparam.data tensor(True, device='cuda:0')\nparam.grad.data tensor(True, device='cuda:0') \n\nparam.data tensor(True, device='cuda:0')\nparam.grad.data tensor(True, device='cuda:0') \n\nparam.data tensor(True, device='cuda:0')\nparam.grad.data tensor(True, device='cuda:0') \n\nparam.data tensor(True, device='cuda:0')\nparam.grad.data tensor(True, device='cuda:0') \n\nparam.data tensor(True, device='cuda:0')\nparam.grad.data tensor(True, device='cuda:0') \n\nparam.data tensor(True, device='cuda:0')\nparam.grad.data tensor(True, device='cuda:0') \n\nloss is tensor(97.8701, device='cuda:0', grad_fn=<NegBackward>)\nWarning: Traceback of forward call that caused the error:\n  File \"main.py\", line 476, in <module>\n    train_loss = train(model, train_iterator, optimizer, criterion, CLIP)\n  File \"main.py\", line 383, in train\n    output = model(src, trg)\n  File \"\/home\/shreyansh\/anaconda3\/lib\/python3.7\/site-packages\/torch\/nn\/modules\/module.py\", line 532, in __call__\n    result = self.forward(*input, **kwargs)\n  File \"main.py\", line 348, in forward\n    output = self.decoder(trg, hidden, encoder_outputs)\n  File \"\/home\/shreyansh\/anaconda3\/lib\/python3.7\/site-packages\/torch\/nn\/modules\/module.py\", line 532, in __call__\n    result = self.forward(*input, **kwargs)\n  File \"main.py\", line 310, in forward\n    probabilities = self.stable_softmax(prediction)\n  File \"main.py\", line 251, in stable_softmax\n    numerator = torch.exp(z)\n (print_stack at \/pytorch\/torch\/csrc\/autograd\/python_anomaly_mode.cpp:57)\nTraceback (most recent call last):\n  File \"main.py\", line 476, in <module>\n    train_loss = train(model, train_iterator, optimizer, criterion, CLIP)\n  File \"main.py\", line 391, in train\n    loss.backward()\n  File \"\/home\/shreyansh\/anaconda3\/lib\/python3.7\/site-packages\/torch\/tensor.py\", line 195, in backward\n    torch.autograd.backward(self, gradient, retain_graph, create_graph)\n  File \"\/home\/shreyansh\/anaconda3\/lib\/python3.7\/site-packages\/torch\/autograd\/__init__.py\", line 99, in backward\n    allow_unreachable=True)  # allow_unreachable flag\nRuntimeError: Function 'ExpBackward' returned nan values in its 0th output.\n<\/code>\nThanks for the great debugging so far.\nAre you able to reproduce this issue using random input data?\nIf so, could you post the complete code including all seeds etc. which would reproduce this issue please, so that we can debug further?\nI am pasting the code here, alternatively, the code and data can be cloned from here with link \"https:\/\/github.com\/desiredeveloper\/npmtplus\".\nThe model won\u2019t work well for longer sequences as it takes the product of probabilities, however, I am getting the error even when I am testing on a smaller sequence length (say 5). If length is restricted on random input data then the error can be reproduced.\n<code class=\"lang-auto\">import torch\nimport torch.nn as nn\nimport torch.nn.functional as F\nimport torch.optim as optim\n\nfrom torchtext.datasets import TranslationDataset\nfrom torchtext.data import Field, BucketIterator\n\nimport spacy\nimport numpy as np\n\nimport random\nimport math\nimport time\n\nSEED = 1234\n\nrandom.seed(SEED)\nnp.random.seed(SEED)\ntorch.manual_seed(SEED)\ntorch.cuda.manual_seed(SEED)\ntorch.backends.cudnn.deterministic = True\ntorch.autograd.set_detect_anomaly(True)\n\nspacy_en = spacy.load('en')\n\ndef tokenize_en(text):\n    \"\"\"\n    Tokenizes English text from a string into a list of strings (tokens) and reverses it\n    \"\"\"\n    return [tok.text for tok in spacy_en.tokenizer(text)][::-1]\n\ndef tokenize_hi(text):\n    \"\"\"\n    Tokenizes Hindi text from a string into a list of strings (tokens) \n    \"\"\"\n    return text.split()\n\nSRC = Field(tokenize = tokenize_en, \n            init_token = '<sos>', \n            eos_token = '<eos>', \n            lower = True)\n\nTRG = Field(tokenize = tokenize_hi, \n            init_token = '<sos>', \n            eos_token = '<eos>', \n            lower = True)\n\ntrain_data, valid_data, test_data  = TranslationDataset.splits(\n                                      path='IITB_small',\n                                      validation='dev',\n                                      exts = ('.en', '.hi'), \n                                      fields = (SRC, TRG))\n\nprint(f\"Number of training examples: {len(train_data.examples)}\")\nprint(f\"Number of validation examples: {len(valid_data.examples)}\")\nprint(f\"Number of testing examples: {len(test_data.examples)}\")\n\nvars(train_data.examples[0])\n\nSRC.build_vocab(train_data, min_freq = 2)\nTRG.build_vocab(train_data, min_freq = 2,specials=['<pad>','<sop>','<eop>'])\n\nprint(f\"Unique tokens in source (en) vocabulary: {len(SRC.vocab)}\")\nprint(f\"Unique tokens in target (hi) vocabulary: {len(TRG.vocab)}\")\n\ndevice = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n\ndevice \n\nBATCH_SIZE = 2\n\ntrain_iterator, valid_iterator, test_iterator = BucketIterator.splits(\n    (train_data, valid_data, test_data),\n    batch_size = BATCH_SIZE, \n    device = device)\n\n\"\"\"# EnCoder Parameters\"\"\"\n\ninput_dim = len(SRC.vocab)\nembed_dim = 10\nhidden_dim = 10\nsegment_dim = 10\nn_layers = 6\ndropout = 0.4\nsegment_threshold = 5\ntemperature = 0.1\n\n\"\"\"# Building Encoder\"\"\"\n\nclass Encoder(nn.Module):\n  def __init__(self,input_dim,embed_dim,hidden_dim,segment_dim,n_layers,dropout,segment_threshold,device):\n    super().__init__()\n    self.input_dim = input_dim\n    self.hidden_dim = hidden_dim\n    self.n_layers = n_layers\n    self.segment_threshold = segment_threshold\n    self.segment_dim = segment_dim\n    self.device = device\n    \n    self.embedding = nn.Embedding(input_dim,embed_dim)\n    self.rnn = nn.GRU(embed_dim,hidden_dim,n_layers,dropout=dropout,bidirectional=True)\n\n    self.segmentRnn = nn.GRU(hidden_dim*2,segment_dim,n_layers,dropout=dropout)\n    self.fc = nn.Linear(hidden_dim*2,hidden_dim)\n    self.dropout = nn.Dropout(dropout)\n\n  def forward(self,input):\n\n    #input = [src len, batch size]\n    embedded = self.dropout(self.embedding(input))\n    #embedded = [src len, batch size, emb dim]\n\n    outputs, hidden = self.rnn(embedded)\n    #outputs = [src len, batch size, hid dim * num directions]\n    #hidden = [n layers * num directions, batch size, hid dim]\n        \n    segment_encoding, hidden = self.segment_rnn(outputs)\n    #segment_encoding = [src len* (src len+1)\/2, batch size, segment_dim*num_directions]\n    #hidden = [n layers * num_directions, batch size, hid dim]\n\n    # hidden = torch.tanh(self.fc(torch.cat((hidden[-2],hidden[-1]),dim=1)))\n\n    return segment_encoding,hidden\n\n  def segment_rnn(self,outputs):\n    N = outputs.shape[0]\n    batch_size = outputs.shape[1]\n    dp_forward = torch.zeros(N, N, batch_size, self.segment_dim).to(self.device)\n    dp_backward = torch.zeros(N, N, batch_size, self.segment_dim).to(self.device)\n\n    for i in range(N):\n      hidden_forward = torch.randn(self.n_layers, batch_size, self.hidden_dim).to(self.device)\n      for j in range(i, min(N, i + self.segment_threshold)):\n        \n        # outputs[j] = [batch size, hidden_dim* num_direction]\n        next_input = outputs[j].unsqueeze(0)\n        # next_input = [1, batch size, hidden_dim* num_direction]\n        \n        out, hidden_forward = self.segmentRnn(next_input,hidden_forward)\n        #out = [1, batch size, segment_dim]\n        #hidden_forward = [n layers , batch size, hid dim]\n\n        dp_forward[i][j] = out.squeeze(0)\n\n    for i in range(N):\n      hidden_backward = torch.randn(self.n_layers, batch_size, self.hidden_dim).to(self.device)\n      for j in range(i, max(-1, i - self.segment_threshold), -1):\n\n        # outputs[j] = [batch size, hidden_dim* num_direction]\n        next_input = outputs[j].unsqueeze(0)\n        # next_input = [1, batch size, hidden_dim* num_direction]\n        \n        out, hidden_backward = self.segmentRnn(next_input,hidden_backward)\n        #out = [1, batch size, segment_dim]\n        #hidden_backward = [n layers , batch size, hid dim]\n        \n        dp_backward[j][i] = out.squeeze(0)\n    \n    dp = torch.cat((dp_forward,dp_backward),dim=3)\n    dp_indices = torch.triu_indices(N, N)\n    dp = dp[dp_indices[0],dp_indices[1]]\n    return dp,torch.cat((hidden_forward,hidden_backward),dim=2)\n\n\"\"\"# Defining Attn Network\"\"\"\n'''\nAttention is calculated over encoder_outputs S(i,j) and context representation\nof previously generated segments (from Target Decoder)\n'''\nclass Attention(nn.Module):\n  def __init__(self, enc_hid_dim, dec_hid_dim):\n    super().__init__()\n\n    self.attn = nn.Linear((enc_hid_dim * 2) + dec_hid_dim, dec_hid_dim)\n    self.v = nn.Linear(dec_hid_dim, 1, bias = False)\n\n  def forward(self, encoder_outputs, output_target_decoder):\n      \n    #encoder_outputs = [no. of segments, batch size, enc hid dim * 2]\n    #output_target_decoder = [batch size, dec hid dim]\n    batch_size = encoder_outputs.shape[1]\n    src_len = encoder_outputs.shape[0]\n    \n    #repeat decoder hidden state src_len times\n    output_target_decoder = output_target_decoder.unsqueeze(1).repeat(1, src_len, 1)\n    \n    encoder_outputs = encoder_outputs.permute(1, 0, 2)\n    \n    #output_target_decoder = [batch size, no. of segments, dec hid dim]\n    #encoder_outputs = [batch size, no. of segments, enc hid dim * 2]\n    \n    energy = torch.tanh(self.attn(torch.cat((output_target_decoder, encoder_outputs), dim = 2))) \n    #energy = [batch size,  no. of segments, dec hid dim]\n    attention = self.v(energy).squeeze(2)\n    #attention= [batch size,  no. of segments]\n    a = F.softmax(attention, dim=1)\n    #a = [batch size,  no. of segments]\n    a = a.unsqueeze(1)\n    #a = [batch size, 1,  no. of segments]\n    weighted = torch.bmm(a, encoder_outputs)\n    #weighted = [batch size, 1, enc hid dim * 2]\n    weighted = weighted.permute(1, 0, 2)\n    #weighted = [1, batch size, enc hid dim * 2]\n    return weighted\n    \n\n\"\"\"# Decoder Parameters\"\"\"\n\noutput_dim = len(TRG.vocab)\nDEC_HEADS = 8\nDEC_PF_DIM = 512\n# embed_dim = 256\n# hidden_dim = 256\n# segment_dim = 256\n# n_layers = 6\n# dropout = 0.4\n# segment_threshold = 5\n\n\"\"\"# Building Decoder\"\"\"\n\nclass Decoder(nn.Module):\n  def __init__(self, output_dim, embed_dim, hidden_dim,segment_dim,n_layers, dropout, attention):\n    super().__init__()\n    self.output_dim = output_dim\n    self.n_layers = n_layers\n    self.hidden_dim = hidden_dim\n    self.attention = attention\n    self.device = device\n    self.embedding = nn.Embedding(self.output_dim, embed_dim)\n    self.rnn = nn.GRU(embed_dim,hidden_dim,n_layers,dropout=dropout)\n    self.rnn = nn.GRU(embed_dim,hidden_dim,n_layers,dropout=dropout)\n    self.segmentRnn = nn.GRU(hidden_dim,hidden_dim,n_layers,dropout=dropout)\n    self.fc_out = nn.Linear((hidden_dim * 2) + hidden_dim + embed_dim, self.output_dim)\n    # self.soft = nn.LogSoftmax(dim=1)\n    self.soft = nn.Softmax(dim=1)\n    self.dropout = nn.Dropout(dropout)\n    \n  def stable_softmax(self,x):\n    z = x - torch.max(x,dim=1,keepdim=True).values\n    numerator = torch.exp(z)\n    denominator = torch.sum(numerator, dim=1, keepdims=True)\n    softmax = numerator \/ denominator\n    return softmax\n  \n  def forward(self, input, hidden, encoder_outputs):\n          \n    #input = [target_len,batch size]\n    #hidden = [batch size, dec hid dim]\n    #encoder_outputs = [src len, batch size, enc hid dim * 2]\n    \n    embedded = self.embedding(input)\n    #embedded = [target_len, batch size, emb dim]\n    \n    output_target_decoder,hidden_target_decoder = self.rnn(embedded)\n    #output_target_decoder = [target_len, batch size, hidden_dim]\n    #hidden_target_decoder = [n layers , batch size, hidden_dim]\n    \n    trg_len = input.shape[0]\n    batch_size = input.shape[1]\n    trg_vocab_size = self.output_dim\n    # later to be passed in constructor (currently accessing through Globals)\n    sop_symbol = TRG.vocab.stoi['<sop>']\n    eop_symbol = TRG.vocab.stoi['<eop>']\n    \n    alpha = torch.zeros(batch_size,trg_len).to(self.device)\n    alpha[:,0] = 1\n    for end in range(1,trg_len):\n\n      for phraseLen in range(end,0,-1):\n        start = end - phraseLen + 1\n        weighted = self.attention(encoder_outputs, output_target_decoder[start-1])\n        \n        sop_vector = (torch.ones(1,batch_size,dtype=torch.int64)*sop_symbol).to(self.device)\n        input_phrase = input[start:end+1,:]\n        input_phrase = torch.cat((sop_vector,input_phrase),0)\n        eop_vector = (torch.ones(1,batch_size,dtype=torch.int64)*eop_symbol).to(self.device)\n        input_phrase = torch.cat((input_phrase,eop_vector),0)\n        \n        phraseEmbedded = self.embedding(input_phrase)\n        \n        # currEmbedded = phraseEmbedded[0,:,:]\n        # rnn_input = torch.cat((currEmbedded.unsqueeze(0), weighted), dim = 2)\n        \n        phraseProb = torch.ones(batch_size).to(self.device)\n        for t in range(input_phrase.shape[0]-1):\n          rnn_input = phraseEmbedded[t].unsqueeze(0)\n          output, hidden = self.segmentRnn(rnn_input)\n          \n          output = output.squeeze(0)\n          weighted = weighted.squeeze(0)\n          rnn_input = rnn_input.squeeze(0)\n          \n          prediction = self.fc_out(torch.cat((output, weighted, rnn_input), dim = 1))\n          #prediction = [batch size, output dim]\n          # probabilities = self.soft(prediction)\n          # phraseProb *= torch.exp(probabilities[torch.arange(batch_size),input_phrase[t+1]])\n\n          probabilities = self.stable_softmax(prediction)\n          phraseProb *= probabilities[torch.arange(batch_size),input_phrase[t+1]]\n        \n        alpha[:,end] = alpha[:,end].clone() + phraseProb*alpha[:,start-1].clone()\n    \n    return alpha\n      \nclass NP2MT(nn.Module):\n  def __init__(self, encoder, decoder, device):\n    super().__init__()\n    \n    self.encoder = encoder\n    self.decoder = decoder\n    self.device = device\n      \n  def forward(self, src, trg, teacher_forcing_ratio = 0.5):\n    \n    #src = [src len, batch size]\n    #trg = [trg len, batch size]\n    #teacher_forcing_ratio is probability to use teacher forcing\n    #e.g. if teacher_forcing_ratio is 0.75 we use teacher forcing 75% of the time\n    \n    batch_size = src.shape[1]\n    trg_len = trg.shape[0]\n    trg_vocab_size = self.decoder.output_dim\n    \n    ''' moved this to Decoder now\n    # later to be passed in constructor (currently accessing through Globals)\n    sop_symbol = TRG.vocab.stoi['<sop>']\n    eop_symbol = TRG.vocab.stoi['<eop>']\n    \n    #tensor to store decoder outputs\n    outputs = torch.zeros(trg_len, batch_size, trg_vocab_size).to(self.device)\n    '''\n    \n    #encoder_outputs is representation of all phrases states of the input sequence, back and forwards\n    #hidden is the final forward and backward hidden states, passed through a linear layer (batch_size*hidden_dim)\n    encoder_outputs, hidden = self.encoder(src)\n    output = self.decoder(trg, hidden, encoder_outputs)\n    return output[:,-1]\n\nattn = Attention(hidden_dim, hidden_dim)\nenc = Encoder(input_dim, embed_dim, hidden_dim, segment_dim, n_layers, dropout, segment_threshold, device)\ndec = Decoder(output_dim, embed_dim, hidden_dim, segment_dim, n_layers, dropout, attn)\n\nmodel = NP2MT(enc, dec, device).to(device)\n\ndef init_weights(m):\n  for name, param in m.named_parameters():\n    if 'weight' in name:\n      nn.init.normal_(param.data, mean=0, std=0.01)\n    else:\n      nn.init.constant_(param.data, 0)\n            \nmodel.apply(init_weights)\n\ndef count_parameters(model):\n  return sum(p.numel() for p in model.parameters() if p.requires_grad)\n\nprint(f'The model has {count_parameters(model):,} trainable parameters')\n\noptimizer = optim.Adam(model.parameters())\nTRG_PAD_IDX = TRG.vocab.stoi[TRG.pad_token]\ncriterion = nn.CrossEntropyLoss(ignore_index = TRG_PAD_IDX)\n\ndef train(model, iterator, optimizer, criterion, clip):\n  \n  model.train()\n  \n  epoch_loss = 0\n  \n  for i, batch in enumerate(iterator):\n    \n    src = batch.src\n    trg = batch.trg\n    \n    optimizer.zero_grad()\n    \n    output = model(src, trg)\n    \n    loss = -torch.log(output).mean()\n\n    loss.backward()\n    \n    # print(\"Before optimization step\\n\\n\")\n    # for name, param in model.named_parameters():\n    #   if param.requires_grad:\n    #       print(name, torch.isnan(param.data).any())\n    \n    torch.nn.utils.clip_grad_norm_(model.parameters(), clip)\n    \n    optimizer.step()\n\n    # print(\"After optimization step\\n\\n\")\n\n    # for name, param in model.named_parameters():\n    #   if param.requires_grad:\n    #       print(name, torch.isnan(param.data).any())\n    \n    epoch_loss += loss.item()\n    \n  return epoch_loss \/ len(iterator)\n\ndef evaluate(model, iterator, criterion):\n    \n  model.eval()\n  \n  epoch_loss = 0\n  \n  with torch.no_grad():\n\n    for i, batch in enumerate(iterator):\n\n        src = batch.src\n        trg = batch.trg\n\n        output = model(src, trg, 0) #turn off teacher forcing\n\n        #trg = [trg len, batch size]\n        #output = [trg len, batch size, output dim]\n\n        output_dim = output.shape[-1]\n        \n        output = output[1:].view(-1, output_dim)\n        trg = trg[1:].view(-1)\n\n        #trg = [(trg len - 1) * batch size]\n        #output = [(trg len - 1) * batch size, output dim]\n\n        loss = criterion(output, trg)\n\n        epoch_loss += loss.item()\n      \n  return epoch_loss \/ len(iterator)\n\ndef epoch_time(start_time, end_time):\n  elapsed_time = end_time - start_time\n  elapsed_mins = int(elapsed_time \/ 60)\n  elapsed_secs = int(elapsed_time - (elapsed_mins * 60))\n  return elapsed_mins, elapsed_secs\n\nN_EPOCHS = 10\nCLIP = 1\n\nbest_valid_loss = float('inf')\n\nfor epoch in range(N_EPOCHS):\n    \n  start_time = time.time()\n  \n  train_loss = train(model, train_iterator, optimizer, criterion, CLIP)\n  # valid_loss = evaluate(model, valid_iterator, criterion)\n  \n  end_time = time.time()\n  \n  epoch_mins, epoch_secs = epoch_time(start_time, end_time)\n  \n  # if valid_loss < best_valid_loss:\n  #   best_valid_loss = valid_loss\n  #   torch.save(model.state_dict(), 'npmt-model.pt')\n  \n  print(f'Epoch: {epoch+1:02} | Time: {epoch_mins}m {epoch_secs}s')\n  print(f'\\tTrain Loss: {train_loss:.3f} | Train PPL: {math.exp(train_loss):7.3f}')\n  # print(f'\\t Val. Loss: {valid_loss:.3f} |  Val. PPL: {math.exp(valid_loss):7.3f}')\n\n# model.load_state_dict(torch.load('npmt-model.pt'))\n\n# test_loss = evaluate(model, test_iterator, criterion)\n\n# print(f'| Test Loss: {test_loss:.3f} | Test PPL: {math.exp(test_loss):7.3f} |')\n<\/code>\nThanks  for the help, I finally found the issue.\n\nThese alpha here denotes probability after projection over target vocabulary, I was implementing this equation as is.\nAs you can see there is a summation over multiplication of probabilities due to this alpha values were underflowing.\nI wasn\u2019t aware of this logsumexp with link \"https:\/\/pytorch.org\/docs\/master\/generated\/torch.logsumexp.html\" trick that could have been used to avoid underflow of probabilities by computing all probabilities in log scale.\nIf anyone is interested, go through this link with link \"https:\/\/www.xarg.org\/2016\/06\/the-log-sum-exp-trick-in-machine-learning\/\" to understand the trick. The implementation can be found here - https:\/\/pytorch.org\/docs\/master\/generated\/torch.logsumexp.html\nlogsumexp has saved my arse a time or two also "},{"x":"Is it possible to run a model with fbgemm qconfig on mobile? Or is it x86 only? Simply plugging such model into demo app triggers qnnpack assert here https:\/\/github.com\/pytorch\/pytorch\/blob\/master\/aten\/src\/ATen\/native\/quantized\/cpu\/qconv_prepack.cpp#L223\nIt seems like FBGEMM support was disabled by this commit for some reason https:\/\/github.com\/pytorch\/pytorch\/commit\/6fead9afd4cdc6306fb0e2180ca625160b59ea71","y":"FBGEMM is supported only for x86. You can get very good accuracies for qnnpack also.\nPlease make sure that when you set:\n<code class=\"lang-auto\">qconfig = torch.quantization.get_default_qconfig('qnnpack')\n<\/code>\nYou also do:\n<code class=\"lang-auto\">torch.backends.quantized.engine = 'qnnpack'\n<\/code>\nbefore running the model.\nThe poorer accuracy numbers are likely due to FBGEMM saturating for large weight\/activation values, due to this issue:\n\n\ngithub.com with link \"https:\/\/github.com\/intel\/mkl-dnn\/blob\/f38fecf5b76421fe277cfb15ec1d5090f1d30c07\/doc\/advanced\/int8_computations.md#1-inputs-of-mixed-type-u8-and-s8\"\n\n\nintel\/mkl-dnn\/blob\/f38fecf5b76421fe277cfb15ec1d5090f1d30c07\/doc\/advanced\/int8_computations.md#1-inputs-of-mixed-type-u8-and-s8 with link \"https:\/\/github.com\/intel\/mkl-dnn\/blob\/f38fecf5b76421fe277cfb15ec1d5090f1d30c07\/doc\/advanced\/int8_computations.md#1-inputs-of-mixed-type-u8-and-s8\"\n<code class=\"lang-md\">Int8 Computation Aspects {#dev_guide_int8_computations}\n=======================================================\n\n> This document uses **int8** to denote 8-bit integer no matter whether it is\n> signed or unsigned. To emphasize the signedness of the data type\n> **u8** (`uint8_t`) or **s8** (`int8_t`) are used. In particular, if a\n> primitive has two inputs the types would be written using \"\/\". For instance:\n> - int8 GEMM denotes any integer GEMM with 8-bit integer inputs, while\n> - u8\/s8 GEMM denotes dnnl_gemm_u8s8s32() only.\n\nThe operation primitives that work with the int8 data type\n(#dnnl::memory::data_type::s8 and #dnnl::memory::data_type::u8)\ntypically use s32 (`int32_t`) as an intermediate data type\n(#dnnl::memory::data_type::s32) to avoid integer overflows.\n\nFor instance, the int8 average [pooling]( dev_guide_pooling) primitive\naccumulates the int8 input values in a window to an s32 accumulator, then\ndivides the result by the window size, and then stores the result back to the\nint8 destination:\n\n<\/code>\n\n  This file has been truncated. show original with link \"https:\/\/github.com\/intel\/mkl-dnn\/blob\/f38fecf5b76421fe277cfb15ec1d5090f1d30c07\/doc\/advanced\/int8_computations.md#1-inputs-of-mixed-type-u8-and-s8\"\n\n\n\n\n\n","z":"I wasn\u2019t able to get good results from QNNPACK compatible per tensor quantization qconfig. Target metric value relative to fp32 model:\nget_default_qconfig('fbgemm') -> 99.8%\nget_default_qconfig('qnnpack') -> 58.5%\ndefault_qconfig -> 54.4%\nIs there any way to reduce that gap without changing architecture?\nFBGEMM is supported only for x86. You can get very good accuracies for qnnpack also.\nPlease make sure that when you set:\n<code class=\"lang-auto\">qconfig = torch.quantization.get_default_qconfig('qnnpack')\n<\/code>\nYou also do:\n<code class=\"lang-auto\">torch.backends.quantized.engine = 'qnnpack'\n<\/code>\nbefore running the model.\nThe poorer accuracy numbers are likely due to FBGEMM saturating for large weight\/activation values, due to this issue:\n\n\ngithub.com with link \"https:\/\/github.com\/intel\/mkl-dnn\/blob\/f38fecf5b76421fe277cfb15ec1d5090f1d30c07\/doc\/advanced\/int8_computations.md#1-inputs-of-mixed-type-u8-and-s8\"\n\n\nintel\/mkl-dnn\/blob\/f38fecf5b76421fe277cfb15ec1d5090f1d30c07\/doc\/advanced\/int8_computations.md#1-inputs-of-mixed-type-u8-and-s8 with link \"https:\/\/github.com\/intel\/mkl-dnn\/blob\/f38fecf5b76421fe277cfb15ec1d5090f1d30c07\/doc\/advanced\/int8_computations.md#1-inputs-of-mixed-type-u8-and-s8\"\n<code class=\"lang-md\">Int8 Computation Aspects {#dev_guide_int8_computations}\n=======================================================\n\n> This document uses **int8** to denote 8-bit integer no matter whether it is\n> signed or unsigned. To emphasize the signedness of the data type\n> **u8** (`uint8_t`) or **s8** (`int8_t`) are used. In particular, if a\n> primitive has two inputs the types would be written using \"\/\". For instance:\n> - int8 GEMM denotes any integer GEMM with 8-bit integer inputs, while\n> - u8\/s8 GEMM denotes dnnl_gemm_u8s8s32() only.\n\nThe operation primitives that work with the int8 data type\n(#dnnl::memory::data_type::s8 and #dnnl::memory::data_type::u8)\ntypically use s32 (`int32_t`) as an intermediate data type\n(#dnnl::memory::data_type::s32) to avoid integer overflows.\n\nFor instance, the int8 average [pooling]( dev_guide_pooling) primitive\naccumulates the int8 input values in a window to an s32 accumulator, then\ndivides the result by the window size, and then stores the result back to the\nint8 destination:\n\n<\/code>\n\n  This file has been truncated. show original with link \"https:\/\/github.com\/intel\/mkl-dnn\/blob\/f38fecf5b76421fe277cfb15ec1d5090f1d30c07\/doc\/advanced\/int8_computations.md#1-inputs-of-mixed-type-u8-and-s8\"\n\n\n\n\n\n\nThanks. With engine set preparation, calibration, and conversion of the model work fine. But evaluation triggers errors like: Error in QNNPACK: failed to create convolution with 0.1966128 input scale, 1.698165 kernel scale, and 0.2075303 output scale: convolution scale 1.608829 is greater or equal to 1.0. The cause seems to be in the SE block implemented via 1x1 convolution that receives 1x1 input. I probably should have used Linear anyway, but maybe it will be useful to someone.\nOk, I\u2019ve managed to get good result from QNNPACK. Maybe torch.backends.quantized.engine should be mentioned somewhere on quantization page with link \"https:\/\/pytorch.org\/docs\/master\/quantization.html\"?\nGreat that this worked! We will make sure to mention this on our quantization page. Thanks for the suggestion! cc "},{"x":"I am using torch.jit to trace a pretrained vanilla resnet50 to import over to iOS and call using Pytorch Mobile \/\/ C++ on iOS.\n<code class=\"lang-auto\">- (NSInteger)predictImage:(void*)imageBuffer forLabels:(NSInteger)labelCount {\n    int outputLabelIndex = -1;\n    try {\n        std::cout << \"\\npredictImage\";\n    at::Tensor tensor = torch::from_blob(imageBuffer, {1, 3, 224, 224}, at::kFloat);\n        std::cout << \"\\npredictImageTwo\";\n    torch::autograd::AutoGradMode guard(false);\n    at::AutoNonVariableTypeMode non_var_type_mode(true);\n\/\/ Pass in image tensor to C++ scripted torch module\n    **auto outputTensor = _impl.forward({tensor}).toTensor();**\n        std::cout << \"\\nReceived outputTensor\";\n<\/code>\nThe line before I print received outputTensor never prints because I never receive anything from _impl.forward in the Predict Image function in my \u201cTorchModule.mm\u201d file in my iOS project.\nBasically, I\u2019ve gotten resnet18, 34, and mobilenet to work using Pytorch Mobile and the iOS demo but won\u2019t receive the output tensor for resnet50 and above\u2026 is resnet50 supported? If someone is able to get a resnet50 working for mobile could they help?\nMuch appreciated!","y":"\nThe master is back to normal. You can try recompiling from source code. Let me know if you have any questions.","z":"<code class=\"lang-auto\">#from torchvision.models import inception_v3\ndevice = torch.device('cpu')\n\nmodel = models.resnet50(pretrained=True)\nmodel.load_state_dict(models.resnet50(pretrained=True).state_dict())\nmodel = nn.Sequential(\n    #ImageScale(),\n    model,\n    nn.Softmax(1)\n)\nmodel.eval()\ninput_tensor = torch.rand(1,3,224,224)\nscript_model = torch.jit.trace(model, input_tensor)\nscript_model.save(\"models\/resnet50.pt\")\n<\/code>\nJust for reference this is how I\u2019m saving my model before taking it into iOS. Worked perfectly for resnet34 and resnet18\nHi Haris, this is a known issue. We\u2019ve been working on fixing it. The problem is that the pthreadpool runs into a deadlock situation when running resnet50.  There are a couple of work around for it, you can try\n\nSet the number of thread to one in ThreadPool.cc\nUse a different mutex in int ThreadPool::getNumThreads() const function\nUse std::unique_lock<std::mutex> guard(executionMutex_, std::defer_lock); instead\n\nThen recompile the PyTorch from source code by following the link here - https:\/\/pytorch.org\/mobile\/ios\/#build-pytorch-ios-libraries-from-source.\nHi Tao!\nThanks so much for the response! For \u201c3.\u201d just to clarify you\u2019re suggesting std:: unique_lock as an alternative mutex to use for the 2nd solution? Will try this tomorrow and update here. Also I don\u2019t have to do both one and two do I? I can do either one xor two?\nSorry for the confusion, I just re-edited the comment. Actually, I have a fix being reviewed here - https:\/\/github.com\/pytorch\/pytorch\/pull\/29885. If you\u2019d like to try it out, you can patch that PR.\nI cloned the Pytorch source repo and then followed the instructions to recompile\/build iOS libraries (with the same file patch\/changes as the PR)\nI then replaced the install folder under Pods\/Libtorch\/Install in my iOS project with the newly compiled install folder; I don\u2019t get any errors regarding path changes and Swift builds the project correctly.\nHowever, when calling Predict Image on any model now (including previously working resnet34), I get the following check failederror:\n48%20AM806\u00d7113 31.3 KB with link \"https:\/\/discuss.pytorch.org\/uploads\/default\/original\/3X\/a\/2\/a22090b1fb60f9ad74a1d827b0e883c78d301cb8.png\"\nTracing into the Dispatcher.h in the repo, I find it\u2019s breaking here\n<code class=\"lang-auto\">  const std::string dispatchKeyStr = toString(*dispatchKey);\n  TORCH_CHECK(false, \"Could not run '\", dispatchTable.operatorName(), \"' with arguments\",\n          \" from the '\", dispatchKeyStr, \"' backend. '\",\n          dispatchTable.operatorName(), \"' is only available for these backends: \",\n          dispatchTable.listAllDispatchKeys(), \".\");\n}\n<\/code>\nPreviously my Libtorch was installed from Cocopods specifically version 1.3.1. Is the version compiled from the repo I got from following the steps an equivalent version?\nUsing a 1.4 nightly build of pytorch for the Python jit trace, was working perfectly fine before.\nI\u2019ve seen false check errors solved by updating Pytorch or using nightly builds?\nIn this case for the patch how can I prevent this, or could I take a specific file with the pthread change from my newly compiled install folder and replace at Libtorch in my project?\nThanks in advance,\nHaris\nHi Hussain, if you use BUILD_PYTORCH_MOBILE=1 IOS_ARCH=arm64 .\/scripts\/build_ios.sh to build your libraries, you shouldn\u2019t see that error.\nIf you\u2019re still seeing that, I believe that was come out this morning or later yesterday. Obviously, our mobile CI failed to do its job. I\u2019m working on adding the simulator tests now. Sorry for the frustration. Will have updates here once we\u2019ve fixed it.\nYep that\u2019s exactly how I built it and got the issue.\nYeah please let me know here when the Mobile CI is fixed and what versions it should work on\/I should be building, I would greatly appreciate it!\n\nThe master is back to normal. You can try recompiling from source code. Let me know if you have any questions.\nHey, I recompiled source code after pulling on Monday and it worked just fine! Thanks for all the help.\nJust a quick question before closing thread, what are the limitations on the types of Pytorch models that can currently go mobile with a trace? Ex) Inception, Faster RCNN (for object detection), segmentation models with encoders and decoders etc\u2026\nMost models should be compatible with tracing or scripting.  More details are at https:\/\/pytorch.org\/docs\/stable\/jit.html .  If any ScriptModule works on server but not mobile, we would consider that a bug."},{"x":"Hello,\nI\u2019ve tried to use custom model on Android but forward fails with error I unable to understand.\nI\u2019ve scripted my model using TorchScript annotation method and now I am trying to preform a forward on mobile.\nModel looks something like that:\n<code class=\"lang-auto\">config = ...\nclass WrapRPN(nn.Module):\n    def __init__(self):\n        super().__init__()\n        self.rpn = RPN(config).eval().cpu()\n    def forward(self, features):\n        # type: (Dict[str, torch.Tensor]) -> Dict[str, torch.Tensor]\n        mock_input : InputClass = InputClass(torch.rand((N, 320, 320)))\n        instances = self.rpn(mock_input, features)\n        output : Dict[str, torch.Tensor] = {}\n        for idx in range(len(instances)):\n            inst : Instances = instances[idx]\n            box_tensor : torch.Tensor = inst.proposal_boxes.tensor\n            output[str(idx)] = box_tensor\n        return output\n<\/code>\nIt has been converted and loaded to mobile, but fails on runtime\n<code class=\"lang-auto\">E\/AndroidRuntime: FATAL EXCEPTION: main\n    Process: org.pytorch.helloworld, PID: 20157\n    java.lang.RuntimeException: Unable to start activity ComponentInfo{org.pytorch.helloworld\/org.pytorch.helloworld.MainActivity}: com.facebook.jni.CppException: forward() Expected a value of type 'Dict[str, Tensor]' for argument 'features' but instead found type 'Dict[str, Tensor]'.\n    Position: 1\n    Declaration: forward(ClassType<WrapRPN> self, Dict(str, Tensor) features) -> (Dict(str, Tensor)) (checkArg at ..\/aten\/src\/ATen\/core\/function_schema_inl.h:194)\n    (no backtrace available)\n        at android.app.ActivityThread.performLaunchActivity(ActivityThread.java:3784)\n        at android.app.ActivityThread.handleLaunchActivity(ActivityThread.java:3955)\n        at android.app.servertransaction.LaunchActivityItem.execute(LaunchActivityItem.java:91)\n        at android.app.servertransaction.TransactionExecutor.executeCallbacks(TransactionExecutor.java:149)\n        at android.app.servertransaction.TransactionExecutor.execute(TransactionExecutor.java:103)\n        at android.app.ActivityThread$H.handleMessage(ActivityThread.java:2392)\n        at android.os.Handler.dispatchMessage(Handler.java:107)\n        at android.os.Looper.loop(Looper.java:213)\n        at android.app.ActivityThread.main(ActivityThread.java:8147)\n        at java.lang.reflect.Method.invoke(Native Method)\n        at com.android.internal.os.RuntimeInit$MethodAndArgsCaller.run(RuntimeInit.java:513)\n        at com.android.internal.os.ZygoteInit.main(ZygoteInit.java:1100)\n     Caused by: com.facebook.jni.CppException: forward() Expected a value of type 'Dict[str, Tensor]' for argument 'features' but instead found type 'Dict[str, Tensor]'.\n    Position: 1\n    Declaration: forward(ClassType<WrapRPN> self, Dict(str, Tensor) features) -> (Dict(str, Tensor)) (checkArg at ..\/aten\/src\/ATen\/core\/function_schema_inl.h:194)\n    (no backtrace available)\n        at org.pytorch.NativePeer.forward(Native Method)\n        at org.pytorch.Module.forward(Module.java:37)\n        at org.pytorch.helloworld.MainActivity.onCreate(MainActivity.java:66)\n        at android.app.Activity.performCreate(Activity.java:8068)\n        at android.app.Activity.performCreate(Activity.java:8056)\n        at android.app.Instrumentation.callActivityOnCreate(Instrumentation.java:1320)\n        at android.app.ActivityThread.performLaunchActivity(ActivityThread.java:3757)\n        \t... 11 more\n<\/code>\nJava code is following:\n<code class=\"lang-auto\">    final Tensor inputTensor = TensorImageUtils.bitmapToFloat32Tensor(bitmap,\n        TensorImageUtils.TORCHVISION_NORM_MEAN_RGB, TensorImageUtils.TORCHVISION_NORM_STD_RGB);\n\n    Map<String, IValue> hm = new HashMap<String, IValue>();\n    List<String> keys = Arrays.asList(\"p2\", \"p3\", \"p4\", \"p5\", \"p6\");;\n    for (String key : keys) {\n      hm.put(key, IValue.from(inputTensor));\n    }\n    final IValue rpn_input = IValue.dictStringKeyFrom(hm);\n    module.forward(rpn_input);\n<\/code>\nWhat does it mean?\nExpected a value of type 'Dict[str, Tensor]' for argument 'features' but instead found type 'Dict[str, Tensor]' ","y":"Hello ,\nThanks one more time for this finding.\nIt happened as in jni tensorType was deduced from the first entry value of dictionary, including shape, requires_grad etc.\nWhile torchscript function did not have it. Dict is not covariant, so the subtype check required equal KeyType(str) and ValueType(TensorType0).\nTensorType\u2019s of function argument and provided value were different and typecheck failed.\nError message did not include that additional information about TensorType.\nThis problem was fixed on android-jni level in commit:\n\n\ngithub.com\/pytorch\/pytorch with link \"https:\/\/github.com\/pytorch\/pytorch\/commit\/5ada5363fc18d36ddf3f6b5f54123b80a42ec819\"\n\n\n\n\n\n\n\n\nGenericDict\/List type use unshapedType() (#30428) with link \"https:\/\/github.com\/pytorch\/pytorch\/commit\/5ada5363fc18d36ddf3f6b5f54123b80a42ec819\"\n\n\n\n        committed 11:31PM - 26 Nov 19 UTC\n\n\nIvanKobzarev with link \"https:\/\/github.com\/IvanKobzarev\"\n\n\n+5\n-6 with link \"https:\/\/github.com\/pytorch\/pytorch\/commit\/5ada5363fc18d36ddf3f6b5f54123b80a42ec819\"\n\n\n\n\n\nSummary:\nPull Request resolved: https:\/\/github.com\/pytorch\/pytorch\/pull\/30428\nReported issue https:\/\/discuss.pytorch.org\/t\/incomprehensible-behaviour\/61710\nSteps to reproduce:\n```\nclass WrapRPN(nn.Module):\n def __init__(self):\n super().__init__()\n def forward(self, features):\n # type: (Dict[str, Tensor]) -> int\n...\n\n\n\n\n\n\n\nIt was merged in master recently.\nSeparate issue for more detailed error messages in TensorType checks.\n\n\ngithub.com\/pytorch\/pytorch with link \"https:\/\/github.com\/pytorch\/pytorch\/issues\/30418\"\n\n\n\n\n\n\n\n\nMore detailed information about TensorType in error messages with link \"https:\/\/github.com\/pytorch\/pytorch\/issues\/30418\"\n\n\n\n        opened 09:34PM - 25 Nov 19 UTC\n\n\nIvanKobzarev with link \"https:\/\/github.com\/IvanKobzarev\"\n\n\n\n\n\n\ud83d\ude80 Feature\nMore detailed information about TensorType, including TensorType specifiers as shape, strides, req_grad etc. in error messages\nMotivation\nReported issue https:\/\/discuss.pytorch.org\/t\/incomprehensible-behaviour\/61710\nError message is...\n\n\nenhancement\ntriaged\n\n\n\n\n\n\n\nAndroid nightlies (snapshots) are already republished with this fix, the error should not happen with them.\nTo use nightlies (to force refresh dependencies gradle has argument --refresh-dependencies)\n<code class=\"lang-auto\">repositories {\n    maven {\n        url \"https:\/\/oss.sonatype.org\/content\/repositories\/snapshots\"\n    }\n}\n\ndependencies {\n    ...\n    implementation 'org.pytorch:pytorch_android:1.4.0-SNAPSHOT'\n    implementation 'org.pytorch:pytorch_android_torchvision:1.4.0-SNAPSHOT'\n    ...\n}\n<\/code>","z":"So, I\u2019ve spent a little time to understand what is going on and to find a way to overcome this issue.\nWhat I\u2019ve noticed:\n\nIf we use constructions such as IValue.dictStringKeyFrom(hm), where hm is HashMap<String, IValue> or if we use IValue.listFrom(lst), where lst is List<IValue> we will obtain the behaviour described above.\nBut if we use IValue.from(arr), where arr of type Tensor[], we will not face this issue and JAVA won\u2019t tell that it\u2019s expected List[Tensor] but got List[Tensor]\n\n\nConcluding this, there\u2019s no way to construct analogue of (2) using dictionaries.\nIValue (https:\/\/pytorch.org\/docs\/stable\/org\/pytorch\/IValue.html) doesn\u2019t have overriden method public static IValue dictStringKeyFrom(Map<String, T> map), where T is Tensor.\nI think in this case it might work, but it does not deny the fact there might be a bug\nThanks for this finding.\nI reproduced it locally and debugging it. List[Tensor] will be represented as a separate type on libtorch IValue side.\nLooks like we have some unexpected behavior on jni with Dict types which is converted to libtorch IValue{GenericDict}}.\nHello ,\nThanks one more time for this finding.\nIt happened as in jni tensorType was deduced from the first entry value of dictionary, including shape, requires_grad etc.\nWhile torchscript function did not have it. Dict is not covariant, so the subtype check required equal KeyType(str) and ValueType(TensorType0).\nTensorType\u2019s of function argument and provided value were different and typecheck failed.\nError message did not include that additional information about TensorType.\nThis problem was fixed on android-jni level in commit:\n\n\ngithub.com\/pytorch\/pytorch with link \"https:\/\/github.com\/pytorch\/pytorch\/commit\/5ada5363fc18d36ddf3f6b5f54123b80a42ec819\"\n\n\n\n\n\n\n\n\nGenericDict\/List type use unshapedType() (#30428) with link \"https:\/\/github.com\/pytorch\/pytorch\/commit\/5ada5363fc18d36ddf3f6b5f54123b80a42ec819\"\n\n\n\n        committed 11:31PM - 26 Nov 19 UTC\n\n\nIvanKobzarev with link \"https:\/\/github.com\/IvanKobzarev\"\n\n\n+5\n-6 with link \"https:\/\/github.com\/pytorch\/pytorch\/commit\/5ada5363fc18d36ddf3f6b5f54123b80a42ec819\"\n\n\n\n\n\nSummary:\nPull Request resolved: https:\/\/github.com\/pytorch\/pytorch\/pull\/30428\nReported issue https:\/\/discuss.pytorch.org\/t\/incomprehensible-behaviour\/61710\nSteps to reproduce:\n```\nclass WrapRPN(nn.Module):\n def __init__(self):\n super().__init__()\n def forward(self, features):\n # type: (Dict[str, Tensor]) -> int\n...\n\n\n\n\n\n\n\nIt was merged in master recently.\nSeparate issue for more detailed error messages in TensorType checks.\n\n\ngithub.com\/pytorch\/pytorch with link \"https:\/\/github.com\/pytorch\/pytorch\/issues\/30418\"\n\n\n\n\n\n\n\n\nMore detailed information about TensorType in error messages with link \"https:\/\/github.com\/pytorch\/pytorch\/issues\/30418\"\n\n\n\n        opened 09:34PM - 25 Nov 19 UTC\n\n\nIvanKobzarev with link \"https:\/\/github.com\/IvanKobzarev\"\n\n\n\n\n\n\ud83d\ude80 Feature\nMore detailed information about TensorType, including TensorType specifiers as shape, strides, req_grad etc. in error messages\nMotivation\nReported issue https:\/\/discuss.pytorch.org\/t\/incomprehensible-behaviour\/61710\nError message is...\n\n\nenhancement\ntriaged\n\n\n\n\n\n\n\nAndroid nightlies (snapshots) are already republished with this fix, the error should not happen with them.\nTo use nightlies (to force refresh dependencies gradle has argument --refresh-dependencies)\n<code class=\"lang-auto\">repositories {\n    maven {\n        url \"https:\/\/oss.sonatype.org\/content\/repositories\/snapshots\"\n    }\n}\n\ndependencies {\n    ...\n    implementation 'org.pytorch:pytorch_android:1.4.0-SNAPSHOT'\n    implementation 'org.pytorch:pytorch_android_torchvision:1.4.0-SNAPSHOT'\n    ...\n}\n<\/code>\n\n\n\n zetyquickly:\n\nif we use IValue.listFrom(lst) , where lst is List<IValue> we will obtain the behaviour described above\n\n\nThanks a lot for the FIX! Tell me please, does it also affect also behviour of List<IValue>? When I tried it also fails with this \u201cList not a List\u201d error\nYes, that problem affected both our Generic containers (Dict and List) when the element type was TensorType. After the fix GenericList is also initialized with c10::unshapedType(firstElement) which should fix the problem like \u2018List[Tensor] is not List[Tensor]\u2019\n\n\ngithub.com with link \"https:\/\/github.com\/pytorch\/pytorch\/blob\/master\/android\/pytorch_android\/src\/main\/cpp\/pytorch_jni_common.cpp#L509\"\n\n\npytorch\/pytorch\/blob\/master\/android\/pytorch_android\/src\/main\/cpp\/pytorch_jni_common.cpp#L509 with link \"https:\/\/github.com\/pytorch\/pytorch\/blob\/master\/android\/pytorch_android\/src\/main\/cpp\/pytorch_jni_common.cpp#L509\"\n<code class=\"lang-cpp\">\n              facebook::jni::JArrayClass<JIValue::javaobject>::javaobject()>(\n              \"toList\");\n  auto jarray = jMethodGetList(jivalue);\n  size_t n = jarray->size();\n  if (n == 0) {\n    return at::IValue{c10::impl::GenericList(c10::TensorType::get())};\n  }\n\n\n  auto jivalue_first_element = jarray->getElement(0);\n  auto first_element = JIValue::JIValueToAtIValue(jivalue_first_element);\n  c10::impl::GenericList list{c10::unshapedType(first_element.type())};\n  list.reserve(n);\n  list.push_back(first_element);\n  for (auto i = 1; i < n; ++i) {\n    auto jivalue_element = jarray->getElement(i);\n    auto element = JIValue::JIValueToAtIValue(jivalue_element);\n    list.push_back(element);\n  }\n  return at::IValue{list};\n} else if (JIValue::kTypeCodeDictStringKey == typeCode) {\n  static const auto jMethodGetDictStringKey =\n<\/code>\n\n\n\n\n\n"},{"x":"I have a custom model that is a variation on YOLOv3, to test the results, I have asserted that the inputTensor on device is the same as that I am loading on the computer. The output (which has detections and classifications) is giving near identical object and class confidences. However, the locations (x,y,w,h) are slightly off. Is this expected behaviour? Do you know if there is anything in particular I should investigate in my model or the trace of my model?","y":"Hi Ivan,\nThanks for your response. I ended up solving the error. It turned out it was a problem with the tracing \u2013 see here: Torch.jit.trace() only works on example input? with link \"https:\/\/discuss.pytorch.org\/t\/torch-jit-trace-only-works-on-example-input\/62478\"","z":"Hello ,\nDo you use completely the same model on mobile and desktop, without any quantization on mobile?\nIf you already debugged it, maybe you have some ideas which operator\/layer is producing different results?\nCould you please dump all operators that your model use, here are steps how to do it:\nFirst we need dump_operator_names executable which can be built with command:\n<code class=\"lang-auto\">BUILD_BINARIES=1 python setup.py build\n<\/code>\nAfter succesfull build you can find it in path like .\/build\/lib.linux-x86_64-3.7\/torch\/bin\/dump_operator_names.\nIt depends on libcaffe2_observers.so, libtorch.so, libc10.so, so they should be either located in the same folder or be installed on the system.\n<code class=\"lang-auto\">\nmkdir tmp\ncp .\/build\/lib.linux-x86_64-3.7\/torch\/bin\/dumpop_operator_names tmp\/\ncp .\/build\/lib.linux-x86_64-3.7\/torch\/lib\/libcaffe2_observers.so tmp\ncp .\/build\/lib.linux-x86_64-3.7\/torch\/lib\/libtorch.so tmp\ncp .\/build\/lib.linux-x86_64-3.7\/torch\/lib\/libc10.so tmp\n<\/code>\nAfter that you can run it, specifing model and output file.\n<code class=\"lang-auto\">.\/dump_operator_names --model=model.pt --output=model_ops.yaml\n<\/code>\nHi Ivan,\nThanks for your response. I ended up solving the error. It turned out it was a problem with the tracing \u2013 see here: Torch.jit.trace() only works on example input? with link \"https:\/\/discuss.pytorch.org\/t\/torch-jit-trace-only-works-on-example-input\/62478\""},{"x":"My model contains nn.Functional.interpolate layer and it is converted properly to .pt model using jit.\nBut Loading same model in android giving issues.\nThis is happening in case of interpolate layer only.\nEdit - Error trace\njava.lang.RuntimeException: Unable to start activity ComponentInfo{org.pytorch.custom_model\/org.pytorch.custom_model.MainActivity}: com.facebook.jni.CppException: false CHECK FAILED at \u2026\/c10\/core\/Backend.h (tensorTypeIdToBackend at \u2026\/c10\/core\/Backend.h:106)\n(no backtrace available)","y":"For anyone who is following this thread, the issue was resolved at https:\/\/github.com\/pytorch\/pytorch\/issues\/29806","z":"Provide a stacktrace please\nHey  I have edit the trace in the post. Kindly look into it\nThe most common reason for this is version discrepancies between the PyTorch version you trace the model with and the PyTorch Mobile one.\nBest regards\nThomas\nHi  , I am using pytorch_android 1.3.1 and pytorch android torchvision 1.3.1\nand pytorch version 1.3.1 cpu. I didn\u2019t find any discrepancies for versions.\nCan you enlighten more?\nThanks\nMohit Ranawat\nHi,I meet the same problem as you. Now I am trying to retrain the model with pytorch 1.3.If you have solved the problem, I hope you can give me some advice, thank you. You can contact with me. My email address is 1209648713. com.\nHi  I didn\u2019t got any solution yet. Hope I\u2019ll get the solution.\nIf you got the solution tell me\nI meet this problem too(on iOS though). The model I use is from here but I use my own images for SRGAN.\n\n\ngithub.com with link \"https:\/\/github.com\/leftthomas\/SRGAN\/blob\/master\/model.py\"\n\n\nleftthomas\/SRGAN\/blob\/master\/model.py with link \"https:\/\/github.com\/leftthomas\/SRGAN\/blob\/master\/model.py\"\n<code class=\"lang-py\">import math\nimport torch\nfrom torch import nn\n\n\nclass Generator(nn.Module):\n    def __init__(self, scale_factor):\n        upsample_block_num = int(math.log(scale_factor, 2))\n\n        super(Generator, self).__init__()\n        self.block1 = nn.Sequential(\n            nn.Conv2d(3, 64, kernel_size=9, padding=4),\n            nn.PReLU()\n        )\n        self.block2 = ResidualBlock(64)\n        self.block3 = ResidualBlock(64)\n        self.block4 = ResidualBlock(64)\n        self.block5 = ResidualBlock(64)\n        self.block6 = ResidualBlock(64)\n        self.block7 = nn.Sequential(\n<\/code>\n\n  This file has been truncated. show original with link \"https:\/\/github.com\/leftthomas\/SRGAN\/blob\/master\/model.py\"\n\n\n\n\n\n\nP.S. I tried onnx->coreml and onnx->tensorflow pb -> coreml and both way failed. Very frustrated as a beginner.\nhey  onnx to tensorflow conversion is not stable.\nBetter to not use it.You can go fro keras to tensorflow and you can transfer your weights from pytorch to keras.\nHope this helps.\nThanks,\nMohit Ranawat\nFor anyone who is following this thread, the issue was resolved at https:\/\/github.com\/pytorch\/pytorch\/issues\/29806"},{"x":"I tried to create tensor follow below code:\n<code class=\"lang-auto\">...\npunc_character_torch = torch.tensor([191, 193, 194, 195, 196], dtype=token.dtype, device=device)\n<\/code>\nConvert this code to Torchscript and run it on my pc normally. But when i loaded torchscript to mobile, i got below issue:\n<code class=\"lang-auto\">2019-12-06 10:24:21.652 15260-15260\/org.pytorch.demo E\/AndroidRuntime: FATAL EXCEPTION: main\n    Process: org.pytorch.demo, PID: 15260\n    java.lang.RuntimeException: is_variable() || !defined() CHECK FAILED at ..\/torch\/csrc\/autograd\/variable.h\n    The above operation failed in interpreter, with the following stack trace:\n    at code\/__torch__\/torchscript\/Model.py:1686:27\n          ops.prim.RaiseException(\"Exception\")\n        if torch.gt(alpha, 0):\n          pass\n        else:\n          ops.prim.RaiseException(\"Exception\")\n        device = ops.prim.device(encoder_outputs)\n        out_dtype = ops.prim.dtype(encoder_outputs)\n        encoder_output_hidden_size = torch.size(encoder_outputs, -1)\n        max_encoder_length = torch.size(encoder_outputs, 1)\n        punc_character_torch = torch.tensor([191, 193, 194, 195, 196], dtype=ops.prim.dtype(token), device=device, requires_grad=False)\n                               ~~~~~~~~~~~~ <--- HERE\n<\/code>\nIt seem that torchscript still does not support create tensor on mobile device. Any idea to tackle this problem?","y":"Can you try this with the latest nightly (1.4.0-SNAPSHOT)?  Instructions are in the first half of this post: [Android] Something's went wrong with pytorch_android-1.4.0-SNAPSHOT with link \"https:\/\/discuss.pytorch.org\/t\/android-somethings-went-wrong-with-pytorch-android-1-4-0-snapshot\/61009\"\nThis part of the code was refactored and the issue should either be fixed or at least have a clearer error message.","z":"Can you try this with the latest nightly (1.4.0-SNAPSHOT)?  Instructions are in the first half of this post: [Android] Something's went wrong with pytorch_android-1.4.0-SNAPSHOT with link \"https:\/\/discuss.pytorch.org\/t\/android-somethings-went-wrong-with-pytorch-android-1-4-0-snapshot\/61009\"\nThis part of the code was refactored and the issue should either be fixed or at least have a clearer error message."},{"x":"After I used torch.quantization.quantize_dynamic() to quantize the original model, I saved and loaded the quantized model. But, when I ran inference, it returned this error. The original model still ran inference well, I don\u2019t know why\n<code class=\"lang-auto\">Traceback (most recent call last):\n  File \"inference.py\", line 81, in <module>\n    output = infer(args.text, model)\n  File \"inference.py\", line 30, in infer\n    mel_outputs, mel_outputs_postnet, _, alignments = model.inference(sequence)\n  File \"\/media\/tma\/DATA\/Khai-folder\/Tacotron2-PyTorch\/model\/model.py\", line 542, in inference\n    encoder_outputs = self.encoder.inference(embedded_inputs)\n  File \"\/media\/tma\/DATA\/Khai-folder\/Tacotron2-PyTorch\/model\/model.py\", line 219, in inference\n    self.lstm.flatten_parameters()\n  File \"\/media\/tma\/DATA\/miniconda3\/envs\/ttsv\/lib\/python3.6\/site-packages\/torch\/nn\/modules\/module.py\", line 576, in __get\nattr__\n    type(self).__name__, name))\nAttributeError: 'LSTM' object has no attribute 'flatten_parameters'\n<\/code>","y":"Actually, when you load your quantized model, you need to quantize your initial model first.\n<code class=\"lang-auto\">quantized_model = torch.quantization.quantize_dynamic(\n    model, {nn.LSTM, nn.Linear}, dtype=torch.qint8\n)         \/\/ Do s.t like this first before loading your quantized model\n<\/code>","z":"Which pytorch version are you using? I think this is a known problem and should go away with 1.5 release.\nCould you wait for that or re-try with the nightly build and see if this issue goes away?\nI\u2019m using pytorch version 1.4.0\nHope next release will fix this issue.\n, I am facing the same issue with Pytorch version - 1.5.1 . Have you been able to solve it?\nActually, when you load your quantized model, you need to quantize your initial model first.\n<code class=\"lang-auto\">quantized_model = torch.quantization.quantize_dynamic(\n    model, {nn.LSTM, nn.Linear}, dtype=torch.qint8\n)         \/\/ Do s.t like this first before loading your quantized model\n<\/code>"},{"x":"Hello all,\nI try to quantize nn.TransformerEncoder, but get errors during inference.\nThe problem is with nn.MultiheadAttention, which is basically a set of nn.Linear operations and should work OK after quantization.\nMinimal example:\n<code class=\"lang-python\">import torch\n\nmlth = torch.nn.MultiheadAttention(512, 8)\npossible_input = torch.rand((10, 10, 512))\nquatized = torch.quantization.quantize_dynamic(mlth)\nquatized(possible_input, possible_input, possible_input)\n<\/code>\nIt fails with:\n<code class=\"lang-auto\">\/opt\/miniconda\/lib\/python3.7\/site-packages\/torch\/nn\/functional.py in multi_head_attention_forward(query, key, value, embed_dim_to_check, num_heads, in_proj_weight, in_proj_bias, bias_k, bias_v, add_zero_attn, dropout_p, out_proj_weight, out_proj_bias, training, key_padding_mask, need_weights, attn_mask, use_separate_proj_weight, q_proj_weight, k_proj_weight, v_proj_weight, static_k, static_v)\n   3946     assert list(attn_output.size()) == [bsz * num_heads, tgt_len, head_dim]\n   3947     attn_output = attn_output.transpose(0, 1).contiguous().view(tgt_len, bsz, embed_dim)\n-> 3948     attn_output = linear(attn_output, out_proj_weight, out_proj_bias)\n   3949 \n   3950     if need_weights:\n\n\/opt\/miniconda\/lib\/python3.7\/site-packages\/torch\/nn\/functional.py in linear(input, weight, bias)\n   1610         ret = torch.addmm(bias, input, weight.t())\n   1611     else:\n-> 1612         output = input.matmul(weight.t())\n   1613         if bias is not None:\n   1614             output += bias\n\nAttributeError: 'function' object has no attribute 't'\n<\/code>\n \nThat's because `.weight` is not parameter anymore, but the method (for components of the quantized module).\nYou can check it like:\n<code class=\"lang-auto\">mlth.out_proj.weight\n<\/code>\n<code class=\"lang-auto\">Parameter containing:\ntensor([[-0.0280,  0.0016,  0.0163,  ...,  0.0375,  0.0153, -0.0435],\n        [-0.0168,  0.0310, -0.0211,  ..., -0.0258,  0.0043, -0.0094],\n        [ 0.0412, -0.0078,  0.0262,  ...,  0.0328,  0.0439,  0.0066],\n        ...,\n        [-0.0278,  0.0337,  0.0189,  ..., -0.0402,  0.0193, -0.0163],\n        [ 0.0034, -0.0364, -0.0418,  ..., -0.0248, -0.0375, -0.0236],\n        [-0.0312,  0.0236,  0.0404,  ...,  0.0266,  0.0255,  0.0265]],\n       requires_grad=True)\n<\/code>\nwhile\n<code class=\"lang-auto\">quatized.out_proj.weight\n<\/code>\n<code class=\"lang-auto\"><bound method Linear.weight of DynamicQuantizedLinear(in_features=512, out_features=512, qscheme=torch.per_tensor_affine)>\n<\/code>\nCan you please guide me about this? Is it expected behavior? Should I report it to pyTorch GitHub issues?\nIt looks like quantization break all the module which use .weight inside.\nThanks in advance","y":"hi , I am able to run your example without issues on the nighly.  What version of PyTorch are you using?  Can you check if using a more recent version \/ a nightly build fixes your issue?","z":"hi , I am able to run your example without issues on the nighly.  What version of PyTorch are you using?  Can you check if using a more recent version \/ a nightly build fixes your issue?\nHi \nThanks for the reply. Indeed, in the nightly version, there\u2019s no error. At the same time, nn.Multihead doesn\u2019t compress, nevertheless, it\u2019s just a set of Linear operations \nIs there any information regarding adding quantization to the layer (or for instance nn.Embedings)?\nyes, currently nn.MultiheadAttention is not supported yet in eager mode quantization.  There are folks working on adding support for both this and embeddings quantization.\nGood to hear that. Is there any open information when it will be released (at least approximately)?\nhi , one other thing you could try is https:\/\/pytorch.org\/blog\/pytorch-1.6-released\/#graph-mode-quantization , which we just released today in v1.6.  It might be easier to make multiheadattention work in graph mode.\nAs far as first class quantization for nn.MultiheadAttention and nn.EmbeddingBag \/ nn.Embedding - we don\u2019t have a specific timeline we can share, but it should be on the order of months (not weeks or years) - we have folks actively working on this.\n thanks a lot for your answer"},{"x":"#onnx #jit with link \"\/c\/jit\/13\" #quantization with link \"\/c\/quantization\/17\"\nHi, I am very confused.\nWhile tracing to ONNX my quantized model faced an error. This happens with fused QuantizedConvReLU2d. I use OperatorExportTypes.ONNX_ATEN_FALLBACK.\nPytorch version is 1.6.0.dev20200520\n<code class=\"lang-auto\">Traceback (most recent call last):\n  File \".\/tools\/caffe2_converter.py\", line 115, in <module>\n    caffe2_model = export_caffe2_model(cfg, model, first_batch)\n  File \"\/root\/some_detectron2\/detectron2\/export\/api.py\", line 157, in export_caffe2_model\n    return Caffe2Tracer(cfg, model, inputs).export_caffe2()\n  File \"\/root\/some_detectron2\/detectron2\/export\/api.py\", line 95, in export_caffe2\n    predict_net, init_net = export_caffe2_detection_model(model, inputs)\n  File \"\/root\/some_detectron2\/detectron2\/export\/caffe2_export.py\", line 144, in export_caffe2_detection_model\n    onnx_model = export_onnx_model(model, (tensor_inputs,))\n  File \"\/root\/some_detectron2\/detectron2\/export\/caffe2_export.py\", line 63, in export_onnx_model\n    export_params=True,\n  File \"\/root\/anaconda2\/envs\/pytorch-gpu\/lib\/python3.7\/site-packages\/torch\/onnx\/__init__.py\", line 172, in export\n    custom_opsets, enable_onnx_checker, use_external_data_format)\n  File \"\/root\/anaconda2\/envs\/pytorch-gpu\/lib\/python3.7\/site-packages\/torch\/onnx\/utils.py\", line 92, in export\n    use_external_data_format=use_external_data_format)\n  File \"\/root\/anaconda2\/envs\/pytorch-gpu\/lib\/python3.7\/site-packages\/torch\/onnx\/utils.py\", line 530, in _export\n    fixed_batch_size=fixed_batch_size)\n  File \"\/root\/anaconda2\/envs\/pytorch-gpu\/lib\/python3.7\/site-packages\/torch\/onnx\/utils.py\", line 366, in _model_to_graph\n    graph, torch_out = _trace_and_get_graph_from_model(model, args)\n  File \"\/root\/anaconda2\/envs\/pytorch-gpu\/lib\/python3.7\/site-packages\/torch\/onnx\/utils.py\", line 319, in _trace_and_get_graph_from_model\n    torch.jit._get_trace_graph(model, args, strict=False, _force_outplace=False, _return_inputs_states=True)\n  File \"\/root\/anaconda2\/envs\/pytorch-gpu\/lib\/python3.7\/site-packages\/torch\/jit\/__init__.py\", line 284, in _get_trace_graph\n    outs = ONNXTracedModule(f, strict, _force_outplace, return_inputs, _return_inputs_states)(*args, **kwargs)\n  File \"\/root\/anaconda2\/envs\/pytorch-gpu\/lib\/python3.7\/site-packages\/torch\/nn\/modules\/module.py\", line 577, in __call__\n    result = self.forward(*input, **kwargs)\n  File \"\/root\/anaconda2\/envs\/pytorch-gpu\/lib\/python3.7\/site-packages\/torch\/jit\/__init__.py\", line 372, in forward\n    self._force_outplace,\n  File \"\/root\/anaconda2\/envs\/pytorch-gpu\/lib\/python3.7\/site-packages\/torch\/jit\/__init__.py\", line 358, in wrapper\n    outs.append(self.inner(*trace_inputs))\n  File \"\/root\/anaconda2\/envs\/pytorch-gpu\/lib\/python3.7\/site-packages\/torch\/nn\/modules\/module.py\", line 575, in __call__\n    result = self._slow_forward(*input, **kwargs)\n  File \"\/root\/anaconda2\/envs\/pytorch-gpu\/lib\/python3.7\/site-packages\/torch\/nn\/modules\/module.py\", line 561, in _slow_forward\n    result = self.forward(*input, **kwargs)\n  File \"\/root\/anaconda2\/envs\/pytorch-gpu\/lib\/python3.7\/contextlib.py\", line 74, in inner\n    return func(*args, **kwds)\n  File \"\/root\/some_detectron2\/detectron2\/export\/caffe2_modeling.py\", line 319, in forward\n    features = self._wrapped_model.backbone(images.tensor)\n  File \"\/root\/anaconda2\/envs\/pytorch-gpu\/lib\/python3.7\/site-packages\/torch\/nn\/modules\/module.py\", line 575, in __call__\n    result = self._slow_forward(*input, **kwargs)\n  File \"\/root\/anaconda2\/envs\/pytorch-gpu\/lib\/python3.7\/site-packages\/torch\/nn\/modules\/module.py\", line 561, in _slow_forward\n    result = self.forward(*input, **kwargs)\n  File \"\/root\/DensePose_ADASE\/densepose\/modeling\/quantize_caffe2.py\", line 166, in new_forward\n    p5, p4, p3, p2 = self.bottom_up(x)  # top->down\n  File \"\/root\/anaconda2\/envs\/pytorch-gpu\/lib\/python3.7\/site-packages\/torch\/nn\/modules\/module.py\", line 575, in __call__\n    result = self._slow_forward(*input, **kwargs)\n  File \"\/root\/anaconda2\/envs\/pytorch-gpu\/lib\/python3.7\/site-packages\/torch\/nn\/modules\/module.py\", line 561, in _slow_forward\n    result = self.forward(*input, **kwargs)\n  File \"\/root\/anaconda2\/envs\/pytorch-gpu\/lib\/python3.7\/site-packages\/timm\/models\/efficientnet.py\", line 350, in forward\n    x = self.conv_stem(x)\n  File \"\/root\/anaconda2\/envs\/pytorch-gpu\/lib\/python3.7\/site-packages\/torch\/nn\/modules\/module.py\", line 575, in __call__\n    result = self._slow_forward(*input, **kwargs)\n  File \"\/root\/anaconda2\/envs\/pytorch-gpu\/lib\/python3.7\/site-packages\/torch\/nn\/modules\/module.py\", line 561, in _slow_forward\n    result = self.forward(*input, **kwargs)\n  File \"\/root\/anaconda2\/envs\/pytorch-gpu\/lib\/python3.7\/site-packages\/torch\/nn\/intrinsic\/quantized\/modules\/conv_relu.py\", line 71, in forward\n    input, self._packed_params, self.scale, self.zero_point)\nRuntimeError: Tried to trace <__torch__.torch.classes.quantized.Conv2dPackedParamsBase object at 0x5600474e9670> but it is not part of the active trace. Modules that are called during a trace must be registered \nas submodules of the thing being traced.\n<\/code>\nMay presense of pre_forward hooks in self.bottom_up(x) (but not the self.conv_stem(x)) affect tracing such way?\nModel were QAT with preserving hooks from commit https:\/\/github.com\/pytorch\/pytorch\/pull\/37233\nAlso PT -> ONNX -> Caffe2 exporting works on this very model without quantization patching","y":"we are not working on onnx conversions, feel free to submit PRs to add the support.","z":"P.S. here\u2019s also a warning\n<code class=\"lang-auto\">\/root\/anaconda2\/envs\/pytorch-gpu\/lib\/python3.7\/site-packages\/torch\/nn\/quantized\/modules\/utils.py:10: UserWarning: 0quantize_tensor_per_tensor_affine current rounding mode is not set to round-to-nearest-ties-to-e\nven (FE_TONEAREST). This will cause accuracy issues in quantized models. (Triggered internally at  \/opt\/conda\/conda-bld\/pytorch_1589958443755\/work\/aten\/src\/ATen\/native\/quantized\/affine_quantizer.cpp:25.)\n  float(wt_scale), int(wt_zp), torch.qint8)\n<\/code>\ncc  is this related to TorchBind object?\n could you give a minimal repo of the issue?\n \nI\u2019ve prepared a repro. It might be not minimal but it mocks the pipeline I use.\nTwo files:\n<code class=\"lang-auto\"># network.py\n\nimport torch\n\nclass ConvModel(torch.nn.Module):\n    def __init__(self):\n        super(ConvModel, self).__init__()\n        self.conv_stem = torch.nn.Conv2d(\n            3, 5, 2, bias=True\n        ).to(dtype=torch.float)\n\n        self.bn1 = torch.nn.BatchNorm2d(5)\n        self.act1 = torch.nn.ReLU()\n\n    def forward(self, x):\n        x = self.conv_stem(x)\n        x = self.bn1(x)\n        x = self.act1(x)\n        return x\n<\/code>\n<code class=\"lang-auto\"># actions.py\n\nimport torch\nimport io\nimport onnx\nfrom torch.onnx import OperatorExportTypes\n\n\ndef ConvModel_decorate(cls):\n\n    def fuse(self):\n        torch.quantization.fuse_modules(\n            self, \n            ['conv_stem', 'bn1', 'act1'], \n            inplace=True\n        )\n\n    cls.fuse = fuse\n    return cls\n\ndef fuse_modules(module):\n    module_output = module\n    if callable(getattr(module_output, \"fuse\", None)):\n        module_output.fuse()\n    for name, child in module.named_children():\n        new_child = fuse_modules(child)\n        if new_child is not child:\n            module_output.add_module(name, new_child)\n    return module_output\n\ndef create_and_update_model():\n    import network\n    network.ConvModel = ConvModel_decorate(network.ConvModel)\n    model = network.ConvModel()\n    backend = 'qnnpack'\n    model = fuse_modules(model)\n    model.qconfig = torch.quantization.get_default_qat_qconfig(backend)\n    torch.backends.quantized.engine = backend\n    torch.quantization.prepare_qat(model, inplace=True)\n    model.apply(torch.nn.intrinsic.qat.freeze_bn_stats)\n    return model\n\ndef QAT(model):\n    N = 100\n    for idx in range(N):\n        input_tensor = torch.rand(1, 3, 6, 6)\n        model(input_tensor)\n    return model\n\nif __name__ == '__main__':\n    model = create_and_update_model()\n    model = QAT(model)\n    torch.quantization.convert(model, inplace=True)\n    \n    model.eval()\n    inputs = torch.rand(1, 3, 6, 6)\n    # Export the model to ONNX\n    with torch.no_grad():\n        with io.BytesIO() as f:\n            torch.onnx.export(\n                model,\n                inputs,\n                f,\n                opset_version=11,\n                operator_export_type=OperatorExportTypes.ONNX_ATEN_FALLBACK,\n                verbose=True,  # NOTE: uncomment this for debugging\n                export_params=True,\n            )\n            onnx_model = onnx.load_from_string(f.getvalue())\n<\/code>\nError:\n<code class=\"lang-auto\">(pytorch-gpu) root:~\/some_detectron2# \/root\/anaconda2\/envs\/pytorch-gpu\/bin\/python \/root\/some_detectron2\/min_repro\/actions.py\n\/root\/anaconda2\/envs\/pytorch-gpu\/lib\/python3.7\/site-packages\/torch\/nn\/quantized\/modules\/utils.py:10: UserWarning: 0quantize_tensor_per_tensor_affine current rounding mode is not set to round-to-nearest-ties-to-even (FE_TONEAREST). This will cause accuracy issues in quantized models. (Triggered internally at  \/opt\/conda\/conda-bld\/pytorch_1589958443755\/work\/aten\/src\/ATen\/native\/quantized\/affine_quantizer.cpp:25.)\n  float(wt_scale), int(wt_zp), torch.qint8)\n\/root\/anaconda2\/envs\/pytorch-gpu\/lib\/python3.7\/site-packages\/torch\/onnx\/utils.py:243: UserWarning: `add_node_names' can be set to True only when 'operator_export_type' is `ONNX`. Since 'operator_export_type' is not set to 'ONNX', `add_node_names` argument will be ignored.\n  \"`{}` argument will be ignored.\".format(arg_name, arg_name))\n\/root\/anaconda2\/envs\/pytorch-gpu\/lib\/python3.7\/site-packages\/torch\/onnx\/utils.py:243: UserWarning: `do_constant_folding' can be set to True only when 'operator_export_type' is `ONNX`. Since 'operator_export_type' is not set to 'ONNX', `do_constant_folding` argument will be ignored.\n  \"`{}` argument will be ignored.\".format(arg_name, arg_name))\nTraceback (most recent call last):\n  File \"\/root\/some_detectron2\/min_repro\/actions.py\", line 65, in <module>\n    export_params=True,\n  File \"\/root\/anaconda2\/envs\/pytorch-gpu\/lib\/python3.7\/site-packages\/torch\/onnx\/__init__.py\", line 172, in export\n    custom_opsets, enable_onnx_checker, use_external_data_format)\n  File \"\/root\/anaconda2\/envs\/pytorch-gpu\/lib\/python3.7\/site-packages\/torch\/onnx\/utils.py\", line 92, in export\n    use_external_data_format=use_external_data_format)\n  File \"\/root\/anaconda2\/envs\/pytorch-gpu\/lib\/python3.7\/site-packages\/torch\/onnx\/utils.py\", line 530, in _export\n    fixed_batch_size=fixed_batch_size)\n  File \"\/root\/anaconda2\/envs\/pytorch-gpu\/lib\/python3.7\/site-packages\/torch\/onnx\/utils.py\", line 366, in _model_to_graph\n    graph, torch_out = _trace_and_get_graph_from_model(model, args)\n  File \"\/root\/anaconda2\/envs\/pytorch-gpu\/lib\/python3.7\/site-packages\/torch\/onnx\/utils.py\", line 319, in _trace_and_get_graph_from_model\n    torch.jit._get_trace_graph(model, args, strict=False, _force_outplace=False, _return_inputs_states=True)\n  File \"\/root\/anaconda2\/envs\/pytorch-gpu\/lib\/python3.7\/site-packages\/torch\/jit\/__init__.py\", line 284, in _get_trace_graph\n    outs = ONNXTracedModule(f, strict, _force_outplace, return_inputs, _return_inputs_states)(*args, **kwargs)\n  File \"\/root\/anaconda2\/envs\/pytorch-gpu\/lib\/python3.7\/site-packages\/torch\/nn\/modules\/module.py\", line 577, in __call__\n    result = self.forward(*input, **kwargs)\n  File \"\/root\/anaconda2\/envs\/pytorch-gpu\/lib\/python3.7\/site-packages\/torch\/jit\/__init__.py\", line 372, in forward\n    self._force_outplace,\n  File \"\/root\/anaconda2\/envs\/pytorch-gpu\/lib\/python3.7\/site-packages\/torch\/jit\/__init__.py\", line 358, in wrapper\n    outs.append(self.inner(*trace_inputs))\n  File \"\/root\/anaconda2\/envs\/pytorch-gpu\/lib\/python3.7\/site-packages\/torch\/nn\/modules\/module.py\", line 575, in __call__\n    result = self._slow_forward(*input, **kwargs)\n  File \"\/root\/anaconda2\/envs\/pytorch-gpu\/lib\/python3.7\/site-packages\/torch\/nn\/modules\/module.py\", line 561, in _slow_forward\n    result = self.forward(*input, **kwargs)\n  File \"\/root\/some_detectron2\/min_repro\/network.py\", line 14, in forward\n    x = self.conv_stem(x)\n  File \"\/root\/anaconda2\/envs\/pytorch-gpu\/lib\/python3.7\/site-packages\/torch\/nn\/modules\/module.py\", line 575, in __call__\n    result = self._slow_forward(*input, **kwargs)\n  File \"\/root\/anaconda2\/envs\/pytorch-gpu\/lib\/python3.7\/site-packages\/torch\/nn\/modules\/module.py\", line 561, in _slow_forward\n    result = self.forward(*input, **kwargs)\n  File \"\/root\/anaconda2\/envs\/pytorch-gpu\/lib\/python3.7\/site-packages\/torch\/nn\/intrinsic\/quantized\/modules\/conv_relu.py\", line 71, in forward\n    input, self._packed_params, self.scale, self.zero_point)\nRuntimeError: Tried to trace <__torch__.torch.classes.quantized.Conv2dPackedParamsBase object at 0x564c572bd980> but it is not part of the active trace. Modules that are called during a trace must be registered as submodules of the thing being traced.\n<\/code>\n<code class=\"lang-auto\"><\/code>\nHi ,\nFirst, your model does not run with the given inputs. The quantized model expects a quantized input, but inputs in your script is float-valued. QuantWrapper can be used to force quantization\/dequantization for inputs\/outputs of the model, respectively:\n<code class=\"lang-auto\">@@ -31,7 +31,7 @@ def fuse_modules(module):\n def create_and_update_model():\n     import network\n     network.ConvModel = ConvModel_decorate(network.ConvModel)\n-    model = network.ConvModel()\n+    model = torch.quantization.QuantWrapper(network.ConvModel())\n<\/code>\nSecond, there\u2019s a strange difference in behavior here between when ONNX is tracing the model and when we use the standalone TorchScript tracer. Tracing the model works fine when we use the standalone tracer. To workaround this issue, you can do this:\n<code class=\"lang-auto\">@@ -54,16 +54,19 @@ if __name__ == '__main__':\n     \n     model.eval()\n     inputs = torch.rand(1, 3, 6, 6)\n+    traced = torch.jit.trace(model, (inputs,))\n+\n     # Export the model to ONNX\n     with torch.no_grad():\n         with io.BytesIO() as f:\n             torch.onnx.export(\n-                model,\n+                traced,\n                 inputs,\n                 f,\n                 opset_version=11,\n                 operator_export_type=OperatorExportTypes.ONNX_ATEN_FALLBACK,\n                 verbose=True,  # NOTE: uncomment this for debugging\n                 export_params=True,\n+                example_outputs=traced(inputs)\n             )\n             onnx_model = onnx.load_from_string(f.getvalue())\n<\/code>\nWe will investigate this difference in tracing\nThank you \nI had knew that we should trace a model before passing it to ONNX export, but with link \"https:\/\/discuss.pytorch.org\/t\/onnx-tried-to-trace-submodule-but-it-is-not-part-of-the-active-trace\/80320\/3\".\nFor now I know it for sure\nCould you please help me reveal what\u2019s going on with traced model during exporting when I see the following:\n<code class=\"lang-auto\">Traceback (most recent call last):\n  File \".\/tools\/caffe2_converter.py\", line 115, in <module>\n    caffe2_model = export_caffe2_model(cfg, model, first_batch)\n  File \"\/root\/some_detectron2\/detectron2\/export\/api.py\", line 157, in export_caffe2_model\n    return Caffe2Tracer(cfg, model, inputs).export_caffe2()\n  File \"\/root\/some_detectron2\/detectron2\/export\/api.py\", line 95, in export_caffe2\n    predict_net, init_net = export_caffe2_detection_model(model, inputs)\n  File \"\/root\/some_detectron2\/detectron2\/export\/caffe2_export.py\", line 147, in export_caffe2_detection_model\n    onnx_model = export_onnx_model(model, (tensor_inputs,))\n  File \"\/root\/some_detectron2\/detectron2\/export\/caffe2_export.py\", line 66, in export_onnx_model\n    example_outputs=traced(inputs[0])\n  File \"\/root\/anaconda2\/envs\/pytorch-gpu\/lib\/python3.7\/site-packages\/torch\/onnx\/__init__.py\", line 172, in export\n    custom_opsets, enable_onnx_checker, use_external_data_format)\n  File \"\/root\/anaconda2\/envs\/pytorch-gpu\/lib\/python3.7\/site-packages\/torch\/onnx\/utils.py\", line 92, in export\n    use_external_data_format=use_external_data_format)\n  File \"\/root\/anaconda2\/envs\/pytorch-gpu\/lib\/python3.7\/site-packages\/torch\/onnx\/utils.py\", line 530, in _export\n    fixed_batch_size=fixed_batch_size)\n  File \"\/root\/anaconda2\/envs\/pytorch-gpu\/lib\/python3.7\/site-packages\/torch\/onnx\/utils.py\", line 384, in _model_to_graph\n    fixed_batch_size=fixed_batch_size, params_dict=params_dict)\n  File \"\/root\/anaconda2\/envs\/pytorch-gpu\/lib\/python3.7\/site-packages\/torch\/onnx\/utils.py\", line 171, in _optimize_graph\n    torch._C._jit_pass_onnx_unpack_quantized_weights(graph, params_dict)\nRuntimeError: quantized::conv2d_relu expected scale to be 7th input\n<\/code>\nHow could it be that layer has lost its parameters?\n\n\n\n zetyquickly:\n\nexpected scale to be 7th input\n\n\nwe did some refactor in this PR: https:\/\/github.com\/pytorch\/pytorch\/pull\/35923\/files that removed some arguments from quantized::conv2d related ops. Does it work for quantized::conv2d?\n thankyou, this allowed us to take a step forward!\nChanged fusing configuration.\nNow all QuantizedConvReLU2d to QuantizedConv2d + QuantizedReLU. Don\u2019t know whether it\u2019s work but it produces a graph but it\u2019s inconsistent. It causes an error that I\u2019ve seen already.\nSomething wrong with produced ONNX graph.\n<code class=\"lang-auto\">Traceback (most recent call last):\n  File \".\/tools\/caffe2_converter.py\", line 115, in <module>\n    caffe2_model = export_caffe2_model(cfg, model, first_batch)\n  File \"\/root\/some_detectron2\/detectron2\/export\/api.py\", line 157, in export_caffe2_model\n    return Caffe2Tracer(cfg, model, inputs).export_caffe2()\n  File \"\/root\/some_detectron2\/detectron2\/export\/api.py\", line 95, in export_caffe2\n    predict_net, init_net = export_caffe2_detection_model(model, inputs)\n  File \"\/root\/some_detectron2\/detectron2\/export\/caffe2_export.py\", line 147, in export_caffe2_detection_model\n    onnx_model = export_onnx_model(model, (tensor_inputs,))\n  File \"\/root\/some_detectron2\/detectron2\/export\/caffe2_export.py\", line 66, in export_onnx_model\n    example_outputs=traced(inputs[0])\n  File \"\/root\/anaconda2\/envs\/pytorch-gpu\/lib\/python3.7\/site-packages\/torch\/onnx\/__init__.py\", line 172, in export\n    custom_opsets, enable_onnx_checker, use_external_data_format)\n  File \"\/root\/anaconda2\/envs\/pytorch-gpu\/lib\/python3.7\/site-packages\/torch\/onnx\/utils.py\", line 92, in export\n    use_external_data_format=use_external_data_format)\n  File \"\/root\/anaconda2\/envs\/pytorch-gpu\/lib\/python3.7\/site-packages\/torch\/onnx\/utils.py\", line 557, in _export\n    _check_onnx_proto(proto)\nRuntimeError: Attribute 'kernel_shape' is expected to have field 'ints'\n\n==> Context: Bad node spec: input: \"735\" input: \"98\" input: \"99\" output: \"743\" op_type: \"Conv\" attribute { name: \"dilations\" ints: 1 ints: 1 type: INTS } attribute { name: \"group\" i: 1 type: INT } attribute { na\nme: \"kernel_shape\" type: INTS } attribute { name: \"pads\" ints: 1 ints: 1 ints: 1 ints: 1 type: INTS } attribute { name: \"strides\" ints: 1 ints: 1 type: INTS }\n<\/code>\nThis is very location where quantized output is dequantized and fed into Conv of RPN.\nHere are the bits of a graph output:\n<code class=\"lang-auto\">...\n%98 : Long(1:1),\n%99 : Long(1:1),\n...\n%620 : QUInt8(1:1638400, 64:25600, 128:200, 200:1) = _caffe2::Int8Relu[Y_scale=0.045047003775835037, Y_zero_point=119](%619), scope: __module._wrapped_model.backbone\/__module._wrapped_model.backbone.p2_out\/__module._wrapped_model.backbone.p2_out.2 # \/root\/anaconda2\/envs\/pytorch-gpu\/lib\/python3.7\/site-packages\/torch\/nn\/quantized\/functional.py:381:0\n...\n%735 : Float(1:1638400, 64:25600, 128:200, 200:1) = _caffe2::Int8Dequantize(%620), scope: __module._wrapped_model.backbone\/__module._wrapped_model.backbone.dequant_out # \/root\/anaconda2\/envs\/pytorch-gpu\/lib\/python3.7\/site-packages\/torch\/nn\/quantized\/modules\/__init__.py:74:0\n...\n%743 : Float(1:1638400, 64:25600, 128:200, 200:1) = onnx::Conv[dilations=[1, 1], group=1, kernel_shape=annotate(List[int], []), pads=[1, 1, 1, 1], strides=[1, 1]](%735, %98, %99), scope: __module._wrapped_model.proposal_generator\/__module._wrapped_model.proposal_generator.rpn_head\/__module._wrapped_model.proposal_generator.rpn_head.conv # \/root\/anaconda2\/envs\/pytorch-gpu\/lib\/python3.7\/site-packages\/torch\/nn\/modules\/conv.py:374:0\n<\/code>\nImportant that eager mode model without quantization smoothly passes through this convertion pipeline and it is not data dependent.\nIf you are interested this is detectron2 export to Caffe2 pipeline with link \"https:\/\/github.com\/facebookresearch\/detectron2\/tree\/master\/detectron2\/export\"\n\n\n\n zetyquickly:\n\nRuntimeError: quantized::conv2d_relu expected scale to be 7th input\n\n\nPlease re-try with pytorch nightly build, we recently fixed this so you shouldn\u2019t be seeing this error anymore.\n\n\n\n zetyquickly:\n\n<code class=\"lang-auto\">RuntimeError: Attribute 'kernel_shape' is expected to have field 'ints'\n<\/code>\n\n\nSeems like the conv layer is not quantized so it produces onnx::Conv as opposed to the _caffe2::Int8Conv operator. Currently the onnx export path to caffe2 does not support partially quantized model, so it expects the entire pytorch model to be able to get quantized.\nThank you very much ,\nI am still eager to find a solution. I\u2019ve tried your assumptions, installed fresh build and tried again. Re-run QAT on model (just to make sure) and exporting process.\nNow it says that MaxPool cannot be created.\n<code class=\"lang-auto\">Traceback (most recent call last):\n  File \".\/tools\/caffe2_converter.py\", line 114, in <module>\n    caffe2_model = export_caffe2_model(cfg, model, first_batch)\n  File \"\/root\/some_detectron2\/detectron2\/export\/api.py\", line 157, in export_caffe2_model\n    return Caffe2Tracer(cfg, model, inputs).export_caffe2()\n  File \"\/root\/some_detectron2\/detectron2\/export\/api.py\", line 95, in export_caffe2\n    predict_net, init_net = export_caffe2_detection_model(model, inputs)\n  File \"\/root\/some_detectron2\/detectron2\/export\/caffe2_export.py\", line 151, in export_caffe2_detection_model\n    onnx_model = export_onnx_model(model, (tensor_inputs,))\n  File \"\/root\/some_detectron2\/detectron2\/export\/caffe2_export.py\", line 53, in export_onnx_model\n    traced = torch.jit.trace(model, inputs, strict=False)\n  File \"\/root\/anaconda2\/envs\/pytorch-gpu\/lib\/python3.7\/site-packages\/torch\/jit\/__init__.py\", line 900, in trace\n    check_tolerance, strict, _force_outplace, _module_class)\n  File \"\/root\/anaconda2\/envs\/pytorch-gpu\/lib\/python3.7\/site-packages\/torch\/jit\/__init__.py\", line 1054, in trace_module\n    module._c._create_method_from_trace(method_name, func, example_inputs, var_lookup_fn, strict, _force_outplace)\n  File \"\/root\/anaconda2\/envs\/pytorch-gpu\/lib\/python3.7\/site-packages\/torch\/nn\/modules\/module.py\", line 575, in __call__\n    result = self._slow_forward(*input, **kwargs)\n  File \"\/root\/anaconda2\/envs\/pytorch-gpu\/lib\/python3.7\/site-packages\/torch\/nn\/modules\/module.py\", line 561, in _slow_forward\n    result = self.forward(*input, **kwargs)\n  File \"\/root\/anaconda2\/envs\/pytorch-gpu\/lib\/python3.7\/contextlib.py\", line 74, in inner\n    return func(*args, **kwds)\n  File \"\/root\/some_detectron2\/detectron2\/export\/caffe2_modeling.py\", line 319, in forward\n    features = self._wrapped_model.backbone(images.tensor)\n  File \"\/root\/anaconda2\/envs\/pytorch-gpu\/lib\/python3.7\/site-packages\/torch\/nn\/modules\/module.py\", line 575, in __call__\n    result = self._slow_forward(*input, **kwargs)\n  File \"\/root\/anaconda2\/envs\/pytorch-gpu\/lib\/python3.7\/site-packages\/torch\/nn\/modules\/module.py\", line 561, in _slow_forward\n    result = self.forward(*input, **kwargs)\n  File \"\/root\/DensePose_ADASE\/densepose\/modeling\/quantize.py\", line 205, in new_forward\n    return {\"p2\": p2_out, \"p3\": p3_out, \"p4\": p4_out, \"p5\": p5_out, \"p6\": self.top_block(p5_out)[0]}\n  File \"\/root\/anaconda2\/envs\/pytorch-gpu\/lib\/python3.7\/site-packages\/torch\/nn\/modules\/module.py\", line 575, in __call__\n    result = self._slow_forward(*input, **kwargs)\n  File \"\/root\/anaconda2\/envs\/pytorch-gpu\/lib\/python3.7\/site-packages\/torch\/nn\/modules\/module.py\", line 561, in _slow_forward\n    result = self.forward(*input, **kwargs)\n  File \"\/root\/some_detectron2\/detectron2\/modeling\/backbone\/fpn.py\", line 177, in forward\n    return [F.max_pool2d(x, kernel_size=1, stride=2, padding=0)]\n  File \"\/root\/anaconda2\/envs\/pytorch-gpu\/lib\/python3.7\/site-packages\/torch\/_jit_internal.py\", line 210, in fn\n    return if_false(*args, **kwargs)\n  File \"\/root\/anaconda2\/envs\/pytorch-gpu\/lib\/python3.7\/site-packages\/torch\/nn\/functional.py\", line 576, in _max_pool2d\n    input, kernel_size, stride, padding, dilation, ceil_mode)\nRuntimeError: createStatus == pytorch_qnnp_status_success INTERNAL ASSERT FAILED at \"\/opt\/conda\/conda-bld\/pytorch_1590649859799\/work\/aten\/src\/ATen\/native\/quantized\/cpu\/qpool.cpp\":313, please report a bug to PyTorch. failed to create QNNPACK MaxPool operator\n\n<\/code>\nPath to the source with link \"https:\/\/github.com\/pytorch\/pytorch\/blob\/352731bd6edefbec707fbef3662f4a60934b8443\/aten\/src\/ATen\/native\/quantized\/cpu\/qpool.cpp#L330\"\nLooks weird, why didn\u2019t it happen earlier?\nUPD: It looks like nested F.max_pool2d won\u2019t quantize. Test showed that it works with float32 after convert\n\n\n\n supriyar:\n\nSeems like the conv layer is not quantized so it produces onnx::Conv as opposed to the _caffe2::Int8Conv operator.\n\n\nIs it possible to find a workaround for now? Do I understand correctly, that it is impossible to have a network with quantize dequantize during inference in Caffe2 export?\nUPD: what if we just make all Convs are quantized for ONNX.export not fail\n\n\n\n zetyquickly:\n\nUPD: what if we just make all Convs are quantized for ONNX.export not fail\n\n\nIf all convs in the network are quantized it should work and you will see _caffe2::Int8Conv ops in the converted network.\n\n\n\n zetyquickly:\n\nDo I understand correctly, that it is impossible to have a network with quantize dequantize during inference in Caffe2 export?\n\n\nYou would have quantize dequantize at the start and end of the network. Which implies all the network ops are quantized.\nThank you  , I see\nI have another question about non-quant operations in network\nWhat do you think if we register C10 export like it is done here with link \"https:\/\/github.com\/pytorch\/pytorch\/blob\/master\/caffe2\/operators\/roi_align_op.cc#L302\"\nwould it be possible to patch non-quantized operators from torch.nn.ConvTranspose2d to torch.ops._caffe2.ConvTranspose2d to use them as is? Or is it better to implement quantized version of nn.ConvTranspose2d?\n I could implement a PR of such functionality if it is valid\nI think we are already working on quantized version of conv2d transpose, cc \nHello, can you now export the quantized model to Caffe2, and then export Caffe2 to ncnn? Thank you!\nHello, \nFirst of all I haven\u2019t managed to export quantized network to Caffe2. I do not know is it possible to export it to ncnn. Just a suggestion, maybe it is better to export model to ONNX and than to ncnn\nhello,   When will it be possible to support the conversion of the quantified model to onnx, I hope there is a reference time, thank you\uff01\nwe are not working on onnx conversions, feel free to submit PRs to add the support."},{"x":"I have the following decoder that after a few steps only predicts the EOS token. Overfitting on a dummy, tiny dataset is impossible because of this so it seems that there is a big error in the code.\n<code class=\"lang-python\">Decoder(\n  (embedding): Embeddings(\n    (word_embeddings): Embedding(30002, 768, padding_idx=3)\n    (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n    (dropout): Dropout(p=0.5, inplace=False)\n  )\n  (ffn1): FFN(\n    (dense): Linear(in_features=768, out_features=512, bias=False)\n    (layernorm): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n    (dropout): Dropout(p=0.5, inplace=False)\n    (activation): GELU()\n  )\n  (rnn): GRU(512, 512, batch_first=True, bidirectional=True)\n  (ffn2): FFN(\n    (dense): Linear(in_features=1024, out_features=512, bias=False)\n    (layernorm): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n    (dropout): Dropout(p=0.5, inplace=False)\n    (activation): GELU()\n  )\n  (selector): Sequential(\n    (0): Linear(in_features=512, out_features=30002, bias=True)\n    (1): LogSoftmax(dim=-1)\n  )\n)\n<\/code>\nThe forward is relatively straightforward (see what I did there?): pass the input_ids to the embedding and a FFN, then use that representation in the RNN with the given sembedding as initial hidden state. Pass the output through another FFN and do softmax. Return logits and last hidden states of the RNN. In the next step, use those hidden states as the new hidden states, and the highest predicted token as the new input.\n<code class=\"lang-python\">def forward(self, input_ids, sembedding):\n    embedded = self.embedding(input_ids)\n    output = self.ffn1(embedded)\n    output, hidden = self.rnn(output, sembedding)\n    output = self.ffn2(output)\n    logits = self.selector(output)\n\n    return logits, hidden\n<\/code>\nsembedding is the initial hidden_state for the RNN. This is similar to an encoder-deocder architecture only here we do not train the encoder but we do have access to pretrained encoder representations.\nIn my training loop I start off each batch with a SOS token and feed every top predicted token to next step until target_len is reached. I also swap randomly between teacher forced training.\n<code class=\"lang-python\">def step(self, batch, teacher_forcing_ratio=0.5):\n    batch_size, target_len = batch[\"input_ids\"].size()[:2]\n    # Init first decoder input woth SOS (BOS) token\n    decoder_input = torch.tensor([[self.tokenizer.bos_token_id]] * batch_size).to(self.device)\n    batch[\"input_ids\"] = batch[\"input_ids\"].to(self.device)\n\n    # Init first decoder hidden_state: one zero'd second embedding in case the RNN is bidirectional\n    decoder_hidden = torch.stack((batch[\"sembedding\"],\n                                  torch.zeros(*batch[\"sembedding\"].size()))\n                                 ).to(self.device) if self.model.num_directions == 2 \\\n        else batch[\"sembedding\"].unsqueeze(0).to(self.device)\n\n    loss = torch.tensor([0.]).to(self.device)\n\n    use_teacher_forcing = random.random() < teacher_forcing_ratio\n    # contains tuples of predicted and correct words\n    tokens = []\n    for i in range(target_len):\n        # overwrite previous decoder_hidden\n        output, decoder_hidden = self.model(decoder_input, decoder_hidden)\n        batch_correct_ids = batch[\"input_ids\"][:, i]\n\n        # NLLLoss compute loss between predicted classes (bs x classes) and correct classes for _this word_\n        # set to ignore the padding index\n        loss += self.criterion(output[:, 0, :], batch_correct_ids)\n\n        batch_predicted_ids = output.topk(1).indices.squeeze(1).detach()\n\n        # if use teacher training: use current correct word for next prediction\n        # else do NOT use teacher training: us current predction for next prediction\n        decoder_input = batch_correct_ids.unsqueeze(1) if use_teacher_forcing else batch_predicted_ids\n\n    return loss, loss.item() \/ target_len\n<\/code>\nI also clip the gradients after each step:\n<code class=\"lang-python\">clip_grad_norm_(self.model.parameters(), 1.0)\n<\/code>\nAt first subsequent predictions are already relatively identical, but after a few iterations there\u2019s a bit more variation. But relatively quickly ALL predictions turn into other words (but always the same ones), eventually turning into EOS tokens (edit: after changing the activation to ReLU, another token is always predicted - it seems like a random token that always gets repeated). Note that this already happens after 80 steps (batch_size 128).\nI found that the returned hidden state of the RNN contains a lot of zeros. I am not sure if that is the problem but it seems like it could be related.\n<code class=\"lang-python\">tensor([[[  3.9874e-02,  -6.7757e-06,   2.6094e-04,  ...,  -1.2708e-17,\n            4.1839e-02,   7.8125e-03],\n         [ -7.8125e-03,  -2.5341e-02,   7.8125e-03,  ...,  -7.8125e-03,\n           -7.8125e-03,  -7.8125e-03],\n         [ -0.0000e+00, -1.0610e-314,   0.0000e+00,  ...,   0.0000e+00,\n            0.0000e+00,   0.0000e+00],\n         [  0.0000e+00,   0.0000e+00,   0.0000e+00,  ...,   0.0000e+00,\n           -0.0000e+00,  1.0610e-314]]], device='cuda:0', dtype=torch.float64,\n       grad_fn=<CudnnRnnBackward>)\n<\/code>\nI have no idea what might be going wrong although I suspect that the issue is rather with my step than with the model. I already tried playing with the learning rate, disabling some layers (LayerNorm, dropout, ffn2), using pretrained embeddings and freezing or unfreezing them, and disabling teacher forcing, using bidrectional vs unidirectional GRU. The end result is always the same.\nIf you have any pointers, that would be very helpful. I have googled many things concerning neural networks always predicting the same item and I have tried all the suggestions that I could find. Any new ones, no matter how crazy, are welcome!","y":"In my case the issue appeared to be that the  dtype  of the initial hidden state was a double and the input was a float. I don\u2019t quite understand  why  that is an issue, but casting the hidden state to a float solved the issue. If you have any intuition about why this might be a problem for PyTorch, do let me know in the comments.","z":"Not really any solution \u2013 I cannot see any obvious issues in your code \u2013 just some ideas you might want to consider. It essentially aims to simplify to a more tried and tested decoder architecture:\n\n\nI\u2019ve never seen a linear layer between an embedding and an RNN layer. Not sure what this means semantically, but I would skip that one for a start.\n\n\nIt don\u2019t think it makes sense that the decoder is bidirectional. Since you feed it one token\/word step by step, the output should be the same anyway. I would remove the stacking to remove any potential point of error.\n\n\nI would start with the basic decoder (without attention; not relevant for you anyway) of the basis Seq2Seq tutorial with link \"https:\/\/pytorch.org\/tutorials\/intermediate\/seq2seq_translation_tutorial.html\". Again, just because it\u2019s tried and tested\u2026including by myself :).\n\n\nTo accommodate the Seq2Seq tutorial, I would also work with batches of size 1. You seem to use larger batches, but that\u2019s not obivous to do for a decoder. I only do it when I know that that my target sequences in a batch have the same lengths (e.g., for an autoencoder).\n\n\nIn short, I would start with the most basic architecture\/setup using tested code (snippets) as far as possible (and batch_size=1) to get it working (= easy overfitting on a small dataset). Once that seems fine, I add complexity to improve performance and\/or accuracy \u2013 or to modify the basic model to my specific use case.\nThanks. I started from the seq2seq tutorial, but somewhere along the way things apparently went wrong. Unofrtunately I hadn\u2019t put my code into version control yet so it is hard to traceback my steps. As a last resort I\u2019ll try again from scratch.\n\nIn my case I need a linear between the embedding and the RNN because the embeddings are pretrained and  sembedding  are pretrained, but they are not the same size. So I need a linear transformation from on to the other\nyou are right, yet disabling bidrectionality does not change anything\nfrom what I have read, I think batched decoding is not an issue: in your loss function you just ignore the index for the padding token. Yes, you have some computational overhead for items in the batch that have already reached their EOS, but that\u2019s generally worth it compared to the gains that you get from batch_sizes of 64 and more. It does require more memory, though\n\nThanks for your input!\nIn my case the issue appeared to be that the  dtype  of the initial hidden state was a double and the input was a float. I don\u2019t quite understand  why  that is an issue, but casting the hidden state to a float solved the issue. If you have any intuition about why this might be a problem for PyTorch, do let me know in the comments.\nInteresting issue. Unfortunately, I have no idea, but good to know in the future.\nI posted a separate issue about this in case anyone has an idea. Is it required that input and hidden for GRU have the same dtype? with link \"https:\/\/discuss.pytorch.org\/t\/is-it-required-that-input-and-hidden-for-gru-have-the-same-dtype\/96221\" It seems that PyTorch should at least give a warning if the hidden state should be float32.\nIf you use a batch size of one, do you still run optimizer.step after every step (so after every single datapoint)  or do you accumulate?"},{"x":"Hi,\nI am getting the following error:\n<code class=\"lang-auto\">Traceback (most recent call last):\n  File \"train.py\", line 549, in <module>\n    task.execute()\n  File \"train.py\", line 271, in execute\n    train_loss, train_acc = self.train(phase)\n  File \"train.py\", line 147, in train\n    answer = self.model(batch)\n  File \"\/home\/eprox\/.local\/lib\/python3.6\/site-packages\/torch\/nn\/modules\/module.py\", line 541, in __call__\n    result = self.forward(*input, **kwargs)\n  File \"\/home\/eprox\/uom\/clnli\/clnli-code\/rnn-impl\/models\/bilstm.py\", line 69, in forward\n    premise_embed = self.embedding(batch.premise)\n  File \"\/home\/eprox\/.local\/lib\/python3.6\/site-packages\/torch\/nn\/modules\/module.py\", line 541, in __call__\n    result = self.forward(*input, **kwargs)\n  File \"\/home\/eprox\/.local\/lib\/python3.6\/site-packages\/torch\/nn\/modules\/sparse.py\", line 114, in forward\n    self.norm_type, self.scale_grad_by_freq, self.sparse)\n  File \"\/home\/eprox\/.local\/lib\/python3.6\/site-packages\/torch\/nn\/functional.py\", line 1484, in embedding\n    return torch.embedding(weight, input, padding_idx, scale_grad_by_freq, sparse)\nRuntimeError: index out of range: Tried to access index 12209 out of table with 4979 rows. at \/pytorch\/aten\/src\/TH\/generic\/THTensorEvenMoreMath.cpp:418\n<\/code>\nModule code:\n<code class=\"lang-auto\">class bilstm(nn.Module):\n\tdef __init__(self, sent_embed, d_hidden, dp_ratio, device):\n\t\tsuper(BiLSTM, self).__init__()\n\t\tself.hidden_size = d_hidden\n\t\tself.directions = 2\n\t\tself.num_layers = 2\n\t\tself.concat = 4\n\t\tself.device = device\n\t\tself.out_dim = 3\n\t\tself.embedding = nn.Embedding.from_pretrained(sent_embed)\n\t\tprint('SentEmbed:'); print(sent_embed.shape) \n\t\t# Output: torch.Size([4980, 1024])\n\t\tself.embed_dim = sent_embed_dim\n\t\t\n\t\tprint(self.embedding) # Output: Embedding(4980, 1024)\n\t\tprint('EMBED DIM IS: {}'.format(self.embed_dim)) # EMBED DIM IS: 1024\n\t\tself.projection = nn.Linear(self.embed_dim, self.hidden_size)\n\t\tself.lstm = nn.LSTM(self.hidden_size, self.hidden_size, self.num_layers,\n\t\t\t\t\t\t\t\t\tbidirectional = True, batch_first = True, dropout = dp_ratio)\n\t\tself.relu = nn.LeakyReLU()\n\t\tself.dropout = nn.Dropout(p = dp_ratio)\n\n\t\tself.lin1 = nn.Linear(self.hidden_size * self.directions * self.concat, self.hidden_size)\n\t\tself.lin2 = nn.Linear(self.hidden_size, self.hidden_size)\n\t\tself.lin3 = nn.Linear(self.hidden_size, self.out_dim)\n\n\t\tfor lin in [self.lin1, self.lin2, self.lin3]:\n\t\t\tnn.init.xavier_uniform_(lin.weight)\n\t\t\tnn.init.zeros_(lin.bias)\n\n\t\tself.out = nn.Sequential(\n\t\t\tself.lin1,\n\t\t\tself.relu,\n\t\t\tself.dropout,\n\t\t\tself.lin2,\n\t\t\tself.relu,\n\t\t\tself.dropout,\n\t\t\tself.lin3\n\t\t)\n\n\tdef forward(self, batch):\n\t\tprint('*premiseshape:'); print(batch.premise.shape)\n\t\t# Output: torch.Size([128, 35]\n\t\tprint('*embeddingshape:'); print(self.embedding)\n\t\t# Output: Embedding(4980, 1024)\n\t\tprint('*projectionshape:'); print(self.projection)\n\t\t# Output: Linear(in_features=1024, out_features=100, bias=True)\n\t\tpremise_embed = self.embedding(batch.premise)\n\t\thypothesis_embed = self.embedding(batch.hypothesis)\n\n\t\tpremise_proj = self.relu(self.projection(premise_embed))\n\t\thypothesis_proj = self.relu(self.projection(hypothesis_embed))\n\n\t\th0 = c0 = torch.tensor([]).new_zeros((self.num_layers * self.directions, batch.batch_size, self.hidden_size)).to(self.device)\n\n\t\t_, (premise_ht, _) = self.lstm(premise_proj, (h0, c0))\n\t\t_, (hypothesis_ht, _) = self.lstm(hypothesis_proj, (h0, c0))\n\n\t\tpremise = premise_ht[-2:].transpose(0, 1).contiguous().view(batch.batch_size, -1)\n\t\thypothesis = hypothesis_ht[-2:].transpose(0, 1).contiguous().view(batch.batch_size, -1)\n\n\t\tcombined = torch.cat((premise, hypothesis, torch.abs(premise - hypothesis), premise * hypothesis), 1)\n\t\treturn self.out(combined)\n<\/code>\nBased on the outputs in the comments, it seems like the embedding and the sentence embedding are the correct size. So I cannot understand why it is trying to access the index 12209.\nCould someone please advise me on this?","y":"Based on the error message it seems that the input to the embedding contains an index of 12209, while the number of embeddings is set to 4949.\nCould you check it via print(batch.premise.max()) and make sure the indices are in the valid range ([0, num_embeddings-1])?","z":"Based on the error message it seems that the input to the embedding contains an index of 12209, while the number of embeddings is set to 4949.\nCould you check it via print(batch.premise.max()) and make sure the indices are in the valid range ([0, num_embeddings-1])?"},{"x":"Hi all,\nI am trying to convert this tensorflow code into pytorch. For example, I converted the below tensorflow code\n<code class=\"lang-auto\">tf.get_variable(\"char_embeddings\", [len(data.char_dict), data.char_embedding_size]),  char_index)  # [num_sentences, max_sentence_length, max_word_length, emb]\n\n<\/code>\ninto\n<code class=\"lang-auto\">class CharEmbeddings(nn.Module):\n    def __init__(self, config, data):\n          ....\n          self.embeddings = nn.init.xavier_uniform_(torch.empty(len(data.char_dict), data.char_embedding_size))\n   \n\n    def forward(self, char_index):\n        # [num_sentences, max_sentence_length, max_word_length, emb]\n        char_emb = self.embeddings[char_index]       \n<\/code>\nFrom what I read, if no initializer is passed to the get_variable here with link \"https:\/\/www.tensorflow.org\/api_docs\/python\/tf\/compat\/v1\/get_variable\", the glorot_uniform_initializer  will be used which is I think it equivalent to xavier_uniform_\nTwo questions here:\n\nIs that conversion valid?\nShould I expect the original embeddings self.embeddings to backpropagate and update its values? Is that the expected behavior from the tensorflow version as well? Should I add requires_grad to the embeddings tensor?\n","y":"\n\nProbably yes, but you should compare the default arguments to both methods, as each framework might use other defaults.\n\n\nself.embeddings is not created as an nn.Parameter, so Autograd won\u2019t calculate the gradients for this tensor. You could use:\n\n\n<code class=\"lang-python\">emb = torch.empty(len(...))\nnn.init.xqavier_uniform_(emb)\nself.embeddings = nn.Parameter(emb)\n<\/code>\ninstead to create a parameter, which will get gradients.","z":"\n\nProbably yes, but you should compare the default arguments to both methods, as each framework might use other defaults.\n\n\nself.embeddings is not created as an nn.Parameter, so Autograd won\u2019t calculate the gradients for this tensor. You could use:\n\n\n<code class=\"lang-python\">emb = torch.empty(len(...))\nnn.init.xqavier_uniform_(emb)\nself.embeddings = nn.Parameter(emb)\n<\/code>\ninstead to create a parameter, which will get gradients.\nThanks for your response. I don\u2019t get the difference here. Why a tensor with requires_grad will not get gradients while the Parameter will do? I am not sure if I understand required_grad well though. Any explanation?\nThanks!\n\n\n\n Ahmed_Abdelaziz:\n\nWhy a tensor with requires_grad will not get gradients while the Parameter will do?\n\n\nBoth will get gradients, but in your initial post you didn\u2019t set requires_grad=True in the tensor creation, so self.embeddings wouldn\u2019t get any gradients.\nAdditionally to that, a registered nn.Parameter will be automatically pushed to the device, if you call model.to() and will also be returned in model.parameters(), while a tensor will not.\nI got it, thank you!\nFollowing up on this question, I see that the model I am migrating from TensorFlow uses get_variable to intiazliae network biases\nhidden_bias = tf.get_variable(\"hidden_bias_{}\".format(i), [hidden_size])\nSo I assumed I can use torch.nn.init.xavier_uniform_ to init network bias in Torch too\ntorch.nn.init.xavier_uniform_(self.input.bias) but I got that error\nFan in and fan out can not be computed for tensor with less than 2 dimensions\nAny idea how to mimic the same init as TensorFlow for biases\nI don\u2019t know how TF initializes the bias, but as the error message claims, xavier_uniform cannot be used on parameters with less than 2 dimensions (which is the case for your bias parameter).\nYes true. However I did investigate the histograms for the bias from TF and it seems they pass Fan-in = Fan-out\nfan_in and fan_out are calculated as:\n<code class=\"lang-python\">def _calculate_fan_in_and_fan_out(tensor):\n    dimensions = tensor.dim()\n    if dimensions < 2:\n        raise ValueError(\"Fan in and fan out can not be computed for tensor with fewer than 2 dimensions\")\n\n    num_input_fmaps = tensor.size(1)\n    num_output_fmaps = tensor.size(0)\n    receptive_field_size = 1\n    if tensor.dim() > 2:\n        receptive_field_size = tensor[0][0].numel()\n    fan_in = num_input_fmaps * receptive_field_size\n    fan_out = num_output_fmaps * receptive_field_size\n\n    return fan_in, fan_out\n<\/code>\nWould you then set both values to bias.size(0)?\nIf that\u2019s the case, you could manually apply it to the xavier_uniform method, which is defined as:\n<code class=\"lang-python\">    fan_in, fan_out = _calculate_fan_in_and_fan_out(tensor)\n    std = gain * math.sqrt(2.0 \/ float(fan_in + fan_out))\n    a = math.sqrt(3.0) * std  # Calculate uniform bounds from standard deviation\n<\/code>\nand replace _calculate_fan_in_and_fan_out with tensor.size(0) for both values."},{"x":"Hey I\u2019m kinda a newbie\u2026\nHow can I calculate closest tensor in list to another tensor?\nExample:\nSay I have:\n<code class=\"lang-auto\">[tensor([1,1,1]),tensor([5,5,5]),tensor([10,10,10])]\n<\/code>\nand:\n<code class=\"lang-auto\">tensor([2,2,2])\n<\/code>\nIf would want:\n<code class=\"lang-auto\">tensor([1,1,1])\n<\/code>\nHow can I achieve this? Thank you","y":"You could calculate the distance between the tensors using torch.norm and use argmin to get the index corresponding to the smallest distance:\n<code class=\"lang-python\">a = torch.stack([torch.tensor([1,1,1]), torch.tensor([5,5,5]), torch.tensor([10,10,10])]).float()\nb = torch.tensor([2,2,2]).float()\nmin_idx = torch.norm(a - b.unsqueeze(0), dim=1).argmin()\nprint(a[min_idx])\n> tensor([1., 1., 1.])\n<\/code>","z":"You could calculate the distance between the tensors using torch.norm and use argmin to get the index corresponding to the smallest distance:\n<code class=\"lang-python\">a = torch.stack([torch.tensor([1,1,1]), torch.tensor([5,5,5]), torch.tensor([10,10,10])]).float()\nb = torch.tensor([2,2,2]).float()\nmin_idx = torch.norm(a - b.unsqueeze(0), dim=1).argmin()\nprint(a[min_idx])\n> tensor([1., 1., 1.])\n<\/code>\nThank you! I compared this with my old solution, and this is like 10X faster!"},{"x":"I am semi-new to nlp and language modeling and I was trying to duplicate the pytorch example for the word_language_model with my own code and I got stuck when generating output after training the RNN. In previous models I have used I generally got output by just using torch.max() but I noticed that this did not work for my model and the only way I could get actual sentences was by copying what is in the generate.py file.\n<code class=\"lang-auto\">output, hidden = model(input, hidden)\n                word_weights = output.squeeze().div(args.temperature).exp().cpu()\n                word_idx = torch.multinomial(word_weights, 1)[0]\n<\/code>\nI don\u2019t understand why after getting the output we have to use .div().exp().cpu() or what the purpose of torch.multinomial() is. I tried reading the docs but I didn\u2019t really see how it applies to this scenario. If anyone could help explain this I would greatly appreciate it, thanks!","y":"I\u2019ll try to help \nI am starting with multinomial (as it is a more straightforward part).\nWhen you have a trained language model, you need to have a strategy of sampling new sentences. The easiest way is to get a token with maximum likelihood - I assume that you already did it with max(). But this scenario has a significant drawback: your decoding is greedy. At every decoding\/sampling step, the only possible choice is the top token, resulting in deterministic behavior and inability to sample anything else. To add some variance to the results, we can randomize the next word.\nImagine at timestep t we have the following predictions:\na: 0.15\nthe: 0.10\nis: 0.09\nwas: 0.05\nMax approach will always select \u201ca\u201d. If we want to give slightly less likely tokens a chance to appear, we can select randomly. But not in a way when each token has the same probabiliy. We want to sample them with relative likelihoods. That\u2019s why we use multinomial function. It might be easier if you are familiar with numpy choice function with p parameter used: https:\/\/numpy.org\/doc\/stable\/reference\/random\/generated\/numpy.random.Generator.choice.html#numpy.random.Generator.choice\nIt allows us to generate a different sentence every time but keeping relative probability.\n(there are also more sophisticated methods like beam search).\nExample code also uses a concept of temperature. Basically, it is a calibration(?) of given probabilities. If the temperature is low, the sampler will be more conservative - sticking more to those very likely tokens. On the other hand, if the temperature is high - less likely tokens will have a relatively higher likelihood - therefore, there is a bigger chance they will be selected. You can somewhat expect that low temperatures will produce \u201cboring but correct\u201d results and high \u201cinteresting with errors\u201d in practice.\nFor more on temperature, please check: https:\/\/stackoverflow.com\/questions\/58764619\/why-should-we-use-temperature-in-softmax\nHope it helps ^^","z":"I\u2019ll try to help \nI am starting with multinomial (as it is a more straightforward part).\nWhen you have a trained language model, you need to have a strategy of sampling new sentences. The easiest way is to get a token with maximum likelihood - I assume that you already did it with max(). But this scenario has a significant drawback: your decoding is greedy. At every decoding\/sampling step, the only possible choice is the top token, resulting in deterministic behavior and inability to sample anything else. To add some variance to the results, we can randomize the next word.\nImagine at timestep t we have the following predictions:\na: 0.15\nthe: 0.10\nis: 0.09\nwas: 0.05\nMax approach will always select \u201ca\u201d. If we want to give slightly less likely tokens a chance to appear, we can select randomly. But not in a way when each token has the same probabiliy. We want to sample them with relative likelihoods. That\u2019s why we use multinomial function. It might be easier if you are familiar with numpy choice function with p parameter used: https:\/\/numpy.org\/doc\/stable\/reference\/random\/generated\/numpy.random.Generator.choice.html#numpy.random.Generator.choice\nIt allows us to generate a different sentence every time but keeping relative probability.\n(there are also more sophisticated methods like beam search).\nExample code also uses a concept of temperature. Basically, it is a calibration(?) of given probabilities. If the temperature is low, the sampler will be more conservative - sticking more to those very likely tokens. On the other hand, if the temperature is high - less likely tokens will have a relatively higher likelihood - therefore, there is a bigger chance they will be selected. You can somewhat expect that low temperatures will produce \u201cboring but correct\u201d results and high \u201cinteresting with errors\u201d in practice.\nFor more on temperature, please check: https:\/\/stackoverflow.com\/questions\/58764619\/why-should-we-use-temperature-in-softmax\nHope it helps ^^"},{"x":"I\u2019m trying to create *_key_padding_mask for torch.nn.Transformer. So I want to create a mask using the length of sentence data in the batch.\nAssuming number of time steps  <s>t=7<\/s> t=8\nEdit: changed value of t\n<code class=\"lang-auto\">batch_size = 5\nidx = torch.tensor([3,4,2,1,6])\n\n\nmask = torch.tensor([[0,0,0,1,1,1,1,1],\n                     [0,0,0,0,1,1,1,1],\n                     [0,0,1,1,1,1,1,1],\n                     [0,1,1,1,1,1,1,1],\n                     [0,0,0,0,0,0,1,1]])\n<\/code>\nHow do I get mask using idx with vectorized code?","y":"\u2019s solution with torch.repeat_interleave is quite interesting. However, I found something simple.\nUpper triangular matrix\n<code class=\"lang-auto\">t = 8\ntri = torch.triu( torch.ones(8,8), diagonal=1 )\ntri\ntensor([[0., 1., 1., 1., 1., 1., 1., 1.],\n        [0., 0., 1., 1., 1., 1., 1., 1.],\n        [0., 0., 0., 1., 1., 1., 1., 1.],\n        [0., 0., 0., 0., 1., 1., 1., 1.],\n        [0., 0., 0., 0., 0., 1., 1., 1.],\n        [0., 0., 0., 0., 0., 0., 1., 1.],\n        [0., 0., 0., 0., 0., 0., 0., 1.],\n        [0., 0., 0., 0., 0., 0., 0., 0.]])\n<\/code>\nClose but if the idx tensor says 3 then index 3 must also be masked.\n<code class=\"lang-auto\">tri.fill_diagonal_(1)\ntri\ntensor([[1., 1., 1., 1., 1., 1., 1., 1.],\n        [0., 1., 1., 1., 1., 1., 1., 1.],\n        [0., 0., 1., 1., 1., 1., 1., 1.],\n        [0., 0., 0., 1., 1., 1., 1., 1.],\n        [0., 0., 0., 0., 1., 1., 1., 1.],\n        [0., 0., 0., 0., 0., 1., 1., 1.],\n        [0., 0., 0., 0., 0., 0., 1., 1.],\n        [0., 0., 0., 0., 0., 0., 0., 1.]])\n<\/code>\nNow this upper triangular matrix can be referenced to create a mask\n<code class=\"lang-auto\">tri[3]\ntensor([0., 0., 0., 1., 1., 1., 1., 1.])\n\nidx = torch.tensor([4,1,2,5,3,6])\ntri[idx]\ntensor([[0., 0., 0., 0., 1., 1., 1., 1.],\n        [0., 1., 1., 1., 1., 1., 1., 1.],\n        [0., 0., 1., 1., 1., 1., 1., 1.],\n        [0., 0., 0., 0., 0., 1., 1., 1.],\n        [0., 0., 0., 1., 1., 1., 1., 1.],\n        [0., 0., 0., 0., 0., 0., 1., 1.]])\n<\/code>\nSince we only need 0 or 1 we can make to dtype bool or byte and this way the method can work for large values of t.","z":"<code class=\"lang-auto\">import torch\n\nbatch_size = 5\n\nidx = torch.tensor([3,4,1,1,6])\n\n# I changed the mask a little\n\nmask = torch.tensor([[0,0,0,1,1,1,1,1],\n\n                     [0,0,0,0,1,1,1,1],\n\n                     [0,0,1,1,1,1,1,1],\n\n                     [0,1,1,1,1,1,1,1],\n\n                     [0,0,0,0,0,0,1,1]])\n\nprint(torch.gather(mask, dim=1, index=idx.unsqueeze(-1).expand(mask.size(0), -1)))\n<\/code>\nNot getting the expected output.\nMask\u2019s value in my question is the expected output. You already seem to have initialized the mask with the expected output.\nIt seems not easy for current pytorch. Because current operations only accept scalars as the parameters instead of tensor. Something like torch.range(tensor start, tensor end) sovles your problem. You can also refer to https:\/\/github.com\/pytorch\/nestedtensor (which is still experimental)\nThis piece of code may be a good start:\n<code class=\"lang-auto\">torch.repeat_interleave(torch.tensor([0, 1, 0, 1]), torch.tensor([3, 8-3 4, 8-4])).view(2, 8)\n# gives \n# tensor([[0, 0, 0, 1, 1, 1, 1, 1],\n#         [0, 0, 0, 0, 1, 1, 1, 1]])\n<\/code>\nWe can obtain the one-line code:\n<code class=\"lang-auto\">torch.repeat_interleave(torch.tensor([0, 1]*batch_size), torch.stack([idx, 8-idx], dim=1).view(-1)).view(batch_size, 8)\n<\/code>\n\u2019s solution with torch.repeat_interleave is quite interesting. However, I found something simple.\nUpper triangular matrix\n<code class=\"lang-auto\">t = 8\ntri = torch.triu( torch.ones(8,8), diagonal=1 )\ntri\ntensor([[0., 1., 1., 1., 1., 1., 1., 1.],\n        [0., 0., 1., 1., 1., 1., 1., 1.],\n        [0., 0., 0., 1., 1., 1., 1., 1.],\n        [0., 0., 0., 0., 1., 1., 1., 1.],\n        [0., 0., 0., 0., 0., 1., 1., 1.],\n        [0., 0., 0., 0., 0., 0., 1., 1.],\n        [0., 0., 0., 0., 0., 0., 0., 1.],\n        [0., 0., 0., 0., 0., 0., 0., 0.]])\n<\/code>\nClose but if the idx tensor says 3 then index 3 must also be masked.\n<code class=\"lang-auto\">tri.fill_diagonal_(1)\ntri\ntensor([[1., 1., 1., 1., 1., 1., 1., 1.],\n        [0., 1., 1., 1., 1., 1., 1., 1.],\n        [0., 0., 1., 1., 1., 1., 1., 1.],\n        [0., 0., 0., 1., 1., 1., 1., 1.],\n        [0., 0., 0., 0., 1., 1., 1., 1.],\n        [0., 0., 0., 0., 0., 1., 1., 1.],\n        [0., 0., 0., 0., 0., 0., 1., 1.],\n        [0., 0., 0., 0., 0., 0., 0., 1.]])\n<\/code>\nNow this upper triangular matrix can be referenced to create a mask\n<code class=\"lang-auto\">tri[3]\ntensor([0., 0., 0., 1., 1., 1., 1., 1.])\n\nidx = torch.tensor([4,1,2,5,3,6])\ntri[idx]\ntensor([[0., 0., 0., 0., 1., 1., 1., 1.],\n        [0., 1., 1., 1., 1., 1., 1., 1.],\n        [0., 0., 1., 1., 1., 1., 1., 1.],\n        [0., 0., 0., 0., 0., 1., 1., 1.],\n        [0., 0., 0., 1., 1., 1., 1., 1.],\n        [0., 0., 0., 0., 0., 0., 1., 1.]])\n<\/code>\nSince we only need 0 or 1 we can make to dtype bool or byte and this way the method can work for large values of t.\n Nice solution!"},{"x":"Hi everyone, I\u2019m using pretrain word embedding for nmt task. I have idea about using word features such as named entity to improve nmt quality. Is it possible to concate one hot vector (named entity) to my pretrain word embedding and use it for my nmt model ?\nFor example:\nGiven sentence: My name is James .\nNE annotated sentence: My|O name|O is|O James|PERSON\nWith James|PERSON, i will concate one-hot vector of PERSON tag (e.g [1,0,0,0]) to word embedding vector of \u201cJames\u201d (e.g [4,5,6]). So result is [4,5,6,1,0,0,0]","y":"Sure, you can concatenate vectors. For example, say you have\n\n\nembed with embed.shape = (batch_size, seq_len, embed_dim)\n\n\ncustom with custom.shape = (batch_size, seq_len, custom_dim)\n\n\nYou can do:\nX = torch.cat([embed, custom], 2)\nThen X.shape = (batch_size, seq_len, embed_dim+custom_dim)","z":"Sure, you can concatenate vectors. For example, say you have\n\n\nembed with embed.shape = (batch_size, seq_len, embed_dim)\n\n\ncustom with custom.shape = (batch_size, seq_len, custom_dim)\n\n\nYou can do:\nX = torch.cat([embed, custom], 2)\nThen X.shape = (batch_size, seq_len, embed_dim+custom_dim)\nyes you can do that,\njust add extras embedding of suitable dimension,\nembedding = nn.Embedding(vocab_size, dim),\nembedding.shape = (batch_size, seq_len, dims)\nextended_dim = (batch_size, seq_len, extended_dims)\nfinal = torch.cat([embedding, extended],2)]\nthe final is the vector with embeddings of name_entity as well. \nHello, does this guarantee that your custom embeddings will train ?\nIf you will use standard pytorch blocks (contained in nn module) - they will. If you create tensors by yourself that should be updated by autograd you need to wrap them inside nn.Parameter. In any case you can check if parameter is actually updated by checking value of your_custom_parameter.grad after doing loss.backward(). If it will be non-zero that means it will be updated by the optimizer."},{"x":"In order to add noise to the XNOR-Net, I need to modify the trained weights which contains only 1 and -1. So I think the problem is how to  generate a tensor with random number of 1 and -1, and then multiply this tensor with the trained weights. The solution of mine is following:\n<code class=\"lang-auto\"> def add_noise_to_weights(m):\n            s = m.data.size()\n            n = m.data.nelement()\n            r = round(n*0.01) #0.01 is the noise ratio\n            mask = -torch.ones([1, r]).type_as(m.data)\n            neg_mask = torch.ones([1, n-r]).type_as(m.data)\n            t = torch.cat((mask, neg_mask),1).reshape(s)\n            idx = torch.randperm(t.nelement())\n            t = t.view(-1)[idx].view(t.size())\n            m.data = m.data.mul(t)\n           return m\n<\/code>\nDespite it works, the code is too complicated, so could you guys have some simply solutions?","y":"If I understand correctly, you just want to generate a tensor which contains -1 and 1 values ?\nWould something like this work for you ?\n<code class=\"lang-auto\">a = torch.Tensor([-1, 1])\nidx = np.random.randint(2, size=your_shape)\nnoise = a[idx]\n<\/code>\nEdit :\nFurthermore, if you want to have some control on the number of 1 or -1 in your tensor, I would suggest:\n<code class=\"lang-auto\">a = torch.Tensor([-1, 1])\nidx = np.random.choice(2, size=your_shape, p=[r, 1-r])\nnoise = a[idx]\n<\/code>","z":"If I understand correctly, you just want to generate a tensor which contains -1 and 1 values ?\nWould something like this work for you ?\n<code class=\"lang-auto\">a = torch.Tensor([-1, 1])\nidx = np.random.randint(2, size=your_shape)\nnoise = a[idx]\n<\/code>\nEdit :\nFurthermore, if you want to have some control on the number of 1 or -1 in your tensor, I would suggest:\n<code class=\"lang-auto\">a = torch.Tensor([-1, 1])\nidx = np.random.choice(2, size=your_shape, p=[r, 1-r])\nnoise = a[idx]\n<\/code>\n\n\n\n tux:\n\nidx = np.random.choice(2, size=your_shape, p=[r, 1-r])\n\n\nThank you very much for your reply! Based on your reply,I think this may be the right  answer to my question.\n<code class=\"lang-auto\">s = x.data.size()\nt = torch.from_numpy(numpy.random.choice([-1, 1], size=s, p=[0.5, 0.5])).type_as(x)\nx = x.data.mul(t)\n<\/code>"},{"x":"In my IR that came from the quantized resnet model from torchvision, I have one max pool op which operates on quantized tensors:\n<code class=\"lang-auto\">  %input.3 : QUInt8(1, 64, 56, 56) = aten::max_pool2d(%input.2, %1279, %1282, %1285, %1288, %1289), scope: __module.maxpool # \/home\/masa\/anaconda3\/lib\/python3.7\/site-packages\/torch\/nn\/functional.py:488:0\n<\/code>\nDoes this dispatch into quantized::max_pool2d below?\n\n\ngithub.com with link \"https:\/\/github.com\/pytorch\/pytorch\/blob\/master\/aten\/src\/ATen\/native\/quantized\/cpu\/qpool.cpp#L414\"\n\n\npytorch\/pytorch\/blob\/master\/aten\/src\/ATen\/native\/quantized\/cpu\/qpool.cpp#L414 with link \"https:\/\/github.com\/pytorch\/pytorch\/blob\/master\/aten\/src\/ATen\/native\/quantized\/cpu\/qpool.cpp#L414\"\n<code class=\"lang-cpp\">\n    #ifdef USE_PYTORCH_QNNPACK\n    if (at::globalContext().qEngine() == at::QEngine::QNNPACK &amp;&amp; qx.scalar_type() == kQUInt8) {\n      return qnnpack_maxpool(qx, kernel_size, stride, padding, dilation, ceil_mode);\n    }\n    #endif\n    return at::max_pool2d(qx, kernel_size, stride, padding, dilation, ceil_mode);\n  }\n};\n\n\nstatic auto registry = torch::RegisterOperators().op(\n    \"quantized::max_pool2d(Tensor qx, \"\n    \"int[] kernel_size, \"\n    \"int[] stride, \"\n    \"int[] padding, \"\n    \"int[] dilation,\"\n    \"bool ceil_mode) -> Tensor\",\n    torch::RegisterOperators::options().kernel<QMaxPool2D_arr_args>(\n        DispatchKey::QuantizedCPUTensorId));\n\n\n} \/\/ namespace\n} \/\/ namespace native\n<\/code>\n\n\n\n\n\n\nOr more generally, I want to know about aten dispatch mechanism. Any pointer is appreciated.\ncc  ","y":"Yes, that is correct. it is dispatch here: https:\/\/github.com\/pytorch\/pytorch\/blob\/master\/aten\/src\/ATen\/native\/Pooling.cpp#L128\nWe have multiple ways to do dispatch right now in PyTorch, one common place is in native_functions.yaml, you can take a look at: https:\/\/github.com\/pytorch\/pytorch\/blob\/master\/aten\/src\/ATen\/native\/README.md","z":"Yes, that is correct. it is dispatch here: https:\/\/github.com\/pytorch\/pytorch\/blob\/master\/aten\/src\/ATen\/native\/Pooling.cpp#L128\nWe have multiple ways to do dispatch right now in PyTorch, one common place is in native_functions.yaml, you can take a look at: https:\/\/github.com\/pytorch\/pytorch\/blob\/master\/aten\/src\/ATen\/native\/README.md"},{"x":"I tries to set relu in qconfig in prepare method of quantisation and came across this errors during .convert\n<code class=\"lang-auto\">args=args=[criterion,evaluation_loader, converter, opt, True]\n            self.validation(fuse_m,args)\n            \n            # Convert to Quantize Model\n            quantise_model=torch.quantization.convert(fuse_m, inplace=True)\n<\/code>\n<code class=\"lang-auto\">Traceback (most recent call last):\n  File \"test_5step_quant.py\", line 474, in <module>\n    m.model(\"\/\/media\/ai\/OCR_DATA\/IIIT5k\",15)\n  File \"test_5step_quant.py\", line 448, in model\n    self.test(opt)\n  File \"test_5step_quant.py\", line 315, in test\n    quantise_model=torch.quantization.convert(fuse_m, inplace=False)\n  File \"\/home\/remote\/.local\/lib\/python3.6\/site-packages\/torch\/quantization\/quantize.py\", line 293, in convert\n    convert(mod, mapping, inplace=True)\n  File \"\/home\/remote\/.local\/lib\/python3.6\/site-packages\/torch\/quantization\/quantize.py\", line 293, in convert\n    convert(mod, mapping, inplace=True)\n  File \"\/home\/remote\/.local\/lib\/python3.6\/site-packages\/torch\/quantization\/quantize.py\", line 294, in convert\n    reassign[name] = swap_module(mod, mapping)\n  File \"\/home\/remote\/.local\/lib\/python3.6\/site-packages\/torch\/quantization\/quantize.py\", line 316, in swap_module\n    new_mod = mapping[type(mod)].from_float(mod)\n  File \"\/home\/remote\/.local\/lib\/python3.6\/site-packages\/torch\/nn\/quantized\/modules\/activation.py\", line 44, in from_float\n    return ReLU(mod.inplace)\n  File \"\/home\/remote\/.local\/lib\/python3.6\/site-packages\/torch\/nn\/quantized\/modules\/activation.py\", line 34, in __init__\n    assert not inplace, 'torch.nn.quantized.ReLU does not support inplace'\nAssertionError: torch.nn.quantized.ReLU does not support inplace\n\n<\/code>","y":"should be fixed in https:\/\/github.com\/pytorch\/pytorch\/pull\/33105, cc ","z":"should be fixed in https:\/\/github.com\/pytorch\/pytorch\/pull\/33105, cc "},{"x":"Hi,\nI successfully traced the model, but looks like I cannot save it.\nI thought that if I could do something like  model = trace(model,  (params) ) then it is ready so saving? Am I wrong?\nTorch version is \u20181.0.0.dev20190130\u2019\nHere is the trace:\n---------------------------------------------------------------------------\nRuntimeError                              Traceback (most recent call last)\n<ipython-input-23-4014fec4f00b> in <module>\n----> 1 ctc_model.save(\"ctc_test.ph\")\n\nRuntimeError: \ncould not export python function call <python_value>. Remove calls to python functions before     export.:\n.jit.script_method\ndef forward(self, x, x_length):\n    h_t, x_length = self.rnn(x, x_length)\n                    ~~~~~~~~ <--- HERE","y":"Try putting \"rnns\" in the __constants__ attribute. We should work on having a better error msg here","z":"You cannot export a model if in contains a Python function call. Is self.rnn() a Python function?\nI think if self.rnn() is subclassed from torch.jit.ScriptModule(), then it should be possible to trace &amp; save it.\nCould you provide a script\/model we can use the reproduce the problem? It\u2019s hard to say what\u2019s going on here without more information. Thanks!\nThank you very much for your replies!\nActually I think I fixed my original question: self.rnn was a nn.Module and now I also made in ScriptModule, but now I have a new problem. Looks like I cannot loop over nn.ModuleList. I tried to index it in the loop but did not work as well. Is it even possible to use jit for nn.ModuleList?\nI omitted some parts for brevity:\nclass PyramidalRNNENcoder(ScriptModule):\n\n__constants__ = ['num_layers']\n\ndef __init__(self, num_mels, encoder_size, num_layers, downsampling=None, dropout=0.0):\n    super(PyramidalRNNENcoder, self).__init__()\n    ...\n    self.rnns =nn.ModuleList()\n    for i in range(num_layers):\n        input_size = num_mels*2 if i == 0 else encoder_size*2\n        lstm_i = nn.LSTM(input_size,\n                                 hidden_size=encoder_size, bidirectional=True)\n        initialize_lstm(lstm_i)           \n        self.rnns.append(lstm_i)\n    self.num_layers = num_layers\n    ...\n    \n.jit.script_method\ndef forward(self, x, x_length):\n    batch_size = x.size(0)\n    ...\n    idx = 0\n    \n    for rnn in self.rnns:\n      ~~~~~~~~~~~~~~~~~~  RuntimeError:  python value of type 'ModuleList' cannot be used as a tuple:\n      rnn_result = rnn(data)\nTry putting \"rnns\" in the __constants__ attribute. We should work on having a better error msg here"},{"x":"Hi I am trying to write a Classification Bert which is trained on multiple datasets.\nBecause the number of classes of different datasets are different, during training I need to use different final linear layer for different datasets. Besides I don\u2019t want to overwrite the previous final linear layers because I want to test the performance drop after fine-tuning on other datasets.\nCurrently I can\u2019t make it work.\nHere is the implementation of my Classification Bert.\n<code class=\"lang-auto\">class ClassificationBert(nn.Module):\n    def __init__(self):\n        super().__init__()\n        self.bert = BertModel.from_pretrained('bert-base-uncased')\n        self.linear = nn.Sequential(nn.Linear(768, 128),\n                                    nn.Tanh())\n        self.classifier = None\n        self.datasets = []\n        self.classifiers = nn.ModuleList()\n    \n    def forward(self, x):\n        all_hidden, _ = self.bert(x)\n        pooled_output = torch.mean(all_hidden, 1)\n        features = self.linear(pooled_output)\n        predict = self.classifier(features)\n        return predict\n    \n    def add_dataset(self, dataset, num_outputs):\n        if dataset not in self.datasets:\n            self.datasets.append(dataset)\n            self.classifiers.append(nn.Linear(128, num_outputs))\n    \n    def set_dataset(self, dataset):\n        assert dataset in self.datasets\n        self.classifier = self.classifiers[self.datasets.index(dataset)]\n<\/code>\nAnd below is where I used add_dataset and set_dataset.\n<code class=\"lang-auto\">\n    datasets = ['ag_news_csv', 'yahoo_answers_csv']\n    model = ClassificationBert()\n\n    tasks = []\n\n    for dataset in datasets:\n        num_outputs = 0\n        if dataset == 'ag_news_csv':\n            num_outputs = 4\n        elif dataset == 'yahoo_answers_csv':\n            num_outputs = 10\n        else:\n            num_outputs = 14\n        model.add_dataset(dataset, num_outputs)\n        model.set_dataset(dataset)\n        model = model.cuda()\n        dataset_path = os.path.join(args.data_path, dataset) + '\/'\n        train(dataset_path, model)\n\n        tasks.append(dataset)\n        for task in tasks:\n            model.set_dataset(task)\n            dataset_path = os.path.join(args.data_path, dataset) + '\/'\n            train_labeled_set, val_set, test_set, n_labels = get_data(\n                    dataset_path, args.n_labeled)\n            test_loader = Data.DataLoader(\n                    dataset=test_set, batch_size=512, shuffle=False)\n            criterion = nn.CrossEntropyLoss()\n            test_loss, test_acc = validate(\n                    test_loader, model, criterion, 'Test Stats')\n            print(\"Task : {}, acc : {}\".format(\n                    dataset, test_acc))\n<\/code>\nYou can find I use two datasets and after fine-tuning on each dataset, I want to test the performance drop of every dataset that this model has been trained on before.\nBut I ran into an error:\n<code class=\"lang-auto\">Traceback (most recent call last):\n  File \"drive\/My Drive\/MixText\/code1\/train.py\", line 190, in <module>\n    main()\n  File \"drive\/My Drive\/MixText\/code1\/train.py\", line 98, in main\n    test_loader, model, criterion, 'Test Stats')\n  File \"drive\/My Drive\/MixText\/code1\/train.py\", line 119, in validate\n    correct += (np.array(predicted.cpu()) ==\nRuntimeError: CUDA error: device-side assert triggered\n<\/code>\nThe corresponding code is below:\n<code class=\"lang-auto\">def validate(valloader, model, criterion, mode):\n    model.eval()\n    with torch.no_grad():\n        loss_total = 0\n        total_sample = 0\n        acc_total = 0\n        correct = 0\n\n        for batch_idx, (inputs, targets, length) in enumerate(valloader):\n            inputs, targets = inputs.cuda(), targets.cuda(non_blocking=True)\n            outputs = model(inputs)\n            loss = criterion(outputs, targets)\n\n            _, predicted = torch.max(outputs.data, 1)\n            print(predicted.shape)\n            print(targets.shape)\n            correct += (np.array(predicted.cpu()) ==\n                        np.array(targets.cpu())).sum()\n            loss_total += loss.item() * inputs.shape[0]\n            total_sample += inputs.shape[0]\n\n        acc_total = correct\/total_sample\n        loss_total = loss_total\/total_sample\n\n    return loss_total, acc_total\n<\/code>\nI checked the shape of two tensors and didn\u2019t find mismatch.\nWould someone love to help me figure out the mistake or what\u2019s the correct way to dynamically replace the last linear layer?","y":"Sorry for not really answering your question, but you might want to test the training on the CPU first. Here the error messages are most of the time more useful than CUDA errors.\nApart form that, you don\u2019t really replace the last linear layer. You simple have multiple linear layers and choose one dynamically, which is essentially the idea behind multitask learning. And from a quick look at your code, it seems alright. But I didn\u2019t check any details.\nWhat\u2019s the error when running in the CPU?","z":"Sorry for not really answering your question, but you might want to test the training on the CPU first. Here the error messages are most of the time more useful than CUDA errors.\nApart form that, you don\u2019t really replace the last linear layer. You simple have multiple linear layers and choose one dynamically, which is essentially the idea behind multitask learning. And from a quick look at your code, it seems alright. But I didn\u2019t check any details.\nWhat\u2019s the error when running in the CPU?\nChris is right, CUDA side errors are not usually useful. You can do the following to see the full stack trace on GPU\n<code class=\"lang-python\">import os\nos.environ['CUDA_LAUNCH_BLOCKING'] = \"1\"\n<\/code>\nAlso, a simple solution for an automated last layer would be to pass the target column of your csv to the model when you initialize it. Then use the number of unique values in that to set the last linear layer.\nFor example\n<code class=\"lang-python\">class ClassificationBert(nn.Module):\n    def __init__(self, target_col):\n        super().__init__()\n        self.bert = BertModel.from_pretrained('bert-base-uncased')\n        self.num_classes = target_col.nunique() # considering its a pandas series\n<\/code>\nand then using the num_classes for your linear layer."},{"x":"I have a binary dependent variable and I am unclear as to:\n\nget BCEWithLogitsLoss to work\nincorporate pos_weight (how exactly do I calculate the weights, is it of the total data set?) One class has 7000 observations and the other has 2224 in the total data set. Should it just be a tensor thats like: torch.tensor([0.3, 0.7]) (more emphasis on the pos samples that are under-represented)?\n\nFor my loss function, CrossEntropyLoss is working, but I believe BCEWithLogitsLoss should be used instead (I think?).\nMy model outputs logits that look like:\n<code class=\"lang-auto\">tensor([[ 0.5015, -0.0165],\n        [ 0.5486,  0.0320],\n        [ 0.4227,  0.1604],\n        [ 0.2781, -0.0317],\n        [ 0.2667,  0.2109],\n        [ 0.1847, -0.1724],\n        [ 0.2727, -0.0598],\n        [ 0.3827,  0.1195],\n        [ 0.2796, -0.2183],\n        [ 0.6082, -0.1816],\n        [ 0.4710, -0.0551],\n        [ 0.1589,  0.0477],\n<\/code>\nMy labels look like:\n<code class=\"lang-auto\">tensor([[0],\n        [0],\n        [0],\n        [0],\n        [0],\n        [0],\n        [0],\n        [0],\n        [1],\n        [0],\n        [0],\n        [1],\n        [0],\n        [0],\n<\/code>\nThe error I get with BCEWithLogitsLoss is always; bool value of Tensor with more than one value is ambiguous.\nI believe I got the weights to work appropriately:\n<code class=\"lang-auto\"># helper function to count target distribution inside tensor data sets\ndef target_count(tensor_dataset):\n    count0 = 0\n    count1 = 0\n    total = []\n    for i in tensor_dataset:\n        if i[1].item() == 0:\n            count0 += 1\n        elif i[1].item() == 1:\n            count1 += 1\n    total.append(count0)\n    total.append(count1)\n    return torch.tensor(total)\n\n\n# prepare weighted sampling for imbalanced classification\ndef create_sampler(target_tensor, tensor_dataset):\n    class_sample_count = target_count(tensor_dataset)\n\n    weight = 1. \/ class_sample_count.float()\n    samples_weight = torch.tensor([weight[t[1]] for t in tensor_dataset])\n    sampler = torch.utils.data.WeightedRandomSampler(weights=samples_weight,\n                                                     num_samples=len(samples_weight),\n                                                     replacement=True)\n    return sampler\n\n\ntrain_sampler = create_sampler(target_count(train_dataset), train_dataset)\nval_sampler = create_sampler(target_count(val_dataset), val_dataset)\ntest_sampler = create_sampler(target_count(test_dataset), test_dataset)\n<\/code>","y":"\n\n\n localh:\n\nI am curious as to why, for binary classification, the model outputs 2 columns of logits that look like the above instead of 1 column.\n\n\nYou could deal with a binary classification use case in different ways:\n\nYou could use a single output and treat the output as the logit (or probability) representing the nagative and positive class. For this use case you would use nn.BCEWithLogitsLoss (or nn.BCELoss, if you are applying a sigmoid at the end. Note that logits + nn.BCEWithLogitsLoss give you more numerical stability)\nAlternatively you could treat the binary classification as a 2-class multi-class classification. Foir this approach you would use two output units and either use logits + nn.CrossEntropyLoss or F.log_Softmax + nn.NLLLoss\n\n","z":"Which line of code is raising this error?\nIs it some code in your posted code snippets or is it raised by BCEWithLogitsLoss during the training?\nThanks for the response!\nSo I think I figured it out, and it seems like it was a silly issue. I am very new to PyTorch and I have been jumping into new transformer models while also playing with \u201cold\u201d CNNs. The transformer BERT model, which I had an extensive guide for, worked fine and outputted logits that looked like the following for a 2 label classification task.\n<code class=\"lang-auto\">tensor([[ 0.5015, -0.0165],\n        [ 0.5486,  0.0320],\n        [ 0.4227,  0.1604],\n        [ 0.2781, -0.0317],\n        [ 0.2667,  0.2109],\n        [ 0.1847, -0.1724],\n        [ 0.2727, -0.0598],\n        [ 0.3827,  0.1195],\n        [ 0.2796, -0.2183],\n        [ 0.6082, -0.1816],\n        [ 0.4710, -0.0551],\n        [ 0.1589,  0.0477],\n<\/code>\nFor my CNN, I mistakenly made my output to be 2 (to make the output look like the above working transformer), instead of 1; which I thought was just a difference in PyTorch from Keras, as the last layer in Keras for a 2 label classification problem looks like keras.layers.Dense(1, activation=\"sigmoid\").\nWith my number of classes configuration set to 1, instead of 2 now, BCEWithLogitsLoss seems to work as intended via:\ncriterion = nn.BCEWithLogitsLoss()\n<code class=\"lang-auto\">        # `batch` contains two pytorch tensors:\n        #   [0]: input ids\n        #   [1]: labels\n        b_input_ids = batch[0].cuda()\n        b_labels = batch[1].cuda().type(torch.cuda.FloatTensor)\n\n        # clear previously calculated gradients\n        model.zero_grad()\n\n        # forward propagation (evaluate model on training batch)\n        logits = model(b_input_ids)\n\n        # calculate cross entropy loss\n        loss = criterion(logits, b_labels)\n<\/code>\nWhile this is not the place for huggingface\/transformers discussion, I am curious as to why, for binary classification, the model outputs 2 columns of logits that look like the above instead of 1 column. I also realize that the way the transformers package has setup its API (https:\/\/huggingface.co\/transformers\/model_doc\/bert.html) makes it so that if:\nnum_labels = 1; its a regression\nnum_labels >= 2; its classification\nand thus may be why.\nThanks for your time!\n\n\n\n localh:\n\nI am curious as to why, for binary classification, the model outputs 2 columns of logits that look like the above instead of 1 column.\n\n\nYou could deal with a binary classification use case in different ways:\n\nYou could use a single output and treat the output as the logit (or probability) representing the nagative and positive class. For this use case you would use nn.BCEWithLogitsLoss (or nn.BCELoss, if you are applying a sigmoid at the end. Note that logits + nn.BCEWithLogitsLoss give you more numerical stability)\nAlternatively you could treat the binary classification as a 2-class multi-class classification. Foir this approach you would use two output units and either use logits + nn.CrossEntropyLoss or F.log_Softmax + nn.NLLLoss\n\n\nThanks so much for the clear and detailed explanation. This makes a lot of sense!"},{"x":"I\u2019m trying to implement a skip-gram model. When I pass the input vector to the hidden layer, I get the error:\nRuntimeError: Expected object of scalar type Long but got scalar type Float for argument #2 \u2018mat2\u2019\n<code class=\"lang-auto\">ONE_HOT_SIZE  = len(dictionary)\nEMBEDDING_DIM = 200\nEPOCH_NUM     = 5\n\ndef one_hot(index):\n    one_hot_array = torch.zeros(ONE_HOT_SIZE, dtype=torch.long)\n    one_hot_array[index] = 1.\n    return one_hot_array\n\nclass SkipGramModel(nn.Module):\n    def __init__(self, one_hot_size, embedding_dim):\n        super(SkipGramModel, self).__init__()\n        self.hidden = nn.Linear(one_hot_size, embedding_dim)\n        self.linear_out = nn.Linear(embedding_dim, one_hot_size)\n    def forward(self, x):\n        embed = self.hidden(x)\n        out   = self.linear_out(embed)\n        out   = F.log_softmax(out, dim=0)\n        return out\n\nmodel         = SkipGramModel(ONE_HOT_SIZE, EMBEDDING_DIM)\nloss_function = nn.NLLLoss()\noptimizer     = optim.SGD(model.parameters(), lr=0.01)\n\nlosses = []\ntotal_count = EPOCH_NUM * len(targets)\n\nwith tqdm(total = total_count) as pbar:\n    for epoch in range(EPOCH_NUM):\n        total_loss = 0\n        for target in targets:\n            # Convert input word and target to one-hot vectors\n            word   = one_hot(target[0])\n            target = one_hot(target[1])\n\n            # Reset gradient\n            model.zero_grad()\n\n            # Get the output from the model\n            prediction = model(word)\n            \n            # Calculate loss\n            loss = loss_function(prediction, target)\n\n            # Optimize\n            loss.backward()\n            optimizer.step()\n\n            # Add loss to total loss\n            total_loss += loss\n\n            # Update progress bar\n            pbar.update(1)\n        losses.append(total_loss)\n<\/code>\nError message:\n<code class=\"lang-auto\">---------------------------------------------------------------------------\nRuntimeError                              Traceback (most recent call last)\n<ipython-input-39-884ed7fa9d4b> in <module>\n     11 \n     12             # Get the output from the model\n---> 13             prediction = model(word)\n     14 \n     15             # Calculate loss\n\nc:\\users\\archan\\appdata\\local\\programs\\python\\python36\\lib\\site-packages\\torch\\nn\\modules\\module.py in __call__(self, *input, **kwargs)\n    491             result = self._slow_forward(*input, **kwargs)\n    492         else:\n--> 493             result = self.forward(*input, **kwargs)\n    494         for hook in self._forward_hooks.values():\n    495             hook_result = hook(self, input, result)\n\n<ipython-input-36-238e5eb99042> in forward(self, x)\n      5         self.linear_out = nn.Linear(embedding_dim, one_hot_size)\n      6     def forward(self, x):\n----> 7         embed = self.hidden(x)\n      8         out   = self.linear_out(embed)\n      9         out   = F.log_softmax(out, dim=0)\n\nc:\\users\\archan\\appdata\\local\\programs\\python\\python36\\lib\\site-packages\\torch\\nn\\modules\\module.py in __call__(self, *input, **kwargs)\n    491             result = self._slow_forward(*input, **kwargs)\n    492         else:\n--> 493             result = self.forward(*input, **kwargs)\n    494         for hook in self._forward_hooks.values():\n    495             hook_result = hook(self, input, result)\n\nc:\\users\\archan\\appdata\\local\\programs\\python\\python36\\lib\\site-packages\\torch\\nn\\modules\\linear.py in forward(self, input)\n     90     \n     91     def forward(self, input):\n---> 92         return F.linear(input, self.weight, self.bias)\n     93 \n     94     def extra_repr(self):\n\nc:\\users\\archan\\appdata\\local\\programs\\python\\python36\\lib\\site-packages\\torch\\nn\\functional.py in linear(input, weight, bias)\n   1406         ret = torch.addmm(bias, input, weight.t())\n   1407     else:\n-> 1408         output = input.matmul(weight.t())\n   1409         if bias is not None:\n   1410             output += bias\n\nRuntimeError: Expected object of scalar type Long but got scalar type Float for argument #2 'mat2'\n<\/code>","y":"Could you print the shape or words?\nnn.Linear expects the input as a tensor of shape [batch_size, *, in_feautres].\nAlso, nn.NLLLoss expects a LongTensor as the target containing the class indices. It should therefore not be one-hot encoded.","z":"word is probably a LongTensor, which will yield this error.\nYou can transform it to a FloatTensor by calling .float() on it:\n<code class=\"lang-python\">word = word.float()\nprediction = model(word)\n<\/code>\nNot relevant for this error, but which loss function are you using?\nI\u2019m using the NLLLoss function. After making the change you suggested, I get the error \u201cExpected two dimensions, got one\u201d\nCould you print the shape or words?\nnn.Linear expects the input as a tensor of shape [batch_size, *, in_feautres].\nAlso, nn.NLLLoss expects a LongTensor as the target containing the class indices. It should therefore not be one-hot encoded.\nI messed up by feeding the training set one by one. I\u2019ll feed batches instead. Thanks for the help!\nAnd why shouldn\u2019t the vector consist of one-hots?\nShouldn\u2019t each class be true or false instead?\nNo, as given in the docs with link \"https:\/\/pytorch.org\/docs\/stable\/nn.html#torch.nn.NLLLoss\", the loss expects class indices instead:\n\nThe target that this loss expects should be a class index in the range [0,C\u22121] where C = number of classes; if ignore_index is specified, this loss also accepts this class index (this index may not necessarily be in the class range).\n\nYou can see the applied formula in the CrossEntropyLoss docs with link \"https:\/\/pytorch.org\/docs\/stable\/nn.html#torch.nn.CrossEntropyLoss\". Note that nn.CrossEntropyLoss will internally use log_softmax + NLLLoss.\nI read it. I\u2019ll pass the target without encoding it. Thanks a bunch!\nI thought the error stated that it was expecting a LongTensor instead of a FloatTensor. I put .float() to the corresponding place and the error disappeared.\nWhy is this error message indicating it is actually expecting a FlaotTensor?\nThanks!\nThe RuntimeError claims that the first argument to the operation is the \u201cright\u201d one and the second argument has the wrong data type.\nIn this case the second argument is the weight as seen in the stack trace, which explains the error message."},{"x":"I am trying to change the value in my model\u2019s state dict, but even after updating the state dict, the value does not change, any help would be appreciated.\n<code class=\"lang-auto\">sd = model.state_dict() \nsd['encoder.layer.11.output.LayerNorm._running_mean'] = layer_norm_stats['encoder.layer.11.output.LayerNorm._running_mean']  # Layer norm stats is a dict containing mean of the layer norm \nprint(model.state_dict()['encoder.layer.11.output.LayerNorm._running_mean'])  # prints all zero tensor, which is the original value\nprint(layer_norm_stats['encoder.layer.11.output.LayerNorm._running_mean'] )  # prints the running mean, which is not the zero vector\n<\/code>","y":"The values in the model parameters won\u2019t be changed, if you assign a new tensor to the key in the state_dict.\nYou could either load the manipulated state_dict afterwards or change the parameter\u2019s value inplace as shown here:\n<code class=\"lang-python\">model = nn.Linear(1, 1)\nprint(model.weight)\n> Parameter containing:\ntensor([[0.8777]], requires_grad=True)\n\nsd = model.state_dict()\nsd['weight'] = torch.tensor([[1.]])\nprint(model.weight)\n> Parameter containing:\ntensor([[0.8777]], requires_grad=True)\n\nmodel.load_state_dict(sd)\nprint(model.weight)\n> Parameter containing:\ntensor([[1.]], requires_grad=True)\n\n\n# or\nmodel = nn.Linear(1, 1)\nprint(model.weight)\n> Parameter containing:\ntensor([[-0.8112]], requires_grad=True)\n\nwith torch.no_grad():\n    sd = model.state_dict()\n    sd['weight'].fill_(1.)\nprint(model.weight)\n> Parameter containing:\ntensor([[1.]], requires_grad=True)\n<\/code>","z":"Shouldn\u2019t you look at\nsd['encoder.layer.11.output.LayerNorm._running_mean']  in the second print statement if it has changed ?\nThanks for your reply, that is my question, should not changing a copy of the state dict also directly change the model values\nThe values in the model parameters won\u2019t be changed, if you assign a new tensor to the key in the state_dict.\nYou could either load the manipulated state_dict afterwards or change the parameter\u2019s value inplace as shown here:\n<code class=\"lang-python\">model = nn.Linear(1, 1)\nprint(model.weight)\n> Parameter containing:\ntensor([[0.8777]], requires_grad=True)\n\nsd = model.state_dict()\nsd['weight'] = torch.tensor([[1.]])\nprint(model.weight)\n> Parameter containing:\ntensor([[0.8777]], requires_grad=True)\n\nmodel.load_state_dict(sd)\nprint(model.weight)\n> Parameter containing:\ntensor([[1.]], requires_grad=True)\n\n\n# or\nmodel = nn.Linear(1, 1)\nprint(model.weight)\n> Parameter containing:\ntensor([[-0.8112]], requires_grad=True)\n\nwith torch.no_grad():\n    sd = model.state_dict()\n    sd['weight'].fill_(1.)\nprint(model.weight)\n> Parameter containing:\ntensor([[1.]], requires_grad=True)\n<\/code>\nThank you that was helpful!"},{"x":"I\u2019m learning about RNNs from the Udacity PyTorch class, and I think I understand the basic concept. Now I\u2019m trying to get myself familiar with the details, starting with understanding the dimensions of inputs and outputs of different layers in an RNN.\n\n\ninput_size  \u2013  The number of expected features in the input x.\n\nhidden_size  \u2013 The number of features in the hidden state h.\n\nnum_layers  \u2013 Number of recurrent layers.\n\nI copied these parameter descriptions from the docs, but I\u2019m having a hard time visualizing my network based on these parameters.\n\nWhat do the docs mean by \u201cfeatures in the input x\u201d? Does it refer to having multiple variables? Or like an image might have multiple channels?\nWhat are \u201cfeatures in the hidden state\u201d? Again, the word \u201cfeatures\u201d seems pretty ambiguous, and I don\u2019t know what it means at all.\n\nI get that the hidden state has some \u201cmemory\u201d of its previous self and is updated by combining the current input to the hidden layer with the previous hidden state. I don\u2019t get the specifics of what\u2019s going on though. For example, here is a question from the class, which I have no idea how to answer:\nSay you\u2019ve defined a GRU layer with  input_size = 100 ,  hidden_size = 20 , and  num_layers=1 . What will the dimensions of the hidden state be if you\u2019re passing in data, batch first, in batches of 3 sequences at a time?\nUnfortunately, RNNs are not explained as well as CNNs were in the previous lesson. Thanks for your help!","y":"Since you posted in nlp, I assume you work with text.\nA very common application is sentence classification (e.g., for sentence classification), where each sentence is a sequence of words. Let\u2019s say you have a batch 3 sentences, each containing 10 words (nn.LSTM and nn.GRU require by default sequences of the same length; you can look up padding and packing with link \"https:\/\/discuss.pytorch.org\/t\/tensorflow-esque-bucket-by-sequence-length\/41284\/15\")\nThat means your batch has the shape (batch_size, seq_len), i.e., (3, 10) with the numbers above. Not that each sentence\/sequence is a vector of integers reflecting the index of a word in your vocabulary.\nThe next step is to push the batch through a nn.Embedding layer to map words (represented by ther indices) to word vectors of size, say, 100. The output shape after the embedding layer is then (batch_size, seq_len, embed_dim), i.e., (3, 10, 100) with the numbers above.\nThis tensor can now serve as input for your nn.LSTM or nn.GRU which expect as input (batch_size, seq_len, input_size) \u2013 not that by default, they actually expect (seq_len, batch_size, input_size); so either you transform() for tensor or you define your RNN layer with batch_first=True.\nAnyway, embed_dim, i.e., the size of your word vectors defines input_size, 100 in the example above. Summing up\n\n\nbatch_size is the number of sentences in your batch (e.g., 3)\n\nseq_len is the number of items in your sequences such as words in a sentence (e.g., 10)\n\ninput_size is the size of the tensor\/vector that represents a single(!) item in your sequence such as 100-dim word vectors for each word in a sentence.\n\nThe shape of inputs and outputs are very well defined; see, for example, for nn.LSTM with link \"https:\/\/pytorch.org\/docs\/master\/generated\/torch.nn.LSTM.html\".","z":"Very crudely speaking, number of features refer to the size of the vectors\/tensors.\nOutside end-to-end neural networks, the term feature had a more tangible meaning, since feature engineering was an import processing step you had to do \u201cmanually\u201d. For example, to use a SVM to classify a text document had extract the text document into a set of features. This could some very naive features such as #words, #characters. In this case, each document would be represented by 2 numerical features. In practice, you would have more meaningful features but they would have a clear semantic meaning.\nWith end-to-end neural networks, this semantic meaning is usually latent and not obvious. For example, each word in a text may be represented by a 300-dim vector of numerical values, i.e., the word has 300 features. This vector places a word in a 300-dim space in relation to other words. However, you usually don\u2019t really know what an individual value in the vector means. For example, The 25th entry in the 300-dim vector does not tell you that the word is a noun, verb, adjective, etc.\nSo in case you use an LSTM for text processing where the LSTM processes a sequences of words, each words is represented by a vector of size input_size. This representation is needed since word are symbolic representations. For example, the words \u201ccat\u201d and \u201ckitten\u201d are only similar to you because your mental models of both concepts (animal, 4 legs, furry, meows, etc.) are similar. For a computer, these are completely different things. A vector representation now allows to map \u201ccat\u201d and \u201ckitten\u201d (and all other word) to a numerical representation where the vector for \u201ccat\u201d and the vector for \u201ckitten\u201d are closer together compared to, say, the vector of \u201ccat\u201d and the vector of \u201ctrain\u201d.\nIn contrast, if you use an LSTM for time series prediction of already scalar numerical values, input_size is just 1.\nFor the resulting dimensions of the hidden state, you best consult the PyTorch docs, but it definitely does not depend on the input_size.\nI\u2019m afraid I don\u2019t quite understand your explanation. I\u2019m sorry!\nI\u2019ve scoured the PyTorch RNN docs, and there is little to no explanation of how data should be formatted for input, and certainly no information about how the shape of the data is changing as it moves through the network.\nFor example, I THINK my forward pass I need to have my data in the shape (batch_size, seq_length, input_size), but there are no concrete examples of what seq_length is or what input_size is. How are they different?\nSince you posted in nlp, I assume you work with text.\nA very common application is sentence classification (e.g., for sentence classification), where each sentence is a sequence of words. Let\u2019s say you have a batch 3 sentences, each containing 10 words (nn.LSTM and nn.GRU require by default sequences of the same length; you can look up padding and packing with link \"https:\/\/discuss.pytorch.org\/t\/tensorflow-esque-bucket-by-sequence-length\/41284\/15\")\nThat means your batch has the shape (batch_size, seq_len), i.e., (3, 10) with the numbers above. Not that each sentence\/sequence is a vector of integers reflecting the index of a word in your vocabulary.\nThe next step is to push the batch through a nn.Embedding layer to map words (represented by ther indices) to word vectors of size, say, 100. The output shape after the embedding layer is then (batch_size, seq_len, embed_dim), i.e., (3, 10, 100) with the numbers above.\nThis tensor can now serve as input for your nn.LSTM or nn.GRU which expect as input (batch_size, seq_len, input_size) \u2013 not that by default, they actually expect (seq_len, batch_size, input_size); so either you transform() for tensor or you define your RNN layer with batch_first=True.\nAnyway, embed_dim, i.e., the size of your word vectors defines input_size, 100 in the example above. Summing up\n\n\nbatch_size is the number of sentences in your batch (e.g., 3)\n\nseq_len is the number of items in your sequences such as words in a sentence (e.g., 10)\n\ninput_size is the size of the tensor\/vector that represents a single(!) item in your sequence such as 100-dim word vectors for each word in a sentence.\n\nThe shape of inputs and outputs are very well defined; see, for example, for nn.LSTM with link \"https:\/\/pytorch.org\/docs\/master\/generated\/torch.nn.LSTM.html\"."},{"x":"def train(net, data, epochs=10, n_seqs=10, n_steps=50, lr=0.001, clip=5, val_frac=0.1, device=torch.device(\u2018cpu\u2019),\nname=\u2018checkpoint\u2019, early_stop=True, plot=False):\n# initialize the process group\n\"\"\"\nTraining loop.\n\"\"\"\nnet.train() # switch into training mode\nopt = torch.optim.Adam(net.parameters(), lr=lr) # initialize optimizer\ncriterion = nn.CrossEntropyLoss() # initialize loss function\n\n# create training and validation data\nval_idx = int(len(data) * (1 - val_frac))\ndata, val_data = data[:val_idx], data[val_idx:]\n\n# net = DDP(net)\n# net = nn.DistributedDataParallel(net, device_ids=[0,1,2])\n# net = torch.nn.DataParallel(net, device_ids=[0,1,2])\n# net = torch.nn.DataParallel(net)\n\nnet.to(device) # move neural net to GPU\/CPU memory\n\nmin_val_loss = 10.**10 # initialize minimal validation loss\ntrain_history = {'epoch': [], 'step': [], 'loss': [], 'val_loss': []}\n\nn_chars = len(net.chars) # get size of vocabulary\n\n# main loop over training epochs\nfor e in range(epochs):\n    hidden = None # reste hidden state after each epoch\n    \n    # loop over batches\n    for x, y in get_batches(data, n_seqs, n_steps):\n\n        # encode data and create torch-tensors\n        x = one_hot_encode(x, n_chars)\n        inputs, targets = torch.from_numpy(x).to(device), torch.tensor(y, dtype=torch.long).to(device)\n        \n        # reset gradient information\n        net.module.zero_grad()\n        \n        # generate network output\n        output, hidden = net.forward(inputs, hidden)\n        \n        # compute loss\n        loss = criterion(output, (targets.view(n_seqs * n_steps)).type(torch.LongTensor))\n        \n        # compute gradients\n        loss.backward()\n\n        # gradient clipping to prevent exploding gradients\n        nn.utils.clip_grad_norm_(net.module.parameters(), clip)\n        \n        # optmize\n        opt.step()\n\n        # prevent backpropagating through the entire training history\n        # by detaching hidden state and cell state\n        hidden = (hidden[0].detach(), hidden[1].detach())\n    \n    # validation step is done without tracking gradients\n    with torch.no_grad():\n        val_h = None\n        val_losses = []\n        \n        for x, y in get_batches(val_data, n_seqs, n_steps):\n            x = one_hot_encode(x, n_chars)\n            inputs, targets = torch.from_numpy(x).to(device), torch.tensor(y, dtype=torch.long).to(device)\n\n            output, val_h = net.forward(inputs, val_h)\n            \n            val_loss = criterion(output, (targets.view(n_seqs*n_steps)).type(torch.LongTensor))\n            val_losses.append(val_loss.item())\n        \n        # compute mean validation loss over batches\n        mean_val_loss = np.mean(val_losses)\n        \n        # track progress\n        train_history['epoch'].append(e+1)\n        train_history['loss'].append(loss.item())\n        train_history['val_loss'].append(mean_val_loss)\n    \n    # print training progress\n    print(\"{}   Epoch: {:.0f}\/{:.0f}   Loss: {:.4f}   Val Loss: {:.4f}\".format(\n        datetime.now().strftime('%H:%M:%S'),\n        e+1, epochs,\n        loss.item(),\n        mean_val_loss))\n    \n    # save model checkpoint if validation loss has decreased\n    if mean_val_loss < min_val_loss:\n        save_checkpoint(net, opt, name+'.net', train_history=train_history)\n        min_val_loss = mean_val_loss\n    \n    # if validation loss has not decreased for the last 10 epochs, stop training\n    if early_stop:\n        if e - np.argmin(train_history['val_loss']) > 10:\n            # display.clear_output()\n            print('Validation loss does not decrease further, stopping training.')\n            break","y":"I guess get_batches might not return anything, so that the complete training and thus the loss calculation will be skipped and will later raise this error in:\n<code class=\"lang-python\">train_history['loss'].append(loss.item())\n<\/code>","z":"I guess get_batches might not return anything, so that the complete training and thus the loss calculation will be skipped and will later raise this error in:\n<code class=\"lang-python\">train_history['loss'].append(loss.item())\n<\/code>\nThank you! I solved the problem of batch_size so it did work."},{"x":"I have a huge list of binary strings: [010101, 011111111, 0101111100011\u2026]. The length of each string is different: from 1 to 10000 characters. The output of the Neural Network is simple: yes\/no.\nCan you please provide an example of how to define and train simple LSTM that works with this data? LSTM should read input character by character and at each step predict yes\/no.","y":"There are a couple of things to unpack:\n\n\nSequences of 10,000 are extremely long. Sure, LSTMs proud themselves to have long-term memory but 10k time steps seems awfully long. I\u2019m not saying that it won\u2019t work, it\u2019s just something too keep in mind.\n\n\nSince the range of you sequences varies a lot, have a look at BucketIterator with link \"https:\/\/torchtext.readthedocs.io\/en\/latest\/data.html#bucketiterator\" to generate batches where all sequences within one batch have the same or at least very similar size.\n\n\nSince batches can still have sequences of different lengths, you should have a look at pack_padded_sequences with link \"https:\/\/pytorch.org\/docs\/master\/generated\/torch.nn.utils.rnn.pack_padded_sequence.html#torch.nn.utils.rnn.pack_padded_sequence\".\n\n\nOnce the batches are right setting up the network model is relatively straightforward.\n\n","z":"There are a couple of things to unpack:\n\n\nSequences of 10,000 are extremely long. Sure, LSTMs proud themselves to have long-term memory but 10k time steps seems awfully long. I\u2019m not saying that it won\u2019t work, it\u2019s just something too keep in mind.\n\n\nSince the range of you sequences varies a lot, have a look at BucketIterator with link \"https:\/\/torchtext.readthedocs.io\/en\/latest\/data.html#bucketiterator\" to generate batches where all sequences within one batch have the same or at least very similar size.\n\n\nSince batches can still have sequences of different lengths, you should have a look at pack_padded_sequences with link \"https:\/\/pytorch.org\/docs\/master\/generated\/torch.nn.utils.rnn.pack_padded_sequence.html#torch.nn.utils.rnn.pack_padded_sequence\".\n\n\nOnce the batches are right setting up the network model is relatively straightforward.\n\n"},{"x":"I\u2019m trying to run the codes in (EXPERIMENTAL) STATIC QUANTIZATION WITH EAGER MODE IN PYTORCH with link \"https:\/\/pytorch.org\/tutorials\/advanced\/static_quantization_tutorial.html#experimental-static-quantization-with-eager-mode-in-pytorch\", but there are no dataset and model files available, such as imagenet_1k, mobilenet_quantization.pth and so on.\nSo anyone can provide the address of the necessary files and dataset in this tutorial?","y":"Thanks for your replay. I find the download urls are in the Makefile in github pytorch\/tutorial.","z":"Also, static quantification of google colab is not available\n can you take a look?\n Need your help.\nHi frank,  to run the tutorial, you will need to download imagenet dataset yourself, as we cannot upload imagenet images. The dataset can be obtained by using: https:\/\/pytorch.org\/docs\/stable\/_modules\/torchvision\/datasets\/imagenet.html#ImageNet\nFor the tutorial, you only need the floating point models as the tutorial walks through the process of creating a quantized model from a floating point model. The floating point mobilenet model can be found at: https:\/\/github.com\/pytorch\/vision\/blob\/master\/torchvision\/models\/mobilenet.py#L9.\nWe will add  more documentation to the tutorial so that these steps are clearer.\nThanks for your replay. I find the download urls are in the Makefile in github pytorch\/tutorial."},{"x":"<code class=\"lang-auto\">I tried to quantizate my own shuffle model through 'STATIC QUANTIZATION WITH EAGER MODE IN PYTORCH'.\nBut when it comes to forward propagation, the time to assess model losses, I failed\n<\/code>\nRuntimeError: No function is registered for schema aten::native_batch_norm(Tensor input, Tensor? weight, Tensor? bias, Tensor? running_mean, Tensor? running_var, bool training, float momentum, float eps) -> (Tensor, Tensor, Tensor) on tensor type QuantizedCPUTensorId; available functions are CPUTensorId, CUDATensorId, MkldnnCPUTensorId, VariableTensorId\nThe above operation failed in interpreter, with the following stack trace:\nat code\/ torch \/shuff_slim\/___torch_mangle_297.py:316:13","y":"Please fold batchnorms with preceding convs manually. Look for the APIs to do this for you in the Resnext-101 tutorial.","z":"Please fold batchnorms with preceding convs manually. Look for the APIs to do this for you in the Resnext-101 tutorial.\n could you share the link for the Resnext 101 tutorial?\nOr could you provide an example ? That would be very helpful\nYou can find tutorial for post training static quantization here: https:\/\/pytorch.org\/tutorials\/advanced\/static_quantization_tutorial.html"},{"x":"The weights obtained from PyTorch per tensor quantization of Conv2d can be used in Int8Conv from caffe2. But from Int8Conv definitions, I understood that it only accepts scale as a float and not an array.\nIs it possible to use the PerChannel quantization in Caffe2?","y":"Unfortunately, Caffe2 Int8Conv doesn\u2019t support per-channel quantization. The DNNLOWP engine that uses FBGEMM backend does support group-wise quantization if that helps you. Please see https:\/\/github.com\/pytorch\/pytorch\/blob\/master\/caffe2\/quantization\/server\/conv_groupwise_dnnlowp_op_test.py for example of using group-wise quantization.","z":"Unfortunately, Caffe2 Int8Conv doesn\u2019t support per-channel quantization. The DNNLOWP engine that uses FBGEMM backend does support group-wise quantization if that helps you. Please see https:\/\/github.com\/pytorch\/pytorch\/blob\/master\/caffe2\/quantization\/server\/conv_groupwise_dnnlowp_op_test.py for example of using group-wise quantization."},{"x":"Hi, I\u2019m trying to quantize mobilenet V3 but get stuck in quantizing Squeeze Block. When I train it in subset dataset and use convert to quantized model, it works. but when I evaluate quantized model in eval set, It throws an error of Mul operation. Can you tell me how can I implement Squeeze block (SElayer). thank you so much.\n<code class=\"lang-auto\">class SqueezeBlock(nn.Module):\n    def __init__(self, exp_size, divide=4):\n        super(SqueezeBlock, self).__init__()\n        self.avg_pool = nn.AdaptiveAvgPool2d(1)\n        self.dense = nn.Sequential(\n            nn.Linear(exp_size, exp_size \/\/ divide),\n            nn.ReLU(inplace=False),\n            nn.Linear(exp_size \/\/ divide, exp_size),\n            h_sigmoid()\n        )\n        self.mul = torch.nn.quantized.FloatFunctional()\n\n    def forward(self, x):\n        batch, channels, height, width = x.size()\n        out = self.avg_pool(x).view(batch, channels)\n        out = self.dense(out)\n        out = out.view(batch, channels, 1, 1)\n        return self.mul.mul(x,out)\n<\/code>\n<code class=\"lang-auto\">File \"\/home\/X\/Documents\/thancuong\/mb3_quantized\/quantized_mb3.py\", line 121, in forward\n    return self.mul.mul(x,out)\n  File \"\/home\/X\/.virtualenvs\/torch_0.4\/lib\/python3.5\/site-packages\/torch\/nn\/quantized\/modules\/functional_modules.py\", line 146, in mul\n    zero_point=self.zero_point)\nRuntimeError: Mul operands must be the same size! (check_inputs at \/pytorch\/aten\/src\/ATen\/native\/quantized\/cpu\/qmul.cpp:20)\nframe #0: c10::Error::Error(c10::SourceLocation, std::string const&amp;) + 0x33 (0x7f077b622813 in \/home\/X\/.virtualenvs\/torch_0.4\/lib\/python3.5\/site-packages\/torch\/lib\/libc10.so)\n<\/code>","y":"You can actually try to comment out the two lines as https:\/\/github.com\/pytorch\/pytorch\/pull\/30442, since the tensor iterator supports broadcast.","z":"You can actually try to comment out the two lines as https:\/\/github.com\/pytorch\/pytorch\/pull\/30442, since the tensor iterator supports broadcast.\nthank you, I will try it"},{"x":"I\u2019ve read the pytorch quantization document with link \"https:\/\/pytorch.org\/docs\/stable\/quantization.html\" , and I think it should quantize nn.Conv2d module as well as nn.Linear. But, when I try the dynamic quantization, it only converts the nn.Linear.\nI used the following simple dummy test:\n<code class=\"lang-auto\">class dumy_CNN(nn.Module):\n    def __init__(self, ni, no):\n        super().__init__()              \n        self.conv = nn.Conv2d(ni, no, 8, 2, 3)\n        self.lin = nn.Linear(1024, 4)\n    def forward(self, x):\n        out = self.lin(self.conv(x))\n        return out    \n\nmodel_test = dumy_CNN(2,10)\nmodel_qn = torch.quantization.quantize_dynamic(\n        model_test, {nn.Linear, nn.Conv2d} , dtype= torch.qint8\n        )\n<\/code>\nBut the model_qn looks like:\n<code class=\"lang-auto\">dumy_CNN(\n (conv): Conv2d(2, 10, kernel_size=(8, 8), stride=(2, 2), padding=(3, 3))\n  (lin): DynamicQuantizedLinear(in_features=1024, out_features=4, scale=1.0, zero_point=0)\n)\n<\/code>\nI also checked the weights to make sure about the above issue:\n<code class=\"lang-auto\">model_qn.conv.weight.data[0,0,0,0].item()\n0.02230245992541313\n<\/code>","y":"Hi , Dynamic quantization is currently supported only for nn.Linear and nn.LSTM, please see: https:\/\/pytorch.org\/docs\/stable\/quantization.html#torch.quantization.quantize_dynamic","z":"Hi , Dynamic quantization is currently supported only for nn.Linear and nn.LSTM, please see: https:\/\/pytorch.org\/docs\/stable\/quantization.html#torch.quantization.quantize_dynamic"},{"x":"Hi, I am trying to use torch. MultiheadAttention for the following use case:\nI have documents of |Q| queries, and sentences of length |K| (here, K==V). I would like for each Q to attend to all K, and ultimately, I will combine the |Q| context vectors. If I am batching these inputs, I understand that I can pass key_padding_mask=|B|x|K| where |B| is my batch size, to the model to mask the appropriate keys in each batch. I also understand that I can use attn_mask=|Q|x|K| to mask which k a given output (in my case I want |Q| outputs) attends to.\nHowever, because I have a variable length |Q|, how can I mask this batchwise? Can I simply pass attn_mask=|B|x|Q|x|K| and get the desired effect? Essentially, certain entire distributions over |K| are invalid because that q is masked. I guess I can ust 0 out those vectors after the computation? Is my understanding even correct, or is the implementation of MultiheadAttention not valid for my use-case?","y":"Note: this feature was merged  on Jan 23 - https:\/\/github.com\/pytorch\/pytorch\/pull\/31996 - and is not in the pip distribution of pytorch\nEDIT: Note that this is available in the most recent pip release, 1.5: https:\/\/github.com\/pytorch\/pytorch\/releases\n\n\ngithub.com\/pytorch\/pytorch with link \"https:\/\/github.com\/pytorch\/pytorch\/pull\/31996\"\n\n\n\n\n\n\n\n\nSupport 3D attention mask in MultiheadAttention. with link \"https:\/\/github.com\/pytorch\/pytorch\/pull\/31996\"\n\n\npytorch:master \u2190 pytorch:3d_attn_mask\n\n\n\n        opened 06:23PM - 09 Jan 20 UTC\n\n\nzhangguanheng66 with link \"https:\/\/github.com\/zhangguanheng66\"\n\n\n+57\n-22 with link \"https:\/\/github.com\/pytorch\/pytorch\/pull\/31996\/files\"\n\n\n\n\n\n\n\n\n\n","z":"It\u2019s actually pretty surprising to me that you cannot mask the query with this function. Am I not using it as intended?\nNevermind, this does exist, I was just getting an error because your first dimension for attn_mask needs to be batch_size * num_heads. I just had batch_size. This is explained in F.multi_head_attention_forward, but not in the MultiheadAttention forward method.\nNote: this feature was merged  on Jan 23 - https:\/\/github.com\/pytorch\/pytorch\/pull\/31996 - and is not in the pip distribution of pytorch\nEDIT: Note that this is available in the most recent pip release, 1.5: https:\/\/github.com\/pytorch\/pytorch\/releases\n\n\ngithub.com\/pytorch\/pytorch with link \"https:\/\/github.com\/pytorch\/pytorch\/pull\/31996\"\n\n\n\n\n\n\n\n\nSupport 3D attention mask in MultiheadAttention. with link \"https:\/\/github.com\/pytorch\/pytorch\/pull\/31996\"\n\n\npytorch:master \u2190 pytorch:3d_attn_mask\n\n\n\n        opened 06:23PM - 09 Jan 20 UTC\n\n\nzhangguanheng66 with link \"https:\/\/github.com\/zhangguanheng66\"\n\n\n+57\n-22 with link \"https:\/\/github.com\/pytorch\/pytorch\/pull\/31996\/files\"\n\n\n\n\n\n\n\n\n\n"},{"x":"Hi all, I am writing a simple neural network using LSTM to get some understanding of NER. I understand the whole idea but got into trouble with some dimension issues, here\u2019s the problem:\n<code class=\"lang-auto\">class NERModel(nn.Module):\n    \"\"\"\n    Encoder for NER model.\n    Args:\n        - vocab_size: vocabulary size, integer.\n        - embedding_size: embedding size, integer.\n        - enc_units: hidden size of LSTM layer, integer.\n        - ffc_units: hidden units of feedforward layer, integer.\n        - num_labels: number of named entities. The value should be (actual_num_labels + 1),\n            because zero paddings are added to the sequences.\n    \"\"\"\n    \n    def __init__(self, vocab_size, embedding_size, enc_units, ffc_units, num_labels):\n        super(NERModel, self).__init__()\n        # Word embedding layer.\n        self.embedding = nn.Embedding(vocab_size, embedding_size)\n        # LSTM layer with units of enc_units\n        self.LSTM = nn.LSTM(embedding_size, enc_units, batch_first=True)\n        self.dense1 = nn.Linear(enc_units, ffc_units)\n        self.dense2 = nn.Linear(ffc_units, num_labels)\n        \n    def forward(self, x):\n        \"\"\"\n        Args:\n            - x: Input tensor of shape (batch_size, sequence_length)\n        Return:\n            Tensor of shape (batch_size, sequence_length, num_labels)\n        \"\"\"\n        x = self.embedding(x)\n        # after embedding: torch.Size([64, 124, 256])\n        x, _ = self.LSTM(x)\n        # after lstm: torch.Size([64, 124, 256])\n        x = self.dense1(x)\n        # after linear 1: torch.Size([64, 124, 256])\n        x = self.dense2(x)\n        # after linear 2: torch.Size([64, 124, 6])\n        output = F.log_softmax(x, dim=1)\n        # after softmax: torch.Size([64, 124, 6])\n        return output\n\n# initialize model\nmodel = NERModel(vocab_size, embedding_size, enc_units, ffc_units, num_labels)\n\noptimizer = torch.optim.Adam(model.parameters(), lr = learning_rate)\ncriterion = nn.CrossEntropyLoss()\n\nfor i, (value, label) in enumerate(train_loader):\n        print(value.shape)\n        optimizer.zero_grad()\n        outputs = model(value)\n        # outputs shape: torch.Size([64, 124, 6])\n        # label shape: torch.Size([64, 124])\n        loss = criterion(outputs, label)\n<\/code>\nThings all looked good but I got the following error reported:\n<code class=\"lang-auto\">---------------------------------------------------------------------------\nValueError                                Traceback (most recent call last)\n<ipython-input-9-92530e221aaf> in <module>()\n---> 14         loss = criterion(outputs, label)\n\n~\/anaconda3\/lib\/python3.6\/site-packages\/torch\/nn\/modules\/module.py in __call__(self, *input, **kwargs)\n    491             result = self._slow_forward(*input, **kwargs)\n    492         else:\n--> 493             result = self.forward(*input, **kwargs)\n    494         for hook in self._forward_hooks.values():\n    495             hook_result = hook(self, input, result)\n\n~\/anaconda3\/lib\/python3.6\/site-packages\/torch\/nn\/modules\/loss.py in forward(self, input, target)\n    940     def forward(self, input, target):\n    941         return F.cross_entropy(input, target, weight=self.weight,\n--> 942                                ignore_index=self.ignore_index, reduction=self.reduction)\n    943 \n    944 \n\n~\/anaconda3\/lib\/python3.6\/site-packages\/torch\/nn\/functional.py in cross_entropy(input, target, weight, size_average, ignore_index, reduce, reduction)\n   2054     if size_average is not None or reduce is not None:\n   2055         reduction = _Reduction.legacy_get_string(size_average, reduce)\n-> 2056     return nll_loss(log_softmax(input, 1), target, weight, None, ignore_index, None, reduction)\n   2057 \n   2058 \n\n~\/anaconda3\/lib\/python3.6\/site-packages\/torch\/nn\/functional.py in nll_loss(input, target, weight, size_average, ignore_index, reduce, reduction)\n   1879         if target.size()[1:] != input.size()[2:]:\n   1880             raise ValueError('Expected target size {}, got {}'.format(\n-> 1881                 out_size, target.size()))\n   1882         input = input.contiguous().view(n, c, 1, -1)\n   1883         target = target.contiguous().view(n, 1, -1)\n\nValueError: Expected target size (64, 6), got torch.Size([64, 124])\n<\/code>\nI have outputs shape torch.Size([64, 124, 6]) and label shape: torch.Size([64, 124]). It seems that loss function want me to have outputs shape torch.Size([64, 6, 124]). I don\u2019t get the reason why it is like this, and can some one tell me how to modify it?","y":"\n\n\n tcsn_wty:\n\nBut if I understand correctly, you meant that I need to do the permutation before log_softmax called, am I correct?\n\n\nIf you want to keep F.log_softmax(x, dim=1), then yes.\nOtherwise use dim=2, if you want to permute the tensor afterwards.\n\n\n\n tcsn_wty:\n\nAs for loss function, if I\u2019m using nn.CrossEntropyLoss, do you meant that there\u2019s no need to apply F.log_softmax in forward() function?\n\n\nYes, since it will be applied twice at the moment (in your forward and inside nn.CrossEntropyLoss).","z":"nn.CrossEntropy expects a model output of the shape [batch_size, nb_classes, *additional_dimensions] and a target in [batch_size, *additional_dimensions] containing the class indices in the range [0, nb_classes] as explained in the docs with link \"https:\/\/pytorch.org\/docs\/stable\/nn.html?highlight=crossentropyloss#torch.nn.CrossEntropyLoss\".\nFor an output of [batch_size=64, nb_classes=124, additional=6], the target should have the shape [64, 6] and contain values in [0, 123].\nCould you explain your use case a bit and what the dimensions in your tensors mean?\nThanks for the reply.\nAll commented dimensions are results by calling tensor.shape.\nI\u2019ll further explain the specific numbers\n<code class=\"lang-auto\"># print(model) result\nNERModel(\n  (embedding): Embedding(30290, 256)\n  (LSTM): LSTM(256, 256, batch_first=True)\n  (dense1): Linear(in_features=256, out_features=256, bias=True)\n  (dense2): Linear(in_features=256, out_features=6, bias=True)\n)\n\nbatch_size = 64\nlength_of_each_input = 124 words\n<\/code>\nAs forward() function being invoked, I didn\u2019t modify any intermediate dimensions and the output by my neural network was [64, 124, 6]. 64 is the batch size, and for each sentence with 124 words, 6 log_softmax results were calculated for each word.\nIt seems that I need to make it [64, 6, 124] so that I could call the loss function?\nYes, you should permute the output to be able to call nn.CrossEntropyLoss.\nI\u2019m not sure how your forward is implemented, but I would recommend to check the shapes for all intermediate tensors.\nI did output every intermediate tensor shape in forward() function, and they were printed when training. To make the code easier to read, I replaced those command with the actual results.\n<code class=\"lang-auto\">def forward(self, x):\n        \"\"\"\n        Args:\n            - x: Input tensor of shape (batch_size, sequence_length)\n        Return:\n            Tensor of shape (batch_size, sequence_length, num_labels)\n        \"\"\"\n        x = self.embedding(x)\n        # after embedding: torch.Size([64, 124, 256])\n        x, _ = self.LSTM(x)\n        # after lstm: torch.Size([64, 124, 256])\n        x = self.dense1(x)\n        # after linear 1: torch.Size([64, 124, 256])\n        x = self.dense2(x)\n        # after linear 2: torch.Size([64, 124, 6])\n        output = F.log_softmax(x, dim=1)\n        # after softmax: torch.Size([64, 124, 6])\n        return output\n<\/code>\nAs you can see, I didn\u2019t modify the dimensions at all. Is it common that I usually need to permute the columns in order to call loss function?\nThe shapes look alright, if you add the discussed permute.\nHowever, the F.log_softmax operation should be applied in the class dimension, so in your case dim2 (or permute the tensor before the operation).\nAlso, since you are using F.log_softmax, you should use nn.NLLLoss as the criterion.\nnn.CrossEntropyLoss will apply F.log_softmax and nn.NLLLoss internally.\nThanks! I tried permute just now and the error was eliminated. But if I understand correctly, you meant that I need to do the permutation before log_softmax called, am I correct?\nAs for loss function, if I\u2019m using nn.CrossEntropyLoss, do you meant that there\u2019s no need to apply F.log_softmax in forward() function?\n\n\n\n tcsn_wty:\n\nBut if I understand correctly, you meant that I need to do the permutation before log_softmax called, am I correct?\n\n\nIf you want to keep F.log_softmax(x, dim=1), then yes.\nOtherwise use dim=2, if you want to permute the tensor afterwards.\n\n\n\n tcsn_wty:\n\nAs for loss function, if I\u2019m using nn.CrossEntropyLoss, do you meant that there\u2019s no need to apply F.log_softmax in forward() function?\n\n\nYes, since it will be applied twice at the moment (in your forward and inside nn.CrossEntropyLoss).\nOn a side note: You might want to add a non-linearity such as ReLU between your two linear layers. Otherwise, you don\u2019t gain much from having two linear layers."},{"x":"\ndevice = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\") \ntorch.cuda.is_available() #True\nword_model = word_model.to(device)\nword_model.cuda()\nfor i,(x,y) in enumerate(dataset):\n  if i < 1:\n    x = x.cuda()\n    output = word_model(x)\n    i+=1\n    print(output)\n  if(i>=1):\n    break\n\nAnd it shows :\nRuntimeError: Expected object of device type cuda but got device type cpu for argument #2 'mat2' in call to _th_mm \nI guess that my input and model are not in the same device, but not sure why.","y":"Okay, i just solved the problem by myself, the reason of this is the Attn() function which i wrote outside the model class as another def() function, and the Attn() function will not be moved to the GPU, so I create a new nn.Module class for Attn and i wrote : self.attn = Attn(hidden_size) in the model.","z":"below is my model, but i\u2019m not sure if this is where the error come from\n\nclass MojiNet(nn.Module):\n\n    def __init__(self,vocab_size,embedding_size,hidden_size,pretrained_embed,embed_dropout,model_dropout,num_layers,class_num):\n\n        super(MojiNet,self).__init__()\n\n        self.hidden_size = hidden_size\n\n        if(pretrained_embed):\n          self.embedding = nn.Embedding.from_pretrained(torch.FloatTensor(numpy_embed)).to(device)\n        else:\n          self.embedding = nn.Embedding(vocab_size,embedding_size)\n        self.embed_dropout = nn.Dropout(embed_dropout)\n        self.sent_gru = nn.GRU(embedding_size,hidden_size,num_layers,dropout=(0 if num_layers == 1 else model_dropout),bidirectional=True,batch_first=True)\n        self.fc = nn.Linear(hidden_size,class_num)\n        self.fc_dropout = nn.Dropout(0.5)\n        # self.attn = Attn(hidden_size)\n    def forward(self,x):\n        x = self.embedding(x)\n        x = self.embed_dropout(x)\n        #(batch,max_len,embedding_size)\n        y,_ = self.sent_gru(x)\n        #batch,max_len,2*hidden_size(bidirectional)\n        y = y[:,:,:hidden_size] + y[:,:,hidden_size:]\n        #batch,max_len,hidden_size(head + tail)\n        alpha = Attn(y,hidden_size)\n        #batch,1,max_len\n        r = alpha.bmm(y).squeeze(1)\n        #batch,hidden_size\n        h = torch.tanh(r)\n        #batch,hidden_size\n        output = self.fc(h)\n        #batch,class_num\n        output = self.fc_dropout(output)\n        return output\n\nI create the Attention function Attn() outside the model, but im not sure if this is the problem,since it didn\u2019t showed the information of it\nOkay, i just solved the problem by myself, the reason of this is the Attn() function which i wrote outside the model class as another def() function, and the Attn() function will not be moved to the GPU, so I create a new nn.Module class for Attn and i wrote : self.attn = Attn(hidden_size) in the model."},{"x":"Hello,\nI have trained a language model and now I want to fine-tune this pre-trained model.\n<code class=\"lang-auto\">model.summary()\nmodel.load_encoder('lmtest') \nmodel.freeze()\nmodel.summary()\n<\/code>\nBefore loading the encoder I look at model summary and once again after loading and freezing. However in both summaries trainable modules and number of parameters are still same. I would expect freeze() to set everything other than last layer to non-trainable if I understood correctly. So why does not freeze() change anything (visible)?\nI am quite new to pytorch and I would appreciate your guidance.","y":"Try this,\n<code class=\"lang-auto\">for name, params in your_learner_object.model.named_parameters():\n    if params.requires_grad:\n        print(name)\n<\/code>","z":"After freeze, try printing,\n<code class=\"lang-python\">for name, params in model.named_parameters():\n    if params.requires_grad:\n        print(name)\n<\/code>\nThanks a lot! I tried it and got the following error:\n 'RNNLearner' object has no attribute 'named_parameters'\nThis post with link \"https:\/\/stackoverflow.com\/questions\/11685936\/why-am-i-getting-attributeerror-object-has-no-attribute\" says it might be an indentation error but I checked and there was none.\nWhat\u2019s RNN Learner? Also if it\u2019s a model which inherits from nn.module then the above 3 line will definitely run.\nIt is from ULMFit architecture and takes Sequential.RNN which inherits nn.Sequential as an input as I understand.\n\n\ngithub.com with link \"https:\/\/github.com\/fastai\/fastai\/blob\/564896d7b84b59bee40db19ee298a1028235442b\/fastai\/text\/learner.py#L45\"\n\n\nfastai\/fastai\/blob\/564896d7b84b59bee40db19ee298a1028235442b\/fastai\/text\/learner.py#L45 with link \"https:\/\/github.com\/fastai\/fastai\/blob\/564896d7b84b59bee40db19ee298a1028235442b\/fastai\/text\/learner.py#L45\"\n<code class=\"lang-py\">\n    for i,w in enumerate(itos_new):\n        r = stoi_wgts[w] if w in stoi_wgts else -1\n        new_w[i] = enc_wgts[r] if r>=0 else wgts_m\n        if dec_bias is not None: new_b[i] = dec_bias[r] if r>=0 else bias_m\n    wgts['0.encoder.weight'] = new_w\n    if '0.encoder_dp.emb.weight' in wgts: wgts['0.encoder_dp.emb.weight'] = new_w.clone()\n    wgts['1.decoder.weight'] = new_w.clone()\n    if dec_bias is not None: wgts['1.decoder.bias'] = new_b\n    return wgts\n\n\nclass RNNLearner(Learner):\n    \"Basic class for a `Learner` in NLP.\"\n    def __init__(self, data:DataBunch, model:nn.Module, split_func:OptSplitFunc=None, clip:float=None,\n                 alpha:float=2., beta:float=1., metrics=None, **learn_kwargs):\n        is_class = (hasattr(data.train_ds, 'y') and (isinstance(data.train_ds.y, CategoryList) or\n                                                     isinstance(data.train_ds.y, LMLabelList)))\n        metrics = ifnone(metrics, ([accuracy] if is_class else []))\n        super().__init__(data, model, metrics=metrics, **learn_kwargs)\n        self.callbacks.append(RNNTrainer(self, alpha=alpha, beta=beta))\n        if clip: self.callback_fns.append(partial(GradientClipping, clip=clip))\n        if split_func: self.split(split_func)\n<\/code>\n\n\n\n\n\n\n\ngithub.com with link \"https:\/\/github.com\/fastai\/fastai\/blob\/564896d7b84b59bee40db19ee298a1028235442b\/fastai\/text\/models\/awd_lstm.py#L162\"\n\n\nfastai\/fastai\/blob\/564896d7b84b59bee40db19ee298a1028235442b\/fastai\/text\/models\/awd_lstm.py#L162 with link \"https:\/\/github.com\/fastai\/fastai\/blob\/564896d7b84b59bee40db19ee298a1028235442b\/fastai\/text\/models\/awd_lstm.py#L162\"\n<code class=\"lang-py\">\n        self.output_dp = RNNDropout(output_p)\n        if bias: self.decoder.bias.data.zero_()\n        if tie_encoder: self.decoder.weight = tie_encoder.weight\n\n\n    def forward(self, input:Tuple[Tensor,Tensor])->Tuple[Tensor,Tensor,Tensor]:\n        raw_outputs, outputs = input\n        output = self.output_dp(outputs[-1])\n        decoded = self.decoder(output)\n        return decoded, raw_outputs, outputs\n\n\nclass SequentialRNN(nn.Sequential):\n    \"A sequential module that passes the reset call to its children.\"\n    def reset(self):\n        for c in self.children():\n            if hasattr(c, 'reset'): c.reset()\n\n\ndef awd_lstm_lm_split(model:nn.Module) -> List[List[nn.Module]]:\n    \"Split a RNN `model` in groups for differential learning rates.\"\n    groups = [[rnn, dp] for rnn, dp in zip(model[0].rnns, model[0].hidden_dps)]\n    return groups + [[model[0].encoder, model[0].encoder_dp, model[1]]]\n\n\n<\/code>\n\n\n\n\n\nTry this,\n<code class=\"lang-auto\">for name, params in your_learner_object.model.named_parameters():\n    if params.requires_grad:\n        print(name)\n<\/code>\nThanks a lot! It works now!\nIt only prints parameters from the last layer so it means  freeze() works I think \n\n1.layers.0.weight\n1.layers.0.bias\n1.layers.2.weight\n1.layers.2.bias\n1.layers.4.weight\n1.layers.4.bias\n1.layers.6.weight\n1.layers.6.bias\n"},{"x":"Hi,\nApologies if my model itself is downright trash, please correct in that case. I am just learning pytorch.\nSo, I am trying to do sentiment analysis on imdb dataset. 1-gram, sending the word index to the embedding-bag, then single fc layer.\n<code class=\"lang-auto\">\nclass ffn(nn.Module):\n    def __init__(self):\n        super().__init__()\n        self.embedBag = nn.EmbeddingBag(1000, 20,sparse=True)\n        self.fc = nn.Linear(20, 1)\n        \n    def forward(self, x):\n        x = self.embedBag(x)\n        y = self.fc(x)\n        return t.sigmoid(y)\n\n \nmod = ffn()\n#using batch_size 1 so that no padding is required\nimdb_data = t.utils.data.DataLoader(imdb_ds(train_x, train_y),batch_size=1)\noptim = t.optim.Adagrad(mod.parameters(), lr=0.001 )\n\nloss_ls = []\nfor i,(x,y) in enumerate(imdb_data):\n    optim.zero_grad()\n    ybar = mod(x)\n    loss = F.binary_cross_entropy(ybar,t.tensor(y,dtype=t.float32))\n    loss_ls.append(loss.item())\n    loss.backward()\n    optim.step()\n    if i%5000==0: print(loss)\n<\/code>\nIs there a problem with loss? Is sigmoid giving problem? My loss is just stuck around 0.7, randomly fluctuating.","y":"It\u2019s usually better to remove the sigmoid and use nn.BCEWithLogitsLoss or F.binary_cross_entropy_with_logits.\nHowever, your model might be too small, so I would recommend to add a relu and another linear layer to it.\nLet me know, if that helps. ","z":"It\u2019s usually better to remove the sigmoid and use nn.BCEWithLogitsLoss or F.binary_cross_entropy_with_logits.\nHowever, your model might be too small, so I would recommend to add a relu and another linear layer to it.\nLet me know, if that helps. \nThanks for your suggestion. It really helped \n2 questions, if you don\u2019t mind:\n\nI used binary cross with logits. But then when I wrote my function to find accuracy, I had to take sigmoid of the output of the model. So, isn\u2019t it better to directly put sigmoid or softmax in model itself? When does the reverse become more useful?\nWhy have functions like sigmoid, tanh have been moved from nn.functional to torch.sigmoid, torch.tanh? Them being inside nn.functional makes kinda more sense intuitively, that functions are inside functional. Pretty sure team pytorch has better reasons than mine \n\n\n\n\nYes, you could use a sigmoid to get the probabilities and calculate the accuracy using a threshold. Alternatively you could also use a threshold directly on the logits (no sigmoid), but a threshold in the range [0, 1] might be easier and more intuitive to use.\nIf you put the sigmoid directly into the model, nn.BCELoss will apply torch.log internally, while passing logits to nn.BCEWithLogitsLoss will use the log-sum-exp trick with link \"https:\/\/en.wikipedia.org\/wiki\/LogSumExp#log-sum-exp_trick_for_log-domain_calculations\" as seen in these lines of code with link \"https:\/\/github.com\/pytorch\/pytorch\/blob\/35cdb785228b8abea1d3bdb844aa5980e6642f8d\/aten\/src\/ATen\/native\/Loss.cpp#L202-L218\" and will thus yield more numerical stability.\n\n\nI don\u2019t have a strong opinion on it and feel free to chime in your opinion on what\u2019s the less confusing approach.  Generally \u201cmathematical\u201d functions should go into the torch namespace, while NN-specific methods should stay in the nn.functional namespace as described here with link \"https:\/\/github.com\/pytorch\/pytorch\/issues\/22117\". torch.nn.functional.sigmoid should still work I think.\n\n\nAs the Sigmoid\/Relu are still inside torch.nn, I feel it is a bit confusing.\nI feel they should be moved to torch.math, and these sigmoid\/relu should be moved to torch.math.functional.\nI wouldn\u2019t create a new math namespace, as this would mean that also matmul, dot, sum etc. should be moved there, wouldn\u2019t it?\nFeel free to add your suggestions to the linked GitHub issue to discuss it further. "},{"x":"I have a sequence of L events stored in a tensor of shape L x W, where for each event its last feature is a timestamp which indicates when the event appears (it is actually a musical sequence in which events are notes, and the timestamp indicates when notes are played).\nWhat I\u2019d like to do is then to split this sequence into blocks according to their timestamp.\nMore concretely, let\u2019s assume my sequence is:\n<code class=\"lang-auto\">tensor([\n    [n1, 0],\n    [n2, 0.25],\n    [n3, 0.75],\n    [n4, 1],\n    [n5, 1.5],\n    [n6, 2.1],\n    ...\n])\n<\/code>\nWhat I want is to split the sequence into blocks so that each block contains the events appearing during a 1 second window, i.e. in the previous example return:\n<code class=\"lang-auto\">[\n    tensor([\n        [n1, 0],\n        [n2, 0.25],\n        [n3, 0.75],\n    ]),\n    tensor([\n        [n4, 1],\n        [n5, 1.5]\n    ]),\n    tensor([\n        [n6, 2.1]\n    ]),\n    ...\n]\n<\/code>\nThe only solution I see is to \u201cnaively\u201d use Python lists and for loops, so I was wondering if many exists a magic function that enables to do this in a fast way. I was thinking about using torch.split which is much faster and might do the job but I can\u2019t find how to use it.\nbenchmark comparison between for loop and torch.split:\n<code class=\"lang-auto\">$ python3 -m timeit -s 'import torch; seq=torch.tensor([0, 0.25, 0.75, 1, 1.5, 2.1, 3]).reshape(-1,1)' '\nblocks = []\nblock = []\nlimit = 1\nfor event in seq:\n    if event[-1] >= limit:\n        blocks.append(torch.stack(block, dim=0))\n        limit += 1\n        block = [event]\n    else:\n        block.append(event)\nblocks.append(torch.stack(block, dim=0))\n'\n1000 loops, best of 3: 418 usec per loop\n$ python3 -m timeit -s 'import torch; seq=torch.tensor([0, 0.25, 0.75, 1, 1.5, 2.1, 3]).reshape(-1,1)' 'torch.split(seq, [3, 2, 1, 1])'\n100000 loops, best of 3: 12.2 usec per loop\n<\/code>\nIs there an efficient way to solve this problem?\nBest,\nAlain","y":"Thanks a lot for your very precise answer !\nI modified it a bit in order not to use Numpy and it works great:\n<code class=\"lang-auto\">def get_split_wo_numpy(x):\n    d = x[:,-1].to(torch.int32)\n    _, counts = torch.unique(d, return_counts=True)\n    return torch.split(x, counts.tolist())\n\n# create tensor\nN = 50\nc0 = torch.randn(N)\nc1 = torch.arange(N) + 0.1 * torch.randn(N)\nx = torch.stack((c0, c1), dim=1)\n\nprint(*get_split_wo_numpy(x), sep='\\n')\n> tensor([[-0.3783,  0.0240]])\ntensor([[0.5671, 1.0883],\n        [0.1693, 1.9510]])\ntensor([[0.4059, 2.9808]])\ntensor([[-0.8332,  3.9922]])\ntensor([[-1.7366,  4.8935]])\ntensor([[0.1554, 5.9210]])\ntensor([[-0.5020,  7.0563]])\ntensor([[0.8593, 8.1128],\n        [0.6790, 8.8930]])\ntensor([[-1.3425,  9.9942]])\ntensor([[ 0.4977, 11.1460]])\ntensor([[-0.6046, 12.0957]])\ntensor([[ 0.0482, 13.0584]])\ntensor([[-3.5339, 14.0837],\n        [ 0.3017, 14.8644]])\ntensor([[-1.0470, 15.8537]])\ntensor([[ 0.5970, 17.0868],\n        [ 1.1420, 18.0000]])\ntensor([[ 0.2047, 18.9484]])\ntensor([[ 0.6992, 19.9839]])\ntensor([[-0.7491, 21.0039]])\ntensor([[-1.5463, 22.0174]])\ntensor([[-1.1534, 23.0604],\n        [-1.6386, 23.9969]])\ntensor([[-0.6788, 24.9266]])\ntensor([[-0.6330, 25.9962]])\ntensor([[ 0.8385, 27.0327],\n        [-0.5460, 27.9246]])\ntensor([[ 1.0593, 29.0393]])\ntensor([[ 0.8780, 30.1633]])\ntensor([[ 0.5493, 31.0493],\n        [ 0.3947, 31.9193]])\ntensor([[ 0.1004, 32.8814]])\ntensor([[-1.3328, 34.1098]])\ntensor([[ 0.5115, 35.0216],\n        [-0.1866, 35.9086]])\ntensor([[-0.5092, 37.1763],\n        [ 0.6438, 37.8571]])\ntensor([[ 0.3718, 38.8968]])\ntensor([[-1.6322, 40.0297],\n        [-0.7793, 40.9798]])\ntensor([[ 0.2476, 41.9675]])\ntensor([[ 0.4328, 43.0768]])\ntensor([[ 0.8585, 44.1284],\n        [ 0.1131, 44.8837]])\ntensor([[-0.2042, 46.1367]])\ntensor([[ 0.7368, 47.0504],\n        [-0.1325, 47.9358]])\ntensor([[-1.1966, 48.9396]])\n<\/code>\nMoreover, assuming that the sequence is sorted, one can replace torch.unique with torch.unique_consecutive. These minor modifications enable to increase a bit the speed of your original function:\n<code class=\"lang-auto\">%timeit get_split(x)\n%timeit get_split_wo_numpy(x)\n%timeit get_split_wo_numpy_consecutive(x)\n1000 loops, best of 3: 707 \u00b5s per loop\n1000 loops, best of 3: 565 \u00b5s per loop\n1000 loops, best of 3: 405 \u00b5s per loop\n<\/code>\nThe wrong thing with my for loop is that it assumed that for all i there were an event whose timestamp is between i and i+1, which is not true in the general case.\nThanks again for your solution !\nBest,\nAlain","z":"If you could use numpy, then this method should work:\n<code class=\"lang-python\">def get_splits(x):\n    bins = np.arange(0, np.ceil(x[:,1].max())+1)\n    d = torch.from_numpy(np.digitize(x.numpy()[:, 1], bins))\n    _, counts = torch.unique(d, return_counts=True)\n    return torch.split(x, counts.tolist())\n\n# Create tensor\nc0 = torch.arange(50).view(50, 1).float()\nc1 = torch.arange(50).view(50, 1) + torch.randn(50, 1) * 0.1\nx = torch.cat((c0, c1), dim=1)\n\nprint(get_splits(x))\n> (tensor([[ 0.0000, -0.0317]]),\n tensor([[1.0000, 1.0548]]),\n tensor([[2.0000, 2.1610]]),\n tensor([[3.0000, 3.0097]]),\n tensor([[4.0000, 4.0108]]),\n tensor([[5.0000, 5.0012]]),\n tensor([[6.0000, 6.0031],\n         [7.0000, 6.9909]]),\n tensor([[8.0000, 8.2395],\n         [9.0000, 8.8865]]),\n tensor([[10.0000,  9.9980]]),\n tensor([[11.0000, 11.0808],\n         [12.0000, 11.9696]]),\n tensor([[13.0000, 12.9840]]),\n tensor([[14.0000, 14.0748]]),\n tensor([[15.0000, 15.0449]]),\n tensor([[16.0000, 16.0221]]),\n tensor([[17.0000, 17.2724],\n         [18.0000, 17.9412]]),\n tensor([[19.0000, 19.0168]]),\n tensor([[20.0000, 20.0405]]),\n tensor([[21.0000, 21.0120],\n         [22.0000, 21.8588]]),\n tensor([[23.0000, 22.9400]]),\n tensor([[24.0000, 24.0344],\n         [25.0000, 24.8939]]),\n tensor([[26.0000, 26.0936],\n         [27.0000, 26.9038]]),\n tensor([[28.0000, 27.8985]]),\n tensor([[29.0000, 28.9492]]),\n tensor([[30.0000, 29.8341]]),\n tensor([[31.0000, 30.9124]]),\n tensor([[32.0000, 31.9069]]),\n tensor([[33.0000, 33.0332],\n         [34.0000, 33.8976]]),\n tensor([[35.0000, 34.8466]]),\n tensor([[36.0000, 35.9875]]),\n tensor([[37.0000, 36.9951]]),\n tensor([[38.0000, 37.9140]]),\n tensor([[39.0000, 39.1045]]),\n tensor([[40.0000, 40.0363]]),\n tensor([[41.0000, 41.0674],\n         [42.0000, 41.7366]]),\n tensor([[43.0000, 42.9243]]),\n tensor([[44.0000, 44.1054],\n         [45.0000, 44.9093]]),\n tensor([[46.0000, 45.9525]]),\n tensor([[47.0000, 46.9660]]),\n tensor([[48.0000, 47.9752]]),\n tensor([[49.0000, 48.9403]]))\n<\/code>\n%timeit get_split(x) returns\n211 \u00b5s \u00b1 10.9 \u00b5s per loop (mean \u00b1 std. dev. of 7 runs, 1000 loops each),\nwhile your loop would take\n1.35 ms \u00b1 104 \u00b5s per loop (mean \u00b1 std. dev. of 7 runs, 1000 loops each).\nNote however, that I don\u2019t get valid results using your loop:\n<code class=\"lang-python\">def reference(seq):\n    blocks = []\n    block = []\n    limit = 1\n    for event in seq:\n        if event[-1] >= limit:\n            blocks.append(torch.stack(block, dim=0))\n            limit += 1\n            block = [event]\n        else:\n            block.append(event)\n    blocks.append(torch.stack(block, dim=0))\n    return blocks\n\nprint(reference(x))\n> [tensor([[ 0.0000, -0.0317]]),\n tensor([[1.0000, 1.0548]]),\n tensor([[2.0000, 2.1610]]),\n tensor([[3.0000, 3.0097]]),\n tensor([[4.0000, 4.0108]]),\n tensor([[5.0000, 5.0012]]),\n tensor([[6.0000, 6.0031],\n         [7.0000, 6.9909]]),\n tensor([[8.0000, 8.2395]]),\n tensor([[9.0000, 8.8865]]),\n tensor([[10.0000,  9.9980]]),\n tensor([[11.0000, 11.0808]]),\n tensor([[12.0000, 11.9696]]),\n tensor([[13.0000, 12.9840]]),\n tensor([[14.0000, 14.0748]]),\n tensor([[15.0000, 15.0449]]),\n tensor([[16.0000, 16.0221]]),\n tensor([[17.0000, 17.2724]]),\n tensor([[18.0000, 17.9412]]),\n tensor([[19.0000, 19.0168]]),\n tensor([[20.0000, 20.0405]]),\n tensor([[21.0000, 21.0120]]),\n tensor([[22.0000, 21.8588]]),\n tensor([[23.0000, 22.9400]]),\n tensor([[24.0000, 24.0344]]),\n tensor([[25.0000, 24.8939]]),\n tensor([[26.0000, 26.0936]]),\n tensor([[27.0000, 26.9038]]),\n tensor([[28.0000, 27.8985]]),\n tensor([[29.0000, 28.9492]]),\n tensor([[30.0000, 29.8341]]),\n tensor([[31.0000, 30.9124]]),\n tensor([[32.0000, 31.9069]]),\n tensor([[33.0000, 33.0332]]),\n tensor([[34.0000, 33.8976]]),\n tensor([[35.0000, 34.8466]]),\n tensor([[36.0000, 35.9875]]),\n tensor([[37.0000, 36.9951]]),\n tensor([[38.0000, 37.9140]]),\n tensor([[39.0000, 39.1045]]),\n tensor([[40.0000, 40.0363]]),\n tensor([[41.0000, 41.0674]]),\n tensor([[42.0000, 41.7366]]),\n tensor([[43.0000, 42.9243]]),\n tensor([[44.0000, 44.1054]]),\n tensor([[45.0000, 44.9093]]),\n tensor([[46.0000, 45.9525]]),\n tensor([[47.0000, 46.9660]]),\n tensor([[48.0000, 47.9752]]),\n tensor([[49.0000, 48.9403]])]\n<\/code>\nThanks a lot for your very precise answer !\nI modified it a bit in order not to use Numpy and it works great:\n<code class=\"lang-auto\">def get_split_wo_numpy(x):\n    d = x[:,-1].to(torch.int32)\n    _, counts = torch.unique(d, return_counts=True)\n    return torch.split(x, counts.tolist())\n\n# create tensor\nN = 50\nc0 = torch.randn(N)\nc1 = torch.arange(N) + 0.1 * torch.randn(N)\nx = torch.stack((c0, c1), dim=1)\n\nprint(*get_split_wo_numpy(x), sep='\\n')\n> tensor([[-0.3783,  0.0240]])\ntensor([[0.5671, 1.0883],\n        [0.1693, 1.9510]])\ntensor([[0.4059, 2.9808]])\ntensor([[-0.8332,  3.9922]])\ntensor([[-1.7366,  4.8935]])\ntensor([[0.1554, 5.9210]])\ntensor([[-0.5020,  7.0563]])\ntensor([[0.8593, 8.1128],\n        [0.6790, 8.8930]])\ntensor([[-1.3425,  9.9942]])\ntensor([[ 0.4977, 11.1460]])\ntensor([[-0.6046, 12.0957]])\ntensor([[ 0.0482, 13.0584]])\ntensor([[-3.5339, 14.0837],\n        [ 0.3017, 14.8644]])\ntensor([[-1.0470, 15.8537]])\ntensor([[ 0.5970, 17.0868],\n        [ 1.1420, 18.0000]])\ntensor([[ 0.2047, 18.9484]])\ntensor([[ 0.6992, 19.9839]])\ntensor([[-0.7491, 21.0039]])\ntensor([[-1.5463, 22.0174]])\ntensor([[-1.1534, 23.0604],\n        [-1.6386, 23.9969]])\ntensor([[-0.6788, 24.9266]])\ntensor([[-0.6330, 25.9962]])\ntensor([[ 0.8385, 27.0327],\n        [-0.5460, 27.9246]])\ntensor([[ 1.0593, 29.0393]])\ntensor([[ 0.8780, 30.1633]])\ntensor([[ 0.5493, 31.0493],\n        [ 0.3947, 31.9193]])\ntensor([[ 0.1004, 32.8814]])\ntensor([[-1.3328, 34.1098]])\ntensor([[ 0.5115, 35.0216],\n        [-0.1866, 35.9086]])\ntensor([[-0.5092, 37.1763],\n        [ 0.6438, 37.8571]])\ntensor([[ 0.3718, 38.8968]])\ntensor([[-1.6322, 40.0297],\n        [-0.7793, 40.9798]])\ntensor([[ 0.2476, 41.9675]])\ntensor([[ 0.4328, 43.0768]])\ntensor([[ 0.8585, 44.1284],\n        [ 0.1131, 44.8837]])\ntensor([[-0.2042, 46.1367]])\ntensor([[ 0.7368, 47.0504],\n        [-0.1325, 47.9358]])\ntensor([[-1.1966, 48.9396]])\n<\/code>\nMoreover, assuming that the sequence is sorted, one can replace torch.unique with torch.unique_consecutive. These minor modifications enable to increase a bit the speed of your original function:\n<code class=\"lang-auto\">%timeit get_split(x)\n%timeit get_split_wo_numpy(x)\n%timeit get_split_wo_numpy_consecutive(x)\n1000 loops, best of 3: 707 \u00b5s per loop\n1000 loops, best of 3: 565 \u00b5s per loop\n1000 loops, best of 3: 405 \u00b5s per loop\n<\/code>\nThe wrong thing with my for loop is that it assumed that for all i there were an event whose timestamp is between i and i+1, which is not true in the general case.\nThanks again for your solution !\nBest,\nAlain\nInteger rounding was a clever trick! Thanks for sharing the improved approach "},{"x":"I\u2019m new to NLP. I\u2019m trying to use the Torch\u2019s nn.Embeddings and nn.RNN().\nI noticed a strange thing while doing this. For some reason, my model trains on CPU. But I cannot send it to GPU for training.\nHere is the Model architecture:\n#single layer RNN.\nimport torch.nn as nn\nimport torch\nclass RNN(nn.Module):\ndef init(self, ip_dim, emb_dim, hidden_dim, op_dim):\nsuper().init()\nself.embedding = nn.Embedding(ip_dim, emb_dim, padding_idx=0)\nself.rnn = nn.RNN(emb_dim, hidden_dim)\nself.fc = nn.Linear(hidden_dim, op_dim)\ndef forward(self, text):\ntext = torch.transpose(text, 0,1)\nemb = self.embedding(text)\nop, hidden = self.rnn(emb)\nout = self.fc(hidden)\nreturn out\nHere is me instantiating the model:\nemb_dim_len = 100\nhdim = 256\nnet = RNN(ip_dim = len(v2i), # v2i  = vocab to index. dictionary with elements like: {\u201cice\u201d : 1}\nemb_dim = emb_dim_len, hidden_dim = hdim,\nop_dim = len(Classes))\nnet.to(torch.device(\u2018cuda:0\u2019))\n#Throws the following error:\nreturn self._apply(lambda t: t.cuda(device))\n#RuntimeError: CUDA error: device-side assert triggered","y":"Were you able to use the GPU in the past?\nIf so, are you able to create a simple tensor on the device via: torch.randn(10, device='cuda')?","z":"Please use \u2018preformatted text Ctrl-Shift-C\u2019  for posting codes.\nAlso, seems like def init might be first issue to resolve, make it dunder.\nWere you able to use the GPU in the past?\nIf so, are you able to create a simple tensor on the device via: torch.randn(10, device='cuda')?"},{"x":"How do I use nn.CrossEntropyLoss() for seq2seq where my predication is of size (BS, seq_len, vocab_size) and truth of size (BS, seq_len), for example\n<code class=\"lang-auto\">predication = torch.randn(2, 3, 5, requires_grad=True) # (BS, seq_len, vocab_size)\ntarget = torch.empty(2, 3, dtype=torch.long).random_(5) # (BS, seq_len)\n<\/code>\n<code class=\"lang-auto\">predication: # size = (2, 3, 5)\ntensor([[[-1.3824, -1.4598, -0.3210, -0.2991,  0.2965],\n         [ 0.2591, -0.5094, -0.7029,  0.2963, -1.8912],\n         [ 2.0020, -1.1158,  1.1687, -0.5815, -0.4416]],\n\n        [[ 2.9818,  0.4093,  1.9568,  0.0664, -0.3604],\n         [-0.6369, -0.3365, -1.3922, -0.6929, -0.1229],\n         [ 0.6589, -1.3124, -2.0313, -1.4866, -1.8163]]], requires_grad=True)\n<\/code>\n<code class=\"lang-auto\">target: # size = (2, 3)\ntensor([[4, 3, 3],\n        [3, 2, 1]])\n<\/code>\ni.e. [-1.3824, -1.4598, -0.3210, -0.2991,  0.2965] is the probabilities of words in my vocabulary for the first word in first batch first sequence, which predicts word with label(or index)=4 in the vocabulary and that is the same as the ground truth.","y":"Try to permute the dimensions in your predication tensor to [batch_size, nb_classes, seq_len], i.e. [2, 5, 3] and it should work.","z":"Try to permute the dimensions in your predication tensor to [batch_size, nb_classes, seq_len], i.e. [2, 5, 3] and it should work."},{"x":"I am solving token classification problem. There is a huge class imbalance in data. Some of the classes have a million samples, others several hundred thousands. I saw many recommended to assign class weights to  do 1\/amount of samples per class. But this way gives me very small numbers. \nDoes it make sense or I have to multiply this on smth like 10^n?","y":"You could multiply the weights, if you are suspecting rounding errors or any other numerical issues.\nThe weights are used relatively, so you should be able to add a constant offset to the tensor.","z":"You could multiply the weights, if you are suspecting rounding errors or any other numerical issues.\nThe weights are used relatively, so you should be able to add a constant offset to the tensor."},{"x":"Hello,\nI am relatively new to deep learning with PyTorch. I tried to build a multi-layer feed-forward neural network that takes a tf-idf vector of a news title and outputs 0 or 1 whether the title is fake or not. The dataset used for the task was the onion with link \"https:\/\/www.kaggle.com\/chrisfilo\/onion-or-not\/download\/dQVLW3qxplJ6fZx6WL1A%2Fversions%2F4TX2bU7n80d7gIJWQml8%2Ffiles%2FOnionOrNot.csv?datasetVersionNumber=1\" dataset. Unfortunately, the part of preprocessing cannot change. So any optimization can be done on the model\u2019s architecture. My code is this:\n<code class=\"lang-auto\">import sys\nimport time\nimport nltk\nimport numpy\nimport pandas\nimport torch.nn\nimport statistics\nimport torch.utils\nimport sklearn.metrics\nimport sklearn.model_selection\nimport sklearn.feature_extraction.text\n\n# preprocessing\ninput_dataframe = pandas.read_csv('onion-or-not.csv', encoding='utf-8')\ntokenized_vector = dict()\nfor i in input_dataframe.index:\n    tokenized_vector[i] = nltk.word_tokenize(input_dataframe.loc[i][0])\nstemmer = nltk.PorterStemmer()\nstopwords = set(nltk.corpus.stopwords.words('english'))\nfor tokens in tokenized_vector:\n    for counter, token in enumerate(tokenized_vector[tokens]):\n        if stemmer.stem(tokenized_vector[tokens][counter]) not in stopwords:\n            tokenized_vector[tokens][counter] = stemmer.stem(tokenized_vector[tokens][counter])\n        else:\n            tokenized_vector[tokens].remove(tokenized_vector[tokens][counter])\nvectorizer = sklearn.feature_extraction.text.TfidfVectorizer()\npreprocessed_tokenized_vector = list()\nfor i in tokenized_vector:\n    preprocessed_tokenized_vector.append(' '.join(tokenized_vector[i]))\nX = vectorizer.fit_transform(preprocessed_tokenized_vector)\ntf_idf_df = pandas.DataFrame(X.todense(), columns=vectorizer.get_feature_names(), dtype=numpy.float16)\npreprocessed_data = pandas.concat([tf_idf_df, input_dataframe.iloc[:, 1:]], axis=1, sort=False)\nprint('Total size of dataframe: ', round(sys.getsizeof(preprocessed_data) \/ 2**20, 2), 'MB')\ndel X\ndel tf_idf_df\ndel preprocessed_tokenized_vector\ndel tokenized_vector\ndel input_dataframe\ncols = pandas.DataFrame(preprocessed_data.columns[:-1].tolist(), columns=['tokens'])\n\n# initialize dataset\nX = preprocessed_data.iloc[:, :-1]\nY = pandas.concat([preprocessed_data.iloc[:, -1], abs(preprocessed_data.iloc[:, -1] - 1)], axis=1).astype(numpy.int8)\ndel preprocessed_data\nY.columns = ['valid', 'fake']\nx_fit, x_test, y_fit, y_test = \\\n    sklearn.model_selection.train_test_split(X, Y,\n                                             test_size=0.25, random_state=42)\nx_train, x_val, y_train, y_val = \\\n    sklearn.model_selection.train_test_split(x_fit,\n                                             y_fit,\n                                             test_size=0.10,\n                                             random_state=42)\ndel x_fit\ndel y_fit\n\n# prepare data for pytorch\nx_train = torch.from_numpy(x_train.to_numpy()).float()\ny_train = torch.from_numpy(y_train.to_numpy()).float()\ntrain_dataset = torch.utils.data.TensorDataset(x_train, y_train)\ndel x_train\ndel y_train\nx_val = torch.from_numpy(x_val.to_numpy()).float()\ny_val = torch.from_numpy(y_val.to_numpy()).float()\nval_dataset = torch.utils.data.TensorDataset(x_val, y_val)\ndel x_val\ndel y_val\nx_test = torch.from_numpy(x_test.to_numpy()).float()\ny_test = torch.from_numpy(y_test.to_numpy()).float()\ntest_dataset = torch.utils.data.TensorDataset(x_test, y_test)\ndel x_test\ndel y_test\ntrain_loader = torch.utils.data.DataLoader(train_dataset)\nval_loader = torch.utils.data.DataLoader(val_dataset)\ntest_loader = torch.utils.data.DataLoader(test_dataset)\ndel train_dataset\ndel val_dataset\ndel test_dataset\nX = torch.from_numpy(X.to_numpy()).float()\nY = torch.from_numpy(Y.to_numpy()).float()\n\n# model architecture - This part must be optimized\nmodel = torch.nn.Sequential(\n    torch.nn.Linear(X.shape[1], 1024),\n    torch.nn.ReLU(),\n    torch.nn.Linear(1024, 512),\n    torch.nn.ReLU(),\n    torch.nn.Linear(512, 2),\n    torch.nn.Softmax(dim=1)\n)\ncriterion = torch.nn.BCELoss()\nlearning_rate = 1e-5\noptimizer = torch.optim.Adam(model.parameters(), lr=learning_rate)\ndef get_device():\n    device = None\n    if torch.cuda.is_available():\n        device = torch.device('cuda')\n    else:\n        device = torch.device('cpu')\n    return device\ndevice = get_device()\nmodel.to(device)\nearly_stopping = False\nprevent = 5\nconsecutive = False\nmessage = ' '\nepoch = 0\nepochs = 50\nprev_mean_valid_loss = numpy.Inf\nstart = 0\ntrain_loss = []\nvalid_loss = []\nhistory = []\n\n#fit model\nprint('Time: ', start, ' (in seconds)')\nwhile not early_stopping and epoch < epochs:\n    if epoch == 0:\n        start = time.time()\n\n    # prep model for training\n    model.train()\n    for x_train, y_train in train_loader:\n        # forward pass\n        y_hat = model(x_train.to(device))\n        # calculate the loss\n        loss = criterion(y_hat, y_train.to(device))\n        # clear the gradients of all optimized variables\n        optimizer.zero_grad()\n        # backward pass\n        loss.backward()\n        # perform a single optimization step (parameter update)\n        optimizer.step()\n        # update running training loss\n        train_loss.append(loss.item())\n    # shut down autograd to begin evaluation\n    with torch.no_grad():\n        # prep model for evaluation\n        model.eval()\n        for x_val, y_val in val_loader:\n            # forward pass\n            y_hat = model(x_val.to(device))\n            # calculate the loss\n            loss = criterion(y_hat, y_val.to(device))\n            # update running validation loss\n            valid_loss.append(loss.item())\n    # early stopping conditional\n    if prev_mean_valid_loss <= statistics.mean(valid_loss):\n        if consecutive is True:\n            prevent -= 1\n        consecutive = True\n        if prevent < 0:\n            early_stopping = True\n            message = '\\tPrevious average Validation error was lower than\\\n                current Validation error'\n    else:\n        consecutive = False\n\n    # print results after 2 epochs\n    if epoch % 2 == 1:\n        end = time.time()\n        print('Epoch: ', epoch+1, '\\t Time: +', end-start, '\\t Training\\\n        loss: ', statistics.mean(train_loss), '\\t Validation loss: ',\n              statistics.mean(valid_loss))\n        start = time.time()\n\n    # update epoch's validation loss variable\n    prev_mean_valid_loss = statistics.mean(valid_loss)\n\n    # early stopping message\n    if early_stopping is True:\n        print('\\t\\tStopping at epoch: ', epoch + 1, message)\n        epoch = epochs - 1\n    epoch += 1\n\n# test model\ntest_loss = []\n# initialize timer\nstart = time.time()\n# test model\nmodel.eval()\nwith torch.no_grad():\n    for x_test, y_test in test_loader:\n        yhat = model(x_test.to(device))\n        loss = criterion(yhat, y_test.to(device))\n        test_loss.append(loss.item())\n# end time checkpoint\nend = time.time()\n# print test results\nprint('\\tTime: {:.10} \\tTest Loss: {:.15f}'.format(end-start,\n                                                   statistics.mean(test_loss)))\n\n<\/code>\nThe problem is that the model\u2019s architecture seems to be too \u201cheavy\u201d. Is there any possibility that the architecture of the model could be optimized in memory and computation complexity?\nThanks in advance !","y":"If you want to reduce the model capacity, you could lower the number of hidden neurons in the model.\nAlso note that nn.BCELoss is expecting a single neuron with a sigmoid applied on it for a binary classification use case.\nSo you would have to change the last linear layer to nn.Linear(512, 1) and use nn.Sigmoid. Or remove the softmax and sigmoid, return the raw logits, and use nn.BCEWithLogitsLoss`.\nIf you want to use two neurons for the binary classification, you could keep the last linear layer, remove the softmax, and use nn.CrossEntropyLoss.","z":"If you want to reduce the model capacity, you could lower the number of hidden neurons in the model.\nAlso note that nn.BCELoss is expecting a single neuron with a sigmoid applied on it for a binary classification use case.\nSo you would have to change the last linear layer to nn.Linear(512, 1) and use nn.Sigmoid. Or remove the softmax and sigmoid, return the raw logits, and use nn.BCEWithLogitsLoss`.\nIf you want to use two neurons for the binary classification, you could keep the last linear layer, remove the softmax, and use nn.CrossEntropyLoss.\nThank you very much ! My code is now executable. I changed the output layer of my model and there is now only one neuron that uses Sigmoid activation. I also reduced the number of hidden neurons. The new model architecture can be examined below:\n\nThere is one small bit that is still ambiguous. The validation loss increases in every epoch,  while the training loss decreases. Are there any more problems?\nThanks in advance !\n11176\u00d7103 21.3 KB with link \"https:\/\/discuss.pytorch.org\/uploads\/default\/original\/3X\/a\/1\/a10728e5e02ea0adaca77f4636f89438b4f79b3f.png\"\nGood to hear it\u2019s working now!\nYour model might be overfitting, so you could try to add regularization techniques such as dropout layers, weight decay etc.\nI used a Dropout layer between the hidden and the output layer with .1 probability and added weight decay of .2 and it works like a charm! Thanks again!"},{"x":"Hi,\nI was using this snippet of code to load my finetuned GPT2 and it was working absolutely fine:\ntokenizer = GPT2Tokenizer.from_pretrained(\u2018gpt2-medium\u2019)\nmodel = GPT2LMHeadModel.from_pretrained(\u2018gpt2-medium\u2019)\nmodel_path = \u2018\/content\/drive\/My Drive\/PREDICTION\/POETRY\/EDset3_trained_models\/gpt2_medium_SET3_ED.pt\u2019\nmodel.load_state_dict(torch.load(model_path))\nmodel = model.to(device)\nAs of this morning, having made no changes, I get the following error:\nRuntimeError: Error(s) in loading state_dict for GPT2LMHeadModel:\nMissing key(s) in state_dict: \u201ctransformer.h.0.attn.masked_bias\u201d, \u201ctransformer.h.1.attn.masked_bias\u201d","y":"Hi Kiran,\nis this issue solved here with link \"https:\/\/github.com\/huggingface\/transformers\/issues\/4309\" already or are you still running into these issues?","z":"Hi Kiran,\nis this issue solved here with link \"https:\/\/github.com\/huggingface\/transformers\/issues\/4309\" already or are you still running into these issues?"},{"x":"I am trying to solve a text classification problem. My training data has input as a sequence of 80 numbers in which each represent a word and target value is just a number between 1 and 3.\nI pass it through this model:\n<code class=\"lang-auto\">class Model(nn.Module):\n    def __init__(self, tokenize_vocab_count):\n        super().__init__()\n        self.embd = nn.Embedding(tokenize_vocab_count+1, 300)\n        self.embd_dropout = nn.Dropout(0.3)\n        self.LSTM = nn.LSTM(input_size=300, hidden_size=100, dropout=0.3, batch_first=True)\n        self.lin1 = nn.Linear(100, 1024)\n        self.lin2 = nn.Linear(1024, 512)\n        self.lin_dropout = nn.Dropout(0.8)\n        self.lin3 = nn.Linear(512, 3)\n    \n    def forward(self, inp):\n        inp = self.embd_dropout(self.embd(inp))\n        inp, (h_t, h_o) = self.LSTM(inp)\n        h_t = F.relu(self.lin_dropout(self.lin1(h_t)))\n        h_t = F.relu(self.lin_dropout(self.lin2(h_t)))\n        out = F.softmax(self.lin3(h_t))\n        return out\n<\/code>\nMy training loop is as follows:\n<code class=\"lang-auto\">model = Model(tokenizer_obj.count+1).to('cuda')\n\noptimizer = optim.AdamW(model.parameters(), lr=1e-2)\nloss_fn = nn.CrossEntropyLoss()\n\nEPOCH = 10\n\nfor epoch in range(0, EPOCH):\n     for feature, target in tqdm(author_dataloader):\n         train_loss = loss_fn(model(feature.to('cuda')).view(-1,  3), target.to('cuda'))\n         optimizer.zero_grad()\n         train_loss.backward()\n         optimizer.step()\n      print(f\"epoch: {epoch + 1}\\tTrain Loss : {train_loss}\")\n<\/code>\nI printed out the feature and target dimension and it is as follows:\n<code class=\"lang-auto\">torch.Size([64, 80]) torch.Size([64])\n<\/code>\nHere 64 is the batch_size.\nI am not doing any validation as of now.\nWhen I train I am getting a constant loss value and no change\n<code class=\"lang-auto\">\/home\/koushik\/Software\/miniconda3\/envs\/fastai\/lib\/python3.7\/site-packages\/torch\/nn\/modules\/rnn.py:50: UserWarning: dropout option adds dropout after all but last recurrent layer, so non-zero dropout expects num_layers greater than 1, but got dropout=0.3 and num_layers=1\n  \"num_layers={}\".format(dropout, num_layers))\n  0%|                                                                                                                                                 | 0\/306 [00:00<?, ?it\/s]\/media\/koushik\/Backup Plus\/Code\/Machine Deep Learning\/NLP\/src\/Deep Learning\/model.py:20: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.\n  out = F.softmax(self.lin3(h_t))\n100%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588| 306\/306 [00:03<00:00, 89.36it\/s]\nepoch: 1        Train Loss : 1.0986120700836182\n100%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588| 306\/306 [00:03<00:00, 89.97it\/s]\nepoch: 2        Train Loss : 1.0986120700836182\n100%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588| 306\/306 [00:03<00:00, 89.35it\/s]\nepoch: 3        Train Loss : 1.0986120700836182\n100%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588| 306\/306 [00:03<00:00, 89.17it\/s]\nepoch: 4        Train Loss : 1.0986120700836182\n100%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588| 306\/306 [00:03<00:00, 88.72it\/s]\nepoch: 5        Train Loss : 1.0986120700836182\n100%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588| 306\/306 [00:03<00:00, 87.75it\/s]\nepoch: 6        Train Loss : 1.0986120700836182\n100%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588| 306\/306 [00:03<00:00, 85.67it\/s]\nepoch: 7        Train Loss : 1.0986120700836182\n100%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588| 306\/306 [00:03<00:00, 85.40it\/s]\nepoch: 8        Train Loss : 1.0986120700836182\n100%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588| 306\/306 [00:03<00:00, 84.49it\/s]\nepoch: 9        Train Loss : 1.0986120700836182\n100%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588| 306\/306 [00:03<00:00, 84.21it\/s]\nepoch: 10       Train Loss : 1.0986120700836182\n<\/code>\nCan anyone please help","y":"nn.CrossEntropyLoss uses F.log_softmax and nn.NLLLoss internally, so it expects raw logits as the model outputs.\nRemove the F.softmax at the end of your model and pass the output of self.lin(3) directly to the criterion.\nLet me know, if that helps. ","z":"nn.CrossEntropyLoss uses F.log_softmax and nn.NLLLoss internally, so it expects raw logits as the model outputs.\nRemove the F.softmax at the end of your model and pass the output of self.lin(3) directly to the criterion.\nLet me know, if that helps. \nYes, model training now. Thank you! \nI am having another problem now. I am training the model but the loss is going up and down repeatedly. It\u2019s not decreasing or converging. I tried to decrease the learning rate but it didn\u2019t work.\nHere\u2019s the loss plot at lr = 1e-3 for 30 epochs:-\n\nHere\u2019s the loss plot at lr = 1e-6 for 30 epochs:-\n\nHere\u2019s the loss plot at lr = 1e-9 for 30 epochs:-\n"},{"x":"What is the reason behind this restriction?\nIn the documentation for all recurrent layers is written:\ndropout \u2013 If non-zero, introduces a Dropout layer on the outputs of each RNN layer except the last layer\nBut why? Is it an implementation issue? Or is there research on this topic?\nWhen using only 1 LSTM layer I would not be able to use dropout, but it helps performance (when implemented manually) for my (time series forecasting) problem.\nThank you very much","y":"So the dropout in the last layer would be operating on what is the output of the RNN.\nThis means you can do it yourself on the output if needed, an option you don\u2019t have for the inner layers.\nNote that the dropout implemented by the RNN is not the dropout using one random draw for all timesteps.\nBest regards\nThomas","z":"So the dropout in the last layer would be operating on what is the output of the RNN.\nThis means you can do it yourself on the output if needed, an option you don\u2019t have for the inner layers.\nNote that the dropout implemented by the RNN is not the dropout using one random draw for all timesteps.\nBest regards\nThomas\nthanks that makes sense!"},{"x":"My aim is to make a five-category text classification\nI am running transformers fine tuning bert  with cnnbase model but my program stops at loss.backward() without any prompt in cmd.\nI debug find that the program stop at the loss.backward line without any error prompt\nMy program executed successfully in rnn base such as lstm and rcnn.\nBut when I am running some cnnbase model the strange bug  appears.\nMy cnn model code:\n<code class=\"lang-auto\">import torch\nimport torch.nn as nn\nimport torch.nn.functional as F\nfrom transformers.modeling_bert import BertPreTrainedModel, BertModel\nn_filters = 200\nfilter_sizes = [2,3,4]\nclass BertCNN(BertPreTrainedModel):\n    def __init__(self, config):\n        super(BertPreTrainedModel, self).__init__(config)\n        self.num_filters = n_filters\n        self.filter_sizes = filter_sizes\n        self.bert = BertModel(config)\n        for param in self.bert.parameters():\n            param.requires_grad = True\n        self.convs = nn.ModuleList(\n            [nn.Conv2d(1, self.num_filters, (k, config.hidden_size))\n                for k in self.filter_sizes])\n        self.dropout = nn.Dropout(config.hidden_dropout_prob)\n        self.fc_cnn = nn.Linear(self.num_filters *\n                                len(self.filter_sizes), config.num_labels)\n\n    def conv_and_pool(self, x, conv):\n        x = F.relu(conv(x)).squeeze(3)\n        x = F.max_pool1d(x, x.size(2)).squeeze(2)\n        return x\n\n    def forward(self, input_ids,\n                attention_mask=None, token_type_ids=None, head_mask=None):\n        outputs = self.bert(input_ids,\n                            attention_mask=attention_mask,\n                            token_type_ids=token_type_ids,\n                            head_mask=head_mask)\n        encoder_out, text_cls = outputs\n        out = encoder_out.unsqueeze(1)\n        out = torch.cat([self.conv_and_pool(out, conv)\n                         for conv in self.convs], 1)\n        out = self.dropout(out)\n        out = self.fc_cnn(out)\n        return out\n<\/code>\nMy train code:\n<code class=\"lang-auto\">        for step, batch in enumerate(data):\n            self.model.train()\n            batch = tuple(t.to(self.device) for t in batch)\n            input_ids, input_mask, segment_ids, label_ids = batch\n            print(\"input_ids, input_mask, segment_ids, label_ids SIZE: \\n\")   \n            print(input_ids.size(), input_mask.size(),segment_ids.size(), label_ids.size()) \n            # torch.Size([2, 80]) torch.Size([2, 80]) torch.Size([2, 80]) torch.Size([2])\n            logits = self.model(input_ids, segment_ids, input_mask)\n            print(\"logits and label ids size: \",logits.size(), label_ids.size())\n            # torch.Size([2, 5]) torch.Size([2])\n            loss = self.criterion(output=logits, target=label_ids) #loss function:CrossEntropyLoss()\n            if len(self.n_gpu) >= 2:\n                loss = loss.mean()\n            if self.gradient_accumulation_steps > 1:\n                loss = loss \/ self.gradient_accumulation_steps\n            if self.fp16:\n                with amp.scale_loss(loss, self.optimizer) as scaled_loss:\n                    scaled_loss.backward()\n                clip_grad_norm_(amp.master_params(self.optimizer), self.grad_clip)\n            else:\n                loss.backward() # I debug find that the program stop at this line without any error prompt\n<\/code>\nHELP~\uff01~   \u3001\nI  posted my questions on various community platforms\uff0cstackoverflow\u3001other github repositories.\nNo one replied to me.\nbug937\u00d7549 7.73 KB with link \"https:\/\/discuss.pytorch.org\/uploads\/default\/original\/3X\/d\/5\/d57093d8a6a26ed4b284901a26fb726efd9dde35.png\"\nI changed the batch size to 1 the bug still occured. sad\u2026","y":"I I tried to run my program on linux platform, and it ran successfully.\nTherefore, it is very likely that it is caused by different os\nPrevious os\uff1awin 10","z":"the step1 logits \uff1a\nlogits tensor([[ 0.8831, -0.0368, -0.2206, -2.3484, -1.3595]], device=\u2018cuda:1\u2019,\ngrad_fn=)\nthe step1 loss\uff1a\ntensor(1.5489, device=\u2018cuda:1\u2019, grad_fn=NllLossBackward>)\nbut why can\u2019t loss.backward()?\nCould you try to create a (small) executable code snippet, so that we could try to reproduce this issue?\nI I tried to run my program on linux platform, and it ran successfully.\nTherefore, it is very likely that it is caused by different os\nPrevious os\uff1awin 10"},{"x":"I have a matrix M in size of n*d, where n is the number of vectors and d is the dimension of each vector. Now, I want to find the closest pair of this n vectors. Be note, I only need the closest pair.\nI have two ways of doing this:\n\nUse the PairwiseDistance and argmin function. However, this method uses too much memory. Half of the returned matrix is useless.\n\nfunctional.pdist works well, it does not double computing. However, this function returns a condensed vector. I do not know how to recover the (i,j) index from this condensed vector.\n\nThe input matrix M changes and need to recompute the closest pair in every iteration. So, I want the whole process to be as efficient as possible.\nAny suggestions about solving this efficiently?","y":"Hi Fly!\n\n\n\n flyaway:\n\nI have a matrix M in size of n*d, where n is the number of vectors and d is the dimension of each vector. Now, I want to find the closest pair of this n vectors.\n\n\nfunctional.pdist works well, it does not double computing.\n\nAny suggestions about solving this efficiently?\n\n\nIn practice, for reasonable values of n and d, I expect that\npdist will do about as well as you can.\nHowever, the so-called computational cost of your pdist\nsolution is a suboptimal O(d * n^2).  If you search on\nclosest-pair problem, you will see that your problem can\nbe solved in O(d * n * log n) time.  However, these\n\u201cfaster\u201d algorithms don\u2019t vectorize naturally, so you give\nup much of the benefit of using a gpu (and even pipelines\nin a cpu).  Thus your \u201cnaive\u201d pdist solution may well run\nfaster (even as it requires more operations).\nGood luck.\nK. Frank","z":"I solved this by recovering the (i,j) index from the k index in the functional.pdist.\nThe code:\n<code class=\"lang-python\">def get_index(k, n):\n    p = n-1\n    kk = k\n    j = 1\n    i = 0\n    while kk >= p:\n        kk = kk-p\n        p = p-1\n        j += 1\n        i += 1\n    return i, j+kk\n\n<\/code>\nHi Fly!\n\n\n\n flyaway:\n\nI have a matrix M in size of n*d, where n is the number of vectors and d is the dimension of each vector. Now, I want to find the closest pair of this n vectors.\n\n\nfunctional.pdist works well, it does not double computing.\n\nAny suggestions about solving this efficiently?\n\n\nIn practice, for reasonable values of n and d, I expect that\npdist will do about as well as you can.\nHowever, the so-called computational cost of your pdist\nsolution is a suboptimal O(d * n^2).  If you search on\nclosest-pair problem, you will see that your problem can\nbe solved in O(d * n * log n) time.  However, these\n\u201cfaster\u201d algorithms don\u2019t vectorize naturally, so you give\nup much of the benefit of using a gpu (and even pipelines\nin a cpu).  Thus your \u201cnaive\u201d pdist solution may well run\nfaster (even as it requires more operations).\nGood luck.\nK. Frank"},{"x":"Hey guys,\ncan anyone of you could explain why such things below happened?\n<code class=\"lang-auto\"> def __init__(self, config):\n        super(BertForMultitask, self).__init__(config)\n        self.bert = BertModel(config)\n        self.dropout = nn.Dropout(config.hidden_dropout_prob)\n        self.apply(self.init_weights)\n        self.multitask_ratio = self.set_update_ratio(config, 0.7)\n\n        #####################\n        ####    Part A. This works     ####\n        #####################\n        self.classifier_student = nn.Linear(config.hidden_size, 2)\n        self.classifier_squad = nn.Linear(config.hidden_size, 2)\n        self.classifier_usermatch = nn.Linear(config.hidden_size, 2)\n        self.classifier_stsb = nn.Linear(config.hidden_size, 1)\n\n        ########################\n        ####    Part B. Below doesn't     ####\n        ########################        \n        self.classifier = {}\n        for task in config.tasks:\n             if task in ['student_response', 'squad', 'user_match']:\n                 self.classifier[task] = nn.Linear(config.hidden_size, self.config.num_labels)\n             elif task in ['sts-b']:\n                 self.classifier[task] = nn.Linear(config.hidden_size, 1)\n<\/code>\nI\u2019ve been implementing multitask model that deals with several different tasks\nand I\u2019ve trained with each of different Linear layer for each task after the inputs\npass through a BERT model.\nHowever, the problem here is that when I defined the Linear layers for each task\njust like the part B - dictionary holding each linear layers for each tasks - then it seems\nthe model doesn\u2019t hold the trained parameters of that layer after it completes the training.\nPart A where each layer is defined separately, not aggregated in a single python dictionary,\nworks just fine.\nIs there any secret behind this means of defining layers?\nAnyone knows any clue?","y":"Plain Python containers, such as list and dict won\u2019t be properly registered inside your module, so use nn.ModuleDict with link \"https:\/\/pytorch.org\/docs\/stable\/nn.html#torch.nn.ModuleDict\" in that case (or nn.ModuleList instead of list).","z":"Plain Python containers, such as list and dict won\u2019t be properly registered inside your module, so use nn.ModuleDict with link \"https:\/\/pytorch.org\/docs\/stable\/nn.html#torch.nn.ModuleDict\" in that case (or nn.ModuleList instead of list).\nThanks a lot for the clear reply!!\nShould I just fix the normal dictionary like\n<code class=\"lang-auto\">self.classifier = nn.ModuleDict({})\n<\/code>\nthen add elements??\nAnd what do you mean by \u2018not properly registered inside the module\u2019??\nDoes that mean that the model won\u2019t be able to properly record operations and data flows\nfor that variable??\nJust create it via:\n<code class=\"lang-python\">self.classifier = nn.ModuleDict()\n<\/code>\n\n\n\n hongtaesuk:\n\nDoes that mean that the model won\u2019t be able to properly record operations and data flows\nfor that variable?\n\n\nNo, Autograd will still track all operations on parameters etc. However, the parameters (of the child modules) won\u2019t be registered internally, so that model.parameters() won\u2019t return them (they might be thus missing in when you pass the parameters with this call to the optimizer). Also model.to() won\u2019t grab these parameters and they will stay on the initial device.\nWoah\u2026!\ngoosebumps!! haha\nmodel.to() didn\u2019t grab those parameters and it actually arose a operation confliction\nin two different devices (cpu &amp; gpu).\nI don\u2019t think I understood the whole things you\u2019ve said,\nbut I guess what I produce as a parent module which holds the normal python dict,\nproduces - somehow, what I didn\u2019t know in advance - child modules internally\nbut those normal python dicts are not registered properly,\nis this correct?\nSorry to bother you.\nYes, your explanation is correct. \nTherefore you could still use lists and dicts, if you explicitly don\u2019t want to register the content inside the module.\nA bit unrelated to this, but self.param = nn.Parameter(...) and self.register_buffer will register the parameters\/buffers properly, while a simple assignment of a tensor (self.tensor = torch.tensor(...)) will not.\nFeel free to ask, in case something is unclear or you want more information.\nAwesome!!!\nso good to get such a quality answers \nThanks a lot ptrblck!!\nand merry Christmas"},{"x":"I am a bit confused about LSTM input and output dimensions\n<code class=\"lang-auto\">Here is my network:\nIntent_LSTM(\n  (embedding): Embedding(41438, 400)\n  (lstm): LSTM(400, 512, num_layers=2, batch_first=True, dropout=0.5)\n  (dropout): Dropout(p=0.5, inplace=False)\n  (fc): Linear(in_features=512, out_features=3, bias=True)\n).\n<\/code>\nAlso this is my forward pass:\n<code class=\"lang-auto\">def forward(self, x):\n        \"\"\"\n        Perform a forward pass\n\n        \"\"\"\n        batch_size = x.size(0)\n\n        x = x.long()\n        embeds = self.embedding(x)\n\n        lstm_out, hidden = self.lstm(embeds)\n\n    \n        # stack up lstm outputs\n        lstm_out = lstm_out.contiguous().view(-1, self.hidden_dim)\n\n\n        out = self.dropout(lstm_out)\n        out = self.fc(out)\n\n\n        \n        # reshape to be batch_size first\n        out = out.view(batch_size, -1,3)\n        #print(\"sig_out\",sig_out.shape)\n        out = out[:, -1,:] # get last batch of labels\n        \n        # return last sigmoid output and hidden state\n        return out\n<\/code>\nHere the shape of my embdeddings is : [50, 150, 400] 50 being batch size, 150 seq lenth of my input. 400 being my embedded dimensions. I am feeding this into my LSTM. But when I was going through pytorch documentation. It states that input has to be in the form :\n**input**  of shape (seq_len, batch, input_size) with link \"https:\/\/pytorch.org\/docs\/stable\/nn.html\"\nSo should the input be converted to this format. ([150,50,400]) ?\nIf yes how do I do that?","y":"You define your LSTM with batch_first=True. This changes the expected dimensions if the input to (batch, seq_len, input_size). Try setting batch_first=False, and you will see that an error is thrown due to incorrect dimensions.","z":"You define your LSTM with batch_first=True. This changes the expected dimensions if the input to (batch, seq_len, input_size). Try setting batch_first=False, and you will see that an error is thrown due to incorrect dimensions."},{"x":"Computing Singular Value Decomposition (SVD) in Pytorch always give me this error:\nU, _, V = torch.svd(torch.mm(src_vec, tgt_vec.t() ) )\nIntel MKL ERROR: Parameter 4 was incorrect on entry to SLASCL.\nRuntimeError: svd_cuda: the updating process of SBDSDC did not converge (error: 23)\nEven I moved the Tensors to cpu but still getting the same issue.\nAny thoughts on this issue?","y":"I think my inputs contain nan, thats why I got an invalid svd.","z":"If you really think there is an error with torch.svd, open an issue on pytorch github here with link \"https:\/\/github.com\/pytorch\/pytorch\/issues\"\nI think my inputs contain nan, thats why I got an invalid svd."},{"x":"I am running a code which runs totally fine on CPU but fails in the second epoch when running on GPU. I am suspecting it is memory issue because it happens in second epoch. But not really able to pin down where exactly is the issue.\nError:\n<code class=\"lang-auto\"> for epoch in range(N_EPOCHS):\n     30         train_loss = train(seq_model, train_iter, optimizer, criterion, CLIP)\n---> 31         valid_loss = evaluate(seq_model, test_iter, criterion)\n     32 \n     33         if valid_loss < best_valid_loss:\n\n<ipython-input-17-36056d622cf9> in evaluate(seq_model, test_iterator, criterion)\n      7         question = batch.question\n      8         answer = batch.answer\n----> 9         output = seq_model(question,answer)\n     10 \n     11         loss=criterion(output[1:].view(-1, 2004), answer[1:].view(-1))\n\n\/opt\/anaconda3\/lib\/python3.7\/site-packages\/torch\/nn\/modules\/module.py in __call__(self, *input, **kwargs)\n    539             result = self._slow_forward(*input, **kwargs)\n    540         else:\n--> 541             result = self.forward(*input, **kwargs)\n    542         for hook in self._forward_hooks.values():\n    543             hook_result = hook(self, input, result)\n\n<ipython-input-15-dc2f26ea3a12> in forward(self, question, answer, teacher_forcing_ratio)\n     21         for i in range(1,100):\n     22 \n---> 23             prediction,enc_hidd_state,enc_hidd_cell = self.decoder(input, enc_hidd_state, enc_hidd_cell)\n     24 \n     25             outputs[i] = prediction\n\n\/opt\/anaconda3\/lib\/python3.7\/site-packages\/torch\/nn\/modules\/module.py in __call__(self, *input, **kwargs)\n    539             result = self._slow_forward(*input, **kwargs)\n    540         else:\n--> 541             result = self.forward(*input, **kwargs)\n    542         for hook in self._forward_hooks.values():\n    543             hook_result = hook(self, input, result)\n\n<ipython-input-14-d16c62e83aec> in forward(self, answer, hidden, cell)\n     19         f_embed = self.embed(answer)\n     20         #print(\"embed\"+str(f_embed.shape))\n---> 21         f_lstm,(hidden,cell) = self.lstm(f_embed.float(),(hidden,cell))\n     22         #print(\"lstm\"+str(f_lstm.shape))\n     23         f_linear = self.linear(f_lstm)\n\n\/opt\/anaconda3\/lib\/python3.7\/site-packages\/torch\/nn\/modules\/module.py in __call__(self, *input, **kwargs)\n    539             result = self._slow_forward(*input, **kwargs)\n    540         else:\n--> 541             result = self.forward(*input, **kwargs)\n    542         for hook in self._forward_hooks.values():\n    543             hook_result = hook(self, input, result)\n\n\/opt\/anaconda3\/lib\/python3.7\/site-packages\/torch\/nn\/modules\/rnn.py in forward(self, input, hx)\n    562             return self.forward_packed(input, hx)\n    563         else:\n--> 564             return self.forward_tensor(input, hx)\n    565 \n    566 \n\n\/opt\/anaconda3\/lib\/python3.7\/site-packages\/torch\/nn\/modules\/rnn.py in forward_tensor(self, input, hx)\n    541         unsorted_indices = None\n    542 \n--> 543         output, hidden = self.forward_impl(input, hx, batch_sizes, max_batch_size, sorted_indices)\n    544 \n    545         return output, self.permute_hidden(hidden, unsorted_indices)\n\n\/opt\/anaconda3\/lib\/python3.7\/site-packages\/torch\/nn\/modules\/rnn.py in forward_impl(self, input, hx, batch_sizes, max_batch_size, sorted_indices)\n    524         if batch_sizes is None:\n    525             result = _VF.lstm(input, hx, self._get_flat_weights(), self.bias, self.num_layers,\n--> 526                               self.dropout, self.training, self.bidirectional, self.batch_first)\n    527         else:\n    528             result = _VF.lstm(input, batch_sizes, hx, self._get_flat_weights(), self.bias,\n\nRuntimeError: cuDNN error: CUDNN_STATUS_EXECUTION_FAILED\n<\/code>\nDecoder definition:\n<code class=\"lang-auto\">class Decoder(nn.Module):\n    def __init__(self,embedding_matrix):\n    \n        super().__init__()\n\n        self.embed = nn.Embedding(2004,600)\n\n        self.embed.weight = nn.Parameter(embedding_matrix,requires_grad=False)\n\n        self.lstm = nn.LSTM(600,256,bidirectional =False)\n\n        self.linear = nn.Linear(256,2004)\n      \n    \n    \n    \n    def forward(self,answer, hidden, cell):\n        \n        f_embed = self.embed(answer)\n        #print(\"embed\"+str(f_embed.shape))\n        f_lstm,(hidden,cell) = self.lstm(f_embed.float(),(hidden,cell))\n        #print(\"lstm\"+str(f_lstm.shape))\n        f_linear = self.linear(f_lstm)\n        #print(\"linera\"+str(f_linear.shape))\n        f_relu = torch.relu(f_linear)\n        \n        return f_relu,hidden, cell \n<\/code>\nany insight on how to resolve would be highly appreciated.","y":"Do you re-initialize your hidden sate after each batch? See this Seq2Seq PyTorch tutoria with link \"https:\/\/pytorch.org\/tutorials\/intermediate\/seq2seq_translation_tutorial.html\"l, specifically the use if the initHidden() methods of the encoder and decoder.\nWithout re-initalizing (or the usage of detach() at the right spot), the backpropagation path of your RNN continuously grows, definitely leading to memory issues.","z":"Do you re-initialize your hidden sate after each batch? See this Seq2Seq PyTorch tutoria with link \"https:\/\/pytorch.org\/tutorials\/intermediate\/seq2seq_translation_tutorial.html\"l, specifically the use if the initHidden() methods of the encoder and decoder.\nWithout re-initalizing (or the usage of detach() at the right spot), the backpropagation path of your RNN continuously grows, definitely leading to memory issues.\nThanks for the quick response Chris.\nI did not know about the initialization of the hidden state of encoder and realized i somehow deleted the torch.no_grad() condition during evaluation. it seems both things combined led to the failure.\nCorrected both and code worked fine "},{"x":"I have a dataset which is in csv format having text as first field and class label as second field . The number of classes are 6 . How can i create a field object for this. Example code tutorials are helpful","y":"for Mutli classification:\nLABEL = data.LabelField(), for multi classification, we donot need to set dtype as pytorch expects it be as numericalized long tensors\nIMO, best place to start : https:\/\/github.com\/bentrevett\/pytorch-sentiment-analysis\nHappy learning","z":"The pipeline to load dataset is very similar like this with link \"https:\/\/github.com\/pytorch\/text\/blob\/master\/torchtext\/experimental\/datasets\/text_classification.py\".\nFor the multiclass classification problem, there are different models for that. You may take a look at the Bag-Of-Word model here with link \"https:\/\/pytorch.org\/tutorials\/beginner\/text_sentiment_ngrams_tutorial.html\"\nfor Mutli classification:\nLABEL = data.LabelField(), for multi classification, we donot need to set dtype as pytorch expects it be as numericalized long tensors\nIMO, best place to start : https:\/\/github.com\/bentrevett\/pytorch-sentiment-analysis\nHappy learning"},{"x":"Hi,\nI tried to make a CNN network for document classification. I used Keras previously.\nSo I am new to PyTorch and this indexing part is a pretty confusing part.\nI consider 150 words\/documents. I made my word to index dictionary and convert each word in the documents to the index.\nSo as a training sample I have the following tensor:\n<code class=\"lang-auto\">Sample input size:  torch.Size([1, 150])\nSample input: \n tensor([[1685,  190, 5459,  727, 1295,  772, 5460,  102, 4425, 9076,  935,    7,\n         1200, 9077,   25,   83,  498,  830, 2169,    7, 4426,   27,  533, 1296,\n          199,  167,  433, 5461, 4427,  592,   26, 6298,   23,   34, 9078,   15,\n          149, 5462, 9079,  285,  128, 6299, 1201,   15,   46,  416,  190, 9080,\n          399,  139,   29, 3175,  900, 2170,  772,   54, 2880,  158,  482,   15,\n          371, 5463, 9081, 3488, 1686,   26, 9082, 5464,   22,  901,  336, 1748,\n         9083, 5465, 1531,  694,  134, 5466,  313, 9084, 9085, 5467,  772, 5468,\n         2881,    5, 3488,   26, 5463,  371, 5469, 2695,  679, 1921,  167, 9086,\n         2170,  520, 4428,  450,   72,  336, 6300,  521,   26,  695,  694, 1297,\n           46, 6301,  433,  100,  337,   33,   61, 5470,  620,    6, 3176, 9087,\n            2, 2326, 9088,  451,  339,  695,  935,  772, 2039, 9089,   33, 6302,\n           61,   60, 2696,    2, 2327, 9090,  451,  773, 2697,   15,   83,  498,\n         1531, 1114,    7,   34, 1922,  290]])\n\nSample label size:  torch.Size([1])\nSample label: \n tensor([1.])\n<\/code>\nI made the following network:\n<code class=\"lang-auto\">class CNN(nn.Module):\n    \n    def __init__(self, vocab_size, output_size, embedding_dim, prob_drop):\n        super(CNN, self).__init__()\n        \n        #Arguments\"\n        filter_sizes = [1,2,3]\n        num_filters = 36\n\n        self.vocab_size = vocab_size\n        self.output_size = output_size\n        \n        self.embedding = nn.EmbeddingBag(vocab_size, embedding_dim, sparse=True)\n        self.conv1 = nn.Conv2d(1,num_filters, (filter_sizes[0], embedding_dim))\n        self.conv2 = nn.Conv2d(1,num_filters, (filter_sizes[1], embedding_dim))\n        self.conv3 = nn.Conv2d(1,num_filters, (filter_sizes[2], embedding_dim))\n#         self.conv4 = nn.Conv2d(num_filters, (fiter_sizes[3], embedding_dim))\n        self.dropout = nn.Dropout(prob_drop)\n    \n    def conv_(self, val, conv_layer):\n        \n        conv_out = conv_layer(val)\n      \n        activation = F.relu(conv_out.squeeze(3))# activation.size() = (batch_size, out_channels, dim1)\n        max_out = F.max_pool1d(activation, activation.size()[2]).squeeze(2)# maxpool_out.size() = (batch_size, out_channels)\n\n        return max_out\n    \n    def forward(self, x):\n        x = x.long()\n       \n        input_ = self.embedding(x)\n    \n        input_ = input_.unsqueeze(1)\n      \n        out1 = self.conv_(input_,self.conv1)\n        \n        out2 = self.conv_(input_,self.conv2)\n        \n     out3 = self.conv_(input_,self.conv3)\n        \n        all_out = torch.cat((out1, out2, out3), 1) (batch_size, num_kernels*out_channels)\n        fc_in = self.dropout(all_out) \n        logits = self.label(fc_in)\n        \n        return logit\n<\/code>\n<code class=\"lang-auto\">vocab_size = len(vocab_to_int)+1\noutput_size = 1\n\nembedding_dim = 100 \nprob_drop =0.1\nnet = CNN(vocab_size, output_size, embedding_dim, prob_drop)\nlr = 0.001\n\ncriterion = nn.CrossEntropyLoss()\noptimizer = torch.optim.Adam(net.parameters(), lr = lr)\n\n<\/code>\nthe training part for one sample is as follow:\n<code class=\"lang-auto\">net.train()\nfor e in range(epochs):\n    \n\n    for inputs, labels_ in train_one_loader:\n        print(inputs.size())\n\n        # zero the parameter gradients\n        optimizer.zero_grad()\n        \n        \n        outputs = net(inputs)\n        \n        loss = criterion(outputs, labels)\n        loss.backward()\n        optimizer.step()\n\n\n<\/code>\nHowever, the input size that I receive from embedding in training is [1,100] instead of [1,150,100] and it causes the error.\nI am guessing that I miss one step in my training loop, but I can not figure it out.\nWould you please help me to solve this problem?","y":"nn.EmbeddingBag != nn.Embedding\nThe first will give you the average of all the embeddings in the sequence while the second will give you the embeddings in a sequence.  I think you want to use the latter rather than the former.","z":"but nn.Conv2d expects 4 dimensional input, it would not work for three dimensional input.\nfor image it is like,\n[batch_size, number_of_channels, height, width]\noutput of nn.Embedding will be like,\n[number_of_words, embedding_dimension]\nwe could use nn.Conv1d after nn.Embedding, that is something like,\n<code class=\"lang-auto\">conv = nn.Conv1d(1, 10, 3) # suppose in_channels is 1, out_channels is 10, kernel_size is 3\nemb = nn.Embedding(150, 100) # 150 words, 100 embedding size\ndoc = torch.LongTensor([0, 1, 2]) # suppose our document has 3 words\nz = emb(doc.long()) # output shape [3, 100]\ne = z.reshape(3, 1, 100) # reshape it to 3 words, 1 channel, 100 embedding size, to use it with conv1d\nconv(e).shape\n<\/code>\ntorch.Size([3, 10, 98])\nor if we use nn.Conv2d after nn.Embedding, then we will have to split embedding size 100, into 10x10,\n<code class=\"lang-auto\">conv = nn.Conv2d(1, 10, 3) # suppose in_channels is 1, out_channels is 10, kernel_size is 3\nemb = nn.Embedding(150, 100) # 150 words, 100 embedding size\ndoc = torch.LongTensor([0, 1, 2]) # suppose our document has 3 words\nz = emb(doc.long()) # output shape [3, 100]\ne = z.reshape(3, 1, 10, 10) # reshape it to 3 words, 1 channel, 10x10 embedding size, to use it with conv2d\nconv(e).shape\n<\/code>\ntorch.Size([3, 10, 8, 8])\n, thanks for your explanation.\n\n\n\n vainaijr:\n\nemb = nn.Embedding(150, 100)\n\n\nnn.Embeddin will receive 2 numbers. The first number is the length of the (vocabulary size +1)  and not 150, which is the length of each document. The second number is the embedding dimension, which I considered as 100. I follow the steps here, from here: <a>https:\/\/pytorch.org\/tutorials\/beginner\/nlp\/word_embeddings_tutorial.html.<\/a>\nWhat I suppose to get from the embedding layer in the training process is a tensor size of (Batch size, length of the document, embedding dimension), which I am not getting this tensor size. I am getting a tensor size of [batch size,100].\nThe reason that I posted this question is that for some reason, it looks like that my embedding layer couldn\u2019t convert the indices in my sample to their corresponding embedding vector.\nAbout the CNN part, in NLP, we usually have nnConv2d(1, number of each kind of kernel\n(window size of words, word embedding dimension)). In making CNN for text, we keep the width of the kernel in the size of embedding and change the width word by word. Please visit this link:\n<a>http:\/\/www.wildml.com\/2015\/11\/understanding-convolutional-neural-networks-for-nlp\/.<\/a>\nTherefore, my question remains unanswered.\nAny thoughts on this?\nIn case it helps, here\u2019s my code with link \"https:\/\/github.com\/chrisvdweth\/ml-toolkit\/blob\/master\/pytorch\/models\/text\/classifier\/cnn.py\" that link with link \"http:\/\/www.wildml.com\/2015\/11\/understanding-convolutional-neural-networks-for-nlp\/\" you\u2019ve posted. I use nn.Conv1d here, but as  said, nn.Conv2d works as well but the dimensions have to be adjusted.\nnn.EmbeddingBag != nn.Embedding\nThe first will give you the average of all the embeddings in the sequence while the second will give you the embeddings in a sequence.  I think you want to use the latter rather than the former.\n, Thanks for sharing the code! Your blog is one of the most comprehensive blogs in the CNN text classification that I have read so far.\nMost of the examples that I found have used Conv2d but I could not see the dimension adjustments in the codes. Would you please explain why do we need to adjust the embedding dimension of 100 to 10x10?\n, Thanks! You are right! I wanted to use nn.Embedding. Now, I\u2019m getting the correct dimension from the embedding layer.\n, WildML is not my blog. I wish I had the time and skill for that :). But I still can give it a shot at the explanation.\nOnce pushed a sequence through the embedding layer, you have a 2-dim tensor: (seq_len, embed_dim); we can ignore the batch size dimension here to keep it simple. Now this has the same shape as an image, so your sequence can be pushed trough a nn.Conv2d layer. However, it does not make any semantic sense to convolve over the embedding dimension.\nLook up the docs with link \"https:\/\/pytorch.org\/docs\/stable\/nn.html\" and check at the constructor of nn.Conv2d:\nConv2d(in_channels, out_channels, kernel_size, stride=1, padding=0, dilation=1, groups=1, bias=True, padding_mode='zeros')\n\nWhere kernel_size is either a tuple of two ints reflecting the size of the kernel, e.g., (3, 5), or a single int, say 3, which implies a square kernel (3, 3). As I said, it does not make sense to convolve over the mebedding dimension. As such you have, you have to set the kernel size like:\nConv2d(in_channels, out_channels, kernel_size=(3, embed_dim), ...)\n\nOr any other size than 3. This snippet kernel_size=(_, embed_dim) you should see in all the examples that use nn.Conv2d for text classification. If not, I would argue the model is implemented not correctly. Note that nn.Conv2d is happily using, say, kernel_size=(4, 4). It throws no error, but it\u2019s semantically wrong in this case of text classification.\nnn.Conv1d just simplifies this a bit. The constructor looks very similar:\nConv1d(in_channels, out_channels, kernel_size, stride=1, padding=0, dilation=1, groups=1, bias=True, padding_mode='zeros')\n\nHowever, kernel_size can only be single int and not a tuple of ints since you convolve over just one dimension. As such, you would create the layer like:\nConv1d(in_channels, out_channels, kernel_size=3, ...)\n\nwhich here implies the kernel size of (3, embed_dim).\nIn short, you can use both nn.Conv2d and nn.Conv2d. The only difference is that with nnConv2d you have to be tad more careful how you define the kernel size. With nn.Conv1d you cannot simply set the kernel size incorrectly.\nI hope that helps.\n, Thanks for the clarification about the blog  and your comprehensive explanation! I visited the code that you shared, and it helped me a lot.\nAhaaaa got it! Now it is crystal clear to me.\nThere is no point in having Conv2d because, in the text, the kernel needs to move word by word while we keep the embedding dimension as a constant.  And, by using Con2d, we consider a spare dimension throughout the convolution.\nThanks,  again!"},{"x":"In my code, I\u2019ve used nn.batchnorm1d(64) as one of the layers where 64 is the batch size. Now after training my net I want to test the result for a single sample but it shows a error saying that I need to pass 64 samples. Is there any workaround to test the net with single sample?","y":"If the batch norm layers are between linear layers, the shape should most likely once be changed before the first linear layer.\nAnyway, you could define a Permute layer and use it inside your nn.Sequential container:\n<code class=\"lang-python\">class Permute(nn.Module):\n    def __init__(self, dims):\n        super(Permute, self).__init__()\n        self.dims = dims\n        \n    def forward(self, x):\n        x = x.permute(self.dims).contiguous()\n        return x\n<\/code>","z":"The in_features argument you are passing defines the number of channels C in a [N, C, L] input or the sequence length L in an input with the shape [N, L].\nIt does not specify the batch size N.\nFor evaluation and testing you should call model.eval() which will apply the running stats to the samples and which will work with single samples as well.\nHey , I\u2019m using an RNN encoder-deocder model and at the decoder side I have few linear layers. I\u2019m using batchnorm in between these layer. My input to batchnorm1d layer is of the shape (seq_len, batch, hidden_size) i.e [1, 64, 256].\nSo what should I pass to batchnorm1d().\nI tried passing 64 but after training the net when I\u2019m passing a single sample it shows the error \u2018running_mean should contain 1 element not 64\u2019.\nPlease help me here.\nI think you would have to permute the activation from [seq, batch_size, features] to [batch_size, features, seq] via x = x.permute(1, 2, 0) and pass it to the batch norm layer.\nCurrently you are using the batch dimension to normalize the activations, which won\u2019t work.\nAlternatively, you could also set batch_first=True in your RNN, which should return the tensor as [batch_size, seq, features] and permute the last two dimensions,\nThanks for giving your time. Now I understand it. One more question though\nMy batch norm layer is in between linear layers and all of them are stacked in nn.Sequential(). Is there a way to permute the input inside nn.Sequential() because if not then I have to separate out the batch norm layer which makes the code little messy.\nIf the batch norm layers are between linear layers, the shape should most likely once be changed before the first linear layer.\nAnyway, you could define a Permute layer and use it inside your nn.Sequential container:\n<code class=\"lang-python\">class Permute(nn.Module):\n    def __init__(self, dims):\n        super(Permute, self).__init__()\n        self.dims = dims\n        \n    def forward(self, x):\n        x = x.permute(self.dims).contiguous()\n        return x\n<\/code>"},{"x":"Hi all, sorry for basic question.\nIs it a correct way to build_vocab()?\n<code class=\"lang-auto\">import torch\nfrom torchtext.data import Dataset, Example, Field\nfrom torchtext.data import Iterator, BucketIterator\n\nTEXT  = Field(sequential=True, tokenize=lambda x: x.split(), lower=True)\nLABEL = Field(sequential=False, use_vocab=False)\n\ndata = [(\"The mountain is hight\", \"A\"), (\"Work is quite interesting\", \"B\")]\nfs = [('text', LABEL), ('category', TEXT)]\nexamples = list(map(lambda x: Example.fromlist(list(x), fields=fs), data))\ndt = Dataset(examples, fields=fs)\nTEXT.build_vocab(dt, vectors=\"glove.6B.100d\")\n\nprint(len(TEXT.vocab))\n\nfor el in data:\n    tokens = el[0].split()\n    print(tokens)\n    for t in tokens:\n        print(TEXT.vocab.stoi[t])\n<\/code>\nAsking because in my working code (above is a test) all indices are set to 0, as also in this example.","y":"\n\n\n al314:\n\nbatch_size\n\n\nYou mean data_iter is still text now? At some point, I remember, it numericalizes text into tensor. data_iter should be ready to use to train model.\nIf not, you can use TEXT.vocab.stoi() to numericalize tokens as a list and convert to tensors torch.Tensor([tok_ids]).","z":"It generates vocab based only on the second entry (a.k.a. category). Therefore, any tokens in text are unk, whose id is 0. IMO\nYou could write something similar like build_vocab_from_iterator with link \"https:\/\/github.com\/pytorch\/text\/blob\/a5880a3da7928dd7dd529507eec943a307204de7\/torchtext\/vocab.py#L547\". The constructor of the Vocab class needs a counter of tokens. Take a look at the Vocab class here with link \"https:\/\/github.com\/pytorch\/text\/blob\/master\/torchtext\/vocab.py\"\nThanks!\nNow I fixed my code snippet into\n<code class=\"lang-auto\">import torch\nfrom torchtext.data import Dataset, Example, Field\nfrom torchtext.data import Iterator, BucketIterator\n\nTEXT  = Field(sequential=True, tokenize=lambda x: x.split(), \n                       lower=True, use_vocab=True)\nLABEL = Field(sequential=False, use_vocab=False)\n\ndata = [(\"shop street mountain is hight\", \"a\"), \n         (\"work is interesting\", \"b\")]\n\nFIELDS = [('text', TEXT), ('category', LABEL)]\n\nexamples = list(map(lambda x: Example.fromlist(list(x), fields=FIELDS), \n                                 data))\n\ndt = Dataset(examples, fields=FIELDS)\n\nTEXT.build_vocab(dt, vectors=\"glove.6B.100d\")\nLABEL.build_vocab(dt, vectors=\"glove.6B.100d\")\n\nprint(TEXT.vocab.stoi[\"is\"])\n\ndata_iter = Iterator(dt, batch_size=4, sort_key=lambda x: len(x))\n<\/code>\nBut now I have the next question )\nHow do I transform text data in dt or data_iter into numerical format suitable to be fed in into the model?\nNow I have iterator over \u2018dt\u2019, but it contains text field as a text, not as numerical torch tensors.\nAs I understand TEXT field now contains mappings to tensors, but I need to use dt or data_iter as input to the model.\nUpdate: reformulated into the question: Creating input for the model from the raw text with link \"https:\/\/discuss.pytorch.org\/t\/creating-input-for-the-model-from-the-raw-text\/68219\"\n\n\n\n al314:\n\nbatch_size\n\n\nYou mean data_iter is still text now? At some point, I remember, it numericalizes text into tensor. data_iter should be ready to use to train model.\nIf not, you can use TEXT.vocab.stoi() to numericalize tokens as a list and convert to tensors torch.Tensor([tok_ids])."},{"x":"Hi all,\nIn the official documentation tutorial on text classification\nhttps:\/\/pytorch.org\/tutorials\/beginner\/text_sentiment_ngrams_tutorial.html\nthe next data set is used\n<code class=\"lang-auto\">train_dataset, test_dataset = \ntext_classification.DATASETS['AG_NEWS'](root='\/.pytorch_datasets', ngrams=2, vocab=None)\n<\/code>\nwhich contains elements which are already numerical tensors and perfectly suitable for feeding into neural net\n<code class=\"lang-auto\">train_dataset[0]\n\n(2,\n tensor([572, 564, 2, 2326,   ...]))\n\n<\/code>\nHow do I prepare the same input from the raw text?\nSo far I have managed to convert only single words into vectors (thanks for the help here with link \"https:\/\/discuss.pytorch.org\/t\/is-it-a-correct-way-to-build-vocab-torchtext\/68094\/3\" )\n<code class=\"lang-auto\">import torch\nimport torch.nn as nn\nfrom torchtext.data import Dataset, Example, Field\nfrom torchtext.data import Iterator, BucketIterator\n# https:\/\/pytorch.org\/tutorials\/beginner\/nlp\/word_embeddings_tutorial.html\n\nTEXT  = Field(sequential=True, tokenize=lambda x: x.split(), lower=True, use_vocab=True)\nLABEL = Field(sequential=False, use_vocab=False)\n\ndata = [(\"shop street mountain is hight\", \"a\"), \n         (\"work is interesting\", \"b\")]\n\nFIELDS = [('text', TEXT), ('category', LABEL)]\n\nexamples = list(map(lambda x: Example.fromlist(list(x), fields=FIELDS), data))\n\ndt = Dataset(examples, fields=FIELDS)\n\nTEXT.build_vocab(dt, vectors=\"glove.6B.100d\")\nLABEL.build_vocab(dt, vectors=\"glove.6B.100d\")\n\nprint(TEXT.vocab.stoi[\"is\"])\n\ndata_iter = Iterator(dt, batch_size=4, sort_key=lambda x: len(x))\n\nVOCAB_SIZE = len(TEXT.vocab)\nembedding = nn.Embedding(VOCAB_SIZE, 32)\n\nprint(embedding(torch.tensor(TEXT.vocab.stoi[\"is\"])))\n<\/code>\nBut this is only for single words while in the raw text I have several sentences per label.\nCan you please advise me on how to convert variable-length pieces of text into same dimensional tensors like we have in built-in data set text_classification.DATASETS['AG_NEWS']?","y":"Or, you could load your data with a new torchtext abstraction. Text classification datasets, mentioned by you, follow the same new abstraction. It should be very straightforward to copy\/paste and write your own pipeline link with link \"https:\/\/github.com\/pytorch\/text\/blob\/master\/torchtext\/datasets\/text_classification.py\".","z":"Hi Alex,\nPlease follow this tutorial : https:\/\/mlexplained.com\/2018\/02\/08\/a-comprehensive-tutorial-to-torchtext\/\nOr, you could load your data with a new torchtext abstraction. Text classification datasets, mentioned by you, follow the same new abstraction. It should be very straightforward to copy\/paste and write your own pipeline link with link \"https:\/\/github.com\/pytorch\/text\/blob\/master\/torchtext\/datasets\/text_classification.py\"."},{"x":"Please take a moment and have a look at the notebook here with link \"https:\/\/github.com\/udacity\/deep-learning-v2-pytorch\/blob\/master\/recurrent-neural-networks\/char-rnn\/Character_Level_RNN_Solution.ipynb\". This is a simple tutorial concerning LSTMs taught at Udemy\u2019s Pytorch Course.\nThere are two sections in this IPython notebook that confuses me greatly.\n\nWhy is it necessary to use contiguous() when using an LSTM?\nand more importantly :\nWhy doesnt the training procedure fail! becasue of the following code snippet :\n\n<code class=\"lang-auto\">    counter = 0\n    n_chars = len(net.chars)\n    for e in range(epochs):\n        # initialize hidden state\n        h = net.init_hidden(batch_size)\n        \n        for x, y in get_batches(data, batch_size, seq_length):\n            counter += 1\n            \n            # One-hot encode our data and make them Torch tensors\n            x = one_hot_encode(x, n_chars)\n            inputs, targets = torch.from_numpy(x), torch.from_numpy(y)\n            \n            if(train_on_gpu):\n                inputs, targets = inputs.cuda(), targets.cuda()\n\n            # Creating new variables for the hidden state, otherwise\n            # we'd backprop through the entire training history\n            h = tuple([each.data for each in h])\n\n            # zero accumulated gradients\n            net.zero_grad()\n            \n            # get the output from the model\n            output, h = net(inputs, h)\n            print(f'output.shape: {output.shape}')\n            print(f'y.shape :{targets.shape}')\n            print(targets[0,:])\n            # calculate the loss and perform backprop\n            loss = criterion(output, targets.view(batch_size*seq_length).long())\n            loss.backward()\n            # `clip_grad_norm` helps prevent the exploding gradient problem in RNNs \/ LSTMs.\n            nn.utils.clip_grad_norm_(net.parameters(), clip)\n            opt.step()\n            \n            # loss stats\n            if counter % print_every == 0:\n<\/code>\nWhat I\u2019m specifically refering to is this line:\n<code class=\"lang-auto\">loss = criterion(output, targets.view(batch_size*seq_length).long())\n\n<\/code>\nbasically, here the author is using a one-hot encoded output with the shape (batch, sequence_length, features) with a normal not-one-hot encoded target tensor of shape (batchsize, sequence_length)!\nwhy does it not fail? how is crossentropy doing its job when the two tensors are not both one-hot  encoded?!\nif you go and one-hot encode the targets as well, you will face the error :\nRuntimeError: multi-target not supported at C:\/w\/1\/s\/windows\/pytorch\/aten\/src\\THCUNN\/generic\/ClassNLLCriterion.cu:15\nThe usage of contiguous() seems not to do any good and the only way to get this to work seems like this!\nalso have a side question, what does  weight = next(self.parameters()).data mean?\nWhy did t he author do  :\n<code class=\"lang-auto\">weight = next(self.parameters()).data\nhidden = (weight.new(self.n_layers, batch_size, self.n_hidden).zero_(),\n                weight.new(self.n_layers, batch_size, self.n_hidden).zero_())\n<\/code>\nwhats this weight.new? how does he\/she know what parameters to send? why did not the author simply use tensor.zeros() instead and do :\n<code class=\"lang-auto\">hidden_state = torch.zeros(num_layer*direction, batch_size, hidden_size).to(device)\ncellstate = torch.zeros_like(hidden_state).to(device)\nhiddenstates = (hidden_state,cellstate)\n<\/code>\nCan anyone please explain to me what is happening here?\nI grealy appreciate it","y":"\n\n\n Shisho_Sama:\n\nbasically, here the author is using a one-hot encoded output with the shape (batch, sequence_length, features) with a normal not-one-hot encoded target tensor of shape (batchsize, sequence_length) !\n\n\nFor this part what I can tell you is that nn.CrossEntropyLoss() does take output as one-hot encoded and targets with indices that isn\u2019t encoded into one-hot. Internally it converts it into one-hot encoding and computes the loss.\nYou can find this easily on documentation and its source code.","z":"\n\n\n Shisho_Sama:\n\nbasically, here the author is using a one-hot encoded output with the shape (batch, sequence_length, features) with a normal not-one-hot encoded target tensor of shape (batchsize, sequence_length) !\n\n\nFor this part what I can tell you is that nn.CrossEntropyLoss() does take output as one-hot encoded and targets with indices that isn\u2019t encoded into one-hot. Internally it converts it into one-hot encoding and computes the loss.\nYou can find this easily on documentation and its source code."},{"x":"<code class=\"lang-auto\">class FC(nn.Module):\n    def __init__(self, opt):\n        super(FC, self).__init__()\n        self.encoder = nn.Embedding(opt.VOCAB_SIZE, opt.EMBEDDING_DIM)\n        self.gru = nn.Sequential(\n            nn.GRU(input_size=100,hidden_size=opt.LINER_HID_SIZE),\n            nn.ReLU(False),\n        )\n        self.fc_1 = nn.Sequential(\n            nn.Linear(10000, opt.LINER_HID_SIZE),\n            #nn.BatchNorm1d(opt.LINER_HID_SIZE),\n            nn.ReLU(False),\n            nn.Linear(opt.LINER_HID_SIZE, opt.NUM_CLASS_1*10),\n            nn.ReLU(False),\n            nn.Linear(opt.NUM_CLASS_1*10, opt.NUM_CLASS_1),\n            nn.Dropout(0.5),\n        )\n        self.fc_2 = nn.Sequential(\n            nn.Linear(10000, opt.LINER_HID_SIZE),\n            #nn.BatchNorm1d(opt.LINER_HID_SIZE),\n            nn.ReLU(False),\n            nn.Linear(opt.LINER_HID_SIZE, opt.NUM_CLASS_2*10),\n            nn.ReLU(False),\n            nn.Linear(opt.NUM_CLASS_2*10, opt.NUM_CLASS_2),\n            nn.Dropout(0.5),\n        )\n        self.fc_3 = nn.Sequential(\n            nn.Linear(10000, opt.LINER_HID_SIZE),\n            #nn.BatchNorm1d(opt.LINER_HID_SIZE),\n            nn.ReLU(False),\n            nn.Linear(opt.LINER_HID_SIZE, opt.NUM_CLASS_3*10),\n            nn.ReLU(False),\n            nn.Linear(opt.NUM_CLASS_3*10, opt.NUM_CLASS_3),\n            nn.Dropout(0.5),\n        )\n\n    def forward(self, x):\n        x = x.long()\n        outputs = self.encoder(x)\n        outputs = outputs.long()\n        outputs = self.gru(outputs)\n        outputs = outputs.view(outputs.size()[0], -1)\n        output_1 = self.fc_1(outputs)\n        output_2 = self.fc_2(outputs)\n        output_3 = self.fc_3(outputs)\n        return (output_1, output_2, output_3)\n<\/code>\nI consider that I have just transformed the \u2018output\u2019 to LongTensor but there is also an error 'expected object of scalar type Long but got scalar type Float for argument #2 \u2018mat2\u2019 ', I am confused and could not solve the problem, please help me!","y":"If I understand properly, the error is when you call self.gru.\nBased on the documentation https:\/\/pytorch.org\/docs\/stable\/nn.html#torch.nn.GRU, you have to pass float numbers to GRU object, so you have to remove .long() before passing outputs to self.gru.\nBy the way, can you test your model by passing hidden_intitial_state like below example in the documentation:\n<code class=\"lang-python\">rnn = nn.GRU(10, 20, 2)\ninput = torch.randn(5, 3, 10)\nh0 = torch.randn(2, 3, 20)\noutput, hn = rnn(input, h0)\n<\/code>\nIn the end, I tested a simple model and for sure you have to pass float numbers to GRU.\nPlease let me know the result.","z":"Hi, can you play add stacktrace error here? Which line is giving you this?\nUNHE]61{QX3F2RY5SBMDU`X.png1085\u00d7445 30.8 KB with link \"https:\/\/discuss.pytorch.org\/uploads\/default\/original\/2X\/a\/ad89cab6e9ce0db00377f78f6fb24ad87cdff0e7.png\"\nIt is the traceback. Thank you very much!\nIf I understand properly, the error is when you call self.gru.\nBased on the documentation https:\/\/pytorch.org\/docs\/stable\/nn.html#torch.nn.GRU, you have to pass float numbers to GRU object, so you have to remove .long() before passing outputs to self.gru.\nBy the way, can you test your model by passing hidden_intitial_state like below example in the documentation:\n<code class=\"lang-python\">rnn = nn.GRU(10, 20, 2)\ninput = torch.randn(5, 3, 10)\nh0 = torch.randn(2, 3, 20)\noutput, hn = rnn(input, h0)\n<\/code>\nIn the end, I tested a simple model and for sure you have to pass float numbers to GRU.\nPlease let me know the result.\nYou advice helps a lot. I tested the example in my program and the example ran normally.Then I changed the input into the \u2018outputs\u2019 above. The error \u2018expected float but long given\u2019 appeared. I removed the \u2018outputs = outputs.long()\u2019 and the program ran normally.\nThis is the code after modifying.\n<code class=\"lang-auto\">import sys\nimport os\nsys.path.append(os.getcwd())\nimport torch\nimport torch.nn as nn\nfrom config import Config\n\n\nclass FC(nn.Module):\n    def __init__(self, opt):\n        super(FC, self).__init__()\n        self.encoder = nn.Embedding(opt.VOCAB_SIZE, opt.EMBEDDING_DIM)\n        self.rnn = nn.GRU(100,128,2)\n        self.relu = nn.ReLU(False)\n        self.fc_1 = nn.Sequential(\n            nn.Linear(12800, opt.LINER_HID_SIZE),\n            #nn.BatchNorm1d(opt.LINER_HID_SIZE),\n            nn.ReLU(False),\n            nn.Linear(opt.LINER_HID_SIZE, opt.NUM_CLASS_1*10),\n            nn.ReLU(False),\n            nn.Linear(opt.NUM_CLASS_1*10, opt.NUM_CLASS_1),\n            nn.Dropout(0.5),\n        )\n        self.fc_2 = nn.Sequential(\n            nn.Linear(12800, opt.LINER_HID_SIZE),\n            #nn.BatchNorm1d(opt.LINER_HID_SIZE),\n            nn.ReLU(False),\n            nn.Linear(opt.LINER_HID_SIZE, opt.NUM_CLASS_2*10),\n            nn.ReLU(False),\n            nn.Linear(opt.NUM_CLASS_2*10, opt.NUM_CLASS_2),\n            nn.Dropout(0.5),\n        )\n        self.fc_3 = nn.Sequential(\n            nn.Linear(12800, opt.LINER_HID_SIZE),\n            #nn.BatchNorm1d(opt.LINER_HID_SIZE),\n            nn.ReLU(False),\n            nn.Linear(opt.LINER_HID_SIZE, opt.NUM_CLASS_3*10),\n            nn.ReLU(False),\n            nn.Linear(opt.NUM_CLASS_3*10, opt.NUM_CLASS_3),\n            nn.Dropout(0.5),\n        )\n\n    def forward(self, x):\n        x = x.long()\n        outputs = self.encoder(x)\n        h0 = torch.randn(2,100,128).cuda()\n        outputs,ht = self.rnn(outputs, h0)\n        outputs = self.relu(outputs)\n        outputs = outputs.view(outputs.size()[0], -1)\n        output_1 = self.fc_1(outputs)\n        output_2 = self.fc_2(outputs)\n        output_3 = self.fc_3(outputs)\n        return (output_1, output_2, output_3)\n\n\n<\/code>\nThanks very much again!\nYou\u2019re welcome mate.\nBut you should be careful about two things:\n\nIf you want to use your code on GPU, you just need to create an instance of your model like this:\n\n<code class=\"lang-auto\">model = FC()\nmodel.cuda()\n<\/code>\nI mean, you do not need to call  cuda  for  h0  in your  forward  method.\n\nYou should not pass  random  values as  h0  to your  GRU  object. If you do not want to initialize it (as before), pass zeros like this:\n\n<code class=\"lang-auto\">h0 = torch.zeros(2, 100, 128)\n<\/code>\nGood luck\nOh! These are undoubtedly useful ideas. I\u2019ve ignored the settings and I will correct my program."},{"x":"Below is a basic model that I want to use to learn whether a text belongs to one of two classes. Somehow it doesn\u2019t actually learn anything and gets stuck around the mean (prevalence) of the classes.\nPreprocessing\nI used Torchtext to preprocess texts into padded sequences with a fixed length of 500 and created data loaders with batch_size 64. In my case I have two classes that are mutually exclusive (i.e., each text belongs to one class).\nModel\n<code class=\"lang-auto\">class RNN(nn.Module):\n    def __init__(self, emb_dim, hidden_size, num_layers, num_classes):\n        super(RNN, self).__init__()\n        self.hidden_size = hidden_size\n        self.num_layers = num_layers\n        self.embedding = nn.Embedding(len(TEXT.vocab), emb_dim)        \n        self.lstm = nn.LSTM(emb_dim, hidden_size, num_layers, batch_first=True)\n        self.fc = nn.Linear(hidden_size, num_classes)\n\n    def forward(self, x):\n        x = self.embedding(x)\n        out, _ = self.lstm(x)\n        out = self.fc(out[-1, :, :])\n        return out\n<\/code>\nTraining\n<code class=\"lang-auto\">def train_model(model, criterion, optimizer, num_epochs=25):\n    # Loop over the range of epochs\n    i = 0\n    for epoch in range(num_epochs):\n        # Init stats for current epoch\n        running_corrects = 0\n        running_total = 0\n        running_loss = 0.0\n        \n        print('=> Epoch {}'.format(epoch + 1))\n        \n        # Train\n        model.train()\n        for inputs, labels in train_dl:\n            # Move to the GPU if possible\n            inputs = inputs.to(device)\n            labels = labels.to(device)\n                     \n            # Calculate the loss\n            outputs = model(inputs)\n            i += 1\n            loss = criterion(outputs, torch.argmax(labels, dim=1))\n            running_loss += loss.item()\n\n            running_corrects += (torch.argmax(outputs, dim=1) == torch.argmax(labels, dim=1)).sum().item()\n            running_total += outputs.size(0) # batch-size \n            \n            optimizer.zero_grad()\n            loss.backward()\n            optimizer.step()\n        \n        print('Train loss: {:.4f}, acc: {}\/{} - {:.4f}%'.format(\n            loss.item(), \n            running_corrects, \n            running_total, \n            running_corrects\/running_total))\n                             \n    return model\n<\/code>\n<code class=\"lang-auto\"># Hyper-parameters\nhidden_size = 128\nemb_size = 64\nnum_layers = 2\nbatch_size = 64\nnum_epochs = 20\nlearning_rate = 1e-3\nnum_classes = 2\n\n# Model\nmodel = RNN(emb_size, hidden_size, num_layers, num_classes).apply(weights_init_uniform_rule).to(device)\ncriterion = nn.CrossEntropyLoss()\noptimizer = torch.optim.Adam(model.parameters(), lr=learning_rate)\ntrained_model = train_model(model, criterion, optimizer, num_epochs)\n<\/code>\nOutput\n<code class=\"lang-auto\">=> Epoch 1\nTrain loss: 0.5666, acc: 9203\/12406 - 0.7418%\n=> Epoch 2\nTrain loss: 0.5607, acc: 9238\/12406 - 0.7446%\n=> Epoch 3\nTrain loss: 0.5779, acc: 9278\/12406 - 0.7479%\n=> Epoch 4\nTrain loss: 0.5647, acc: 9293\/12406 - 0.7491%\n=> Epoch 5\nTrain loss: 0.5620, acc: 9301\/12406 - 0.7497%\n=> Epoch 6\nTrain loss: 0.5798, acc: 9317\/12406 - 0.7510%\n=> Epoch 7\nTrain loss: 0.6341, acc: 9313\/12406 - 0.7507%\n=> Epoch 8\nTrain loss: 0.5561, acc: 9315\/12406 - 0.7508%\n=> Epoch 9\nTrain loss: 0.5261, acc: 9316\/12406 - 0.7509%\n=> Epoch 10\nTrain loss: 0.4997, acc: 9317\/12406 - 0.7510%\n=> Epoch 11\nTrain loss: 0.4767, acc: 9322\/12406 - 0.7514%\n=> Epoch 12\nTrain loss: 0.4280, acc: 9321\/12406 - 0.7513%\n=> Epoch 13\nTrain loss: 0.5455, acc: 9325\/12406 - 0.7517%\n=> Epoch 14\nTrain loss: 0.4680, acc: 9324\/12406 - 0.7516%\n=> Epoch 15\nTrain loss: 0.5636, acc: 9323\/12406 - 0.7515%\n=> Epoch 16\nTrain loss: 0.4159, acc: 9324\/12406 - 0.7516%\n=> Epoch 17\nTrain loss: 0.5905, acc: 9325\/12406 - 0.7517%\n=> Epoch 18\nTrain loss: 0.4072, acc: 9325\/12406 - 0.7517%\n=> Epoch 19\nTrain loss: 0.6096, acc: 9323\/12406 - 0.7515%\n=> Epoch 20\nTrain loss: 0.6035, acc: 9324\/12406 - 0.7516%\n<\/code>\nMy problem is that the loss is bouncing up and down and the accuracy is stuck around the mean of the classes. I tried modifying the learning rate (0.1 to 0.0001) but it doesn\u2019t make a difference. It seems to me that the model is not actually learning anything. Any suggestions what I might be doing wrong are highly appreciated!","y":"Are you making use of the hidden state?\nMaybe you could use the lstm like it\u2019s done in this tutorial:\nhttps:\/\/pytorch.org\/tutorials\/beginner\/nlp\/sequence_models_tutorial.html","z":"Are you making use of the hidden state?\nMaybe you could use the lstm like it\u2019s done in this tutorial:\nhttps:\/\/pytorch.org\/tutorials\/beginner\/nlp\/sequence_models_tutorial.html\nThat was the answer, thank you so much! I ended up following the advice from this discussion with link \"https:\/\/discuss.pytorch.org\/t\/how-to-correctly-give-inputs-to-embedding-lstm-and-linear-layers\/15398\".\nThe final code is now:\n<code class=\"lang-auto\">class RNN(nn.Module):\n    def __init__(self, emb_dim, hidden_size, num_layers, num_classes):\n        super(RNN, self).__init__()\n        self.hidden_size = hidden_size\n        self.num_layers = num_layers\n        self.embedding = nn.Embedding(len(TEXT.vocab), emb_dim)        \n        self.lstm = nn.LSTM(emb_dim, hidden_size, num_layers)\n        self.fc = nn.Linear(hidden_size, num_classes)\n\n    def forward(self, text):\n        embeds = self.embedding(text)\n        lstm_output, (last_hidden_state, last_cell_state) = self.lstm(embeds)\n        linear_input = last_hidden_state[-1]\n        out = self.fc(linear_input)\n        \n        return out\n<\/code>"},{"x":"I am trying to classify sequences by a binary feature. I have a dataset of sequence\/label pairs and am using a simple one-layer LSTM to classify each sequence. Before I implemented minibatching, I was getting reasonable accuracy on a test set (80%), and the training loss would go from 0.6 to 0.3 (averaged).\nI implemented minibatching, using parts of this tutorial: https:\/\/pytorch.org\/tutorials\/beginner\/chatbot_tutorial.html\nHowever, now my model won\u2019t do better than 70-72% (70% of the data has one label) with batch size set to 1 and all other parameters exactly the same. Additionally, the loss starts out at 0.0106 and quickly gets really really small, with no significant change in results. I feel like the results between no batching and batching with size 1 should be the same, so I probably have a bug, but for the life of me I can\u2019t find it. My code is below.\nTraining code (one epoch):\n<code class=\"lang-auto\">for i in t:\n    model.zero_grad()\n\n    # prep inputs\n    last = i+self.params['batch_size']\n    last = last if last < len(train_data) else len(train_data)\n    batch_in, lengths, batch_targets = self.batch2TrainData(train_data[shuffled][i:last], word_to_ix, label_to_ix)\n\n    # forward pass.\n    tag_scores = model(batch_in, lengths)\n\n    # compute loss, then do backward pass, then update gradients\n    loss = loss_function(tag_scores, batch_targets)\n    loss.backward()\n\n    # Clip gradients: gradients are modified in place\n    nn.utils.clip_grad_norm_(model.parameters(), 50.0)\n\n    optimizer.step()\n<\/code>\nFunctions:\n<code class=\"lang-auto\">def prep_sequence(self, seq, to_ix):\n    idxs = [to_ix[w] for w in seq]\n    return torch.tensor(idxs, dtype=torch.long)\n\n# transposes batch_in\ndef zeroPadding(self, l, fillvalue=0):\n    return list(itertools.zip_longest(*l, fillvalue=fillvalue))\n\n# Returns padded input sequence tensor and lengths\ndef inputVar(self, batch_in, word_to_ix):\n    idx_batch = [self.prep_sequence(seq, word_to_ix) for seq in batch_in]\n    lengths = torch.tensor([len(idxs) for idxs in idx_batch])\n    padList = self.zeroPadding(idx_batch)\n    padVar = torch.LongTensor(padList)\n    return padVar, lengths\n\n# Returns all items for a given batch of pairs\ndef batch2TrainData(self, batch, word_to_ix, label_to_ix):\n    # sort by dec length\n    batch = batch[np.argsort([len(x['turn']) for x in batch])[::-1]]\n    input_batch, output_batch = [], []\n    for pair in batch:\n        input_batch.append(pair['turn'])\n        output_batch.append(pair['label'])\n    inp, lengths = self.inputVar(input_batch, word_to_ix)\n    output = self.prep_sequence(output_batch, label_to_ix)\n    return inp, lengths, output\n<\/code>\nModel:\n<code class=\"lang-auto\">class LSTMClassifier(nn.Module):\n\n    def __init__(self, params, vocab_size, tagset_size, weights_matrix=None):\n        super(LSTMClassifier, self).__init__()\n        self.hidden_dim = params['hidden_dim']\n\n        if weights_matrix is not None:\n            self.word_embeddings = nn.Embedding.from_pretrained(weights_matrix)\n        else:\n            self.word_embeddings = nn.Embedding(vocab_size, params['embedding_dim'])\n\n        # The LSTM takes word embeddings as inputs, and outputs hidden states\n        # with dimensionality params['hidden_dim'].\n        self.lstm = nn.LSTM(params['embedding_dim'], self.hidden_dim, bidirectional=False)\n\n        # The linear layer that maps from hidden state space to tag space\n        self.hidden2tag = nn.Linear(self.hidden_dim, tagset_size)\n\n    def forward(self, batch_in, lengths):\n        embeds = self.word_embeddings(batch_in)\n        packed = nn.utils.rnn.pack_padded_sequence(embeds, lengths)\n        lstm_out, _ = self.lstm(packed)\n        outputs, _ = nn.utils.rnn.pad_packed_sequence(lstm_out)\n        tag_space = self.hidden2tag(outputs)\n        tag_scores = F.log_softmax(tag_space, dim=0)\n        return tag_scores[-1]\n<\/code>\nThanks!","y":"For anyone else with a similar issue, I got it to work. I removed the log_softmax calculation, so this:\n<code class=\"lang-auto\">tag_space = self.hidden2tag(outputs)\ntag_scores = F.log_softmax(tag_space, dim=0)\nreturn tag_scores[-1]\n<\/code>\nbecomes this:\n<code class=\"lang-auto\">tag_space = self.hidden2tag(outputs)\nreturn tag_space[-1]\n<\/code>\nI also changed NLLLoss to CrossEntropyLoss, (not shown above), and initialized CrossEntropyLoss with no parameters (aka no ignore_index).\nI am not certain why these changes were necessary (the docs even say that NLLLoss should be run after a log_softmax layer), but they got my model working and brought my loss back to a reasonable range (~0.5).","z":"For anyone else with a similar issue, I got it to work. I removed the log_softmax calculation, so this:\n<code class=\"lang-auto\">tag_space = self.hidden2tag(outputs)\ntag_scores = F.log_softmax(tag_space, dim=0)\nreturn tag_scores[-1]\n<\/code>\nbecomes this:\n<code class=\"lang-auto\">tag_space = self.hidden2tag(outputs)\nreturn tag_space[-1]\n<\/code>\nI also changed NLLLoss to CrossEntropyLoss, (not shown above), and initialized CrossEntropyLoss with no parameters (aka no ignore_index).\nI am not certain why these changes were necessary (the docs even say that NLLLoss should be run after a log_softmax layer), but they got my model working and brought my loss back to a reasonable range (~0.5)."},{"x":"Overview\nI want to use -1 as an ignore index of an embedding layers.\nBut it does not work.\nEnvironment\nPython: 3.7.1\ntorch: 1.1.0\nTo Reproduce\n<code class=\"lang-python\">embedding = nn.Embedding(10, 3, padding_idx=-1)\ntokens = torch.tensor([-1, 2]).long()\nembedding(tokens)\n<\/code>\nThe third line, in the above, shows\n<code class=\"lang-auto\">RuntimeError: index out of range at ..\/aten\/src\/TH\/generic\/THTensorEvenMoreMath.cpp:193\n<\/code>","y":"<code class=\"lang-auto\">>>> embedding = nn.Embedding(10, 3, padding_idx=-1)\n>>> tokens = torch.tensor([1, 2]).long()   # positive indexes here\n>>> embedding(tokens)\ntensor([[-0.6424, -1.2020,  0.4287],\n        [ 0.5234,  1.2113,  0.8808]], grad_fn=<EmbeddingBackward>)\n<\/code>","z":"<code class=\"lang-auto\">>>> embedding = nn.Embedding(10, 3, padding_idx=-1)\n>>> tokens = torch.tensor([1, 2]).long()   # positive indexes here\n>>> embedding(tokens)\ntensor([[-0.6424, -1.2020,  0.4287],\n        [ 0.5234,  1.2113,  0.8808]], grad_fn=<EmbeddingBackward>)\n<\/code>\nThank you for your reply.\nI think padding_idx=-1 means -1 map to [0., 0., 0.], as the example of https:\/\/pytorch.org\/docs\/stable\/nn.html#embedding, but is it not correct?\nProbably, does padding_idx=-1 mean there are no padding index?\nI think -ve padding means negative indexing.\nJust set\n<code class=\"lang-auto\">tokens = torch.tensor([3, 2]).long()\n<\/code>\nThis may be the embedding index problem <0.\nNegative is bad in here.\nAlso, by default is long so you don\u2019t need to use long()"},{"x":"I received the following error:\nFailedPreconditionError: \/home\/timucin\/Thesis\/Tagger-master8\/train; Is a directory\n[[{{node input_queues\/examples_queue\/parallel_read\/ReaderReadV2}} = ReaderReadV2[_device=\"\/job:localhost\/replica:0\/task:0\/device:CPU:0\"](input_queues\/examples_queue\/parallel_read\/TFRecordReaderV2, input_queues\/examples_queue\/parallel_read\/filenames)]]\nwhile running a training session of semantic role labeling. I`m using python 2.7 (anaconda) with TensorFlow 1.12 on Ubuntu 18.04.\nWhen I first run the script, train directory is created to store checkpoints and the session terminates with no error.\nDuring the second trial, with the train directory already created, the script looks at the checkpoints to assess whether there are new steps on top of already completed ones and terminates with the above mentioned error. When I empty the content of the directory it behaves the same way and terminates with error. Upon deleting the directory it runs without any problem.\nI would appreciate to receive your feedback\u2026\nThanks in advance,\nTimucin\nTrace back:\nINFO:tensorflow:Total trainable variables size: 8187305\nINFO:tensorflow:Create CheckpointSaverHook.\nINFO:tensorflow:Graph was finalized.\nINFO:tensorflow:Restoring parameters from train\/model.ckpt-10\nINFO:tensorflow:Running local_init_op.\nINFO:tensorflow:Done running local_init_op.\nINFO:tensorflow:Error reported to Coordinator: <class \u2018tensorflow.python.framework.errors_impl.FailedPreconditionError\u2019>, \/home\/timucin\/Thesis\/Tagger-master8\/train; Is a directory\n[[{{node input_queues\/examples_queue\/parallel_read\/ReaderReadV2}} = ReaderReadV2[_device=\"\/job:localhost\/replica:0\/task:0\/device:CPU:0\"](input_queues\/examples_queue\/parallel_read\/TFRecordReaderV2, input_queues\/examples_queue\/parallel_read\/filenames)]]\nINFO:tensorflow:Saving checkpoints for 10 into train\/model.ckpt.\nTraceback (most recent call last):\nFile \"<ipython-input-6-5b359d738d7d>\", line 1, in <module>\nrunfile(\u2019\/home\/timucin\/Thesis\/Tagger-master8\/main.py\u2019, args=\u2018train --model_name deepatt --vocab_path \/home\/timucin\/Thesis\/Tagger-master-old\/word_dict \/home\/timucin\/Thesis\/Tagger-master-old\/label_dict --data_path \/home\/timucin\/Thesis\/Tagger-master8\/ --model_dir train --model_params=feature_size=100,hidden_size=200,filter_size=800,residual_dropout=0.2,num_hidden_layers=10,attention_dropout=0.1,relu_dropout=0.1 --training_params=batch_size=4096,eval_batch_size=1024,optimizer=Adadelta,initializer=orthogonal,use_global_initializer=false,initializer_gain=1.0,train_steps=15,learning_rate_decay=piecewise_constant,learning_rate_values=[1.0,0.5,0.25],learning_rate_boundaries=[400000,500000],device_list=[0],clip_grad_norm=1.0\u2019, wdir=\u2019\/home\/timucin\/Thesis\/Tagger-master8\u2019)\nFile \"\/home\/timucin\/anaconda2\/lib\/python2.7\/site-packages\/spyder_kernels\/customize\/spydercustomize.py\", line 704, in runfile\nexecfile(filename, namespace)\nFile \"\/home\/timucin\/anaconda2\/lib\/python2.7\/site-packages\/spyder_kernels\/customize\/spydercustomize.py\", line 100, in execfile\nbuiltins.execfile(filename, *where)\nFile \"\/home\/timucin\/Thesis\/Tagger-master8\/main.py\", line 892, in <module>\ntrain(parsed_args)\nFile \"\/home\/timucin\/Thesis\/Tagger-master8\/main.py\", line 540, in train\nexperiment.train()\nFile \"\/home\/timucin\/anaconda2\/lib\/python2.7\/site-packages\/tensorflow\/contrib\/learn\/python\/learn\/experiment.py\", line 389, in train\nsaving_listeners=self._saving_listeners)\nFile \"\/home\/timucin\/anaconda2\/lib\/python2.7\/site-packages\/tensorflow\/contrib\/learn\/python\/learn\/experiment.py\", line 886, in _call_train\ninput_fn=input_fn, steps=steps, max_steps=max_steps, monitors=hooks)\nFile \"\/home\/timucin\/anaconda2\/lib\/python2.7\/site-packages\/tensorflow\/python\/util\/deprecation.py\", line 488, in new_func\nreturn func(*args, **kwargs)","y":"Problem is solved. Was due to the improper organization of the directory structure\u2026","z":"This is PyTorch forums, answering Tensorflow queries can be a bit difficult.\nIndeed this is correct but I guess the problem is not related with any of those libraries but rather with file I\/O.\nBut in any case this is good idea to post it to another platform like askUbuntu\u2026\nThanks any way\u2026\nProblem is solved. Was due to the improper organization of the directory structure\u2026\ncould you tell me how you solved it concretely?\ni meet a similar problem.\nthanks a lot!"},{"x":"I want to know how GRU is implemented in pytorch. But I can\u2019t find the implementation of GRU although I looked in here with link \"https:\/\/github.com\/pytorch\/pytorch\/tree\/949559552004db317bc5ca53d67f2c62a54383f5\/aten\/src\/THNN\". Can anyone tell me where did GRU implement in pytorch?","y":"The \u201cnative\u201d implantations are in\nhttps:\/\/github.com\/pytorch\/pytorch\/blob\/master\/aten\/src\/ATen\/native\/RNN.cpp and\nhttps:\/\/github.com\/pytorch\/pytorch\/blob\/master\/aten\/src\/ATen\/native\/cuda\/RNN.cu\nThere also are CuDNN bindings\u2026","z":"Hi,\nAre you looking for the source code? If so you can find it here with link \"https:\/\/github.com\/pytorch\/pytorch\/blob\/master\/torch\/nn\/modules\/rnn.py\".\nThe \u201cnative\u201d implantations are in\nhttps:\/\/github.com\/pytorch\/pytorch\/blob\/master\/aten\/src\/ATen\/native\/RNN.cpp and\nhttps:\/\/github.com\/pytorch\/pytorch\/blob\/master\/aten\/src\/ATen\/native\/cuda\/RNN.cu\nThere also are CuDNN bindings\u2026\nThank  for your reply.\nI am a novice in C+ and although I looked for the how does narrow works, I still do not know clearly what does hidden_slice return.\n<code class=\"lang-auto\">Tensor hidden_slice(const Tensor&amp; t, int64_t start, int64_t end) {\n  return t.narrow(0, start, end - start);\n}\ntpair_of<Tensor> hidden_slice(const tpair_of<Tensor>&amp; t, int64_t start, int64_t end) {\n  return std::make_tuple(hidden_slice(std::get<0>(t), start, end),\n                         hidden_slice(std::get<1>(t), start, end));\n}\n<\/code>\nCan you please explain me what does hidden_slice return.\nSo it\u2019s overloaded: calling it with a pair (second variant) will return a tuple with sliced elements by calling the first variant on each of the two members.\nBest regards\nThomas\nThank you  "},{"x":"I have a matrix A with dimension [batch_size,N,M,D] and another matrix B with dimension [batch_size,P,D]. I want to get a tensor C as output with dimension [batch_size,N,M,P] in the following way:\nC[i,j] = matrix_dot_product(A[i,j], B[i]) 0<=i<batch_size, 0<=j<N\nWhat is the most memory-efficient way to do this?\nThanks!","y":"You can use Einstein summation.\n<code class=\"lang-python\">import torch\nbatch_size, N, M, D, P = 4, 5, 6, 7, 8\nA = torch.rand(batch_size, N, M, D)\nB = torch.rand(batch_size, P, D)\nprint(torch.einsum('bnmd,bpd -> bnmp', [x, w]).size()) # batch_size, N, M, P\n<\/code>","z":"You can use Einstein summation.\n<code class=\"lang-python\">import torch\nbatch_size, N, M, D, P = 4, 5, 6, 7, 8\nA = torch.rand(batch_size, N, M, D)\nB = torch.rand(batch_size, P, D)\nprint(torch.einsum('bnmd,bpd -> bnmp', [x, w]).size()) # batch_size, N, M, P\n<\/code>\nIt does not seem that it is giving the correct result.\n<code class=\"lang-auto\">> C = torch.zeros(batch_size, N,M,P)\n> for i in range(batch_size):\n>     for j in range(N):\n>         C[i,j] = torch.mm(A[i,j],B[i].transpose(0,1))\n> is_same = C==torch.einsum('bnmd,bpd -> bnmp', [A, B])\n<\/code>\nis_same is not all 1\u2019s.\nIt works just fine for me:\n<code class=\"lang-python\">import torch\nbatch_size, N, M, D, P = 2, 2, 2, 2, 3\nA = torch.arange(16).view(batch_size, N, M, D)\nB = torch.arange(16, 28).view(batch_size, P, D)\nC = torch.zeros(batch_size, N, M, P)\nfor i in range(batch_size):\n    for j in range(N):\n        C[i,j] = torch.mm(A[i,j],B[i].transpose(0,1))\n\nF = torch.einsum('bnmd,bpd -> bnmp', [A, B])\n\nprint(F.equal(C.long())) # Prints True\n<\/code>\n\n\n\n LeviViana:\n\nF = torch.einsum(\u2018bnmd,bpd -> bnmp\u2019, [A, B]) print(F.equal(C.long()))\n\n\nMy bad. Made a silly mistake before. Thank you."},{"x":"I get this error size mismatch, m1: [50 x 52480], m2: [256 x 2] and try to fix it by change linear dim to [52480 x 2] it can train but always get 0 loss\nHere is my code.\n<code class=\"lang-auto\">class LSTMtagger(nn.Module):\n\n    def __init__(self, vocab_size, output_size, embedding_dim, hidden_dim, num_layers):\n        super(LSTMtagger, self).__init__()\n\n        self.word_embeddings = nn.Embedding(vocab_size, embedding_dim)\n\n        # The LSTM takes word embeddings as inputs, and outputs hidden states\n        # with dimensionality hidden_dim.\n        self.lstm = nn.LSTM(embedding_dim, hidden_dim, num_layers, batch_first=True, dropout = 0.5)\n\n        self.dropout = nn.Dropout(p=0.3)\n        # The linear layer that maps from hidden state space to tag space\n        self.fc = nn.Linear(hidden_dim, output_size)\n        \n        self.sigmoid = nn.Sigmoid()\n\n    def forward(self, sentence):\n        print(sentence.size())\n        embeds = self.word_embeddings(sentence)\n        print(embeds.size())\n        lstm_out, _ = self.lstm(embeds.view(len(sentence), 205, -1))\n        print(lstm_out.size())\n        d_out = self.dropout(lstm_out)\n       \n        \n#         d_out = d_out.view(50*256,205,1)\n#         print(d_out.size())\n        tag_space = self.fc(d_out.view(len(sentence), -1))\n        tag_scores = F.sigmoid(tag_space)\n        print(tag_scores)\n        return tag_scores\n<\/code>\n<code class=\"lang-auto\">vocab_size = len(vocab_to_int) + 1 # +1 for the 0 padding\noutput_size = 2\nembedding_dim = 205\nhidden_dim = 256\nnum_layers = 2\n\nnet = LSTMtagger(vocab_size, output_size, embedding_dim, hidden_dim, num_layers)\nprint(net)\n<\/code>\n<code class=\"lang-auto\">model = LSTMtagger(vocab_size, output_size, embedding_dim, hidden_dim, num_layers)\nloss_function = nn.NLLLoss()\noptimizer = optim.SGD(model.parameters(), lr=0.1)\n\n<\/code>","y":"Your current code throws an error, if I pass an input of shape [50, 256] to it in the self.lstm layer.\nFor the batch_first=True argument, the input should be [batch_size, seq, feature], so just passing embeds without the view gets rid of this error.\nIf you would like to use the output features of all time samples, setting in_features=256*256 should work, as you already tried.\nThat being said, if you are using nn.NLLLoss, you should apply F.log_softmax on your output instead of F.sigmoid.\nCould you change that and try to train your model again?","z":"Your current code throws an error, if I pass an input of shape [50, 256] to it in the self.lstm layer.\nFor the batch_first=True argument, the input should be [batch_size, seq, feature], so just passing embeds without the view gets rid of this error.\nIf you would like to use the output features of all time samples, setting in_features=256*256 should work, as you already tried.\nThat being said, if you are using nn.NLLLoss, you should apply F.log_softmax on your output instead of F.sigmoid.\nCould you change that and try to train your model again?"},{"x":"I am trying to setup a simple RNN using LSTM. I am facing issue with passing the hidden state of RNN from one batch to another. For example, if my batch_size = 64, and I am using batch_first = True, hidden_size = 100 and nlayers = 3.\n<code class=\"lang-auto\">__init__():\n    self.rnn = nn.LSTM(input_size = 40, hidden_size=100, num_layers = 3, batch_first=True)\n    self.hidden = None\n..\n..\nforward():\n    out_pkd, (h, c) = self.rnn(in_pkd, self.hidden)\n    self.hidden = (h.detach(), c.detach())\n<\/code>\nThen the shape of my hidden state is [3, 64, 100]. Now if my last batch is smaller than the batch_size 64, say 55, then I get the following error because the hidden state from the second last batch was of size [3, 64, 100].\nRuntimeError: Expected hidden[0] size (3, 55, 100), got (3, 64, 100)\nAbove error occurs at line:\n<code class=\"lang-auto\">out_pkd, (h, c) = self.rnn(in_pkd, self.hidden)\n<\/code>\nHow do I handle the case of smaller batch size for last batch in this case?","y":"You need to reset\/initialize your hidden state before each forward pass, in this step you can adjust it to your current batch size. The most common pattern is to have a method in your model class like:\ndef init_hidden(self, batch_size):\n    if self.rnn_type == 'gru':\n        return torch.zeros(self.num_layers * self.directions_count, batch_size, self.rnn_hidden_dim).to(self.device)\n    elif self.rnn_type == 'lstm':\n        return (torch.zeros(self.num_layers * self.directions_count, batch_size, self.rnn_hidden_dim).to(self.device),\n                torch.zeros(self.num_layers * self.directions_count, batch_size, self.rnn_hidden_dim).to(self.device))\n    else:\n        raise Exception('Unknown rnn_type. Valid options: \"gru\", \"lstm\"')\n\nNote that this one is already flexible enough to support different types of RNNs, different number of layers, and whether the RNN layer is bidirectional or not. The method can be much simple if you don\u2019t need this.\nAssuming that you call forward() with a parameter inputs that contains the current batch with a shape (batch_size, ...). You can call self.init_hidden(inputs[0]) as first statement in your forward() method \u2013 or before the forward() method in the loop that iterates over the batches.\nThat should fix your problem.","z":"You need to reset\/initialize your hidden state before each forward pass, in this step you can adjust it to your current batch size. The most common pattern is to have a method in your model class like:\ndef init_hidden(self, batch_size):\n    if self.rnn_type == 'gru':\n        return torch.zeros(self.num_layers * self.directions_count, batch_size, self.rnn_hidden_dim).to(self.device)\n    elif self.rnn_type == 'lstm':\n        return (torch.zeros(self.num_layers * self.directions_count, batch_size, self.rnn_hidden_dim).to(self.device),\n                torch.zeros(self.num_layers * self.directions_count, batch_size, self.rnn_hidden_dim).to(self.device))\n    else:\n        raise Exception('Unknown rnn_type. Valid options: \"gru\", \"lstm\"')\n\nNote that this one is already flexible enough to support different types of RNNs, different number of layers, and whether the RNN layer is bidirectional or not. The method can be much simple if you don\u2019t need this.\nAssuming that you call forward() with a parameter inputs that contains the current batch with a shape (batch_size, ...). You can call self.init_hidden(inputs[0]) as first statement in your forward() method \u2013 or before the forward() method in the loop that iterates over the batches.\nThat should fix your problem.\nThanks, this helps. I actually also want to learn the initial hidden state sent to the LSTM by wrapping self.hidden in nn.Parameter. In that case, I don\u2019t think it would be right to reinit with zeros each time, right?\nIf then I initialized self.hidden to\n<code class=\"lang-auto\">self.hidden = nn.Parameter(torch.zeros(self.num_layers * self.directions_count, batch_size, self.rnn_hidden_dim))\n<\/code>\nand then did backprop for current batch, how do I feed this same hidden state to the LSTM during next batch?\nYou can re-init the hidden state with 0s each time; it\u2019s a common practice. For learning a (maybe) better initial hidden state, you can have a look at this earlier post with link \"https:\/\/discuss.pytorch.org\/t\/learn-initial-hidden-state-h0-for-rnn\/10013\".\nThanks a lot. That post was quite helpful.\n\nI tried to use your code of passing hidden states accross batches.\nI see that i will face the last batch issue as well. But let\u2019s keep that aside for now.\nI tried to debug using pycharm. And I observed that self.hidden was getting reinitialized to None everytime.\nBatch 1 : Starts with None , than changes according to (h,c)\nBatch 2 : Starts with None again :? How is this possible ?\nHi,  I still don\u2019t get it - why it is right to pass 0s as hidden state each time? Thanks!\nThere\u2019s nothing inherently special about 0s, it\u2019s merely a way to represent \u201cno prior knowledge\u201d.\nFor example, when you want to classify individual sentences, your batches are completely independent \u2013 that is, the outcome for the sentences in Batch 2 should not depend in the outcome of Batch 1. Of course, if you\u2019re sentences depend on each other, then you don\u2019t want to re-initialize for each batch.\nYou don\u2019t have to re-initialize to 0s anyway. You can always try without (you probably still have to detach it after each batch) and see how it effects the results. You can also re-initialize with random values for each batch."},{"x":"Given a 3d tenzor, say: batch x sentence length x embedding dim\n<code class=\"lang-auto\">a = torch.rand((10, 1000, 96)) \n<\/code>\nand an array(or tensor) of actual lengths for each  sentence\n<code class=\"lang-auto\">lengths =  torch .randint(1000,(10,))\n\noutputs tensor([ 370., 502., 652., 859., 545., 964., 566., 576.,1000., 803.])\n<\/code>\nHow to fill tensor \u2018a\u2019 with zeros after certain index along dimension 1 (sentence length) according to tensor \u2018lengths\u2019  ?\nI want smth like that :\n<code class=\"lang-auto\">a[ : , lengths : , : ]  = 0\n<\/code>\nOne way of doing it (slow if batch size is big enough):\n<code class=\"lang-auto\">for i, length in enumerate( lengths ):\n    a[ i , length  : , : ]  = 0<\/code>","y":"This works:\n<code class=\"lang-python\">>>> import torch\n>>> a = torch.rand(2, 3, 4)\n>>> print(a)\ntensor([[[0.5066, 0.5184, 0.1193, 0.6062],\n         [0.4995, 0.1689, 0.6175, 0.7917],\n         [0.7996, 0.0225, 0.1145, 0.4249]],\n\n        [[0.0975, 0.2995, 0.5857, 0.0806],\n         [0.4922, 0.4778, 0.9133, 0.1418],\n         [0.6594, 0.4907, 0.3268, 0.1211]]])\n>>> l = torch.randint(3, (2, ))\n>>> print(l)\ntensor([1, 2])\n>>> l += torch.arange(2) * 3\n>>> b = torch.arange(6).view(2, 3)\n>>> b = b < l.view(2, 1)\n>>> a * b.view(2, 3, 1).float()\ntensor([[[0.5066, 0.5184, 0.1193, 0.6062],\n         [0.0000, 0.0000, 0.0000, 0.0000],\n         [0.0000, 0.0000, 0.0000, 0.0000]],\n\n        [[0.0975, 0.2995, 0.5857, 0.0806],\n         [0.4922, 0.4778, 0.9133, 0.1418],\n         [0.0000, 0.0000, 0.0000, 0.0000]]])\n>>> \n<\/code>\nI hope it helps !","z":"Hi,\nExcuse me, if you have a tensor like this:\n<code class=\"lang-auto\">tensor([[[ 1.1937, -0.7235, -0.1802, -0.5610],\n         [-0.7524,  1.3047, -1.5577,  1.8352],\n         [ 1.1573, -1.8952,  0.4175, -0.2085]],\n\n        [[ 0.4069, -0.1069, -0.3838, -0.2991],\n         [-0.5824, -0.4965, -0.1542,  1.1482],\n         [ 0.5182,  0.5445,  1.1730, -0.2523]]])\n<\/code>\nand lengths = torch.tensor([1, 2])\ndo you want to have sth like this?\n<code class=\"lang-auto\">tensor([[[ 1.1937, -0.7235, -0.1802, -0.5610],\n         [ 0.0000,  0.0000,  0.0000,  0.0000],\n         [ 1.1573, -1.8952,  0.4175, -0.2085]],\n\n        [[ 0.4069, -0.1069, -0.3838, -0.2991],\n         [-0.5824, -0.4965, -0.1542,  1.1482],\n         [ 0.0000,  0.0000,  0.0000,  0.0000]]])\n<\/code>\nor sth like this:\n<code class=\"lang-auto\">tensor([[[ 1.4429, -0.8365, -0.2565,  0.2319],\n         [ 0.0000,  0.0000,  0.0000,  0.0000],\n         [ 0.0000,  0.0000,  0.0000,  0.0000]],\n\n        [[ 1.7247,  0.2831,  0.0687,  0.6557],\n         [ 0.5110,  1.7607,  0.8107,  0.3624],\n         [ 0.0000,  0.0000,  0.0000,  0.0000]]])\n<\/code>\nThanks\nI want second option :\n<code class=\"lang-auto\">tensor([[[ 1.4429, -0.8365, -0.2565,  0.2319],\n         [ 0.0000,  0.0000,  0.0000,  0.0000],\n         [ 0.0000,  0.0000,  0.0000,  0.0000]],\n\n        [[ 1.7247,  0.2831,  0.0687,  0.6557],\n         [ 0.5110,  1.7607,  0.8107,  0.3624],\n         [ 0.0000,  0.0000,  0.0000,  0.0000]]])\n<\/code>\nThis works:\n<code class=\"lang-python\">>>> import torch\n>>> a = torch.rand(2, 3, 4)\n>>> print(a)\ntensor([[[0.5066, 0.5184, 0.1193, 0.6062],\n         [0.4995, 0.1689, 0.6175, 0.7917],\n         [0.7996, 0.0225, 0.1145, 0.4249]],\n\n        [[0.0975, 0.2995, 0.5857, 0.0806],\n         [0.4922, 0.4778, 0.9133, 0.1418],\n         [0.6594, 0.4907, 0.3268, 0.1211]]])\n>>> l = torch.randint(3, (2, ))\n>>> print(l)\ntensor([1, 2])\n>>> l += torch.arange(2) * 3\n>>> b = torch.arange(6).view(2, 3)\n>>> b = b < l.view(2, 1)\n>>> a * b.view(2, 3, 1).float()\ntensor([[[0.5066, 0.5184, 0.1193, 0.6062],\n         [0.0000, 0.0000, 0.0000, 0.0000],\n         [0.0000, 0.0000, 0.0000, 0.0000]],\n\n        [[0.0975, 0.2995, 0.5857, 0.0806],\n         [0.4922, 0.4778, 0.9133, 0.1418],\n         [0.0000, 0.0000, 0.0000, 0.0000]]])\n>>> \n<\/code>\nI hope it helps !\n\n\n\n LeviViana:\n\nb = b < l.view(2, 1)\n\n\nWhat does this command do?\nIt creates a binary tensor whose values are calculated with respect to the comparison of l and b. The .view(2, 1) is for broadcasting purposes.\n<code class=\"lang-python\">>>> import torch\n>>> l = torch.randint(3, (2, ))\n>>> l # the original form of the lengths\ntensor([2, 1])\n>>> l += torch.arange(2) * 3\n>>> l # the modified form of the lengths for the comparison to work\ntensor([2, 4])\n>>> b = torch.arange(6).view(2, 3)\n>>> b\ntensor([[0, 1, 2],\n        [3, 4, 5]])\n>>> b < l.view(2, 1)\ntensor([[1, 1, 0],\n        [1, 0, 0]], dtype=torch.uint8)\n<\/code>\nThe trick is to use torch.arange and then compare it to the indices you want to create the effect of zeroing starting from a given point. I hope it is clear !\nThanks LeviViana with link \"https:\/\/discuss.pytorch.org\/u\/LeviViana\"\nNice solution\nI also found a bit different approach here:\n\n\nstackoverflow.com with link \"https:\/\/stackoverflow.com\/questions\/57548180\/filling-torch-tensor-with-zeros-after-certain-index\/57550753#57550753\"\n\n\n with link \"https:\/\/stackoverflow.com\/users\/1714410\/shai\"\n\nFilling torch tensor with zeros after certain index with link \"https:\/\/stackoverflow.com\/questions\/57548180\/filling-torch-tensor-with-zeros-after-certain-index\/57550753#57550753\"\n\n\npython, nlp, pytorch\n\n\n  \n  answered by\n  Shai with link \"https:\/\/stackoverflow.com\/users\/1714410\/shai\"\n  on 05:07AM - 19 Aug 19 UTC with link \"https:\/\/stackoverflow.com\/questions\/57548180\/filling-torch-tensor-with-zeros-after-certain-index\/57550753#57550753\"\n\n\n\n\n\n\n"},{"x":"What is the order of the hidden and cell states in the tuple that is returned by LSTM? Particularly, in the word LM model, in generate.py , we have:\nwith open(args.outf, 'w') as outf:\n    with torch.no_grad():  # no tracking history\n        for i in range(args.words):\n            output, hidden = model(input, hidden)\n            ....\n            ......\n\nIn the above code, hidden is a tuple with two tensors, each of same shape. Since this is an LSTM, it returns both h and c states. But, how is h and c ordered in the tuple?\nMy understanding is this:\nh, c = hidden\n\nIs this correct? Or is it the other way around?\nThanks!","y":"Hi. The outputs for the LSTM is shown in the attached figure. \nThe output shape for h_n would be (num_layers * num_directions, batch, hidden_size). This is basically the output for the last timestep. Your output is (2,1,1500) so you are using 2 layers*1 (unidirectional) , 1 sample and a hidden size of 1500).\nNow the LSTM would return for you output, (h_n, c_n). In your case, (h_n, c_n) is named hidden. So by indexing hidden you can extract the h_n and c_n (i.e hidden[0] = h_n and hidden[1] = c_n)","z":"It\u2019s correct if it\u2019s not an LSTM Cell. If you\u2019r using nn.LSTM, then this would return output, (h_n, c_n), however if your using nn.LSTMCell, then this would return h_1, c_1. The order is the hidden state and then the cell state. So in your case, hidden would be equal to (h_n, c_n).\nThank you!\nYes, I\u2019m using nn.LSTM with 2 hidden layers (unidirectional). So, I also want to ask about the shape of h_n specifically in this case. For example, I get the shape of h_n as torch.Size([2, 1, 1500])\nIf I understand it correctly, I can get the hidden states of the hidden layers by indexing into them in the order:\nfirst_hidden_layer_hidden_state = h_n[0]    # torch.Size([1, 1500])\nsecond_hidden_layer_hidden_state = h_n[-1]    # torch.Size([1, 1500])\n\nIs this a correct way of indexing and getting the hidden states?\nP.S.: I\u2019m just interested only in these hidden states because I want to pass them to a downstream network for further processing.\nHi. The outputs for the LSTM is shown in the attached figure. \nThe output shape for h_n would be (num_layers * num_directions, batch, hidden_size). This is basically the output for the last timestep. Your output is (2,1,1500) so you are using 2 layers*1 (unidirectional) , 1 sample and a hidden size of 1500).\nNow the LSTM would return for you output, (h_n, c_n). In your case, (h_n, c_n) is named hidden. So by indexing hidden you can extract the h_n and c_n (i.e hidden[0] = h_n and hidden[1] = c_n)\nThank you for clear explanation. It was very helpful! I just want to confirm one point again:\nSince I\u2019m using 2 layer LSTM network, I want to extract hidden state of the last hidden layer. Is the following way of indexing correct way to do:\n\nfirst_hidden_layer_hidden_state = h_n[0]    # torch.Size([1, 1500])\nsecond_hidden_layer_hidden_state = h_n[-1]    # torch.Size([1, 1500])\n\nIf the above is correct, I\u2019m interested in second_hidden_layer_hidden_state.\n\nAlso, is it analogous for the cell state as well?\n\nfirst_hidden_layer_cell_state = c_n[0]    # torch.Size([1, 1500])\nsecond_hidden_layer_cell_state = c_n[-1]    # torch.Size([1, 1500])\n\nThank you!\nTo retrieve the hidden or cell states of the last (or any of the) hidden layers, the diagram in whats-the-difference-between-hidden-and-output-in-pytorch-lstm with link \"https:\/\/stackoverflow.com\/a\/48305882\"  is very helpful and makes it easy to understand.\nIn short, the number ranges from [0, 1, 2, 3, \u2026, n] for [hidden_layer_1, hidden_layer_2, hidden_layer_3, \u2026, hidden_layer_n] for n hidden layers.\nSo, with two hidden layers, the hidden  and cell states of the second (or last) hidden layer can be extracted using:\n\n\n\n kmario23:\n\nfirst_hidden_layer_hidden_state = h_n[0] # torch.Size([1, 1500])\nsecond_hidden_layer_hidden_state = h_n[-1] # torch.Size([1, 1500])\n\n\nanalogously for the cell state:\n\n\n\n kmario23:\n\nfirst_hidden_layer_cell_state = c_n[0] # torch.Size([1, 1500])\nsecond_hidden_layer_cell_state = c_n[-1] # torch.Size([1, 1500])\n\n"},{"x":"The forward function of torch.nn.parallel.DataParallel calls its member function \u201cscatter\u201d to replicate the input data into all of the devices:\n<code class=\"lang-auto\">class DataParallel(Module):\n    def forward(self, replicate_model=True, gather_grad=True, *inputs, **kwargs):\n        inputs, kwargs = self.scatter(gather_grad, inputs, kwargs, self.device_ids)\n        ...\n<\/code>\nAnd the scatter function was finally implemented by the module torch.nn.parallel._functions.Scatter like this:\n<code class=\"lang-auto\">class Scatter(Function):\n    \n    def forward(ctx, target_gpus, chunk_sizes, dim, input):\n       ...\n\n    \n    def backward(ctx, *grad_output):\n        return None, None, None, Gather.apply(ctx.input_device, ctx.dim, *grad_output)\n<\/code>\nSo I think at the backward process of DataParallel, the backward function of module Scatter should be called. And in this way it could gather all of the gradients distributed from every device.\nBut when I try to do something in the backward function of Scatter, just like print a line:\n<code class=\"lang-auto\">class Scatter(Function):\n    \n    def backward(ctx, *grad_output):\n        print(\"try to print something.\")\n        return None, None, None, Gather.apply(ctx.input_device, ctx.dim, *grad_output)\n<\/code>\nI always got nothing printed. It seems that Scatter::backward() function has never been called.\nI wonder why the function is not been called and then how DataParallel gather the gradients from all of the devices? Is there anything wrong with my testing?\nThanks very much!","y":"I found it is my misunderstood.\nIt is Broadcast::backward()  rather than Scatter::backward() gathers the grads from all of the devices.","z":"I found it is my misunderstood.\nIt is Broadcast::backward()  rather than Scatter::backward() gathers the grads from all of the devices."},{"x":"How can I input a sentence and extract the feature vector from the layer before the last layer from BERT? I think it should be length 768.","y":"You can look at what the BertForSequenceClassification model does in it\u2019s forward with link \"https:\/\/github.com\/huggingface\/pytorch-pretrained-BERT\/blob\/3ba5470eb85464df62f324bea88e20da234c423f\/pytorch_pretrained_bert\/modeling.py#L867\".\nThe pooled_output obtained from self.bert would seem to be the features you are looking for.\nBest regards\nThomas","z":"what do you mean by\n\n\u201cHow can I input a sentence and extract the feature vector from the layer before the last layer from BERT?\u201d\n\nDo you want someone to give you exact code to do that?\nI see this https:\/\/github.com\/huggingface\/pytorch-pretrained-BERT\nbut I am not sure how I can extract features with it.\nFor example, I can give an image to resnet50 and extract the vector of length 2048 from the layer before softmax.\nI am not sure how to do this for pretrained BERT.\nAlso, I am not sure if the link is what I am supposed to use.\nYes, some ready code maybe online?\nYou can look at what the BertForSequenceClassification model does in it\u2019s forward with link \"https:\/\/github.com\/huggingface\/pytorch-pretrained-BERT\/blob\/3ba5470eb85464df62f324bea88e20da234c423f\/pytorch_pretrained_bert\/modeling.py#L867\".\nThe pooled_output obtained from self.bert would seem to be the features you are looking for.\nBest regards\nThomas\nI also ran the following code https:\/\/github.com\/huggingface\/pytorch-pretrained-BERT\/blob\/master\/examples\/extract_features.py\nthough it does not seem very straightforward to interpret the output:\n<code class=\"lang-auto\">$ python extract_features.py --input_file test_bert.txt --output_file out_bert.txt --bert_model bert-base-uncased\n$ cat test_bert.txt \nMy name is mona jalal and I live in boston\n<\/code>\n\n\n\n Mona_Jalal:\n\nI am not sure how to do this for pretrained BERT.\nAlso, I am not sure if the link is what I am supposed to use.\nYes, some ready code maybe online?\n\n\nAs a general expectation, I think this might be a lot to expect from a fellow community member (who is spending their time doing work for you). There should be some effort from your side to understand the code \/ repository as well. That particular repository is in fact one of the best documented and coded one."},{"x":"I am really new to pytorch and want to learn lstm for sentence classification. So I coded a lstm shown below( I have pretrained glove embeddings)\n<code class=\"lang-auto\">class FirstLSTM(nn.Module):\n    def __init__(self,max_features,max_len,embed_dim,lstm_units,embedding_weights):\n        super(FirstLSTM,self).__init__()\n        \n        # the variables are defined outside, so to make use of them in other methods , \n        # I am defining them in init..\n        \n        self.max_features=max_features                           # vocab size\n        self.max_len=max_len                                     # sequence length\n        self.embed_dim=embed_dim                                 # embedding dimension\n        self.lstm_units=lstm_units                               # lstm output size\n        self.num_layers=1                                       # number of layers in lstm\n        self.embeddings_matrix=torch.tensor(embedding_weights)  # converting them to tensors\n        self.out=1                                             # dimension of the output\n        \n        # now we have defined the embedding layer\n        # this is the classic way to define.....\n#         self.embed_layer=nn.Embedding(num_embeddings=self.max_features,\n#                                       embedding_dim=self.embed_dim)\n        \n        # but we have to use a pretrained embedding\n        # below shows the way for this\n        self.embed_layer=nn.Embedding.from_pretrained(self.embeddings_matrix,freeze=True)\n        \n        # lstm layer\n        # note that I made batch_first = True to replicate with keras...\n        self.lstm_layer=nn.LSTM(input_size=self.embed_dim,hidden_size=self.lstm_units,\n                                   num_layers=self.num_layers,batch_first=True)\n        \n        self.output_layer=nn.Linear(in_features=self.lstm_units,out_features=self.out)\n        \n    # this will return the initial hidden state\n    # The axes semantics are (num_layers, minibatch_size, hidden_dim) even batch_first=True\n    # as there are two hidden states we will just return the tuple\n    # https:\/\/pytorch.org\/tutorials\/beginner\/nlp\/sequence_models_tutorial.html\n    def initialize_hidden_state(self,batch_size):\n        return (Variable(torch.zeros((self.num_layers,batch_size,self.lstm_units)).cuda()),\n                Variable(torch.zeros((self.num_layers,batch_size,self.lstm_units)).cuda()))\n    \n    def forward(self,X):\n        batch_shape=X.shape[0]\n        initial_hidden_state=self.initialize_hidden_state(batch_shape)\n        X=Variable(X.cuda())\n        \n        # forward pass\n        \n        # actually embedding layer needs the tensor of type long, but we pass as an int\n        # so I will typecaste the long\n        embeds=self.embed_layer(X.long())\n        output,h,c=self.lstm_layer(embeds,initial_hidden_state)\n#         print(\"after lstm\",output.shape)\n        # output of last layer for each example in batch\n        output=self.output_layer(output[:,self.max_len-1])\n        \n        return output\n\n    \nmodel=FirstLSTM(max_features,max_len,embed_dim,lstm_units,embeddings_matrix)\n# pushing the model to gpu\nmodel.cuda()\nprint(model)\n<\/code>\nand after this, I am iterating through the epoch loop and batch loop and I landed with this error.\n<code class=\"lang-auto\"># fixing the optimizer\n# you have to paramters to the adam so that it know through it optimize\noptimizer=torch.optim.Adam(model.parameters(),lr=1e-3)\n\n# fitting the data\nfor ep_num in range(epochs):\n    print(\"Epoch Number:\",ep_num+1)\n    # iterating through batch\n    for X,y in train_iterator:\n        \n        # Zero the gradients before running the backward pass.\n        optimizer.zero_grad()\n        \n        # pushing them to gpu\n#         X=X.cuda()\n#         y=y.cuda()\n        \n        y_out=model(X)\n        print(y_out.shape)\n        break\n    break\n        \n<\/code>\nNote:  train iterator is of type dataloader\nand the error is given below :\nScreenshot from 2018-12-08 02-21-21.png774\u00d7550 51 KB with link \"https:\/\/discuss.pytorch.org\/uploads\/default\/original\/2X\/c\/c8ee98bbf9a35d6ca1cd46403b2926e5e3592f3c.png\"\nCan I know where the problem is","y":"your lstm layer returns a 3-tuple but you unpack it as 2","z":"your lstm layer returns a 3-tuple but you unpack it as 2\nThanks I worked but I landed with another one . I just changed that line. So my new code look this\n<code class=\"lang-auto\">class FirstLSTM(nn.Module):\n    def __init__(self,max_features,max_len,embed_dim,lstm_units,embedding_weights):\n        super(FirstLSTM,self).__init__()\n        \n        # the variables are defined outside, so to make use of them in other methods , \n        # I am defining them in init..\n        \n        self.max_features=max_features                           # vocab size\n        self.max_len=max_len                                     # sequence length\n        self.embed_dim=embed_dim                                 # embedding dimension\n        self.lstm_units=lstm_units                               # lstm output size\n        self.num_layers=1                                       # number of layers in lstm\n        self.embeddings_matrix=torch.tensor(embedding_weights)  # converting them to tensors\n        self.out=1                                             # dimension of the output\n        \n        # now we have defined the embedding layer\n        # this is the classic way to define.....\n#         self.embed_layer=nn.Embedding(num_embeddings=self.max_features,\n#                                       embedding_dim=self.embed_dim)\n        \n        # but we have to use a pretrained embedding\n        # below shows the way for this\n        self.embed_layer=nn.Embedding.from_pretrained(self.embeddings_matrix,freeze=True)\n        \n        # lstm layer\n        # note that I made batch_first = True to replicate with keras...\n        self.lstm_layer=nn.LSTM(input_size=self.embed_dim,hidden_size=self.lstm_units,\n                                   num_layers=self.num_layers,batch_first=True)\n        \n        self.output_layer=nn.Linear(in_features=self.lstm_units,out_features=self.out)\n        \n    # this will return the initial hidden state\n    # The axes semantics are (num_layers, minibatch_size, hidden_dim) even batch_first=True\n    # as there are two hidden states we will just return the tuple\n    # https:\/\/pytorch.org\/tutorials\/beginner\/nlp\/sequence_models_tutorial.html\n    def initialize_hidden_state(self,batch_size):\n        return (Variable(torch.zeros((self.num_layers,batch_size,self.lstm_units)).cuda()),\n                Variable(torch.zeros((self.num_layers,batch_size,self.lstm_units)).cuda()))\n    \n    def forward(self,X):\n        batch_shape=X.shape[0]\n        initial_hidden_state=self.initialize_hidden_state(batch_shape)\n        X=Variable(X.cuda())\n        \n        # forward pass\n        print(X)\n        \n        # actually embedding layer needs the tensor of type long, but we pass as an int\n        # so I will typecaste the long\n        embeds=self.embed_layer(X.long())\n        output,hidden_state=self.lstm_layer(embeds,initial_hidden_state)\n#         print(\"after lstm\",output.shape)\n        # output of last block of lstm for each example in batch\n        output=self.output_layer(output[:,self.max_len-1])\n        \n        return output\n\n    \nmodel=FirstLSTM(max_features,max_len,embed_dim,lstm_units,embeddings_matrix)\n# pushing the model to gpu\nmodel.cuda()\nmodel\n<\/code>\nand when iterating through epoch loop, I got this one\nScreenshot from 2018-12-10 23-51-15.png797\u00d7586 60.6 KB with link \"https:\/\/discuss.pytorch.org\/uploads\/default\/original\/2X\/7\/71b6315c968a2e8e0482d47cc84c49104f9a947b.png\""},{"x":"In The Annotated Encoder-Decoder with link \"https:\/\/bastings.github.io\/annotated_encoder_decoder\/\", the encoder module is this:\n<code class=\"lang-auto\">class Encoder(nn.Module):\n    \"\"\"Encodes a sequence of word embeddings\"\"\"\n    def __init__(self, input_size, hidden_size, num_layers=1, dropout=0.):\n        super(Encoder, self).__init__()\n        self.num_layers = num_layers\n        self.rnn = nn.GRU(input_size, hidden_size, num_layers, \n                          batch_first=True, bidirectional=True, dropout=dropout)\n        \n    def forward(self, x, mask, lengths):\n        \"\"\"\n        Applies a bidirectional GRU to sequence of embeddings x.\n        The input mini-batch x needs to be sorted by length.\n        x should have dimensions [batch, time, dim].\n        \"\"\"\n        packed = pack_padded_sequence(x, lengths, batch_first=True)\n        output, final = self.rnn(packed)\n        output, _ = pad_packed_sequence(output, batch_first=True)\n\n        # we need to manually concatenate the final states for both directions\n        fwd_final = final[0:final.size(0):2]\n        bwd_final = final[1:final.size(0):2]\n        final = torch.cat([fwd_final, bwd_final], dim=2)  # [num_layers, batch, 2*dim]\n\n        return output, final\n<\/code>\nand I don\u2019t undersstand the last comment in the forward method, regarding the final hidden states. What is going on there, and why would I need to do that?\nThanks!","y":"Because the encoder is bidirectional. This means that one RNN is trained in one direction across your inputs, and the other is trained in the opposite direction. In order to represent each timestep of your RNN as informed by both what comes before, and what comes after, we represent each state as the concatenation of both of those RNN\u2019s at that timestep.\ne.g. if we have the phrase \u2018the quick brown fox\u2019, we encode it in both directions as:\n[the quick, brown, fox]\n[fox, brown, quick, the]\nWhere each word carries with it information that came from the previous words. After, and to the point of your question, the states representing these words need to be concatenated pairwise. Because you are taking only the final state, you concatenate just those two RNN states, which have information about everything in the sequence going left-to-right, AND right-to-left.","z":"Because the encoder is bidirectional. This means that one RNN is trained in one direction across your inputs, and the other is trained in the opposite direction. In order to represent each timestep of your RNN as informed by both what comes before, and what comes after, we represent each state as the concatenation of both of those RNN\u2019s at that timestep.\ne.g. if we have the phrase \u2018the quick brown fox\u2019, we encode it in both directions as:\n[the quick, brown, fox]\n[fox, brown, quick, the]\nWhere each word carries with it information that came from the previous words. After, and to the point of your question, the states representing these words need to be concatenated pairwise. Because you are taking only the final state, you concatenate just those two RNN states, which have information about everything in the sequence going left-to-right, AND right-to-left.\nThank you very much! "},{"x":"Hi there,\nI am working on a sentiment analysis project with the SST-1 dataset using the Torchtext library. As a baseline, I want to create a vanilla softmax classifier as a 1-linear-layer net with log-softmax and negative log-likelihood loss.\nI read the doc of nn.NLLLoss() with link \"https:\/\/pytorch.org\/docs\/stable\/nn.html#torch.nn.NLLLoss\", and (I think) I understand what it does. Meaning that it requires a tensor of size [minibatch, classes] as the input and a tensor of size [classes] as the target to be able to categorize each batch item into one of the classes.\nNow this is where I hit the wall. I computed the log probabilities with log_softmax and got a tensor of (minibatch, classes) (here, it has torch.Size([10, 6])) which I used as the input; then used the tensor of labels (torch.Size([10])) as the target.\nHowever, I still got the following:\nValueError: Expected input batch_size (10) to match target batch_size (4).\nCould you please point out where things went wrong and how could I fix it? Any help is greatly appreciated!\nBelow is the relevant code snippet:\n(Python 3.5.6, PyTorch 1.0.0, torchtext 0.3.1)\n<code class=\"lang-auto\">TEXT = torchtext.data.Field()\nLABEL = torchtext.data.Field(sequential=False, is_target=True)\ntrain, val, test = torchtext.datasets.SST.splits(\n    TEXT, LABEL, fine_grained=True)\nTEXT.build_vocab(train)\nLABEL.build_vocab(train)\ntrain_iter, val_iter, test_iter = torchtext.data.BucketIterator.splits((train, val, test), batch_size=10, device=-1, repeat=False)\n\nclass Softmax(nn.Module):\n    def __init__(self, vocab_size, n_classes, batch_size):\n        super(Softmax, self).__init__()\n        self.linear = nn.Linear(vocab_size, n_classes, bias = True)\n        \n    def forward(self, x):\n        out = self.linear(x)\n        log_probs = F.log_softmax(out, dim=1)\n        return log_probs\n\ndef get_one_hot(batch, batch_size, vocab_size):\n    new_tensor = torch.zeros(batch_size, vocab_size) \n    word_indices = torch.transpose(batch.text, 0, 1)\n    for batch_item, word_ix in enumerate(word_indices):\n        new_tensor[batch_item][word_ix] = 1\n    return new_tensor\n\ndef train_softmax(train_iter):\n    losses = []\n    model = Softmax(VOCAB_SIZE, N_CLASSES, BATCH_SIZE)\n    loss_fn = nn.NLLLoss()\n    optimizer = optim.SGD(model.parameters(), LEARNING_RATE)\n\n    for epoch in range(EPOCHS):\n        epoch_loss = 0\n        for batch in train_iter:\n            \n            x = Variable(torch.FloatTensor(get_one_hot(batch, BATCH_SIZE, VOCAB_SIZE)), requires_grad=True)\n            y = Variable(batch.label)\n            \n            model.zero_grad()\n            log_probs = model.forward(x)\n            loss = loss_fn(log_probs, y) # here is the problem\n            loss.backward()\n            optimizer.step()\n\n            epoch_loss += loss.item()\n        losses.append(epoch_loss)\n    return model, losses\n<\/code>","y":"UPDATE\nAs I was iterating over the training set, I realized that the last batch contains only 4 labels as opposed to the expected 10. Since it was the last batch, this was the value that the variable target.size(0) referred to after finishing the iteration, which ultimately caused the ValueError raise.\nTake-home message: Know thy dataset inside out ","z":"Can you mention the constant values too?\nBATCH_SIZE, VOCAB_SIZE, N_CLASSES\nUnrelated to this question, some minor suggestions:\n\nUse log_probs = model(x) instead of log_probs = model.forward(x) to make sure the necessary hooks are in place.\nIn pytorch 1.0, you do not need to have Variable and use the tensors directly.\n\nYeah, sure:\n<code class=\"lang-auto\">BATCH_SIZE = train_iter.batch_size = 10\nVOCAB_SIZE = len(TEXT.vocab) = 18282\nN_CLASSES = len(LABEL.vocab) = 6\n<\/code>\nActually, I tried to train it with different batch sizes, but when I plugged in anything greater than 4, the ValueError mentioned above occurred.\nThanks for the suggestions though!\nAnother thing I noticed:\nif I try to include the subtrees in the training (that is, modifying the following line, while leaving everything else unchanged):\n<code class=\"lang-auto\">train, val, test = torchtext.datasets.SST.splits(\n    TEXT, LABEL, train_subtrees=True, fine_grained=True)\n<\/code>\nI get the following:\nValueError: Expected input batch_size (10) to match target batch_size (2).\nSo I don\u2019t know if the error is due to something peculiar to the SST dataset or torchtext, but I just couldn\u2019t wrap my head around it\u2026 So please, if anyone encountered something similar, what could be the root of the problem?\nUPDATE\nAs I was iterating over the training set, I realized that the last batch contains only 4 labels as opposed to the expected 10. Since it was the last batch, this was the value that the variable target.size(0) referred to after finishing the iteration, which ultimately caused the ValueError raise.\nTake-home message: Know thy dataset inside out "},{"x":"Using torch.nn\u2019s GRU gives the error Expected object of scalar type Long but got scalar type Float for argument #2 \u2018mat2\u2019. I converted the input before passing it to the gru and also printed its type() to verify that it is a Long tensor.\n<code class=\"lang-auto\">in1 = x.unsqueeze(0).type(torch.LongTensor).\nin2 = (hxs * masks).unsqueeze(0).type(torch.LongTensor).\nprint(in1.type(),in2.type()) #Gives torch.cuda.LongTensor\nx, hxs = self.gru(in1, in2)\n<\/code>\nThe gru is defined as self.gru = nn.GRU(recurrent_input_size, hidden_size).cuda()","y":"Have you checked your weights? Initially Pytorch Tensors have Float type and it will cause problems when you want to use different type of tensors. For example:\n\nFor solution, you should change your layer weight type. Also you can set default tensor type if you are using all LongTensor:\n<code class=\"lang-auto\">torch.set_default_tensor_type('torch.cuda.LongTensor')\n<\/code>","z":"Have you checked your weights? Initially Pytorch Tensors have Float type and it will cause problems when you want to use different type of tensors. For example:\n\nFor solution, you should change your layer weight type. Also you can set default tensor type if you are using all LongTensor:\n<code class=\"lang-auto\">torch.set_default_tensor_type('torch.cuda.LongTensor')\n<\/code>"},{"x":"Hi all,\nPlease find current implementation of LSTM classifier I am using below:\n<code class=\"lang-auto\">class LSTMClassifier(nn.Module):\n    def __init__(self, embedding_dim, hidden_dim, vocab_size, label_size):\n        super(LSTMClassifier, self).__init__()\n        self.hidden_dim = hidden_dim\n        self.word_embeddings = nn.Embedding(vocab_size, embedding_dim)\n        self.lstm = nn.LSTM(embedding_dim, hidden_dim)\n        self.hidden2label = nn.Linear(hidden_dim, label_size)\n        self.hidden = self.init_hidden()\n\n    def init_hidden(self):\n        # the first is the hidden h\n        # the second is the cell  c\n        return (autograd.Variable(torch.zeros(1, 1, self.hidden_dim)),\n                autograd.Variable(torch.zeros(1, 1, self.hidden_dim)))\n\n    def forward(self, sentence):\n        embeds = self.word_embeddings(sentence)\n        x = embeds.view(len(sentence), 1, -1)\n        lstm_out, self.hidden = self.lstm(x, self.hidden)\n        y  = self.hidden2label(lstm_out[-1])\n        log_probs = F.log_softmax(y)\n        return log_probs\n<\/code>\nDuring training the network, I am detaching the hidden state from it\u2019s history on last batch.\n<code class=\"lang-auto\">def train(..., ...):\n    ...\n    ...\n    for batch in batches:\n        model.hidden = model.init_hidden()\n        ...\n    ...\n<\/code>\nHowever, I want to learn the params in the initial hidden state. What\u2019s the proper way to make the initial hidden state learnable during training?","y":"\n\n\n backprop7:\n\nself.hidden = self.init_hidden()\n\n\nIn the __init__() function, try making the hidden and cell state as nn.Parameters.\n<code class=\"lang-auto\">self.hidden = nn.Parameter(torch.zeros(1, 1, self.hidden_dim))\nself.cell_state = nn.Parameter(torch.zeros(1, 1, self.hidden_dim))\n<\/code>\n\n\n\n backprop7:\n\nmodel.hidden = model.init_hidden()\n\n\nAnd no need to init_hidden() for every batch, as they are learned.","z":"\n\n\n backprop7:\n\nself.hidden = self.init_hidden()\n\n\nIn the __init__() function, try making the hidden and cell state as nn.Parameters.\n<code class=\"lang-auto\">self.hidden = nn.Parameter(torch.zeros(1, 1, self.hidden_dim))\nself.cell_state = nn.Parameter(torch.zeros(1, 1, self.hidden_dim))\n<\/code>\n\n\n\n backprop7:\n\nmodel.hidden = model.init_hidden()\n\n\nAnd no need to init_hidden() for every batch, as they are learned."},{"x":"I have a tensor P, with dimension: (batch-size x  num-layers x length x embedding-size)\nI want to concatenate the embeddings across all layers, so eventually, I want a tensor with the following dimensions:\n(batch-size  x length x num-layers*embedding-size)\nLet\u2019s take an example:\nP = torch.randn(10,3,105,1024)\nwhere batch-size = 10, num-layers = 3, length-of-sentence=105, embedding-size=1024.\nI want to concatenate the embeddings of 3 layers for each time-stamp in the sentence.\nOne way I can do this is:\n<code class=\"lang-auto\">batch_size = 10\nconcats = []\nfor idx in range(batch_size):\n    concats.append(torch.cat([P[idx][0], P[idx][1], P[idx][2]], dim=1)[None, :, :])\nQ = torch.cat(concats, dim=0)\n<\/code>\nQ\u2019s dimension : (10,105,3072)\nNote that, R = P.view(10, 105, -1) also gives me a tensor with similar dimensions as that of Q, but it will be a different tensor than Q, as it is concatenating the first layer for time-stamp 1,2,3 etc.\nIs there any faster memory-efficient way of getting the Q tensor?","y":"You can use .permute to swap axes and then apply .view to merge the last two dimensions.\n<code class=\"lang-python\">>>> d = torch.randn(10, 3, 105, 1024)\n>>> d.shape\ntorch.Size([10, 3, 105, 1024])\n>>> d = d.permute(0, 2, 1, 3)\n>>> d.shape\ntorch.Size([10, 105, 3, 1024])\n>>> d = d.contiguous().view(10, 105, -1)\n>>> d.shape\ntorch.Size([10, 105, 3072])\n<\/code>","z":"You can use .permute to swap axes and then apply .view to merge the last two dimensions.\n<code class=\"lang-python\">>>> d = torch.randn(10, 3, 105, 1024)\n>>> d.shape\ntorch.Size([10, 3, 105, 1024])\n>>> d = d.permute(0, 2, 1, 3)\n>>> d.shape\ntorch.Size([10, 105, 3, 1024])\n>>> d = d.contiguous().view(10, 105, -1)\n>>> d.shape\ntorch.Size([10, 105, 3072])\n<\/code>"},{"x":"Hi,\nHow can I use several features in my net, e.g. current token, next token, current pos-tag, next pos-tag?\nDo I need to embed the pos tags? Or is just their index is ok? I\u2019m very confused about how to make this work.\nThanks!","y":"You may need two embedding matrices, one for token, one for pos tags. And the network is like this:\ninput token index -> token embedding matrices -> token embedding\ninput pos-tag index -> pos-tag embedding matrices -> pos-tag embedding\nand then concatenate token embedding and its corresponding pos-tag embedding","z":"You may need two embedding matrices, one for token, one for pos tags. And the network is like this:\ninput token index -> token embedding matrices -> token embedding\ninput pos-tag index -> pos-tag embedding matrices -> pos-tag embedding\nand then concatenate token embedding and its corresponding pos-tag embedding\nThanks, I\u2019ll try that!\nDo you think you can give me a small example of how to do this? I\u2019m getting myself into a mess \nHere is an example:\n<code class=\"lang-auto\">import torch\n\n# prepare embeddings\nVOCAB_SIZE = 100\nPOS_TAG_SIZE = 20\nWORD_EMBED_DIM = 5\nPOS_TAG_DIM = 3\nword_embeddings = torch.nn.Embedding(VOCAB_SIZE, WORD_EMBED_DIM)\npos_tag_embeddings = torch.nn.Embedding(POS_TAG_SIZE, POS_TAG_DIM)\n\n# prepare training data (here I use some random generated LongTensors)\nNUM_TRAIN_EXAMPLES = 1000\nMAX_SEQ_LEN = 10\nword_sequence = torch.randint(VOCAB_SIZE, (NUM_TRAIN_EXAMPLES, MAX_SEQ_LEN), dtype=torch.long)\npos_tag_sequence = torch.randint(POS_TAG_SIZE, (NUM_TRAIN_EXAMPLES, MAX_SEQ_LEN), dtype=torch.long)\n\n# mock training of a batch\nBATCH_SIZE = 8\nword_batch = word_sequence[0 : 0 + BATCH_SIZE]\npos_tag_batch = pos_tag_sequence[0 : 0 + BATCH_SIZE]\n\nword_embed_output = word_embeddings(word_batch)\npos_tag_embed_output = pos_tag_embeddings(pos_tag_batch)\n\nprint('word_embed_output shape: {}'.format(word_embed_output.size()))\nprint('pos_tag_embed_outputshape :{}'.format(pos_tag_embed_output.size()))\n\n# concatenate word_embed_output and pos_tag_embed_outputshape\nfinal_embed = torch.cat((word_embed_output, pos_tag_embed_output), dim=-1)\nprint('final embed shape: {}'.format(final_embed.size()))\n<\/code>\nIf you have some pretrained embeddings, you can wrap them in tensors and load them into embedding matrices using torch.nn.Embedding.from_pretrained().\nBelow is an example from the offical document  https:\/\/pytorch.org\/docs\/stable\/nn.html\n<code class=\"lang-auto\">>>> # FloatTensor containing pretrained weights\n>>> weight = torch.FloatTensor([[1, 2.3, 3], [4, 5.1, 6.3]])\n>>> embedding = nn.Embedding.from_pretrained(weight)\n>>> # Get embeddings for index 1\n>>> input = torch.LongTensor([1])\n>>> embedding(input)\ntensor([[ 4.0000,  5.1000,  6.3000]])\n<\/code>\nThank you so much!!!"},{"x":"Hi,\nI\u2019m trying to create tensor from a variable data, which is a list. According to the documentation with link \"https:\/\/pytorch.org\/tutorials\/beginner\/nlp\/pytorch_tutorial.html\" , I should be able to that using torch.Tensor() method. I have ensured that the list doesn\u2019t contain any strings.\nThe list looks like this:\n[tensor([[ 4.6291e-03, -1.3071e-05, -1.0033e-03,  \u2026,  3.7078e-02,\n7.7851e-03,  3.4532e-02],\n[ 7.5976e-03, -1.4478e-05,  6.4205e-03,  \u2026,  2.3013e-02,\n9.6373e-03,  3.6175e-02],\n[ 1.0242e-02, -2.6630e-05,  1.5521e-03,  \u2026,  3.2181e-02,\n1.6385e-02,  8.6033e-03],\n\u2026,\n[ 6.9822e-03, -4.2445e-05,  5.6378e-03,  \u2026,  1.9608e-02,\n8.2644e-03,  1.8788e-02],\n[ 6.9822e-03, -4.2445e-05,  5.6378e-03,  \u2026,  1.9608e-02,\n8.2644e-03,  1.8788e-02],\n[ 7.3356e-03, -2.1180e-05,  4.7856e-03,  \u2026,  1.7249e-02,\n6.8688e-03,  4.0911e-02]])]\nHowever, I am getting ValueError: only one element tensors can be converted to Python scalars. I\u2019m unable to figure out what exactly is the error trying to say.\nPlease see below the code snippet.codeSnippet.png636\u00d7520 59.8 KB with link \"https:\/\/discuss.pytorch.org\/uploads\/default\/original\/2X\/6\/681209cacee52d5c59b2b15b394d4cbd4914bacc.png\"\nI think I might have missed something very trivial, but I\u2019ve been stuck here for quite some time. Any direction as to why this error occurs would be greatly appreciated.\nThanks in advance ","y":"Hi!\nI have mistaken list of tensor as a simple python list. (So I have also changed the title of question.)\nThe variable \u2018data\u2019 was actually a list of tensors with only one item. And I can\u2019t create a tensor from a list of tensors using torch.Tensor() method. Hence the error.\nI used the below method to turn the list of tensor into a single tensor with link \"https:\/\/discuss.pytorch.org\/t\/how-to-turn-a-list-of-tensor-to-tensor\/8868\" :\nX = torch.stack(data)\nAnd it works now. Thanks! ","z":"If I understood your question correctly, you have a python list and want to convert to a pytorch tensor.\n<code class=\"lang-auto\">python_list = [[1,2,3], [4, 5, 6]]\n\ntorch_list = torch.tensor(python_list)\n<\/code>\nCan you point out where you are getting the error in the code?\nYes, that\u2019s right.\nIn #3,\nX = torch.FloatTensor(data)\nThis is where I\u2019m getting the error. I have also tried X = torch.Tensor(data) , but ended up with the same error.\nHi!\nI have mistaken list of tensor as a simple python list. (So I have also changed the title of question.)\nThe variable \u2018data\u2019 was actually a list of tensors with only one item. And I can\u2019t create a tensor from a list of tensors using torch.Tensor() method. Hence the error.\nI used the below method to turn the list of tensor into a single tensor with link \"https:\/\/discuss.pytorch.org\/t\/how-to-turn-a-list-of-tensor-to-tensor\/8868\" :\nX = torch.stack(data)\nAnd it works now. Thanks! "},{"x":"Hello,\nI am new to pytorch and I am trying to predict diabetes classification.\n<code class=\"lang-auto\">import torch.optim as optim\nimport torch\nimport numpy as np\nfrom torch.autograd import Variable\nfrom torch.utils.data import Dataset, DataLoader\nmodel = DeepNet(8)\n#8 input features, and 3 outputs\n\noptimizer = optim.SGD(model.parameters(), lr=0.0001)\nloss_fn = nn.CrossEntropyLoss()\n#batch size of 200 is indicated in the parameters of the dataloader.\nloss_list = []\nfor epoch in range(100):  # loop over the dataset for 100 epochs\n    avg_loss = 0\n    for index, data in enumerate(train_loader, 0):  \n        inputs, labels = data\n        inputs, labels = Variable(inputs), Variable(labels)\n        \n        optimizer.zero_grad()\n\n        outputs = model(inputs)\n        print(inputs.size(), labels.size(), outputs.size())\n        loss = loss_fn(outputs, labels.long())\n        avg_loss += loss.data.numpy().ravel()[0]\n        loss.backward()\n        optimizer.step()\n    loss_list.append(avg_loss\/inputs.shape[0])\nplt.plot(loss_list)\nprint(\"The loss value at 100th epoch:\", loss_list[99])\n<\/code>\nHowever, I am having an error: multi-target not supported at c:\\a\\w\\1\\s\\tmp_conda_3.6_091443\\conda\\conda-bld\\pytorch_1544087948354\\work\\aten\\src\\thnn\\generic\/ClassNLLCriterion.c:21.\"\nHere are the sizes of the inputs, labels, and outputs, respectively:\ntorch.Size([200, 8]) torch.Size([200, 1]) torch.Size([200, 3])\nAny help is greatly appreciated! Thank you very much!","y":"There is a convention that the labels should be a 1-dimensional array, not a 2-dimensional array. I.e., right now, you have 200x1 as the label array, but it should be simply 200. Try\nloss = loss_fn(outputs, labels.view(-1).long())\n\ninstead of\nloss = loss_fn(outputs, labels.long())\n\nThis be able to solve your issue. Btw I see that you are using Variable etc, which is old PyTorch code (it has been deprecated and removed in newer versions) \u2013 just wanted to mention that in case you encounter errors related to that later on when you update your PyTorch installation to the current ones.","z":"There is a convention that the labels should be a 1-dimensional array, not a 2-dimensional array. I.e., right now, you have 200x1 as the label array, but it should be simply 200. Try\nloss = loss_fn(outputs, labels.view(-1).long())\n\ninstead of\nloss = loss_fn(outputs, labels.long())\n\nThis be able to solve your issue. Btw I see that you are using Variable etc, which is old PyTorch code (it has been deprecated and removed in newer versions) \u2013 just wanted to mention that in case you encounter errors related to that later on when you update your PyTorch installation to the current ones.\n\n\n\n rasbt:\n\n.view(-1)\n\n\nThank you very very much!!!"},{"x":"I\u2019m new to pytorch and learning the Embedding module.\nThey say \u2018input to the module is a list of indices, and the output is the corresponding word embeddings.\u2019\u2019\nand I don\u2019t know what they mean \u2018corresponding\u2019. Is it \u2018mean\u2019 value? or \u2018variation\u2019?\nI tested it but cannot get good answer.\nembedding = nn.Embedding(10, 3) #an Embedding module containing 10 tensors of size 3\ninput = torch.LongTensor([[1,2,4,5],[4,3,2,9]]) # a batch of 2 samples of 4 indices each\nwrd_embedding = embedding(input); print(wrd_embedding)\n\n<code class=\"lang-auto\">\ntensor([[[ 0.7576,  0.7259,  0.0674],\n         [-0.0827,  1.8416, -0.4799],\n         [-0.2899,  1.4135, -0.0972],\n         [ 0.4071, -1.5048,  1.9368]],\n         [[-0.2899,  1.4135, -0.0972],\n         [-0.6687,  0.5834,  0.0072],\n         [-0.0827,  1.8416, -0.4799],\n         [-0.4928,  0.5937, -0.1569]]], grad_fn=<EmbeddingBackward>)\n<\/code>\nprint(torch.var(wrd_embedding[:, 0],1))\nprint(torch.sum(wrd_embedding[:, 0],1))\n\n<code class=\"lang-auto\">tensor([0.1518, 0.8701], grad_fn=&amp;lt;VarBackward1&amp;gt;)\n\ntensor([1.5509, 1.0264],grad_fn=&amp;lt;SumBackward2&amp;gt;)\n<\/code>\nMy question : What does the input value, [[1,2,4,5], [4,3,2,9]] do?","y":"Each item of input, like 1, will be changed to its embeddings. 1 means Embedding layer\u2019s weight first row, like this:\n\nYou can get the embed layer weight by embedding.weight.\nSearch word2vectors to learn more.","z":"Each item of input, like 1, will be changed to its embeddings. 1 means Embedding layer\u2019s weight first row, like this:\n\nYou can get the embed layer weight by embedding.weight.\nSearch word2vectors to learn more."},{"x":"Hi, I am using torchtext pacakge for some nlp task, I was wondering that is there any comprehensive \u201cofficial\u201d tutorial for torchtext package?\nI find several tutorial online, most of them are for version < 0.1.1.\nThanks!","y":"\n\n\nMachine Learning Explained \u2013 8 Feb 18 with link \"https:\/\/mlexplained.com\/2018\/02\/08\/a-comprehensive-tutorial-to-torchtext\/\"\n\n\n\nA Comprehensive Introduction to Torchtext (Practical Torchtext part 1) with link \"https:\/\/mlexplained.com\/2018\/02\/08\/a-comprehensive-tutorial-to-torchtext\/\"\nIf you\u2019ve ever worked on a project for deep learning for NLP, you\u2019ll know how painful and tedious all the preprocessing is. Before you start training your model, you have to: Read the d\u2026\n\n\n\n\n\n","z":"\n\n\nMachine Learning Explained \u2013 8 Feb 18 with link \"https:\/\/mlexplained.com\/2018\/02\/08\/a-comprehensive-tutorial-to-torchtext\/\"\n\n\n\nA Comprehensive Introduction to Torchtext (Practical Torchtext part 1) with link \"https:\/\/mlexplained.com\/2018\/02\/08\/a-comprehensive-tutorial-to-torchtext\/\"\nIf you\u2019ve ever worked on a project for deep learning for NLP, you\u2019ll know how painful and tedious all the preprocessing is. Before you start training your model, you have to: Read the d\u2026\n\n\n\n\n\nThanks!\nI also find some good tutorials as well:\n\n\nanie.me with link \"http:\/\/anie.me\/On-Torchtext\/\"\n\n\nA Tutorial on Torchtext with link \"http:\/\/anie.me\/On-Torchtext\/\"\nAbout 2-3 months ago, I encountered this library: Torchtext. I nonchalantly scanned through the README file and realize I have no idea how to use it or what kind of problem is it solving. I moved on.\n\n\n\n\n\nit is based on version < 0.1.1, but it still works well as a reference."},{"x":"L1 loss by default uses reduction=mean and weighs loss on differently sized instances.\nBut consider the following scenario:\nI have a set size for batches and image dimensions, but each image has different masks where only a certain region is of interest.\nI think it could be useful to scale each instance\u2019s loss by the area of the region of interest (i.e. number of ones in the mask).\nI don\u2019t believe elements which are exactly zero are ignored in L1Loss, are they?\nCould something like this work?\n<code class=\"lang-python\"># Assume 'input', 'output', 'mask' all are NCHW\n\noutput = model(input)\ndiff = input*mask - output*mask\nweighted_diff = mask.sum([1,2,3]) * diff\nloss = L1_loss(weighted_diff, torch.zeros_like(weighted_diff))\n<\/code>","y":"I am not sure about the use case.\nBut do you think, the losses from the images with \u201cless mask area\u201d should be weighed higher? If not, the model might get away by ignoring them.\ni.e.,\n<code class=\"lang-auto\">output = model(input)\nabs_diff = torch.abs((input - output)*mask).sum([1,2,3])\nloss = abs_diff \/ (mask.sum([1,2,3]) + eps)  # eps ~ 10-6\n<\/code>","z":"I am not sure about the use case.\nBut do you think, the losses from the images with \u201cless mask area\u201d should be weighed higher? If not, the model might get away by ignoring them.\ni.e.,\n<code class=\"lang-auto\">output = model(input)\nabs_diff = torch.abs((input - output)*mask).sum([1,2,3])\nloss = abs_diff \/ (mask.sum([1,2,3]) + eps)  # eps ~ 10-6\n<\/code>\nAh yes you are correct!\nI meant scaling it by the inverse scale of the mask which is what I assume you are referring to in the last line of code.\nI am curious if there was some torch component or loss function argument that did this \u201cbehind the scenes\u201d. I think it might just be better to do it explicitly like you suggested.\nI haven\u2019t come across a specific implementation in torch that gives this functionality so far.\nYou might need to look around in the docs."},{"x":"In the Pytorch docs for nn.GRU I found this:\n\nh_0 (num_layers * num_directions, batch, hidden_size): tensor containing the initial hidden state for each element in the batch. Defaults to zero if not provided.\n\nHowever, while playing around and learning GRU I came across a case where the GRU\u2019s forward pass succeeds even when the size of initial hidden state is incorrect.\nThis can be seen in the code below:\nCan someone please shed light on this behaviour?\nCode:\n<code class=\"lang-python\">batch_size = 3\nin_dim = 12\nhid_dim = 5\ntimesteps = 7\nnum_layers = 2\ngru = nn.GRU(in_dim, hid_dim, bidirectional=False, batch_first=True, num_layers=num_layers)\ninp = Variable(torch.Tensor(batch_size, timesteps, in_dim).normal_(-0.1, 0.1))\n\nprint('# Batch size correctly provided as 3')\nx = torch.Tensor(num_layers, 3, hid_dim).normal_(-0.1, 0.1) # Batch size correctly provided as 3\nhid = Variable(x)\no, h = gru(inp, hid)\nprint(o.size())\nprint(h.size())\n\nprint('# Batch size wrongly provided as 1')\nx = torch.Tensor(num_layers, 1, hid_dim).normal_(-0.1, 0.1) # Batch size wrongly provided as 1\nhid = Variable(x)\no, h = gru(inp, hid)\nprint(o.size())\nprint(h.size())\n\nprint('# Batch size wrongly provided as 2')\nx = torch.Tensor(num_layers, 2, hid_dim).normal_(-0.1, 0.1) # Batch size wrongly provided as 2\nhid = Variable(x)\no, h = gru(inp, hid)\nprint(o.size())\nprint(h.size())\n<\/code>\n<h1>Batch size correctly provided as 3 in initial hidden state<\/h1>\ntorch.Size([3, 7, 5])\ntorch.Size([2, 3, 5])\n<h1>Batch size wrongly provided as 1 in initial hidden state<\/h1>\ntorch.Size([3, 7, 5])\ntorch.Size([2, 3, 5])\n<h1>Batch size wrongly provided as 2 in initial hidden state<\/h1>\n\nRuntimeError                              Traceback (most recent call last)\n in ()\n24 x = torch.Tensor(num_layers, 2, hid_dim).normal_(-0.1, 0.1) # Batch size wrongly provided as 2\n25 hid = Variable(x)\n\u2014> 26 o, h = gru(inp, hid)\n27 print(o.size())\n28 print(h.size())\n~\/.local\/lib\/python3.5\/site-packages\/torch\/nn\/modules\/module.py in call(self, *input, **kwargs)\n323         for hook in self._forward_pre_hooks.values():\n324             hook(self, input)\n\u2013> 325         result = self.forward(*input, **kwargs)\n326         for hook in self._forward_hooks.values():\n327             hook_result = hook(self, input, result)\n~\/.local\/lib\/python3.5\/site-packages\/torch\/nn\/modules\/rnn.py in forward(self, input, hx)\n167             flat_weight=flat_weight\n168         )\n\u2013> 169         output, hidden = func(input, self.all_weights, hx)\n170         if is_packed:\n171             output = PackedSequence(output, batch_sizes)\n~\/.local\/lib\/python3.5\/site-packages\/torch\/nn\/_functions\/rnn.py in forward(input, *fargs, **fkwargs)\n383             return hack_onnx_rnn((input,) + fargs, output, args, kwargs)\n384         else:\n\u2013> 385             return func(input, *fargs, **fkwargs)\n386\n387     return forward\n~\/.local\/lib\/python3.5\/site-packages\/torch\/nn\/_functions\/rnn.py in forward(input, weight, hidden)\n243             input = input.transpose(0, 1)\n244\n\u2013> 245         nexth, output = func(input, hidden, weight)\n246\n247         if batch_first and batch_sizes is None:\n~\/.local\/lib\/python3.5\/site-packages\/torch\/nn\/_functions\/rnn.py in forward(input, hidden, weight)\n83                 l = i * num_directions + j\n84\n\u2014> 85                 hy, output = inner(input, hidden[l], weight[l])\n86                 next_hidden.append(hy)\n87                 all_output.append(output)\n~\/.local\/lib\/python3.5\/site-packages\/torch\/nn\/_functions\/rnn.py in forward(input, hidden, weight)\n112         steps = range(input.size(0) - 1, -1, -1) if reverse else range(input.size(0))\n113         for i in steps:\n\u2013> 114             hidden = inner(input[i], hidden, *weight)\n115             # hack to handle LSTM\n116             output.append(hidden[0] if isinstance(hidden, tuple) else hidden)\n~\/.local\/lib\/python3.5\/site-packages\/torch\/nn\/_functions\/rnn.py in GRUCell(input, hidden, w_ih, w_hh, b_ih, b_hh)\n58     h_r, h_i, h_n = gh.chunk(3, 1)\n59\n\u2014> 60     resetgate = F.sigmoid(i_r + h_r)\n61     inputgate = F.sigmoid(i_i + h_i)\n62     newgate = F.tanh(i_n + resetgate * h_n)\nRuntimeError: The size of tensor a (3) must match the size of tensor b (2) at non-singleton dimension 0","y":"This is a bug, and has been fixed on the master branch of pytorch: https:\/\/github.com\/pytorch\/pytorch\/pull\/3925","z":"This is a bug, and has been fixed on the master branch of pytorch: https:\/\/github.com\/pytorch\/pytorch\/pull\/3925\nThanks  \nAnd I found that this only happens when the GRU(or ther RNN layers) was on cpu, and the hidden states contained a Tensor (instead of a cuda Tensor), and I thought that this was a feature specially available for CPU inplementation."},{"x":"I understand that individual words are embedded in the first hidden layer of a CNN. The size of this embedding layer would be (Vocab size,Embedding Size). What I don\u2019t understand is how does pytorch process a sequence of words. For example let\u2019s say I\u2019m trying to predict the sentiment of a sentence. I would pass in a vector of indices, of fixed size that represent the words in the sentence. How does pytorch work with this vector. I can\u2019t visualize it. If I pass a single world I would assume that the neural node corresponding to that word gets fired and we get the resulting embedding. What happens if we pass a sequence of words?","y":"The size of the embedding weight tensor is (Vocab size,Embedding Size), but according to the docs with link \"http:\/\/pytorch.org\/docs\/0.3.0\/nn.html#torch.nn.Embedding\" the embedding layer takes input of shape (batch_size, words) where each element is the index of a word, and produces output of shape (batch_size, words, embedding_size) where each word index has been replaced by the corresponding embedding vector.\nBasically, the embedding is applied individually to each word of the input.","z":"The size of the embedding weight tensor is (Vocab size,Embedding Size), but according to the docs with link \"http:\/\/pytorch.org\/docs\/0.3.0\/nn.html#torch.nn.Embedding\" the embedding layer takes input of shape (batch_size, words) where each element is the index of a word, and produces output of shape (batch_size, words, embedding_size) where each word index has been replaced by the corresponding embedding vector.\nBasically, the embedding is applied individually to each word of the input.\nThanks that clears things up."},{"x":"I am trying to use pre-trained word embedding in Pytorch, Wv.p. I have a word2vec pre-trained dataset of 114044 words and my dataset contains 426 unique words.\nTo use that embedding I load the pickle file and copy the embeddings by:\nself.word_embedding.weight.data.copy_(torch.from_numpy(Wv))\nbut I get an error when running as:\nRuntimeError: inconsistent tensor size, expected tensor [426 x 50] and src [114044 x 50] to have the same number of elements, but got 21300 and 5702200 elements respectively at \/Users\/soumith\/code\/builder\/wheel\/pytorch-src\/torch\/lib\/TH\/generic\/THTensorCopy.c:121\nWhat wrong am I doing? The number of words can obviously be equivalent to pre-trained dataset.","y":"The pretrained embedding was trained on a vocab of 114044 unique words.\nBut your embedding layer was be initialised for a vocab of 426 words.\nI see three possible solutions\u2026\n\nInitialise the embedding layer for a vocab of 114044 and make sure that the 426 words of your new dataset use the word indices from the 114044 word vocabulary.\nExtract the embedding data for those 426 words from the 114044 word pretrained embedding.\nTrain new embeddings\n","z":"The pretrained embedding was trained on a vocab of 114044 unique words.\nBut your embedding layer was be initialised for a vocab of 426 words.\nI see three possible solutions\u2026\n\nInitialise the embedding layer for a vocab of 114044 and make sure that the 426 words of your new dataset use the word indices from the 114044 word vocabulary.\nExtract the embedding data for those 426 words from the 114044 word pretrained embedding.\nTrain new embeddings\n\nThank you for your answer. The concept is clear to me now."},{"x":"Hi ,\nI need to build an RNN (without using nn.RNN) with following specifications :\n\nIt should have set of weights [\n\n\n\nIt is a chanracter RNN.\n\n\nIt should have 1 hidden layer\n\n\nWxh (from input layer to hidden layer )\n\n\nWhh (from the recurrent connection in the hidden layer)\n\n\nW ho  (from hidden layer to output layer)\n\n\nI need to use Tanh for hidden layer\n\n\nI need to use softmax for output layer.\n\n\nI am implemented the code . I am using CrossEntropyLoss() as loss function .\nWhich is giving me error as\nRuntimeError: multi-target not supported at \/opt\/conda\/conda-bld\/pytorch_1513368888240\/work\/torch\/lib\/THNN\/generic\/ClassNLLCriterion.c:22\nHere is my code for model :\n<code class=\"lang-auto\">\nclass CharRNN(torch.nn.Module):\n\n    def __init__(self,input_size,hidden_size,output_size, n_layers = 1):\n\n        super(CharRNN, self).__init__()\n        self.input_size  = input_size\n        self.hidden_size = hidden_size\n        self.n_layers    = 1\n\n        self.x2h_i = torch.nn.Linear(input_size + hidden_size, hidden_size)\n        self.x2h_f = torch.nn.Linear(input_size + hidden_size, hidden_size)\n        self.x2h_o = torch.nn.Linear(input_size + hidden_size, hidden_size)\n        self.x2h_q = torch.nn.Linear(input_size + hidden_size, hidden_size)\n        self.h2o   = torch.nn.Linear(hidden_size, output_size)\n        self.sigmoid = torch.nn.Sigmoid()\n        self.softmax = torch.nn.Softmax()\n        self.tanh    = torch.nn.Tanh()\n\n    def forward(self, input, h_t, c_t):\n\n        combined_input = torch.cat((input,h_t),1)\n\n        i_t = self.sigmoid(self.x2h_i(combined_input))\n        f_t = self.sigmoid(self.x2h_f(combined_input))\n        o_t = self.sigmoid(self.x2h_o(combined_input))\n        q_t = self.tanh(self.x2h_q(combined_input))\n\n        c_t_next = f_t*c_t + i_t*q_t\n        h_t_next = o_t*self.tanh(c_t_next)\n\n        output = self.softmax(h_t_next)\n        return output, h_t, c_t\n    \n    def initHidden(self):\n        return torch.autograd.Variable(torch.zeros(1, self.hidden_size))\n\n    def weights_init(self,model):\n    \n        classname = model.__class__.__name__\n        if classname.find('Linear') != -1:\n            model.weight.data.normal_(0.0, 0.02)\n            model.bias.data.fill_(0)\n<\/code>\nand this is the code for training the model :\n<code class=\"lang-auto\">\ninput_tensor  = torch.autograd.Variable(torch.zeros(seq_length,n_vocab))\ntarget_tensor = torch.autograd.Variable(torch.zeros(seq_length,n_vocab))\n\nmodel   = CharRNN(input_size = n_vocab, hidden_size = hidden_size, output_size = output_size)\nmodel.apply(model.weights_init)\n\ncriterion = nn.CrossEntropyLoss()\noptimizer = torch.optim.Adam(model.parameters(), lr = learning_rate)\n\nfor i in range(n_epochs):\n    print(\"Iteration\", i)\n    \n    start_idx    = np.random.randint(0, n_chars-seq_length-1)\n    train_data   = raw_text[start_idx:start_idx + seq_length + 1]\n    \n    input_tensor = torch.autograd.Variable(seq2tensor(train_data[:-1],n_vocab), requires_grad = True)\n    target_tensor= torch.autograd.Variable(seq2tensor(train_data[1:],n_vocab), requires_grad = False).long()\n    \n    loss = 0\n    \n    h_t = torch.autograd.Variable(torch.zeros(1,hidden_size))\n    c_t = torch.autograd.Variable(torch.zeros(1,hidden_size))\n    \n    for timestep in range(seq_length):\n        \n        output, h_t, c_t = model(input_tensor[timestep].view(1,n_vocab), h_t, c_t)\n        \n        loss += criterion(output,target_tensor[timestep].view(1,n_vocab))\n        \n    loss.backward()\n    optimizer.step()\n    optimizer.zero_grad()\n    \n    x_t = input_tensor[0].view(1,n_vocab)\n    h_t = torch.autograd.Variable(torch.zeros(1,hidden_size))\n    c_t = torch.autograd.Variable(torch.zeros(1,hidden_size))\n    \n    gen_seq = []\n    \n    for timestep in range(100):\n        output, h_t, c_t = model(x_t, h_t, c_t)\n        ix = np.random.choice(range(n_vocab), p=output.data.numpy().ravel())\n        x_t = torch.autograd.Variable(torch.zeros(1,n_vocab))\n        x_t[0,ix] = 1\n        gen_seq.append(idx2char[ix])\n        \n    txt = ''.join(gen_seq)\n    print ('----------------------')\n    print (txt)\n    print ('----------------------')\n<\/code>\nCan you please help me ?\nThanks in advance.","y":"Sure, then you would have to call F.log_softmax(output) before passing it to NLLLoss or add it as a layer in your model.\nCrossEntropyLoss basically combines a log_softmax with NLLLoss.","z":"CrossEntropyLoss needs a 1-dimensional target.\nFrom the docs:\n\nThis criterion expects a class index (0 to C-1) as the target for each value of a 1D tensor of size minibatch\n\nExample:\n<code class=\"lang-auto\">batch_size = 10\nn_classes = 5\ndata = Variable(torch.randn(batch_size, n_classes))\ntarget = Variable(torch.LongTensor(batch_size).random_(n_classes))\n\ncriterion = nn.CrossEntropyLoss()\nloss = criterion(data, target)\n<\/code>\nHi \nThanks for your reply.\nI checked the dimension of output and target\n<code class=\"lang-auto\">output  torch.Size([1, 97])\ntarget  torch.Size([97])\n<\/code>\nHow can I convert the target size to [1]\nI tried view(1,1)  , but that did not work\ntarget.unsqueeze_(0) will add a new dimension.\nHowever, this won\u2019t help, since you would need a single entry with values between [0, 96].\nCould you print again the shape of target_tensor[timestep] in the  line of code where the loss is calculated?\n\n\n\n ptrblck:\n\ntarget_tensor[timestep]\n\n\nHi ,\ntarget_tensor[timestep].size()  torch.Size([97])\nIs it the probability of all 97 classes?\nI assume n_vocab is the number of all possible classes, right?\nHi ,\nYes it is\u2026You are right.\nTarget is an one hot encoded matrix.\nWould it work if you just all torch.max on it to get the current class?\nOr do you have \u201csoft\u201d probabilities, i.e. some other values than [0, 0, 1, 0, 0, ...]?\ntarget variable contains only 0 , 1. So , can I simply apply torch.max ?\nYep, you can drop the max value and just use the index:\n<code class=\"lang-auto\">target = torch.FloatTensor([0, 0, 1, 0, 0])\n_, idx = torch.max(target, 0)\n<\/code>\nHi  , thanks a lot for your help. You are a saviour.\nHowever , I have a small doubt.\nI need pass the max index to cross entropy  ? or the value ?\nYou need to pass the class index to the criterion.\nOtherwise it would be one all the time. \nOkay\u2026 will it  be applicable for NLLLoss ?\nSure, then you would have to call F.log_softmax(output) before passing it to NLLLoss or add it as a layer in your model.\nCrossEntropyLoss basically combines a log_softmax with NLLLoss.\nHi , just one thing\u2026 I am curious how index is used in cross entropy here ?\nI mean theoritically , cross entropy takes values ,right ?\nCould you please clear the doubt ?\nThanks a lot again \nThe formula from the docs with link \"http:\/\/pytorch.org\/docs\/master\/nn.html#torch.nn.CrossEntropyLoss\" shows that just the class index is used.\nSo basically you save another transformation needed for one-hot encoded targets."},{"x":"Hello! I\u2019m trying to move to 0.4.0 and improve sequence to sequence model performance. And I\u2019m stuck at loss calculating. Sorry for my poor English\u2026 I\u2019ll try to explain my problem.\nEarlier on 0.3 version I was running single \u201cdataset-unit\u201d through model and then calculating loss. Dataset-unit is a pair of 2 tensors: input sentence and target-sentence + target indexes of words from vocabulary.\nInput-sentence: torch.Size([13, 1, 100])\nTarget-sentence: torch.Size([3, 1, 100])\nTarget-indexes: torch.Size([3])\nWhere 100 is embedding size, 13 and 3 are amounts of embedded words.\nModel:\n<code class=\"lang-auto\">class SeqModel(nn.Module):\n  def __init__(self, input_size, hidden_size, output_size, layers):\n    super(SeqModel, self).__init__()\n\n    self.encoder = nn.GRU(input_size, hidden_size, layers)\n    self.decoder = nn.GRU(input_size, hidden_size, layers)\n    self.out = nn.Linear(hidden_size, output_size)\n\n    self.hidden_state = None\n    self.start_hidden = Variable(torch.zeros(layers, hidden_size))\n    if USE_CUDA:\n      self.start_hidden = self.start_hidden.cuda()\n\n  def encode(self, embedded_sentence):\n    outputs, self.hidden_state = self.encoder(embedded_sentence,  self.start_hidden)\n    return outputs\n\n  def decode(self, inputs):\n    outputs_h, self.hidden_state = self.decoder(inputs, self.hidden_state)\n    outputs = F.log_softmax(self.out(outputs_h.squeeze(0)), dim=-1)\n    return outputs\n    \n  def forward(self, embedded_sentence, reply_vectors):\n    enc_outs = self.encode(embedded_sentence)\n    dec_outs = self.decode(reply_vectors)\n    return dec_outs\n<\/code>\nAnd calculating loss was like this:\n<code class=\"lang-auto\">def train_iter(model, criterion, data):\n  loss = 0\n  loss_val = 0\n  for phrase, reply in data:\n    data_enc_in = phrase['vectors']   # =  input-sentence, e.g. torch.Size([13, 1, 100]) \n    data_dec_in = reply['vectors']   # =  target-sentence, e.g. torch.Size([3, 1, 100])\n    data_target = reply['indexes']   # =  target-indexes, e.g. torch.Size([3])\n    \n    outputs = model(data_enc_in, data_dec_in)\n    loss = criterion(outputs.squeeze(1), data_target)\n    loss.backward()\n    loss_val += loss.data[0]\n  return loss_val \/ len(data)\n<\/code>\nAnd after several such iterations I was calling opt.step()\nBut now I want to pass through the model the whole batch to improve performance. Model is slightly different:\n<code class=\"lang-auto\">class SeqModel(nn.Module):\n  def __init__(self, input_size, hidden_size, output_size, layers):\n    super(SeqModel, self).__init__()\n\n    self.encoder = nn.GRU(input_size, hidden_size, layers)\n    self.decoder = nn.GRU(input_size, hidden_size, layers)\n    self.out = nn.Linear(hidden_size, output_size)\n\n    self.hidden_state = None\n    self.start_hidden = torch.zeros(layers, hidden_size).to(device)\n\n  def encode(self, embedded_sentence):\n    h_stack = [self.start_hidden for h in range(embedded_sentence.shape[1])]\n    h0 = torch.stack(h_stack, dim=1)\n    \n    outputs, self.hidden_state = self.encoder(embedded_sentence, h0)\n    return outputs\n\n  def decode(self, inputs):\n    outputs_h, self.hidden_state = self.decoder(inputs, self.hidden_state)\n    outputs = F.log_softmax(self.out(outputs_h.squeeze(0)), dim=-1)\n    return outputs\n    \n  def forward(self, embedded_sentence, reply_vectors):\n    enc_outs = self.encode(embedded_sentence)\n    dec_outs = self.decode(reply_vectors)\n    return dec_outs\n<\/code>\nAnd the dataset_unit contains:\nInput-sentence: torch.Size([13, 100])\nTarget-sentence: torch.Size([3, 100])\nTarget-indexes: torch.Size([3])\nI removed second dimension to allow tensors stacking.\nNow I\u2019m trying to do the same demo-train-iteration but for whole batch at once:\n<code class=\"lang-auto\">seq_simple_test = SeqModel(EMBEDDING_SIZE, 100, VOCAB_SIZE, 1).to(device)\n\n# Batch of input sentences, torch.Size([13, 100])\ninp = torch.stack([test_unit[0]['vectors'] for test_unit in test_batch], dim=1)\n# Batch of target sentences, torch.Size([3, 100])\ndinp = torch.stack([test_unit[1]['vectors'] for test_unit in test_batch], dim=1) \n\nprint(inp.size(), dinp.size())\n# ==> torch.Size([13, 2, 100]) torch.Size([3, 2, 100])\n# Where 2 is a batch size\n\ndec_outs = seq_simple_test.forward(inp, dinp)\n\nprint(dec_outs.size())\n# ==> torch.Size([3, 2, 35620])\n# Where 2 is a batch size and 35620 is a vocabulary (and softmax) size\n\nopt_test = optim.Adam(seq_simple_test.parameters(), lr=0.001)\ncriterion_test = nn.NLLLoss()\n\nfor i in range(len(test_batch)):\n  replies = torch.index_select(dec_outs, 1, torch.LongTensor([i]).to(device)).squeeze(1)\n  print(replies.size())\n  # ==> torch.Size([3, 35620])\n  target_reply = test_batch[i][1]['indexes']\n  print(target_reply.size())\n  # ==> torch.Size([3])\n  loss = criterion_test(replies, target_reply)\n  loss.backward()\n\nopt_test.step()\n<\/code>\nAnd I\u2019m getting:\n\nTrying to backward through the graph a second time, but the buffers have already been freed. Specify retain_graph=True when calling backward the first time.\n\nI tried solutions from other threads. For example I can call loss.backward(retain_graph=True), but recalculating graph will decrease my model\u2019s performance (Tried. This takes 7 minutes on 2000-size batch).\nI\u2019ve also tried:\n<code class=\"lang-auto\">loss = 0\nfor i in range(len(test_batch)):\n  replies = torch.index_select(dec_outs, 1, torch.LongTensor([i]).to(device)).squeeze(1)\n  target_reply = test_batch[i][1]['indexes']\n  loss += criterion_test(replies, target_reply)\nloss.backward()\n<\/code>\nThis works, but I\u2019ve read that loss+= criterion(\u2026) is not recommended, because it increases memory usage. I\u2019ve tried this method on batch=2000 samples:\nFirst time:\nRan batch througn model: 37 milliseconds\nCalculating loss for 1minute, 24 seconds\n(1 minute, 24 seconds means ~24 samples in a second. This is 4 times slower than I had earlier)\nAt the second time it falls with:\n\ncuda runtime error (2) : out of memory at \/pytorch\/aten\/src\/THC\/generic\/THCStorage.cu:58\n(I\u2019m using Google Colab)\n\nI\u2019ve also tried to pass whole batch results to criterion, but it seems that NLLLoss doesn\u2019t support it.\nSo, how do I calculate the loss to keep performance high? Thank you in advance!","y":"\n\n\n Defake:\n\nloss = criterion_test(dec_outs.view(-1, vocab_size, batch_size), targets.view(-1, batch_size))\n\n\nI think you need to do criterion_test(dec_outs.view(-1, vocab_size),targets.view(-1))\nIn your case, ( C )-> vocab_size and (N)-> (batch_size*seq_length). I am assuming all the batches have the same sequence length. If not, you\u2019ll have to use pack_padded_sequence and also mask the loss for the pad token.","z":"You have to reset gradients with optimizer.zero_grad() before calling again loss.backward()\nIt doesn\u2019t make sense. I need to accumulate gradient.\nFor example:\n\n\n\n\nHow to implement accumulated gradient in pytorch (i.e. iter_size in caffe prototxt) with link \"https:\/\/discuss.pytorch.org\/t\/how-to-implement-accumulated-gradient-in-pytorch-i-e-iter-size-in-caffe-prototxt\/2522\"\n\n\n    how to can i accumulate gradient during gradient descent in pytorch (i.e. iter_size in caffe prototxt). \nCurrently, my code is: \n     for iter, (images, labels, indices) in enumerate(train_loader, 0):\n \n            optimizer.zero_grad() \n            outputs = net(Variable(images.cuda()))\n            loss    = criterion(outputs, Variable(labels.cuda()))\n            loss.backward()\n            optimizer.step()\n\nDo i do this? \n     for iter in range(N):\n \n            optimizer.zero_grad() \n        \u2026\n  \n\n\nI\u2019m sorry, I said a stupid thing. Are you reinitializing the hidden states between samples yet? If you don\u2019t want to reinizitialize them, have you tried to detach the hidden states between batches? Usually you don\u2019t need the entire history of the hidden states between one example and one other.\nWhen a new tensor is passed to model.encode() function  (or to model.forward()) then self.hidden_state become reinitialized.\n\nreinitializing the hidden states between samples\n\nEarlier, when I passed one training sample to the model at a time, the hidden_state was reinitialized between samples each time.\nBut now I pass whole batch to the model and there\u2019s no need to reinitialize hidden_state between samples.\nThe problem I have is I need to calculate loss for the whole batch. When I\u2019m trying to pass my decoder outputs and my target indexes (torch.Size([3, 2, 35620]) and torch.Size([3, 2])) to NLLLoss, I get an error about inconsistent tensor sizes. It seems NLLLoss asks me to use one-hot encoding for computing loss. But one-hot encoding will greatly increase memory usage, that makes impossible to use big batches in Google Colab.\nNLLLoss doesn\u2019t ask for one-hot encoding. Input to the NLLLoss is (N,C) where C is the number of classes and the target is (N). Are you inputting elements one-by-one and accumulating the loss for each element? Can you recode so that you can input larger batches instead of batch size of 1? Accumulating loss for each element will surely result in a huge memory requirement.\n\nCan you recode so that you can input larger batches instead of batch size of 1?\n\nThis is my question: how to recode it to input the entire batch instead of calculating loss one by one.\nIf I try to input the batch to the criterion, I get:\n\nValueError: Expected target size (7, 35620), got torch.Size([7, 50])\n\nI\u2019m inputting torch.Size([7, 50, 35620]) as decoder outputs and torch.Size([7, 50]) as target indexes.\nWhere 50 is a batch_size, 35620 \u2013 vocabulary size (number of classes), and 7 is a number of predicted words.\nIt works when I pass it one by one (torch.Size([7, 35620]) with torch.Size([7])), and I don\u2019t know how to adapt it for the batch input.\nEDIT:\nOMG, I tried \u201cbatch_first\u201d way earlier and this didn\u2019t work. But I didn\u2019t try to put batch_size to the last position like torch.Size([7, 35620, 50]) and torch.Size([7, 50]). And this works now. Is this correct way to compute loss?\n<code class=\"lang-auto\">opt.zero_grad()\nfor i in range(iterations):\n  ...\n  loss = criterion_test(dec_outs.view(-1, vocab_size, batch_size), targets.view(-1, batch_size))\n  loss.backward()\n## after few iterations\nopt.step()\n<\/code>\n\n\n\n Defake:\n\nloss = criterion_test(dec_outs.view(-1, vocab_size, batch_size), targets.view(-1, batch_size))\n\n\nI think you need to do criterion_test(dec_outs.view(-1, vocab_size),targets.view(-1))\nIn your case, ( C )-> vocab_size and (N)-> (batch_size*seq_length). I am assuming all the batches have the same sequence length. If not, you\u2019ll have to use pack_padded_sequence and also mask the loss for the pad token.\nYes, you are right! The training speed of your calculating method is comparable with that I tried, but loss decreasing is much faster.\nSpeed now is just mind-blowing. 30 times faster that I had earlier. Thank you!!"},{"x":"I\u2019m doing a personal study with the annotated Transformer(http:\/\/nlp.seas.harvard.edu\/2018\/04\/03\/attention.html), and\nI\u2019m trying to use masked_fill operation like following,\nscores = scores.masked_fill(mask == 0, -1e9)\nthe input is written as mask is vary in size because of the data format (text sentence).\nit works fine for first input, (data: [torch.cuda.FloatTensor of size 1x8x21x21 (GPU 0)])\nbut in second input(data: [torch.cuda.FloatTensor of size 1x8x9x9 (GPU 0)]), it gives following error message\n<code class=\"lang-auto\">RuntimeError: The expanded size of the tensor (9) must match the existing size (8) at non-singleton dimension 3. at \/pytorch\/torch\/lib\/TH\/generic\/THTensor.c:309\n<\/code>\nI thought it caused by data type, but it was in vain.\nwhere am I messed up? any suggestion would be very helpful.","y":"From the error mesage, it is a size issue on the 3rd dimension, where one is of size 8 and the other of size 9.\nI would print the size of the tensors before the operation to check the dimensions.","z":"From the error mesage, it is a size issue on the 3rd dimension, where one is of size 8 and the other of size 9.\nI would print the size of the tensors before the operation to check the dimensions.\nthank you for your reply, and I found the problem while this post was suspended due to my first written post!\nmask size was changed due to my debugging code inserted while ago."},{"x":"in the task of NLP, such as neural machine translation, the source sentences have different length, if I want to put a batch in the RNN, they must have the same length.\nI have read many examples but they only put one sample in the RNN at a time, I wonder if I padding zeros when use word embeddings before input the network, does it work?  will these zeros change my final consequence or not?\nI want to know how to do this job best","y":"Try pad_sequence with link \"https:\/\/pytorch.org\/docs\/stable\/nn.html#pad-sequence\".","z":"Try pad_sequence with link \"https:\/\/pytorch.org\/docs\/stable\/nn.html#pad-sequence\".\nAs to consequences of padding with zeros, there is none. From a very high level view, think of it as writing a sentence with many white spaces before it. This does not alter the meaning in anyway, to the reader the sentence is the same with or without preceding white spaces, neural networks are able to learn this indifference."},{"x":"I know its really simple but I am new to NN and I am having a lot of difficulties understanding the relation between the math and the PyTorch code. I am trying to replicate a paper that uses attention weights and I need to implement this feed forward neural network with two inputs.\n<code class=\"lang-markdown\">$$ c_i = W_1 tanh(W_2m_i + W_3v_a + b_i$$\n<\/code>\n$m_i$ is a vector of embeddings of a single word 1 for each in the sentence and $v_a$ is the embedding vector of\nThe model parameters are: $$W_1 \\in R^{1xd}, W_2 \\in R^{dxd}, W_3 \\in R^{dxd}, b_1 \\in {}$$\nThe resulting  ${c_1, c_2,..., c_N}$  after being passed through a softmax will represent the weight that is given to each word in the sentence","y":"Hi, I guess your model will be like:\n<code class=\"lang-python\">import torch\nimport torch.nn as nn\nimport torch.nn.functional as F\nfrom torch.autograd import Variable\n\nclass YourModel(nn.Module):\n    def __init__(self):\n        super(YourModel, self).__init__()\n        self.lin1 = nn.Linear(5, 5)\n        self.lin2 = nn.Linear(5, 5)\n\n    def forward(self, m_i, v_a):\n        y1 = self.lin1(m_i)\n        y2 = self.lin2(v_a)\n        y = F.tanh(y1+y2)\n        y = F.softmax(y)\n        return y\n<\/code>\nThen,\n<code class=\"lang-python\">model = YourModel()\nm_i = Variable(torch.Tensor([1,2,3,4,5]))\nv_a = Variable(torch.Tensor([6,7,8,9,10]))\noutput = model.forward(m_i, v_a)\n<\/code>\nyou can get output as follows:\n<code class=\"lang-python\">Variable containing:\n 0.0563\n 0.4156\n 0.4156\n 0.0563\n 0.0563\n[torch.FloatTensor of size 5]\n\n<\/code>","z":"Hi, I guess your model will be like:\n<code class=\"lang-python\">import torch\nimport torch.nn as nn\nimport torch.nn.functional as F\nfrom torch.autograd import Variable\n\nclass YourModel(nn.Module):\n    def __init__(self):\n        super(YourModel, self).__init__()\n        self.lin1 = nn.Linear(5, 5)\n        self.lin2 = nn.Linear(5, 5)\n\n    def forward(self, m_i, v_a):\n        y1 = self.lin1(m_i)\n        y2 = self.lin2(v_a)\n        y = F.tanh(y1+y2)\n        y = F.softmax(y)\n        return y\n<\/code>\nThen,\n<code class=\"lang-python\">model = YourModel()\nm_i = Variable(torch.Tensor([1,2,3,4,5]))\nv_a = Variable(torch.Tensor([6,7,8,9,10]))\noutput = model.forward(m_i, v_a)\n<\/code>\nyou can get output as follows:\n<code class=\"lang-python\">Variable containing:\n 0.0563\n 0.4156\n 0.4156\n 0.0563\n 0.0563\n[torch.FloatTensor of size 5]\n\n<\/code>\nThanks Ken, thats a nice example, i got the intuition now!\nOne quick question, i think i should also pass the the y variable through a linear layer since there is W_1 multiplying it in the original function.\n\n$$ c_i =  W_1( tanh(W_2m_i + W_3v_a + b_i )$$\n\nYou think this is necesary or just applying the softmax straight away is ok?\n<code class=\"lang-auto\">def forward(self, m_i, v_a):\n        y1 = self.lin1(m_i)\n        y2 = self.lin2(v_a)\n        y = F.tanh(y1+y2)\n        y = self.lin3(y)\n        y = F.softmax(y)\n        return y\n<\/code>\nYes, you need to add lin3 "},{"x":"Hello all,\nI have been trying to recreate the Softmargin Softmax function found here: <a>Softmargin Softmax paper<\/a>.\nI took the current Softmax function in Activation.py and changed the the forward step with my custom Python calculation.\n<code class=\"lang-auto\">def forward(self, input):\n        m = 0\n        input = input.exp()\n        sumexp = torch.sum(input,dim=1)\n        expm = math.exp(-m)\n    \n        # calculate softmargin softmax\n        for x in range (input.size(0)):\n            for y in range(input.size(1)):\n                input[x,y] = (input[x,y]*expm)\/(sumexp[x] - input[x,y] + (input[x,y] * expm))\n         \n        #normalize the weights\n        sumnorm = torch.sum(input,dim=1)\n        for x in range (input.size(0)):\n            for y in range(input.size(1)):\n                input[x,y] = input[x,y]\/sumnorm[x]\n\n        return input\n<\/code>\nIf I now run my model, it does not crash, but seems to keep calculating something whitout progressing in the model.\nDid I look over some important step? Or is it just so inefficient?\nAny help is appreciated.\nedit\nThe function appears to be running very,very slow.\nDoes somebody know how to speed it up?","y":"Prefer not to use for loops, try to vectorize your code as much as possible.\nRefer this old question I asked with link \"https:\/\/discuss.pytorch.org\/t\/softargmax-in-pytorch-in-2d\/17931\" for checking my implementation of softargmax which I believe you are lookin for. It\u2019s pretty decent and fast\u2026","z":"Prefer not to use for loops, try to vectorize your code as much as possible.\nRefer this old question I asked with link \"https:\/\/discuss.pytorch.org\/t\/softargmax-in-pytorch-in-2d\/17931\" for checking my implementation of softargmax which I believe you are lookin for. It\u2019s pretty decent and fast\u2026\nThank you very much the pointer, I was able to redo do the function and now it runs (almost) as fast as the original softmax! For reference here is the function (used for a 3D tensor and calculated over the rows):\n<code class=\"lang-auto\">    def forward(self, input):\n        m = 0\n        expm = math.exp(-m) #calculate m\n        input = torch.exp(input)\n        \n        #keep track of the original dimensions\n        s0 = input.size(0)\n        s1 = input.size(1)\n        s2 = input.size(2)\n        \n        #input = input.transpose(1,2).contiguous().view(-1,s1) #use if you want to calculate over columns instead of rows\n        input = input.view(-1,s2)\n        \n        #calculate softmargin softmax\n        expsum = torch.sum(input, dim=1).view(-1,1) # sum and reshape for softmax\n        input = (input*expm)\/(expsum - input + (input*expm))\n        \n        #normalize\n        normsum = torch.sum(input, dim=1).view(-1,1) #sum and reshape for normalization\n        input = input\/normsum\n        \n        input = input.view(s0,s1,s2)\n        #input = input.view(s0,s2,s1).transpose(1,2).contiguous() #  #use if you want to calculate over columns instead of rows\n        \n        return input\n<\/code>"},{"x":"I have been working on sequence-to-sequence models and tried many different variants. I saved and loaded the models. They have different levels of performance, but most seem to work within expectation.\nNow I\u2019m trying the idea of sharing the embedding matrix and the output linear layer\u2019s matrix. On this forum, I found a way of doing that. It looks like a hack to me, but initially it seems to work. Later I realized saving and loading the model breaks it.\nWithin the model class, I do the following:\n<code class=\"lang-auto\">self.embeds = nn.Embedding(vocab_size, embed_len)\nself.out_linear = nn.Linear(embed_len, vocab_size, bias=False)\nself.out_linear.weight.data = self.embeds.weight.data\n<\/code>\nDuring training, this actually achieves the best validation error so far. However, when I attempt to load a saved model, the outputs turn out to be complete gibberish. The code I use to save and load the model is:\n<code class=\"lang-auto\">torch.save(model.state_dict(), file)\nmodel.load_state_dict(torch.load(file))\n<\/code>\nI have no reason to believe the error is elsewhere, since the weight sharing is the only change between this model and the previous model that works ok. Can anyone tell me what is wrong? What is the best way to share the matrix between the embedding layer and the linear layer?\nAny help would be greatly appreciated!","y":"I figured it out. Basically I should use torch.matmul with self.embeds.weight. Doing this solved the saving \/ loading problem. The hacky way apparently doesn\u2019t work!\nlogits is a Variable of shape (Batch, Embedding_Dimension) and self.embeds.weight is a Parameter with shape (Vocabulary_Size, Embedding_Dimension)\n<code class=\"lang-auto\">logits = torch.matmul(logits, self.embeds.weight.t())\n<\/code>","z":"I figured it out. Basically I should use torch.matmul with self.embeds.weight. Doing this solved the saving \/ loading problem. The hacky way apparently doesn\u2019t work!\nlogits is a Variable of shape (Batch, Embedding_Dimension) and self.embeds.weight is a Parameter with shape (Vocabulary_Size, Embedding_Dimension)\n<code class=\"lang-auto\">logits = torch.matmul(logits, self.embeds.weight.t())\n<\/code>"},{"x":"I am creating a layer which will acts as both embedding as well as linear layer. This is to tie the weights.\n<code class=\"lang-auto\">class embedding_linear(nn.Module):\n    def  __init__(self,vocab_size, dmodel=dmodel, pad=True):\n        '''\n        Tied weights for decoder embedding layer and pre-softmax linear layer.\n        \n        vocab_size: size of vocabulary used. It may be different for both source and target\n        dmodel: dimension of the word vector\n        pad: the pad index in the vocabulary\n        '''\n        super(embedding_linear,self).__init__()\n        self.dmodel = dmodel\n        self.weights = nn.Parameter(torch.Tensor(vocab_size,dmodel))\n        self.bias = nn.Parameter(torch.Tensor(dmodel))\n        self.weights.data.normal_(-1,1)\n        if pad:\n            self.pad_idx = 0\n            self.weights.data[0].fill_(0)\n        else:\n            self.pad_idx = -1\n        \n        \n    def forward(self, inputs, emb=True):\n        if emb:\n            outputs = F.embedding(inputs, self.weights * (self.dmodel ** 0.5), self.pad_idx, False,2, False, False)\n        else:\n            outputs = F.linear(inputs,self.weights.t(),self.bias)\n        return outputs\n            \n<\/code>\nBut when I test the layer with the following command it always returns zeroes\nboth negative and positive zeroes\nimage.png1100\u00d7185 15.7 KB with link \"https:\/\/discuss.pytorch.org\/uploads\/default\/original\/2X\/7\/72661d89f961d200150a09d26d9d2a5453d362aa.png\"\ninitially I thought it must be precision problem and tried casting the outputs to other types by using the .type(), But nothing changed.\nI then tried using the official embedding layer and got this output\nimage.png1090\u00d7167 18.4 KB with link \"https:\/\/discuss.pytorch.org\/uploads\/default\/original\/2X\/d\/dc95c84a9a74e006c1cdca9f990df9a726eb84ee.png\"\nwhere it was not all zeroes.\nI also matched my implemention from the official repo and they were almost same.\nPlease tell me what to do? and why is it returning all zeroes.","y":"Figured it out!!\nCorrection is max_norm should be None not False\n<code class=\"lang-auto\">outputs = F.embedding(inputs, self.weights * (self.dmodel ** 0.5), self.pad_idx, None,2, False, False)\n<\/code>","z":"Figured it out!!\nCorrection is max_norm should be None not False\n<code class=\"lang-auto\">outputs = F.embedding(inputs, self.weights * (self.dmodel ** 0.5), self.pad_idx, None,2, False, False)\n<\/code>"},{"x":"Hi, I have successfully completed training a BiLSTM model which has been saved as model.model. When I loaded it using load_state_dict(torch.load()), it gives me an error says:\nUnexpected key(s) in state_dict: \u201clayer1.lstm.weight_ih_l1\u201d, \u201clayer1.lstm.weight_hh_l1\u201d, \u201clayer1.lstm.bias_ih_l1\u201d, \u201clayer1.lstm.bias_hh_l1\u201d, \u201clayer1.lstm.weight_ih_l1_reverse\u201d, \u201clayer1.lstm.weight_hh_l1_reverse\u201d, \u201clayer1.lstm.bias_ih_l1_reverse\u201d, \u201clayer1.lstm.bias_hh_l1_reverse\u201d.\nAny help is much appreciated","y":"Maybe your trained model had two or more layers in the layer1.lstm (l1 is the second) and the model you\u2019re loading it to only one?\nBest regards\nThomas","z":"Maybe your trained model had two or more layers in the layer1.lstm (l1 is the second) and the model you\u2019re loading it to only one?\nBest regards\nThomas"},{"x":"Hello everyone! Does anyone know how to convert this reparametrization function to pytorch? In particular, I don\u2019t know how to properly do the  tf.random.normal(shape=(batch, latent_dim)) in pytorch.\n<code class=\"lang-auto\">batch, latent_dim = z_mean.shape\nepsilon = tf.random.normal(shape=(batch, latent_dim))\n z = z_mean + tf.math.exp(0.5 * z_logsigma) * epsilon\n<\/code>\nOn another topic, does anyone know which is the best alternative for tf.nn.sigmoid_cross_entropy_with_logits() in pytorch?\nThank you","y":"Ok so for the first function you could do something like this\n<code class=\"lang-auto\">epsilon = torch.normal(0,1, size=(batch, latent_dim))\nz = z_mean + torch.exp(0.5 * z_logsigma) * epsilon\n<\/code>\nand for the loss you can use BCE with logits loss here with link \"https:\/\/pytorch.org\/docs\/stable\/generated\/torch.nn.BCEWithLogitsLoss.html#torch.nn.BCEWithLogitsLoss\".","z":"Ok so for the first function you could do something like this\n<code class=\"lang-auto\">epsilon = torch.normal(0,1, size=(batch, latent_dim))\nz = z_mean + torch.exp(0.5 * z_logsigma) * epsilon\n<\/code>\nand for the loss you can use BCE with logits loss here with link \"https:\/\/pytorch.org\/docs\/stable\/generated\/torch.nn.BCEWithLogitsLoss.html#torch.nn.BCEWithLogitsLoss\"."},{"x":"Hi all,\nI just shifted from keras and finding some difficulty to validate my code. Currently I am training a LSTM network\nfor text generation on a character level but I observe that my loss is not decreasing. I am now doubting whether my model is wrongly built. Kindly someone help me with this.\nHeres the code:\n\nclass CharLevelLanguageModel(torch.nn.Module):\ndef init(self,vocab_size,emb_dim,hidden_dim,batch_size):\nsuper(CharLevelLanguageModel,self).init()\nself.embedddings = torch.nn.Embedding(vocab_size,emb_dim,padding_idx=0)\nself.lstm = torch.nn.LSTM(emb_dim,hidden_dim,1,batch_first=True)\nself.linear = torch.nn.Linear(hidden_dim,vocab_size)\nself.batch_size = batch_size\nself.hidden_dim = hidden_dim\nself.hidden_state = self.init_hidden()\ndef init_hidden(self):\n    return (Variable(torch.zeros(1,self.batch_size,self.hidden_dim)))\n\ndef forward(self,x):\n    embeds = self.embedddings(x)\n    output,self.hidden_state = self.lstm(embeds)\n    return F.log_softmax(self.linear(F.tanh(output[:,-1,:])))\n\ntrain = data_utils.TensorDataset(torch.LongTensor(dataX),torch.LongTensor(dataY))\ntrain_loader = data_utils.DataLoader(train,batch_size=100,drop_last=True)\nmodel = CharLevelLanguageModel(len(char_to_id)+1,100,50,100)\ncriterion = torch.nn.NLLLoss()\noptimizer = torch.optim.Adam(model.parameters())\nlosses = []\nfor i in range(3):\ntotal_loss = torch.FloatTensor([0])\nfor batch_idx,train in enumerate(train_loader):\n    model.init_hidden()\n    x,y = Variable(train[0]),Variable(train[1])\n    y_pred = model(x)\n    #print()\n    loss = criterion(y_pred,y)\n    total_loss+=loss.data\n    loss.backward()\n    optimizer.zero_grad()\n    optimizer.step()\nlosses.append(total_loss)\n\n\n\nBlockquote\n\nThe losses are:\n\n[\n6798.0005\n[torch.FloatTensor of size 1],\n6798.0005\n[torch.FloatTensor of size 1],\n6798.0005\n[torch.FloatTensor of size 1]]\n","y":"Zero the grad before the backward, bot after.\nBest regards\nThomas","z":"Zero the grad before the backward, bot after.\nBest regards\nThomas\nGreat . It looks like the error is decreasing now. Thanks a lot.\nOther than that is the model built correctly?"},{"x":"In the PyTorch Seq2Seq tutorial with link \"http:\/\/pytorch.org\/tutorials\/intermediate\/seq2seq_translation_tutorial.html\", the embedding and hidden context vectors were forces into a (1, 1, -1) shape.\nIs there a reason for the (1, 1, -1) shape? Why do we need a 1x1x* 3D vector to keep a single vector in the inner most tensor?\nWhy can\u2019t we just use a (-1) 1-dimensional vector?","y":"Because it then gets passed to a GRU which needs input of shape (seq_len, batch_size, features).","z":"Because it then gets passed to a GRU which needs input of shape (seq_len, batch_size, features).\nThanks for the info!\nI understand that it\u2019s doing SGD where batch_size=1.\nBut any idea why is the seq_len=1 in the case of the tutorial?\n\n\n\n alvations:\n\nany idea why is the seq_len=1 in the case of the tutorial?\n\n\npresumably because the elements of the sequence are fed to the GRU one at a time which is necessary given that at each timestep you want to feed the output of the previous timestep back in to the GRU.\nThanks again for the clarification.\nAre there instances where GRU take more than one input? If not, is seq_len always =1 for GRU?\nIn this case we need seq_len=1 because we need to use the output of the previous timestep as the input to the next. However when the input timesteps are available in advance you could feed it the entire sequence in one go."},{"x":"I wanted to train a char-lstm with the foll. parameters using one-hot encoding of the characters. My input was a 3D tensor of [batchsize x sequence length x one-hot dimension] and my target was also a 3D tensor of the same size.\nTo use cross entropy loss, I had to first flatten my input to a 2D tensor of [seq len*batchsize x one hot dim] and had to convert my target from one-hot to a suitable 1D tensor. I then had to use .squeeze() because .view() was not sufficient and I also had to typecast from FloatTensor to LongTensor. I hope these compatibility issues are fixed in the next release so that we can run cross-entropy loss on the inputs and targets just as they are.","y":"Questions for you:\n\nWhy couldn\u2019t you just load target labels as indices? This will be more memory efficient and arguably run faster in calculating cross entropy loss.\n\n\n\n\n\n\n samlanka:\n\nI then had to use .squeeze() because .view() was not sufficient and I also had to typecast from FloatTensor to LongTensor.\n\n\nIsn\u2019t it nature that you would get LongTensor when converting one-hot to label indices? Every straightforward way to convert in PyTorch I can think of gives this property, e.g. torch.max.\n3)\n\n\n\n samlanka:\n\nI then had to use .squeeze() because .view() was not sufficient\n\n\n.view can always do what .squeeze do.\n4)\n\n\n\n samlanka:\n\nI had to first flatten my input to a 2D tensor of [seq len*batchsize x one hot dim] and had to convert my target from one-hot to a suitable 1D tensor.\n\n\nYou don\u2019t need to \u201cflatten\u201d your input. No where in the doc says that http:\/\/pytorch.org\/docs\/master\/nn.html#recurrent-layers, and RNN modules always only accepts 3D input I believe. Simply set batch_first=True should be fine.\nMoreover, since data dim (i.e. one hot dim) is last dimension, shouldn\u2019t it actually be very simple and straightforward? Something along the lines of\n<code class=\"lang-auto\">loss = criterion(out.view(-1, data_dim), labels.max(-1)[1].view(-1))\n<\/code>\nshould just work.","z":"Questions for you:\n\nWhy couldn\u2019t you just load target labels as indices? This will be more memory efficient and arguably run faster in calculating cross entropy loss.\n\n\n\n\n\n\n samlanka:\n\nI then had to use .squeeze() because .view() was not sufficient and I also had to typecast from FloatTensor to LongTensor.\n\n\nIsn\u2019t it nature that you would get LongTensor when converting one-hot to label indices? Every straightforward way to convert in PyTorch I can think of gives this property, e.g. torch.max.\n3)\n\n\n\n samlanka:\n\nI then had to use .squeeze() because .view() was not sufficient\n\n\n.view can always do what .squeeze do.\n4)\n\n\n\n samlanka:\n\nI had to first flatten my input to a 2D tensor of [seq len*batchsize x one hot dim] and had to convert my target from one-hot to a suitable 1D tensor.\n\n\nYou don\u2019t need to \u201cflatten\u201d your input. No where in the doc says that http:\/\/pytorch.org\/docs\/master\/nn.html#recurrent-layers, and RNN modules always only accepts 3D input I believe. Simply set batch_first=True should be fine.\nMoreover, since data dim (i.e. one hot dim) is last dimension, shouldn\u2019t it actually be very simple and straightforward? Something along the lines of\n<code class=\"lang-auto\">loss = criterion(out.view(-1, data_dim), labels.max(-1)[1].view(-1))\n<\/code>\nshould just work.\nThanks for your reply! \nI agree that index labels are more memory efficient, but having different encoders for inputs and labels just made things slightly more difficult.\nThe flattening of my input was for Cross-Entropy Loss.\nIn your command, out.view() yields a [1, data_dim] tensor. This won\u2019t pass through CE Loss. You need a .squeeze() to get a 1D tensor. At least that\u2019s how it was for me \nIt will give you a [batch*seq, data_dim] tensor. CrossEntropyCriterion certainly requires input to be 2D."},{"x":"Hi,\nI want to train an RNN in batches. Suppose:\n<code class=\"lang-auto\">batch_size = 2\nseq_len = 3\ninput_size = 5\n<\/code>\nAnd the data comes in shape of (batch_size, seq_len, input_size). For example:\n<code class=\"lang-auto\">x = np.array([\n    [[0,0,0,0,0], [1,1,1,1,1], [2,2,2,2,2]],\n    [[3,3,3,3,3], [4,4,4,4,4], [5,5,5,5,5]]\n])\n\nx = torch.from_numpy(x)\n# (0 ,.,.) = \n#  0  0  0  0  0\n#  1  1  1  1  1\n#  2  2  2  2  2\n#\n# (1 ,.,.) = \n#  3  3  3  3  3\n#  4  4  4  4  4\n#  5  5  5  5  5\n# [torch.LongTensor of size 2x3x5]\n<\/code>\nNow, I know that I have to reshape it to (seq_len, batch_size, input_size) to be able to feed it to RNN, but I can\u2019t figure how. view() doesn\u2019t do it properly:\n<code class=\"lang-auto\">x.view(3, 2, 5)\n#  (0 ,.,.) = \n#  0  0  0  0  0\n#  1  1  1  1  1\n#\n#  (1 ,.,.) = \n#  2  2  2  2  2\n#  3  3  3  3  3\n#\n#  (2 ,.,.) = \n#  4  4  4  4  4\n#  5  5  5  5  5\n# [torch.LongTensor of size 3x2x5]\n<\/code>\nwhich I think is wrong. I guess the resulting tensor should be like this:\n<code class=\"lang-auto\">  (0 ,.,.) = \n  0  0  0  0  0\n  3  3  3  3  3\n\n  (1 ,.,.) = \n  1  1  1  1  1\n  4  4  4  4  4 \n\n  (2 ,.,.) = \n  2  2  2  2  2\n  5  5  5  5  5 \n<\/code>\n\nAm I right?\nIf yes, how can I do it? Should I initialize a tensor and populate it iteratively or are there better ways?\n\nThanks","y":"<code class=\"lang-auto\">x.permute(1,0,2)\n<\/code>","z":"<code class=\"lang-auto\">x.permute(1,0,2)\n<\/code>"},{"x":"I have a standard dataloader which loads images.\nOn top of every image I want to add a static tensor.\nBut I  want to clamp this to (0,1).\nThis new image is used to train a model.\nThe following code roughly show the important steps.\n(everything is on gpu)\nstatic_tensor = torch.load(path)\nfor img in dataloader:\n  img = img.cuda()\n  addition_tensor = img + static_tensor\n  clamped_tensor = addition_tensor.clamp(0,1)\n  eval = model(clamped_tensor)\n  loss = criterion (eval, label)\n  loss.backward()\n  optimizer.step()\n\nThis creates out of memory errors.\nBut if I change the clamping into\nclamped_tensor = additon_tensor.data.clamp(0,1)\n\nit no longer creates this error.\nWhat is the reason behind this?\nEdit: I noticed that my static tensor has requires_grad = True.\nAm I correct to assume that this tensor \u2018saves\u2019 the autograd with the model and the graph of this gets bigger in every instance of the for loop?","y":"Hi,\nYour static Tensor should not have requires_grad=True I guess.\nAnd in that case, doing any op here will increase the memory usage as we need to save some values for the backward.\nNote that you should never use .data in general!\nHere you can use .detach() which will have similar behavior but will avoid weird side effects!","z":"Hi,\nYour static Tensor should not have requires_grad=True I guess.\nAnd in that case, doing any op here will increase the memory usage as we need to save some values for the backward.\nNote that you should never use .data in general!\nHere you can use .detach() which will have similar behavior but will avoid weird side effects!"},{"x":"I am trying to use a reference matrix to slice another matrix with more dimensions:\n<code class=\"lang-python\"># The tensor that we use for indexing\nreference = torch.rand(bsize, nfeat)\nindex = torch.argsort(reference)[:, :bottomk]\n# The target tensor that we want to slice\ntarget = torch.rand(bsize, nsample, nfeat)\n<\/code>\nHow can I perform slicing using index? I want to automatically broadcast the second dimension. Can you suggest me how to do it?\n<code class=\"lang-python\"># The for-loop version. But I want a more elegant and faster version \n# that does not loop over batch\nfor i in range(bsize):\n    target[i, :, index[i, :]] = 0\n<\/code>","y":"can\u2019t say that gather() is fast, but following should be a loop-less equivalent for reading\n<code class=\"lang-auto\">target.gather(-1,index.unsqueeze(1).expand(-1,nsamples,-1))\n<\/code>\nfor writing, look at Tensor.scatter_ or Tensor.masked_fill_, expanding index\/mask similarly","z":"Why target is size of bsize * nsample * nfeat. How do you want to slice when your reference is size of bsize * nfeat. Please give an a example  in detail. Probably I can help with it.\nI edited the question to clarify your question.\ncan\u2019t say that gather() is fast, but following should be a loop-less equivalent for reading\n<code class=\"lang-auto\">target.gather(-1,index.unsqueeze(1).expand(-1,nsamples,-1))\n<\/code>\nfor writing, look at Tensor.scatter_ or Tensor.masked_fill_, expanding index\/mask similarly"},{"x":"Hi!\nI am moving tensors between the CPU and GPU memory with .to(device) and .cpu().\nI found out that all tensor that get in or out of the nn.Linear layer are locked in GPU memory.\nThat is why I created my own Linear layer and I found out that if require_grad=False I get the expected  use of memory in the GPU.\nIf require_grad=True, then I cannot swap the tensors to CPU memory.\n.cpu() or deleting the reference with del tensor_var have no impact if require_grad=True.\nCan someone explain the background to this behaviour and maybe suggest a workaround to be able to use Pytorch\u2019s autograd?\nA huge thank you in advance for every answer!","y":"\n\n\n marcelwa:\n\nThe tensors should stay part of the computation graph so that backpropagation works.\nThe question is why does Pytorch not free the memory in the GPU for tensors that are part of the computation graph?\n\n\nPyTorch doesn\u2019t free these tensors because they are needed for the backward pass to work properly, since they are part of the computation graph.\nIn your example you are creating new tensors x, w, and l, which doesn\u2019t delete the computation graph. I.e. l.mean().backward() would still work.\nYou won\u2019t be able to access the .grad attribute from x and w anymore, but the gradients will still be calculated. If you change the CPU tensors to e.g. x1 and w1, you would be able to print x.grad and w.grad after the backward pass.\nYou cannot move the computation graph to the CPU after its creation by moving leaf variables to the CPU (at least I\u2019m not aware of a method to do so).","z":"Could you explain what \u201clocked in GPU memory\u201d means and how you are measuring it?\nNote that PyTorch caches the GPU memory, so after deleting a tensor, the memory will not be immediately released to avoid synchronizing (and thus expensive) memory reallocations.\nTo measure the GPU memory usage, I use\n<code class=\"lang-auto\">stats = torch.cuda.memory_stats()\ncurrent_active_byte =  stats[\"active_bytes.all.current\"]\n<\/code>\nand calculate the difference before and after the statement of interest.\nThe value of current_active_byte does not decrease with tensor_var.cpu() or del tensor_var if  require_grad=True.\nI cannot reproduce this behavior using this code snippet:\n<code class=\"lang-python\">def print_active_bytes():\n    stats = torch.cuda.memory_stats()\n    current_active_byte =  stats[\"active_bytes.all.current\"]\n    print(current_active_byte)\n\n\n# initial usage\nprint_active_bytes()\n> 0\n\n# vanilla tensor\nx = torch.randn(1024, device='cuda')\nprint_active_bytes()\n> 4096\n\ndel x\nprint_active_bytes()\n> 0\n\n# requires_grad=True\nx = torch.randn(1024, device='cuda', requires_grad=True)\nprint_active_bytes()\n> 4096\n\ndel x\nprint_active_bytes()\n> 0\n<\/code>\nI can reproduce it with the following code\n<code class=\"lang-auto\">import torch\n\ndef active_bytes():\n    stats = torch.cuda.memory_stats()\n    current_active_byte =  stats['active_bytes.all.current']\n    return current_active_byte\n\n\n# initial usage\nprint(\"Init usage {}\". format(active_bytes()))\n\n# vanilla tensor\nx = torch.randn((256, 128), device='cuda')\nw = torch.randn((128, 512), device='cuda')\nl = torch.matmul(x, w)\nprint(\"Vanilla tensor {}\". format(active_bytes()))\n\ndel x\nprint(\"Vanilla tensor: del x {}\". format(active_bytes()))\ndel w\nprint(\"Vanilla tensor: del w {}\". format(active_bytes()))\nl = l.cpu()\nprint(\"Vanilla tensor: l = l.cpu() {}\". format(active_bytes()))\n\n# requires_grad=True\nx = torch.randn((256, 128), device='cuda', requires_grad=True)\nw = torch.randn((128, 512), device='cuda', requires_grad=True)\nl = torch.matmul(x, w)\nprint(\"requires_grad=True {}\". format(active_bytes()))\n\ndel x\nprint(\"requires_grad=True: del x {}\". format(active_bytes()))\ndel w\nprint(\"requires_grad=True: del w {}\". format(active_bytes()))\nl = l.cpu()\nprint(\"requires_grad=True: l = l.cpu() {}\". format(active_bytes()))\n<\/code>\nThe output I get is\n<code class=\"lang-auto\">Init usage 0\nVanilla tensor 917504\nVanilla tensor: del x 786432\nVanilla tensor: del w 524288\nVanilla tensor: l = l.cpu() 0\nrequires_grad=True 917504\nrequires_grad=True: del x 917504\nrequires_grad=True: del w 917504\nrequires_grad=True: l = l.cpu() 393216\n<\/code>\nIn my \u201cproduction\u201d code .cpu() does not free up memory as it does in this example, but I did not come up with a code snippet to show that behaviour yet.\n\n\n\n marcelwa:\n\nIn my \u201cproduction\u201d code .cpu() does not free up memory as it does in this example\n\n\nThis is expected, as moving l to the CPU won\u2019t detach it from the computation graph.\nAdd\n<code class=\"lang-python\">l = l.detach()\nprint(\"requires_grad=True: l = l.cpu() {}\". format(active_bytes()))\n<\/code>\nafter the cpu() operation, the graph should be freed, and you should get 0 active bytes again.\nNote that cuda(), cpu() and to() operations are differentiable, so that the computation graph will be kept alive and you can backpropagate through these operations.\nIf I understood it correctly, when I would detach the tensor, then backward() does not work anymore and then there would no point of using Pytorch anymore.\nAdditionally, changing the second part to\n<code class=\"lang-auto\"># requires_grad=True\nx = torch.randn((256, 128), device='cuda', requires_grad=True)\nw = torch.randn((128, 512), device='cuda', requires_grad=True)\nl = torch.matmul(x, w)\nprint(\"requires_grad=True {}\". format(active_bytes()))\n\nx = x.cpu()\nprint(\"requires_grad=True: x = x.cpu() {}\". format(active_bytes()))\nw = w.cpu()\nprint(\"requires_grad=True: w = w.cpu() {}\". format(active_bytes()))\nl = l.cpu()\nprint(\"requires_grad=True: l = l.cpu() {}\". format(active_bytes()))\n<\/code>\nwith the output\n<code class=\"lang-auto\">requires_grad=True 917504\nrequires_grad=True: x = x.cpu() 917504\nrequires_grad=True: w = w.cpu() 917504\nrequires_grad=True: l = l.cpu() 393216\n<\/code>\nshows that the inputs to the matrix multiplication are not freed.\nThe tensors should stay part of the computation graph so that backpropagation works.\nThe question is why does Pytorch not free the memory in the GPU for tensors that are part of the computation graph?\nThe tensors are not lost, they are just stored somewhere else.\n\n\n\n marcelwa:\n\nThe tensors should stay part of the computation graph so that backpropagation works.\nThe question is why does Pytorch not free the memory in the GPU for tensors that are part of the computation graph?\n\n\nPyTorch doesn\u2019t free these tensors because they are needed for the backward pass to work properly, since they are part of the computation graph.\nIn your example you are creating new tensors x, w, and l, which doesn\u2019t delete the computation graph. I.e. l.mean().backward() would still work.\nYou won\u2019t be able to access the .grad attribute from x and w anymore, but the gradients will still be calculated. If you change the CPU tensors to e.g. x1 and w1, you would be able to print x.grad and w.grad after the backward pass.\nYou cannot move the computation graph to the CPU after its creation by moving leaf variables to the CPU (at least I\u2019m not aware of a method to do so).\n\n\n\n ptrblck:\n\nYou cannot move the computation graph to the CPU after its creation by moving leaf variables to the CPU (at least I\u2019m not aware of a method to do so).\n\n\nAlright, that\u2019s not what I have hoped for  but thank you a lot for your help!"},{"x":"Please, advise how to make this work.\n<code class=\"lang-auto\">class HalfReLU(nn.Module):\n\tdef __init__(self):\n\t\tsuper(HalfReLU, self).__init__()\n\n\tdef forward(self, x):\n\t\thalf = x.shape[1]\/\/2\n\t\tx[:, :half] = nn.functional.relu(x[:, :half], inplace=False)\n\t\treturn x\n<\/code>\nI\u2019m getting\n\nRuntimeError: one of the variables needed for gradient computation has been modified by an inplace operation: [torch.cuda.FloatTensor [128, 128, 8, 8]], which is output 0 of SliceBackward, is at version 1; expected version 0 instead. Hint: the backtrace further above shows the operation that failed to compute its gradient. The variable in question was changed in there or anywhere later. Good luck!\n\nWhether relu is inplace or not doesn\u2019t matter. Works (or just doesn\u2019t give errors, don\u2019t know for sure) in case of things like\n<code class=\"lang-auto\">\t\tx[:, :half] *= 2\n<\/code>\nand\n<code class=\"lang-auto\">\t\tx[:, :half] = nn.functional.sigmoid(x[:, :half])\n<\/code>\nbut not in case of relu. I\u2019m guessing it\u2019s sort of a bug\/edge case? For leaky_relu also doesn\u2019t work.","y":"I\u2019d just use torch.cat of the pristine and relued parts.","z":"I\u2019d just use torch.cat of the pristine and relued parts.\nWorks. Very slow vs plain ReLU, though. But even simply\n<code class=\"lang-auto\">x = nn.functional.relu(x[:, :], inplace=True)\n<\/code>\nis itself halfway slower vs\n<code class=\"lang-auto\">x = nn.functional.relu(x, inplace=True)\n<\/code>\nNumbers? (input sizes, cpu vs. gpu, \u2026)\nFor me and a decently sized array, it doesn\u2019t matter:\n<code class=\"lang-python\">x = torch.randn(1000,1000)\n%timeit torch.nn.functional.relu(x[:, :], inplace=True)\n71.8 \u00b5s \u00b1 59.1 ns per loop (mean \u00b1 std. dev. of 7 runs, 10000 loops each)\n\n%timeit torch.nn.functional.relu(x, inplace=True)\n69 \u00b5s \u00b1 141 ns per loop (mean \u00b1 std. dev. of 7 runs, 10000 loops each)\n<\/code>\nIf relu is slowing down your network, you\u2019re doing something wrong.\nYou can be a tiny bit faster than relu + cat if you clone + use inplace relu (which works with autograd). But TBH I\u2019d stay in the functional world. Optimizing ReLU likely isn\u2019t a huge win.\nInteresting. I\u2019ve tried changing that for a large network, so I can\u2019t say what specific size it was slow for. Regarding slowdowns and autograd, what can you say about TensorFlow\u2019s XLA performance? From benchmarks I\u2019ve seen it seems to deliver quite fantastic optimizations. And that\u2019s with partly due to optimizing things like relu (to cut on memory movements in this case), right? Relus and stuff seem to matter much more in the Tensor Core days than they did before Volta\/Turing, especially on unlocked GPUs (not 20xx, 30xx series)."},{"x":"Let\u2019s suppose I have the code below and I want to calculate the jacobian of L, which is the prediction made by a neural network in Pytorch, L is of size nx1 where n is the number of samples in a mini batch. In order to avoid a for loop for each entry of L (n entries) to calculate the jacobian for each sample in the mini batch some codes I found just sum the n predictions of the neural network (L) with respect with the inputs and then calculate the gradient of the sum. First I can\u2019t understand why is the gradient of the sum the same of the sum of the gradients for each sample in pytorch architecture. Second I tried both with the sum and with a for loop and the results diverge. Could it be due to numerical approximations or because the sum just doesn\u2019t make sense?\nThe code is below, where both functions belong to a nn.module:\n<code class=\"lang-auto\">def forward(self, x):\n        with torch.set_grad_enabled(True):\n            def function(x,t):\n                 self.n = n = x.shape[1]\/\/2\n\n                 qqd = x.requires_grad_(True)\n                 L = self._lagrangian(qqd).sum()\n                 J = grad(L, qqd, create_graph=True)[0]\n\n        \ndef _lagrangian(self, qqd):\n    x = F.softplus(self.fc1(qqd))\n    x = F.softplus(self.fc2(x))\n    x = F.softplus(self.fc3(x))\n    L = self.fc_last(x)\n    return L\n<\/code>\nThe other way would be to perform a for loop for each sample in the mini batch and remove the sum in the jacobian. However the results are slightly different 10\u207b5 to 10\u207b6 for predictions. However these differences are higher for loss computations, which I suppose is due to error propagation in the MSE loss function.","y":"Hi,\n\nFirst I can\u2019t understand why is the gradient of the sum the same of the sum of the gradients for each sample in pytorch architecture.\n\nThis is happens because of the linearity of the derivatives:\nd\/dqqd(L0 + L1 + L2) = dL0\/dqqd + dL1\/dqqd + dL2\/dqqd\nBut this assumes that L0, L1 and L2 are independent. So if in your forward, the result for each batch actually depends on the result from the others, you cannot separate the equation as L0 will depends on L1 etc.\nAlso errors of the order of 1e-5 are expected when doing these things and are simply due to the non-associativity of floating point numbers: because you compute things in a different order, you will get small errors. This is expected.","z":"Hi,\n\nFirst I can\u2019t understand why is the gradient of the sum the same of the sum of the gradients for each sample in pytorch architecture.\n\nThis is happens because of the linearity of the derivatives:\nd\/dqqd(L0 + L1 + L2) = dL0\/dqqd + dL1\/dqqd + dL2\/dqqd\nBut this assumes that L0, L1 and L2 are independent. So if in your forward, the result for each batch actually depends on the result from the others, you cannot separate the equation as L0 will depends on L1 etc.\nAlso errors of the order of 1e-5 are expected when doing these things and are simply due to the non-associativity of floating point numbers: because you compute things in a different order, you will get small errors. This is expected.\nHi, huge thank you. Also, I\u2019m trying to replicate a code from jax framework (https:\/\/jax.readthedocs.io\/en\/latest\/). However, despite having everything exactly the same I got different results. Basically I have same weight initialization and same hyperparameters. The code both in pytorch and jax makes use of inverse or pseudo inverse. I know there are other discussions about reproducibility specially between Pytorch and TensorFlow. But do you have any ideia if it\u2019s an intrinsic difference in the calculations between pytorch and jax or if it may be due a bug in my code(I already verified everything)?\nIt depends how different the results are.\nIf it is just small values then most likely numerical precision because of float operation ordering and different backends.\nIf it is bigger, you will have to try and reduce your example as much as possible to find where the problem is. Or share it in a new Topic to see if people can help you \nHi. Just one more thing. Even if qdd has shape n_sampesxn_inputs(neural network) does the gradient of the sum of L which has shape n_samples gives the same gradient of a for loop over L0\/qqd_sample0, L1\/qqd_sample1 and so on? I assume in this case there is an independency but in the MSE loss function the differences can be of order of magnitude of 10^0. I suppose the results diverge even more when n_samples(mini batch) increases. Is this due to error propagation? Then of course since there are so many little differences the convergence will be different. I\u2019m asking this because despite having everything the same I don\u2019t get the same results as in jax. At the end of the first epochs the results are pretty similar, however after some epochs the results diverge, even though in pytorch I get convergence faster. So there might be some error propagation due to the non associativity of float numbers. But shouldn\u2019t it be presented in jax too? I mean those very small values below 10\u207b5\/10-6 are noise and this noise is different according to different operations or in this case between frameworks? Also, in the JAX code they don\u2019t perform the sum of L. However using a for loop to calculate the jacobian for each output of the neural network for each sample in the mini batch would be ridiculous computationally expensive in pytorch, taht\u2019s why I\u2019m using the sum since all outputs of the neural netowrk (L1,L2,L3\u2026) are independent of each other since they depend on different samples.\n\n\n\n saul_santos:\n\nthe sum of L which has shape n_samples gives the same gradient of a for loop over L0\/qqd_sample0, L1\/qqd_sample1 and so on?\n\n\nYou need the independence of the computation for this to work. If you have it it will be fine.\n\nMSE loss function the differences can be of order of magnitude of 10^0. I suppose the results diverge even more when n_samples(mini batch) increases. Is this due to error propagation?\n\nNote that the 1e-6 error happen in one op. But then when you keep doing computation, this error is usually amplified by every operation that you perform. So for a deep network or after few iterations, this error can grow quite a lot.\n\nAt the end of the first epochs the results are pretty similar, however after some epochs the results diverge\n\nThis is totally expected when you use neural nets: they will end up converging to different places in the space of parameters.\nBut most neural network loss functions are well behaved enough that all of these optima that are found give similar loss value (even though you can have very different parameter values).\nYou can see this as having the same impact as sampling a different set of initial weights.\n\nBut shouldn\u2019t it be presented in jax too?\n\nThis is present in jax as well. So you will have the same behavior of very small differences in one op that grow over time."},{"x":"hi there,\nI have a question about the mechanism of the Pytorch autograd.\nFor the usual loss functions, we have two inputs which are yhat(real output) and y(expected output), as shown in the following two codes.\n<code class=\"lang-auto\">def mse_loss (yhat, y):\n    loss = torch.mean((y - yhat) ** 2)\n    return loss\n<\/code>\n<code class=\"lang-auto\">def cross_entropy_loss(yhat,y):\n    L = len(yhat)\n    loss = - (1 \/ L) * (torch.mm(y.T, torch.log(yhat)) \n                         + torch.mm((1 - y).T, torch.log(1 - yhat)))\n    return loss\n<\/code>\nHowever, I want to use a neural network to minimize my own loss function, which doesn\u2019t have y (expected output), will the loss.backward() function still return me cerrect result?\n<code class=\"lang-auto\">def my_loss(yhat):\n     L = len(yhat)\n     loss= (1\/L) * torch.mm(yhat.T, torch.log(yhat))\n     return loss\n<\/code>","y":"Yes. To the autograd machinery, any scalar function will do as a loss and it will just go over the computational graph regardless of how you arrived there (within the restrictions of needing differentiable things and only following nodes that have requires_grad, which e.g. the true labels usually do not have).\nBest regards\nThomas","z":"Yes. To the autograd machinery, any scalar function will do as a loss and it will just go over the computational graph regardless of how you arrived there (within the restrictions of needing differentiable things and only following nodes that have requires_grad, which e.g. the true labels usually do not have).\nBest regards\nThomas\nHi Tom,\nThanks for your answer, I already try this function in my model and it works."},{"x":"Hi All,\nI am relatively new to PyTorch, and I am trying to find the second derivative. However, it is always zero for some reason. Below is the code:\n<code class=\"lang-auto\">import torch\nfrom torch.autograd import grad\ndev = torch.device('cpu')\nif torch.cuda.is_available():\n    dev = torch.device('cuda')\n    torch.set_default_tensor_type('torch.cuda.FloatTensor')\ndef Sec_Der(y,x):\n        \n    duxdxyz = grad(y[:, 0].unsqueeze(1), x, torch.ones(x.size()[0], 1, device=dev), create_graph=True, retain_graph=True)[0]\n    duydxyz = grad(y[:, 1].unsqueeze(1), x, torch.ones(x.size()[0], 1, device=dev), create_graph=True, retain_graph=True)[0]\n    duzdxyz = grad(y[:, 2].unsqueeze(1), x, torch.ones(x.size()[0], 1, device=dev), create_graph=True, retain_graph=True)[0]\n        \n    sec_der = grad(duzdxyz[:,0].unsqueeze(1), x, torch.ones(x.size()[0], 1), create_graph=True, retain_graph=True)[0]\n    print(sec_der)\n            \n    return sec_der\nx = 2*torch.rand((5,3))\nx.requires_grad_(True)\nx.retain_grad()\nprint(x)\ny = x**3 + 2*x\nprint(y)\nsec_der = Sec_Der(y,x)\n<\/code>","y":"Now you tricked yourself with too much linearity.\nWhile relu is nonlinear globally (else we would famously not have the universal representation property) it is linear in a neighborhood of almost every input (if you pardon the mathematics lingo), rendering your network linear in a neighborhood of your inputs.\nTaking a more global view, the derivative is piecewise constant, and the second derivative 0 where defined.","z":"I think you tricked yourself with the indexing.\nAs y[:, 2] only depends on the last column in x, only the last column of the gradient is nonzero (do print(duzdxyz)). Then you take the sec_der as the grad of an all-zero column of duzdxyz.\nBest regards\nThomas\nMany thanks, , for the reply.\nI see. Yes, it seems that I tricked myself with the indexing. But even if I defined y differently, I still get zeros. Let\u2019s say that\n<code class=\"lang-auto\">m = torch.nn.Linear(3, 3)\ny = m(torch.relu(m(torch.relu(m(x)))))\n<\/code>\nNow, print(duzdxyz) are not zeros\nNow you tricked yourself with too much linearity.\nWhile relu is nonlinear globally (else we would famously not have the universal representation property) it is linear in a neighborhood of almost every input (if you pardon the mathematics lingo), rendering your network linear in a neighborhood of your inputs.\nTaking a more global view, the derivative is piecewise constant, and the second derivative 0 where defined.\nThanks for the explanation . It makes sense. In this case, what type of layers can I use to capture the second derivative of data? Maybe, I keep the linear function but use other activations? Any recommendations?\nAt the risk of being a math nerd: Don\u2019t say \u201cthe (second) derivative of the data\u201d. It is the \"derivative of  with respect to \" (well actually you typically compute hession-vector products when using second derivatives with backprop, but hey).\nMany (most?) of the loss functions are sufficiently nonlinear, but you would, of course also get second derivatives of the function specified by your neural network if you used an activation function that is curved (e.g. tanh where it is not saturated).\n Thanks "},{"x":"Hi there,\nI have a model with trained weights W and I want to train that model on different task without forgetting previous knowledge. I have another matrix V of the same size as W, and\n<code class=\"lang-auto\"># A.requires_grad=True\n# B.requires_grad=False\nV = torch.matmul(A, B) # then V.requires_grad=True\n<\/code>\nNew Weights W1 = W + V so W1 has automatically requires_grad=True and I don\u2019t want to update W1, W, V, and B I only want to update A, How I can do that?\nThanks!","y":"Sorry, Solve it. It\u2019s already there. I got confused previously.\nThanks!","z":"Sorry, Solve it. It\u2019s already there. I got confused previously.\nThanks!"},{"x":"Hi,\nwhen using eval mode, I need to use gradients in part of my model. In forward method, I set the requires_grad flag of parameters to True and expect the following module will calculate and store gradients, but after the maxpooling layer, requires_grad flag becomes false and I don\u2019t know why\u2026\n<code class=\"lang-auto\">        xl1 = self.conv_block1(xf3)\n        xl2 = self.conv_block2(xf4)\n        xl3 = self.conv_block3(xf5)\n        \n        for x in [xl1, xl2, xl3]:\n            x.requires_grad_(True)\n        xl1.register_hook(self.save_gradient)\n        xl2.register_hook(self.save_gradient)\n        xl3.register_hook(self.save_gradient)\n        \n        xl1 = self.max1(xl1) #xl1.requires_grad is False, here self.max1 = nn.MaxPool2d(kernel_size=56, stride=56)\n\n<\/code>\nThanks in advance!","y":"I find out that I made a mistake that I wrap these code in torch.no_grad() ","z":"I find out that I made a mistake that I wrap these code in torch.no_grad() "},{"x":"Let us assume that we want to multiply 2 tensors:\n<code class=\"lang-auto\">t_a = torch.randn(2, 3, 5, 5) #  2 batch, 3 channels, 5 rows, 5 cols\nt_b = torch.tensor([0.26, 0.28, 0.45]) # 1D tensor\n<\/code>\nIn the problem I am reading about they use unsqueeze and unsqueeze in-place before multiplying the tensors as such:\n<code class=\"lang-auto\">t_c = t_b.unsqueeze(-1).unsqueeze_(-1)  # first expand dims\n\nresult = t_a * t_c # then multiply\n<\/code>\nI assume we are expanding the tensor t_b to be the same in at least one dimension as t_a, for the purpose of broadcasting. So t_c should end up being what shape in the end?\nWhat does t_b.unsqueeze(-1) do? Where does it add a dimension?\nWhy are we using unsqueeze_(-1) in-place here? Is this because the first unsqueeze creates a new object in memory and the second one is just modifying the second object instead of creating a third?\nThanks for the help!","y":"unsqueeze(-1) adds a new dimension after the last index, so the shape of t_b transforms into:\n<code class=\"lang-auto\">t_b.unsqueeze(-1).unsqueeze(-1)\n (3)  (3,1)        (3,1,1)      <- shapes\n<\/code>\nFor comparison, if we wanted to add the new dimensions in \u201cfront\u201d we would do:\n<code class=\"lang-auto\">t_b.unsqueeze(0).unsqueeze(0)\n (3)  (1,3)        (1,1,3)      <- shapes\n<\/code>\nChaining two unsqeeze calls (or any Python method for that matter) together is equivalent to\n<code class=\"lang-auto\">t_b = t_b.unsqueeze(-1)\nt_b = t_b.unsqueeze(-1)\n<\/code>","z":"unsqueeze(-1) adds a new dimension after the last index, so the shape of t_b transforms into:\n<code class=\"lang-auto\">t_b.unsqueeze(-1).unsqueeze(-1)\n (3)  (3,1)        (3,1,1)      <- shapes\n<\/code>\nFor comparison, if we wanted to add the new dimensions in \u201cfront\u201d we would do:\n<code class=\"lang-auto\">t_b.unsqueeze(0).unsqueeze(0)\n (3)  (1,3)        (1,1,3)      <- shapes\n<\/code>\nChaining two unsqeeze calls (or any Python method for that matter) together is equivalent to\n<code class=\"lang-auto\">t_b = t_b.unsqueeze(-1)\nt_b = t_b.unsqueeze(-1)\n<\/code>\nThanks.\nSo in this scenario we would be broadcasting 0.26 to the first channel, 0.28 to the second, and 0.45 to the third channel?\nWhy don\u2019t we need to add a 4th dimension, in-front, for the batch?\nLastly, would t_b = t_b[None] do the same as t_b = t_b.unsqueeze(-1)?\nI suggest taking a look at the documentation about broadcasting semantics with link \"https:\/\/pytorch.org\/docs\/stable\/notes\/broadcasting.html\".\nAccording to the docs:\n\nTwo tensors are \u201cbroadcastable\u201d if the following rules hold:\n\nEach tensor has at least one dimension.\nWhen iterating over the dimension sizes, starting at the trailing dimension, the dimension sizes must either be equal, one of them is 1, or one of them does not exist.\n\n\nIn this case we have the shapes (2,3,5,5) and (3,1,1). The trailing dimension is the leftmost apparently.\n<code class=\"lang-auto\">Valid according to point 2, since \"one of them does not exist\"\nv\n(2,3,5,5)\n  (3,1,1)\n<\/code>\nMy intuition is that when you match the channels, it will distribute each number in t_b for each channel. Since t_b does not have a fourth dimension, the process simply gets repeated for each \u201citem\u201d in the fourth dimension.\nI would suggest you play around with tensors that are easier to sanity check (print the things and see what happens), e.g. play around with this code:\n<code class=\"lang-auto\">a = torch.ones(3, 4, 4) \nb = torch.tensor([1, 2, 3]) \nprint(a*b.unsqueeze(-1).unsqueeze(-1))\n\na = torch.ones(2, 3, 4, 4) \nb = torch.tensor([1, 2, 3]) \nprint(a*b.unsqueeze(-1).unsqueeze(-1))\n<\/code>\n\nLastly, would t_b = t_b[None] do the same as t_b = t_b.unsqueeze(-1) ?\n\nNo it would not, it would be equal to t_b.unsqueeze(0). Doing t_b[..., None], would be equivalent to .unsqueeze(-1). Also, t_b[..., None, None] would be equal to t_b.unsqueeze(-1).unsqueeze(-1).\nAwesome answers. Thanks for taking the time to thoughtfully give me that insight."},{"x":"I am building a model that outputs a scaler, and I need to compute the gradient of those outputs with respect to their inputs. I am doing something like the following:\n<code class=\"lang-auto\">\n\ndef get_forces(outputs, coordinates):\n    forces = []\n    for index, (hash, image) in enumerate(coordinates.items()):\n        for i, coordinate in image: \n            forces_ = -torch.autograd.grad(\n                                      outputs[index], \n                                      coordinate,  \n                                      create_graph=True,\n                                      retain_graph=True\n             )[0]\n            forces.append(forces_)\n    return torch.stack(forces)\n\n\nwhile not converged:\n    optimizer.zero_grad()  # clear previous gradients\n    outputs = model(inputs)    # Note that inputs have requires_grad = True\n    outputs = torch.stack(list(outputs.values())).sum(0)\n    \n\n    all_grad = get_gradients(outputs, coordinates[0])\n\n    outputs = {\"scalar\": outputs, \"gradiend\": all_grad}\n\n    criterion = CustomLossFunction(gradient=True)\n    loss = criterion(outputs, targets)    # This loss uses both scalar and gradient\n    \n    loss.backward()  \n    optimizer.step()\n\n    epoch += 1\n\n    \n    if epoch == stop:\n        converged = True\n<\/code>\nThe first epoch passes correctly, but at the second epoch I get:\n<code class=\"lang-auto\">RuntimeError: Trying to backward through the graph a second time, but \nhe saved intermediate results have already been freed. Specify \nretain_graph=True when calling backward the first time.\n<\/code>\nHow can this be solved? How can I compute the gradient of the output of my neural network with respect to its inputs without having this issue? Meanwhile, I am reading on the forum about how other people have solved this.","y":"Hi,\nThis is most likely not linked to your get_gradients() function.\nBut more to the fact that something that you using in the loop (inputs or coordinates) already has a history (you made some operations with it while it was requiring gradients) and so this part of the graph is shared across iterations.\nYou want to make sure that this is not the case and that you re-create the graph at each iteration.\nIn particular here, if you get gradients for coordinates from outputs while only inputs were used. I guess you have a graph that links inputs and coordinates and this one is shared across iterations.\nYou should move that computation inside the while loop.","z":"Hi,\nThis is most likely not linked to your get_gradients() function.\nBut more to the fact that something that you using in the loop (inputs or coordinates) already has a history (you made some operations with it while it was requiring gradients) and so this part of the graph is shared across iterations.\nYou want to make sure that this is not the case and that you re-create the graph at each iteration.\nIn particular here, if you get gradients for coordinates from outputs while only inputs were used. I guess you have a graph that links inputs and coordinates and this one is shared across iterations.\nYou should move that computation inside the while loop.\nThis makes total sense and it is the case. This is what I am doing:\n<code class=\"lang-auto\">Coordinates === mapping function ===> Features  ====> model ====> output\n<\/code>\nThe training loop is effectively from Features instead of Coordinates (these have requires_grad=True). The reason for that is because features are not being optimized and to avoid recomputing them on each epoch I just calculated them once. After your explanation, my mapping function has to be moved into model or the training loop so that inputs are really Coordinates and not Features. Is there any way I can make this work without mapping coordinates into features at every epoch?\nHi,\nThere is no magic trick to force the autograd to only keep part of the graph I\u2019m afraid.\nIf you want to avoid recomputing this, you can call .backward(retain_graph=True) which might increase you peak memory a little bit. Also if you do that, I would recommend you either:\n\nMove the content of the while loop into a different function. To make sure that at the end of the loop, all the temporary variable are deleted before starting the next operation\nAdd a bunch of del foo at the end of your loop for outputs, all_grad, and loss. (again to make sure they don\u2019t stay alive until the next iteration).\n\nThanks . Your replies were very helpful. After moving the mapping function to the training loop I can see effectively that .backward() does not complain anymore. Now the next problem I am trying to solve is gradients are basically zero where they should not be  I hope to figure it out.\nYou can do .register_hook(fn) on any Tensor and fn will be called with the grad computed for this Tensor. That can help you print out gradients to track it down \nGood luck!"},{"x":"Dear all, i am having issues with training of my feed forward NNs\nI have tried my best to resolve the issues but failed so far. so I come to you all for some help.\nLet me describe the whole code as concisrely as possible.\nBasically, this is an actor critic structure to find optimal control using HDP structure.\nThere are two networks CriticNN and ActorNN , both are feed forward NNs.  Their structures and optimizers are defined as:\n<code class=\"lang-auto\">class Network(nn.Module):\n    def __init__(self,D_in,D_out):\n        super().__init__()\n        \n        # Inputs to hidden layer linear transformation\n        self.lin1 = nn.Linear(D_in, 8)\n        \n        self.lin2=nn.Linear(8,4)\n        # Output layer,\n        self.output = nn.Linear(4, D_out)\n        \n        # Define sigmoid activation and softmax output \n        #self.tanh = F.tanh()\n        \n        \n    def forward(self, x):# this is where the data flows in the network, respecting \n                         #sequence of layers in forward method is very important.\n        # Pass the input tensor through each of our operations\n        \n        x = self.lin1(x)\n        x = F.relu(x)\n        \n        x = self.lin2(x)\n        x = F.relu(x)\n                \n        x = self.output(x)\n        y = F.tanh(x)\n        \n        return y\n\n\ncriterion = torch.nn.MSELoss(size_average=False)\n\n\noptimizer_c = torch.optim.SGD(criticNN.parameters(), \n                            lr=0.001) \noptimizer_a = torch.optim.SGD(actorNN.parameters(), \n                            lr=0.001)\n                            \n<\/code>\nactor,NN , criticNN and model NN are instantiated and\nsome essential functions are declared next.\n<code class=\"lang-auto\">criticNN = Network(2, 1)\nactorNN=Network(2,1)\nmodel=Network(3,2)\ndef to_np(x):\n    return x.data.cpu().numpy()\n\ndef instant_reward(state,control,Q,R):\n    \n    cc=np.matmul(Q, state)\n    rew1=np.matmul(state.T,cc)\n    rew2=control*control*R\n    res=rew1+rew2\n    return res\n\nq11,q12,q21,q22=1,0,0,1 \nQ=np.array([[q11,q12],[q21,q22]])\nR=0.001\n<\/code>\nRest of the problem is described in the comments along with the code to\nfacilitate the understanding. please see the comments with code\n<code class=\"lang-auto\"># a pretrained NN called \"model\" is used here in eval mode \n#this will be used in reward generation later\nmodel.eval()\n######## essential parameters for \n\n\ntrain_losses_c = []\ntrain_losses_a = []\nvalid_losses = []\nvalid_score = []\nepochs_c=[]\nepochs_a=[]\n\n#### epochs for actor and critic traninig ##############\nepoch_iter_c =range(1, 10)\nepoch_iter_a =range(1, 10)\n\nq11,q12,q21,q22=10,0,0,10 \nQ=np.array([[q11,q12],[q21,q22]])\nR=1\n\n####input single data for training actorNN and criticNN\nx0=np.array([[0.1,0.1]])\n\n##### training of critic and actor \n###### iteration range is defined, each iteration...\n#critic is trained for several epochs, followed by actorNN\n\nfor iteration in range(1,4):\n    print (\"iteration=\",iteration)\n    \n    #first iteration has \"control\" initialized and value (cost function) initialized\n    if iteration==1:\n    \n        control=np.array([[0.001]])\n        value=0\n        print(\"initial action value chosen\",control)\n        print(\"initial value is\",value)\n    # for other iteraitons, the control value is outputted by...\n    #previously trained actorNN (i.e. trained in previous iteratoin in several epochs)\n    #similarly, \"value\" is outputted by previously trained criticNN (ie.trained in previous iteration)\n    else:\n        #torch no grad is used as actorNN and  criticNN used here are trained in previous iteration \n        with torch.no_grad():\n\n            #actorNN.eval()\n            #calculate action from already trained actiorNN\n            action=actorNN(torch.from_numpy(x0).float())\n            control=to_np(action)\n            print(\"control value outputted by trained actorNN is\",control)\n\n            #criticNN.eval()\n            #calculate cost function (value) from already trained critic NN\n            concat_input_system_model_c=np.concatenate((x0,control),axis=1)\n            #prepare tensor to feed into trained system NN \n            concat_input_system_model_c_torch=torch.from_numpy(concat_input_system_model_c)\n            #feed in to the system NN\n            x1=model(concat_input_system_model_c_torch.float())\n\n            value=criticNN(x1)\n            print(\"value outputted by trained criticNN is\",value)\n        \n        \n    #in each iteration critic and actorNN are trained for several epochs one after another. \n    idx_c=0 \n    idx_a=0\n    tloss_avg_c=0\n    tloss_avg_a=0\n    \n    # training of criticNN for several epochs \n    #the criticNN is put in train mode\n    criticNN.train()\n    \n    for epoch in epoch_iter_c:   \n        epochs_c.append(epoch)\n        idx_c+=1\n\n        \n        #criticNN estimation\n        estimation_criticNN=criticNN(torch.from_numpy(x0).float())\n        \n        #reward is generated, check that it is torch\n        #this reward generation requires \"control\" outputted above by previously trained actorNN\n        reward=instant_reward(state=x0.reshape(-1,1),control=control,Q=Q,R=R)\n        reward_tensor=torch.from_numpy(reward).float()\n        \n        #calculate the target\n        #target calculation requires the \"value\" generated above by trained criticNN....\n        #which is done outside epoch loop within if-else statement after iteration assignement. \n        # that is why, no grad is activated here as backprop should be done only ....\n        #in criticNN that outputs \"estimation_criticNN\" above.\n        with torch.no_grad():\n            target = reward_tensor + value # predict y based on x\n\n        \n        \n        loss_c = criterion(estimation_criticNN,target) # compute loss\n\n        optimizer_c.zero_grad() # clear gradients\n        loss_c.backward() # compute gradients\n        optimizer_c.step() # apply gradients\n\n        tloss_avg_c += loss_c.item()\n\n        tloss_avg_c \/= idx_c\n        train_losses_c.append(tloss_avg_c)\n\n        print(\" Epoch : %s , Critic train loss: %s \" %(epoch,tloss_avg_c))\n    \n    \n    #critic training is done,\n    #keep the criticNN tranied weights frozen and train actorNN\n    \n    ############train actor#################################################3\n    \n    criticNN.eval()\n    \n    actorNN.train()\n\n    for epoch in epoch_iter_a:   \n\n        epochs_a.append(epoch)\n        idx_a += 1\n        \n        #estimation of control is outputted by actorNN which must be trained now\n        estimation_control=actorNN(torch.from_numpy(x0).float())\n        #convert to numpy\n        estimation_control_numpy = to_np(estimation_control)\n        print(\"Control value is\",estimation_control_numpy)\n        \n        #reward is generated using estimation control outputted by actorNN just above\n        reward_a        = instant_reward(state=x0.reshape(-1,1),control=estimation_control_numpy,Q=Q,R=R)\n        reward_a_tensor = torch.from_numpy(reward_a).float()\n        #print(\"reward_a_tensor\",reward_a_tensor)\n        \n        #concatenation of x0 and estimation_control to feed into \"model\"\n        concat_input_system_model_a=np.concatenate((x0,estimation_control_numpy),axis=1)      \n        #prepare tensor to feed into trained system NN \n        concat_input_system_model_a_torch = torch.from_numpy(concat_input_system_model_a)\n        #feed in to the system NN to obtain x1 from x0 which is \n        x1_a=model(concat_input_system_model_a_torch.float())\n        #print(\"x1_a\",x1_a)\n        \n        \n        #calculate the target for actor training\n        #the target is calculated usign reward generated just above ...\n        #which is(sensitive to estimation by actorNN) and prediction by criticNN trained above...\n        #the criticNN here is in eval mode\n        \n        target_a = reward_a_tensor + criticNN(x1_a) # predict y based on x\n        print(\"target for action network is\",target_a)\n        #this target value is to be minimised. thus, grad is set true.\n        target_a = Variable(target_a, requires_grad = True)\n        \n        \n        #crete zeros like\n        zeros_like=torch.zeros_like(target_a)\n        #print(\"zeros like\",zeros_like)\n        \n        #loss is generated using target and zero as target value is to be minimised for actor ...\n        # weight update\n        loss_a = criterion(target_a,zeros_like) # compute loss\n        #print(\"loss a\",loss_a)\n        \n        #loss_a = Variable(loss_a, requires_grad = True)\n        \n        optimizer_a.zero_grad() # clear gradients\n        loss_a.backward() # compute gradients\n        optimizer_a.step() # apply gradients\n\n        tloss_avg_a += loss_a.item()\n        #print(\"loss item\",loss_a.item())\n        \n        tloss_avg_a \/= idx_a\n        train_losses_a.append(tloss_avg_a)\n\n        print(\" Epoch : %d , Actor Train loss: %s \" %(epoch,tloss_avg_a))\n        print('')\n\n<\/code>\non running it, we can see that loss of critic goes down, indicated by Critic train loss,  and the value outputted by critic also changes , idicated by \u201cvalue outputted by trained criticNN\u201d after iteration 2 included.\nLoss of actorNN also goes down in each iteration , as indicated by \u201cactor train loss\u201d\nbut the value outputted by actorNN  does not change , indicated by \u201ccontrol value is\u201d\nand hence actorNN does not learn.\nthis is my main problem. I am thankful for all the help.\nhere is the what is obtained after running it\nNote: the criticNN and actor NN is trained here suing one data sample x0 and different targets . I have tried training in the similar way using batches of x0, but same results.\n<code class=\"lang-auto\">iiteration= 1\ninitial action value chosen [[0.001]]\ninitial value is 0\n Epoch : 1 , Critic train loss: 0.004236207809299231 \n Epoch : 2 , Critic train loss: 0.004186923382803798 \n Epoch : 3 , Critic train loss: 0.002742775638277332 \n Epoch : 4 , Critic train loss: 0.0016725545089381435 \n Epoch : 5 , Critic train loss: 0.0011056517212030786 \n Epoch : 6 , Critic train loss: 0.0008119643504162216 \n Epoch : 7 , Critic train loss: 0.0006415206117607239 \n Epoch : 8 , Critic train loss: 0.0005293509762042151 \n Epoch : 9 , Critic train loss: 0.0004488066545300284 \nControl value is [[0.32846534]]\ntarget for action network is tensor([[0.4479]], grad_fn=<AddBackward0>)\n Epoch : 1 , Actor Train loss: 0.20059673488140106 \n\nControl value is [[0.32846534]]\ntarget for action network is tensor([[0.4479]], grad_fn=<AddBackward0>)\n Epoch : 2 , Actor Train loss: 0.20059673488140106 \n\nControl value is [[0.32846534]]\ntarget for action network is tensor([[0.4479]], grad_fn=<AddBackward0>)\n Epoch : 3 , Actor Train loss: 0.1337311565876007 \n\nControl value is [[0.32846534]]\ntarget for action network is tensor([[0.4479]], grad_fn=<AddBackward0>)\n Epoch : 4 , Actor Train loss: 0.08358197286725044 \n\nControl value is [[0.32846534]]\ntarget for action network is tensor([[0.4479]], grad_fn=<AddBackward0>)\n Epoch : 5 , Actor Train loss: 0.0568357415497303 \n\nControl value is [[0.32846534]]\ntarget for action network is tensor([[0.4479]], grad_fn=<AddBackward0>)\n Epoch : 6 , Actor Train loss: 0.04290541273852189 \n\nControl value is [[0.32846534]]\ntarget for action network is tensor([[0.4479]], grad_fn=<AddBackward0>)\n Epoch : 7 , Actor Train loss: 0.03478602108856042 \n\nControl value is [[0.32846534]]\ntarget for action network is tensor([[0.4479]], grad_fn=<AddBackward0>)\n Epoch : 8 , Actor Train loss: 0.029422844496245187 \n\nControl value is [[0.32846534]]\ntarget for action network is tensor([[0.4479]], grad_fn=<AddBackward0>)\n Epoch : 9 , Actor Train loss: 0.025557731041960696 \n\niteration= 2\ncontrol value outputted by trained actorNN is [[0.32846534]]\nvalue outputted by trained criticNN is tensor([[0.1400]])\n Epoch : 1 , Critic train loss: 0.09390095621347427 \n Epoch : 2 , Critic train loss: 0.09281281009316444 \n Epoch : 3 , Critic train loss: 0.06080528721213341 \n Epoch : 4 , Critic train loss: 0.037084988318383694 \n Epoch : 5 , Critic train loss: 0.024520622007548808 \n Epoch : 6 , Critic train loss: 0.018012114086498818 \n Epoch : 7 , Critic train loss: 0.01423531560936854 \n Epoch : 8 , Critic train loss: 0.011750161385584977 \n Epoch : 9 , Critic train loss: 0.009965951214901729 \nControl value is [[0.32846534]]\ntarget for action network is tensor([[0.4790]], grad_fn=<AddBackward0>)\n Epoch : 1 , Actor Train loss: 0.22944745421409607 \n\nControl value is [[0.32846534]]\ntarget for action network is tensor([[0.4790]], grad_fn=<AddBackward0>)\n Epoch : 2 , Actor Train loss: 0.22944745421409607 \n\nControl value is [[0.32846534]]\ntarget for action network is tensor([[0.4790]], grad_fn=<AddBackward0>)\n Epoch : 3 , Actor Train loss: 0.15296496947606406 \n\nControl value is [[0.32846534]]\ntarget for action network is tensor([[0.4790]], grad_fn=<AddBackward0>)\n Epoch : 4 , Actor Train loss: 0.09560310592254004 \n\nControl value is [[0.32846534]]\ntarget for action network is tensor([[0.4790]], grad_fn=<AddBackward0>)\n Epoch : 5 , Actor Train loss: 0.06501011202732723 \n\nControl value is [[0.32846534]]\ntarget for action network is tensor([[0.4790]], grad_fn=<AddBackward0>)\n Epoch : 6 , Actor Train loss: 0.04907626104023721 \n\nControl value is [[0.32846534]]\ntarget for action network is tensor([[0.4790]], grad_fn=<AddBackward0>)\n Epoch : 7 , Actor Train loss: 0.03978910217919047 \n\nControl value is [[0.32846534]]\ntarget for action network is tensor([[0.4790]], grad_fn=<AddBackward0>)\n Epoch : 8 , Actor Train loss: 0.033654569549160816 \n\nControl value is [[0.32846534]]\ntarget for action network is tensor([[0.4790]], grad_fn=<AddBackward0>)\n Epoch : 9 , Actor Train loss: 0.029233558195917428 \n\niteration= 3\ncontrol value outputted by trained actorNN is [[0.32846534]]\nvalue outputted by trained criticNN is tensor([[0.1711]])\n Epoch : 1 , Critic train loss: 0.09431567788124084 \n Epoch : 2 , Critic train loss: 0.09324267506599426 \n Epoch : 3 , Critic train loss: 0.06110667188962301 \n Epoch : 4 , Critic train loss: 0.03728598294158777 \n Epoch : 5 , Critic train loss: 0.02466679352025191 \n Epoch : 6 , Critic train loss: 0.018129166194962132 \n Epoch : 7 , Critic train loss: 0.014335107055330087 \n Epoch : 8 , Critic train loss: 0.011838365811485028 \n Epoch : 9 , Critic train loss: 0.010045669610674511 \nControl value is [[0.32846534]]\ntarget for action network is tensor([[0.5096]], grad_fn=<AddBackward0>)\n Epoch : 1 , Actor Train loss: 0.2596904933452606 \n\nControl value is [[0.32846534]]\ntarget for action network is tensor([[0.5096]], grad_fn=<AddBackward0>)\n Epoch : 2 , Actor Train loss: 0.2596904933452606 \n\nControl value is [[0.32846534]]\ntarget for action network is tensor([[0.5096]], grad_fn=<AddBackward0>)\n Epoch : 3 , Actor Train loss: 0.17312699556350708 \n\nControl value is [[0.32846534]]\ntarget for action network is tensor([[0.5096]], grad_fn=<AddBackward0>)\n Epoch : 4 , Actor Train loss: 0.10820437222719193 \n\nControl value is [[0.32846534]]\ntarget for action network is tensor([[0.5096]], grad_fn=<AddBackward0>)\n Epoch : 5 , Actor Train loss: 0.0735789731144905 \n\nControl value is [[0.32846534]]\ntarget for action network is tensor([[0.5096]], grad_fn=<AddBackward0>)\n Epoch : 6 , Actor Train loss: 0.05554491107662519 \n\nControl value is [[0.32846534]]\ntarget for action network is tensor([[0.5096]], grad_fn=<AddBackward0>)\n Epoch : 7 , Actor Train loss: 0.04503362920312654 \n\nControl value is [[0.32846534]]\ntarget for action network is tensor([[0.5096]], grad_fn=<AddBackward0>)\n Epoch : 8 , Actor Train loss: 0.038090515318548394 \n\nControl value is [[0.32846534]]\ntarget for action network is tensor([[0.5096]], grad_fn=<AddBackward0>)\n Epoch : 9 , Actor Train loss: 0.03308677874042323 \n<\/code>","y":"You use .data here:\n<code class=\"lang-auto\">def to_np(x):\n    return x.data.cpu().numpy()\n<\/code>\nYou can replace it with\n<code class=\"lang-auto\">def to_np(x):\n    return x.detach().cpu().numpy()\n<\/code>\nAlso note that in both case, if you go to numpy, the autograd won\u2019t be able to compute gradients. So no intermediary results should be done using numpy arrays, only Tensor should be used.","z":"Hi,\nThis line looks suspicious:         target_a = Variable(target_a, requires_grad = True)\nNote that Variable are not a thing anymore so you can remove it.\nAlso it seems like both inputs to criterion(target_a,zeros_like) are actually leaf Tensors with no history? I think you break the graph earlier. In particular, reward_a is computed with estimation_control_numpy which contains a .data breaking the graph (note that you should not use .data anymore. You can replace it with .detach() here).\n\n\n\n albanD:\n\nThis line looks suspicious:         target_a = Variable(target_a, requires_grad = True)\nNote that Variable are not a thing anymore so you can remove it.\n\n\nHello thank you very much for your time.\n\" This line looks suspicious:         target_a = Variable(target_a, requires_grad = True)\nNote that Variable are not a thing anymore so you can remove it.\"\nI deleted it , but leads to same results.\n\" I think you break the graph earlier. In particular, reward_a is computed with estimation_control_numpy which contains a .data breaking the graph (note that you should not use .data anymore. You can replace it with .detach() here).\"\nI think you indicate something very useful.\nwhere can i find .data ? i do not see it explicitly. so how to reslve this problem?  sorry , but i cannot imagine using .detach. can you suggest how?\nideally, i want the  \u201cestimation_control_numpy\u201d to lead the target_a formation, and backprop within the graph.\nthanks.\nYou use .data here:\n<code class=\"lang-auto\">def to_np(x):\n    return x.data.cpu().numpy()\n<\/code>\nYou can replace it with\n<code class=\"lang-auto\">def to_np(x):\n    return x.detach().cpu().numpy()\n<\/code>\nAlso note that in both case, if you go to numpy, the autograd won\u2019t be able to compute gradients. So no intermediary results should be done using numpy arrays, only Tensor should be used.\nThank you  AlbanD. that worked.\nI do  all the intermediatery operations in torch and now I can see the network leanring.\nmany thanks indeed."},{"x":"I want to do the following calculation:\n<code class=\"lang-python\">l1 = f(x.detach(), y)\nl1.backward()\nl2 = -1*f(x, y.detach())\nl2.backward()\n<\/code>\nwhere f is some function, and x and y are tensors that require gradient. Notice that x and y may both be the results of previous calculations which utilize shared parameters (for example, maybe x=g(z) and y=g(w) where g is an nn.Module ).\nThe issue is that l1 and l2 are both numerically identical, up to the minus sign, and it seems wasteful to repeat the calculation f(x,y) twice. It would be nicer to be able to calculate it once, and apply backward twice on the result. Is there any way of doing this?\nOne possibility is to manually call autograd.grad and update the w.grad field of each nn.Parameter w . But I\u2019m wondering if there is a more direct and clean way to do this, using the backward function.","y":"Ho sorry I think I misread the title of the topic and though it was an error \nDoes the function f has any parameter into it?\nIf there is nothing else in there, and the only way to get to the parameters is via x and y, I would do:\n<code class=\"lang-python\">x, y = g(input, params)\n# f must have NO parameters\n# Equivalent (by linearity of the gradient) to \n# l1 = f(x.detach(), y)\n# l1.backward()\n# l2 = -1*f(x, y.detach())\n# l2.backward()\ny.register_hook(lambda x: -x)\nloss = f(x, y)\nloss.backward()\n<\/code>\nNote that you should double check that you get the same gradients still as I might be missing something ","z":"Hi,\nI\u2019m afraid the only thing you can do is to give retain_graph=True the first time you call backward.\nCan you please elaborate on how this would help?\nThis flag prevents the graph internal states from being freed. So you will be able to backprop through the graph as many times as you want (as mentioned in the error message you shared).\nThanks. I would appreciate if you can give a code snippet demonstrating your suggestion. I am aware of  retain_graph, but I don\u2019t see how setting it to True will help accomplish what I asked for. It seems to me that there is still a missing ingredient that would let me to do backward twice on the retained graph, each time keeping a different part of the graph detached.\n<code class=\"lang-auto\">l1 = f(x.detach(), y)\nl1.backward(retain_graph=True)\nl2 = -1*f(x, y.detach())\nl2.backward()\n<\/code>\nThis should not throw the error you mentioned \nThank you, but I did not mention any error. I wanted to avoid calculating the function f twice. I think that in your solution, it is calculated twice, right? Unless PyTorch automatically caches the first calculation of f and then in the second invocation the cached graph is reused somehow, including the detached subgraph composed of the ancestors of x?\nHo sorry I think I misread the title of the topic and though it was an error \nDoes the function f has any parameter into it?\nIf there is nothing else in there, and the only way to get to the parameters is via x and y, I would do:\n<code class=\"lang-python\">x, y = g(input, params)\n# f must have NO parameters\n# Equivalent (by linearity of the gradient) to \n# l1 = f(x.detach(), y)\n# l1.backward()\n# l2 = -1*f(x, y.detach())\n# l2.backward()\ny.register_hook(lambda x: -x)\nloss = f(x, y)\nloss.backward()\n<\/code>\nNote that you should double check that you get the same gradients still as I might be missing something \nThis solved my problem nicely. Thanks.\nHere is code demonstrating that the solution works:\n<code class=\"lang-python\">import torch\nlin = torch.nn.Linear(1,1,bias=False)\nlin.weight.data[:] = 1.0\na = torch.tensor([1.0])\nb = torch.tensor([2.0])\nloss_func = lambda x,y: (x-y).abs()\n\n# option 1: this is the inefficient option, presented in the original question\nlin.zero_grad()\nx = lin(a)\ny = lin(b)\nloss1 = loss_func(x.detach(),y)\nloss1.backward(retain_graph=True)\nloss2 = -1*loss_func(x,y.detach()) # second invocation of `loss_func` - not efficient!\nloss2.backward()\nprint(lin.weight.grad)\n\n# option 2: this is the efficient method, suggested by . \nlin.zero_grad()\nx = lin(a)\ny = lin(b)\nx.register_hook(lambda t: -t)\nloss = loss_func(x,y) # only one invocation of `loss_func` - more efficient!\nloss.backward()\nprint(lin.weight.grad) # the output of this is identical to the previous print, which confirms the method\n\n# option 3 - this should not be equivalent to the previous options, used just for comparison\nlin.zero_grad()\nx = lin(a)\ny = lin(b)\nloss = loss_func(x,y)\nloss.backward()\nprint(lin.weight.grad)\n\n<\/code>\nHowever, it would be nice to know if there is a more general alternative, that works also for non-linear cases. In other words, it would be interesting to know how to perform the following calculation, but reducing to a single invocation of f(x,y), instead of two:\n<code class=\"lang-python\">l1 = u(f(x.detach(), y))\nl1.backward()\nl2 = v(f(x, y.detach()))\nl2.backward()\n<\/code>\nHere, u and v are two functions.\nHi,\n\nnon-linear cases\n\nThe gradient is always linear \nThis won\u2019t work if u\/v\/f have paramters though because the flipping of the gradient for the second loss only happens before f. So parameters in these functions would just see the sum of the two losses."},{"x":"Hi all,\nSuppose my my input img is processed by adding noise (noisy_img) before feed into model, when I tried gradients = autograd.grad(outputs=output, inputs=img)  I can\u2019t get the gradient. But if I use gradients = autograd.grad(outputs=output, inputs=noisy_img) , it seems working without error. I set both img and noisy_img requires_grad=True.\nWhat\u2019s the proper way to get the gradients of output w.r.t. img in this case?\nThank you.","y":"Hi,\nYou should remove all the calls to Variable and that should fix it:\n<code class=\"lang-auto\">        img = img.view(img.size(0), -1)\n        img = img.cuda().requires_grad_()\n       \n        noisy_img = add_noise(img)\n<\/code>","z":"Hi,\nYou need to make sure that you set the requires_grad field before using the img to compute the noisy_img.\nIf you do so, can you share the code that generates the noisy image as well as the error you see?\nHi albanD,\nHere is a simple DAE code\n<code class=\"lang-auto\">import torch\nfrom torch import nn\nfrom torch.autograd import Variable\nfrom torch.utils.data import DataLoader\nfrom torchvision import transforms\nfrom torchvision.datasets import MNIST\nimport torch.autograd as autograd\nimport numpy as np\nnp.random.seed(42)\ntorch.manual_seed(42)\n\n# ref: https:\/\/github.com\/ReyhaneAskari\/pytorch_experiments\/blob\/master\/DAE.py\n\nnum_epochs = 20\nbatch_size = 128 \nlearning_rate = 1e-3\nimg_transform = transforms.Compose([\n    transforms.ToTensor(),\n])\n\ndataset = MNIST('..\/data', transform=img_transform, download=False)\ndataloader = DataLoader(dataset, batch_size=batch_size, shuffle=True)\n\ndef to_img(x):\n    x = x.view(x.size(0), 1, 28, 28)\n    return x\n\ndef add_noise(img):\n    mean=0.\n    std=1\n    noise = torch.randn(img.size()) * std + mean\n    noisy_img = img + noise.cuda()\n    return noisy_img\n\nclass autoencoder(nn.Module):\n    def __init__(self):\n        super(autoencoder, self).__init__()\n        self.encoder = nn.Sequential(\n            nn.Linear(28 * 28, 256),\n            nn.ReLU(True),\n            nn.Linear(256, 64),\n            nn.ReLU(True))\n        self.decoder = nn.Sequential(\n            nn.Linear(64, 256),\n            nn.ReLU(True),\n            nn.Linear(256, 28 * 28),\n            nn.Sigmoid())\n\n    def forward(self, x):\n        x = self.encoder(x)\n        x = self.decoder(x)\n        return x\n\n\nmodel = autoencoder().cuda()\ncriterion = nn.BCELoss()\noptimizer = torch.optim.Adam(\n    model.parameters(), lr=learning_rate, weight_decay=1e-5)\n\ndef calc_gradient_penalty(output, img):\n    \n\n    gradients = autograd.grad(outputs=output, inputs=img,\n                              grad_outputs=torch.ones(output.size()).cuda(),\n                              create_graph=True, retain_graph=True, only_inputs=True)[0]\n    \n    gradient_penalty = ((gradients.norm(2, dim=1) - 1) ** 2).mean() \n   \n    return gradient_penalty\n\nfor epoch in range(num_epochs):\n    for data in dataloader:\n        img, _ = data\n        img = img.view(img.size(0), -1)\n        img = Variable(img).cuda()\n        img = autograd.Variable(img, requires_grad=True)\n       \n        noisy_img = add_noise(img)\n        noisy_img = Variable(noisy_img).cuda()\n        \n        # ===================forward=====================\n        output = model(noisy_img)\n        gradient_penalty = calc_gradient_penalty(output, img)\n        print(gradient_penalty)\n        \n        img.requires_grad = False\n        loss = criterion(output, img)\n        loss = loss + gradient_penalty\n\n        # ===================backward====================\n        optimizer.zero_grad()\n        loss.backward()\n        optimizer.step()\n<\/code>\nThe error I got before print is RuntimeError: One of the differentiated Tensors appears to not have been used in the graph. Set allow_unused=True if this is the desired behavior.\nThank you.\nHi,\nYou should remove all the calls to Variable and that should fix it:\n<code class=\"lang-auto\">        img = img.view(img.size(0), -1)\n        img = img.cuda().requires_grad_()\n       \n        noisy_img = add_noise(img)\n<\/code>\nHi,\nThank you. It works. Can you explain why img = autograd.Variable(img, requires_grad=True) not working in this case? It still sets requires_grad=True right?\nThanks for your time.\nThe problem is with Variable(noisy_img) I think that effectively does the same as noisy_img.detach() and so breaks the graph."},{"x":"I require to update grads of an intermediate tensor variable using the  register_hook  method. Since the variable isn\u2019t a leaf-variable, I require to add the  retain_grad()  method to it after which, I can use the  register_hook  method to alter the grads.\n<code class=\"lang-auto\">score.retain_grad()\nh = score.register_hook(lambda grad: grad * torch.FloatTensor(...))\n<\/code>\nThis works perfectly fine during the training ( model.train() ) phase. However, it gives an error during the evaluation phase ( model.eval() ).\nThe error:\n<code class=\"lang-auto\">File \"\/home\/envs\/darthvader\/lib\/python3.6\/site-packages\/torch\/tensor.py\", line 198, in register_hook\n    raise RuntimeError(\"cannot register a hook on a tensor that \"\nRuntimeError: cannot register a hook on a tensor that doesn't require gradient\n<\/code>\nHow could the model automatically disable the  register_hook  method when it in the eval()  phase?","y":"Hi,\nI think the simplest thing to do here is to guard the register_hook() call with if score.requires_grad:.\nAlso side note: you don\u2019t need to call retain_grad() for the hook to work. Only if you want to be able to read the value in the .grad field on the Tensor.","z":"Hi,\nI think the simplest thing to do here is to guard the register_hook() call with if score.requires_grad:.\nAlso side note: you don\u2019t need to call retain_grad() for the hook to work. Only if you want to be able to read the value in the .grad field on the Tensor.\nHi ,\nThanks for the answer ^^. You are correct, the register_hook works without retain_grad(). The code works perfectly fine during the eval() phase now.\nI was confused about using if score.requires_grad: on the score variable as it is not a leaf-node variable, but an intermediate-node variable.\n\n\n\n someAdjectiveNoun:\n\nI was confused about using if score.requires_grad: on the score variable as it is not a leaf-node variable, but an intermediate-node variable.\n\n\nThe intermediate Tensor will have have their requires_grad=True in python. So you can use that as a hint if they are used in the computation of something that needs gradient computed.\n\n\n\n albanD:\n\nThe intermediate Tensor will have have their requires_grad=True in python. So you can use that as a hint if they are used in the computation of something that needs gradient computed.\n\n\nYes, exactly. Somehow, it completely slipped my mind even when I checked the grad_fn attribute of the score variable in question."},{"x":"Assume an image classification task using a neural net where loss function is defined as L .Consider an image x ,while performing gradient descent on the loss, the gradient of L with respect to x is dL\/dx . Now about my doubt. After this I want to calculate the gradient of norm of dL\/dx wrt to weights of neural net. how do I do that?","y":"Hi,\nYou can do the following:\n<code class=\"lang-python\">x = torch.rand()\noutput = net(x)\nL = crit(output, label)\n\ndLdx = autograd.grad(L, x, create_graph=True)[0]\n\ndLdx.backward()\n# Now you have the grads as usual in the .grad fields\n<\/code>","z":"Hi,\nYou can do the following:\n<code class=\"lang-python\">x = torch.rand()\noutput = net(x)\nL = crit(output, label)\n\ndLdx = autograd.grad(L, x, create_graph=True)[0]\n\ndLdx.backward()\n# Now you have the grads as usual in the .grad fields\n<\/code>\nThanks,this works perfectly"},{"x":"Can\u2019t find the problem place.I cant find inplace problem operations. RuntimeError: one of the variables needed for gradient computation has been modified by an inplace operation: [torch.cuda.FloatTensor [1, 5]], which is output 0 of UnsqueezeBackward0, is at version 321; expected version 316 instead\nhere is the forward code:\ndef forward(self, x):\n    if len(x.shape) == 3:\n\n        x1 = x.view(1, self.channels, self.bars, self.input_dim) \n\n        x2 = self.norm(x1)\n\n    else:\n\n        x2 = self.norm(x)\n\n    save = torch.zeros(x2.size()[0], self.act_size).to(device)\n\n    out = torch.zeros(self.channels).to(device)\n\n    for b in range(x2.size()[0]):\n\n        y0 = x2[b][0].view(1, self.bars, self.input_dim)\n\n        y1 = x2[b][1].view(1, self.bars, self.input_dim)\n\n        y2 = x2[b][2].view(1, self.bars, self.input_dim)\n\n        y3 = x2[b][3].view(1, self.bars, self.input_dim)\n\n        y4 = x2[b][4].view(1, self.bars, self.input_dim)\n\n        lstm_out_0, hid_0 = self.lstm_0(y0)\n\n        lstm_out_1, hid_1 = self.lstm_1(y1)\n\n        lstm_out_2, hid_2 = self.lstm_2(y2)\n\n        lstm_out_3, hid_3 = self.lstm_3(y3)\n\n        lstm_out_4, hid_4 = self.lstm_4(y4)\n\n        out[0] = hid_0[0][-1]\n\n        out[1] = hid_1[0][-1]\n\n        out[2] = hid_2[0][-1]\n\n        out[3] = hid_3[0][-1]\n\n        out[4] = hid_4[0][-1]\n\n        save[b] = self.dense(out)\n\n    return save","y":"\n\n\n Stihl:\n\nout[4] = hid_4[0][-1]\n\n\nI guess each of these calls are actually modifying out inplace. But it\u2019s value is needed by the dense(out) call?\nCan you try not to change out inplace and just torch.cat() your hidden layers together?","z":"\/pytorch\/torch\/csrc\/autograd\/python_anomaly_mode.cpp:57: UserWarning: Traceback of forward call that caused the error:\nFile \u201c\/usr\/lib\/python3.6\/runpy.py\u201d, line 193, in _run_module_as_main\n\u201cmain\u201d, mod_spec)\nFile \u201c\/usr\/lib\/python3.6\/runpy.py\u201d, line 85, in _run_code\nexec(code, run_globals)\nFile \u201c\/usr\/local\/lib\/python3.6\/dist-packages\/ipykernel_launcher.py\u201d, line 16, in \napp.launch_new_instance()\nFile \u201c\/usr\/local\/lib\/python3.6\/dist-packages\/traitlets\/config\/application.py\u201d, line 664, in launch_instance\napp.start()\nFile \u201c\/usr\/local\/lib\/python3.6\/dist-packages\/ipykernel\/kernelapp.py\u201d, line 499, in start\nself.io_loop.start()\nFile \u201c\/usr\/local\/lib\/python3.6\/dist-packages\/tornado\/platform\/asyncio.py\u201d, line 132, in start\nself.asyncio_loop.run_forever()\nFile \u201c\/usr\/lib\/python3.6\/asyncio\/base_events.py\u201d, line 438, in run_forever\nself._run_once()\nFile \u201c\/usr\/lib\/python3.6\/asyncio\/base_events.py\u201d, line 1451, in _run_once\nhandle._run()\nFile \u201c\/usr\/lib\/python3.6\/asyncio\/events.py\u201d, line 145, in _run\nself._callback(*self._args)\nFile \u201c\/usr\/local\/lib\/python3.6\/dist-packages\/tornado\/platform\/asyncio.py\u201d, line 122, in _handle_events\nhandler_func(fileobj, events)\nFile \u201c\/usr\/local\/lib\/python3.6\/dist-packages\/tornado\/stack_context.py\u201d, line 300, in null_wrapper\nreturn fn(*args, **kwargs)\nFile \u201c\/usr\/local\/lib\/python3.6\/dist-packages\/zmq\/eventloop\/zmqstream.py\u201d, line 462, in _handle_events\nself._handle_recv()\nFile \u201c\/usr\/local\/lib\/python3.6\/dist-packages\/zmq\/eventloop\/zmqstream.py\u201d, line 492, in _handle_recv\nself._run_callback(callback, msg)\nFile \u201c\/usr\/local\/lib\/python3.6\/dist-packages\/zmq\/eventloop\/zmqstream.py\u201d, line 444, in _run_callback\ncallback(*args, **kwargs)\nFile \u201c\/usr\/local\/lib\/python3.6\/dist-packages\/tornado\/stack_context.py\u201d, line 300, in null_wrapper\nreturn fn(*args, **kwargs)\nFile \u201c\/usr\/local\/lib\/python3.6\/dist-packages\/ipykernel\/kernelbase.py\u201d, line 283, in dispatcher\nreturn self.dispatch_shell(stream, msg)\nFile \u201c\/usr\/local\/lib\/python3.6\/dist-packages\/ipykernel\/kernelbase.py\u201d, line 233, in dispatch_shell\nhandler(stream, idents, msg)\nFile \u201c\/usr\/local\/lib\/python3.6\/dist-packages\/ipykernel\/kernelbase.py\u201d, line 399, in execute_request\nuser_expressions, allow_stdin)\nFile \u201c\/usr\/local\/lib\/python3.6\/dist-packages\/ipykernel\/ipkernel.py\u201d, line 208, in do_execute\nres = shell.run_cell(code, store_history=store_history, silent=silent)\nFile \u201c\/usr\/local\/lib\/python3.6\/dist-packages\/ipykernel\/zmqshell.py\u201d, line 537, in run_cell\nreturn super(ZMQInteractiveShell, self).run_cell(*args, **kwargs)\nFile \u201c\/usr\/local\/lib\/python3.6\/dist-packages\/IPython\/core\/interactiveshell.py\u201d, line 2718, in run_cell\ninteractivity=interactivity, compiler=compiler, result=result)\nFile \u201c\/usr\/local\/lib\/python3.6\/dist-packages\/IPython\/core\/interactiveshell.py\u201d, line 2822, in run_ast_nodes\nif self.run_code(code, result):\nFile \u201c\/usr\/local\/lib\/python3.6\/dist-packages\/IPython\/core\/interactiveshell.py\u201d, line 2882, in run_code\nexec(code_obj, self.user_global_ns, self.user_ns)\nFile \u201c\u201d, line 1223, in \nvalue_v = net_crt(states_v)\nFile \u201c\/usr\/local\/lib\/python3.6\/dist-packages\/torch\/nn\/modules\/module.py\u201d, line 541, in call\nresult = self.forward(*input, **kwargs)\nFile \u201c\u201d, line 998, in forward\nsave[b] = self.dense(out)\nFile \u201c\/usr\/local\/lib\/python3.6\/dist-packages\/torch\/nn\/modules\/module.py\u201d, line 541, in call\nresult = self.forward(*input, **kwargs)\nFile \u201c\/usr\/local\/lib\/python3.6\/dist-packages\/torch\/nn\/modules\/container.py\u201d, line 92, in forward\ninput = module(input)\nFile \u201c\/usr\/local\/lib\/python3.6\/dist-packages\/torch\/nn\/modules\/module.py\u201d, line 541, in call\nresult = self.forward(*input, **kwargs)\nFile \u201c\/usr\/local\/lib\/python3.6\/dist-packages\/torch\/nn\/modules\/linear.py\u201d, line 87, in forward\nreturn F.linear(input, self.weight, self.bias)\nFile \u201c\/usr\/local\/lib\/python3.6\/dist-packages\/torch\/nn\/functional.py\u201d, line 1372, in linear\noutput = input.matmul(weight.t())\n\n\n\n Stihl:\n\nout[4] = hid_4[0][-1]\n\n\nI guess each of these calls are actually modifying out inplace. But it\u2019s value is needed by the dense(out) call?\nCan you try not to change out inplace and just torch.cat() your hidden layers together?\nThank you \nIt works now"},{"x":"I am trying to write code for some of the Meta-Learning algorithms. I understand that there are a few packages available for easy and hassle-free implementation of Meta-Learning algorithms (higher with link \"https:\/\/github.com\/facebookresearch\/higher\", pytorch-meta with link \"https:\/\/github.com\/tristandeleu\/pytorch-meta\") but I want to understand a few things conceptually.\nRecently, a few meta-learning algorithm implementations such as Learning to Reweight with link \"https:\/\/github.com\/danieltan07\/learning-to-reweight-examples\", Meta-Weight Net with link \"https:\/\/github.com\/xjtushujun\/meta-weight-net\", etc. have not been using higher or pytorch-meta, and, instead, have been using a custom nn.Module (see code below) written by Daniel(link for this code: https:\/\/github.com\/danieltan07\/learning-to-reweight-examples\/blob\/master\/meta_layers.py). Basically, it\u2019s the usual PyTorch code for supervised learning with the only change being: using custom nn.Module instead of the nn.Module.\nI am pasting the relevant code below. The nn.Module shown below (Daniel\u2019s code) is what people have been using for their meta learning algorithms thereby not requiring the additional packages I have mentioned above.\n<code class=\"lang-auto\">class MetaModule(nn.Module):\n    # adopted from: Adrien Ecoffet https:\/\/github.com\/AdrienLE\n    def params(self):\n       for name, param in self.named_params(self):\n            yield param\n    \n    def named_leaves(self):\n        return []\n    \n    def named_submodules(self):\n        return []\n    \n    def named_params(self, curr_module=None, memo=None, prefix=''):       \n        if memo is None:\n            memo = set()\n\n        if hasattr(curr_module, 'named_leaves'):\n            for name, p in curr_module.named_leaves():\n                if p is not None and p not in memo:\n                    memo.add(p)\n                    yield prefix + ('.' if prefix else '') + name, p\n        else:\n            for name, p in curr_module._parameters.items():\n                if p is not None and p not in memo:\n                    memo.add(p)\n                    yield prefix + ('.' if prefix else '') + name, p\n                    \n        for mname, module in curr_module.named_children():\n            submodule_prefix = prefix + ('.' if prefix else '') + mname\n            for name, p in self.named_params(module, memo, submodule_prefix):\n                yield name, p\n    \n    def update_params(self, lr_inner, first_order=False, source_params=None, detach=False):\n        if source_params is not None:\n            for tgt, src in zip(self.named_params(self), source_params):\n                name_t, param_t = tgt\n                # name_s, param_s = src\n                # grad = param_s.grad\n                # name_s, param_s = src\n                grad = src\n                if first_order:\n                    grad = to_var(grad.detach().data)\n                tmp = param_t - lr_inner * grad\n                self.set_param(self, name_t, tmp)\n        else:\n\n            for name, param in self.named_params(self):\n                if not detach:\n                    grad = param.grad\n                    if first_order:\n                        grad = to_var(grad.detach().data)\n                    tmp = param - lr_inner * grad\n                    self.set_param(self, name, tmp)\n                else:\n                    param = param.detach_()\n                    self.set_param(self, name, param)\n\n    def set_param(self,curr_mod, name, param):\n        if '.' in name:\n            n = name.split('.')\n            module_name = n[0]\n            rest = '.'.join(n[1:])\n            for name, mod in curr_mod.named_children():\n                if module_name == name:\n                    self.set_param(mod, rest, param)\n                    break\n        else:\n            setattr(curr_mod, name, param)\n            \n    def detach_params(self):\n        for name, param in self.named_params(self):\n            self.set_param(self, name, param.detach())   \n                \n    def copy(self, other, same_var=False):\n        for name, param in other.named_params():\n            if not same_var:\n                param = to_var(param.data.clone(), requires_grad=True)\n            self.set_param(name, param)\n<\/code>\nUsing such MetaModule one can create MetaLinear, MetaConv2D, etc. which can be used instead of nn.Linear, nn.Conv2D:\n<code class=\"lang-auto\">class MetaLinear(MetaModule):\n    def __init__(self, *args, **kwargs):\n        super().__init__()\n        ignore = nn.Linear(*args, **kwargs)\n       \n        self.register_buffer('weight', to_var(ignore.weight.data, requires_grad=True))\n        self.register_buffer('bias', to_var(ignore.bias.data, requires_grad=True))\n        \n    def forward(self, x):\n        return F.linear(x, self.weight, self.bias)\n    \n    def named_leaves(self):\n        return [('weight', self.weight), ('bias', self.bias)]\n    \nclass MetaConv2d(MetaModule):\n    def __init__(self, *args, **kwargs):\n        super().__init__()\n        ignore = nn.Conv2d(*args, **kwargs)\n        \n        self.stride = ignore.stride\n        self.padding = ignore.padding\n        self.dilation = ignore.dilation\n        self.groups = ignore.groups\n        \n        self.register_buffer('weight', to_var(ignore.weight.data, requires_grad=True))\n        \n        if ignore.bias is not None:\n            self.register_buffer('bias', to_var(ignore.bias.data, requires_grad=True))\n        else:\n            self.register_buffer('bias', None)\n        \n    def forward(self, x):\n        return F.conv2d(x, self.weight, self.bias, self.stride, self.padding, self.dilation, self.groups)\n    \n    def named_leaves(self):\n        return [('weight', self.weight), ('bias', self.bias)]\n<\/code>\nI have the following question:\n\n\nWhy should one need to create a custom nn.Module and then do setattr(name, params) to update nn.Parameters (as is done in the update_params function in MetaModule class) in a way that these operations are recorded in the computation graph as well? Why can\u2019t I directly use setattr(name, params) in my regular training loop (i.e. def train(*args, **kwargs) function) with standard, built-in nn.Module?\n\n\nI do understand that there\u2019s another way to deal with this (link: [resolved] Implementing MAML in PyTorch with link \"https:\/\/discuss.pytorch.org\/t\/resolved-implementing-maml-in-pytorch\/4053\"). For instance, the def forward(self, x) function for nn.Module can be modified to def forward(self, x, weights) instead so that the code works. But I am not fully clear about this as well. I understand the nn.Parameters don\u2019t record history and hence we need to operator on other Tensors and then copy those values to nn.Parameters but I wonder if I can do what I want without having to resort to this technique as well.\n\n\nNote: My question is somewhat similar to Second order derivatives in meta-learning with link \"https:\/\/discuss.pytorch.org\/t\/second-order-derivatives-in-meta-learning\/76656\". However, what I am asking is conceptual and not necessarily a request for a workaround. And the only response in that thread is mine, so I am still not clear about implementing meta-learning algorithms in PyTorch.","y":"I think what you are trying to say is that if I want the nn.Parameters to \u201crecord\u201d history, the example I talked about above uses register_buffer instead of nn.Parameter as a neat hack.\nI think it\u2019s starting to make sense now. Here\u2019s what I think is going on (code: Daniel\u2019s code for \u2018Learning To Reweight\u2019 algorithm with link \"https:\/\/github.com\/danieltan07\/learning-to-reweight-examples\/blob\/master\/Learning%20to%20Reweight%20Examples%20for%20Robust%20Deep%20Learning.ipynb\"):\n<code class=\"lang-auto\">def train_lre():\n    net, opt = build_model() # uses MetaModule to create the model instead of nn.Module\n    \n    meta_losses_clean = []\n    net_losses = []\n    plot_step = 100\n\n    smoothing_alpha = 0.9\n    \n    meta_l = 0\n    net_l = 0\n    accuracy_log = []\n    for i in tqdm(range(hyperparameters['num_iterations'])):\n        net.train()\n        # Line 2 get batch of data\n        image, labels = next(iter(data_loader))\n        # since validation data is small I just fixed them instead of building an iterator\n        # initialize a dummy network for the meta learning of the weights\n        meta_net = LeNet(n_out=1)\n        meta_net.load_state_dict(net.state_dict())\n\n        if torch.cuda.is_available():\n            meta_net.cuda()\n\n        image = to_var(image, requires_grad=False)\n        labels = to_var(labels, requires_grad=False)\n\n        # Lines 4 - 5 initial forward pass to compute the initial weighted loss\n        y_f_hat  = meta_net(image)\n        cost = F.binary_cross_entropy_with_logits(y_f_hat,labels, reduce=False)\n        eps = to_var(torch.zeros(cost.size()))\n        l_f_meta = torch.sum(cost * eps)\n\n        meta_net.zero_grad()\n        \n        # Line 6 perform a parameter update\n        grads = torch.autograd.grad(l_f_meta, (meta_net.params()), create_graph=True)\n        meta_net.update_params(hyperparameters['lr'], source_params=grads)\n        \n        # Line 8 - 10 2nd forward pass and getting the gradients with respect to epsilon\n        y_g_hat = meta_net(val_data)\n\n        l_g_meta = F.binary_cross_entropy_with_logits(y_g_hat,val_labels)\n\n        grad_eps = torch.autograd.grad(l_g_meta, eps, only_inputs=True)[0]\n        \n        # Line 11 computing and normalizing the weights\n        w_tilde = torch.clamp(-grad_eps,min=0)\n        norm_c = torch.sum(w_tilde)\n\n        if norm_c != 0:\n            w = w_tilde \/ norm_c\n        else:\n            w = w_tilde\n\n        # Lines 12 - 14 computing for the loss with the computed weights\n        # and then perform a parameter update\n        y_f_hat = net(image)\n        cost = F.binary_cross_entropy_with_logits(y_f_hat, labels, reduce=False)\n        l_f = torch.sum(cost * w)\n\n        opt.zero_grad()\n        l_f.backward()\n        opt.step()\n\n    return np.mean(acc_log[-6:-1, 1])\n<\/code>\nThe trick here is to use the buffers (created via register_buffer in MetaLinear, etc.)  to access nn.Parameter to create trainable tensors (weight and bias) and use them (instead of nn.Parameters which don\u2019t record any history) via named_leaves, named_params, update_params, and set_param functions to do the meta-learning. The nn.Parameters are updated via opt.step() whereas all the intermediate nn.Parameter updates required for meta-learning are handled via the buffer variables (viz. weight and bias created via \u2018register_buffer\u2019).\n Sorry for bothering you so much but I think that does explain it, don\u2019t you think?","z":"Hi,\n\nI think the set_param function here is mainly built to be able to handle nested names. For example if your module is sequential that contains a conv. Then the name will be 0.weight. But you cannot use python\u2019s setattr on that, you need to do first access \u201c0\u201d then \u201cweight\u201d.\nI don\u2019t think you can get around some kind of logic like that. The main reason being that you don\u2019t want to override the original Parameters. Because you need to be able to backpropagate all the way back to them to update them. And so the intermediary Tensors can\u2019t just be these Parameters modified inplace.\n\nHope this helps.\nThanks   for responding so quickly. I understand your points and the part about handling nested names. But I want understand why is it a problem when I use setattr() for each of those weights\/biases separately via MetaModule - something like this:\n<code class=\"lang-auto\">new_name_params = ...\nfor xxx in modules_of_network:\n   for name, p in xxx.named_parameters():\n       setattr(xxx, name, new_named_params[name])\n<\/code>\nI think I understand the problems in this approach to some degree but I am not clear how using setattr() in a custom nn.Module - MetaModule doesn\u2019t throw the same kind of errors even though the parameters witness an in-place update.\nsetattr is actually the same as doing xxx.name = new_named_params[name].\nSo if it is already a nn.Parameter, you won\u2019t be able to set a Tensor with history there.\nI understand. But that still doesn\u2019t explain how MetaModule enables Meta-Learning without using packages such as higher and others.\nHi,\nIt handles the params is a different way compared to the regular nn.Module. In particular, it allows the parameters to have some history associated with them by not having them be nn.Parameters.\nI think what you are trying to say is that if I want the nn.Parameters to \u201crecord\u201d history, the example I talked about above uses register_buffer instead of nn.Parameter as a neat hack.\nI think it\u2019s starting to make sense now. Here\u2019s what I think is going on (code: Daniel\u2019s code for \u2018Learning To Reweight\u2019 algorithm with link \"https:\/\/github.com\/danieltan07\/learning-to-reweight-examples\/blob\/master\/Learning%20to%20Reweight%20Examples%20for%20Robust%20Deep%20Learning.ipynb\"):\n<code class=\"lang-auto\">def train_lre():\n    net, opt = build_model() # uses MetaModule to create the model instead of nn.Module\n    \n    meta_losses_clean = []\n    net_losses = []\n    plot_step = 100\n\n    smoothing_alpha = 0.9\n    \n    meta_l = 0\n    net_l = 0\n    accuracy_log = []\n    for i in tqdm(range(hyperparameters['num_iterations'])):\n        net.train()\n        # Line 2 get batch of data\n        image, labels = next(iter(data_loader))\n        # since validation data is small I just fixed them instead of building an iterator\n        # initialize a dummy network for the meta learning of the weights\n        meta_net = LeNet(n_out=1)\n        meta_net.load_state_dict(net.state_dict())\n\n        if torch.cuda.is_available():\n            meta_net.cuda()\n\n        image = to_var(image, requires_grad=False)\n        labels = to_var(labels, requires_grad=False)\n\n        # Lines 4 - 5 initial forward pass to compute the initial weighted loss\n        y_f_hat  = meta_net(image)\n        cost = F.binary_cross_entropy_with_logits(y_f_hat,labels, reduce=False)\n        eps = to_var(torch.zeros(cost.size()))\n        l_f_meta = torch.sum(cost * eps)\n\n        meta_net.zero_grad()\n        \n        # Line 6 perform a parameter update\n        grads = torch.autograd.grad(l_f_meta, (meta_net.params()), create_graph=True)\n        meta_net.update_params(hyperparameters['lr'], source_params=grads)\n        \n        # Line 8 - 10 2nd forward pass and getting the gradients with respect to epsilon\n        y_g_hat = meta_net(val_data)\n\n        l_g_meta = F.binary_cross_entropy_with_logits(y_g_hat,val_labels)\n\n        grad_eps = torch.autograd.grad(l_g_meta, eps, only_inputs=True)[0]\n        \n        # Line 11 computing and normalizing the weights\n        w_tilde = torch.clamp(-grad_eps,min=0)\n        norm_c = torch.sum(w_tilde)\n\n        if norm_c != 0:\n            w = w_tilde \/ norm_c\n        else:\n            w = w_tilde\n\n        # Lines 12 - 14 computing for the loss with the computed weights\n        # and then perform a parameter update\n        y_f_hat = net(image)\n        cost = F.binary_cross_entropy_with_logits(y_f_hat, labels, reduce=False)\n        l_f = torch.sum(cost * w)\n\n        opt.zero_grad()\n        l_f.backward()\n        opt.step()\n\n    return np.mean(acc_log[-6:-1, 1])\n<\/code>\nThe trick here is to use the buffers (created via register_buffer in MetaLinear, etc.)  to access nn.Parameter to create trainable tensors (weight and bias) and use them (instead of nn.Parameters which don\u2019t record any history) via named_leaves, named_params, update_params, and set_param functions to do the meta-learning. The nn.Parameters are updated via opt.step() whereas all the intermediate nn.Parameter updates required for meta-learning are handled via the buffer variables (viz. weight and bias created via \u2018register_buffer\u2019).\n Sorry for bothering you so much but I think that does explain it, don\u2019t you think?\nHi,\nYes I think it does.\nNote that having the intermediary results as buffer vs just regular attributes doesn\u2019t change much.\nIt will only change when you get the state dict or move the module to a different device. But hopefully you should not be doing that in the middle of the forward pass \nI didn\u2019t get you, . Can you elaborate a bit?\nFor any Tensor in the nn.Module, you can store it in self by doing self.foo = your_tensor or by doing self.register_buffer(\"foo\", your_tensor).\nI think that both will have the behavior that you want: you can access them by using self.foo and they can have history.\nI think that the first one might be simpler to read as it is basic python semantic.\nI can\u2019t think of any reason why you need it to actually be a buffer (maybe I\u2019m missing something though)."},{"x":"I am reading in the book Deep Learning with PyTorch that by calling the nn.Module.parameters() method that it will call submodules defined in the module\u2019s init constructor. To understand and help visualize the processes I would like to use an ensemble as an example from ptrblck with link \"https:\/\/discuss.pytorch.org\/t\/custom-ensemble-approach\/52024\/4\":\n<code class=\"lang-auto\">class MyEnsemble(nn.Module):\n    def __init__(self, modelA, modelB, nb_classes=10):\n        super(MyEnsemble, self).__init__()\n        self.modelA = modelA\n        self.modelB = modelB\n        # Remove last linear layer\n        self.modelA.fc = nn.Identity()\n        self.modelB.fc = nn.Identity()\n        \n        # Create new classifier\n        self.classifier = nn.Linear(2048+512, nb_classes)\n        \n    def forward(self, x):\n        x1 = self.modelA(x.clone())  # clone to make sure x is not changed by inplace methods\n        x1 = x1.view(x1.size(0), -1)\n        x2 = self.modelB(x)\n        x2 = x2.view(x2.size(0), -1)\n        x = torch.cat((x1, x2), dim=1)\n        \n        x = self.classifier(F.relu(x))\n        return x\n<\/code>\nIn this nn.Module both self.modelA = modelA and self.modelB = modelB are being called in the init constructor. Therefore, by calling MyEnsemble.parameters() we would be returned the params which autograd would calculate the gradients wrt the parameters of the models MyEnsemble, modelA, and modelB?\nUnless, of course, requires_grad=False for self.modelA and self.modelB - in which case autograd would not calculate the gradients wrt the parameters of these models and only that of MyEnsemble?\nIs this thinking correct? Please correct my language if it is off. I am trying to learn as best I can and all help is appreciated.\nDavid A.","y":"\n\n\n David_Alford:\n\nTherefore, by calling MyEnsemble.parameters() we would be returned the params which autograd would calculate the gradients wrt the parameters of the models MyEnsemble , modelA , and modelB ?\nUnless, of course, requires_grad=False for self.modelA and self.modelB - in which case autograd would not calculate the gradients wrt the parameters of these models and only that of MyEnsemble ?\n\n\nYes, you are correct that the gradients won\u2019t be calculated for parameters, which are using requires_grad=False. However, model.parameters() would still return all parameters, if you are not filtering them out.","z":"\n\n\n David_Alford:\n\nTherefore, by calling MyEnsemble.parameters() we would be returned the params which autograd would calculate the gradients wrt the parameters of the models MyEnsemble , modelA , and modelB ?\nUnless, of course, requires_grad=False for self.modelA and self.modelB - in which case autograd would not calculate the gradients wrt the parameters of these models and only that of MyEnsemble ?\n\n\nYes, you are correct that the gradients won\u2019t be calculated for parameters, which are using requires_grad=False. However, model.parameters() would still return all parameters, if you are not filtering them out.\nOk. Thank you for the clarification.\nSo if I call Module.parameters().grad I will be able to see the gradients? And I should call optimizer.zero_grad() each epoch to clear the gradients so they don\u2019t accumulate? (Doesnt matter where you call zero_grad()).\nEDIT: Also, what do you think is the area of PyTorch most people have trouble with? My focus is Computer Vision. Is there a concept\/area in PyTorch that I can focus my energy on where you think people commonly make mistakes?\nThanks\nModule.parameters().grad won\u2019t work directly and you would have to create a list or iterate it:\n<code class=\"lang-python\">for param in Module.parameters():\n    print(param.grad)\n<\/code>\n\n\n\n David_Alford:\n\nAnd I should call optimizer.zero_grad() each epoch to clear the gradients so they don\u2019t accumulate?\n\n\nThe usual workflow is to zero out the gradients after each iteration, if you don\u2019t want to accumulate the gradients.\n\n\n\n David_Alford:\n\nDoesnt matter where you call zero_grad()\n\n\nAs long as zero_grad() is not called after the backward() call and before optimizer.step(), it should be OK.\n\n\n\n David_Alford:\n\nAlso, what do you think is the area of PyTorch most people have trouble with? My focus is Computer Vision. Is there a concept\/area in PyTorch that I can focus my energy on where you think people commonly make mistakes?\n\n\nDo you mean here in the discussion board or in the general framework usage?\n\n\n\n ptrblck:\n\nModule.parameters().grad won\u2019t work directly and you would have to create a list or iterate it:\n<code class=\"lang-auto\">for param in Module.parameters():\n    print(param.grad)\n<\/code>\n\n\nThat makes sense.\n\n\n\n ptrblck:\n\nDo you mean here in the discussion board or in the general framework usage?\n\n\nIn the general framework usage. I see you are very knowledgeable about PyTorch. Where would you recommend someone focus their energy when learning PyTorch? Any areas you see more mistakes than others that I could focus on?\nThank you for taking the time for the responses.\nWhen you are trying to learn PyTorch, I would suggest to pick an interesting (and personal) project you could spend some time on. E.g. if you are interested in photography and would like to experiment with some style transfer approaches, this would be a great way to learn more about GANs and other architectures. You would learn the framework just by working on the project. \nOn the other hand, if you would like to contribute to PyTorch, I would recommend to have a look at the usability \/ simple-fixes or misc category here with link \"https:\/\/github.com\/pytorch\/pytorch\/projects\/5\".\nAlso, the good first issue with link \"https:\/\/github.com\/pytorch\/pytorch\/issues?q=is%3Aopen+is%3Aissue+label%3A%22good+first+issue%22\" label is useful to check for some starter PRs. The Contribution Guide with link \"https:\/\/pytorch.org\/docs\/stable\/community\/contribution_guide.html\" is a good way to get started.\nGenerally, I would say that a lot of new users would have some trouble with language models, i.e. how the shapes are used in RNNs, where and when to detach() the activations etc."},{"x":"Hello.\nWhile working with a project involving an RNN, I\u2019ve ran into an error of:\nRuntimeError: one of the variables needed for gradient computation has been modified by an inplace operation\nHere is a minimum working example to reproduce the error:\n<code class=\"lang-auto\"># Setup\nimport torch\ntorch.autograd.set_detect_anomaly(True)\n\n# Initialization\ninput_dim = 8\nmodel = torch.nn.Linear(input_dim, 1)\noptimizer = torch.optim.Adam(model.parameters(), lr=1e-3)\nvalues = list()\nforward_input = torch.rand(1, input_dim)\n\n# Forward\nn_iters = 4\nfor i in range(n_iters):\n    value = model(forward_input)  \n    values.append(value)\n\n# First iteration works\nvalue = values[0].clone()\nloss = value.sum()\noptimizer.zero_grad()\nloss.backward()\noptimizer.step()\n\n# Second iteration fails\nvalue = values[1].clone()\nloss = value.sum()\noptimizer.zero_grad()\nloss.backward()\noptimizer.step()\n<\/code>\n(PS: I know this is a silly example, but it was the simplest way I could reproduce the error  )\nWhy does this happen? Any help would be GREATLY appreciated. Here is the full traceback:\n<code class=\"lang-auto\">RuntimeError                              Traceback (most recent call last)\n<ipython-input-15-70245d07c321> in <module>\n     28 loss = value.sum()\n     29 optimizer.zero_grad()\n---> 30 loss.backward()\n     31 optimizer.step()\n\n~\/projects\/summarization\/summarization-project\/lib\/python3.7\/site-packages\/torch\/tensor.py in backward(self, gradient, retain_graph, create_graph)\n    196                 products. Defaults to ``False``.\n    197         \"\"\"\n--> 198         torch.autograd.backward(self, gradient, retain_graph, create_graph)\n    199 \n    200     def register_hook(self, hook):\n\n~\/projects\/summarization\/summarization-project\/lib\/python3.7\/site-packages\/torch\/autograd\/__init__.py in backward(tensors, grad_tensors, retain_graph, create_graph, grad_variables)\n     98     Variable._execution_engine.run_backward(\n     99         tensors, grad_tensors, retain_graph, create_graph,\n--> 100         allow_unreachable=True)  # allow_unreachable flag\n    101 \n    102 \n\nRuntimeError: one of the variables needed for gradient computation has been modified by an inplace operation: [torch.FloatTensor [8, 1]], which is output 0 of TBackward, is at version 2; expected version 1 instead. Hint: the backtrace further above shows the operation that failed to compute its gradient. The variable in question was changed in there or anywhere later. Good luck!\n<\/code>\nEnvironment details:\n\ntorch==1.5.1\ntorchvision==0.6.1\npython version 3.7.7\n","y":"The error is because the weight of the linear layer has changed (through optimizer.step).\nOne might add that here, you have the gradient computation for addmm (which powers linear) pretend it would also want to compute the input derivative for which it would need the weight which, in this trivial use, is not actually the case. If you have a multiple layers, all but the first need the input gradient and you cannot actually change the weight and then compute the (correct) backward, not even with retain_graph.\nBest regards\nThomas","z":"The error is because the weight of the linear layer has changed (through optimizer.step).\nOne might add that here, you have the gradient computation for addmm (which powers linear) pretend it would also want to compute the input derivative for which it would need the weight which, in this trivial use, is not actually the case. If you have a multiple layers, all but the first need the input gradient and you cannot actually change the weight and then compute the (correct) backward, not even with retain_graph.\nBest regards\nThomas"},{"x":"I need the gradient of this warping operation.\n<code class=\"lang-auto\">\/\/ - tensor   :  output of my neural network \n\/\/               tensor.requires_grad = True\n\nwarped_tensor = F.grid_sample(tensor, \n                              grid,\n                              align_corners=True,\n                              mode='bilinear',\n                              padding_mode='zeros').\n<\/code>\nThis operation returns a gradient, however it seems to be not correct. I used the warped_tensor and just the tensor for my loss and with the warped_tensor my network does not correctly optimise the weights.\nIs this an autograd issue or is there some other issue I am not seeing here?\n\nThe same happens when I use #vision:kornia with link \"\/c\/vision\/kornia\/19\"\n<code class=\"lang-auto\">\/\/ - tensor   :  output of my neural network \n\/\/               tensor.requires_grad = True\n\/\/ - M      : transformation Matrix prev created with kornia\n\nwarped_tensor: torch.tensor = kornia.warp_affine(tensor, M, dsize=(h_original, w_original))\n<\/code>","y":"Depends on the warping you apply.\nYou can loose precision when you warp if you collapse multiple input pixels into a single output one?\nI can take a look if you have a small code sample yes !","z":"Hi,\nThis is most likely due to your network. This function is quite widely used and tested so I think it is correct.\nYou can double check by using torch.autograd.gradcheck.gradcheck function wil double typed inputs to verify that the computed gradients are correct.\nWill check my network with torch.autograd.gradcheck.gradcheck.\nHowever isn\u2019t it weird that my network works when I use loss(tensor, truth) but not with loss(warped_tensor, warped_truth)  where warped_truth was applied the exact same transformation.\nI will try to provide you with some executable code soon.\nDepends on the warping you apply.\nYou can loose precision when you warp if you collapse multiple input pixels into a single output one?\nI can take a look if you have a small code sample yes !\nThis makes sense! I am quite certain now that my error lies here.\nHere is a gist using kornia.warp_affine  for the transformation as they use F.grid_sample under the hood.  (source for kornia.warp_affine with link \"https:\/\/github.com\/kornia\/kornia\/blob\/20a9acca2636512522116601ae09c6be0408b486\/kornia\/geometry\/transform\/imgwarp.py#L126\")\nHere is my code:\n\n\ngist.github.com with link \"https:\/\/gist.github.com\/mowolf\/fa1b0c327ddd3add758bac5d990a4638\"\n\n\nhttps:\/\/gist.github.com\/mowolf\/fa1b0c327ddd3add758bac5d990a4638\nmock_transformation_network.py\n<code class=\"Python\">import kornia\nimport numpy as np\nimport torch.nn.functional as F\n\n\ndef get_M():\n    '''\n    Provide mock trafo Matrix and original image sizes\n    :return: w_original, h_original, M \n    '''<\/code>\nThis file has been truncated. show original with link \"https:\/\/gist.github.com\/mowolf\/fa1b0c327ddd3add758bac5d990a4638\"\n\n\n\n\n\n\n\n\nHow can I now make sure that my gradient is not loosing precision? Must the size of the resulting trafo tensor just be big enough? The interpolation with nearest might also have this problem. Would bilinear fix that or should I make sure that I dont scale down?\nI don\u2019t have kornia at hand to try it but yes these sound like good things to try out.\nHas there been any update on this ? My first guesses were too that nearest mode might not work but that did not help in my case."},{"x":"The code is given below. How can the  variable computed in the previous epoch be used for\ncomputing the loss function in the next epoch?\nfor epoch in range((args.start_epoch+1), args.epochs):\nfor input, target in train_loader:\ntarget = target.cuda()\ninput=input.cuda()\ninput_var = torch.autograd.Variable(input)\ntarget_var = torch.autograd.Variable(target)\noutputs, feature = model(input_var)\nif epoch>0:\nl= criterion.forward(feature,target_var, Fea)\nFea=function(model,train_loader)","y":"Even though you are calculating the center manually, does Autograd need to backpropagate through these operations?\nCompare it to the target of a classification use case. While the target tensor is of course not a constant value, it\u2019s a constant in the sense that Autograd will use it to calculate the loss, and based on this compute the gradient for the parameters in the model, which created the prediction.\nWould the same use case be applied here?","z":"If you want to use feature from the previous iteration, you could store it in another variable so that it won\u2019t be overwritten by the model(input_var) call.\nNote that Variables are deprecated since PyTorch 0.4 so you can use tensors in newer versions.\nAlso, call nn.Modules directly via criterion(feature, ...) instead of the .forward method, as the latter approach won\u2019t call into registered hooks and might yield unexpected behavior.\nThank you very much!\nl= criterion(feature,target_var, Fea).forward()\nl.backward(retain_graph=True)\nmay be the solution.\nI want use the class feature centers in the loss function. But when I compute the centers,\nthe GPU memory is not enough. How can I solve it? The code is as follows. Thank you very much!\n<code class=\"lang-auto\">for epoch in range((args.start_epoch+1), args.epochs):\n  Center= computer_Center(model,dataloader, classnum)\n\tfor input, target in train_loader:\n        target = target.cuda()\n        input = input.cuda()\n        input_var = torch.autograd.Variable(input)\n        target_var = torch.autograd.Variable(target)\n        outputs, feature = model(input_var)\n        l = criterion(feature,target_var, Center) .forward()\n        l.backward(retain_graph=True)\n\ndef computer_Center(model,dataloader, classnum):\n    model.train()\n    for i in range(classnum):\n            j=0\n            for input,target in dataloader:\n                target=target.cuda()\n                input = input.cuda()\n                input_var = torch.autograd.Variable(input)\n                target=torch.autograd.Variable(target)\n                _, feature_ext = model(input_var)\n                ind=torch.where(target==i)[0]\n                if ind.shape[0]>0:\n                    if j==0:\n                        feature_mid = feature_ext[ind, :]\n                        feature_sum_mid=feature_mid.sum(0)\n                    else:\n                        feature_mid = feature_ext[ind, :]\n                        feature_sum_mid = feature_sum_mid+feature_mid.sum(0)\n                    j=j+1\n\n            feature_sum_mid=feature_sum_mid.unsqueeze(0)\n            if i==0:\n                feature_sum=feature_sum_mid\n            else:\n                feature_sum=torch.cat([feature_sum,feature_sum_mid],dim=0)\n\n    Center=feature_sum\n    for i in range(7):\n            Center[i,:]=feature_sum[i,:]\/ClaSamNum[i]\n\nreturn Center\n\n<\/code>\nIn you code snippet you are accumulating the model output feature_ext in feature_sum_mid, which will also store the computation graph (including all intermediate tensors).\nIf you want to use this Center tensor as a constant target, you should wrap the calculation in compute_Center into with torch.no_grad() to avoid storing the computation graphs and thus lower the memory usage.\nAlso, Variables are deprecated since PyTorch 0.4, so you can use tensors in newer versions. \nThank you very much. I want to compute the loss for the Center. The Center is not a constant target. Now I computer the Center in the batch-wise. But the performance is not good.\nEven though you are calculating the center manually, does Autograd need to backpropagate through these operations?\nCompare it to the target of a classification use case. While the target tensor is of course not a constant value, it\u2019s a constant in the sense that Autograd will use it to calculate the loss, and based on this compute the gradient for the parameters in the model, which created the prediction.\nWould the same use case be applied here?\nI see. Thank you very very very much!"},{"x":"I am using PyTorch 0.4\nimport torch\nX=torch.randn((100,3))\nY=torch.randn((100))\nw1=torch.tensor(0.1, requires_grad=True)\nw2=torch.tensor(0.1, requires_grad=True)\nw3=torch.tensor(0.1, requires_grad=True)\nW=torch.tensor([0.1, 0.1, 0.1], requires_grad=True)\nW[0]=w1 * w2;  W[1]=w2 * w3;  W[2]=w3 * w1\n#W=torch.cat([w1.view(1),w2.view(1),w3.view(1)])\nYp=torch.sum(X*W, dim=1)\nloss = torch.nn.MSELoss()(Yp, Y)\nloss.backward()\nrun the code and I got:\nRuntimeError: leaf variable has been moved into the graph interior\nuncomment  #W, and it is fine\nuse case:\nw1, w2, w3,\u2026, are many many \u2026 tensors from outputs of some modules\nthen we can assemble them to a big tensor via (1) or (2) below:\n(1) use torch.cat\n(2) create a tensor W, and assign w1, w2, w3 to subsections of W.  It is easier to control where the w1\/w2\/w3 should be put into the W","y":"You can read a nice explanation of leaf variables in this post with link \"https:\/\/discuss.pytorch.org\/t\/leaf-variable-was-used-in-an-inplace-operation\/308\/2?u=ptrblck\".\nUsually you have to avoid modifying a leaf variable with an in-place operation.\nI haven\u2019t seen your error message yet, but it seems it related to an in-place modification, because you are re-assigning the values of W in this line:\nW[0]=w1 * w2; W[1]=w2 * w3; W[2]=w3 * w1","z":"I don\u2019t think you need to initialize W with data and requires_grad=True, if you are overwriting the values in the next line.\nWould this work for you?\n<code class=\"lang-auto\">w1=torch.tensor(0.1, requires_grad=True)\nw2=torch.tensor(0.1, requires_grad=True)\nw3=torch.tensor(0.1, requires_grad=True)\nW=torch.empty(3, requires_grad=False)\nW[0]=w1 * w2; W[1]=w2 * w3; W[2]=w3 * w1\nYp=torch.sum(X*W, dim=1)\nloss = torch.nn.MSELoss()(Yp, Y)\nloss.backward()\nprint(w1.grad)\n<\/code>\nIt works. Thank you!\nI\u2019m curious, why torch.empty works ?\ntorch.empty just creates a tensor with uninitialized values.\nTry to print it after the init and you will see, that the values are quite random.\nYou could also create it with torch.zeros or skip the initialization completely:\n<code class=\"lang-auto\">w1=torch.tensor([0.1], requires_grad=True)\nw2=torch.tensor([0.1], requires_grad=True)\nw3=torch.tensor([0.1], requires_grad=True)\nW=torch.cat((w1 * w2, w2 * w3, w3 * w1))\n<\/code>\nI tried the following, all of them are fine\nW=torch.empty(3, requires_grad=False)\nW=torch.zeros(3, requires_grad=False)\nW=torch.ones(3, requires_grad=False)\nW=torch.randn(3, requires_grad=False); W[0]=0.1; W[1]=0.1;W[2]=0.1\nW_numpy=numpy.array([0.1,0.1,0.1],dtype=\u2018float32\u2019); W=torch.from_numpy(W_numpy)\nW=torch.arange(0,3)\nW=torch.linspace(0,1,3)\nW=torch.logspace(-10, 10,3)\nW=torch.full((3,), 3.141592)\nW=torch.tensor([0.1, 0.1, 0.1])\nIf I use the option \u201crequires_grad=True\u201d with any of the above expressions, then I got\n\u201cRuntimeError: leaf variable has been moved into the graph interior\u201d\nThen I run:\nW=torch.tensor([0.1, 0.1, 0.1])\nW.requires_grad\nOut[17]: False\nW.is_leaf\nOut[18]: True\nW[0]=w1 * w2;  W[1]=w2 * w3;  W[2]=w3 * w1\nW.is_leaf\nOut[19]: False\nW=torch.tensor([0.1, 0.1, 0.1], requires_grad=True)\nW.requires_grad\nOut[20]: True\nW.is_leaf\nOut[21]: True\nW[0]=w1 * w2;  W[1]=w2 * w3;  W[2]=w3 * w1\nW.is_leaf\nOut[22]: False\nSo, what the error message really means?\nYou can read a nice explanation of leaf variables in this post with link \"https:\/\/discuss.pytorch.org\/t\/leaf-variable-was-used-in-an-inplace-operation\/308\/2?u=ptrblck\".\nUsually you have to avoid modifying a leaf variable with an in-place operation.\nI haven\u2019t seen your error message yet, but it seems it related to an in-place modification, because you are re-assigning the values of W in this line:\nW[0]=w1 * w2; W[1]=w2 * w3; W[2]=w3 * w1\n\n\n\n ptrblck:\n\nI haven\u2019t seen your error message yet, but it seems it related to an in-place modification, because you are re-assigning the values of W in this line:\n<code class=\"lang-auto\">\n<\/code>\n\n\nIs that an in-place modification though?\n<code class=\"lang-auto\">\nX=torch.randn((100,3))\nY=torch.randn((100))\n\nw1=torch.tensor(0.1, requires_grad=True)\nw2=torch.tensor(0.1, requires_grad=True)\nw3=torch.tensor(0.1, requires_grad=True)\n\nW=torch.tensor([0.1, 0.1, 0.1])\nprint(id(W[0]),id(W[1]),id(W[2]))\nW[0]=w1 * w2; W[1]=w2 * w3; W[2]=w3 * w1\nprint(id(W[0]),id(W[1]),id(W[2]))\n\n4778466184 4778466184 4778466184\n4778467120 4778467120 4778467120\n<\/code>\nThe addresses are different. In fact, I\u2019m not even sure why each of the elements of the tensor, W, don\u2019t have unique addresses, at least prior to being assigned values based on the w\u2019s. Once tensor elements are assigned values, particularly values based on tensors with requires_grad=True, it seems that they all are given the same address. Is that correct?\nI\u2019m not sure which id is shown if you access the tensor, so I would compare the ids of the complete tensor W which stay the same.\nI am using PyTorch 0.4.1\nI am trying to create a cost-sensitive Loss by modifying the code source of cross entropy loss.\nI wrote new Log_Softmax function as described below\ndef LogSoftmax(input,labels,ksi):\n# input = the output layer of my model (nn.Linear(N,nb_classes)\n# labels = target classes\n# ksi = cost sensitive matrix\nlayer = torch.empty(input.shape,requires_grad=True)\nfor j in range(input.shape[0]):\nfor i in range(input.shape[1]):\nlayer[j,i] = log(ksi[labels[i],i]*exp(input[j,i])) - log(sum([ksi[labels[i],k]*exp(input[j,k]) for k in range(len(ksi))]))\nreturn layer\ninput, labels and ski are tensors and have requires_grad set to True\nI got error message for loss.backward() during training phase.\nFor my function\u2019s return \u2018layer\u2019:\nIf requires_grad = True, I got the following error msg: RuntimeError: leaf variable has been moved into the graph interior.\nIf requires_grad = False, RuntimeError: element 0 of tensors does not require grad and does not have a grad_fn.\nThank you\nI think your input should require gradients as it\u2019s most likely a result of some other model operation.\nIf so, I assume you don\u2019t need to set required_grad=True for layer.\nCould you post some random inputs for all tensors, so that I could debug it?\nAlso, I\u2019m not completely sure how your code works, but maybe the nested loop might be avoided using matrix operations.\nThank you for your reply.\nMy input is in fact the output of my model ie the last fully connected layer (nn.Linear(\u2026,nb_classes)). It works well when using criterion = nn.CrossEntropyLoss(). The problem appears only when using the loss function I have implemented.\nIf I give my model a batch composed of 2 images for instance, I obtain the following :\ninput = tensor([[-0.4151,  0.0702, -0.4210,  0.4268,  0.0183,  0.1305, -0.5303, -0.1792,\n-0.6745, -0.0481, -0.2302],\n[-0.0338, -0.3070, -0.8314,  0.3735,  0.4717, -0.2299, -0.4984, -0.1337,\n-0.3467,  0.1117, -0.2985]], grad_fn=<ThAddmmBackward))\nlabels = tensor([0,10])\nksi = = torch.tensor([[1.,1.,1.,1.,1.,1.,1.,1.,1.,1.,1.],\n[1.,1.,1.,1.,1.,1.,1.,1.,1.,1.,1.],\n[1.,1.,1.,1.,1.,1.,1.,1.,1.,1.,1.],\n[1.,1.,1.,1.,1.,1.,1.,1.,1.,1.,1.],\n[1.,1.,1.,1.,1.,1.,1.,1.,1.,1.,1.],\n[1.,1.,1.,1.,1.,1.,1.,1.,1.,1.,1.],\n[1.,1.,1.,1.,1.,1.,1.,1.,1.,1.,1.],\n[1.,1.,1.,1.,1.,1.,1.,1.,1.,1.,1.],\n[1.,1.,1.,1.,1.,1.,1.,1.,1.,1.,1.],\n[1.,1.,1.,1.,1.,1.,1.,1.,1.,1.,1.],\n[1.,1.,1.,1.,1.,1.,1.,1.,1.,1.,1.]],requires_grad = True)\nAt the moment, my cost sensitive matrix is ones matrix because I just want to debug the code first.\nI have  slightly modified my LogSoftmax function:\ndef LogSoftmax(input,labels,ksi):\nlayer = torch.empty(input.shape)\nfor j in range(input.shape[0]):\nfor i in range(input.shape[1]):\nlayer[j,i] = log(ksi[labels[j],i]*exp(input[j,i])) - log(sum([ksi[labels[j],k]*exp(input[j,k]) for k in range(len(ksi))]))\nreturn layer\nI have resolved the problem this way\ndef LogSoftmax(input,labels,ksi):\nlayer = torch.autograd.Variable(torch.empty(input.shape), requires_grad = True)\nlayer1 = layer.clone()\nfor j in range(input.shape[0]):\nfor i in range(input.shape[1]):\nlayer1[j,i] = torch.log(ksi[labels[j],i]*torch.exp(input[j,i])) - torch.log(sum([ksi[labels[j],k]*torch.exp(input[j,k]) for k in range(len(ksi))]))\nreturn layer1\nHi Ptrblck,\nWould you please help me with that error regarding backward.() (\"RuntimeError: leaf variable has been moved into the graph interior\n\" )\n<code class=\"lang-auto\">      bbb=fake.squeeze(1)\n        bbb1=MASKGaussy.squeeze(1)\n        \n        zzz1=torch.zeros(bbb.shape,requires_grad=True)\n        zzz=torch.zeros(bbb1.shape,requires_grad=True)\n\n        for ii in range(bbb.shape[0]):\n            for ii1 in range(bbb.shape[1]):\n                for ii2 in range(bbb.shape[2]):\n                    if bbb[ii,ii1,ii2]>=0.99:\n                       zzz[ii,ii1,ii2]=1\n                    elif  bbb[ii,ii1,ii2]<0.99:\n                       zzz[ii,ii1,ii2]=0\n            \n        for ii in range(bbb1.shape[0]):\n            for ii1 in range(bbb1.shape[1]):\n                for ii2 in range(bbb1.shape[2]):\n                    if bbb1[ii,ii1,ii2]>=0.99:\n                       zzz1[ii,ii1,ii2]=1\n                    elif  bbb1[ii,ii1,ii2]<0.99:\n                        zzz1[ii,ii1,ii2]=0\n                        \n        L1=nn.MSELoss()(zzz,zzz1)\n        loss2=L1\n        loss2.backward()\n<\/code>"},{"x":"I have video data stored in a Numpy array each of shape (1, 16, 100, 100, 3). The array is huge and every time I load it, I get a memory error.\nIs there a way using Pytorch to load these videos in batches without actually loading the Numpy array so that I don\u2019t get the memory error.\nThanks.","y":"Assuming this numpy array is stored locally as an npy file, you could use np.load with the mmap_mode with link \"https:\/\/numpy.org\/doc\/stable\/reference\/generated\/numpy.load.html\", which would allow you to load sliced from the disc without reading the whole array into memory:\n\nmmap_mode {None, \u2018r+\u2019, \u2018r\u2019, \u2018w+\u2019, \u2018c\u2019}, optional\nIf not None, then memory-map the file, using the given mode (see numpy.memmap with link \"https:\/\/numpy.org\/doc\/stable\/reference\/generated\/numpy.memmap.html#numpy.memmap\" for a detailed description of the modes). A memory-mapped array is kept on disk. However, it can be accessed and sliced like any ndarray. Memory mapping is especially useful for accessing small fragments of large files without reading the entire file into memory.\n","z":"What kind of error are you getting?\nAn array with [16, 100, 100, 3] float32 values would use approx. 1.83MB, which is quite small.\nHi,\nThanks for replying. The array has 93000 samples. Therefore the shape of the array is (93000, 1, 16, 100, 100, 3). I want to load this dataset in batches so that I don\u2019t get a memory error. Please let me know if there is a way out. I will be grateful for your help.\nAssuming this numpy array is stored locally as an npy file, you could use np.load with the mmap_mode with link \"https:\/\/numpy.org\/doc\/stable\/reference\/generated\/numpy.load.html\", which would allow you to load sliced from the disc without reading the whole array into memory:\n\nmmap_mode {None, \u2018r+\u2019, \u2018r\u2019, \u2018w+\u2019, \u2018c\u2019}, optional\nIf not None, then memory-map the file, using the given mode (see numpy.memmap with link \"https:\/\/numpy.org\/doc\/stable\/reference\/generated\/numpy.memmap.html#numpy.memmap\" for a detailed description of the modes). A memory-mapped array is kept on disk. However, it can be accessed and sliced like any ndarray. Memory mapping is especially useful for accessing small fragments of large files without reading the entire file into memory.\n\nThank you very much. It worked."},{"x":"Hi!\nI have a decoder outputing a tensor (let\u2019s call it A) of size [batch_size, num_points, 3], basically a Point-Cloud in three-dimensional euclidean space. I\u2019d like to perform sorting on each of the generated PCs with respect to a specific axis (let\u2019s say z-axis) before feeding the now sorted (named B) [batch_size, num_points, 3] tensor to the next network layers.\nThe problem arise because I\u2019ve no idea on how to differentiably sort A in one step with torch.sort(). Differently, I tried also to reduce A to a new tensor of size [batch_size, num_points, 1], keeping only z-axis values, performing sort and then using the retrieved indices again on the A tensor. By doing so I\u2019ve noticed that while the values returned by torch.sort have requires_grad=True, the same is not true for indices. How this impact on general differentiability? Is there a better way to sort each of the generated PCs with respect to a single axis (I mean xyz here) without loosing differentiability?\nThank you!","y":"You can imagine sort() as doing two operations:\nindices = argsort(x,dim)\ny = gather(x,indices,dim) #x[indices] may work too\nargsort() defines non-trainable permutation\ngather() applies this permutation. It is a differentiable operation - backward pass permutes gradients back to match their original positions.\nNote that you can use indices to gather from other tensors, that basically performs sortByKey() operation. So what you did, sorting z and reordering bigger tensor is valid.","z":"You can imagine sort() as doing two operations:\nindices = argsort(x,dim)\ny = gather(x,indices,dim) #x[indices] may work too\nargsort() defines non-trainable permutation\ngather() applies this permutation. It is a differentiable operation - backward pass permutes gradients back to match their original positions.\nNote that you can use indices to gather from other tensors, that basically performs sortByKey() operation. So what you did, sorting z and reordering bigger tensor is valid.\nPerfect! So, since the effect of sort() in backward is just applying the same permutation to the gradients according to the new indices, the two operations (atomic\/one instruction sort and two steps sort) are perfectly equivalent. I should have checked the resulting gradient by myself!"},{"x":"My torch version is 1.5.1. I found a strange behavior of torch.exp():\n\nimport torch\nx = torch.ones(2, 1, requires_grad=True)\nx = torch.exp(x)\nx[0] = 0\nout = x.mean()\nout.backward()\n\nAfter running this code, I got a running error:\n\nRuntimeError: one of the variables needed for gradient computation has been modified by an inplace operation: [torch.FloatTensor [2, 1]], which is output 0 of ExpBackward, is at version 1; expected version 0 instead. Hint: enable anomaly detection to find the operation that failed to compute its gradient, with torch.autograd.set_detect_anomaly(True).\n\nHowever, if I replace torch.exp() with torch.log() or torch.sin() in the code, it works very well. Can anyone help me to find the solution? Thank you very much.\nEdit: One thing I forgot to mention. If I add x=x+0 before x[0]=0, the code works well too.","y":"So I covered some of that along with many \u201clittle mysteries\u201d in a course with link \"https:\/\/twitter.com\/ThomasViehmann\/status\/1282555652112621568\" recently (slides 28-35). From the summary slide:\nThere are two main situations where inplace operations cannot be used:\n\nWhen operating on leaf tensors, the leaf tensor would be moved into the graph, which would be bad.\nWhen the operation before the inplace wants to have its result to compute the\nbackward. Whether this is the case is not easy to tell from the outside,\nunfortunately.\n\nYou are in the second case. What happens is that exp wants its output to compute the backward (which multiplies by the output) while e.g. log wants its input (because the backward divides by the input). This is the \u201cnot always easy to tell from the outside part\u201d.\nIf you do x = x + 0 you are creating a new tensor (which happens to be numerically identical to the old x) and then assign that to x and modify it inplace, but exp finds its output unmodified. This is, btw. the purpose of the ctx.save_for_backward(...), ctx.saved_tensors exrcise in autograd.Functions.\nPDF of the slides (and Jupyter Notebook to run the code) are on github with link \"https:\/\/github.com\/t-vi\/acdl2020\/\", but apparently not enough interest for video.\nOf course, the authoritative in-depth reference is \u2019s  and my imaginary book from which I made the slides for the talk.\nBest regards\nThomas","z":"So I covered some of that along with many \u201clittle mysteries\u201d in a course with link \"https:\/\/twitter.com\/ThomasViehmann\/status\/1282555652112621568\" recently (slides 28-35). From the summary slide:\nThere are two main situations where inplace operations cannot be used:\n\nWhen operating on leaf tensors, the leaf tensor would be moved into the graph, which would be bad.\nWhen the operation before the inplace wants to have its result to compute the\nbackward. Whether this is the case is not easy to tell from the outside,\nunfortunately.\n\nYou are in the second case. What happens is that exp wants its output to compute the backward (which multiplies by the output) while e.g. log wants its input (because the backward divides by the input). This is the \u201cnot always easy to tell from the outside part\u201d.\nIf you do x = x + 0 you are creating a new tensor (which happens to be numerically identical to the old x) and then assign that to x and modify it inplace, but exp finds its output unmodified. This is, btw. the purpose of the ctx.save_for_backward(...), ctx.saved_tensors exrcise in autograd.Functions.\nPDF of the slides (and Jupyter Notebook to run the code) are on github with link \"https:\/\/github.com\/t-vi\/acdl2020\/\", but apparently not enough interest for video.\nOf course, the authoritative in-depth reference is \u2019s  and my imaginary book from which I made the slides for the talk.\nBest regards\nThomas\nHello Thomas. Thanks for your detailed response. It is really helpful to me.\n\n\n\n tom:\n\nPDF of the slides (and Jupyter Notebook to run the code) are on github with link \"https:\/\/github.com\/t-vi\/acdl2020\/\", but apparently not enough interest for video.\n\n\n+1 for the video if possible:)"},{"x":"What is the best way to predict a categorical variable, and then embed it, as input to another net?\nMy instances are tabular, a mix of categorical and continuous variables. I currently have a siamese net (net1) that uses the instances as input. The categorical variables are integer indices used before an nn.Embedding layer. Once net1 is trained, it is fixed.\nNow, I want net0 to predict (output) these tabular instances and feed them as input to the above net, and backprop the loss through the entire network (net0 => net1), even tho net1 will be fixed and not modified.\n\nWhat seems sensible to me is net0 predicts the one-hot (softmax) representation of the category, and then figure out to change net1 so that nn.Embedding uses a one-hot, not integer index, as the input. However, I don\u2019t believe nn.Embedding can accept a one-hot input (How to make nn.Embedding support one-hot encoded vector input? with link \"https:\/\/discuss.pytorch.org\/t\/how-to-make-nn-embedding-support-one-hot-encoded-vector-input\/50688\")\nnet0 predicts the one-hot (softmax) representation of the category, takes the argmax to get the integer index, and net1 looks up the nn.Embedding using the index. However, I believe that argmax is non-differentiable, which means I cannot pass losses back from net1 to net0 Differentiable argmax with link \"https:\/\/discuss.pytorch.org\/t\/differentiable-argmax\/33020\"\n\nI could have net0 directly predict the embedding of net1, and in postprogressing use kNN to find the nearest category. I don\u2019t love this because a) it requires the net1 embeddings to be fixed and b) it seems a little inelegant.\n\nWhat is my best option here to predict a categorical variable and then embed it?","y":"Yeah. 4. In other words, you take the probability vector for the categories and feed it into a Linear layer that has the embedding as weight.","z":"One approach that is a little cheesy, and perhaps a little slower, but will be very straightforward to implement and understand is:\n\nConvert the categorical variable to a one-hot encoding. Instead of an nn.Embedding layer, have a 1xE (E=embedding size) weight matrix for each dimension in the one-hot. Sum all the 1xE embeddings for the category and maybe squash them?\n\nIn this case, if it is truly one-hot, then only one embedding weight matrix is active. If the input is a probability distribution over different categories, their embeddings are mixed together.\nYeah. 4. In other words, you take the probability vector for the categories and feed it into a Linear layer that has the embedding as weight.\n what is the best way to code this, if I have many categorical variables?\nIf I split the categoricals into an array of one-hot tensors, then it is a little easier to construct the net. However, I am concerned this would not be performant.\nIf I have all the values together, then I might need to have the dataset provide a list of ranges to slice. It\u2019s a little trickier, but is that much faster?\nAlso, is it correct that this will be faster if the categorical one-hot inputs are sparse tensors? I have 10-50 categorical variables, each has about 10 categories, and about 50 more continuous variables. I suspect that with such a low number of categories, that dense is faster.\nSo if I understand this correctly, you have several variables and each of these has a moderate number of categories.\n\nIf they\u2019re the same categories (i.e. you have the same embedding), it is more efficient to batch them.\nIf you have distinct embeddings, combining them would create a block-diagonal matrix with many zeros. That usually isn\u2019t as efficient to process unless you use block-sparse variables, so I\u2019d avoid that.\n\nIf, on the other hand, you have so many categories that using a linear layer is impractical, it is common to do negative sampling: If you first nn\u2019s output is a vector probs, you can grab the K_pos largest indices (i_pos = topk(probs, K_pos, dim=-1).indices or somesuch) and K_neg random indices (i_neg = randint((batch, K_neg)); i_all = torch.cat((i_pos, i_neg), dim=-1)). Then you take probs[:, i_all] @ emb.weights.t()[i_all] + emb.bias() as embeddings.\nThis is not similar to negative sampling in word2vec (I recommend Richard Socher\u2019s\/Chris Manning\u2019s lectures on them if you want a very detailed account), except that there the vectors are leaves and the network following them is shallow.\nBest regards\nThomas\n Thank you for the detailed feedback. I am aware of the negative sampling technique, it was originally used by Collobert + Weston (2008) in their work that was the first fast neural embedding method. (I re-implemented their embedding method and published work on , I changed my username in case you are curious about my work on word embeddings.)\nI am not aware of how this would work in the context of a siamese network. Regardless, I have few categories (roughly 5-30 per category, median 10) per embedding. I have about 10 categorical variables. So this is not my issue.\nYou are right that this is slow.\nI did the most naive thing which was decompose the categoricals into dense vectors, each with their own embedding layer with dimension 8:\nimage1574\u00d71020 133 KB with link \"https:\/\/discuss.pytorch.org\/uploads\/default\/original\/3X\/5\/1\/51cb7fd3394b56a0abb270fa5a662fa0f5395dd7.png\"\n<s>This was about 10x slower with 1000 examples, as I would expect. But with 200K examples, it seems like waaaaay slower, like orders of magnitude, which I did not expect at all. Do you have any idea why? I am currently training on CPU while prototyping, FYI.<\/s>\n[edit: there were some floats in my input categories which exploded the number of classes]\nI do understand that rewriting this network to take sparse inputs for the categories would be faster. I want to avoid this because:\nIf I switch to sparse inputs, then later to pass the output of a softmax into this siamese network for prediction, or moreover to train a joint network with softmax feeding the siamese, I need dense inputs.\np.s. greetings from Berlin\nOh, right. You\u2019re in the first case, so negative sampling is not applicable, and you already know all about it.\nWould a large batch matmul help? You would have the difficulty amending the shorter vectors, but I think it might work.\nBest regards\nThomas\nAre you suggesting that I transform all the categoricals into a one-hot that is of fixed length?\nThank you for the advice by the way. It has been really helpful"},{"x":"For my task, I do not need to compute gradients. I am simply replacing nn.L1Loss with a numpy function (corrcoef) in my loss evaluation but I get the following error:\n\nRuntimeError: Can\u2019t call numpy() on Variable that requires grad. Use var.detach().numpy() instead.\n\nI couldn\u2019t figure out how exactly I should detach the graph (I tried torch.Tensor.detach(np.corrcoef(x, y)) but I still get the same error. I eventually wrapped everything using with torch.no_grad as follow:\n<code class=\"lang-auto\">with torch.no_grad():\n    predFeats = self.forward(x)\n    targetFeats = self.forward(target)\n    loss = torch.from_numpy(np.corrcoef(predFeats.cpu().numpy().astype(np.float32), targetFeats.cpu().numpy().astype(np.float32))[1][1])\n<\/code>\nBut this time I get the following error:\n\nTypeError: expected np.ndarray (got numpy.float64)\n\nI wonder, what am I doing wrong?","y":" You are right.  I got the answer here with link \"https:\/\/stackoverflow.com\/questions\/54165651\/how-to-use-a-numpy-function-as-the-loss-function-in-pytorch-and-avoid-getting-er\"","z":"To my understand:\n<code class=\"lang-auto\">np.corrcoef(x,y)\n<\/code>\nwill return a ndarray type value.\n<code class=\"lang-auto\">torch.from_numpy(ndarray) \u2192 Tensor\n<\/code>\nThis function requires a ndarray type\nCan you print out the value of these codes below:\n<code class=\"lang-auto\">np.corrcoef(predFeats.cpu().numpy().astype(np.float32), targetFeats.cpu().numpy().astype(np.float32))[1][1])\n<\/code>\nto check out where is the problem.\nBecause your error shows that this function returns a numpy.float64 rather than a numpy array\n You are right.  I got the answer here with link \"https:\/\/stackoverflow.com\/questions\/54165651\/how-to-use-a-numpy-function-as-the-loss-function-in-pytorch-and-avoid-getting-er\"\nThanks for the link, but isn\u2019t this breaking the graph?  I ask you because I read this Calculating loss with numpy function with link \"https:\/\/discuss.pytorch.org\/t\/calculating-loss-with-numpy-function\/28796\"."},{"x":"This question might seem to be a bit theoretical and not suited as it is not directly linked to pytorch, but please bear with me as I have been asking this about and haven\u2019t got a reply from anywhere \nWe know that during backpropagation, the weight update of the last layer is calculated by calculating the derivative of the loss with respect to the last layer\u2019s weights and plugging in the differential values of the activation function and the inputs to the last layers. Now, when the (last-1) layer\u2019s derivatives are calculated with the help of the derivatives of the inputs to the last layer ( and the output of the (last-1) layer), then are previous weight values of the last layer used to fill in the equation or the updated weight values used? In other words, are the weights of the last layer updated as soon as the gradients are calculated, or does it wait till calculating all the gradients of all the layers and then updates them simultaneously when we call optimizer.step() ?\nFor a more detailed view of the problem , please see\u2013\nhttps:\/\/drive.google.com\/file\/d\/11RPvQCbMm6yLmQwEcZ_5uOg37F8mdOvr\/view?usp=sharing","y":"Hi,\nIn pytorch, the weights are only updated when you do optimizer.step() which happens after the .backward() pass is completed. So no.\nIn general, if you rewrite all your formulas to add a t parameter to your weights to differentiate the version before the update and the one after. You will see that the gradient only ever depends on the original value, and never on the value after update (t+1). So if you compute the gradients, it should never happen in general, even if you fold the optimizer step into the backward pass.","z":"Hi,\nIn pytorch, the weights are only updated when you do optimizer.step() which happens after the .backward() pass is completed. So no.\nIn general, if you rewrite all your formulas to add a t parameter to your weights to differentiate the version before the update and the one after. You will see that the gradient only ever depends on the original value, and never on the value after update (t+1). So if you compute the gradients, it should never happen in general, even if you fold the optimizer step into the backward pass.\nThanks a lot! That cleared things up \nAnother small query if you will\u2026 in my Equation(2), I have the term as this:\n\nHowever, In \u201cEfficient Backprop\u201d by Yann Le Cun, (http:\/\/yann.lecun.com\/exdb\/publis\/pdf\/lecun-98b.pdf - Page 4 Equation 9), he has a transpose on the Wn term\u2026and I can\u2019t seem to understand why a transpose of the weights is being done\u2026\nThis is most likely due to the differentiation rules depending on the convention you use.\nYou can check this by making sure the different matrix dimensions match.\nOh\u2026 Okay thanks \nHi,\nThere is something else in this context which has been bothering me\u2013\nSuppose we have an input layer X0, weight W1, and output X1, then\n\n\nX1.gradient is a scalar formed by summing up the gradients of all units of X1\n\n\nW1 is a weight matrix containing internal weights w(i,j) which are updated based on :\n* The scalar gradient of X1\n* And the vector X0\n\n\nThis however, brings me up to the conclusion that for each w(i,j) to be updated individually, we should have the scalar X1 and each unit of the X0 matrix available for the specific i and j \u2013 Which is not plausible as the X0 matrix has a different shape as compared to the W1 matrix. How exactly are individual weights of the W1 matrix computed?\nHi,\nWhere does \u201cX1.gradient is a *scalar formed by summing up the gradients of all units of X1\u201d comes from?\nThe gradient for W is the outer product between the gradient of X1 (which is the same size as X1, not a scalar) and X0 which is is input.\nX1 is the output vector\u2013and here\u2019s what i found from that efficient backprop paper:\nScreenshot from 2020-07-27 19-32-03941\u00d7381 54.9 KB with link \"https:\/\/discuss.pytorch.org\/uploads\/default\/original\/3X\/3\/f\/3f501450b327b24e22c76e44672c828f1328dd8a.png\"\nHere The scalar error at the node is not the gradient of the error with respect to X1?\nPage 8 \u2014 9th line of this with link \"http:\/\/yann.lecun.com\/exdb\/publis\/pdf\/lecun-98b.pdf\"\nI think here, the \u201cnode\u201d is a single element of X1. And so its gradient is just a single number.\nAh, but then if \u1e9f is not a single number, that brings down the whole concept of \u2018Zigzagging\u2019, right?\u2013As the weight vectors are actually not changing together in \u2018fixed directions\u2019. Their directions are unique as \u1e9f is a vector. But then, can you give me a pointer to an article or a paper which explains why normalizing is necessary without using this \u2018Zigzagging\u2019 reference  as most of what i have come across uses this. Thanks \nI don\u2019t think a proof exist for neural nets in general.\nBut this considers a simpler case with a single weight. And I guess they claim that it extends similarly when you have more layers\/weights?"},{"x":"I\u2019m using gradient checkpointing (1.7.0.dev20200709) and I\u2019m observing a behavior that I don\u2019t understand.\nBasically, I have a code snippet in my forward that goes\nfoo = bar * baz\nwhere bar is requires_grad=False and baz is requires_grad=True.\nI call this code with the same inputs first outside, then inside a checkpoint() call.\nIn the first case, foo ends up being requires_grad=True which is what I would expect.\nIn the second, foo ends up being requires_grad=False.\nIs this expected? Will this not cause failure of the model to learn? Have I discovered a bug, or is this checkpoint magic that will all turn out fine in the end?","y":"Hi,\nWhen you do checkpointing, the forward is called twice.\nOnce during the forward without tracking gradients (and so requires_grad=False) and once, during the backward, with tracking enabled to recover all the buffers.","z":"Hi,\nWhen you do checkpointing, the forward is called twice.\nOnce during the forward without tracking gradients (and so requires_grad=False) and once, during the backward, with tracking enabled to recover all the buffers.\nCheckpoint magic, got it "},{"x":"Hi!\nI have and autoencoder, and between the coder and decoder I transform the data using torch.sign . When I do it, the backpropagation of the gradient stops in that point.\nIf I replace torch.sign by torch.sigmoid, then I don\u2019t have that problem and the backpropagation goes back to the beginning.\nDo I have to do something different with torch.sign?","y":"Hi,\nthe sign function is not differentiable (or if you look at it in a differentiable manner, the gradient is 0 almost everywhere). So it is expected that you won\u2019t be able to get gradients through it.","z":"Hi,\nthe sign function is not differentiable (or if you look at it in a differentiable manner, the gradient is 0 almost everywhere). So it is expected that you won\u2019t be able to get gradients through it.\nThanks. Thinking it over again it makes sense.\nI see the sign function as a non-linear function and small variations of the input gives the same output, thus the derivative is like the derivative of a constant, 0 (except values near 0)\nA linear approximation of the function would be a sigmoid with parameter beta \\frac{1}{1+e^{-\\beta x}}. Which has derivative \\frac{\\beta \\exp{-\\betax}{(1+exp{-\\beta*x})^2}. If I want to create a new function of this sigmoid with beta, the parameter received in the backward function, grad_output, is the one that has to be evaluated using the formula of the derivative for each element of grad_ouptut?\nHi,\nNote that we have hardsignmoid (https:\/\/pytorch.org\/docs\/master\/nn.functional.html#torch.nn.functional.hardsigmoid) if you\u2019re using the nightly builds or similar functions like hardtanh.\nIf you want to use your custom function:\n\nIf you want the \u201ctrue\u201d gradient to be used, then just implement the function you want and autograd will get the gradient for you.\nIf you want the backward to compute something else than the gradient of your function, you can see this doc with link \"https:\/\/pytorch.org\/docs\/stable\/notes\/extending.html\" that explains how to do that with a custom autograd Function where you will have to specify the backward to use.\n\nThanks again for the answer and the references to the doc.\nI think that to build a custom function is far from my knowledge of pytorch.\nI think that I will multiply the data by \\beta before doing the sigmoid function.\nMost grateful"},{"x":"Hello,\nMy logits are of dimention: torch.Size([64, 26, 7900])\nMy target is of dimension: torch.Size([64, 26])\nIt is so because the output is from LSTM for some NLP task. 7900 is the size of the vocabulary.\nHow do I formulate the loss for this scenario?\n<code class=\"lang-auto\">loss = nn.CrossEntropyLoss()\ninput = torch.randn(64, 26, 7900, requires_grad=True)\ntarget = torch.empty(64,26, dtype=torch.long).random_(5)\noutput = loss(input, target)\noutput.backward()\n<\/code>\nthrows the error:\n\nValueError: Expected target size (64, 7900), got torch.Size([64, 26])\n","y":"As Chetan explained, the model output tensor should contain the class indices in dim1 and additional dimensions afterwards.\nGenerally, nn.CrossEntropyLoss expects these shapes:\n\noutput: [batch_size, nb_classes, *]\n\ntarget [batch_size, *]\n\n","z":"The size of logits must be [64,7900,26].\nAs Chetan explained, the model output tensor should contain the class indices in dim1 and additional dimensions afterwards.\nGenerally, nn.CrossEntropyLoss expects these shapes:\n\noutput: [batch_size, nb_classes, *]\n\ntarget [batch_size, *]\n\n"},{"x":"How do I find out what the version numbers of a Module\u2019s Parameters are?\nI know that pytorch keeps track of this information, because it will raise an exception if you try to backpropagate through a graph that was based on Parameters that have been updated since.\nI need this because I have a pretty complex and unusual computational graph that is dynamically generated, and I would like to perform sanity checks on the version numbers.","y":"Hi,\nYou can access these with the read-only ._version attribute on Tensors.\nNote though that this is an internal implementation. And the absolute values might change (though equal or not equal will of course still be true).","z":"Hi,\nYou can access these with the read-only ._version attribute on Tensors.\nNote though that this is an internal implementation. And the absolute values might change (though equal or not equal will of course still be true).\nThanks!\nTwo follow-up questions:\n\nI want to store the version numbers of all Module parameters at certain points, and then later check if any of these checkpoints used Parameters that are now outdated. Will this work reliably?\nCan I assume that this gets serialized correctly when using state_dict(), and will continue to work after reloading? Or can this cause the version numbers to jump and maybe even become smaller?\n\nHey,\nYes checking if they changed or not will always work reliably.\nSince the deserialized Tensor doesn\u2019t share data with the original one anymore. There will be no link between their version counters. So I would expect that it gets reset to 0 on loading."},{"x":"Hi folks,\nThere is a problem that has bothered me for quite a long time. Assume we are minimizing a loss function  parameterized by , on samples  using SGD, where M is the mini-batch size. Since the PyTorch autograd can only be implicitly created for scalar outputs, I am wondering if there is any efficient way to compute the gradient for each sample, i.e., , without setting the batch size equals to 1 and compute in a for loop (which is too slow)?\nThank you for your help!","y":"If you just have conv\/linear layers, you could use this \u2013 https:\/\/github.com\/cybertronai\/autograd-hacks#per-example-gradients","z":"loss.backward(gradient=torch.ones_like(loss))\nThe shape of loss is (B, )\uff0cwhich is the batch size\nHi Zhaomang,\nThanks so much for your reply! If the loss is set to be a non-scaler, i.e., the sample loss:\n<code class=\"lang-auto\">loss = F.cross_entropy(predictions, results, reduction='none')\nloss.backward(gradient=torch.ones_like(loss))\n<\/code>\nI\u2019m still confused about how to get the gradient on each sample, if I run the following code:\n<code class=\"lang-auto\">for params in model.parameters(): \n    print(params.grad)\n<\/code>\nI expect to get B times the number of tensors returned in the mini-batch case, since there should be an individual result for each sample.\nIn addition, it would be very helpful if you could point me to the document of Tensor.backward() function (if any), so that I can learn more on this. Thanks again!\nIf you just have conv\/linear layers, you could use this \u2013 https:\/\/github.com\/cybertronai\/autograd-hacks#per-example-gradients\nFor the params.grad, it is already the accumulated gradient of B samples.\nBut if you need to obtain the gradient on each sample, you can set\nX.retain_grad() before backward. And then through X.grad to get the gradient of loss on each sample.\nAre you going to extend this to other popular layer types, e.g. ReLu and batchnorm? This would make it useful for most widely used architectures such as Resnets\nIt should work for those architectures as well. What\u2019s missing is support for other layers with trainable parameters, (ie the multiheadattention layer)\nNot with batch norm. If I understood your implementation correctly, layers with batch normalization are just skipped:\n<code class=\"lang-auto\">_supported_layers = ['Linear', 'Conv2d']\n\nif layer_type not in _supported_layers:\n    continue\n<\/code>\nWhich means that for batch normalization layer (which is trainable), grad1 will not be created.\nAh right, the gamma, beta parameters. It seems feasible to extend computation explained here to per-example computation  https:\/\/kratzert.github.io\/2016\/02\/12\/understanding-the-gradient-flow-through-the-batch-normalization-layer.html\nCan the current version of autograd-hacks still be used if I apply other activation functions such as Sigmoid \/ tanh \/ ReLU? Thanks.\nYes, any activation function that works with PyTorch autograd should also work here\nThanks for per-example-gradients function!\nCan I use this function by only importing torch?\nI mean will per-example-gradients be an official function of Pytorch in the future?"},{"x":"Hi, I am trying to implement maxpool fonction from scatch (for fun) and use backward() on it.\nIn my implementation below, the output Y of maxpool is correct (I verified it).\nBut the gradient of the input at a zero tensor, which is wrong.\ncode :\n<code class=\"lang-auto\">import torch\ndef maxpool_fp(X):\n    pool_dim = 2\n    pool_stripe = 2\n    bs, cx, dx, _ = list(X.size())  # batch size ; nb of channel of X ;  dimension of X\n    dy = int(X.shape[2] \/ pool_stripe)  # dimension of Y\n\n    Y = X[:, :, :dy, :dy] * 1  # *1 to avoid: RuntimeError: leaf variable has been moved into the graph interior\n    for yn in range(bs):\n        for yc, x in enumerate(X[yn]):\n            for yh, h in enumerate(range(0, dx, pool_stripe)):\n                for yw, w in enumerate(range(0, dx, pool_stripe)):\n                    Y[yn, yc, yh, yw] = torch.max(x[h:h + pool_dim, w:w + pool_dim]).item()\n    return Y\n\n\nX = torch.randn(2, 2, 8, 8, requires_grad=True)\n\nY = maxpool_fp(X)\nS = torch.sum(Y)\nS.backward()\n\nprint(\"S =\", S) # ==> Correct\nprint(\"X.grad\\n\", X.grad) # ==>  zero tensor !!!!!!!!!!\n<\/code>","y":"Hi,\nThe .item() that you use converts the result of your max into a python number. And we can\u2019t track gradient for it. So it is ignored.\nYou will need to remove this .item() to get proper gradients.","z":"Hi,\nThe .item() that you use converts the result of your max into a python number. And we can\u2019t track gradient for it. So it is ignored.\nYou will need to remove this .item() to get proper gradients.\nGreat. Working now. Thanks a lot\nBut in order to keep track of the gradient I did this :\n<code class=\"lang-auto\">dy = int(X.shape[2] \/ pool_stripe)  # dimension of Y\nY = X[:, :, :dy, :dy] * 1\n<\/code>\nCan you see a more elegant way to do so? Or does it look fine for you ?\nWhy do you need the *1 here?\nCan you share the full code sample maybe that will be clearer \nBefore removing item() the *1 was there avoid the Error:\n\nRuntimeError: leaf variable has been moved into the graph interior\n\nWhen executing the first :\n<code class=\"lang-auto\">\nY[yn, yc, yh, yw] = torch.max(x[h:h + pool_dim, w:w + pool_dim]).item()\n\n<\/code>\nX is_leaf attribute changed from True to False. I put the *1 to set the is_leaf attribute to False in the first place in order to avoid the change.\nAfter removing item(), the *1 is still needed to avoid another Error:\n\nRuntimeError: one of the variables needed for gradient computation has been modified by an inplace operation: [torch.FloatTensor [2, 2]], which is output 0 of SliceBackward, is at version 64; expected version 63 instead. Hint: enable anomaly detection to find the operation that failed to compute its gradient, with torch.autograd.set_detect_anomaly(True).\n\nwhich I can\u2019t really explain\nYou can copy\/past the code below and remove the *1 if you want to reproduce the RuntimeError.  But please note that my problem was fixed by removing the .item(). I put this maxpool() function along with a hardcoded conv2d in a  CNN model, and it works on MNIST.\n<code class=\"lang-auto\">\nimport torch\n\ndef maxpool_fp(X):\n    pool_dim = 2\n    pool_stripe = 2\n    bs, cx, dx, _ = list(X.size())  # batch size ; nb of channel of X ;  dimension of X\n    dy = int(X.shape[2] \/ pool_stripe)  # dimension of Y\n\n    Y = X[:, :, :dy, :dy] * 1  # <=============\n    for yn in range(bs):\n        for yc, x in enumerate(X[yn]):\n            for yh, h in enumerate(range(0, dx, pool_stripe)):\n                for yw, w in enumerate(range(0, dx, pool_stripe)):\n                    Y[yn, yc, yh, yw] = torch.max(x[h:h + pool_dim, w:w + pool_dim])\n    return Y\n\n\nX = torch.randn(2, 2, 8, 8, requires_grad=True)\n\nY = maxpool_fp(X)\nS = torch.sum(Y)\nS.backward()\n\nprint(\"S =\", S)\nprint(\"X.grad\\n\", X.grad)\n<\/code>\nHo right!\nYes, you can\u2019t modify X inplace because it\u2019s a leaf. And when you do Y = X[:, :, :dy, :dy], Y is actually a view into X. And so when you modify Y inplace by doing Y[yn, yc, yh, yw] = foo, you also modify X leading to the error.\nThe right way to fix that is to use .clone() to ensure you get new memory that is not a view of X: Y = X[:, :, :dy, :dy].clone() (should be slightly faster than the *1 you use)."},{"x":"I think I found a pytorch bug but I\u2019m not entirely certain.\nBefore I\u2019m writing an issue I\u2019ll post it here.\nI have uploaded the sample code to google colab and the output with the error is visible in the output field. colab link here https:\/\/colab.research.google.com\/drive\/1UC7mL2FS4Gz5HIQuSeQaSnWFynQmIHzn#scrollTo=V4MjwHAljuTN&amp;uniqifier=2\nDont forget to activate the gpu in notebook settings.\nI am generating 4 RGB random images and put them to my gpu.\nimages = torch.rand(1,3,4,widht,height)\nimages = images.cuda() # this causes the error\nimages = images.requires_grad_(True)\n\nafter that I feed the images into a Maxpool3d and use a dummy loss so I can calculate\nthe gradients of my input images.\nx = nn.MaxPool3d(kernel_size=(1, 2, 2), stride=(1, 2, 2), padding=(0, 0, 0))(images)\ncriterion = nn.MSELoss()\nlabel = torch.ones_like(x)\nloss = criterion(x, label)\nloss.backward()\n\nIf I now analyze the gradients of the images\n grads_images = images.grad\n\nI noticed that about a third of the gradients are very close to zero although there is no\nreason for that.\nTo make this problem clear I am visualizing the gradients.\nI take only one image, use the absolute and scale them between 0 and 1. Then I add\nthe gradients of the red, green and blue channel together so i get a\nnumpy array with size [widht, height]\nThe values in this array can now be visualized with a colormap of plt.\n\nYou can see the right fifth of the gradients are ~0\nBut If I change one line\n images = torch.rand(1,3,4,widht,height) \n images.cuda() # this fixes the gradient error\n\nThe gradients are no longer ~0 in the right fifht of the image.\n(I\u2019m a new user so I can\u2019t paste more than one image so the new image is here https:\/\/imgur.com\/a\/ruuQQr4 and also in the colab)\nIf you don\u2019t want to use colab, the full code is here:\n<code class=\"lang-auto\">import torch\nimport torch.nn as nn\nimport matplotlib.pyplot as plt\nimport numpy as np\n%matplotlib inline\n\"\"\"\nI create 4 random images with batchsize 1 and channelsize 3 (rgb)\nThen I use a 3d Maxpool\nThen I want to visualize the gradients of our images with a target image\nthat consists of only ones.\n\nThe gradients dissapear if I overwrite my input images with images.cuda()\n\"\"\"\nheight = 100 # width of our image\nwidth = 150 # height of our image\nimages = torch.rand(1,3,4,height,width) # create 4 images with this height and width\nimages = images.cuda() # This creates the error\n\"\"\"IF YOU REPLACE THE ABOVE LINE WITH images.cuda() IT WORKS AS INTENDED\"\"\"\nimages = images.requires_grad_(True)\n\npooled_imgs = nn.MaxPool3d(kernel_size=(1, 2, 2), stride=(1, 2, 2), padding=(0, 0, 0))(images)\n\n# create a dummy target (same size as output with ones) and calculate the mse\ncriterion = nn.MSELoss()\nlabel = torch.ones_like(pooled_imgs)\nloss = criterion(pooled_imgs, label)\nloss.backward()\n\n# scale gradients between 0 and 1\npooled_absolute = torch.abs(images.grad)\nmax_grad_value = torch.max(pooled_absolute)\nscaled_grads = pooled_absolute \/ max_grad_value # scale grads between 0 and 1\nscaled_grads = scaled_grads.cpu().numpy()\n\n# scaled grads has shape [Batchsize, Channels(RGB),amnt_images, width, height]\ngrad_pic = scaled_grads[0, :,0, :, :] # use batch 0 and the first image in our depth\n# grad pic has no shape [3,height,width]\n\ngrad_pic = np.transpose(grad_pic, (1, 2, 0)) # shape now [height, width, 3]\n# I want to add all gradients of the 3 channels (red, green blue)\n# so I can use a plt colormap\ngrad_one_channel = np.ones([height, width])\ngrad_one_channel[:,:] = grad_pic[:,:,0]+grad_pic[:,:,1]+grad_pic[:,:,2]\n\nplt.imshow(grad_one_channel, cmap='viridis')\n<\/code>","y":"Hi,\nYes there were some maxpool fixes in 1.6. The one you linked looks like the one.\nHopefully you can just use 1.6 without issues now ","z":"Hi,\nThanks for the colab link!\nRunning with a brand new env there, I get the proper result for each cell though. Can you try again on your side?\nAlso image.cuda() does nothing! You can remove this line and it will have the exact same behavior. (.cuda() cannot be done inplace).\nIt seems that colab switched to torch 1.6 at the time between my question and your reply.\nAt first glance I am also no longer able to reproduce the error with torch 1.6 in the colab.\nI am still able to reproduce the bug with torch 1.4 with copy pasting the first cell to my local machine (deleting %matplotlib inline and adding as last line plt.show())\nI don\u2019t have torch 1.5 installed on my local machine but the code worked yesterday in google colab which was using torch 1.5.\nI can\u2019t use\n\n!pip install torch==1.5.0 torchvision==0.6.0\n\nin colab because torch then complains about\n\nThe NVIDIA driver on your system is too old (found version 10010). Please update your GPU driver by downloading and installing a new version from the URL: http:\/\/www.nvidia.com\/Download\/index.aspx Alternatively, go to: https:\/\/pytorch.org with link \"https:\/\/pytorch.org\/\" to install a PyTorch version that has been compiled with your version of the CUDA driver.\n\nAre you aware of any fixes in torch 1.6 that might have fixed this error?\nEDIT I think I found it myself:\nIn the torch 1.6 bugfixes there is this line\nand I think this fits perfectly for my error\n\nnn.MaxPool3d: fixed incorrect CUDA backward results for non-square output (#36820)\n\n\n\ngithub.com\/pytorch\/pytorch with link \"https:\/\/github.com\/pytorch\/pytorch\/pull\/36820\"\n\n\n\n\n\n\n\n\nFix MaxPool3d CUDA backward incorrect results for non-square output with link \"https:\/\/github.com\/pytorch\/pytorch\/pull\/36820\"\n\n\npytorch:master \u2190 linziyi96:master\n\n\n\n        opened 07:02PM - 17 Apr 20 UTC\n\n\nlinziyi96 with link \"https:\/\/github.com\/linziyi96\"\n\n\n+11\n-1 with link \"https:\/\/github.com\/pytorch\/pytorch\/pull\/36820\/files\"\n\n\n\n\n\n\n\n\n\nHi,\nYes there were some maxpool fixes in 1.6. The one you linked looks like the one.\nHopefully you can just use 1.6 without issues now "},{"x":"Hi All,\nI have been reading through autograd recently.\nWhich operations I should be using in order for gradients to be correctly calculated by autograd?\nThanks!","y":"Hi,\nYou should be using torch implementation of any function you use.\nAll pytorch function will work. Or raise an error if you\u2019re asking for things that are not supported\/non-differentiable.","z":"Hi,\nYou should be using torch implementation of any function you use.\nAll pytorch function will work. Or raise an error if you\u2019re asking for things that are not supported\/non-differentiable.\nThanks, so if I use an operation or function that is not support by autograd, an error is raised?\nYes.\nAll differentiable functions should be implemented though. Only a few linear algebra functions that could be differentiated are not.\nOf course non-differentiable function won\u2019t work.\nAnd the functions like .detach() and torch.no_grad() are built to do things outside of the autograd so they are special cases\n.\nThanks, just to clarify. Using autograd supported operations and functions allows gradients to be correctly calculated?\nYes it will compute the correct gradients if you don\u2019t get any error \nAnd if gradients cannot be correctly computed, I get an error?\nIf the function is not differentiable, you\u2019ll get an error.\nIf gradients cannot be computed (usually due to inplace ops), yes you will get an error.\nIn general, if a gradient is computed, it is correct."},{"x":"I am trying to use a pre-trained VGG16 model to classify CIFAR10 on pyTorch. The model was originally trained on ImageNet.\nHere is how I imported and modified the model:\nfrom torchvision import models\n\nmodel = models.vgg16(pretrained=True).cuda()\nmodel.classifier[6].out_features = 10 \n\nand this is the summary of the model\nprint(model)\n\nVGG(\n  (features): Sequential(\n    (0): Conv2d(3, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n    (1): ReLU(inplace=True)\n    (2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n    (3): ReLU(inplace=True)\n    (4): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n    (5): Conv2d(64, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n    (6): ReLU(inplace=True)\n    (7): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n    (8): ReLU(inplace=True)\n    (9): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n    (10): Conv2d(128, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n    (11): ReLU(inplace=True)\n    (12): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n    (13): ReLU(inplace=True)\n    (14): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n    (15): ReLU(inplace=True)\n    (16): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n    (17): Conv2d(256, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n    (18): ReLU(inplace=True)\n    (19): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n    (20): ReLU(inplace=True)\n    (21): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n    (22): ReLU(inplace=True)\n    (23): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n    (24): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n    (25): ReLU(inplace=True)\n    (26): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n    (27): ReLU(inplace=True)\n    (28): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n    (29): ReLU(inplace=True)\n    (30): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n  )\n  (avgpool): AdaptiveAvgPool2d(output_size=(7, 7))\n  (classifier): Sequential(\n    (0): Linear(in_features=25088, out_features=4096, bias=True)\n    (1): ReLU(inplace=True)\n    (2): Dropout(p=0.5, inplace=False)\n    (3): Linear(in_features=4096, out_features=4096, bias=True)\n    (4): ReLU(inplace=True)\n    (5): Dropout(p=0.5, inplace=False)\n    (6): Linear(in_features=4096, out_features=10, bias=True)\n  )\n)\n\nNow I wanted to train the model on CIFAR10, I created the train loader with batch size = 128.\nBelow is the training function and I added some printing statements to check if everything is working fine or not. The problem is the model outputs for each data point a 1000 prediction (as the original - unmodified version).\ndef train(model, optimizer, train_loader, epoch=5):\n    \"\"\"\n    This function updates\/trains client model on client data\n    \"\"\"\n    model.train()\n    for e in range(epoch):\n        for batch_idx, (data, target) in enumerate(train_loader):\n            data, target = data.to(device), target.to(device)\n            optimizer.zero_grad()\n            output = model(data)\n            print(\"shape output: \", output.shape)\n            probs = F.softmax(output, dim=1) #get the entropy \n            print(\"entropy: \", probs)\n            _, predicted = torch.max(output.data, 1)\n            loss = criterion(output, target)\n            loss.backward()\n            if batch_idx % 100 == 0:    # print every 100 mini-batches\n                print('[%d, %5d] loss: %.3f' %\n                  (e + 1, batch_idx + 1, loss.item()))\n            optimizer.step()\n    return loss.item()\n\nHere is part of the the output which shows the output shape:\nshape output:  torch.Size([128, 1000])\n\nI don\u2019t understand why the model outputs the results in this way. Is there something that I am missing?","y":"Changing the out_features attribute of the linear layer will not recreate the parameters, so you would need to assign a new layer instead:\n<code class=\"lang-python\">model = models.vgg16(pretrained=True)\nin_features = model.classifier[6].in_features\nmodel.classifier[6] = nn.Linear(in_features, out_features)\n<\/code>","z":"Changing the out_features attribute of the linear layer will not recreate the parameters, so you would need to assign a new layer instead:\n<code class=\"lang-python\">model = models.vgg16(pretrained=True)\nin_features = model.classifier[6].in_features\nmodel.classifier[6] = nn.Linear(in_features, out_features)\n<\/code>"},{"x":"I was surprised to realise that it is possible to call backward() in a computational graph that invokes torch.bernoulli(). However, I am not completely sure what happens in the background.\nIn this small example the gradients wrt to the Bernoulli probabilities are as far as I can tell always zero.\n<code class=\"lang-python\">import torch\np = torch.rand(2, 2, requires_grad=True)\nloss = torch.sum(torch.bernoulli(p))\nloss.backward()\ntorch.allclose(p.grad, torch.zeros_like(p))\n<\/code>\nSo I assume this is not invoking some form of surrogate gradient, is it? Is it intentional that the gradients are always 0 in this case?\nI would also be grateful if someone could point me to the respective *.cpp file that implements the backward() call.\nMany Thanks,\nSimon","y":"Hi,\nYes this is expected.\nIt is specified here: https:\/\/github.com\/pytorch\/pytorch\/blob\/727463a727e75858809a325477ac2b62ccd08e7e\/tools\/autograd\/derivatives.yaml#L270-L271","z":"Hi,\nYes this is expected.\nIt is specified here: https:\/\/github.com\/pytorch\/pytorch\/blob\/727463a727e75858809a325477ac2b62ccd08e7e\/tools\/autograd\/derivatives.yaml#L270-L271"},{"x":"I\u2019m trying to implement a custom autograd.Function, where one of the arguments for the forward pass does not need a gradient (let\u2019s say an index or a string or something like that). To illustrated I just modified the example from the tutorial with link \"https:\/\/pytorch.org\/tutorials\/beginner\/examples_autograd\/two_layer_net_custom_function.html\"  as follows:\n<code class=\"lang-auto\">class MyReLU(torch.autograd.Function):\n    \n    def forward(ctx, input, second):\n        print(second)   # <- second nondifferentiable argument\n        ctx.save_for_backward(input)\n        return input.clamp(min=0)\n\n    \n    def backward(ctx, grad_output):\n        input, = ctx.saved_tensors\n        grad_input = grad_output.clone()\n        grad_input[input < 0] = 0\n        return grad_input  # as a \"hack\"(?) you could just add None as a second output\n<\/code>\nNow I get the error\nRuntimeError: function MyReLUBackward returned an incorrect number of gradients (expected 2, got 1)\nIs there a way to define that the second argument doesn\u2019t need a gradient?\nI noticed that you can just use return grad_input, None in the backward() function, but that seems just like a hack. Or is this the way you\u2019re supposed to do it?","y":"Hi,\nNo need to do anything specific in the forward. And in the backward, returning None is the right thing to do ","z":"Hi,\nNo need to do anything specific in the forward. And in the backward, returning None is the right thing to do \nah cool, then it is simpler than I imagined, thanks a lot for you answer! "},{"x":"Hi,\nI would like to keep the maximum value to be 1 and others to be 0 in a tensor. But the backward function does not produce gradient for the tensor.\nI attached the example. Thanks!\nimport torch\nx = torch.Tensor([[0, 1, 2, 3]]).requires_grad_()\nb = (x==x.max(dim=1,keepdim=True)[0]).type(torch.FloatTensor).requires_grad_()\nout = b.sum()\nout.backward()\nx.grad #why is there no grad for x?\nb.grad","y":"\n\u201cb = (x==x.max(dim=1,keepdim=True)[0]).type(torch.FloatTensor)\u201d is actually not differentiable and so that x.grad is none?\n\nIf returns a Tensor that does not require gradients even though the input does. This means it is not differentiable yes.\n\nAnd is there a way to keep the maximum value to be 1 and the others to be 0 in a tensor but the tensor still could have gradient?\n\nIf you think about gradient as \u201chow much the output change if I change this input a little\u201d, then you can see that if the output of your function is \u201cthe maximum value to be 1 and the others to be 0\u201d, then all the gradients will be 0. Because when you change any input a little (if the maximum one does not change), then all the outputs will remain the same.","z":"Hi,\nThe problem is that you have a non-floating point Tensor that you set by hand to requires_grad_() again.\nYou will need to only use differentiable ops here.\nTo keep the values between 0\/1 I would recommend the softmax function that will make sure you get non-zero gradients everywhere (but you won\u2019t be able to reach 0 or 1).\nOr you can use clamp. But then the gradients for the values <0 and >1 will be 0.\n\n\n\n albanD:\n\nclamp\n\n\nThanks .  Does it mean \u201cb = (x==x.max(dim=1,keepdim=True)[0]).type(torch.FloatTensor)\u201d is actually not differentiable and so that  x.grad is none?\nAnd is there a way to keep the maximum value to be 1 and the others to be 0 in a tensor but the tensor still could have gradient? I found a similar question here Set Max value to 1, others to 0 with link \"https:\/\/discuss.pytorch.org\/t\/set-max-value-to-1-others-to-0\/44350\/2\" but it seems one hot is also not differentiable.\n\n\u201cb = (x==x.max(dim=1,keepdim=True)[0]).type(torch.FloatTensor)\u201d is actually not differentiable and so that x.grad is none?\n\nIf returns a Tensor that does not require gradients even though the input does. This means it is not differentiable yes.\n\nAnd is there a way to keep the maximum value to be 1 and the others to be 0 in a tensor but the tensor still could have gradient?\n\nIf you think about gradient as \u201chow much the output change if I change this input a little\u201d, then you can see that if the output of your function is \u201cthe maximum value to be 1 and the others to be 0\u201d, then all the gradients will be 0. Because when you change any input a little (if the maximum one does not change), then all the outputs will remain the same.\nVery clear explanation, thanks  !"},{"x":"Apparently the backward() in a custom autograd.Function does not get called by a python method directly (see also this blog entry regarding coverage tests with link \"https:\/\/www.janfreyberg.com\/blog\/2019-04-01-testing-pytorch-functions\/\"), which is why breakpoints set within these backward() methods never get triggered. Is there another way to use a python debugger for these backward passes?","y":"Hi,\nWhat I usually do when I need this is to trigger it directly from inside the backward function with     import pdb; pdb.set_trace():\n<code class=\"lang-python\">import torch\nfrom torch import autograd\n\nclass F(autograd.Function):\n  \n  def forward(ctx, foo):\n    return foo.clone()\n\n  \n  def backward(ctx, bar):\n    import pdb; pdb.set_trace()\n    return bar.clone()\n\na = torch.tensor([3.2], requires_grad=True)\nb = F.apply(a)\ng = autograd.grad(b, a)[0]\n\n\n\n<\/code>","z":"Hi,\nWhat I usually do when I need this is to trigger it directly from inside the backward function with     import pdb; pdb.set_trace():\n<code class=\"lang-python\">import torch\nfrom torch import autograd\n\nclass F(autograd.Function):\n  \n  def forward(ctx, foo):\n    return foo.clone()\n\n  \n  def backward(ctx, bar):\n    import pdb; pdb.set_trace()\n    return bar.clone()\n\na = torch.tensor([3.2], requires_grad=True)\nb = F.apply(a)\ng = autograd.grad(b, a)[0]\n\n\n\n<\/code>\nThank you very much, I forgot about that solution, it is simple and effective!\nBy the way: In case you  haven\u2019t seen it yet, you can shorten that to breakpoint() which is a built-in since python 3.7 with link \"https:\/\/docs.python.org\/3.7\/library\/functions.html#breakpoint\" - saves a little bit of typing:)\nI did not knew that   Thanks for the tip!"},{"x":"Hi,\nI would like to initialize a zero loss tensor which will be used to accumulate losses dynamically based on some condition. Sometimes, there will not be any loss accumulation and in this case, when I backpropagate, it throws the following run time error\nRuntimeError: element 0 of tensors does not require grad and does not have a grad_fn\nSample code looks like this:\n<code class=\"lang-python\">import torch\n\n# some inputs\nx = torch.rand(10, 3)\n\n# some targets\ny = torch.rand(10, 1)\n\n# some parameter\nW = torch.nn.Parameter(torch.rand(3, 1))\n\n# total loss\ntotal_loss = torch.tensor([0.0])\n\n# some condition\nif torch.rand(1) > 0.5:\n    loss = torch.sum((torch.matmul(x, W) - y) ** 2)\n    total_loss += loss\ntotal_loss.backward()\n<\/code>\nSo, the question is how to initialize this zero loss tensor for handling this?","y":"Ho right you can\u2019t use it inplace\u2026 The first one is definitely best.\nOr use you original code and check that total_loss.requires_grad before calling backward on it.","z":"Hi,\nI guess you have two options:\neither you do the backward only when it happened:\n<code class=\"lang-auto\">total_loss = 0.\n# You code\nif torch.is_tensor(total_loss):\n    total_loss.backward()\n<\/code>\nOr create it with requires grad all the time.\n<code class=\"lang-auto\">total_loss = torch.tensor([0.0], requires_grad=True)\n# You code\ntotal_loss.backward() # This will always work now.\n<\/code>\nI tried your second solution, it throws this run time error.\n<code class=\"lang-python\">total_loss += loss\nRuntimeError: a leaf Variable that requires grad has been used in an in-place operation.\n<\/code>\nHo right you can\u2019t use it inplace\u2026 The first one is definitely best.\nOr use you original code and check that total_loss.requires_grad before calling backward on it.\nThanks, I will go with this solution then."},{"x":"I am connecting two models, one model that extracts entities out of a document and the other uses these entities to generate a text summary. Both models are originally trained disjointly in a pipeline. I am trying to train them jointly in a way that gradients can flow all the way from the end model to the first. However, I am having some problems maintaining the gradients in such an integration.\nFor example, the first model generates the candidate entity starts, ends, and their corresponding scores. The candidate starts and ends they don\u2019t have any gradient function attached to it since they are just all possible candidate combinations up to a length. However, the candidate scores are the result of scoring every candidate using a FNN. I would like to feed the best candidate to the next model without detaching the gradients. What I have now is a normal python function that takes the scores, and returns the indices of the best candidates. I then use these indices to gather the corresponding candidate starts and ends. However, that doesn\u2019t preserve the gradient since originally both candidate_starts and candidate_ends matrices don\u2019t have a grad function attached to it.\n<code class=\"lang-auto\">###Model 1\n# [num-sentences, #candidates]\ncandidate_starts = [\n[1,1,3],\n...\n]\n\n# [num-sentences, #candidates]\ncandidate_ends = [\n[1,2,4],\n...\n]\n\n# [num-sentences, #candidates]\ncandidate_scores = FNN(...) = [\n[0.1, 0, 0.3],\n...\n]\n\n# where candidate_scores.grad_fn != None\n\n###Model 2\n# best entities (entity 1 and entity 3)\nentity_input = [\n   [1], # entities expands for the range from entity_start to entity end, in that case 1 to 1\n   [3,4] # 3 to 4\n]\n\n<\/code>\nI tried gathering the candidate_starts and candidate_ends with the best indices returned from the function but that doesn\u2019t keep gradients. Is there a way that I can gather the indices from the scores tensor instead (in a way similar to max) so that I can use it later in the backward path?\nThanks!","y":"Hi,\nYou will need to use the scores in a differentiable manner and give a differentiable input to the second network if you want gradients to be able to flow back.\nNote that if the inputs of the second network are integers, this most likely won\u2019t be possible as you can\u2019t get gradients for non-continuous types.","z":"Hi,\nYou will need to use the scores in a differentiable manner and give a differentiable input to the second network if you want gradients to be able to flow back.\nNote that if the inputs of the second network are integers, this most likely won\u2019t be possible as you can\u2019t get gradients for non-continuous types."},{"x":"Hi,\nI wonder if there is any way that can directly report parameters gradient after each layer\u2019s backward computation. Through the hook function on tensor or module, we are only able to get the intermediate results of the current layer but not the parameters\u2019 gradient.\nThanks in advance.","y":"Yes, the hook is given the grad as the argument.\nNote that if you want to modify the gradient that is saved in the .grad field, you can return the new value from the hook and it will be used.","z":"Hi,\nYou can add a hook on the Parameter itself with register_hook(). This will be used when the gradients are computed (and just before the .grad field is populated).\n\n\n\n albanD:\n\nParameter\n\n\nThanks for the prompt reply. I register the hook on the weights (e.g., module.weight.register_hook()). However, I got nothing when trying to print weight.grad in this hook function. I think this is because the hook function is called before .grad field is populated as you mentioned. I wonder if the computed gradient is the input of this tensor hook operation.\nThanks again!\nYes, the hook is given the grad as the argument.\nNote that if you want to modify the gradient that is saved in the .grad field, you can return the new value from the hook and it will be used."},{"x":"Hi,\nI\u2019m using a sparse matrix in part of my neural network and I was wondering if there is a way to get the gradients with respect to only the non zero values of a sparse matrix. If I convert the sparse matrix into a dense one, the matrix is going to be too large and it\u2019s going to take too much time to compute the gradient so I want to keep my matrix sparse.\nI would appreciate it if someone can help me with this.","y":"Hi,\nUnfortunately, at the moment, very few of the backward formulas formulas support sparse gradients \nBut you can call autograd.grad(out, inp, sparse_grad_out) to try and propagate sparse gradient in the backward pass.\nIf you want to create them during the backward, I think a custom Function can be used to do the conversion you want.","z":"Hi,\nUnfortunately, at the moment, very few of the backward formulas formulas support sparse gradients \nBut you can call autograd.grad(out, inp, sparse_grad_out) to try and propagate sparse gradient in the backward pass.\nIf you want to create them during the backward, I think a custom Function can be used to do the conversion you want.\nOk, Thanks for the help!"},{"x":"Hello,\nI\u2019m using PyTorch as an audodiff tool to compute the first and second derivatives of a cost function to be used in a (non-deep-learning) optimization tool (ipopt). The cost function depends about 10 parameters. I am using the new torch.autograd.functional.jacobian and torch.autograd.functional.hessian added to PyTorch 1.5. The Jacobian and Hessian get called several times (about 100) with different input parameters until the function is minimized.\nTo be more specific, I am computing the Jacobians and Hessians as:\n<code class=\"lang-auto\">def jac(f, x):\n    x_tensor = torch.tensor(x, requires_grad=True, dtype=torch.float64)   # Convert input variable to torch tensor\n    jac = torch.autograd.functional.jacobian(f, x_tensor)\n    return jac.numpy()\n\ndef hess(f, x):\n    x_tensor = torch.tensor(x, requires_grad=True, dtype=torch.float64)   # Convert input variable to torch tensor\n    hess = torch.autograd.functional.hessian(f, x_tensor)\n    return hess.numpy()\n<\/code>\nSince the functions to compute the derivatives are always the same, I was wondering if there was a way (or if it makes sense) to save the computational graph to avoid computing many times the same quantity, and thus speed up the calculation.\nThanks,\nDaniel","y":"Hi,\nThe short answer is:\nPytorch is actually built in such a way that the overhead of the graph creation is low enough (compared to all the computations you do in the forward) that you can do it at every iteration.\nThe long answer is:\nThis is true for neural network and when each op is quite \u201clarge\u201d but unfortunately, if you have only very small ops, then the overhead might become noticeable.\nThe usual answer is: \u201cuse the jit to get the last drop of perf for your production evaluation\u201d. But since you want to actually use backward here, you might run into troubles.\nYou can try to jit your function f. And I would be curious to know if it leads to any improvement.","z":"Hi,\nThe short answer is:\nPytorch is actually built in such a way that the overhead of the graph creation is low enough (compared to all the computations you do in the forward) that you can do it at every iteration.\nThe long answer is:\nThis is true for neural network and when each op is quite \u201clarge\u201d but unfortunately, if you have only very small ops, then the overhead might become noticeable.\nThe usual answer is: \u201cuse the jit to get the last drop of perf for your production evaluation\u201d. But since you want to actually use backward here, you might run into troubles.\nYou can try to jit your function f. And I would be curious to know if it leads to any improvement.\nThanks for your answer.\nI have tried to use jit to speed up the computation. It doesn\u2019t work. I have been able to jit the original function, but this does not lead to a relevant speedup. I can\u2019t jit the gradients and the Hessian:\n\n\ngist.github.com with link \"https:\/\/gist.github.com\/darteaga\/8df73f503518de9c8c783d11bae4e6ee\"\n\n\nhttps:\/\/gist.github.com\/darteaga\/8df73f503518de9c8c783d11bae4e6ee\ncompare_ipyopt_jax.ipynb\n<code class=\"Jupyter Notebook\">{\n \"cells\": [\n  {\n   \"cell_type\": \"code\",\n   \"execution_count\": 2,\n   \"metadata\": {},\n   \"outputs\": [],\n   \"source\": [\n    \"import torch\\n\",\n    \"import numpy as np\\n\",<\/code>\nThis file has been truncated. show original with link \"https:\/\/gist.github.com\/darteaga\/8df73f503518de9c8c783d11bae4e6ee\"\n\n\n\n\n\n\n\nIn the same gist I compare also to JAX with link \"https:\/\/github.com\/google\/jax\" for gradient and hessian. With JAX it is possible to precompute the gradients using jit. It seems that PyTorch is more or less at the level as JAX for the gradient computation, but not for Hessian computation: Hessian is about 20x slower in PyTorch than in JAX.\n\nIt seems that PyTorch is more or less at the level as JAX for the gradient computation\n\nThat\u2019s good news, that needs that your ops are big enough that the creation of the graph is negligible.\n\nHessian is about 20x slower in PyTorch than in JAX.\n\nThat might be due to other reasons, mainly the way the Hessian is computed: jax can use forward mode AD to speed this up while pytorch does not have forward mode AD (yet  )."},{"x":"When I return a Long gradient, Pytorch 1.5 complains with this error that it should be a floating type. https:\/\/github.com\/pytorch\/pytorch\/blob\/54c05fa34e1fbbb5096746a8ae92e27a08116de4\/torch\/csrc\/autograd\/engine.cpp#L610\nHowever when I cast this to float gradient in Pytorch 1.2, it complains that expected Long gradient but received float.\nHas behavior changed over versions?","y":"Hi,\nGradients for Long Tensors were never really supported. And were raising errors if used at random places.\nWe now unified this and properly raise an error when they are created (as you cannot use them anyway) so that the user can easily pinpoint the issue.","z":"Hi,\nGradients for Long Tensors were never really supported. And were raising errors if used at random places.\nWe now unified this and properly raise an error when they are created (as you cannot use them anyway) so that the user can easily pinpoint the issue."},{"x":"I want to compute the gradient between two tensors in a net. The input X tensor is sent through a set of convolutional layers which give me back and output Z tensor.\nI\u2019m creating a new loss and I would like to know the MSE between gradient  of norm(Y) w.r.t. each element of X. Here the code:\n<code class=\"lang-auto\"># Staring tensors\nX = torch.rand(40, requires_grad=True)\nY = torch.rand(40, requires_grad=True)\n\n# Define loss\nloss_fn = nn.MSELoss()\n\n#Make some calculations\nV = Y*X+2\n\n# Compute the norm\nV_norm = V.norm()\n\n# Computing gradient to calculate the loss\nfor i in range(len(V)):\n    if i == 0:\n        grad_tensor = torch.autograd.grad(outputs=V_norm, inputs=X[i])\n    else:\n        grad_tensor_ = torch.autograd.grad(outputs=V_norm, inputs=X[i])\n        grad_tensor = torch.cat((grad_tensor, grad_tensor_), dim=0)\n        \n# Grund truth\ngt = grad_tensor * 0 + 1\n\n#Loss\nloss_g = loss_fn(grad_tensor, gt)\nprint(loss_g) \n<\/code>\nUnfortunately, I\u2019ve been making tests with torch.autograd.grad(), but I could not figure out how to do it. I get teh following error: RuntimeError: One of the differentiated Tensors appears to not have been used in the graph. Set allow_unused=True if this is the desired behavior.\nSetting allow_unused=True gives me back None which is not an option. Not sure how to compute the loss between the gradients and the norm. Any idea about how to code this loss?","y":"Hi,\nI think the problem is that you forgot that indexing is a proper operation. And so X[i] returns a different Tensor than X, and X[i] was not used to compute the norm, hence the error.\nIf you give X as input, you should get the gradients for all X and then be able to access X.grad[i].","z":"Hi,\nI think the problem is that you forgot that indexing is a proper operation. And so X[i] returns a different Tensor than X, and X[i] was not used to compute the norm, hence the error.\nIf you give X as input, you should get the gradients for all X and then be able to access X.grad[i].\nThat was helpful. Thanks. Another question, related to if this a part of the final loss, with more criterion. Currently I\u2019m doing: loss = criterion_1 + loss_g and then loss.backward(). Is this correct? Or it is twice propagated? one in torch.autograd.grad(outputs=V_norm, inputs=X, retain_graph=True) and the other in loss.backward(). Thanks.\nYes this is correct. You will get the gradient corresponding to the sum of your loss, which is the sum of the gradients in this case.\n\nI try to get the outputs from several models, stack it together and feed into another classifier, but I get the following error:\nRuntimeError: One of the differentiated Tensors appears to not have been used in the graph. Set allow_unused=True if this is the desired behavior.\nBelow is the forward method of my model:\n<code class=\"lang-auto\">def forward_model(self, x, member_id=None):\n        mem_outpus = []\n        for i in range(5):\n            output = self.models[i](x)\n            mem_outpus.append(output)               \n                \n        ensemle_inputs = torch.stack(mem_outpus, dim=1).unsqueeze(dim=1)\n        ensemle_inputs = (ensemle_inputs - self.mu)\/self.std\n        ensemle_outputs = self.classifier(ensemle_inputs)\n        return ensemle_inputs\n<\/code>\nCould you post the code you are using to calculate the gradients?\nI guess this error might be raised by the direct gradient calculation via autograd.grad?\nYes I use autograd.grad.\nHere is the code:\n<code class=\"lang-auto\">import os\nimport torch\nimport torch.nn as nn\nfrom torch.autograd import Variable\nimport torchvision.datasets as dset\nimport torchvision.transforms as transforms\nimport torch.nn.functional as F\nimport torch.optim as optim\n## load mnist dataset\nuse_cuda = torch.cuda.is_available()\n\nroot = '.\/data'\nif not os.path.exists(root):\n    os.mkdir(root)\n    \ntrans = transforms.Compose([transforms.ToTensor(), transforms.Normalize((0.5,), (1.0,))])\n# if not exist, download mnist dataset\ntrain_set = dset.MNIST(root=root, train=True, transform=trans, download=True)\ntest_set = dset.MNIST(root=root, train=False, transform=trans, download=True)\n\nbatch_size = 100\n\ntrain_loader = torch.utils.data.DataLoader(\n                 dataset=train_set,\n                 batch_size=batch_size,\n                 shuffle=True)\ntest_loader = torch.utils.data.DataLoader(\n                dataset=test_set,\n                batch_size=batch_size,\n                shuffle=False)\n\nprint('==>>> total trainning batch number: {}'.format(len(train_loader)))\nprint('==>>> total testing batch number: {}'.format(len(test_loader)))\n\ntrain_args =  {\n            \"epsilon\": 0.3,\n            \"step_size\": 0.1,\n            \"num_steps\": 10}\n\n## network\n\nclass Net_Ensemble(nn.Module):\n    def __init__(self, num_ens, num_classes):\n        super(Net_Ensemble, self).__init__()\n        self.num_feature = num_ens*num_classes*10\n        self.num_classes = num_classes\n        self.conv_1 = nn.Conv2d(1, 4, kernel_size=(1, 1), stride=(1, 1))\n        self.conv_2 = nn.Conv2d(4, 10, kernel_size=(1, 1), stride=(1, 1))\n        self.fc_1 = nn.Linear(in_features=self.num_feature, out_features=100, bias=True)\n        self.fc_2 = nn.Linear(in_features=100, out_features=10, bias=True)\n        self.fc_3 = nn.Linear(in_features=200, out_features=100, bias=True)\n        self.fc_4 = nn.Linear(in_features=100, out_features=10, bias=True)\n\n        self.num_steps = train_args[\"num_steps\"]\n        self.epsilon = train_args[\"epsilon\"]\n        self.step_size = train_args[\"step_size\"]\n        self.rand = False\n\n    def forward_model(self, x):\n        #x = -x-280\n        x = F.relu(self.conv_1(x))\n        x = F.relu(self.conv_2(x))\n        x = x.view(-1, self.num_feature)\n        x = F.relu(self.fc_1(x))\n        x = self.fc_2(x)\n        return x\n\n    def forward(self, x):\n        outputs = self.forward_model(x.detach())    \n        return outputs\n\n\n\nclass LeNet(nn.Module):\n    def __init__(self):\n        super(LeNet, self).__init__()\n        self.conv1 = nn.Conv2d(1, 20, 5, 1)\n        self.conv2 = nn.Conv2d(20, 50, 5, 1)\n        self.fc1 = nn.Linear(4*4*50, 500)\n        self.fc2 = nn.Linear(500, 10)\n\n    def forward(self, x):\n        x = F.relu(self.conv1(x))\n        x = F.max_pool2d(x, 2, 2)\n        x = F.relu(self.conv2(x))\n        x = F.max_pool2d(x, 2, 2)\n        x = x.view(-1, 4*4*50)\n        x = F.relu(self.fc1(x))\n        x = self.fc2(x)\n        return x\n    \n    def name(self):\n        return \"LeNet\"\n\n\nclass model_combined(nn.Module):\n    def __init__(self, netlist_models, voting_net, mu, std):\n        super(model_combined, self).__init__()\n        self.models = nn.ModuleList(netlist_models)       \n        self.classifier = voting_net\n        self.mu = nn.Parameter(mu, requires_grad=False)\n        self.std = nn.Parameter(std, requires_grad=False)\n\n        self.num_steps = train_args[\"num_steps\"]\n        self.epsilon = train_args[\"epsilon\"]\n        self.step_size = train_args[\"step_size\"]\n        self.rand = False\n\n    def forward_model(self, x, member_id=None):\n        outputs = []\n        if member_id is None:\n            range_members = range(0, len(self.models))\n        else:\n            range_members = range(member_id, member_id+1)\n        mem_outpus = []\n        for i in range_members:\n            output = self.models[i](x)\n            mem_outpus.append(output)               \n                \n        ensemle_inputs = -torch.stack(mem_outpus, dim=1).unsqueeze(dim=1)\n        ensemle_inputs = (ensemle_inputs - self.mu)\/self.std\n        ensemle_outputs = self.classifier(ensemle_inputs)\n        return ensemle_outputs\n\n    def forward(self, x, target=None, make_adv=False):\n\n      \n      if make_adv:\n\n        if self.rand:\n            x = x + torch.zeros_like(x).uniform_(-self.epsilon, self.epsilon)\n          \n        prev_training = bool(self.training)\n        self.eval()        \n        \n        inputs = x.clone()\n\n        for i in range(self.num_steps):    \n           x = x.clone().detach().requires_grad_(True)\n           outputs = self.forward_model(x) \n           losses = criterion(outputs, target)\n           loss = torch.mean(losses) \n           grad, = torch.autograd.grad(loss, [x]) \n           with torch.no_grad():\n              step = torch.sign(grad) * self.step_size\n              diff = x + step - inputs\n              diff = torch.clamp(diff, -self.epsilon, self.epsilon)\n              x = torch.clamp(diff + inputs, 0, 1)\n        \n        if prev_training:\n           self.train()    \n      \n      outputs = self.forward_model(x.detach())    \n      return outputs\n\nNet = Net_Ensemble(1, 10)\n\nmu = torch.tensor([0.0])\nstd = torch.tensor([1.0])\n\n## training\nmodels = [LeNet()]\nmodel = model_combined(models, Net, mu, std)\n\nif use_cuda:\n    model = model.cuda()\n\noptimizer = optim.SGD(model.parameters(), lr=0.01, momentum=0.9)\n\ncriterion = nn.CrossEntropyLoss()\n\nfor epoch in range(10):\n    # trainning\n    ave_loss = 0\n    for batch_idx, (x, target) in enumerate(train_loader):\n        optimizer.zero_grad()\n        if use_cuda:\n            x, target = x.cuda(), target.cuda()\n        x, target = Variable(x), Variable(target)\n        out = model(x, target=target, make_adv=True)\n        loss = criterion(out, target)\n        ave_loss = ave_loss * 0.9 + loss.data * 0.1\n        loss.backward()\n        optimizer.step()\n        if (batch_idx+1) % 100 == 0 or (batch_idx+1) == len(train_loader):\n            print('==>>> epoch: {}, batch index: {}, train loss: {:.6f}'.format(\n                epoch, batch_idx+1, ave_loss))\n<\/code>\nThanks for the code.\nYou are detaching x in Net_Ensemble, which is used as self.classifier in model_combined.forward_model. Detaching the activation will cut the computation graph, such that Autograd won\u2019t be able to backpropagate to the original input anymore.\nAfter I\u2019ve removed the .detach() call, the error is gone (I\u2019ve used dummy tensors to test it, not the complete script)."},{"x":"I\u2019m building a model that generates sequences. Internally, the sequences are a (N,1, length, W) tensor of width-W one-hots, but part of the loss function involves converting sequences to strings (using a dictionary) and passing them as an argument to another PyTorch neural network that returns a scalar. Furthermore, certain one-hots are (deterministically) ignored during this conversion.\nIs this okay to do? It isn\u2019t clear to me how gradients would be backpropagated properly, but I\u2019m not getting any errors.","y":"Sure something like that will work:\n<code class=\"lang-auto\">import torch\n\ninp = torch.tensor([[[[0., 0., 1.],\n          [0., 0., 1.],\n          [0., 1., 0.],\n          [0., 1., 0.],\n          [0., 0., 1.],\n          [0., 0., 1.],\n          [0., 1., 0.],\n          [0., 0., 1.],\n          [1., 0., 0.],\n          [0., 0., 1.]]]])\n\nto_remove_line = torch.tensor([0., 1., 0.])\n\n\nto_remove = (inp == to_remove_line).all(-1)\nres = inp[to_remove.logical_not()]\n\nprint(res)\n\n<\/code>\nNote that the == can be relaxed if you\u2019re using floating points by checking that the difference is below a given threshold.","z":"Hi,\nAs you mentioned, strings are \u201cdiscrete\u201d and so gradients for them don\u2019t really make sense. So no you won\u2019t be able to packprop through that op I\u2019m afraid.\nI thought so! Instead, I\u2019ll need to train a model that can accept a (N,1, *, W) tensor, correct? In order to (deterministically) ignore some one-hots in the original (N,1, length, W) tensor, how should I filter so that gradients are still sensible?\nFor instance, suppose I have a (1, 1, 10, 3) tensor, and I want to ignore the one-hot (0,1,0). How would I turn:\n<code class=\"lang-auto\">tensor([[[[0., 0., 1.],\n          [0., 0., 1.],\n          [0., 1., 0.],\n          [0., 1., 0.],\n          [0., 0., 1.],\n          [0., 0., 1.],\n          [0., 1., 0.],\n          [0., 0., 1.],\n          [1., 0., 0.],\n          [0., 0., 1.]]]])\n<\/code>\ninto\n<code class=\"lang-auto\">tensor([[[[0., 0., 1.],\n          [0., 0., 1.],\n          [0., 0., 1.],\n          [0., 0., 1.],\n          [0., 0., 1.],\n          [1., 0., 0.],\n          [0., 0., 1.]]]])\n<\/code>\nin a gradient-friendly way?\nWell, removing entries is easy. You can simply index the elements you want to keep. And the gradients will flow properly back to these elements.\nBe careful though when you generate these one-hot encodings that you do so in a way that returns non-zero gradients. \nThank you! It isn\u2019t clear to me how to index the tensor in bulk like that. How would I do the indexing (without collecting the indices using a for loop?)\nSure something like that will work:\n<code class=\"lang-auto\">import torch\n\ninp = torch.tensor([[[[0., 0., 1.],\n          [0., 0., 1.],\n          [0., 1., 0.],\n          [0., 1., 0.],\n          [0., 0., 1.],\n          [0., 0., 1.],\n          [0., 1., 0.],\n          [0., 0., 1.],\n          [1., 0., 0.],\n          [0., 0., 1.]]]])\n\nto_remove_line = torch.tensor([0., 1., 0.])\n\n\nto_remove = (inp == to_remove_line).all(-1)\nres = inp[to_remove.logical_not()]\n\nprint(res)\n\n<\/code>\nNote that the == can be relaxed if you\u2019re using floating points by checking that the difference is below a given threshold."},{"x":"I am trying to implement a mechanism which needs to be detected when parameters are updated, there are three ways to implement it:\n\nuse register_backward_hook (problematic, it will not work sometimes, proved by experiment and statement in doc.\nuse register_hook on every parameter (usable, but will not be executed right after parameter update)\nfind a way to hook onto inplace operations such as add_, copy_ and addcdiv_, but I am not sure whether inplace operations are gauranteed to be used when you want to update a parameter (using param = some_modify_func(param) is possiple). And is there any clean way to hook onto them whithout creating a wrapper and wrap parameters inside ?\n\nWhat\u2019s your suggestion? Thanks in advance!","y":"Hi,\nYou can definitely use register_hook to know when gradients are computed for a Tensor (it is executed just before the gradient is accumulated and the new value is passed as an argument).\nBut I don\u2019t think you will be able to have a callback every time a Tensor is modified inplace ","z":"Hi,\nYou can definitely use register_hook to know when gradients are computed for a Tensor (it is executed just before the gradient is accumulated and the new value is passed as an argument).\nBut I don\u2019t think you will be able to have a callback every time a Tensor is modified inplace \nThank you! Eventually I chose the second option, currently there is no better option than register_hook "},{"x":"Say I have a top-k indexing matrix P (B*N*k), a weight matrix W(B*N*N) and a target matrix A (B*N*N), I want to get a adjacent matrix that operates as the following loops:\n<code class=\"lang-auto\">for i in range(B):\n         for ii in range(N):\n             for j in range(k):\n                 if weighted:\n                     A[i][ii][P[i][ii][j]] = W[i][ii][P[i][ii][j]]\n                 else:\n                     A[i][ii][P[i][ii][j]] = 1\n<\/code>\nHow to implement it more efficiently and concisely?","y":"You could use a scatter_\/gather approach as seen here:\n<code class=\"lang-python\">B, N, k = 2, 3, 4\n\nP = torch.randint(0, N, (B, N, k))\nW = torch.randn(B, N, N)\nA = torch.zeros(B, N, N)\nA_ = A.clone()\n\n# loop\nfor i in range(B):\n    for ii in range(N):\n        for j in range(k):\n            A[i][ii][P[i][ii][j]] = W[i][ii][P[i][ii][j]]\n\n# scatter\/gather approach\nA_.scatter_(2, P, torch.gather(W, 2, P))\n\n# check\nprint((A == A_).all())\n> tensor(True)\n<\/code>","z":"You could use a scatter_\/gather approach as seen here:\n<code class=\"lang-python\">B, N, k = 2, 3, 4\n\nP = torch.randint(0, N, (B, N, k))\nW = torch.randn(B, N, N)\nA = torch.zeros(B, N, N)\nA_ = A.clone()\n\n# loop\nfor i in range(B):\n    for ii in range(N):\n        for j in range(k):\n            A[i][ii][P[i][ii][j]] = W[i][ii][P[i][ii][j]]\n\n# scatter\/gather approach\nA_.scatter_(2, P, torch.gather(W, 2, P))\n\n# check\nprint((A == A_).all())\n> tensor(True)\n<\/code>"},{"x":"Im using Pytorch3D to take a projection of a mesh and running backprop on the loss on that projection.\nThe simplified version of my task is:\n<code class=\"lang-auto\">mesh = load_objs_as_meshes([os.path.join(path, 'mesh.obj')], device=device)\n\ncriterion = torch.nn.MSELoss()\n\ndeform_verts = torch.full(mesh.verts_packed().shape, 0.0, dtype=torch.float32, device=meta.device, requires_grad=True)\nmesh = mesh.offset_verts(deform_verts)\n\nprojection = project_mesh(smpl_mesh, angle).to(device)[0, :, :, 0]  # gives the projected image at the required angle\nloss = criterion(torch.flatten(projection), torch.flatten(ground_truth))\n\nloss.backward()\n<\/code>\nThe error im getting is :\nRuntimeError: range.second - range.first == t.size() INTERNAL ASSERT FAILED at \/opt\/conda\/conda-bld\/pytorch_1587428094786\/work\/torch\/csrc\/autograd\/generated\/Functions.cpp:57, please report a bug to PyTorch. inconsistent range for TensorList output (copy_range at \/opt\/conda\/conda-bld\/pytorch_1587428094786\/work\/torch\/csrc\/autograd\/generated\/Functions.cpp:57)","y":"It was a bug. Fixed in the latest pytorch3D update","z":"Could you please create an issue here with link \"https:\/\/github.com\/pytorch\/pytorch\/issues\" and, if possible, add an executable code snippet for easier debugging?\nHere is an executable code for the same issue\n<code class=\"lang-auto\">import os\nimport torch\nfrom pytorch3d.io import load_obj\nfrom pytorch3d.structures import Meshes, Textures\nfrom pytorch3d.renderer import (\n    look_at_view_transform,\n    OpenGLPerspectiveCameras,\n    PointLights,\n    RasterizationSettings,\n    MeshRenderer,\n    MeshRasterizer,\n    HardFlatShader\n)\n\n\npath = \"NOMO_preprocess\/data\"\ndevice = torch.device(\"cuda:1\" if torch.cuda.is_available() else \"cpu\")\n\n\n# Load any mesh and set req_grad True for vertices\nverts, faces_idx, _ = load_obj(os.path.join(path, 'male.obj'))\nverts.requires_grad = True\nfaces = faces_idx.verts_idx\nverts_rgb = torch.ones_like(verts)[None]  # (1, V, 3)\ntextures = Textures(verts_rgb=verts_rgb.to(device))\nsmpl_mesh = Meshes(\n    verts=[verts.to(device)],\n    faces=[faces.to(device)],\n    textures=textures\n)\n\ncriterion = torch.nn.MSELoss()\n\n# Project the mesh at any angle\nR, T = look_at_view_transform(1.75, -45, 0, up=((0, 1, 0),), at=((0, -0.25, 0),))\ncameras = OpenGLPerspectiveCameras(device=device, R=R, T=T)\nraster_settings = RasterizationSettings(\n            image_size=512,\n            blur_radius=0.0,\n            faces_per_pixel=1,\n        )\nlights = PointLights(device=device, location=[[0.0, 0.0, -3.0]])\nrenderer = MeshRenderer(\n    rasterizer=MeshRasterizer(\n        cameras=cameras,\n        raster_settings=raster_settings\n    ),\n    shader=HardFlatShader(device=device, lights=lights)\n)\nverts = smpl_mesh.verts_list()[0]\nverts_rgb = torch.ones_like(verts)[None]  # (1, V, 3)\ntextures = Textures(verts_rgb=verts_rgb.to(device))\nsmpl_mesh.textures = textures\nsmpl_mesh.textures._num_faces_per_mesh = smpl_mesh._num_faces_per_mesh.tolist()\nsmpl_mesh.textures._num_verts_per_mesh = smpl_mesh._num_verts_per_mesh.tolist()\n\nprojection = renderer(smpl_mesh)[0, :, :, 0]\n\nground_truth = torch.ones(projection.size(), device=device)\nloss = criterion(projection, ground_truth)\nloss.backward()\n<\/code>\nThis issue is faced for keeping the vertices as trainable parameters as week as when I keep a separate deforming vector is a trainable parameter, as shown in the question.\nIt was a bug. Fixed in the latest pytorch3D update"},{"x":"We know that we can implement our own custom autograd functions by subclassing torch.autograd.Function and implementing the forward and backward passes which operate on Tensors. As shown in https:\/\/pytorch.org\/docs\/stable\/autograd.html#function\n<code class=\"lang-auto\">class Exp(Function):\n\n    \n    def forward(ctx, i):\n        result = i.exp()\n        ctx.save_for_backward(result)\n        return result\n\n    \n    def backward(ctx, grad_output):\n        result, = ctx.saved_tensors\n        return grad_output * result\n<\/code>\nThe question is how can I call the backward function of operations in torch.nn.functional? Is this possible? And what is the simplest way?\nFor example, I want to call the backward function of torch.nn.functional.softmax and get the Jacobian matrix. I have tried torch.autograd.functional.jacobian, but it is experimental now and gets a wrong result when I apply it to softmax and a 3D tensor. Is there an existing backward function that I can directly call?","y":"Hi,\nThe functional methods in torch.nn are not all elementary functions for the autograd. So there isn\u2019t a single backward function to call for them.\nWhat you can do though is get the gradient with the regular autograd:\n<code class=\"lang-auto\"># inp that requires_grad=True and grad_output that match what you want to compute.\n# If you want a full jacobian, you will need a for loop to reconstruct it line by line.\nout = functional.softmax(inp)\ngrad = torch.autograd.grad(out, inp, grad_output)\n<\/code>\nWhat is the issue you have with functional.jacobian? What do you mean by wrong result?","z":"Hi,\nThe functional methods in torch.nn are not all elementary functions for the autograd. So there isn\u2019t a single backward function to call for them.\nWhat you can do though is get the gradient with the regular autograd:\n<code class=\"lang-auto\"># inp that requires_grad=True and grad_output that match what you want to compute.\n# If you want a full jacobian, you will need a for loop to reconstruct it line by line.\nout = functional.softmax(inp)\ngrad = torch.autograd.grad(out, inp, grad_output)\n<\/code>\nWhat is the issue you have with functional.jacobian? What do you mean by wrong result?\nHi albanD,\nThank you for your answer and torch.autograd.grad indeed works for me!\nI checked the docs of functional.jacobian again, and I found that I misunderstood the use of it. After I change the code, the result of functional.jacobian is all good now."},{"x":"Hi,\nI have a simple beginer question about autograd.\nThanks for the nice library.\nI really like the functionality and I understand that it simplifies a lot of code by packaging the derivative against a variable directly in the variable.\nWhat I am fearing is that the .backward() => .grad mechanic relies on the user keeping in mind that there is a mutable internal state, .grad, that is linked with what previous code was executed, which tends to hurt readability.\nLet\u2019s assume I write this:\n<code class=\"lang-python\">>>> w = torch.tensor([1., 1., 1.], requires_grad=True)\n>>> func = w[0] ** 2 + 2 * w[1] + w[2]   # The loss function AST is computed\n>>> func.backward()   # the derivatives of func against each tensor that was declared with required_true is computed and stored in the respective .grad internal states\n>>> w.grad   # I can get the derivatives, handy\ntensor([2., 2., 1.])\n<\/code>\nNow this means I should not write too much code between the .backward() and the .grad, because it is unclear which function it is the derivative of.\nWhen I look in a codebase at a .grad, I do not know immediately what function was used to generate it.\n<code class=\"lang-python\">>>> w = torch.tensor([1., 1., 1.], requires_grad=True)\n>>> func = w[0] ** 2 + 2 * w[1] + w[2]\n>>> func.backward()\n>>> oups_had_forgotten = func + 3 * w[1]\n>>> ... some code ...\n>>> w.grad\ntensor([2., 2., 1.])\n<\/code>\nIf I have not been cautious about my backward, am I using the function func or oups here?\nAlso, if I wanted to compute another derivative, I need to .zero_() the tensor, which is another hard to read step.\nI feel a more functional approach (in appearance, the backend could still behave like now) could be beneficial to try and learn users.\nConsider something like:\n<code class=\"lang-python\">>>> w.grad(func)\ntensor([2., 2., 1.])\n>>> w.grad(oups_had_forgotten)\ntensor([2., 5., 1.])\n<\/code>\nThis functional .grad would have the advantage of explicity and would not store the data in the tensor in place.\nSo, no need for .backward() and no need for .zero_() to clean the user exposed state.\nIs there a function exposing such a behavior today? Would there be big downturns in terms of usability\/performance?\nThank you","y":"Hi,\nThe .backward() and .grad fields are built to work nicely with the torch.nn and torch.optim libraries.\nIt allows you to handle computing the gradients and updating all the parameters without the user having to handle passing all the gradients by themselves.\nIf you are not working with these libraries, I would recommend using torch.autograd.grad(out, inp) that will return the gradient for each input you give.\nThis will do what you want here right?","z":"Hi,\nThe .backward() and .grad fields are built to work nicely with the torch.nn and torch.optim libraries.\nIt allows you to handle computing the gradients and updating all the parameters without the user having to handle passing all the gradients by themselves.\nIf you are not working with these libraries, I would recommend using torch.autograd.grad(out, inp) that will return the gradient for each input you give.\nThis will do what you want here right?\nThanks for your reply !\nIndeed you are correct this is the function I had in mind.\nFrom reading the documentation I did not understand clearly its puprose (maybe an example would have helped)\nI just tried to apply my mental flow with it and it works. The defaults do not allow for an easy try and learn though since they do not retain the graph:\n<code class=\"lang-python\">>>> w = torch.tensor([1., 1., 1.], requires_grad=True)\n>>> func = w[0] ** 2 + 2 * w[1] + w[2]\n>>> oups_had_forgotten = func + 3 * w[1]\n>>> torch.autograd.grad(oups_had_forgotten, w)\n(tensor([2., 5., 1.]),)\n>>> torch.autograd.grad(func, w)\nTraceback (most recent call last):\n  File \"<input>\", line 1, in <module>\n  File \"C:\\Users\\remi\\.virtualenvs\\zerotogan--y2dYRx8\\lib\\site-packages\\torch\\autograd\\__init__.py\", line 158, in grad\n    inputs, allow_unused)\nRuntimeError: Trying to backward through the graph a second time, but the buffers have already been freed. Specify retain_graph=True when calling backward the first time.\n<\/code>\nIs the aggressive freeing of buffers a performance tweek?\nBut if I choose to retain the graph at each step then I can at least do some retry:\n<code class=\"lang-python\">>>> w = torch.tensor([1., 1., 1.], requires_grad=True)\n>>> func = w[0] ** 2 + 2 * w[1] + w[2]\n>>> oups_had_forgotten = func + 3 * w[1]\n>>> torch.autograd.grad(func, w, retain_graph=True)\n(tensor([2., 2., 1.]),)\n>>> torch.autograd.grad(oups_had_forgotten, w, retain_graph=True)\n(tensor([2., 5., 1.]),)\n<\/code>\nThanks !\n\nIndeed you are correct this is the function I had in mind.\n\nPerfect!\n\nIs the aggressive freeing of buffers a performance tweek?\n\nIt is a very important memory improvement yes.\nIn a neural network, most of the memory usage comes from the buffer. So freeing the buffers as you do the backward allows you to significantly reduce the peak memory usage."},{"x":"Suppose we have a weighted loss (theta are the networks parameters):\no = model(data; theta)\nL = wi*F.cross_entropy(o, target, reduction=\u201cnone\u201d),\nwhere the wi are the result of another network (model_w). Assume we\u2019re using SGD to update the parameters in model so that:\ntheta\u2019(wi) = theta-lr x wi x grad(F.cross_entropy(data, target, reduction=\u201cnone\u201d))\nIf the weight network has a loss that\u2019s of the form:\nL_w = loss(data,target,model(data; theta\u2019(wi)))\nis there an easy way to backprop for the weight parameters? Or do I need to implement the backpropagation myself?","y":"Hi,\nThe issue is that you want to backprop through the optmizer step?\nWell the issue is that the optimizers in pytorch are not differentiable  So I would recommend using a library like higher with link \"https:\/\/github.com\/facebookresearch\/higher\/\" that is built do differentiate through gradient updates!","z":"Hi,\nThe issue is that you want to backprop through the optmizer step?\nWell the issue is that the optimizers in pytorch are not differentiable  So I would recommend using a library like higher with link \"https:\/\/github.com\/facebookresearch\/higher\/\" that is built do differentiate through gradient updates!\nSomething like that, yes  I see, I\u2019ll take a look at higher, thanks for the recommendation!"},{"x":"Due to certain data I want my network to only learn on losses that are lower than 1. For that I use the following code:\n<code class=\"lang-auto\">def train(img, label):\n   x = self.net(img)\n    loss_val = self.loss(x, label)\n    self.optimizer.zero_grad()\n        \n    if loss_val < 1:\n        loss_val.backward()\n        self.optimizer.step()\n<\/code>\nIs this correct? Because I have the feeling that the gradients are not reseting so as soon as a loss_val is under 1, all updates on the gradients are done, even the ones on which the osses were higher than 1.\nHow can I otherwise reset all the gradients from the optimiser?","y":"Each forward pass will create a new computation graph.\nTo double check your use case, you can print the gradients via:\n<code class=\"lang-python\">for name, param in net.named_parameters():\n    print(name, param.grad)\n<\/code>\nYou can print it at different places to see, if the gradients are valid or have been zeroed out.","z":"The code looks alright. What do you mean by \u201cthe gradients are not resetting\u201d?\nAre you seeing valid gradients at the end of the iteration even though the loss was >=1?\nI don\u2019t really know how to check that.\nMy worry that the following is happening. I do an iteration where the loss is over 1 and I don\u2019t do a backwards on the loss, so all tensors still hold their gradient trees from the iteration. Then, on the second iteration the gradient trees are then also changed according to the information of the new iteration (while still holding the information of the first iteration) and if the second iteration loss is then lower than 1, the gradients are calculated, but there is also the information about the first iteration that was actually lower than 1.\nEach forward pass will create a new computation graph.\nTo double check your use case, you can print the gradients via:\n<code class=\"lang-python\">for name, param in net.named_parameters():\n    print(name, param.grad)\n<\/code>\nYou can print it at different places to see, if the gradients are valid or have been zeroed out."},{"x":"Hi there,\nMy framework contains two parts: 1). a selection mechanism 2). a predictor. The input data is a sequence but not all items in the sequence are useful. So I just use TopK items in the sequence to predict something.\nBut I failed to calculate the gradients since using top K indices block gradients flow to the selection model. (second part of the code)\nA toy code as following:\nseq = torch.randn(10, 15, 8)                  # batch * seq_length * feature\nsel_model = nn.Linear(8, 1)                   # the selection mechanism\nscores = sel_model(seq).squeeze() \n\n# using topK indices bring the autograd problem\nval, ind = torch.topk(scores, 3, dim=1, largest=True)\nind_exp = ind.unsqueeze(-1).expand(10, 3, 8)\nselected_seq = torch.gather(seq, 1, ind_exp) \n\nfeature = selected_seq.mean(-1)\npred_model = nn.Linear(3, 1)\npred = pred_model(feature).squeeze()\nloss = pred.sum()  # assume there is a loss\nloss.backward()\n\nprint('sel_model grad\uff1a', sel_model.weight.grad)  # grad is None\nprint('select scores grad\uff1a', scores.grad)        # grad is None\nprint('pred_model grad\uff1a', pred_model.weight.grad)\n\nSo how to allow the gradients back to the selection model?","y":"Yeah, well, the core of the problem is that you are not using scores or val in the further calculation. If you then do the calculation based on the input sequence, you cannot expect gradients (which factor through dloss\/dval and dloss\/dscores = dval\/dscores dloss\/dval) to flow back. (You need to use scores.retain_grad() to keep intermediate gradients, btw.)\nNow, if you consider only the indices as the output of your selection model, you could look towards learning techniques in reinforcement learning for discrete decisions.\nBest regards\nThomas","z":"Yeah, well, the core of the problem is that you are not using scores or val in the further calculation. If you then do the calculation based on the input sequence, you cannot expect gradients (which factor through dloss\/dval and dloss\/dscores = dval\/dscores dloss\/dval) to flow back. (You need to use scores.retain_grad() to keep intermediate gradients, btw.)\nNow, if you consider only the indices as the output of your selection model, you could look towards learning techniques in reinforcement learning for discrete decisions.\nBest regards\nThomas\nHey thanks, it is helpful.\nHowever, I\u2019m still wondering what\u2019s the correct way to use top-k values, as top-k will break the chain rule (not sure just in my opinion)? Could you provide some examples?\nBelow is a modified version of the previous code, using top-k values.\ndata = torch.randn(10, 15)  # batch * features\nselect_model = nn.Linear(15, 15)  # each feature has a score\nscores = select_model(data)\n\n# use top-3 features and mask the rest\nval, ind = torch.topk(scores, 3, dim=1, largest=True)\nmasked_scores = torch.zeros_like(scores)\nmasked_scores.scatter_(1, ind, val)\nmasked_data = data * masked_scores  # mask some sequences\n\n# I can use retain_grad to keep grad, but which one I should use?\n# scores.retain_grad()\n# masked_scores.retain_grad()\n# masked_data.retain_grad()\n\n# do the prediction task using the masked data\npred_model = nn.Linear(15, 1)\npred = pred_model(masked_data).squeeze()\nloss = pred.sum()  # assume there is a loss\nloss.backward()\n\nprint('select_model grad\uff1a', select_model.weight.grad.size())\nprint('select scores grad\uff1a', scores.grad)  # grad is None\nprint('masked scores grad\uff1a', masked_scores.grad)  # grad is None\nprint('pred_model grad\uff1a', pred_model.weight.grad.size())\n\n\n\n Stone_Yu:\n\nHowever, I\u2019m still wondering what\u2019s the correct way to use top-k values, as top-k will break the chain rule (not sure just in my opinion)? Could you provide some examples?\n\n\ntopk will propagate gradients from the grad of output values to the grad of inputs all right (in fact, the backward is a scatter to the indices similar to the ones you have), so the topk part you show should work now.\nBest regards\nThomas\nYes. Topk part works well even without retain_grad(). There might be some other problems.\nThanks!"},{"x":"Hi,\nI am considering the use of gradient checkpointing to lessen the VRAM load. From what I understand there were some issues with stochastic nodes (e.g. Dropout) at some point in time to apply gradient checkpointing.\nHowever I have a kind of Bayesian Neural Network which needs quite a bit of memory, hence I am interested in gradient checkpointing. Since all the weights are bayesian, there is stochasticity everywhere which can\u2019t be removed.\nCan I safely apply gradient checkpointing to this Bayesian Neural Network ?","y":"Thanks for the link!\nWhile this notebook gives you a really good example of the usage, note that it\u2019s a bit outdated by now and if I\u2019m not mistaken, \u2019s PR with link \"https:\/\/github.com\/pytorch\/pytorch\/pull\/14253\" should have enabled the bitwise accuracy between standard models and checkpointed models.\nThese tests with link \"https:\/\/github.com\/pytorch\/pytorch\/blob\/41363b299a04a43ecb7520b39843d4c3a27c063a\/test\/test_utils.py#L204-L257\" also should verify this behavior.","z":"Based on the description from the docs with link \"https:\/\/pytorch.org\/docs\/stable\/checkpoint.html\" it seems the default behavior would be a deterministic dropout, which can be disabled:\n\nCheckpointing is implemented by rerunning a forward-pass segment for each checkpointed segment during backward. This can cause persistent states like the RNG state to be advanced than they would without checkpointing. By default, checkpointing includes logic to juggle the RNG state such that checkpointed passes making use of RNG (through dropout for example) have deterministic output as compared to non-checkpointed passes. The logic to stash and restore RNG states can incur a moderate performance hit depending on the runtime of checkpointed operations. If deterministic output compared to non-checkpointed passes is not required, supply  preserve_rng_state=False  to  checkpoint  or  checkpoint_sequential  to omit stashing and restoring the RNG state during each checkpoint.\n\nSo it seems your use case should be fine.\nCould you link to the known issues with stochastic nodes? I remember there were some issues, but couldn\u2019t find these issues quickly.\nThanks for your reply ! \nI found the issue discussed in this tutorial, section \u201cHandling a few special layers in checkpointing\u201d : https:\/\/github.com\/prigoyal\/pytorch_memonger\/blob\/master\/tutorial\/Checkpointing_for_PyTorch_models.ipynb\nThanks for the link!\nWhile this notebook gives you a really good example of the usage, note that it\u2019s a bit outdated by now and if I\u2019m not mistaken, \u2019s PR with link \"https:\/\/github.com\/pytorch\/pytorch\/pull\/14253\" should have enabled the bitwise accuracy between standard models and checkpointed models.\nThese tests with link \"https:\/\/github.com\/pytorch\/pytorch\/blob\/41363b299a04a43ecb7520b39843d4c3a27c063a\/test\/test_utils.py#L204-L257\" also should verify this behavior.\nThank you very much for the informations !  It worked pretty well from my first few experiments.\nHave a nice day "},{"x":"Hi,\nI have a class for my model that uses some other model in it. For example:\nclass Mynet1(nn.Module):\ndef init(self):\nsuper(NN_model, self).init()\nself.fc1 = nn.Linear(4160, 500)\nclass Mynet2(nn.Module):\ndef init(self, Mynet):\nsuper(NN_model, self).init()\nself.fc1 = nn.Linear(4160, 500)\nself.layer = Mynet(\u2026)\nmynet1=Mynet1()\nmodel = torch.nn.Dataparallel(Mynet2(mynet1))\nmodel.cuda()\nSo if I want to run Mynet2 on multiple devices. I will do torch.nn.Dataparallel(Mynet2). Do I also need to take Mynet1 to multiple devices as in do torch.nn.Dataparallel(Mynet1) and then send it to Mynet2?\nOr it will automatically be on multiple devices since I take Mynet2 to multiple devices.\nOne more question related to dataparallel. Let\u2019s say I have my custom conv2d class that uses torch variables like torch.mul, torch.sub etc. So when I was running this code on a single device I used to do \u201c.to(device)\u201d for all the intermediate variable in my custom conv2d class. So now when I run a model using this custom conv2d class on multiple devices using dataparallel I keep on getting this error: \u201cRuntimeError: Expected tensor for argument #1 \u2018input\u2019 to have the same device as tensor for argument #2 \u2018weight\u2019; but device 0 does not equal 1 (while checking arguments for cudnn_batch_norm)\u201d.\nI have taken my model that uses this custom conv2d class to multiple devices using torch.nn.dataparallel. So I don\u2019t understand why I keep on getting this error. Is this because the intermediate variable that I have inside my custom conv2d class are on single device?\nIf so how can I take this intermediate variable to multiple devices?\nShould I define them as model parameters using torch.nn.Parameter?\nThanks","y":"\n\n\n s_n:\n\nSo does dataparallel takes this input to all devices when we use data parallel?\nBecause while passing input (data_var) to my model I take it to default device.\nDoes pytorch create copies of input image as well on all the GPUs when we use dataparallel?\n\n\nnn.DataParallel will split the input tensor in dim0 and will send each chunk to the corresponding model replica on a specific device.\nI.e. your model\u2019s forward will get an input of the shape [batch_size\/\/nb_gpus, *].\nIf you need to create y1 and y2 inside the forward method, you should also consider the current batch size of the input.\nAlso, it is correct to push the input tensor in the DataLoader loop to the default device, since nn.DataParallel will take care of the scattering and gathering of the tensors and parameters.","z":"\n\n\n s_n:\n\nSo if I want to run Mynet2 on multiple devices. I will do torch.nn.Dataparallel(Mynet2). Do I also need to take Mynet1 to multiple devices as in do torch.nn.Dataparallel(Mynet1) and then send it to Mynet2?\nOr it will automatically be on multiple devices since I take Mynet2 to multiple devices.\n\n\nnn.DataParallel will automatically create the model replica with all internal submodules, parameters, and buffers on the specified devices, so you can wrap the main model only into nn.DataParallel.\n\n\n\n s_n:\n\nIs this because the intermediate variable that I have inside my custom conv2d class are on single device?\nIf so how can I take this intermediate variable to multiple devices?\nShould I define them as model parameters using torch.nn.Parameter?\n\n\nIt depends, how you\u2019ve created these tensors.\nAll registered nn.Parameters and buffers (via self.register_buffer('name', torch.randn(1))) will be pushed automatically to the corresponding device.\nHowever, if you have created a plain tensor and pushed it to a device via to('cuda'), you would either have to use the device argument of another parameter or register it as an nn.Parameter (if it needs gradients) of buffer.\nThanks for the reply . So I won\u2019t even have to take mynet1 instance to the device before passing it as an argument to Mynet2?\nRegarding the answer to my second question. I actually don\u2019t need the gradients for these intermediate variables. Can you please explain a bit more on what you mean by \" However, if you have created a plain tensor and pushed it to a device via  to('cuda') , you would either have to use the  device  argument of another parameter or register it as an  nn.Parameter  (if it needs gradients) of buffer\"\nOne solution to this problem I feel would be to define all my intermediate variables as nn.Parameters and make require_grad=False. Also, I can define them self.register_buffer and make grad=False. Will both solutions be similar?\n\n\n\n s_n:\n\nSo I won\u2019t even have to take mynet1 instance to the device before passing it as an argument to Mynet2?\n\n\nThat is correct. Your mynet1 would act as a \u201cstandard\u201d module, e.g. similar to nn.Linear or nn.Conv2d, so you can just pass it into the parent model (Mynet2) and call to() or cuda() on the parent.\n\n\n\n s_n:\n\nCan you please explain a bit more on what you mean by\n\n\nAll nn.Modules, nn.Parameters and buffers will be properly moved to the device, if to() or cuda()\/cpu() is called on the parent device, while plain tensors will not. This code snippet demonstrates the behavior:\n<code class=\"lang-python\">class MyModel(nn.Module):\n    def __init__(self):\n        super(MyModel, self).__init__()\n        self.module = nn.Linear(1, 1)\n        self.param = nn.Parameter(torch.randn(1, 1))\n        self.register_buffer('buf', torch.randn(1, 1))\n        self.ten = torch.randn(1, 1)\n\n    def forward(self, x):\n        return self.module(x) + self.param + self.buf + self.ten\n        \nmodel = MyModel()\nprint(model.module.weight.device)\nprint(model.param.device)\nprint(model.buf.device)\nprint(model.ten.device)\nout = model(torch.randn(1, 1)) # works\n> cpu\ncpu\ncpu\ncpu\n\nmodel.to('cuda')\nprint(model.module.weight.device)\nprint(model.param.device)\nprint(model.buf.device)\nprint(model.ten.device)\nout = model(torch.randn(1, 1).to('cuda')) # fails, as self.ten is still on the CPU\n> cuda:0\ncuda:0\ncuda:0\ncpu\nRuntimeError: expected device cuda:0 but got device cpu\n<\/code>\n\n\n\n s_n:\n\nOne solution to this problem I feel would be to define all my intermediate variables as nn.Parameters and make require_grad=False. Also, I can define them self.register_buffer and make grad=False. Will both solutions be similar?\n\n\nIf you don\u2019t want to train this tensor, use a buffer. Using a parameter with requires_grad=False would also work, but a buffer would be the cleaner approach.\n\nActually for my conv2d function I am using autograd Functions. Like below\nclass Conv2d_function(Function):\n\ndef forward(ctx, x1,x2):\n    \n    y1 = torch.tensor([1,2,3]).to(device)\n    y2 = torch.tensor([4,5,6]).to(device)\n    ....\n\nclass Conv2d(nn.Module):\ndef init(self, in_channels \u2026):\n\u2026\n\u2026\nsuper(Conv2d, self).init()\ndef forward(self, input):\n    return Conv2d_function.apply(input)\n\nCreating model using custom conv2d class\nclass Model():\nlayer1 = Conv2d(\u2026)\nlayer2 = Conv2d(\u2026)\nmodel1 = Model()\nSo when I use Dataparallel for this model1 I get this error \u201cRuntimeError: Expected tensor for argument #1 \u2018input\u2019 to have the same device as tensor for argument #2 \u2018weight\u2019; but device 0 does not equal 1 (while checking arguments for cudnn_batch_norm)\u201d\nIs this error occurring because in the forward function of Conv2d_function class I am defining a tensor using .to(device)?\nActually the tensor y1 and y2 depend on my input to the forward function of class Conv2d so I can\u2019t define those tensor in the init of Conv2d class as register_buffer or Parameter. So I can only define those in my forward function of Conv2d_function class. But with data-parallel I get this above error which I feel is because I am transferring these tensors to GPU in my forward pass.\n\n\n\n s_n:\n\nIs this error occurring because in the forward function of Conv2d_function class I am defining a tensor using .to(device)?\n\n\nYes, and since you are not specifying the device id, the default device will be used in each model replica.\nYou could use the device attribute of the incoming tensors instead:\n<code class=\"lang-python\">y1 = torch.tensor([1,2,3]).to(x1.device)\ny2 = torch.tensor([4,5,6]).to(x1.device)\n<\/code>\nOk. A quick question in line with this. So when we pass input to our model. We run a for loop over the dataloader as shown below:\ndevice = torch.device(\u201ccuda:0\u201d if torch.cuda.is_available() else \u201ccpu\u201d)\nfor batch_idx,(data, target) in enumerate(testloader):\ndata_var = data.to(device)\ntarget_var = target.to(device)\noutput = model(data_var)\nSo does dataparallel takes this input to all devices when we use data parallel?\nBecause while passing input (data_var) to my model I take it to default device.\nDoes pytorch create copies of input image as well on all the GPUs when we use dataparallel?\nBecause I want my tensors (y1, y2 \u2026) to be on all the devices and now I will be initializing these tensors using x1.device\n\n\n\n s_n:\n\nSo does dataparallel takes this input to all devices when we use data parallel?\nBecause while passing input (data_var) to my model I take it to default device.\nDoes pytorch create copies of input image as well on all the GPUs when we use dataparallel?\n\n\nnn.DataParallel will split the input tensor in dim0 and will send each chunk to the corresponding model replica on a specific device.\nI.e. your model\u2019s forward will get an input of the shape [batch_size\/\/nb_gpus, *].\nIf you need to create y1 and y2 inside the forward method, you should also consider the current batch size of the input.\nAlso, it is correct to push the input tensor in the DataLoader loop to the default device, since nn.DataParallel will take care of the scattering and gathering of the tensors and parameters."},{"x":"<code class=\"lang-auto\">   for batch_idx, (data, target) in enumerate(train_loader):\n        meta_data = data\n        meta_data.requires_grad = True\n        print(data.requires_grad) \/\/ prints True, since we are not doing deep copy\n        data, target = data.to(device), target.to(device) \/\/ moving tensors to GPU\n        optimizer.zero_grad()\n        output = model(data)\n        loss = F.nll_loss(output, target)\n        loss.backward()\n        print(meta_data.grad) \/\/prints the corresponding gradient\n        print(data.grad) \/\/ prints None, Why is this the case ??\n        print((data - meta_data.cuda()).norm(2))\n\n<\/code>\nMy question is why is meta_data.grad not none, while data.grad is none ? is it because of moving the data to the gpu ? Thanks in advance!","y":"Hi,\nIt is because .to() is a differentiable op. And so the data that you get back is not a leaf anymore (if you print it, you\u2019ll see the grad_fn attached to it).","z":"Hi,\nIt is because .to() is a differentiable op. And so the data that you get back is not a leaf anymore (if you print it, you\u2019ll see the grad_fn attached to it).\nThank you, this resolves it !"},{"x":"For example, I want to explicitly multiply 2 on the convolution layers\u2019 weight.\nI motify the official conv.py.py with link \"https:\/\/github.com\/pytorch\/pytorch\/blob\/master\/torch\/nn\/modules\/conv.py#L376-L377\"  from :\n<code class=\"lang-auto\">    def forward(self, input):\n        return self._conv_forward(input, self.weight)\n<\/code>\nto\n<code class=\"lang-auto\">    def forward(self, input):\n        new_weight = self.weight * torch.ones(self.weight.size())*2\n        return self._conv_forward(input, new_weight)\n<\/code>\nI noticed that the type of new_weight becomes torch.Tensor  rather than torch.nn.parameter.Parameter.\nSo I wonder whether it\u2019s OK to do this? And can the backward process works correctly?\n\nThanks in advance!","y":"Hey,\nNo need to ping, we look at the posts \nAnd yes it is expected that after doing any differentiable operation on it, you get a Tensor.\nKeep in mind that nn.Parameter are only the leafs stored in the nn.Module that nn recognize as being part of the parameters.\nAlso you can write the same thing as new_weight = self.weight * 2 to make it easier to read ","z":"Hey,\nNo need to ping, we look at the posts \nAnd yes it is expected that after doing any differentiable operation on it, you get a Tensor.\nKeep in mind that nn.Parameter are only the leafs stored in the nn.Module that nn recognize as being part of the parameters.\nAlso you can write the same thing as new_weight = self.weight * 2 to make it easier to read \nThank you very much."},{"x":"Is it possible that two instances of a convolutional layer in my init method can share same set of weights?\nEx:\nself.conv1 = nn.Conv2d(\u2026)\nself.conv2 = MycustomConvFunction(\u2026)\nSo I want self.conv1 and self.conv2 to share same set of weights.\nActually I want self.conv2 for inference and self.conv1 for training.","y":"The most elegant way would probably be the functional API, which would only create a single weight parameter and just use it when it\u2019s needed.\nAlternatively, you could assign the weight parameter to your modules as described here with link \"https:\/\/discuss.pytorch.org\/t\/how-to-use-shared-weights-in-different-layers-of-a-model\/71263\/2\".","z":"The most elegant way would probably be the functional API, which would only create a single weight parameter and just use it when it\u2019s needed.\nAlternatively, you could assign the weight parameter to your modules as described here with link \"https:\/\/discuss.pytorch.org\/t\/how-to-use-shared-weights-in-different-layers-of-a-model\/71263\/2\".\nHi \nI saw the link that you attached. Will I have to every time make the weights equal for two instances in the forward method?\nOr I can just equate the weights of two nn.Linear instances once and they will share same storage location and weight values for all the epochs.\nFor example\nclass testModule(nn.Module):\ndef __init__(self):\n    super(testModule, self).__init__()\n    self.fc1 = nn.Linear(5, 10, bias=True)\n    self.fc2 = MyLinearLayerModel(10, 10, bias=False)\ndef forward(self, x, p=False):\n    if p = True:\n         x = self.fc1(x)\n   else:\n        self.fc2.weight.data  = self.fc1.weight.data\n         x = self.fc2(x)\n    return x\n\nOr I can just do \u201cself.fc2.weight.data  = self.fc1.weight.data\u201d once inside init?"},{"x":"<code class=\"lang-auto\">loss = nn.CrossEntropyLoss()\ninput = torch.randn(3, 5, requires_grad=True)\ntarget = torch.empty(3, dtype=torch.long).random_(5)\noutput = loss(input, target)\noutput.backward()\n<\/code>\nThis is an example of nn.crossentropy loss from Pytorch docs.\nhttps:\/\/pytorch.org\/docs\/stable\/nn.html#torch.nn.CrossEntropyLoss\nThis is run on torch 0.1.5 version and shows no grad_fn error","y":"Hey sorry I got it, in my code there was torch.set_grad_enabled(False); this line, I was struggling with my custom loss function for a day for this error:( . I tried this example to see if same error comes up.","z":"I assume you are running this code in PyTorch 1.5.0 (not 0.1.5)?\nThis code snippet should work and works for me in this version.\nCould you post the error message as well as the output of print(torch.__version__), please?\ntorch1732\u00d7693 95.2 KB with link \"https:\/\/discuss.pytorch.org\/uploads\/default\/original\/3X\/5\/a\/5a7ff69402518b7dcc9b05c93681ee1f5b6e6c3d.png\"\nI am doing this on colab, when i run this snippet on my system with torch 1.4.0 it runs correctly.\nCould you add print(output) before calling output.backward() and post it here?\nYou can add code snippets by wrapping them into three backticks ```, which makes debugging easier.\n<code class=\"lang-auto\">loss = nn.CrossEntropyLoss()\ninput = torch.randn(3, 5, requires_grad=True)\ntarget = torch.empty(3, dtype=torch.long).random_(5)\noutput = loss(input, target)\nprint(output)\noutput.backward()\n\ntensor(1.6685)\n---------------------------------------------------------------------------\nRuntimeError                              Traceback (most recent call last)\n<ipython-input-5-835ec6ff9004> in <module>()\n      4 output = loss(input, target)\n      5 print(output)\n----> 6 output.backward()\n\n1 frames\n\/usr\/local\/lib\/python3.6\/dist-packages\/torch\/autograd\/__init__.py in backward(tensors, grad_tensors, retain_graph, create_graph, grad_variables)\n     98     Variable._execution_engine.run_backward(\n     99         tensors, grad_tensors, retain_graph, create_graph,\n--> 100         allow_unreachable=True)  # allow_unreachable flag\n    101 \n    102 \n\nRuntimeError: element 0 of tensors does not require grad and does not have a grad_fn\n<\/code>\nSorry, no idea what might be going on.\nCould you restart the kernel or are you running in a clean environment?\nI would recommend to create a new virtual environment, reinstall PyTorch and rerun the script.\nI am running on google colab, I only installed Transformers nothing else, maybe you can try it on google colab, same thing happening on kaggle notebooks also\nHey sorry I got it, in my code there was torch.set_grad_enabled(False); this line, I was struggling with my custom loss function for a day for this error:( . I tried this example to see if same error comes up.\nOh OK, so the gradient calculation was disabled globally. I\u2019m glad you figured it out. "},{"x":"I am trying to calculate the gradient (d(loss)\/dj). But I get grad is None\n<code class=\"lang-auto\">class model(nn.Module):\n    def __init__(self):\n        super().__init__()\n        self.fc = nn.Linear(256, 2)\n        \n    def forward(self, j, labels):\n        e = self.fc(j)\n        print(labels.shape)\n        print(e.shape)\n\n        j.requires_grad = True\n        \n        with torch.enable_grad():\n            loss = F.cross_entropy(e, labels, reduction='sum')\n            j.retain_grad()\n            loss.backward()\n            grad = j.grad.detach()\n            \n        return grad\n<\/code>","y":"Hi,\nThe problem I think is that you set j.requires_grad after you used it. So the graph is not created when you do the forward.\nYou should move this at the beginning of the forward() function.","z":"Hi,\nThe problem I think is that you set j.requires_grad after you used it. So the graph is not created when you do the forward.\nYou should move this at the beginning of the forward() function."},{"x":"Hi, I have a neural network the last last layer of which is Sigmoid, but the output of my model(input) contains values that are out from [0\u20261] range. Sorry, I am new in pytorch so I am not familiar with all pipelines. Could you please tell me, should I apply the sigmoid separately? If so, why it is shown in the neural network architecture?","y":"Thanks for the update.\nHave you set the self.testing argument to True, as the last activation wouldn\u2019t be used otherwise as seen here with link \"https:\/\/github.com\/wolny\/pytorch-3dunet\/blob\/62e10674c5cf9f44e252297203213b8c7f23c5f7\/pytorch3dunet\/unet3d\/model.py#L137\"?","z":"That sounds rather like a bug. Could you post your model architecture so that we could take a look at it?\nAlso, which PyTorch, CUDA, cudnn version are you using and how did you build\/install it?\nI use the Unet3D model from pytorch-unet3d repo with link \"https:\/\/github.com\/wolny\/pytorch-3dunet\/blob\/master\/pytorch3dunet\/unet3d\/model.py\". I am training\/predict on the machine with multiple gpus, but use only one GPU device. For training, I use the code provided by the repository but use my own dataloaders. For the prediction I use the following code\n<code class=\"lang-auto\">device = torch.device(\"cuda:3\" if torch.cuda.is_available() else \"cpu\")\nconfig['device'] = device\nmodel = get_model(config)\nutils.load_checkpoint(PATH, model)\n\nwith torch.no_grad():\n    model.to(device)\n    model.eval()\n    arr = arr.to(device)\n    pred = model(arr)\n    pred = pred.data.cpu().numpy()\n    print(np.unique(pred),pred.shape)\n<\/code>\nresult of print(model)\n<code class=\"lang-auto\">....\n....\n)\n  (final_conv): Conv3d(32, 1, kernel_size=(1, 1, 1), stride=(1, 1, 1))\n  (final_activation): Sigmoid()\n)\n<\/code>\nVersions\nPytorch=1.2, Cuda compilation tools, release 10.0, V10.0.130\npytorch was installed via  pip install torch==1.2.0 torchvision==0.4.0\nI don\u2019t know how to check cudnn version, could you please help me with that?\nCould you update to the latest PyTorch release with CUDA10.2 or CUDA11.0 and retry the code?\nPyTorch 1.2.0 is quite old by now and this might have been an already fixed issue.\nAre there any ways to check this without updating to the latest version? The thing is, I am renting a GPU, so the environment was set up for me, it can take a while to install another version of cuda to check this, but if you think that it\u2019s better to use the latest release because there can be some serious bugs in 1.2.0, I would update it asap.\nYou don\u2019t need to install a local CUDA toolkit, if you are not planning to build PyTorch from source or any custom CUDA extensions, since the conda binaries and pip wheels ship with their own CUDA runtimes.\nThanks, I updated the pytorch to 1.7, the result of the prediction is still out of [-1,1] print(np.unique(pred))\n<code class=\"lang-auto\">[-38.52738  -38.197113 -37.780617 ...  10.451873  10.506324  10.691366]\n<\/code>\nCheck of the pytorch version (was installed via pip install torch==1.7.1+cu101 torchvision==0.8.2+cu101 torchaudio==0.7.2 -f https:\/\/download.pytorch.org\/whl\/torch_stable.html)\n<code class=\"lang-auto\">torch.__version__\n\nOut[2]:\n\n'1.7.1+cu101'\n<\/code>\nThanks for the update.\nHave you set the self.testing argument to True, as the last activation wouldn\u2019t be used otherwise as seen here with link \"https:\/\/github.com\/wolny\/pytorch-3dunet\/blob\/62e10674c5cf9f44e252297203213b8c7f23c5f7\/pytorch3dunet\/unet3d\/model.py#L137\"?\nThanks for the help, with model.testing=True the output is correct. Sorry for the confusion, should\u2019ve checked it myself."},{"x":"Greetings everyone,\nI am trying to define a loss and use backward() on it. However, I am getting the error: One of the variables needed for gradient computation has been modified by an inplace operation.\nHere is the code:\n<h1>for displaying the target image, intermittently<\/h1>\nshow_every = 400\n<h1>iteration hyperparameters<\/h1>\noptimizer = optim.Adam([target], lr=0.003)\nsteps = 2000  # decide how many iterations to update your image (5000)\nfor ii in range(1, steps+1):\n# get the features from your target image\ntarget_features = get_features(target, vgg)\n                               \n# content loss\ncontent_loss = torch.mean((target_features[content_layer] - content_features[content_layer])**2)\n\n# photorealistic regularization loss\nreg_loss, reg_grad = PhotorealismRegularization(content.detach().to(\"cpu\").numpy().transpose(0, 2, 3, 1).squeeze().clip(0, 1), target.detach().to(\"cpu\").numpy().transpose(0, 2, 3, 1).squeeze().clip(0, 1))\n\nreg_grad_tensor = image_to_tensor(reg_grad).to(device)\n\n#target.grad += reg_grad_tensor\n\n# augmented style loss\n#style_loss = augmented_style_loss(target, vgg, content_masks, style_masks)\n\nlayers = {\n    '0':'conv1_1',\n    '5':'conv2_1',\n    '10':'conv3_1',\n    '19':'conv4_1',\n    '28':'conv5_1'\n}\n\nstyle_loss = 0.\nx = target.clone().to(device)\n\n# model._modules is a dictionary holding each module in the model\nfor name, layer in vgg._modules.items():\n    x = layer(x)\n    if isinstance(layer, torch.nn.MaxPool2d):\n        style_masks = [layer(mask) for mask in style_masks]\n        content_masks = [layer(mask) for mask in content_masks]\n      \n    if name in layers:\n        target_gram = [gram_matrix(x * Variable(mask.to(device), requires_grad = True)) for mask in content_masks]\n        style_gram = [gram_matrix(x * Variable(mask.to(device), requires_grad = True)) for mask in style_masks]\n        \n        layer_loss = style_weights[layers[name]] * sum(F.mse_loss(t_gram, s_gram) for t_gram, s_gram in zip(target_gram, style_gram))\n        \n        target_feature = target_features[layers[name]]\n        _, d, h, w = target_feature.shape\n        \n        style_loss += layer_loss \/ (d * h * w)\n\n\n# total loss\ntotal_loss = content_weight * content_loss + style_weight * style_loss \n\n# update target image\noptimizer.zero_grad()\ntotal_loss.backward()\noptimizer.step()\n\nKindly let me know where I am going wrong and how to fix it.\nThanks.","y":"Hi,\nYou can try adding this at the beginning of your code:\n<code class=\"lang-python\">\nfor mod in modulelist:\n    if hasattr(mod, \"inplace\"):\n        print(mod)\n        mod.inplace=False\n<\/code>\nAll the ReLU in vgg are inplace  And they modify your img1\/img2 inplace.","z":"The error comes from the fact that a values that ia needed to compute gradients has been changed inplace.\nBy looking quickly at your code, you might want to change style_loss += xxx by style_loss = style_loss + xxx. The first one is inplace, the second one is not.\nThanks albanD.\nI did the change as suggested. However, I still get the error.\nInteresting.\nThere must be another place in your code where you do inplace operations. By setting a subtensor with x[smth] =  or using +=  or using inplace pytorch ops (usually with a trailing underscore like mul_() )\nApart from that method, only one method is used. There has to be a problem somewhere here. None of those three types of inplace operations have been used.\ndef gram_matrix(tensor):\n\u201c\u201d\" Calculate the Gram Matrix of a given tensor\nGram Matrix: https:\/\/en.wikipedia.org\/wiki\/Gramian_matrix\n\u201c\u201d\"\n# get the batch_size, depth, height, and width of the Tensor\n_, d, h, w = tensor.size()\n\n# reshape so we're multiplying the features for each channel\ntensor = tensor.view(d, h * w)\n\n# calculate the gram matrix\ngram = torch.mm(tensor, tensor.t())\n\nreturn gram\n\nPlease help me. I am stuck on this for a long time now.\nHi,\nCould you please post a self contained code sample using triple backtick ``` before and after it to format it properly. So that we can reproduce the issue locally and investigate it further?\n<code class=\"lang-auto\">import torch\nimport torch.optim as optim\nimport torch.nn.functional as F\nfrom torchvision import transforms, models\nfrom torch.autograd import Variable\n\nfrom PIL import Image\nimport matplotlib.pyplot as plt\nimport numpy as np\nfrom skimage.transform import resize\n\ndevice = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n\ndef im_convert(tensor):\n  \n    image = tensor.to(\"cpu\").clone().detach()\n    image = image.numpy().squeeze()\n    image = image.transpose(1, 2, 0) # B G R\n    # undo normalizations\n    image = image * np.array((0.229, 0.224, 0.225)) + np.array((0.485, 0.456, 0.406)) \n    image = image.clip(0, 1)\n\n    return image\n\ndef load_image(path, max_size = 400, shape = None):\n    ''' Load and transform image and make sure the image <= 400\n    pixels in the x-y dims. '''\n\n    image = Image.open(path).convert('RGB')\n\n    size = max_size if (max(image.size) > max_size) else max(image.size)\n\n    if shape is not None:\n        size = shape\n\n\n    in_transform = transforms.Compose([\n                        transforms.Resize(size),\n                        transforms.ToTensor(),\n                        transforms.Normalize((0.485, 0.456, 0.406),\n                                            (0.229, 0.224, 0.225))\n    ])\n\n    # discard the transparent, alpha channel(:3) and add the batch dimension\n    image = in_transform(image)[:3, :, :].unsqueeze(0)\n\n    return image\n\ndef image_to_tensor(x):\n    \"\"\"\n    Transforms np.array to torch.Tensor\n        (W, H)       -> (1, 1, W, H)\n        (W, H, C)    -> (1, C, W, H)\n        (B, W, H, C) -> (B, C, W, H)\n    \"\"\"\n    if x.ndim == 2:\n        return torch.Tensor(x).unsqueeze(0).unsqueeze(0)\n    if x.ndim == 3:\n        return torch.Tensor(x.transpose(2, 0, 1)).unsqueeze(0)\n    if x.ndim == 4:\n        return torch.Tensor(x.transpose(0, 3, 1, 2))\n    raise RuntimeError(\"np.array's ndim is out of range 2, 3 or 4.\")\n    \n    \ndef extract_masks(segment):\n    \"\"\"\n    Extracts the segmentation masks from the segmentated image.\n    Allowed colors are:\n        blue, green, black, white, red,\n        yellow, grey, light_blue, purple.\n    \"\"\"\n    extracted_colors = []\n\n    # BLUE\n    mask_r = segment[..., 0] < 0.1\n    mask_g = segment[..., 1] < 0.1\n    mask_b = segment[..., 2] > 0.9\n    mask = mask_r &amp; mask_g &amp; mask_b\n    extracted_colors.append(mask)\n\n    # GREEN\n    mask_r = segment[..., 0] < 0.1\n    mask_g = segment[..., 1] > 0.9\n    mask_b = segment[..., 2] < 0.1\n    mask = mask_r &amp; mask_g &amp; mask_b\n    extracted_colors.append(mask)\n\n    # BLACK\n    mask_r = segment[..., 0] < 0.1\n    mask_g = segment[..., 1] < 0.1\n    mask_b = segment[..., 2] < 0.1\n    mask = mask_r &amp; mask_g &amp; mask_b\n    extracted_colors.append(mask)\n\n    # WHITE\n    mask_r = segment[..., 0] > 0.9\n    mask_g = segment[..., 1] > 0.9\n    mask_b = segment[..., 2] > 0.9\n    mask = mask_r &amp; mask_g &amp; mask_b\n    extracted_colors.append(mask)\n\n    # RED\n    mask_r = segment[..., 0] > 0.9\n    mask_g = segment[..., 1] < 0.1\n    mask_b = segment[..., 2] < 0.1\n    mask = mask_r &amp; mask_g &amp; mask_b\n    extracted_colors.append(mask)\n\n    # YELLOW\n    mask_r = segment[..., 0] > 0.9\n    mask_g = segment[..., 1] > 0.9\n    mask_b = segment[..., 2] < 0.1\n    mask = mask_r &amp; mask_g &amp; mask_b\n    extracted_colors.append(mask)\n\n    # GREY\n    mask_r = (segment[..., 0] > 0.4) &amp; (segment[..., 0] < 0.6)\n    mask_g = (segment[..., 1] > 0.4) &amp; (segment[..., 1] < 0.6)\n    mask_b = (segment[..., 2] > 0.4) &amp; (segment[..., 2] < 0.6)\n    mask = mask_r &amp; mask_g &amp; mask_b\n    extracted_colors.append(mask)\n\n    # LIGHT_BLUE\n    mask_r = segment[..., 0] < 0.1\n    mask_g = segment[..., 1] > 0.9\n    mask_b = segment[..., 2] > 0.9\n    mask = mask_r &amp; mask_g &amp; mask_b\n    extracted_colors.append(mask)\n\n    # PURPLE\n    mask_r = segment[..., 0] > 0.9\n    mask_g = segment[..., 1] < 0.1\n    mask_b = segment[..., 2] > 0.9\n    mask = mask_r &amp; mask_g &amp; mask_b\n    extracted_colors.append(mask)\n\n    return extracted_colors\n\n\ndef get_all_masks(path):\n    \"\"\"\n    Returns the segmentation masks from the segmentated image.\n    \"\"\"\n    image = Image.open(path)\n    np_image = np.array(image, dtype=np.float) \/ 255\n    return extract_masks(np_image)\n\n\ndef is_nonzero(mask, thrs=0.01):\n    \"\"\"\n    Checks segmentation mask is dense.\n    \"\"\"\n    return np.sum(mask) \/ mask.size > thrs\n\n\ndef get_masks(path_style, path_content):\n    \"\"\"\n    Returns the meaningful segmentation masks.\n    Avoides \"orphan semantic labels\" problem.\n    \"\"\"\n    masks_style = get_all_masks(path_style)\n    masks_content = get_all_masks(path_content)\n\n    non_zero_masks = [\n        is_nonzero(mask_c) and is_nonzero(mask_s)\n        for mask_c, mask_s in zip(masks_content, masks_style)\n    ]\n\n    masks_style = [mask for mask, cond in zip(masks_style, non_zero_masks) if cond]\n    masks_content = [mask for mask, cond in zip(masks_content, non_zero_masks) if cond]\n\n    return masks_style, masks_content\n\n\ndef resize_masks(masks_style, masks_content, size):\n    \"\"\"\n    Resizes masks to given size.\n    \"\"\"\n    resize_mask = lambda mask: resize(mask, size, mode=\"reflect\")\n\n    masks_style = [resize_mask(mask) for mask in masks_style]\n    masks_content = [resize_mask(mask) for mask in masks_content]\n\n    return masks_style, masks_content\n\n\ndef masks_to_tensor(masks_style, masks_content):\n    \"\"\"\n    Transforms masks to torch.Tensor from np.array.\n    \"\"\"\n    masks_style = [image_to_tensor(mask) for mask in masks_style]\n    masks_content = [image_to_tensor(mask) for mask in masks_content]\n\n    return masks_style, masks_content\n\n\ndef masks_loader(path_style, path_content, size):\n    \"\"\"\n    Loads masks.\n    \"\"\"\n    style_masks, content_masks = get_masks(path_style, path_content)\n    style_masks, content_masks = resize_masks(style_masks, content_masks, size)\n    style_masks, content_masks = masks_to_tensor(style_masks, content_masks)\n\n    return style_masks, content_masks\n\ndef get_features(image, model, layers=None):\n    \"\"\" Run an image forward through a model and get the features for \n          a set of layers. Default layers are for VGGNet matching Gatys et al (2016)\n    \"\"\"\n\n    if layers is None:\n        layers = {\n            '0':'conv1_1',\n            '5':'conv2_1',\n            '10':'conv3_1',\n            '19':'conv4_1',\n            '21':'conv4_2',\n            '28':'conv5_1'\n        }\n\n    features = {}\n    x = image\n    # model._modules is a dictionary holding each module in the model\n    for name, layer in model._modules.items():\n        x = layer(x)\n          \n        if name in layers:\n            features[layers[name]] = x\n\n    return features\n\ndef gram_matrix(tensor):\n    \"\"\" Calculate the Gram Matrix of a given tensor \n        Gram Matrix: https:\/\/en.wikipedia.org\/wiki\/Gramian_matrix\n    \"\"\"\n\n    # get the batch_size, depth, height, and width of the Tensor\n    _, d, h, w = tensor.size()\n\n    # reshape so we're multiplying the features for each channel\n    tensor = tensor.view(d, h * w)\n\n    # calculate the gram matrix\n    gram = torch.mm(tensor, tensor.t())\n\n    return gram\n\ndef PhotorealismRegularization(image, target):\n    L = compute_laplacian(image)\n    grad = L.dot(target.reshape(-1, 3))\n    loss = np.sum(target.reshape(3, -1).dot(grad))\n    return torch.tensor(loss), 2. * grad.reshape(*target.shape)\n\nimport io\n\nstyle_path = io.BytesIO(uploaded['tar4.png'])\ncontent_path = io.BytesIO(uploaded['in4.png'])\n\nstyle_mask_path = io.BytesIO(uploaded['seg_tar4.png'])\ncontent_mask_path = io.BytesIO(uploaded['seg_in4.png'])\n\nvgg = models.vgg19(pretrained = True).features\n\nfor param in vgg.parameters():\n    param.requires_grad_(False)\n\nvgg.to(device)\n\n# load content and style\ncontent = load_image(content_path).to(device)\n# resize style to match content\nstyle = load_image(style_path, shape = content.shape[-2:]).to(device)\n\n# load the masks\nstyle_masks, content_masks = masks_loader(\n    style_mask_path,\n    content_mask_path,\n    content.shape[-2:])\n\n# get content and style features\ncontent_features = get_features(content, vgg)\nstyle_features = get_features(style, vgg)\n\n# get the gram matrices\nstyle_grams = {layer : gram_matrix(style_features[layer]) for layer in style_features}\n\n# get the target image\ntarget = content.clone().requires_grad_(True).to(device)\n\nstyle_weights = {'conv1_1': 0.2,\n                 'conv2_1': 0.2,\n                 'conv3_1': 0.2,\n                 'conv4_1': 0.2,\n                 'conv5_1': 0.2}\n\ncontent_weight = 1  # Alpha\nstyle_weight = 1e2  # Gamma\nphotorealismRegularization_weight = 1e4 # lambda\n\n# constants\ncontent_layer = 'conv4_2'\n\n# for displaying the target image, intermittently\nshow_every = 400\n\n# iteration hyperparameters\noptimizer = optim.Adam([target], lr=0.003)\nsteps = 2000  # decide how many iterations to update your image (5000)\n\nfor ii in range(1, steps+1):\n    \n    # get the features from your target image\n    target_features = get_features(target, vgg)\n                                   \n    # content loss\n    content_loss = torch.mean((target_features[content_layer] - content_features[content_layer])**2)\n    \n    # photorealistic regularization loss\n    reg_loss, reg_grad = PhotorealismRegularization(content.detach().to(\"cpu\").numpy().transpose(0, 2, 3, 1).squeeze().clip(0, 1), target.detach().to(\"cpu\").numpy().transpose(0, 2, 3, 1).squeeze().clip(0, 1))\n    \n    reg_grad_tensor = image_to_tensor(reg_grad).to(device)\n    \n    #target.grad += reg_grad_tensor\n    \n    # augmented style loss\n    #style_loss = augmented_style_loss(target, vgg, content_masks, style_masks)\n    \n    layers = {\n        '0':'conv1_1',\n        '5':'conv2_1',\n        '10':'conv3_1',\n        '19':'conv4_1',\n        '28':'conv5_1'\n    }\n\n    style_loss = 0\n    x = target.clone().to(device)\n    \n    # model._modules is a dictionary holding each module in the model\n    for name, layer in vgg._modules.items():\n        x = layer(x)\n        if isinstance(layer, torch.nn.MaxPool2d):\n            style_masks = [layer(mask) for mask in style_masks]\n            content_masks = [layer(mask) for mask in content_masks]\n          \n        if name in layers:\n            target_gram = [gram_matrix(x * Variable(mask.to(device), requires_grad = True)) for mask in content_masks]\n            style_gram = [gram_matrix(x * Variable(mask.to(device), requires_grad = True)) for mask in style_masks]\n            \n            layer_loss = style_weights[layers[name]] * sum(F.mse_loss(t_gram, s_gram) for t_gram, s_gram in zip(target_gram, style_gram))\n            \n            target_feature = target_features[layers[name]]\n            _, d, h, w = target_feature.shape\n            \n            layer_loss = layer_loss \/ (d * h * w)\n            style_loss = style_loss + layer_loss\n\n    \n    # total loss\n    total_loss = content_weight * content_loss + style_weight * style_loss + photorealismRegularization_weight * reg_loss\n\n    # update target image\n    optimizer.zero_grad()\n    total_loss.backward()\n    optimizer.step()\n\n    # display intermediate images and print the loss\n    if ii % show_every == 0:\n        print('Total loss: ', total_loss.item())\n        plt.imshow(im_convert(target))\n        plt.show()\n\n<\/code>\nHi!\nDid this ever get solved? I have basically the exact same issue when trying to calculate the Gram Matrix. I\u2019m using anomaly detection and it is pointing me to the matrix multiplication or transpose operator\u2026\nThis is how I calculate the gram matrix:\n<code class=\"lang-python\">def gram_matrix(activation, N, M):\n    feature_map = activation.reshape((N, M))\n    G = torch.mm(feature_map, feature_map.t())\n    return G\n<\/code>\nTry using activation = activation.view(N,M) instead of reshape\nLet me know if that does the trick.\nIt sadly, does not and the error is still presumably on this line:  G = torch.mm(feature_map, feature_map.t())\nI did try to use  matmul()  instead but with the same result. I also tried to use other functions, but still with the same variables, and then it suddenly worked. So it really seems like the error is in  mm() \u2026\nCan you specify how you extract the activation?\nSure! Here\u2019s my code\n<code class=\"lang-python\">import numpy as np\nimport torch\nfrom torchvision import models\n\nvgg = models.vgg19(pretrained=True)\nmodulelist = list(vgg.features.modules())\nlayer_inds = [0, 2]  # model layers to extract features from\n\n\ndef gram_matrix(activation, N, M):\n    feature_map = activation.view(N, M)\n    G = torch.mm(feature_map, feature_map.t())  # <-- Something's fishy here\n    return G\n\n\ndef render(a, b, c):\n    \"\"\"Random render function, just want to generate some image...\"\"\"\n    lin = torch.linspace(0, 1, 224)\n    X, Y = torch.meshgrid(lin, lin)\n    grid = torch.stack((X, Y, torch.zeros_like(X)))\n    grid = grid * torch.stack((a, b, torch.tensor(1.0))).view(3, 1, 1)\n    grid = (torch.sin(grid)) + 1.0 \/ 2.0\n\n    return grid * c.view(3, 1, 1)\n\n\n# Create ground truth\nPt = [torch.tensor(1.0), torch.tensor(50.), torch.tensor((0.5, 0.6, 0.7))]\ntruth = render(*Pt).unsqueeze(0)\nw = torch.tensor([1e9, 1e9])\n\n\ndef gram_loss(img1, *params):\n    img2 = render(*params).unsqueeze(0)\n    E = []\n\n    for i, layer in enumerate(modulelist[1:]):\n        img1 = layer(img1)\n        img2 = layer(img2)\n        if i > layer_inds[-1]:\n            break\n        if i in layer_inds:\n            N = img1.shape[1]\n            M = img1.shape[2] * img1.shape[3]\n            G1 = gram_matrix(img1, N, M)\n            G2 = gram_matrix(img2, N, M)\n\n            E.append(torch.sum(torch.pow(G1 - G2, 2)) * (1 \/ (4. * (N ** 2) * (M ** 2))))\n\n    L_tot = (torch.stack(E) * w).sum()\n    return L_tot\n\n\ndef run_gd():\n    # Generate random starting parameters\n    P = [torch.tensor(np.random.uniform(0, 1), requires_grad=True, dtype=torch.float32),\n         torch.tensor(np.random.uniform(0, 50), requires_grad=True, dtype=torch.float32),\n         torch.tensor(np.random.uniform(0, 1, size=(3,)), requires_grad=True, dtype=torch.float32)]\n\n    optim = torch.optim.Adam(P, lr=0.01)\n\n    for i in range(100):\n        with torch.autograd.set_detect_anomaly(True):\n            optim.zero_grad()\n            loss = gram_loss(truth, *P)\n            print(\"{}. Loss: {:.6f}\".format(i + 1, loss.detach().numpy()))\n            loss.backward()\n            optim.step()\n\n\nif __name__ == '__main__':\n    run_gd()  # Run gradient descent\n<\/code>\nThis is part of a bigger project to create a differentiable rendering process, hence the render() function and gradient descent. Also, this loss algorithm is based on this paper: https:\/\/arxiv.org\/pdf\/1505.07376.pdf\n\n\n\n Zorobay:\n\n<code class=\"lang-auto\">grid = torch.stack((X.clone(), Y.clone(), torch.zeros_like(X)))\n    grid = grid * torch.stack((a.clone(), b.clone(), torch.tensor(1.0))).view(3, 1, 1)\n<\/code>\n\n\nGive the above modifications a try. Let me know if this was the inplace operation\nNo sorry, that didn\u2019t change anything \n\n\n\n\nEncounter the RuntimeError: one of the variables needed for gradient computation has been modified by an inplace operation with link \"https:\/\/discuss.pytorch.org\/t\/encounter-the-runtimeerror-one-of-the-variables-needed-for-gradient-computation-has-been-modified-by-an-inplace-operation\/836\/5\"\n\n\n    x = x + 1 is not in-place, because it takes the objects pointed to by x, creates a new Variable, adds 1 to x putting the result in the new Variable, and overwrites the object referenced by x to point to the new var. There are no in-place modifications, you only change Python references (you can check that id(x) is different before and after that line). \nOn the other hand, doing x += 1 or x[0] = 1 will modify the data of the Variable in-place, so that no copy is done. However some functions (in y\u2026\n  \n\nCheck this. if it helps. Give me some time to go through your code. There is some niche way you are changing a tensor inplace.\nI knew about this, but can\u2019t find any +=or x[0] = ... in my code. Thanks for all the help!\nHi,\nYou can try adding this at the beginning of your code:\n<code class=\"lang-python\">\nfor mod in modulelist:\n    if hasattr(mod, \"inplace\"):\n        print(mod)\n        mod.inplace=False\n<\/code>\nAll the ReLU in vgg are inplace  And they modify your img1\/img2 inplace.\nAwesome! Thank you  That did it!"},{"x":"Hi,\nI have two convolutions which have the same shared weight. One is using dilation=1, the other is using dilation=2.\nLet x be an input tensor, and w the shared weight.\nIs there anything I can do to save memory when calculating:\n<code class=\"lang-python\">y = F.conv2d(x, weight=w, dilation=1) + F.conv2d(x, weight=w, dilation=2)\n<\/code>\nand doing the backward pass?\nRight now the memory consumption is doubled which feels unnecessary.","y":"Hi,\nThe memory consumption is doubled most likely because both outputs are saved.\nIndeed, to compute the backward of the convolution, we need to know what was the output. So if you do two of them, we need to know both outputs.","z":"Hi,\nThe memory consumption is doubled most likely because both outputs are saved.\nIndeed, to compute the backward of the convolution, we need to know what was the output. So if you do two of them, we need to know both outputs."},{"x":"Hi,\nI am trying to record the values of the gradients as they propagate in time through an RNN. Initially I thought this would be easily accomplished using  the register_hook function by calling it to each parameter, yet I now realize that for some reason, the hook is not called at each step in time, contrary to what I had understood. Take for example the following code:\n<code class=\"lang-python\">import torch\n\nx = torch.randn((1, 1))\nw = torch.ones((2, 1), requires_grad=1)\n\nz = w * x\nz *= w\nz *= w\nz *= w\n\ndef print_grad(grad):\n    print(grad)\n\nh = z.register_hook(print_grad)\nz.sum().backward()\n\n<\/code>\nThis produces as output:\n<code class=\"lang-bash\">tensor([[1.],  [1.]])\n<\/code>\nInstead of the same, but repeated 4 times.\nWhat am I missing here? If nothing, is there a way to record the gradients at each point in time?\nThanks in advance!","y":"The hook will give you the gradient at the point where the hook is registered.\nIf you want to get it after every computation, you need to register one after every computation.","z":"The hook will give you the gradient at the point where the hook is registered.\nIf you want to get it after every computation, you need to register one after every computation.\nOh, I see. I guess for feed-forward models that\u2019s good enough. But for recurrent networks it makes tracing gradients backward in time a bit hard.\nThanks for the answer!\nWell if you have a single call in a for-loop, you will still have one call to the hook per time it was applied. So that works for rnn as well \n<code class=\"lang-auto\">def print_grad(grad):\n    print(grad)\n\nfor i in range(4):\n  z *= w\n  z.register_hook(print_grad)\n<\/code>\nThat will register one hook per iteration of the loop.\nThat would certainly work, but I still have to modify the code in all the RNN models to save the gradients, which is not what I want. The recording and the model itself should be separate. It\u2019s also not what the documentation implies. Taken directly from there:\n<code class=\"lang-python\">def register_hook(self, hook):\n        r\"\"\"Registers a backward hook.\n\n        The hook will be called every time a gradient with respect to the\n        Tensor is computed. The hook should have the following signature::\n\n            hook(grad) -> Tensor or None\n...\n<\/code>\nWhich implies that it should be called at each time step, since we compute a gradient w.r.t. the parameters for each of them. Yet when I do this, I only get one gradient.\nMaybe someone want\u2019s the gradient for the whole sequence, but that can be obtained using the register_backward_hook method in the Module class.\nEvery time you do z = foo(z) in your RNN, you most likely create a new Tensor that is then assigned with the python variable named z. But the original Tensor contained in z and the new one have nothing in common. So if you add a hook to the second one, you won\u2019t get gradient for the first one.\nThis comment refers to the case where you call .bakward() multiple times.\nYes, but I am trying to compute the gradient w.r.t. the parameters, which are tensors that do not change and are repeated across time steps. Also I only call backwards one time. From what I read in the docs, this should give me a gradient for each time step and parameters (i.e. input and recurrent weights and biases), unless I have a bug or misunderstood something\u2026\nIf you use a Tensor multiple times, the gradient associated to it is the sum of the gradients corresponding to each use.\nIf you want to get the gradient for a single use, you need to make a temporary tensor corresponding to that single use.\nSo interestingly enough, the gradient returned in every time step by creating this hook is the same. I\u2019m guessing it\u2019s the sum total of all gradients of z over every time step. Can someone confirm this?\nIf the Tensor is the same, then yes.\nAnd just of curiosity, there\u2019s no way to extract the gradients for every time step separately (rather than deal with the sum) ?\nAs mentioned above, you can do that by having a different Tensor (with same content) for each step."},{"x":"Hi! Here I am with a (maybe) rather weird problem.\nSo, the big picture is that I am trying to use Pytorch\u2019s optimizers to perform  non-linear curve fitting. I have an overall code that is working, but now I need to tweek things to actually work with the model I am interested in.\nI will share here a simplified model, to make it easier to understand the issue.\nLet\u2019s consider this image:\n\nThe cyan curve is the \u201coriginal\u201d input. It known and measured.\nThe other 4 curves are obtained by scaling in aplitude and translating in time the first one. So our model has 2 parameters: onset time (i.e. temporal translation, ton) and  scaling (vp).\nTRUE VALUES\n<code class=\"lang-auto\">tensor([[ 5.0000,  0.5000],\n        [ 5.0000,  0.7500],\n        [15.0000,  0.5000],\n        [15.0000,  0.7500]])\n<\/code>\nESTIMATES\n<code class=\"lang-auto\">tensor([[0.0000, 0.4203],\n        [0.0000, 0.6305],\n        [0.0000, 0.3271],\n        [0.0000, 0.4906]])\n<\/code>\nAs you can see, the first parameter (ton) is not updated at all from its starting value. And that obviuosly affects also the estimate of the amplitude  (vp).\nI am quite sure that the way I coded my model is such that autograd is unable to compute ita gradient correctly. But I am kind of out of ideas, since I am just trying to port this code from Matlab to pytorch, and with a similar implementation Matlab is able to optimize the value of that parameter.\nHere is the whole code I am testing.  It should be a self contained working example.\nAny idea will be greatly appreciated!!\n<code class=\"lang-python\">import torch\nimport torch.nn as nn\ndevice = 'cuda' if torch.cuda.is_available() else 'cpu'\nprint(device)\nimport matplotlib.pyplot as plt\nfrom tqdm import tqdm\n\nclass Net(nn.Module):\n    def __init__(self, p, t, IF, y):\n        super().__init__()\n        self.pars = nn.Parameter(p)\n        self.t = t \n        self.IF = IF\n        self.y = y \n\n    def forward(self):\n        self.pars.detach()\n        return self.cost(self.pars)\n    \n    def cost(self,p):\n        out = kmodel(p,self.t,self.IF) - self.y\n        return torch.sum(out**2)\n\n################\ndef kmodel(p,t,Cp,device=0):\n    torch.autograd.set_detect_anomaly(True)\n    Nv = p.shape[0]\n    Nt = t.shape[1]\n    \n    ton = p[:,0].view(Nv,1).int()\n    vp  = p[:,1].view(Nv,1).to(device)\n    \n    # this is how I tried to introduce the temporal shift\n    # and here is where I probably lost Autograd ...\n    for i in range(Nv):\n        Cp[i,ton[i]:] = Cp[i,:Nt-ton[i]]\n        Cp[i,:ton[i]] = 0\n\n    out = vp*Cp\n    return out\n\n##################\n# Simulate data\n##################\n\nn = 2\np0 = torch.linspace(5, 15, n) # ton\np1 = torch.linspace(0.5, 0.75, n) # vp\np_true = []\nfor i in range(n):\n    for j in range(n):\n                p_true.append([p0[i], p1[j]])\n        \np_true = torch.tensor(p_true).to(device)     \nNv = p_true.shape[0]\nprint(Nv)\n\nIF = torch.tensor([[0.0309, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0220, 0.0000, 0.0000,\n         0.0074, 0.0000, 0.3594, 2.0850, 2.6615, 2.7160, 1.8999, 1.3524, 1.5733,\n         1.8137, 1.7839, 1.4743, 1.4680, 1.4860, 1.3490, 1.3471, 1.3055, 1.3520,\n         1.2053, 1.3447, 1.2073, 1.2478, 1.3872, 1.2890, 1.1951, 1.1463, 1.2195,\n         1.1701, 1.1634, 1.1765, 1.0766, 1.2907, 1.1409, 1.0373, 1.0479, 1.0766,\n         1.0842, 1.0371, 1.0241, 1.0439, 0.9803, 0.9966, 1.1955, 0.9665, 0.9679,\n         1.0223, 0.9714, 0.9415, 1.0756, 1.0336, 0.9744, 0.9825, 1.0016, 0.9592,\n         0.8908, 0.9362, 0.9347, 0.9198, 0.9353, 0.9279, 1.0776]]).to(device)   \nIF = torch.repeat_interleave(IF, repeats=Nv, dim=0).to(device)\n\nt = torch.unsqueeze(torch.linspace(0, 2400*150\/1000\/60, 70), dim=0).to(device)\nt = torch.repeat_interleave(t, repeats=Nv, dim=0)\n\ny_true = kmodel(p_true,t.clone(),IF.clone()).to(device)\ny = y_true + 0.00*torch.randn(y_true.size()).to(device)\n\nplt.figure()\nplt.plot(t.cpu().numpy()[0,:].T,IF.cpu().numpy().T,'-*', color='C9')\nplt.plot(t.cpu().numpy()[:16,:].T,y.cpu().numpy()[:16,:].T);\nplt.show()\n\n##################\n# Optimization\n##################\n\ndef loss(y_pred, y_true):\n    return y_pred\n\ntraining_steps = 10000\nlr = 0.01\np = torch.tensor([[0.0,0.0]]*Nv)\nmodel = Net(p,t, IF, y)\n\nwith tqdm(total=training_steps) as pbar:\n    for epoch in range(training_steps ):\n        # trying to change the value of the learning rate \n        # it didn't really help ...\n        if epoch % 1000 == 0:\n            lr *= 0.1\n            optim = torch.optim.Adamax(model.parameters(), lr = lr)\n        optim.zero_grad()\n        ypred = model()\n        ll = loss(ypred, _)\n        ll.backward()\n        optim.step()\n        \n        # progress bar settings\n        pbar.set_postfix(loss=ll.item(),p_est=list(model.parameters())[0].data[0], lr = lr, refresh=False)\n        pbar.update(1)\n        if ll.item() <= 1e-4:\n            break\n\np_est = list(model.parameters())[0].data\nprint(p_est)\nprint(p_true)\n\ny_est = kmodel(p_est,t,IF)\n\nplt.figure(figsize=(20,10))\nplt.plot(t.cpu().numpy()[0,:].T,IF.cpu().numpy().T, color='C4');\nplt.plot(t.cpu().numpy()[0:16,:].T,y.cpu().numpy()[0:16,:].T, '-*',color='C1');\nplt.plot(t.cpu().numpy()[0:16,:].T,y_est.cpu().numpy()[0:16,:].T, color='C2');\n\n<\/code>","y":"Thank you sooo much for your time and effort!\nI was reading about grid_sample right when I got the notification about your message! ^^\nThis version of the model (teeny tiny change wrt to your suggestion) seems to work (and converges super fast!!:\n<code class=\"lang-python\">def kmodel(p,t,Cp,device=0):\n    torch.autograd.set_detect_anomaly(True)\n    Nv = p.shape[0]\n    Nt = t.shape[1]\n    \n    ton = p[:,0].clone()\n    vp  = p[:,1].clone().view(Nv,1).to(device)\n        \n    Cp = Cp.unsqueeze(-1).unsqueeze(1)\n    affine = torch.zeros(Nv, 2, 3)\n    # The affine transformation we want is identity + translation\n    affine[:, 0, 0] = 1\n    affine[:, 1, 1] = 1\n    affine[:, 1, 2] = -ton #-2 * ton \/ Nt\n    \n    grid = F.affine_grid(affine, (Nv, 1, Nt, 1))#, align_corners=True)\n    Cp = F.grid_sample(Cp, grid) #, align_corners=True)\n    Cp = Cp.squeeze(1).squeeze(-1)\n\n    out = vp*Cp\n    return out\n<\/code>\nNow I need to check if I can grow this toy-model to what I actually need to fit, but I am really optimistic now! ","z":"Hi,\nTrying to run your code on cpu, I get:\n<code class=\"lang-auto\">Traceback (most recent call last):\n  File \"foo.py\", line 71, in <module>\n    y_true = kmodel(p_true,t.clone(),IF.clone())\n  File \"foo.py\", line 36, in kmodel\n    Cp[i,ton[i]:] = Cp[i,:Nt-ton[i]]\nRuntimeError: unsupported operation: some elements of the input tensor and the written-to tensor refer to a single memory location. Please clone() the tensor before performing the operation.\n<\/code>\nThat might be a problem \nI tried  to modify my model function with a .clone() at the line you suggested, but with no change:\n<code class=\"lang-auto\">def kmodel(p,t,Cp,device=0):\n    torch.autograd.set_detect_anomaly(True)\n    Nv = p.shape[0]\n    Nt = t.shape[1]\n    \n    ton = p[:,0].view(Nv,1).int()\n    vp  = p[:,1].view(Nv,1).to(device)\n    \n    for i in range(Nv):\n        Cp[i,ton[i]:] = Cp[i,:Nt-ton[i]].clone()  # <==========\n        Cp[i,:ton[i]] = 0\n\n    out = vp*Cp\n    return out\n<\/code>\nBut even if I run the code with device='cpu', I don\u2019t get that warning\/error \u2026\nFor the other issue of not learning, the problem is that your functions is not differentiable wrt to the shift as you use it. Here you use it as an integer index. So pytorch cannot compute gradient for it.\nWhat kind of gradients were you expecting here?\nFor the absence of warning\/error, it might be because your run an old version of pytorch? I ran the code from the latest nightly build.\n(Assuming I have not simplified my problem too much to actually benefit from a solution) the model should be something like:\ny_est[ t ] = vp * y[t -  ton]\nton doesn\u2019t need to be integer (actually it should be a float!) but it seems I am not able to find a way to make it work. I also tried to pass ton as a float, and then to \u201cfind\u201d the index with a value of the time vector closest to ton, but even that solution gave the same result\n\nFor the absence of warning\/error, it might be because your run an old version of pytorch? I ran the code from the latest nightly build.\n\nYes, unfortunately I am forced to run v1.1 because of issue with the nvidia cuda diver on my work pc \u2026 :\\\nOne solution might be to reuse code from the spatial transformer:\n<code class=\"lang-python\">    # Make it look like a 2D data: (batch, chan, h, w) as (batch_size, 1, Cp.size(1), 1)\n    Cp = Cp.unsqueeze(-1).unsqueeze(1)\n    affine_transfo = torch.zeros(batch_size, 2, 3)\n    # The affine transformation we want is identity + translation\n    affine_transfo[:, 0, 0] = 1\n    affine_transfo[:, 1, 1] = 1\n    affine_transfo[:, 1, 2] = -2 * ton \/ height\n    grid = F.affine_grid(affine_transfo, (batch_size, 1, height, 1), align_corners=True)\n    Cp = F.grid_sample(Cp, grid, align_corners=True)\n    Cp = Cp.squeeze(1).squeeze(-1)\n<\/code>\nThat will get you gradients to the parameters. But not sure how good it\u2019s gonna be.\nAlso I am absolutely not sure how the align_corners flag should be used. So you should double checkt that this is doing what you want \nThank you sooo much for your time and effort!\nI was reading about grid_sample right when I got the notification about your message! ^^\nThis version of the model (teeny tiny change wrt to your suggestion) seems to work (and converges super fast!!:\n<code class=\"lang-python\">def kmodel(p,t,Cp,device=0):\n    torch.autograd.set_detect_anomaly(True)\n    Nv = p.shape[0]\n    Nt = t.shape[1]\n    \n    ton = p[:,0].clone()\n    vp  = p[:,1].clone().view(Nv,1).to(device)\n        \n    Cp = Cp.unsqueeze(-1).unsqueeze(1)\n    affine = torch.zeros(Nv, 2, 3)\n    # The affine transformation we want is identity + translation\n    affine[:, 0, 0] = 1\n    affine[:, 1, 1] = 1\n    affine[:, 1, 2] = -ton #-2 * ton \/ Nt\n    \n    grid = F.affine_grid(affine, (Nv, 1, Nt, 1))#, align_corners=True)\n    Cp = F.grid_sample(Cp, grid) #, align_corners=True)\n    Cp = Cp.squeeze(1).squeeze(-1)\n\n    out = vp*Cp\n    return out\n<\/code>\nNow I need to check if I can grow this toy-model to what I actually need to fit, but I am really optimistic now! \n\n\n\n albanD:\n\nAlso I am absolutely not sure how the align_corners flag should be used. So you should double checkt that this is doing what you want\n\n\nIn pytorch 1.1 I don\u2019t have that input parameter, so I just removed it and it didn\u2019t complain \u2026 "},{"x":"<h1>Context<\/h1>\nI am trying to use Pytorch\u2019s optimizers to perform  non-linear curve fitting . I have an overall code that is working. Here it comes:\n<code class=\"lang-auto\">import torch\nimport torch.nn as nn\nimport torch.nn.functional as F\ndevice = 'cuda' if torch.cuda.is_available() else 'cpu'\nprint(device)\nimport matplotlib.pyplot as plt\n\nclass Net(nn.Module):\n    def __init__(self, p, IF, y, device):\n        super().__init__()\n        self.device = device\n        self.pars   = nn.Parameter(p.to(self.device))\n        self.IF     = IF.to(self.device)\n        self.y      = y.to(self.device)\n\n    def forward(self):\n        self.pars.detach()\n        return self.loss(self.pars)\n    \n    def loss(self,p):\n        out = kmodel(p,self.IF,self.device) - self.y\n        out = torch.sum((out)**2, dim=1)\n        return torch.mean(out)\n\n################\ndef kmodel(p,Cp,device=0):\n    torch.autograd.set_detect_anomaly(True)\n    Nv = p.shape[0]\n    Nt = Cp.shape[1]\n    \n    ton = p[:,0].clone()\n    vp  = p[:,1].clone().view(Nv,1).to(device)\n        \n    Cp = Cp.unsqueeze(-1).unsqueeze(1)\n    affine = torch.zeros(Nv, 2, 3)\n    # The affine transformation we want is identity + translation\n    affine[:, 0, 0] = 1\n    affine[:, 1, 1] = 1\n    affine[:, 1, 2] = -ton #-2 * ton \/ Nt\n    \n    grid = F.affine_grid(affine, (Nv, 1, Nt, 1))#, align_corners=True)\n    Cp = F.grid_sample(Cp, grid) #, align_corners=True)\n    Cp = Cp.squeeze(1).squeeze(-1)\n\n    out = vp*Cp\n    return out\n\n# ================== SIMULATE SOME DATA ==================\nn = 5\np0 = torch.linspace(.1, 0.5, n) # ton\np1 = torch.linspace(0.5, 0.75, n) # vp\np_true = []\nfor i in range(n):\n    for j in range(n):\n                p_true.append([p0[i], p1[j]])\n        \np_true = torch.tensor(p_true).to(device)     \nNv = p_true.shape[0]\nprint(Nv)\n\nIF = torch.tensor([[0.0309, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0220, 0.0000, 0.0000,\n         0.0074, 0.0000, 0.3594, 2.0850, 2.6615, 2.7160, 1.8999, 1.3524, 1.5733,\n         1.8137, 1.7839, 1.4743, 1.4680, 1.4860, 1.3490, 1.3471, 1.3055, 1.3520,\n         1.2053, 1.3447, 1.2073, 1.2478, 1.3872, 1.2890, 1.1951, 1.1463, 1.2195,\n         1.1701, 1.1634, 1.1765, 1.0766, 1.2907, 1.1409, 1.0373, 1.0479, 1.0766,\n         1.0842, 1.0371, 1.0241, 1.0439, 0.9803, 0.9966, 1.1955, 0.9665, 0.9679,\n         1.0223, 0.9714, 0.9415, 1.0756, 1.0336, 0.9744, 0.9825, 1.0016, 0.9592,\n         0.8908, 0.9362, 0.9347, 0.9198, 0.9353, 0.9279, 1.0776]]).to(device)   \nCp = torch.repeat_interleave(IF, repeats=Nv, dim=0).to(device)\n\n%time\ny_true = kmodel(p_true,Cp.clone(),device=device).to(device)\ny = y_true + 0.00*torch.randn(y_true.size()).to(device)\n\nplt.plot(IF.cpu().numpy().T,'-*', color='C9')\nplt.plot(y.cpu().numpy()[:16,:].T);\n\n# ================== OPTIMIZE ==================\n\nfrom tqdm.notebook import tqdm\n\nl = []\ntraining_steps = 2000\nlr = 0.01\n\nCp = torch.repeat_interleave(IF.clone(), repeats=Nv, dim=0)\npar = torch.tensor([[0.1,0.1]]*Nv).to(device) + 1e-6\nmodel = Net(par, Cp, y, device)\noptim = torch.optim.Adamax(model.parameters(), lr = lr)\n                \nwith tqdm(total=training_steps) as pbar:\n    for epoch in range(training_steps):\n        optim.zero_grad()\n        #ypred = model()\n        ll = model() #loss(ypred, _)\n        ll.backward()\n        optim.step()\n        pbar.set_postfix(loss=ll.item(),p_est=list(model.parameters())[0].data[-1], \n                         #grad=list(model.parameters())[0].grad[-1], \n                         lr = lr, \n                         refresh=False)\n        pbar.update(1)\n        l.append(ll.item())\n        if ll.item() <= 1e-3:\n            break\n<\/code>\nEnd goal is to estimate those 2 parameters in my kmodel function, for each one of the provided time series (each row of the tensor y)\n<h1>Problem<\/h1>\nI can access the gradient with:\n<code class=\"lang-auto\">for p in model.parameters():\n    print(p.grad.shape)\n    print(p.grad)\n<\/code>\nand if I do this, I can see that the gradient has a shape of [Nv , 2], which is:\n\n\nNv = # of time series\n2 = # of parameters to estimate\n\nThis is perfect for what I am trying to do.\nHowever, when I compute the loss as:\n<code class=\"lang-auto\">def loss(self,p):\n        out = kmodel(p,self.IF,self.device) - self.y\n        out = torch.sum((out)**2, dim=1)\n        return torch.mean(out)\n<\/code>\nThe info about all the Nv time series I am fitting are collapsed together, as if I was dealing with a single model with (Nv * 2) unknown parameters.\nWhat I would really like to do is to define a loss like:\n<code class=\"lang-auto\">def loss(self,p):\n        out = kmodel(p,self.IF,self.device) - self.y\n        out = torch.sum((out)**2, dim=1)\n        return out\n<\/code>\nSo that I have a different value for each row (they are all independent from each other), and I can do a different backward pass for each one of them.\nIt\u2019s hard to prove, but I feel that grouping all the time series in a single loss value, I am slowing down (and affecting) my convergence, because I am treating all parameters as if they were all  part of one model, while this is not really the case.\nAnd most importantly, having loss for each time series should ideally allow me to scale my problem (almost) linearly with the value of Nv (each backward pass taking care of 2 parameters to optimize), while now every time I had a new row, I increase a lot the complexity of my model \u2026\n<h1>Question<\/h1>\nDo you think I have any chance of succeding in this quest? ","y":"Hi,\nYour problem is a bit different from the classical neural net as you have one weight per sample that you just try to hardcore over-fit to it.\nIn your current code, even though you have a single loss which is the mean, since each element before the mean is computed completely independently from each (different input, different parameter, etc), then the gradients of the mean will be the same as if you called backward on each element of the sum one by one (dividing by the number of elements because of the mean).\nSo you do any of these (and mix) because the different part of your model are independent \nNote that in general, it is better to sum the losses and do a single backward (speed wise).","z":"Hi,\nYour problem is a bit different from the classical neural net as you have one weight per sample that you just try to hardcore over-fit to it.\nIn your current code, even though you have a single loss which is the mean, since each element before the mean is computed completely independently from each (different input, different parameter, etc), then the gradients of the mean will be the same as if you called backward on each element of the sum one by one (dividing by the number of elements because of the mean).\nSo you do any of these (and mix) because the different part of your model are independent \nNote that in general, it is better to sum the losses and do a single backward (speed wise).\n\n\n\n albanD:\n\nSo you do any of these (and mix) because the different part of your model are independent \n\n\nJust to be sure I got your point: gradients are independent for each row in my input dataset, and even if I mix all of them in the loss, autograd is still aware of it and it treats each row\/time series in the exact same way (in terms of updating the parameters\u2019 values when I do optim.step()) as if I were to compute the backward pass for each one of them independently, right?\n\n\n\n albanD:\n\nNote that in general, it is better to sum the losses and do a single backward (speed wise).\n\n\nAre you suggesting that I change my current loss, from:\n<code class=\"lang-auto\">def loss(self,p):\n        out = kmodel(p,self.IF,self.device) - self.y\n        out = torch.sum((out)**2, dim=1)\n        return torch.mean(out)\n<\/code>\nto\n<code class=\"lang-auto\">def loss(self,p):\n        out = kmodel(p,self.IF,self.device) - self.y\n        out = torch.sum((out)**2, dim=1)\n        return torch.sum(out)\n<\/code>\nor\n<code class=\"lang-auto\">def loss(self,p):\n        out = kmodel(p,self.IF,self.device) - self.y\n        return torch.sum((out)**2)\n<\/code>\nWill any of these options make any difference in terms of speed\/memory usage?\nI am especially interested in this because despite this toy example, in a real case for me Nv should be in the order of millions \u2026 \n\nif I were to compute the backward pass for each one of them independently, right?\n\nAutograd just computes the gradients.\nIf the gradients are independent, then what autograd computes will be independent as well \n\nWill any of these options make any difference in terms of speed\/memory usage?\n\nThese three options won\u2019t really make any difference. The only difference between the firrst two is that all the gradients you compute will be scaled by out.nelement().\nFor the third one, you will just add an extra square backward. So not much of a difference either.\nMy personal opinion is that 2 or 3 are better as the gradient computed for each independent time serie is independent of the number of time series in total (the first one just has a constant scaling so it\u2019s not the end of the world)\nFor the squaring, it will depend on your application \n\n\n\n albanD:\n\nNote that in general, it is better to sum the losses and do a single backward (speed wise).\n\n\nSo, what\u2019s the take-home message of this statement?returning a torch.sum is \u201cbetter\u201d than a torch.mean, or am I missing something else here?\nYes.\nSpeed-wise they are the same.\nBut for your application, I would recommend the sum, so that training sample[0] will behavior the same if len(sample) = 10 or len(sample) = 100.\nIf you do the mean, then you will have to scale your lr up when len(sample) increases."},{"x":"Hi everyone! For my research, I have the need to compute a Jacobian-vector product (JVP), where the Jacobian is of the outputs of a nn.Module (on a mini-batch) w.r.t. its parameters. Basically, the output of the JVP has the same size as the outputs of the original network.\nI installed PyTorch 1.5 because of the new functional.jvp. The input to jvp must be a function with tensor inputs\/outputs (in my case: the inputs are all the Parameters of a nn.Module). However, PT does not have a functional implementation of a Module, where the call to the module takes the parameters as arguments (like in JAX).\nIn this case, I would need to pass the parameters explicitly, then copy them inside the module every time the function is used, which looks inefficient.\nIs there a simpler way to combine the new interface with a nn.Module? Or is there a simpler way to achieve this JVP which I am overseeing? Thanks!","y":"Hi,\nYes the nn.Module construction makes it quite hard to be functional as it is based on the fact that the parameters are part of the state.\nBut here you can cheat by removing the parameters from the module and setting the new Tensors one by one before the forward. An example is below, you should re-organize it if you want to use it in real code to allow restoration of the nn.Parameter I think.\n<code class=\"lang-python\">import torch\n\ndef del_attr(obj, names):\n    if len(names) == 1:\n        delattr(obj, names[0])\n    else:\n        del_attr(getattr(obj, names[0]), names[1:])\ndef set_attr(obj, names, val):\n    if len(names) == 1:\n        setattr(obj, names[0], val)\n    else:\n        set_attr(getattr(obj, names[0]), names[1:], val)\n\ndef make_functional(mod):\n    orig_params = tuple(mod.parameters())\n    # Remove all the parameters in the model\n    names = []\n    for name, p in list(mod.named_parameters()):\n        del_attr(mod, name.split(\".\"))\n        names.append(name)\n    return orig_params, names\n\nmod = torch.nn.Linear(1, 10)\norig_params, names = make_functional(mod)\n# mod.parameters() is empty now\n\ndef functional_mod_fw(*params):\n    for name, p in zip(names, params):\n        set_attr(mod, name.split(\".\"), p)\n    return mod(inp)\n\ninp = torch.rand(1, 1)\n\nv = []\nfor p in orig_params:\n    v.append(torch.rand(p.size()))\n\nout = torch.autograd.functional.jvp(functional_mod_fw, orig_params, v=tuple(v))\nprint(out)\n<\/code>","z":"Hi,\nYes the nn.Module construction makes it quite hard to be functional as it is based on the fact that the parameters are part of the state.\nBut here you can cheat by removing the parameters from the module and setting the new Tensors one by one before the forward. An example is below, you should re-organize it if you want to use it in real code to allow restoration of the nn.Parameter I think.\n<code class=\"lang-python\">import torch\n\ndef del_attr(obj, names):\n    if len(names) == 1:\n        delattr(obj, names[0])\n    else:\n        del_attr(getattr(obj, names[0]), names[1:])\ndef set_attr(obj, names, val):\n    if len(names) == 1:\n        setattr(obj, names[0], val)\n    else:\n        set_attr(getattr(obj, names[0]), names[1:], val)\n\ndef make_functional(mod):\n    orig_params = tuple(mod.parameters())\n    # Remove all the parameters in the model\n    names = []\n    for name, p in list(mod.named_parameters()):\n        del_attr(mod, name.split(\".\"))\n        names.append(name)\n    return orig_params, names\n\nmod = torch.nn.Linear(1, 10)\norig_params, names = make_functional(mod)\n# mod.parameters() is empty now\n\ndef functional_mod_fw(*params):\n    for name, p in zip(names, params):\n        set_attr(mod, name.split(\".\"), p)\n    return mod(inp)\n\ninp = torch.rand(1, 1)\n\nv = []\nfor p in orig_params:\n    v.append(torch.rand(p.size()))\n\nout = torch.autograd.functional.jvp(functional_mod_fw, orig_params, v=tuple(v))\nprint(out)\n<\/code>\nThanks! It looks a little strange but it should work. \nSupporting the nn.Module approach would mean going down the same issue as the checkpointing module we have. Which has severe limitations forcing the user to user backward() and can lead to unexpected behavior wrt things requiring gradients. \nI have tested the solution, also including a call to autograd.grad with respect to v, and it seems to work very nicely, thanks!\nOut of curiosity: in my case I need to optimize w.r.t. v, meaning that I have to call the JVP repeatedly for separate values of v. In JAX we have linearize, which is basically a curried version of the JVP:\nhttps:\/\/jax.readthedocs.io\/en\/latest\/jax.html#jax.linearize\nDo you think it would be feasible to do something similar in the new functional interface?\nI\u2019m afraid we don\u2019t have a good solution for this atm \nWhen vmap is out, you will be able to use that and vmap over the v argument.\nIf you\u2019re happy with a hacky way, I can offer that you take the function from the autograd\/functional.py file here with link \"https:\/\/github.com\/pytorch\/pytorch\/blob\/3e9b4332d2645b9b3b52207c30d4dc734e0a379b\/torch\/autograd\/functional.py#L276\" and modify it as follows:\n<code class=\"lang-python\">import torch\nfrom torch.autograd.functional import _as_tuple, _grad_preprocess, _check_requires_grad, _validate_v, _autograd_grad, _fill_in_zeros, _grad_postprocess, _tuple_postprocess\n\ndef fw_linearize(func, inputs, create_graph=False, strict=False):\n    is_inputs_tuple, inputs = _as_tuple(inputs, \"inputs\", \"jvp\")\n    inputs = _grad_preprocess(inputs, create_graph=create_graph, need_graph=True)\n\n\n    outputs = func(*inputs)\n    is_outputs_tuple, outputs = _as_tuple(outputs, \"outputs of the user-provided function\", \"jvp\")\n    _check_requires_grad(outputs, \"outputs\", strict=strict)\n    # The backward is linear so the value of grad_outputs is not important as\n    # it won't appear in the double backward graph. We only need to ensure that\n    # it does not contain inf or nan.\n    grad_outputs = tuple(torch.zeros_like(out, requires_grad=True) for out in outputs)\n\n    grad_inputs = _autograd_grad(outputs, inputs, grad_outputs, create_graph=True)\n    _check_requires_grad(grad_inputs, \"grad_inputs\", strict=strict)\n\n    def lin_fn(v, retain_graph=True):\n        if v is not None:\n            _, v = _as_tuple(v, \"v\", \"jvp\")\n            v = _grad_preprocess(v, create_graph=create_graph, need_graph=False)\n            _validate_v(v, inputs, is_inputs_tuple)\n        else:\n            if len(inputs) != 1 or inputs[0].nelement() != 1:\n                raise RuntimeError(\"The vector v can only be None if the input to \"\n                                   \"the user-provided function is a single Tensor \"\n                                   \"with a single element.\")\n\n        grad_res = _autograd_grad(grad_inputs, grad_outputs, v, create_graph=create_graph, retain_graph=retain_graph)\n\n        jvp = _fill_in_zeros(grad_res, outputs, strict, create_graph, \"back_trick\")\n\n        # Cleanup objects and return them to the user\n        jvp = _grad_postprocess(jvp, create_graph)\n\n        return _tuple_postprocess(jvp, is_outputs_tuple)\n    return lin_fn\n\n\ndef my_fun(x):\n    return x.pow(3).sum()\n\ninp = torch.ones(4)\n\nlin = fw_linearize(my_fun, inp)\n\nv = torch.zeros(4)\nprint(lin(v))\nv = torch.ones(4)\nprint(lin(v))\nv = torch.tensor([1., 0., 0., 0.])\nprint(lin(v))\nv = torch.tensor([0., 1., 0., 0.])\nprint(lin(v))\nv = torch.tensor([0., 0., 1., 0.])\nprint(lin(v))\nv = torch.tensor([0., 0., 0., 1.])\nprint(lin(v))\n\n\n\n\n\n<\/code>\nThanks, it works nicely! Looking forward to the next improvements to the functional module."},{"x":"Hello All,\nI have come across a situation where i have a function written in matlab and i call it in my pytorch model to perform final computation on my model output. But during back propagation, my model parameter\u2019s gradients are not calculated.\nI reproduced the issue with a toy example for better understanding:\nimport torch\nimport numpy as np\ndef b_func(x):\nabc = torch.tensor([[1]],dtype = torch.float)\nx = abc + x\nreturn x\nX = torch.tensor([[2]],dtype = torch.float, requires_grad = True)\nW = X ** 2\nY = W ** 3\nprint(Y)\nZ = b_func(Y)\nprint(Z)\nZ.backward(retain_graph=True)\nprint(X.grad)\nThis is the expected behavior of pytorch and there is no issue here. However, when i perform the below code:\ndef a_func(x):\nabc = np.array([[1]])\nx = x.detach().numpy()\nx = abc + x\nx = torch.tensor(x, requires_grad = True)\nreturn x\nXX = torch.tensor([[2]],dtype = torch.float, requires_grad = True)\nWW = X ** 2\nYY = W ** 3\nprint(Y)\nZZ = a_func(Y)\nprint(ZZ)\nZZ.backward(retain_graph=True)\nprint(XX.grad)\nWhat changes shall i make for gradients to flow in this scenario. Apart from using pytorch tensors.","y":"Hi,\nWhen you use .detach(), you break the graph and so gradients cannot be computed anymore.\nBut the root issue is that for the autograd to work, the autograd engine needs to be able to know how to compute the gradient for each operation that is done.\nUnfortunately, if you don\u2019t use pytorch ops, it cannot know how to compute the gradients and so you won\u2019t be able to get gradients.\nYou will either have to re-implement it using pytorch operations.\nOr you will have to write a custom function where you tell the autograd how to compute the backward for that part where it does not know how to do it. See doc here with link \"https:\/\/pytorch.org\/docs\/stable\/notes\/extending.html\" on how to do this.","z":"Hi,\nWhen you use .detach(), you break the graph and so gradients cannot be computed anymore.\nBut the root issue is that for the autograd to work, the autograd engine needs to be able to know how to compute the gradient for each operation that is done.\nUnfortunately, if you don\u2019t use pytorch ops, it cannot know how to compute the gradients and so you won\u2019t be able to get gradients.\nYou will either have to re-implement it using pytorch operations.\nOr you will have to write a custom function where you tell the autograd how to compute the backward for that part where it does not know how to do it. See doc here with link \"https:\/\/pytorch.org\/docs\/stable\/notes\/extending.html\" on how to do this.\nThank  for quick response. I had that those solutions in my mind, I wish there was a workaround.\nThank you for the response, it makes things more clear now."},{"x":"Hello, I got an error when writing a simple linear regression model using autograd. My model is\ny = w1 * x1 + w2*x2 + b and I need to learn parameters w1, w2 and b. My code is as below:\n<code class=\"lang-python\">x1 = [0.0, 2.0]\ny1 = 1.0\nx2 = [1.0, 3.0]\ny2 = 1.0\nx3 = [1.0, 0.0]\ny3 = 0.0\nx4 = [2.0, 1.0]\ny4 = 0.0\nX = np.array([x1, x2, x3, x4])\nY = np.array([y1, y2, y3, y4])\n\nfeature = torch.from_numpy(X)\nlabel = torch.from_numpy(Y).view(-1, 1)\n\nepochs = 100\nlr = 0.01\nweight = torch.randn((2, 1), requires_grad=True, dtype=torch.float64)\nbias = torch.randn(1, requires_grad=True, dtype=torch.float64)\n\nfor i in range(epochs):\n    predict = torch.mm(feature, weight) + bias.item()\n    loss = torch.sum(predict - label, dim=0)\n    loss.backward()\n    weight = weight - weight.grad*lr\n    bias = bias - bias*lr\n    weight.grad.zero_()\n    bias.grad.zero_()\n<\/code>\nError occurs at line weight.grad.zero_() :\n<code class=\"lang-auto\">AttributeError: 'NoneType' object has no attribute 'zero_'\n<\/code>\nI want to know how to fix it? Thanks ","y":"Hi,\nWhen you do weight = weight - weight.grad*lr, weight now points to a brand new Tensor and so the gradient informations from the original weight Tensor are gone. You can check that after this line, weight.grad is None.\nThe other problem you\u2019re going to encounter is that weight = weight - XXX, this will be tracked by the autgrad which you most likely don\u2019t want.\nTo fix these, you can\n\nChange weight inplace, to avoid the first problem above\nDisable autograd while you update your weights to avoid the second one.\n\nHere is the new code update:\n<code class=\"lang-auto\">for i in range(epochs):\n    predict = torch.mm(feature, weight) + bias.item()\n    loss = torch.sum(predict - label, dim=0)\n    loss.backward()\n    # Disable the autograd\n    with torch.no_grad():\n        # Inplace changes\n        weight.sub_(weight.grad*lr)\n        bias.sub_(bias.grad*lr) # A .grad is missing in your code here I think ;)\n        # Do the reset in no grad mode as well in case you do second order\n        # derivatives later (meaning that weight.grad will requires_grad)\n        weight.grad.zero_()\n        bias.grad.zero_()\n<\/code>","z":"Hi,\nWhen you do weight = weight - weight.grad*lr, weight now points to a brand new Tensor and so the gradient informations from the original weight Tensor are gone. You can check that after this line, weight.grad is None.\nThe other problem you\u2019re going to encounter is that weight = weight - XXX, this will be tracked by the autgrad which you most likely don\u2019t want.\nTo fix these, you can\n\nChange weight inplace, to avoid the first problem above\nDisable autograd while you update your weights to avoid the second one.\n\nHere is the new code update:\n<code class=\"lang-auto\">for i in range(epochs):\n    predict = torch.mm(feature, weight) + bias.item()\n    loss = torch.sum(predict - label, dim=0)\n    loss.backward()\n    # Disable the autograd\n    with torch.no_grad():\n        # Inplace changes\n        weight.sub_(weight.grad*lr)\n        bias.sub_(bias.grad*lr) # A .grad is missing in your code here I think ;)\n        # Do the reset in no grad mode as well in case you do second order\n        # derivatives later (meaning that weight.grad will requires_grad)\n        weight.grad.zero_()\n        bias.grad.zero_()\n<\/code>\nThank you! But I still get the same error following your code:\n<code class=\"lang-auto\">Traceback (most recent call last):\n  File \"\/Users\/audrey\/PycharmProjects\/PyTorchLearning\/main.py\", line 31, in <module>\n    bias.grad.data.zero_()\nAttributeError: 'NoneType' object has no attribute 'data'\n<\/code>\nThen I changed bias.item() into bias\n<code class=\"lang-python\">predict = torch.mm(feature, weight) + bias\n<\/code>\nNo error appeared anymore! I wonder why bias.item() doesn\u2019t work.\nAnyway, my code can run successfully, thanks!\nI think I find the reason why bias.item() doesn\u2019t work. Using scaler makes tensor bias not be tracked.\nExactely: .item() gives you a python number that means that you don\u2019t have Tensors anymore (and so no autograd anymore).\nAlso given you error message, you seem to be using .data. You should not use it and using with torch.no_grad() instead as in my example.\nI tried to use with torch.no_grad() to resolve this error\n<code class=\"lang-auto\">def weights_init_normal(m):\n    classname = m.__class__.__name__\n    lr = 0.01\n    if isinstance(m, nn.Conv2d) or isinstance(m, nn.Linear):\n        torch.nn.init.normal_(m.weight, mean=0.0, std=0.02)\n        with torch.no_grad():\n            m.bias.sub_(m.bias.grad*lr)\n            m.bias.grad.zero_()\n<\/code>\ngot alot of error.\nAttributeError: 'NoneType' object has no attribute 'sub_'\nI found out my bias is NoneType.\nTried several techniques , nothing worked .\nI wanted to initialize the weights of bias to zero.\nPlease help!!!\nHi,\nBy default, the .grad attribute is None. Which means \u201cfull of zeros\u201d.\nOh ,That means, It is ok to not to touch None type bias. However, my model was not converging, may be my error. Thanks for knowledge "},{"x":"Hi! I have a problem that one layer in my model takes up ca 6 GB of GPU RAM for forward pass, so I am unable to run batch sizes larger than 1 on my GPU.\nOf course, I am not interested in running the model with batch_size 1 and looking how to improve.\nI was thinking about \u201cemulating\u201d larger batch size. E.g., passing 10 single-example batches through the model and then call optimiser once (not after each batch-of-1). But how to do it? If I would do it manually, I would sum the gradients of 10 runs, divide by 10 and then use the result to update model parameters, right? But I am a bit clueless how to achieve the same thing using Pytorch built-in optimisers like SGD or Adam?\nOr perhaps there is a better way to achieve the batch effect or even rework the layer for smaller memory footprint?\nI am processing massive 3D data, hence the memory problem. My 3D convolution that causes high memory usage:\n<code class=\"lang-auto\">class My3DConv(nn.Module):\n\n  def __init__(self):\n    # input_size, output_size, kernel_size, stride, padding\n    super().__init__()\n    self.m1 = nn.Sequential(nn.Conv3d(128,64,3,stride=(2,1,1),padding=(1,1,1)), nn.BatchNorm3d(64), nn.ReLU())\n    self.m2 = nn.Sequential(nn.Conv3d(64,64,3,stride=(1,1,1),padding=(0,1,1)), nn.BatchNorm3d(64), nn.ReLU())\n    self.m3 = nn.Sequential(nn.Conv3d(64,64,3,stride=(2,1,1),padding=(1,1,1)), nn.BatchNorm3d(64), nn.ReLU())\n\n  def forward(self, data):\n    data = self.m1(data)\n    data = self.m2(data)\n    data = self.m3(data)\n    return data\n<\/code>","y":"This post with link \"https:\/\/discuss.pytorch.org\/t\/why-do-we-need-to-set-the-gradients-manually-to-zero-in-pytorch\/4903\/20\" gives you some examples with advantages and shortcomings. ","z":"This post with link \"https:\/\/discuss.pytorch.org\/t\/why-do-we-need-to-set-the-gradients-manually-to-zero-in-pytorch\/4903\/20\" gives you some examples with advantages and shortcomings. "},{"x":"Hello, this is the first time I implement BCElogitLoss and I was wondering if my input is correct or not because I experienced a sudden spike of loss in during my training for classification loss. I read from Logit Explanation with link \"https:\/\/stackoverflow.com\/questions\/41455101\/what-is-the-meaning-of-the-word-logits-in-tensorflow\" that the input should be [-inf, inf] and I check my input is something like this :\n<code class=\"lang-auto\">torch.Size([1, 4]) tensor([[-1376.6078, -2134.1909, -1130.7600,  -517.1730]], device='cuda:0',\n       grad_fn=<ViewBackward>)\ntorch.Size([1, 4]) tensor([[-1015.1113, -2017.0060,  -598.0123,  -647.5376]], device='cuda:0',\n       grad_fn=<ViewBackward>)\ntorch.Size([1, 4]) tensor([[ -948.6944, -2063.7595,  -120.7232,   -31.2307]], device='cuda:0',\n       grad_fn=<ViewBackward>)\ntorch.Size([1, 4]) tensor([[-1494.8126, -2984.7998,  -173.2264,  -605.0916]], device='cuda:0',\n       grad_fn=<ViewBackward>)\ntorch.Size([1, 4]) tensor([[ -759.2620, -6767.8813, -6867.0396,   155.1411]], device='cuda:0',\n       grad_fn=<ViewBackward>)\ntorch.Size([1, 4]) tensor([[-1216.1967, -1960.8824,   781.1366,  -871.1536]], device='cuda:0',\n       grad_fn=<ViewBackward>)\n<\/code>\nand my target is something like this :\n<code class=\"lang-auto\">tensor([[0., 0., 1., 0.]], device='cuda:0', grad_fn=<FloorBackward>)\ntensor([[0., 0., 0., 1.]], device='cuda:0', grad_fn=<FloorBackward>)\ntensor([[0., 1., 0., 0.]], device='cuda:0', grad_fn=<FloorBackward>)\ntensor([[0., 0., 0., 1.]], device='cuda:0', grad_fn=<FloorBackward>)\ntensor([[0., 0., 0., 1.]], device='cuda:0', grad_fn=<FloorBackward>)\ntensor([[0., 0., 0., 1.]], device='cuda:0', grad_fn=<FloorBackward>)\ntensor([[0., 0., 1., 0.]], device='cuda:0', grad_fn=<FloorBackward>)\ntensor([[0., 0., 0., 1.]], device='cuda:0', grad_fn=<FloorBackward>)\ntensor([[0., 0., 1., 0.]], device='cuda:0', grad_fn=<FloorBackward>)\ntensor([[0., 0., 0., 1.]], device='cuda:0', grad_fn=<FloorBackward>)\ntensor([[0., 0., 1., 0.]], device='cuda:0', grad_fn=<FloorBackward>)\n<\/code>\nSo is this the correct input format? if it is wrong, can you provide example of how the input should look like. If anyone is wondering, my last layer is just a simple conv2d and then I change the shape into [1,4] just like I show above. Thank you","y":"The input and target shapes look as if you were dealing with a multi-label classification use case, i.e. each sample might belong to zero, one, or more classes.\nIf that\u2019s the case, then your approach should be correct.\nHowever, the example targets seem to be one-hot encoded, which looks like a multi-class classification, where each sample belongs to one class only.\nIn that case you should use nn.CrossEntropyLoss and use class indices as your target tensor via: target = torch.argmax(target,1).","z":"The input and target shapes look as if you were dealing with a multi-label classification use case, i.e. each sample might belong to zero, one, or more classes.\nIf that\u2019s the case, then your approach should be correct.\nHowever, the example targets seem to be one-hot encoded, which looks like a multi-class classification, where each sample belongs to one class only.\nIn that case you should use nn.CrossEntropyLoss and use class indices as your target tensor via: target = torch.argmax(target,1).\nHello  (and luminoussin)!\n\n\n\n luminoussin:\n\nand my target is something like this :\n<code class=\"lang-auto\">tensor([[0., 0., 1., 0.]], device='cuda:0', grad_fn=<FloorBackward>)\n...\n<\/code>\n\n\nIs it a concern that the target contains a grad_fn?  That strikes\nme as odd.\nBest.\nK. Frank\nThat is an unusual use case, but apparently there might be some valid use cases, which require the targets to get gradients:\n<code class=\"lang-python\">x = torch.randn(2, 4, requires_grad=True)\ny = torch.randint(0, 2, (2, 4)).float().requires_grad_(True)\n\ncriterion = nn.BCEWithLogitsLoss()\nloss = criterion(x, y)\nloss.backward()\n\nprint(x.grad)\n> tensor([[ 0.0799,  0.0592, -0.0284, -0.0983],\n          [ 0.0537, -0.0384, -0.0770, -0.0801]])\nprint(y.grad)\n> tensor([[-0.0715,  0.0134, -0.1531,  0.1629],\n          [ 0.0353, -0.1015,  0.0592,  0.0722]])\n<\/code>\nHowever, I don\u2019t know, when you would need to update the targets.\nHi, about that target with grad_fn. you can forget about it, I just print out a similar variable that looks like the target so it has those grad_fn tracking. For my problem, at first I actually looking for multi class classification but after looking for both description, I think I will try for multi label due to future development as well but since I only have training data for multi class classification, do you think it is still possible to train it like on multi label classification?\nIt should be possible, but the performance might be worse than with a standard multi-class classification.\nAlso, of course you might get outputs for a multi-label classification and would have to deal with it somehow. I.e. is your current use case suitable to get e.g. two predicted classes although the target only contains a single class?\nI dont think it is possible then. I only have training data that consist of single predicted classes, I suppose the best way is to create multi-class classification for now. Thank you for your help so far"},{"x":"I read some posts about ModuleList and all of them said that adding modules to ModuleList gives access to parameters of the Neural Network but in \u201cTraining a classifier\u201d example of 60 mins pytorch tutorial the modules are not added to any ModuleList and still the parameters could be accessed using\noptimizer = optim.SGD(net.parameters(), lr=0.001, momentum=0.9)\nThis is confusing. Please clarify how the parameters are accessible even though the modules have not been added to any ModuleList","y":"Yes, that\u2019s the difference between a Python list and an nn.ModuleList.\nAs explained in the linked topics, the parameters wrapped in a plain list won\u2019t be registered, while the parameters from all modules inside an nn.ModuleList will be registered.\nSo if you want to use a list-like container, then the answer to the initial question is: yes, it\u2019s mandatory to use nn.ModuleList instead of list to register all parameters.\nIf you don\u2019t need a list-like container and just want to register parameters or layers inside an nn.Module class, then you can just use the assignment.","z":"You don\u2019t need an nn.ModuleList to register the parameters properly.\nCould you link the topic, where this is stated, please?\nThe parameters are registered with the assignment in your model:\n<code class=\"lang-python\">class MyModel(nn.Module):\n    def __init__(self):\n        super(MyModel, self).__init__()\n        self.fc = nn.Linear(1, 1) # here all parameters of the linear layers are registered\n        self.register_buffer('my_buffer', torch.tensor(1)) # here the buffer is registered\n<\/code>\nThe __setattr__ method with link \"https:\/\/github.com\/pytorch\/pytorch\/blob\/176174a68ba2d36b9a5aaef0943421682ecc66d4\/torch\/nn\/modules\/module.py#L623-L668\" in nn.Module will take care of it.\nNote that plain tensors (not nn.Parameters, nn.Modules or buffers) will not be registered and will thus not be returned in model.parameters() or model.buffers().\nI am referring to this thread of discusion - The difference in usage between nn.ModuleList and python list with link \"https:\/\/discuss.pytorch.org\/t\/the-difference-in-usage-between-nn-modulelist-and-python-list\/7744\"\nIn this thread smth  said\n\nWhen you ask for  model.parameters() , the parameters of layers inside  nn.ModuleList  will be returned. But if it is regular list, we dont look inside the list for  nn.Module  layers.\n\nI found one more thread - When should I use nn.ModuleList and when should I use nn.Sequential? with link \"https:\/\/discuss.pytorch.org\/t\/when-should-i-use-nn-modulelist-and-when-should-i-use-nn-sequential\/5463\/15\"\nHere you said\n\nExactly! If you use a plain python list, the parameters won\u2019t be registered properly and you can\u2019t pass them to your optimizer using  model.parameters() .\n\nYes, that\u2019s the difference between a Python list and an nn.ModuleList.\nAs explained in the linked topics, the parameters wrapped in a plain list won\u2019t be registered, while the parameters from all modules inside an nn.ModuleList will be registered.\nSo if you want to use a list-like container, then the answer to the initial question is: yes, it\u2019s mandatory to use nn.ModuleList instead of list to register all parameters.\nIf you don\u2019t need a list-like container and just want to register parameters or layers inside an nn.Module class, then you can just use the assignment.\nSorry, I am unable to  understand the list-like container scenario. Even if a python list contains let\u2019s say 2 linear layers. We would be iterating the list and using those Linear Layers in the forward method of a user defined class(sub classed from nn.Module). Wouldn\u2019t that work?\nIt would work to use these layers in the forward pass, but model.parametes() will not return the parameters of these layers (and thus optim.SGD(model.parameters(), lr=1.) will not see these parameters either).\nAlso transferring the parameters to a device via model.to('cuda') won\u2019t transfer these parameters to the desired device.\nSo, if a module is added to a python list its parameters get invisible. Thanks for clearly explaining me this concept. I suggest this should be clearly documented with example in pytorch\u2019s documentation of nn.ModuleList.\nnn.ModuleList mentions the advantage here:\n\nModuleList with link \"https:\/\/pytorch.org\/docs\/master\/generated\/torch.nn.ModuleList.html#torch.nn.ModuleList\" can be indexed like a regular Python list, but modules it contains are properly registered, and will be visible by all Module with link \"https:\/\/pytorch.org\/docs\/master\/generated\/torch.nn.Module.html#torch.nn.Module\" methods.\n\nIf you think this description is still confusing, would you be interested in creating a PR with an improvement?\nYes sure. I have never contributed to any open source yet even if it were some documentation.\nThen this is the best time to start doing it! \nYou could create a feature request here with link \"https:\/\/github.com\/pytorch\/pytorch\/issues\", explain your confusion and your suggestion, and wait until the module owners start the discussion.\nI created an issue #38639"},{"x":"hello i found that 1.5 upgrade has change of autograd\nand also there is a tutorial for me who doesn\u2019t know what\u2019s wrong(https:\/\/github.com\/pytorch\/pytorch\/releases)  issue:[torch.optim optimizers changed to fix in-place checks for the changes made by the optimizer]\n<code class=\"lang-auto\">def model(input, target, param):\n    return `(input * param ** 2 - target).norm()`\n\nparam = torch.randn(2, requires_grad=True)\ninput = torch.randn(2)\ntarget = torch.randn(2)\nsgd = optim.SGD([param], lr=0.001)\nloss = model(input, target, param)\nloss.backward(retain_graph=True)\nsgd.step()\nloss.backward()\nparam.grad\n<\/code>\nbefore 1.5, \u2191 codes works. but after 1.5\n<code class=\"lang-auto\">def model(input, target, param):\n    return (input * param ** 2 - target).norm()\n\nparam = torch.randn(2, requires_grad=True)\ninput = torch.randn(2)\ntarget = torch.randn(2)\nsgd = optim.SGD([param], lr=0.001)\nloss = model(input, target, param.clone())\nloss.backward(retain_graph=True)\nsgd.step()\nloss.backward()\nparam.grad\n<\/code>\ni have to put param.clone() into model\nbut in reality, i don\u2019t use a model like above, i just put only inputs into my model.\n<code class=\"lang-auto\">class test1(nn.Module):\n    def __init__(self):\n        super(test1, self).__init__()\n        self.layer1 = nn.Linear(10, 1)\n\n    def forward(self, x):\n        x = self.layer1(x)\n        return x\n\nt = test1().to(device)\n\noptimizer1 = torch.optim.Adam(t.parameters(), lr=0.001)\n\nfor i, (images, labels) in enumerate(data_loader):\n    images = images.view(batch_size, -1)[:, :10].to(device)\n    labels = labels.float().to(device)\n\n    a = t(images)\n    loss = criterion(a, labels)\n\n    optimizer1.zero_grad()\n    loss.backward(retain_graph=True)\n    optimizer1.step()\n    loss.backward()\n<\/code>\nand \u2191 this is my test code.\nhow can i change \u2191 this code?","y":"Hi,\nThe problem is that the original code here was computing wrong gradients.\nYou can modify this quite easily by overriding the linear forward function for this case:\n<code class=\"lang-auto\">class MyLinear(nn.Linear):\n    def forward(self, input):\n        return F.linear(input, self.weight.clone(), self.bias.clone())\n\n# And use this one later:\nself.layer1 = nn.MyLinear(10, 1)\n<\/code>","z":"Hi,\nThe problem is that the original code here was computing wrong gradients.\nYou can modify this quite easily by overriding the linear forward function for this case:\n<code class=\"lang-auto\">class MyLinear(nn.Linear):\n    def forward(self, input):\n        return F.linear(input, self.weight.clone(), self.bias.clone())\n\n# And use this one later:\nself.layer1 = nn.MyLinear(10, 1)\n<\/code>"},{"x":"I am working on a project that requires me to write a method that is not differentiable. Hence I have calculated the gradient w.r.t the input of this method. Now I have tried to use register_hook to return the calculated grad to be able to flow the gradient backwards from there. But register_hook is not being called for this tensor. After reading through a few posts here, I realized that register_hook will not be called for tensors if gradient is not being calculated. I tried to assign gradient by doing this. my_tensor.grad = calculated_grad. However, I do not think that has any effect.\nCan you please help me on this? I am sharing a demo version of my code.\n<code class=\"lang-auto\">class MLP(nn.Module):\n    def __init__(self, input_size, hidden_size, num_classes=2):\n        super(MLP, self).__init__()\n        self.input_size = input_size\n        self.hidden_size  = hidden_size\n        self.fc1 = nn.Linear(self.input_size, self.hidden_size)\n        self.relu = nn.ReLU()\n        self.fc2 = nn.Linear(self.hidden_size,num_classes)\n    def forward(self, x):\n        hidden = self.fc1(x)\n        relu = self.relu(hidden)\n        output = self.fc2(relu)\n        return output\n\nnet = MLP(input_size, hidden_size, num_classes)\nout = net(data)\n#used out on a non-differentiable method\nfinal_output = non-diff(out)\ncalculated_grad = grad_calc(out) # calculated gradient of out with respect to non-diff method.\nloss = criterion(final_output, target)\nloss.backward()\nout.grad = calculated_grad\nopt.step()\n<\/code>\nI would really appreciate your help on this. I have tried so many things to fix this. But nothing worked.","y":"Hi,\nYou can use a custom Function to specify a backward for a given forward. You can see here with link \"https:\/\/pytorch.org\/docs\/stable\/notes\/extending.html\" how to do this.","z":"Hi,\nYou can use a custom Function to specify a backward for a given forward. You can see here with link \"https:\/\/pytorch.org\/docs\/stable\/notes\/extending.html\" how to do this.\nThank you very much . I thought of this but never tried. Let me try and get back to you how that works out. \nI think it worked! Thank you very much . I just have a minor confusion. Do I multiply upstream gradient with the local gradient as the output of the backwards function? Or is it taken care of automatically?\nYou have to multiply the given grad_output with the gradient of that op to get the grad_input. This is basically one application of the chain rule.\nMake sure to use the gradcheck as mentioned in the doc above to make sure your gradient formula is correct.\n I actually did that thinking of chain rule. But wanted to be sure.  And, I will use gradcheck as stated in the doc. Thank you again for your time.\n I tried using the gradcheck. But it is returning an error.\n<code class=\"lang-auto\">RuntimeError: Jacobian mismatch for output 0 with respect to input 0,\nnumerical:tensor([[    0.,     0.,     0.,     0.,     0.],\n        [    0.,     0.,     0., -5000.,     0.],\n        [    0., -5000.,     0.,  5000.,     0.],\n        [    0.,     0.,     0.,     0.,     0.],\n        [    0.,     0.,     0.,  5000.,  5000.]])\nanalytical:tensor([[-0.0346, -0.0000, -0.0000, -0.0000, -0.0000],\n        [-0.0000, -0.0487, -0.0000, -0.0000, -0.0000],\n        [-0.0000, -0.0000, -0.0312, -0.0000, -0.0000],\n        [ 0.0000,  0.0000,  0.0000,  0.6791,  0.0000],\n        [-0.0000, -0.0000, -0.0000, -0.0000, -0.4282]])\n<\/code>\nFollowing your reply to this with link \"https:\/\/discuss.pytorch.org\/t\/solved-how-to-use-gradcheck\/9692\" post, I have created a small input for the gradcheck function.\n<code class=\"lang-auto\">input = (torch.randn(5,1,requires_grad=True), torch.tensor(1), torch.randn(5,1,requires_grad=True), torch.tensor(0.7))\ntest = torch.autograd.gradcheck(custom_back_method, input, eps=1e-4, atol=1e-6)\nprint(test)\n<\/code>\nI have also tried to increase eps as I am using single precision following your suggestion from this with link \"https:\/\/discuss.pytorch.org\/t\/custom-function-gradcheck\/10997\" post. Please note, I also have tried double precision, which yields similar results. If I set both eps and atol to a very large value like 1.0, then gradcheck yields true. I think that is simply because of higher tolerance and high eps.\nNow it seems to me that the analytical gradient makes more sense than the numerical one. Numerical gradient is too large and does not maintain the Identity, which as far as I understand, should. Do you have any suggestions for me?\nYou will need your function to be quite smooth for the numerical gradient to be precision.\nIf you provide double precision Tensors using the default orther args to gradcheck, it should pass.\nIf you get 5000 for the numerical gradient, that would mean that your function is absolutely not smooth!\nYes, that is correct. Output of my custom_back_method is infact binary (0 or 1). In this case, is it not possible to get precise numerical gradient? This gradient value (5000) is highly dependent on the eps value (1e-4).\nIf your forward has a binary output. Then the \u201ctrue\u201d gradient will be 0 almost everywhere. So you won\u2019t be able to use finite difference to check the gradients.\nYes, that is the case for me. Thank you. I have manually calculated the grad and they seem right in this case. Thank you for your time and help! I really appreciate it. "},{"x":"Hi everyone,\nI\u2019m trying to train a segmentation network and I would like to normalize images between 0 and 1. However the ToTensor transform I use outputs very low pixel values. Here\u2019s a code sample\n<code class=\"lang-python\">img_transforms = transforms.Compose([transforms.Grayscale(num_output_channels=1),\n                                     transforms.Resize((100,100), interpolation=2),\n                                     transforms.ToTensor()])\nimg = Image.open(\"myimg.png\")\nimg = np.array(img).astype(float)\nimg*= (255.0\/img.max())\nimg = img.astype(np.uint8)\nprint(img.max())\nimg = Image.fromarray(img)\nimg = img_transforms(img)\nprint(img.max())\n<\/code>\nAnd the given output:\n<code class=\"lang-auto\">255\ntensor(0.1137)\n<\/code>\nI don\u2019t understand why I don\u2019t get 1 as a max value since the ToTensor function is supposed to output values between 0 and 1.\nCould anyone shed some light on what may be occurring?\nThanks","y":"Thank you , the resize transform was not the issue here but I figured out it was actually due to the fact that my images are mainly blue and that the Grayscale transform multiply blue values by 0.114. I solved the problem by manually setting the max value to 1. I still don\u2019t know why the ToTensor() function didn\u2019t normalize the values between 0 and 1 after the Grayscal transform though.\nHere is my solution:\n<code class=\"lang-python\">img = Image.open(\"myimg.png\")\nimg = img_transforms(img)\nimg*= (1.0\/img.max())\n<\/code>","z":"The ToTensor transformation will normalize the uint8 input image by dividing by 255 as seen here with link \"https:\/\/github.com\/pytorch\/vision\/blob\/4ab46e5f7585b86fb2befdc32d22d13635868c4e\/torchvision\/transforms\/functional.py#L112-L113\".\nBased on your code, I guess that the Resize operation might lower the max. values of 255 to a smaller one (although I wouldn\u2019t expect to see such a decrease). Could you try to use PIL.Image.NEAREST as the interpolation method and recheck the code?\nThank you , the resize transform was not the issue here but I figured out it was actually due to the fact that my images are mainly blue and that the Grayscale transform multiply blue values by 0.114. I solved the problem by manually setting the max value to 1. I still don\u2019t know why the ToTensor() function didn\u2019t normalize the values between 0 and 1 after the Grayscal transform though.\nHere is my solution:\n<code class=\"lang-python\">img = Image.open(\"myimg.png\")\nimg = img_transforms(img)\nimg*= (1.0\/img.max())\n<\/code>"},{"x":"I want to get d(model(x))\/dx, but following program returns None.\n<code class=\"lang-auto\">class Net(nn.Module):\n    def __init__(self):\n        super(Net, self).__init__()\n        self.conv1 = nn.Conv2d(1, 32, 3) # 28x28x32 -> 26x26x32\n        self.conv2 = nn.Conv2d(32, 64, 3) # 26x26x64 -> 24x24x64 \n        self.pool = nn.MaxPool2d(2, 2) # 24x24x64 -> 12x12x64\n        self.dropout1 = nn.Dropout2d()\n        self.fc1 = nn.Linear(12 * 12 * 64, 128)\n        self.dropout2 = nn.Dropout2d()\n        self.fc2 = nn.Linear(128, 1)\n\n    def forward(self, x):\n        x = F.relu(self.conv1(x))\n        x = self.pool(F.relu(self.conv2(x)))\n        x = self.dropout1(x)\n        x = x.view(-1, 12 * 12 * 64)\n        x = F.relu(self.fc1(x))\n        x = self.dropout2(x)\n        x = self.fc2(x)\n        return x\n<\/code>\n<code class=\"lang-auto\">model = Net()\ninput = torch.randn((1, 1, 28, 28), requires_grad=True)\ny = model(input)\ny.backward()\nprint(x.grad) # This is None.\n<\/code>\nHow can I get this grad ?","y":"Sorry, It\u2019s stupid mistake.\nThank you.","z":"Should that print(x.grad) be print(input.grad)?\nI tried the following and it worked.\n<code class=\"lang-python\">import torch\nimport torch.nn as nn\nfrom torch.nn import functional as F\n\nclass Net(nn.Module):\n    def __init__(self):\n        super(Net, self).__init__()\n        self.conv1 = nn.Conv2d(1, 32, 3) # 28x28x32 -> 26x26x32\n        self.conv2 = nn.Conv2d(32, 64, 3) # 26x26x64 -> 24x24x64 \n        self.pool = nn.MaxPool2d(2, 2) # 24x24x64 -> 12x12x64\n        self.dropout1 = nn.Dropout2d()\n        self.fc1 = nn.Linear(12 * 12 * 64, 128)\n        self.dropout2 = nn.Dropout2d()\n        self.fc2 = nn.Linear(128, 1)\n\n    def forward(self, x):\n        x = F.relu(self.conv1(x))\n        x = self.pool(F.relu(self.conv2(x)))\n        x = self.dropout1(x)\n        x = x.view(-1, 12 * 12 * 64)\n        x = F.relu(self.fc1(x))\n        x = self.dropout2(x)\n        x = self.fc2(x)\n        return x\n\nmodel = Net()\ninput = torch.randn((1, 1, 28, 28), requires_grad=True)\ny = model(input)\ny.backward()\nprint(input.grad)\n<\/code>\nSorry, It\u2019s stupid mistake.\nThank you."},{"x":"Hello,\nIs it possible to multiply a trained model by a single scalar that is trainable and backpropagate the loss to that scalar?\nI want to do this for two networks, so the idea is to train those two scalars to find how to ideally combine the two models\u2019 weights (not the outputs).\nThanks in advance for any guidance.","y":"Hi Victor!\n\n\n\n victorc25:\n\nit would be adding an activation after every layer of the original models that combines the corresponding layer of the two original models, is that right? What could be a good activation to test with in this case?\n\n\nI was imagining that you had two models with the same architecture,\nthat is, that all of the layers and activations match up.  The only\ndifference is that they have been trained (or fine tuned) differently,\nso that the trained weights differ.\nIn this case, model A\u2019s activation1 and model B\u2019s activation1\nare the same, so I was thinking that you would simply use\nactivation1 after interpolating the results of layer1A and layer1B.\n(Note, the result of interpolating these two results is the same as\nwhat you would get by interpolating the linear layers layer1A and\nlayer1B, and then applying the interpolated layer to the upstream\ninput.)\n(As an aside, I\u2019m not convinced your scheme will work.  If model A\nand model B are trained independently to do the same thing, the\nvalues of the internal \u201chidden\u201d neurons may still be very different,\nand the layer weights may be very different, so averaging them might\nnot make sense.  If one model was first trained, and then fine tuned\nseparately to get models A and B, the weights in A and B might line\nup sensibly, but I could imagine that averaging the models together\ncould simply serve to average away the results of the fine tunings.)\nGood luck.\nK. Frank","z":"Hello Victor!\n\n\n\n victorc25:\n\nIs it possible to multiply a trained model by a single scalar that is trainable and backpropagate the loss to that scalar?\n\n\nYes.  In general you can train additional parameters (e.g., things that\naren\u2019t weights in Linears) by adding them as Parameter with link \"https:\/\/pytorch.org\/docs\/stable\/nn.html#torch.nn.Parameter\"s to your\nmodel (which should be a torch.nn.Module) or to your Optimizer.\nPytorch will do the rest.\n\n\nI want to do this for two networks, so the idea is to train those two scalars to find how to ideally combine the two models\u2019 weights (not the outputs).\n\n\nI\u2019m not sure exactly what you have in mind here.  But here is one idea\nthat might be relevant to your use case:\nYou have model A and model B already trained.  Freeze their layers\n(.requires_grad = False).  Create an Optimizer that contains your\nweight parameter as a single trainable parameter:\n<code class=\"lang-python\">weight_param = torch.FloatTensor ([1.234])\noptim = torch.optim.SGD ([weight_param])\n<\/code>\n(where 1.234 is your initial value of weight_param).\nThen do something like:\n<code class=\"lang-python\">xA = layer1A (input)\nxB = layer1B (input)\nx = weight_param * xA + (1.0 - weight_param) * xB\nx = activation1 (x)\nxA = layer1A (x)\nxB = layer1B (x)\nx = weight_param * xA + (1.0 - weight_param) * xB\nx = activation2 (x)\n...\nloss = criterion (x, target)\noptim.zero_grad()\nloss.backward()\noptim.step()\n<\/code>\n(This code can be rearranged in various ways, but this is the basic\nidea.)\nIs this the line along which you were thinking?\nBest.\nK. Frank\nHello Frank! Thank you very much for your reply!\nSo, the overall idea is that I want to combine two models that were fine-tuned independently into a single one by interpolating their weights, but I would like to do this using the PyTorch framework (and, most importantly, the loss functions and autograd).\nOriginally, I was thinking about a single \u201ccoefficient\u201d for each model that could just multiply all of the model\u2019s trained weight, but I think I understand your code and it makes sense to me!\nIf I understand correctly, it would be adding an activation after every layer of the original models that combines the corresponding layer of the two original models, is that right? What could be a good activation to test with in this case?\nThis sounds great!\nHi Victor!\n\n\n\n victorc25:\n\nit would be adding an activation after every layer of the original models that combines the corresponding layer of the two original models, is that right? What could be a good activation to test with in this case?\n\n\nI was imagining that you had two models with the same architecture,\nthat is, that all of the layers and activations match up.  The only\ndifference is that they have been trained (or fine tuned) differently,\nso that the trained weights differ.\nIn this case, model A\u2019s activation1 and model B\u2019s activation1\nare the same, so I was thinking that you would simply use\nactivation1 after interpolating the results of layer1A and layer1B.\n(Note, the result of interpolating these two results is the same as\nwhat you would get by interpolating the linear layers layer1A and\nlayer1B, and then applying the interpolated layer to the upstream\ninput.)\n(As an aside, I\u2019m not convinced your scheme will work.  If model A\nand model B are trained independently to do the same thing, the\nvalues of the internal \u201chidden\u201d neurons may still be very different,\nand the layer weights may be very different, so averaging them might\nnot make sense.  If one model was first trained, and then fine tuned\nseparately to get models A and B, the weights in A and B might line\nup sensibly, but I could imagine that averaging the models together\ncould simply serve to average away the results of the fine tunings.)\nGood luck.\nK. Frank\nHello Frank!\n\n\n\n KFrank:\n\nI was imagining that you had two models with the same architecture,\nthat is, that all of the layers and activations match up. The only\ndifference is that they have been trained (or fine tuned) differently,\nso that the trained weights differ.\n\n\nYes, that\u2019s exactly the case!\n\n\n\n KFrank:\n\nIn this case, model A\u2019s activation1 and model B\u2019s activation1\nare the same, so I was thinking that you would simply use\nactivation1 after interpolating the results of layer1A and layer1B .\n(Note, the result of interpolating these two results is the same as\nwhat you would get by interpolating the linear layers layer1A and\nlayer1B , and then applying the interpolated layer to the upstream\ninput.)\n\n\nI understand now! Thanks for explaining it to me, it makes sense now. I\u2019ll try that today to see how it goes, but it really sounds like what I wanted to test.\n\n\n\n KFrank:\n\n(As an aside, I\u2019m not convinced your scheme will work. If model A\nand model B are trained independently to do the same thing, the\nvalues of the internal \u201chidden\u201d neurons may still be very different,\nand the layer weights may be very different, so averaging them might\nnot make sense. If one model was first trained, and then fine tuned\nseparately to get models A and B, the weights in A and B might line\nup sensibly, but I could imagine that averaging the models together\ncould simply serve to average away the results of the fine tunings.)\n\n\nAnd you are exactly correct. The models line up (the filters are correlated) and I\u2019ve tested doing this combination manually and it works, but I wanted to test using Pytorch\u2019s automatic differentiation to optimize the search automatically and see how it behaves using some custom loss functions as well, so I think your suggestion will work perfectly!\n\n\n\n KFrank:\n\nGood luck.\n\n\nThanks again!!\n\u201cI\u2019m making a note here: huge success.\u201d\nI had to write 1875 lines of code just to disassemble and reassemble everything as I wanted and test it step by step, but it worked \nThanks, again!"},{"x":"Hello everyone, i am just a beginner in Pytorch, recently i try to adjust STN to make it works on fish eye image, but i am trapped in problem of \u201cone of the variables needed for gradient computation has been modified by an inplace operation: [torch.FloatTensor [1, 2, 28, 2]], which is output 0 of SliceBackward, is at version 14; expected version 13 instead. Hint: the backtrace further above shows the operation that failed to compute its gradient.\u201d when debugging, i try to fix it with some suggestion here but it still doesn\u2019t work, could someone point out the problem in my code for me, thanks \n<code class=\"lang-auto\">class Net(nn.Module):\n    def __init__(self):\n        super(Net, self).__init__()\n        self.conv1 = nn.Conv2d(1, 10, kernel_size=5)\n        self.conv2 = nn.Conv2d(10, 20, kernel_size=5)\n        self.conv2_drop = nn.Dropout2d()\n        self.fc1 = nn.Linear(320, 50)\n        self.fc2 = nn.Linear(50, 10)\n\n        # Spatial transformer localization-network\n        self.localization = nn.Sequential(\n            nn.Conv2d(1, 8, kernel_size=7),\n            nn.MaxPool2d(2, stride=2),\n            nn.ReLU(True),\n            nn.Conv2d(8, 10, kernel_size=5),\n            nn.MaxPool2d(2, stride=2),\n            nn.ReLU(True)\n        )\n\n        # Regressor for the 3 * 2 affine matrix\n        self.fc_loc = nn.Sequential(\n            nn.Linear(10 * 3 * 3, 2*14),\n            # nn.ReLU(True),\n            # nn.Linear(32, 3 * 2)\n        )\n\n        # Initialize the weights\/bias with identity transformation\n        self.fc_loc[0].weight.data.zero_()\n        self.fc_loc[0].bias.data.copy_(torch.tensor([1,0,1,0,1,0,1,0,1,0,1,0,1,0,1,0,1,0,1,0,1,0,1,0,1,0,1,0], dtype=torch.float))\n\n    # Spatial transformer network forward function\n    def stn(self, x):\n        xs = self.localization(x)\n        xs = xs.view(-1, 10 * 3 * 3)\n        theta = self.fc_loc(xs)\n        theta = theta.view(-1, 14, 2)\n        # print('theta size:',theta.size())\n        output_2 = torch.zeros(x.size())\n        print(type(output_2))\n\n        #grid = torch.zeros(14, 1, 1, 2, 28,2)\n        grid = torch.zeros(14, 1, 2, 28, 2)\n\n        beta = torch.zeros(14,2,3)\n        for i in range(14):\n          beta[i,0,0] = theta[0,i,0]\n          beta[i,0,2] = theta[0,i,1]\n\n        for i in range(14):\n\n          # print('grid_size:',grid[i,:,:,:,:].size())\n          grid[i,:,:,:,:] = F.affine_grid(beta[i,:,:].unsqueeze(0), [1, 1, 2, 28]).clone()\n\n          output_2[:,:,2*i:2*i+2,:] = F.grid_sample(x[:,:,2*i:2*i+2,:], grid[i,:,:,:,:]).clone()\n\n        return output_2\n\n    def forward(self, x_temp):\n        # transform the input\n        x = self.stn(x_temp)\n\n        # Perform the usual forward pass\n        x = F.relu(F.max_pool2d(self.conv1(x), 2))\n        x = F.relu(F.max_pool2d(self.conv2_drop(self.conv2(x)), 2))\n        x = x.view(-1, 320)\n        x = F.relu(self.fc1(x))\n        x = F.dropout(x, training=self.training)\n        x = self.fc2(x)\n        return F.log_softmax(x, dim=1)\n<\/code>","y":"Hi,\nThe error states that you are modifying inplace a Tensor that you are slicing. So an op like your_tensor[ind] = xxx.\nIt states that because the original value of this Tensor is needed for backward computations, this is not valid.\nYou should replace this by and out of place operations.\nIn particular, in your last for-loop, you don\u2019t mneed to do grid[i,:,:,:,:] = xxx (an inplace op) you can simply store that in a temporary Tensor and give it to the next line grid_elem = xxx.\nIf that does not fix the issue.\nFor the grid sample that fills in output_2, I usually store all the results in a list and then torch.cat() the result to get the final result.","z":"Hi,\nThe error states that you are modifying inplace a Tensor that you are slicing. So an op like your_tensor[ind] = xxx.\nIt states that because the original value of this Tensor is needed for backward computations, this is not valid.\nYou should replace this by and out of place operations.\nIn particular, in your last for-loop, you don\u2019t mneed to do grid[i,:,:,:,:] = xxx (an inplace op) you can simply store that in a temporary Tensor and give it to the next line grid_elem = xxx.\nIf that does not fix the issue.\nFor the grid sample that fills in output_2, I usually store all the results in a list and then torch.cat() the result to get the final result.\nnice! I try your first suggestion and change my code as \u201coutput_2[:,:,2i:2i+2,:] = F.grid_sample(x[:,:,2i:2i+2,:], F.affine_grid(beta[i,:,:].unsqueeze(0), [1, 1, 2, 28]))\u201d, which avoids in-place operation here, and now I can continue my working. Thanks a lot!"},{"x":"Say I have 2 independently trained models (with identical architecture) with parameters params1 and params2. I\u2019d like to find out if there exists real values w1 and w2 s.t. the model with parameters (w1 x params1 + w2 x params2) \/ 2 performs well on some validation set.\nTo test this, I\u2019ve written the following piece of code.\n<code class=\"lang-auto\">w = torch.ones(2, 1, requires_grad=True, device=args.device)\noptimizer = optim.SGD([w], lr=1e-1, momentum=0.0)\n\nfor rnd in tqdm(range(1, args.epochs+1)):\n    model.train()\n    val_loss, val_acc = 0.0, 0.0 \n    for _, (inputs, labels) in enumerate(val_loader):\n        # pass inputs to device, clear gradients\n        inputs, labels = inputs.to(device=args.device, non_blocking=True),\\\n                        labels.to(device=args.device, non_blocking=True)\n        optimizer.zero_grad()\n        \n        # compute aggregated params\n        aggr_params = (w[0]*model_1_params + w[1]*model_2_params) \/ 2\n        # Load params to the model\n        vector_to_parameters(aggr_params, model.parameters())\n        # forward-backward pass \n        outputs = model(inputs)\n        minibatch_loss = criterion(outputs, labels)\n        minibatch_loss.backward()\n        \n        optimizer.step()\n        \n        # keep track of round loss\/accuracy\n        val_loss += minibatch_loss.item()*outputs.shape[0]\n        _, pred_labels = torch.max(outputs, 1)\n        val_acc += torch.sum(torch.eq(pred_labels.view(-1), labels)).item()\n        \n    # inference after epoch\n    print(w)\n    val_loss, val_acc = val_loss\/len(val_dataset), val_acc\/len(val_dataset)       \n    print(f'| Valid Loss:{val_loss:.3f}|', end='--')\n    print(f'| Valid Acc: {val_acc:.3f}|', end='\\r')\n<\/code>\nHowever, this does not update weights w  at all.  I suspect the line\n\nvector_to_parameters(aggr_params, model.parameters())\n\nmight be breaking autograd. However, I\u2019m not sure.","y":"It\u2019s not going to be very clean. But the following should work:\n<code class=\"lang-python\">import torch\nfrom torch import nn\n\n\nmodel = nn.Sequential(nn.Linear(2, 2))\n# Save the location of the parameters now\n# Since we delete them later, we won't be able to call this function anymore\nmodel_params = list(model.named_parameters())\n\nagreg = torch.rand(6, requires_grad=True)\n\ndef get_last_module(model, indices):\n    mod = model\n    for idx in indices:\n        mod = getattr(mod, idx)\n    return mod\n\ndef replace_weights(agreg, model, model_params):\n    pointer = 0\n    for name, p in model_params:\n        indices = name.split(\".\")\n        mod = get_last_module(model, indices[:-1])\n        p_name = indices[-1]\n        if isinstance(p, nn.Parameter):\n            # We can override Tensors just fine, only nn.Parameters have custom logic\n            delattr(mod, p_name)\n\n        num_param = p.numel()\n        setattr(mod, p_name, agreg[pointer:pointer + num_param].view_as(p))\n        pointer += num_param\n\n\nprint(model)\n\nprint(model[0].weight)\nreplace_weights(agreg, model, model_params)\nprint(model[0].weight)\n\nagreg = agreg * 10\nreplace_weights(agreg, model, model_params)\nprint(model[0].weight)\n<\/code>","z":"Hi,\nCan you share your vector_to_parameters function please?\nWhat most likely happens is that you put that aggr_params back into an nn.Parameter which cannot have history and so it breaks the gradient graph.\nTo fix this, the simplest solution is to delete the nn.Parameter (this is not a parameter anymore as you don\u2019t want to learn it) and replace it with a regular Tensor containing the value you want.\nHi,\nvector_to_parameters is not my own implementation. It is a PyTorch function.\n\n\ngithub.com with link \"https:\/\/github.com\/pytorch\/pytorch\/blob\/master\/torch\/nn\/utils\/convert_parameters.py\"\n\n\npytorch\/pytorch\/blob\/master\/torch\/nn\/utils\/convert_parameters.py with link \"https:\/\/github.com\/pytorch\/pytorch\/blob\/master\/torch\/nn\/utils\/convert_parameters.py\"\n<code class=\"lang-py\">import torch\n\n\ndef parameters_to_vector(parameters):\n    r\"\"\"Convert parameters to one vector\n\n    Arguments:\n        parameters (Iterable[Tensor]): an iterator of Tensors that are the\n            parameters of a model.\n\n    Returns:\n        The parameters represented by a single vector\n    \"\"\"\n    # Flag for the device where the parameter is located\n    param_device = None\n\n    vec = []\n    for param in parameters:\n        # Ensure the parameters are located in the same device\n        param_device = _check_param_device(param, param_device)\n<\/code>\n\n  This file has been truncated. show original with link \"https:\/\/github.com\/pytorch\/pytorch\/blob\/master\/torch\/nn\/utils\/convert_parameters.py\"\n\n\n\n\n\nWow, I didn\u2019t knew we had that. That is a very very very very dangerous function   We need to fix that.\nThe issue is slightly different here then: it uses .data (that should never be used!) and so break the computational graph. I\u2019m afraid you won\u2019t be able to use this function if you want gradient flowing back throw this update.\nI see, that\u2019s ok I guess  In that case, how can I load the aggregated parameters to the model ?\nIt\u2019s not going to be very clean. But the following should work:\n<code class=\"lang-python\">import torch\nfrom torch import nn\n\n\nmodel = nn.Sequential(nn.Linear(2, 2))\n# Save the location of the parameters now\n# Since we delete them later, we won't be able to call this function anymore\nmodel_params = list(model.named_parameters())\n\nagreg = torch.rand(6, requires_grad=True)\n\ndef get_last_module(model, indices):\n    mod = model\n    for idx in indices:\n        mod = getattr(mod, idx)\n    return mod\n\ndef replace_weights(agreg, model, model_params):\n    pointer = 0\n    for name, p in model_params:\n        indices = name.split(\".\")\n        mod = get_last_module(model, indices[:-1])\n        p_name = indices[-1]\n        if isinstance(p, nn.Parameter):\n            # We can override Tensors just fine, only nn.Parameters have custom logic\n            delattr(mod, p_name)\n\n        num_param = p.numel()\n        setattr(mod, p_name, agreg[pointer:pointer + num_param].view_as(p))\n        pointer += num_param\n\n\nprint(model)\n\nprint(model[0].weight)\nreplace_weights(agreg, model, model_params)\nprint(model[0].weight)\n\nagreg = agreg * 10\nreplace_weights(agreg, model, model_params)\nprint(model[0].weight)\n<\/code>\n\n\n\n albanD:\n\neplace_weights(agreg, model, model_params)\n\n\nPerfect, works like a charm. Thank you so much for your time."},{"x":"Hi I\u2019m working on translating some style transfer Torch code to PyTorch and I\u2019m running into some issues probably because I\u2019m not using autograd correctly. I\u2019m able to run all the way through the building of my network as well as optimization steps but the loss never decreases (it just outputs the same thing for every iteration). I\u2019m not particularly experienced with Torch and even less so with PyTorch so chances are I\u2019m missing something obvious.\nI\u2019ve built up my network (a frozen vgg19) into an nn.Sequential that looks like this:\n<details>\n<summary>\nNet<\/summary>\n<code class=\"lang-auto\">Sequential(\n  (0): Conv2d(3, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n  (1): ReLU()\n  (2): StyleLoss(\n    (gram): GramMatrix()\n    (mse): MSELoss()\n  )\n  (3): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n  (4): ReLU()\n  (5): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n  (6): Conv2d(64, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n  (7): ReLU()\n  (8): StyleLoss(\n    (gram): GramMatrix()\n    (mse): MSELoss()\n  )\n  (9): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n  (10): ReLU()\n  (11): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n  (12): Conv2d(128, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n  (13): ReLU()\n  (14): StyleLoss(\n    (gram): GramMatrix()\n    (mse): MSELoss()\n  )\n  (15): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n  (16): ReLU()\n  (17): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n  (18): ReLU()\n  (19): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n  (20): ReLU()\n  (21): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n  (22): Conv2d(256, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n  (23): ReLU()\n  (24): StyleLoss(\n    (gram): GramMatrix()\n    (mse): MSELoss()\n  )\n  (25): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n  (26): ReLU()\n  (27): ContentLoss(\n    (mse): MSELoss()\n  )\n  (28): ReLU()\n  (29): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n  (30): ReLU()\n  (31): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n  (32): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n  (33): ReLU()\n  (34): StyleLoss(\n    (gram): GramMatrix()\n    (mse): MSELoss()\n  )\n)\n<\/code>\n<\/details>\nWith ContentLoss and StyleLoss defined as follows:\n<details>\n<summary>\nContentLoss<\/summary>\n<code class=\"lang-auto\">class ContentLoss(torch.nn.Module):\n\tdef __init__(self, strength, target, normalize):\n\t\tsuper(ContentLoss, self).__init__()\n\t\tself.strength = strength\n\t\tself.target = target\n\t\tself.normalize = normalize\n\t\tself.loss = 0\n\t\tself.mse = torch.nn.MSELoss()\n\n\tdef forward(self, input):\n\t\tprint(input.shape)\n\t\tprint(self.target.shape)\n\t\tprint(input.nelement())\n\t\tprint(self.target.nelement())\n\t\tif input.nelement() == self.target.nelement():\n\t\t\tself.loss = self.mse.forward(input, self.target) * self.strength\n\t\telse:\n\t\t\tprint('WARNING: Skipping content loss')\n\t\toutput = input\n\t\treturn output\n\n\tdef backward(self, input, grad_output):\n\t\tif input.nelement() == self.target.nelement():\n\t\t\tgrad_input = self.mse.backward(input, self.target)\n\t\tif self.normalize:\n\t\t\tgrad_input.div(torch.norm(grad_input, 1) + 1e-8)\n\t\tgrad_input.mul(self.strength)\n\t\tgrad_input.add(grad_output)\n\t\treturn grad_input\n<\/code>\n<\/details>\n<details>\n<summary>\nStyle Loss<\/summary>\n<code class=\"lang-auto\">class GramMatrix(torch.nn.Module):\n    def forward(self, input):\n\t\ta, b, c, d = input.shape  # a=batch size(=1)\n\t\tfeatures = input.contiguous().view(a * b, c * d)  # resise F_XL into \\hat F_XL\n\t\tG = torch.mm(features, features.t()).float()  # compute the gram product\n\t\treturn G.div(a * b * c * d)\n\nclass StyleLoss(torch.nn.Module):\n\tdef __init__(self, strength, target, normalize):\n\t\tsuper(StyleLoss, self).__init__()\n\t\tself.normalize = normalize\n\t\tself.strength = strength\n\t\tself.target = target\n\t\tself.loss = 0\n\t\tself.gram = GramMatrix()\n\n\t\tself.G = None\n\t\tself.mse = torch.nn.MSELoss()\n\n\tdef forward(self, input):\n\t\tself.G = self.gram.forward(input)\n\t\tself.G.div(input.nelement())\n\t\tself.loss = self.mse.forward(self.G, self.target)\n\t\tself.loss = self.loss * self.strength\n\t\toutput = input\n\t\treturn output\n\n\tdef backward(self, input, grad_output):\n\t\tdG = self.mse.backward(self.G, self.target)\n\t\tdG.div(input.nelement())\n\t\tgrad_input = self.gram.backward(input, dG)\n\t\tif self.normalize:\n\t\t\tgrad_input.div(torch.norm(grad_input, 1) + 1e-8)\n\t\tgrad_input.mul(self.strength)\n\t\tgrad_input.add(grad_output)\n\t\treturn grad_input\n<\/code>\n<\/details>\nAnd then finally I\u2019m trying to run my optimization like this:\n<code class=\"lang-auto\">y = net.forward(img)\ndy = torch.zeros(y.shape)\n\ndef closure():\n\t# optimizer.zero_grad()\n\tnet.forward(img) #Torch code uses x for img here\n\ttorch.autograd.backward(img, dy) # and here calls net.backwards instead of autograd\n\tloss = 0\n\tfor mod in content_losses: loss += mod.loss\n\tfor mod in temporal_losses: loss += mod.loss\n\tfor mod in style_losses: loss += mod.loss\n\t# loss.requires_grad_(True)\n\t# loss.backward()\n\tprint(loss.item())\n\treturn loss\n\n# Run optimization.\noptimizer = torch.optim.LBFGS([img.requires_grad_()], lr=args.learning_rate, max_iter=args.num_iterations, tolerance_change=args.tol_loss_relative)\nfor iter in range(args.num_iterations): # this for loop is weird to me as I thought LBFGS handled this internally with the max_iter parameter...\n\toptimizer.step(closure)\n\nimg_out = np.moveaxis(img.detach().squeeze().numpy(),0,-1)\nskimage.io.imsave(args.img_filename.format(0, 1), img_out)\n<\/code>\nThe Torch code I\u2019m following feeds the image into the closure code as x and then substitutes that for the img variable within the closure definition (see comments in closure()), however I wasn\u2019t able to get that working using step() as I needed to give it the reference to the function.\nI\u2019ve also tried to call backward() on the loss value directly in closure() as can be seen in the commented out lines near the bottom.\nEither way the loss ends up printing the same value for every iteration and the final optimized picture does not look stylized (which I think is because only one iteration of optimization is being run).\nHow can I make sure that the image is optimized correctly using the style &amp; content losses I\u2019ve defined?","y":"Usualy yes (if you don\u2019t use torch.no_grad or something similar. Could you provide a gist with a minimum working example? This would be helpful, since we would able to debug ourselves.","z":"I\u2019m not quite sure, how the xxx_losses are calculated.\nFrom your model definition it looks like you\u2019ve embedded the losses directly in the model, but I\u2019m not sure where the variables come from.\nAlso as a quick side note: you should call your model directly (model(input)) to perform the forward pass instead of model.forward(input) as this will make sure all hooks are properly handled.\nThanks for the reply!\nI\u2019m building my network up in two steps which loop through the vgg model (although I\u2019d like to be able to do this for many different models eventually) inserting layers and losses. First I insert all the layers from vgg (which are frozen after insertion) and the StyleLosses as these do not change throughout optimization.\n<details>\n<summary>\nFirst net building loop<\/summary>\n<code class=\"lang-auto\">content_layers = args.content_layers.split(\",\")\nstyle_layers = args.style_layers.split(\",\")\n\nnext_content_i, next_style_i, next_temporal_i, current_layer_index = 0, 0, 0, 0\n\ncnn = torchvision.models.vgg19(pretrained=True)\nnet = torch.nn.Sequential()\n\nblock = 1\nconv = 1\nfor i, layer in enumerate(cnn.features):\n\tif next_content_i < len(content_layers) or next_style_i < len(style_layers):\n\t\tname = 'uhhh'\n\t\tif isinstance(layer, torch.nn.Conv2d):\n\t\t\tname = 'conv'+str(block)+'_'+str(conv)\n\t\telif isinstance(layer, torch.nn.ReLU):\n\t\t\tname = 'relu'+str(block)+'_'+str(conv)\n\t\t\tconv += 1\n\t\t\tlayer = torch.nn.ReLU(inplace=False)\n\t\telif isinstance(layer, torch.nn.MaxPool2d):\n\t\t\tname = 'pool'+str(block)\n\t\t\tif args.pooling == 'avg':\n\t\t\t\tassert(layer.padW == 0 and layer.padH == 0)\n\t\t\t\tkW, kH = layer.kW, layer.kH\n\t\t\t\tdW, dH = layer.dW, layer.dH\n\t\t\t\tavg_pool_layer = torch.nn.SpatialAveragePooling(kW, kH, dW, dH)\n\t\t\t\tnet.add_module(\"pooling_\"+str(block), avg_pool_layer)\n\t\t\telse:\n\t\t\t\tnet.add_module(\"pooling_\"+str(block), layer)\n\t\t\tblock += 1\n\t\t\tconv = 1\n\t\telse:\n\t\t\tcontinue\n\t\tnet.add_module(name, layer)\n\t\tcurrent_layer_index = current_layer_index + 1\n\t\tif next_content_i < len(content_layers) and name == content_layers[next_content_i]:\n\t\t\tprint(\"Earmarking content loss \"+str(next_content_i+1)+\": \"+name)\n\t\t\tlosses_indices.append(current_layer_index)\n\t\t\tlosses_type.append('content_'+name)\n\t\t\tnext_content_i = next_content_i + 1\n\t\tif next_style_i < len(style_layers) and name == style_layers[next_style_i]:\n\t\t\tprint(\"Setting up style loss \"+str(next_style_i+1)+\": \"+name)\n\t\t\tgram = GramMatrix()\n\t\t\ttarget = []\n\t\t\tfor s in range(len(style_images)):\n\t\t\t\ttarget_features = net.forward(style_images[s]).detach()\n\t\t\t\ttarget_i = gram.forward(target_features)\n\t\t\t\ttarget_i.div(target_features.nelement())\n\t\t\t\ttarget_i.mul(style_blend_weights[s])\n\t\t\t\tif s == 0:\n\t\t\t\t\ttarget = target_i\n\t\t\t\telse:\n\t\t\t\t\ttarget.append(target_i)\n\t\t\tloss_module = StyleLoss(args.style_weight, target, args.normalize_gradients)\n\t\t\tnet.add_module(\"style_loss_\"+name, loss_module)\n\t\t\tcurrent_layer_index = current_layer_index + 1\n\t\t\tstyle_losses.append(loss_module)\n\t\t\tnext_style_i = next_style_i + 1\n\ndel cnn\nfor module in net.children():\n    if isinstance(module, torch.nn.Conv2d):\n        # remove these, not used, but uses gpu memory\n        module.gradWeight = None\n        module.gradBias = None\n\nfor param in net.parameters():\n    param.requires_grad = False\n<\/code>\n<\/details>\nAfterwards I want to loop through many different content images so I insert the content loss into the network, run the optimization, remove it, and repeat for each image.\n<details>\n<summary>\nContent loss insertion<\/summary>\n<code class=\"lang-auto\">content_losses, prev_plus_flow_losses = [], []\nadditional_layers = 0\n\nfor i in range(len(losses_indices)):\n\tif losses_type[i].startswith('content'):\n\t\tcontent_loss = get_content_loss_module(net, losses_indices[i] + additional_layers, content_image, args)\n\t\t# insert content\n\t\tindex = losses_indices[i]+additional_layers\n\t\tnew_modules = list(net.children())[:index] + [content_loss] + list(net.children())[index+1:]\n\t\tnet = torch.nn.Sequential(*new_modules)\n\n\t\tadditional_layers = additional_layers + 1\n\t\tcontent_losses.append(content_loss)\n\ndef get_content_loss_module(net, layer_idx, target_img, args):\n\ttmpNet = torch.nn.Sequential()\n\tfor i, layer in enumerate(net.children()):\n\t\tif i == layer_idx: break\n\t\ttmpNet.add_module(\"temp\"+str(i), layer)\n\ttarget = tmpNet.forward(target_img)\n\tloss_module = ContentLoss(args.content_weight, target, args.normalize_gradients)\n\treturn loss_module\n\nrun_optimization(args, net, content_losses, style_losses, temporal_losses, img, mod_idx, run)\n\nfor i,l in enumerate(losses_type):\n    if l.startswith('content'):\n        additional_layers = additional_layers - 1\n        index = losses_indices[i]+additional_layers\n        new_modules = list(net.children())[:index] + list(net.children())[index+1:]\n        net = torch.nn.Sequential(*new_modules)\n<\/code>\n<\/details>\nRegarding replacing model.forward() calls: I\u2019ve tried this in the optimization part but am still running into the same problem, the printed loss value never changes. Is the model(input) call better in all situations? Should I replace all model.forward() calls throughout? Or only in the optimization closure?\nN.B. there are some references to temporal losses throughout the code, these are basically slightly modified content losses but are currently not being inserted yet so temporal_losses is just an empty list. This should eventually be handled in a similar way to the content_losses: being inserted for each individual content image.\nWhat kind of error do you get, if you call loss.backward() in your closure?\nYou should always use the direct model call instead of forward() unless there is a reason to avoid hooks being called (which is probably a very unusual use case).\nI don\u2019t get an error, the loss just never changes and it saves the same picture over and over again.\nI\u2019ve decided to rewrite the code without the custom losses inserted into the network and just calculate them in place (this is a bit more PyTorch-y?). I\u2019ll report back in a few days\u2026\nAlright that didn\u2019t turn out to be so hard, but I\u2019m still running into the same exact error! The loss is the same every single time. To me it seems like the backward() call just isn\u2019t updating anything in the pastiche image even though I\u2019ve set it to require_grad.\nI\u2019ve now changed the building of the network to just insert the modules as read from the vgg19 model without any losses. I\u2019ve wrapped it in a custom Sequential class that returns a list of features to use for calculating the losses when optimizing.\n<details>\n<summary>\nbuild_net()<\/summary>\n<code class=\"lang-auto\">content_layers = args.content_layers.split(\",\")\nstyle_layers = args.style_layers.split(\",\")\n\nmodules_dict = OrderedDict()\nfeature_list = []\n\nnext_content_i, next_style_i = 0, 0\n\nblock = 1\nconv = 1\nfor i, layer in enumerate(cnn.features):\n    if next_content_i < len(content_layers) or next_style_i < len(style_layers):\n        name = 'uhhh'\n        if isinstance(layer, torch.nn.Conv2d):\n            name = 'conv'+str(block)+'_'+str(conv)\n        elif isinstance(layer, torch.nn.ReLU):\n            name = 'relu'+str(block)+'_'+str(conv)\n            layer = torch.nn.ReLU(inplace=False)\n            conv += 1\n        elif isinstance(layer, torch.nn.MaxPool2d):\n            name = 'pool_'+str(block)\n            if args.pooling == 'avg':\n                assert(layer.padW == 0 and layer.padH == 0)\n                kW, kH = layer.kW, layer.kH\n                dW, dH = layer.dW, layer.dH\n                avg_pool_layer = torch.nn.SpatialAveragePooling(kW, kH, dW, dH)\n                layer = avg_pool_layer\n                block += 1\n                conv = 1\n        else:\n            continue\n\n        modules_dict[name] = layer\n\n        if next_content_i < len(content_layers) and name == content_layers[next_content_i]:\n            print(\"Earmarking content loss \"+str(next_content_i+1)+\": \"+name)\n            feature_list.append('content_'+name)\n            next_content_i = next_content_i + 1\n\n        if next_style_i < len(style_layers) and name == style_layers[next_style_i]:\n            print(\"Earmarking style loss \"+str(next_style_i+1)+\": \"+name)\n            feature_list.append('style_'+name)\n            next_style_i = next_style_i + 1\n\nreturn SelectiveSequential(feature_list, modules_dict)\n\nclass SelectiveSequential(torch.nn.Module):\n\tdef __init__(self, to_select, modules_dict):\n\t\tsuper(SelectiveSequential, self).__init__()\n\t\tfor key, module in modules_dict.items():\n\t\t\tself.add_module(key, module)\n\t\tself._to_select = to_select\n\n\tdef forward(self, x, layers_to_return):\n\t\tlayers = [[] for i in range(len(layers_to_return))]\n\t\tfor name, module in self._modules.iteritems():\n\t\t\tx = module(x)\n\t\t\tfor i,l in enumerate(layers_to_return):\n\t\t\t\tif l+\"_\"+name in self._to_select:\n\t\t\t\t\tlayers[i].append(x)\n\t\treturn layers if len(layers_to_return) > 1 else layers[0]\n\ndef gram_matrix(input):\n\ta, b, c, d = input.shape\n\tfeatures = input.contiguous().view(a * b, c * d)\n\tG = torch.mm(features, features.t()).float()\n\treturn G.div(a * b * c * d)\n<\/code>\n<\/details>\nMy optimization now looks like this:\n<code class=\"lang-auto\">cnn = torchvision.models.vgg19(pretrained=True)\nnet = utils.build_net(args, cnn)\ndel cnn\nfor module in net.children():\n    if isinstance(module, torch.nn.Conv2d):\n        # remove these, not used, but uses gpu memory\n        module.gradWeight = None\n        module.gradBias = None\nfor param in net.parameters():\n    param.requires_grad = False\n\nstyle_images = utils.get_style_images(args, 1)\nfile_name = args.content_filename.format(mod_idx)\ncontent_image = skimage.io.imread(file_name)\ncontent_image = skimage.transform.rescale(content_image, scale_current)\ncontent_image = utils.match_color(content_image, np.moveaxis(style_images[0].cpu().squeeze().numpy(), 0, -1))\ncontent_image = torch.from_numpy(content_image.reshape(1, content_image.shape[2], content_image.shape[0], content_image.shape[1])).float()\nimg = content_image.clone()\n\n# get content &amp; flow weighted features\ncontent_features = net(img, ['content'])\nf_xc_c = []\nfor m in range(len(content_features)):\n    f_xc_c.append(torch.autograd.Variable(content_features[m].data, requires_grad=False))\n\n# get style features\ngram_style = None\nfor i, style_image in enumerate(style_images):\n    style_features = net(style_image, ['style'])\n    target_i = [utils.gram_matrix(y) for y in style_features]\n    target_i = [t \/ style_features[n].nelement() for n,t in enumerate(target_i)]\n    target_i = [t * style_blend_weights[i] for t in target_i]\n    if i == 0:\n        gram_style = target_i\n    else:\n        gram_style = [sum(x) for x in zip(gram_style, target_i)]\n\n# init optimizer\npastiche = torch.autograd.Variable(img.data, requires_grad=True)\noptimizer = torch.optim.Adam([pastiche], lr=args.learning_rate)\nmse_loss = torch.nn.MSELoss()\n\n# optimize the images\nfor e in range(args.num_iterations):\n    optimizer.zero_grad()\n    c_feat, s_feat = net(pastiche, ['content','style'])\n    content_loss = 0.\n    for m in range(len(c_feat)):\n        content_loss += args.content_weight * mse_loss(c_feat[m], f_xc_c[m])\n\n    style_loss = 0.\n    for m in range(len(s_feat)):\n        gram_y = utils.gram_matrix(s_feat[m])\n        gram_s = torch.autograd.Variable(gram_style[m].data, requires_grad=False)\n        style_loss += args.style_weight * mse_loss(gram_y, gram_s)\n\n    total_loss = content_loss + style_loss\n    loss = torch.autograd.Variable(total_loss.data, requires_grad=True)\n    loss.backward()\n    optimizer.step()\n    print(total_loss.data.cpu().numpy())\n<\/code>\nHow can I get pastiche to be updated? Aren\u2019t the calls to loss.backward() and\/or optimizer.step() supposed to handle this?\nYes, the backward call and the optimizer are supposed to update your input image.\nHowever, one condition on this is, that you don\u2019t detach the computation graph.\nThis happens e.g. if you use some numpy operations on your data, which autograd cannot trace. Also, this happens if you are re-wrapping variables as it seems to be the case for your loss.\nCurrently you are using loss = Variable(total_loss.data, requires_grad=True), which cuts the graph right at the end so that no gradients will be calculated.\nHave a look at this small code example:\n<code class=\"lang-python\">model = nn.Linear(10, 2)\nx = torch.randn(1, 10, requires_grad=True)\ny = torch.randn(1, 2)\n\noutput = model(x)\nloss = ((output - y)**2).mean()\nloss.backward()\nprint(model.weight.grad)\nprint(x.grad)\n\n# Now let's re-wrap the loss\nmodel = nn.Linear(10, 2)\nx = torch.randn(1, 10, requires_grad=True)\ny = torch.randn(1, 2)\n\noutput = model(x)\nloss = ((output - y)**2).mean()\ndet_loss = torch.tensor(loss.data, requires_grad=True)\ndet_loss.backward()\nprint(model.weight.grad)\nprint(x.grad)\n<\/code>\nCould you try to fix this and then check the gradients in your input image?\nAlso as a small side note, Variables and tensors were merged in 0.4.0, so you should update to the current stable or preview version. You will find install instructions here with link \"https:\/\/pytorch.org\/get-started\/locally\/\".\nHmm ok, I did have some stray numpy operations in gram_matrix() and a few other places which I\u2019ve gone ahead and translated to torch. I had added the Variable wrap because I was getting \u2018RuntimeError: element 0 of tensors does not require grad and does not have a grad_fn\u2019 which I\u2019m still getting now even though I think I\u2019ve replaced all numpy operations and have the pastiche set to require_grad = True.\nCan you spot what detaches the computation graph?\n<code class=\"lang-auto\">def gram_matrix(input):\n\ta, b, c, d = input.size()\n\tfeatures = input.view(a * b, c * d)\n\tG = torch.mm(features, features.t())\n\treturn G.div(a * b * c * d)\n\nClass SelectiveSequential(torch.nn.Module)\n        ...\n\tdef forward(self, x, layers_to_return):\n\t\tlayers = [[] for i in range(len(layers_to_return))]\n\t\tfor name, module in self._modules.iteritems():\n\t\t\tx = module(x)\n\t\t\tfor i,l in enumerate(layers_to_return):\n\t\t\t\tif l+\"_\"+name in self._to_select:\n\t\t\t\t\tlayers[i].append(x)\n\t\treturn layers if len(layers_to_return) > 1 else layers[0]\n\nstyle_images = utils.get_style_images(args, scale_current)\nstyle_images = [s.to(device) for s in style_images]\n\n# get style features\ngram_style = None\nfor i, style_image in enumerate(style_images):\n\tstyle_features = net(style_image, ['style'])\n\ttarget_i = [utils.gram_matrix(y) for y in style_features]\n\ttarget_i = [t.div(style_features[n].numel()) for n,t in enumerate(target_i)]\n\ttarget_i = [t.mul(style_blend_weights[i]) for t in target_i]\n\tif i == 0:\n\t\tgram_style = target_i\n\telse:\n\t\tgram_style = [sum(x) for x in zip(gram_style, target_i)]\n\nfile_name = args.content_filename.format(mod_idx)\ncontent_image = skimage.io.imread(file_name)\ncontent_image = skimage.transform.rescale(content_image, scale_current)\ncontent_image = utils.match_color(content_image, np.moveaxis(style_images[0].cpu().squeeze().numpy(),0,-1)) #TODO hist match multiple styles\ncontent_image = torch.from_numpy(content_image.reshape(1, content_image.shape[2],content_image.shape[0],content_image.shape[1])).float()\ncontent_image = content_image.to(device)\n\n# get content features\ncontent_features = net(content_image, ['content'])\n\ninit_image = content_image\npastiche = init_image.to(device)\npastiche.requires_grad = True\n\noptimizer = torch.optim.Adam([pastiche], lr=args.learning_rate)\nmse_loss = torch.nn.MSELoss()\n\n# optimize the images\nfor e in range(args.num_iterations):\n\toptimizer.zero_grad()\n\tc_feat, s_feat = net(pastiche, ['content','style'])\n\tcontent_loss = 0.\n\tfor m in range(len(c_feat)):\n\t\tcontent_loss += args.content_weight * mse_loss(c_feat[m], content_features[m])\n\n\tstyle_loss = 0.\n\tfor m in range(len(s_feat)):\n\t\tgram_y = utils.gram_matrix(s_feat[m])\n\t\tgram_s = gram_style[m]\n\t\tstyle_loss += args.style_weight * mse_loss(gram_y, gram_s)\n\n\ttotal_loss = content_loss + style_loss\n\ttotal_loss.backward()\n\toptimizer.step()\n\tprint(total_loss.data.cpu().numpy())\n\n# save frame\nimg_out = pastiche.clone().squeeze().mul(255).cpu().clamp(0, 255).numpy()\nimg_out = img_out.transpose(1, 2, 0).astype('uint8')\nskimage.io.imsave(args.img_filename.format(run, mod_idx), img_out)\nif run == args.passes_per_scale - 1 and mod_idx == 1:\n\tskimage.io.imsave(args.img_filename.format(max(*img.shape), mod_idx), img_out)\n<\/code>\nI\u2019m using pytorch==0.4 so I think Variable was just an alias for Tensor, but I\u2019ve removed all Variables anyway. Some other parts of my pipeline rely on pytorch<=0.4 so I\u2019d like to get it working for this version.\nThanks so much for all your help!\nCould you point me to the line throwing the RuntimeError?\nI can\u2019t see any obvious errors skimming through your code.\nIt happens on total_loss.backward().\nI\u2019ve tried rewriting the loop into a closure and calling lbfgs and it still says there is no gradient when calling backward() on the loss.\nCould you please print the content of requires_grad for gram_y, gram_s,  c_feat[m] and content_features [m]?\nAll of those return false. The only variable that prints true is pastiche, which I\u2019ve explicitly set. Is a variable\u2019s requires_grad property supposed to propagate to variables made from it?\nUsualy yes (if you don\u2019t use torch.no_grad or something similar. Could you provide a gist with a minimum working example? This would be helpful, since we would able to debug ourselves.\nI found it\u2026 One of the scripts from a different part of my pipeline that was imported but wasn\u2019t being used at all had a torch.no_grad in it facepalm\nIn any case thanks for all the help and sorry for the wild goose chase"},{"x":"Hi \nI\u2019m implementing a custom loss function, which has a term that involves the gram matrix of a Gaussian RBF kernel.\nSay, for each training iteration, I get a mini-batch (batch size 128) of predicted probabilities for K=5 classes. So the predicted probability tensor has shape=(128,5). Now I wish to compute the Gram matrix (128 by 128) of the Gaussian RBF kernel exp(-||p-q||^2) where p and q are the predicted probability vectors.\nI don\u2019t know if there\u2019s a way of doing this without looping through all the 128x128 possible pairs of p and q, and yet preserves the autograd compatibility so I can use it as part of the loss function.\nCould you please help me with some code example? Thank you in advance!\nBest","y":"You could add a dummy dimension and use broadcasting for this use case:\n<code class=\"lang-python\">a = torch.randn(128, 2)\nb = torch.randn(128, 2)\nres = torch.norm(a.unsqueeze(1)-b, dim=2, p=1)\n\nres_manual = []\nfor a_ in a:\n    for b_ in b:\n        res_manual.append(torch.norm(a_-b_, dim=0, p=1))\nres_manual = torch.stack(res_manual)\nres_manual = res_manual.view(128, 128)\n\nprint((res - res_manual).abs().max())\n> tensor(0.)\n<\/code>","z":"I\u2019m not sure, but wouldn\u2019t torch.mm(mat, mat.t()) calculate the Gram matrix?\nPS: as you can clearly see I\u2019m not an expert in this topic, so tagging certain people might demotivate others to answer in your thread. \nThank you very much for your reply!\nIf mat is the predicted probability matrix with shape (128,5), then torch.mm(mat,mat.t()) gives me the 128 by 128 matrix that contains all the inner products between pairs of rows p,q in mat.\nBut what I\u2019m hoping is to compute a more general function k(p,q) between all pairs of rows p,q in mat and store it in a 128 by 128 matrix. So torch.mm(mat,mat.t()) can be seen as a simple case of this where the function k(p,q) is just the inner product <p,q>.\nOK, I see. Do you have a specific function in mind for k?\nYes, for example, k(p,q)=exp(-||p-q||) where the norm ||p-q|| is the L1 norm.\nYou could add a dummy dimension and use broadcasting for this use case:\n<code class=\"lang-python\">a = torch.randn(128, 2)\nb = torch.randn(128, 2)\nres = torch.norm(a.unsqueeze(1)-b, dim=2, p=1)\n\nres_manual = []\nfor a_ in a:\n    for b_ in b:\n        res_manual.append(torch.norm(a_-b_, dim=0, p=1))\nres_manual = torch.stack(res_manual)\nres_manual = res_manual.view(128, 128)\n\nprint((res - res_manual).abs().max())\n> tensor(0.)\n<\/code>\nThank you so much for the code!\nJust to check, both methods in your code example are compatible with autograd right? coz I want to use the matrix as part of my loss function.\nYes, Autograd will be able to track these operations."},{"x":"Hi all,\nI have a model that has multiple inputs, and I was wondering if it is possible to find the gradients of the output with respect to the inputs.\nThe layout is as follows:\noutput = f(inp1, inp2, \u2026)\nTo do so, we minimize the loss: Solve MSE optimization problem loss ||output-target||\nThen after the model is trained I am interested in finding the sensitivities:\nd(output)\/d(inp1), d(output)\/d(inp2), \u2026\nI believe it is doable, but I am not sure how. Any help\/hint is appreciated.","y":"Hi,\nFirst, make sure your inputs require gradients:\n\nIf they don\u2019t, just call requires_grad_() on them before giving them to your net\nIf they do and are leaf (inp1.is_leaf()) then you\u2019re good to go\nIf they do but are not leafs, you can do inp1.retain_grad() to make sure the .grad field will be populated properly.\n\nThen if output is a scalar, you can simply .backward() on the output.\nOtherwise, if you want the full jacobian, you can use the torch.autograd.functional package from pytorch 1.5.\nIf you want the sum of the gradients for each element in the output, you can do output.sum().backward().","z":"Hi,\nFirst, make sure your inputs require gradients:\n\nIf they don\u2019t, just call requires_grad_() on them before giving them to your net\nIf they do and are leaf (inp1.is_leaf()) then you\u2019re good to go\nIf they do but are not leafs, you can do inp1.retain_grad() to make sure the .grad field will be populated properly.\n\nThen if output is a scalar, you can simply .backward() on the output.\nOtherwise, if you want the full jacobian, you can use the torch.autograd.functional package from pytorch 1.5.\nIf you want the sum of the gradients for each element in the output, you can do output.sum().backward().\nHi albanD,\nMany thanks for the response. May you elaborate on what you mean by leaf? I will try to do it along the lines you suggested and come back to you, Thanks again\nA leaf Tensor is a Tensor that does not have history. You can check it with your_tensor.is_leaf().\nWhen calling .backward(), it will populate the .grad field of all the leaf Tensors that require gradients that were use in the computation of the output.\nThanks again for the explanation\nI have the following after output.backward() statement\ninpt2_grad = grad(output, inp2, torch.ones(inp2.size()[0], 1, device=dev), create_graph=True, retain_graph=True)\ninpt2_grad       = inpt2_grad.cpu().numpy()\nI get the following error at the line where I have: inpt2_grad = grad(output, inp2, torch\u2026\nMismatch in shape: grad_output[0] has a shape of torch.Size([15625, 1]) and output[0] has a shape of torch.Size([]).\nAlso,\nIf I do the following\ninp2_grad = inp2.grad\nprint(inp2_grad)\ninp2_grad       = inp2_grad.detach().cpu().numpy()\nI get None when I print inp2_grad\nAlso, I cannot make it a numpy array\nHi,\nThe issue here is that the grad_output should be of the same size as the output. In this case, the output is a scalar so you can actually leave the grad_output field empty to get a Tensor containing a single 1.\nAutograd.grad does not populate the .grad field of the Tensor, it returns the gradients directly and you should get it the same way you do in your post above. Note that it returns a tuple though so you might need to do inpt2_grad, = xxx.\nThanks, albanD.\nI did the following:\ninpt2_grad = grad(output, inpt2, create_graph=True, retain_graph=True)\nBut now I am encountering a new issue:\nOne of the differentiated Tensors appears to not have been used in the graph. Set allow_unused=True if this is the desired behavior.\nAny insights of what allow_unused does? I tried to read the documentation, but I got lost.\nAlso, if I add the allow_unused=True statement, grad(output,\u2026) returns None.\nThis error means that pytorch cannot find any link between output and inpt2. Meaning that output was not computed in a differentiable way based on inpt2.\nSo you want to double check your code to make sure that output does depend on input2 (or if it does not, you can remove this call and replace it by a Tensor full of 0s).\nThanks. I see.\nSo let me layout the code, and you might be able to tell me if there is something wrong:\nclass Grad_finder:\ndef fun1(x1, x2, x3): \n\n    x1.requires_grad_(True)\n    x1.retain_grad()\n    x2.requires_grad_(True)\n    x2.retain_grad()\n    \n\n    \n    '''\n    we minimize the loss\n    '''\n    for i in range(epochs):\n        L1 = f(x1, x3) # f is a neural network model\n        L2 = g(L1, x2) # g is a defined function\n        L3 = h(x1, x3) # h is a defined function\n    \n        loss = L2 - L3\n        optimizer.zero_grad()\n        loss.backward(retain_graph=True)\n        optimzer.step()\n    \n    # now f is trained \n    L1 = f(x1, x3)\n    L2 = g(L1, x2)\n    \n    return L2\n    \ndef fun2(x1, x2, x3): \n\n    x1.requires_grad_(True)\n    x1.retain_grad()\n    x2.requires_grad_(True)\n    x2.retain_grad()\n    \n    L2 = fun1(x1, x2, x3)\n    \n    '''\n    Here i want to find :\n    dL2\/dx2\n    '''\n    \n    return dL2dx2\n\ndL2dx2 = Grad_finder.fun2(x1, x2, x3)Capture539\u00d7851 76.1 KB with link \"https:\/\/discuss.pytorch.org\/uploads\/default\/original\/3X\/a\/1\/a1f7b63319d19cd5ba7fb9f1469e7a3dd5381c1e.png\"\nOk, so the only potentially differentiable link I can see between L2 and x2 is L2 = g(L1, x2).\nSo what is g here?\nI think this is what I am missing: the relationship between L2 and x2. They are implicitly related. I have to revisit the problem and see how I can proceed. Many thanks, albanD."},{"x":"Hi, I have some problems with fine-tuning the last layer of a Neural network.\nCode is as followed:\n<code class=\"lang-auto\">for layer in self.layers[0:-1]: \n        for param in self.model.fc_layers[layer].parameters():\n            param.requires_grad = False\n        \n        y_prime = self.model(X)\n        loss = self.model.criterion(y_prime, y)\n        \n        self.model.optimize.zero_grad()\n        self.model.zero_grad()\n        \n        loss.backward(retain_graph=1)\n        self.model.optimize.step()\n<\/code>\nModel is a Neural network.\nIt didn\u2019t work as I fine-tune the last layer(re-train), I get the same result while I didn\u2019t fix the parameters of any layers.\nsomething is wrong here, but I can\u2019t figure it out.","y":"Your general workflow should work as shown in this small example:\n<code class=\"lang-python\"># Setup\nmodel = models.resnet18()\ndata = torch.randn(1, 3, 224, 224)\ntarget = torch.randint(0, 1000, (1,))\ncriterion = nn.CrossEntropyLoss()\noptimizer = torch.optim.SGD(model.parameters(), lr=1e-3)\n\n# Train\noptimizer.zero_grad()\nout = model(data)\nloss = criterion(out, target)\nloss.backward()\noptimizer.step()\n\n# Freeze all but last layer\nfor name, param in model.named_parameters():\n    if not 'fc' in name:\n        param.requires_grad = False\n      \noptimizer.zero_grad()\nout = model(data)\nloss = criterion(out, target)\nloss.backward()\n\n# Check grads\ngrad_frozen = model.conv1.weight.grad\ngrad_fc = model.fc.weight.grad\nprint(grad_frozen.abs().sum())\nprint(grad_fc.abs().sum())\n\noptimizer.step()\n<\/code>\nAs you can see, after freezing all parameters but the fc.weight and fc.bias, I get a zero gradient for a previous layer, while I get valid gradients for the last linear layer.\nWhich layers are you checking and are you sure the checked frozen layer is in model.fc_layers?","z":"Could you check, if the parameters of the frozen layers get valid gradients after the backward call via:\n<code class=\"lang-python\">loss.backward()\nprint(self.model.frozen_layer.weight.grad)\nprint(self.model.last_layer.weight.grad)\n<\/code>\nIf the frozen layers yield None, while the last linear layer yields a valid gradient, your code is working as expected.\nhi, thx.\nIt has the error: the model has no \u2018\u2018frozen_layer\u2019\u2019 module.\nSo I change it to\n<code class=\"lang-auto\">print(self.model.fc_layers[-2].weight.grad)\nprint(self.model.last_layer.weight.grad)\n<\/code>\nSince the second last layer is frozen as I wrote the code in question.\nBut the output is not \u201cNone\u201d, is the gradient matrix.\nSo it is not working as expected I think.\nYes, the frozen_layer and last_layer are just placeholder names for your layer names.\nDid you train the \u201cfrozen\u201d layers before and forgot to zero out the gradients?\nIf not, could you post a code snippet which represents your training routine and how and where you are freezing the layers?\nYour initial code snippet looks like it has some indentation issues, as your \u201clayer loop\u201d would be used for each training iteration.\nIs this a copy-paste error or are you using this code in your script as shown here?\noh, sorry, yes, it\u2019s a copy-paste mistake.\nI train the frozen layer before, the function is as followed:\n<code class=\"lang-auto\">def step_mlp(self,X,y):\n        y_prime = self(X)\n        loss = self.criterion(y_prime, y)\n        \n        self.optimize.zero_grad()\n        self.zero_grad()\n        \n        loss.backward(retain_graph=1)\n        self.optimize.step()\n<\/code>\nThen re-train the model by frozen the layers except the last layer as the code in question decription.\nFrozen layer:\n<code class=\"lang-auto\">for layer in self.layers[0:-1]: \n        for param in self.model.fc_layers[layer].parameters():\n            param.requires_grad = False\n<\/code>\nThen re-train:\n<code class=\"lang-auto\">y_prime = self.model(X)\nloss = self.model.criterion(y_prime, y)\n        \nself.model.optimize.zero_grad()\nself.model.zero_grad()\nloss.backward(retain_graph=1)\nself.model.optimize.step()\n<\/code>\nYour general workflow should work as shown in this small example:\n<code class=\"lang-python\"># Setup\nmodel = models.resnet18()\ndata = torch.randn(1, 3, 224, 224)\ntarget = torch.randint(0, 1000, (1,))\ncriterion = nn.CrossEntropyLoss()\noptimizer = torch.optim.SGD(model.parameters(), lr=1e-3)\n\n# Train\noptimizer.zero_grad()\nout = model(data)\nloss = criterion(out, target)\nloss.backward()\noptimizer.step()\n\n# Freeze all but last layer\nfor name, param in model.named_parameters():\n    if not 'fc' in name:\n        param.requires_grad = False\n      \noptimizer.zero_grad()\nout = model(data)\nloss = criterion(out, target)\nloss.backward()\n\n# Check grads\ngrad_frozen = model.conv1.weight.grad\ngrad_fc = model.fc.weight.grad\nprint(grad_frozen.abs().sum())\nprint(grad_fc.abs().sum())\n\noptimizer.step()\n<\/code>\nAs you can see, after freezing all parameters but the fc.weight and fc.bias, I get a zero gradient for a previous layer, while I get valid gradients for the last linear layer.\nWhich layers are you checking and are you sure the checked frozen layer is in model.fc_layers?\nThanks for your detailed example, ptrblck!\nFind the error. It turns out that I didn\u2019t have any frozen layers in model.fc_layers because I make a mistake in my code.\nYour reply helps me to find it.  Thanks a lot!"},{"x":"when I apply Jacobian in PyTorch autograd (torch==1.5.0), it gives me three dimensional output.\nfor example,\n<code class=\"lang-auto\">def exp_reducer(x):\n    return (x**2).sum(dim=1)\ninputs = torch.rand(2, 2)\ntorch.autograd.functional.jacobian(exp_reducer, inputs)\n<\/code>\n<code class=\"lang-auto\">tensor([[[0.3296, 0.9791],\n         [0.0000, 0.0000]],\n\n        [[0.0000, 0.0000],\n         [1.6706, 0.7365]]])\n<\/code>\nand\n<code class=\"lang-auto\">torch.autograd.functional.jacobian(exp_reducer, inputs).shape\n<\/code>\n<code class=\"lang-auto\">(2,2,2)\n<\/code>\nbut I get a 2 dimensional output when using sympy,\n<code class=\"lang-auto\">x0, y0, x1, y1= sympy.symbols('x0, y0, x1, y1')\nf = Matrix([Function('g')(x0, x1), Function('h')(y0, y1)])\nk = Matrix([[x0, x1, y0, y1]])\nf.jacobian(k)\n<\/code>\nthen it give me, this,\nimage1782\u00d7293 18.2 KB with link \"https:\/\/discuss.pytorch.org\/uploads\/default\/original\/3X\/d\/6\/d6bbf608961f07401cf5f6e12d44728e11f5cc8f.png\"\nshould the shape after applying Jacobian be 2d or 3d?","y":"Hi,\nYes we follow a slightly different semantic.\nThe usual definition for multivariate functions is given a 1D input and 1D output, you end up with as 2D Jacobian.\nWhat sympy is doing here is to linearize the inputs from a (2, 2) Tensor into a (4,) Tensor. And so the Jacobian is (2, 4).\nBecause we usually allow multiple Tensors, people very often multiple dimensions and linearizing the Tensors can be confusing if you don\u2019t know how they are layed out in memory, we made a different choice here (similar to what tensorflow or jax are doing).\nHere we don\u2019t linearize Tensors and so you can index your Jacobian the same way you would index your original Tensor.","z":"Hi,\nYes we follow a slightly different semantic.\nThe usual definition for multivariate functions is given a 1D input and 1D output, you end up with as 2D Jacobian.\nWhat sympy is doing here is to linearize the inputs from a (2, 2) Tensor into a (4,) Tensor. And so the Jacobian is (2, 4).\nBecause we usually allow multiple Tensors, people very often multiple dimensions and linearizing the Tensors can be confusing if you don\u2019t know how they are layed out in memory, we made a different choice here (similar to what tensorflow or jax are doing).\nHere we don\u2019t linearize Tensors and so you can index your Jacobian the same way you would index your original Tensor.\nWould it be a good idea to add the symbolic representation to docs?\nWhen computing hessian, I get same dimension confusion,\nfor example,\n<code class=\"lang-auto\">def pow_adder_reducer(x, y):\n  print('x: ', x, 'y: ', y)\n  return (2 * x.pow(2) + 3 * y.pow(2)).sum()\ninputs = (torch.rand(2), torch.rand(2))\ntorch.autograd.functional.hessian(pow_adder_reducer, inputs)\n<\/code>\nPyTorch gives,\n<code class=\"lang-auto\">x:  tensor([0.4406, 0.9028], grad_fn=<ViewBackward>) y:  tensor([0.8790, 0.7052], grad_fn=<ViewBackward>)\n((tensor([[4., 0.],\n          [0., 4.]]), tensor([[0., 0.],\n          [0., 0.]])), (tensor([[0., 0.],\n          [0., 0.]]), tensor([[6., 0.],\n          [0., 6.]])))\n<\/code>\nfrom sympy I get,\nScreenshot (531)1346\u00d7539 19.3 KB with link \"https:\/\/discuss.pytorch.org\/uploads\/default\/original\/3X\/d\/8\/d854cb488227b539967694184824b7894a2d2d97.png\"\nis it doing something like,\n<code class=\"lang-auto\">f1 = 2*(x0**2) + 2*(x1**2)\nf2 = 3*(y0**3) + 3*(y1**3)\n<\/code>\n<code class=\"lang-auto\">hessian(f1, (x0, x1))\n<\/code>\n<code class=\"lang-auto\">hessian(f1, (y0, y1))\n<\/code>\n<code class=\"lang-auto\">hessian(f2, (x0, x1))\n<\/code>\n<code class=\"lang-auto\">hessian(f2, (y0, y1))\n<\/code>\nthat results in four 2x2 matrices.\nYes, this is a similar argument. The indexing is interleaved as output tensor, input tensor, output dim, input dim.\nYou can check the discussion we had about this on the original issue here: https:\/\/github.com\/pytorch\/pytorch\/issues\/30632#issuecomment-575315939\n\nWould it be a good idea to add the symbolic representation to docs?\n\nDefinitely open to suggestions to improve the doc !\nWhat do you mean here by \u201csymbolic representation\u201d? Could you give an example?\nLike this one,\nimage1035\u00d7169 11.3 KB with link \"https:\/\/discuss.pytorch.org\/uploads\/default\/original\/3X\/5\/a\/5a958736bac83f92a0b9ea247725aab4bc3bd05b.png\"\nand this one,\nScreenshot (533)1695\u00d7307 19.3 KB with link \"https:\/\/discuss.pytorch.org\/uploads\/default\/original\/3X\/a\/b\/ab97d00502e6b7ea068ccaae0cec22cd1473ed6b.png\"\nlike this symbolic representation added to docs, this makes it a bit easier to understand when we input >1d tensor.\nThanks, I added it to the issue to work on doc improvements here: https:\/\/github.com\/pytorch\/pytorch\/issues\/34998\nFeel free to post there if you have more ideas !"},{"x":"Hi,\nI\u2019m trying to implement the algorithm described here. with link \"https:\/\/arxiv.org\/abs\/1803.09050\"\nA short description of the related part is as follows.\nScreenshot from 2020-04-22 06-09-37527\u00d7523 48.5 KB with link \"https:\/\/discuss.pytorch.org\/uploads\/default\/original\/3X\/b\/5\/b57c6c089296ba2147aa73d9c884852e440ef74c.png\"\nMy attempt is as follows.\n<code class=\"lang-auto\"># 1. Forward-backward pass on training data\n_, (inputs, labels) =  next(enumerate(train_loader))\ninputs, labels = inputs.to(device=args.device, non_blocking=True),\\\n                        labels.to(device=args.device, non_blocking=True)\nmeta_model.load_state_dict(model.state_dict())\n\ny_hat_f = meta_model(inputs)\ncriterion.reduction = 'none'\nl_f = criterion(y_hat_f, labels)\neps = torch.rand(l_f.size(), requires_grad=False, device=args.device).div(1e6)\neps.requires_grad = True\nl_f = torch.sum(eps * l_f)\n\n# 2. Compute grads wrt model and update its params\nl_f.backward(retain_graph=True)\nmeta_optimizer.step()\n\n# 3. Forward-backward pass on meta data with updated model\n_, (inputs, labels) =  next(enumerate(meta_loader))\ninputs, labels = inputs.to(device=args.device, non_blocking=True),\\\n                        labels.to(device=args.device, non_blocking=True)\n\ny_hat_g = model(inputs)\ncriterion.reduction = 'mean'\nl_g = criterion(y_hat_g, labels)\n\n# 4. Compute grads wrt eps and update weights\neps_grads = torch.autograd.grad(l_g, eps)\n.....\n<\/code>\nAt this line:\n\neps_grads = torch.autograd.grad(l_g, eps)\n\nI get an error saying,\n\nRuntimeError: One of the differentiated Tensors appears to not have been used in the graph. Set allow_unused=True if this is the desired behavior.\n\nIf I set allow_used=True, it returns None to eps_grad.\nAs far as I can tell, autograd loses computation graph for some reason and doesn\u2019t retain the information that eps was used in the computation of l_f, which in turn used in updating parameters of the model.  So, l_g should be differentiable wrt eps but it doesn\u2019t work here.\nHow can I solve this ?","y":"Hi,\nI think you\u2019re missing a create_graph=True in the first backward if you want to be able to backprop through the backward pass.\nAlso pytorch\u2019s optimizers are not differentiable (yet). So you won\u2019t be able to backprop through that step either.\nI would recommend using the higher package to do this: https:\/\/github.com\/facebookresearch\/higher","z":"Hi,\nI think you\u2019re missing a create_graph=True in the first backward if you want to be able to backprop through the backward pass.\nAlso pytorch\u2019s optimizers are not differentiable (yet). So you won\u2019t be able to backprop through that step either.\nI would recommend using the higher package to do this: https:\/\/github.com\/facebookresearch\/higher\nso i should update the parameters manually, presumably looping through them and deducting gradients, given optimizers are not differentiable?\nYes,\nBut the tricky bit is that nn.Parameter() are built to be parameters that you learn. So they cannot have history. So you will have to delete these and replace them with the new updated values as Tensors (and keep them in a different place so that you can still update them with your optimizer).\nThat is why I recommended the library above that does all that for you \nGotcha. Thank a lot.\nThanks a lot for pointing me this library. It made things a lot easier.\nI\u2019ve a question regarding the performance though. Roughly, the code is 4-5 times slower than what it used to be. Given meta learning algorithm does 3x forward-backward passes, it seems sort of OK to me. Just in case, could you take a quick look and tell me whether I\u2019m doing something grossly inefficient in the following code ? Also, I\u2019ve been able to replicate an experiment from the paper so I believe it is OK from the correctness perspective.\n<code class=\"lang-auto\">for rnd in tqdm(range(1, args.epochs+1)):\n    model.train()\n    train_loss, train_acc = 0.0, 0.0 \n    \n    for _, (inputs, labels) in enumerate(train_loader):\n        inputs, labels = inputs.to(device=args.device, non_blocking=True),\\\n                        labels.to(device=args.device, non_blocking=True)\n        opt.zero_grad()\n        \n        with higher.innerloop_ctx(model, opt) as (meta_model, meta_opt):\n            # 1. Update meta model on training data\n            meta_train_outputs = meta_model(inputs)\n            criterion.reduction = 'none'\n            meta_train_loss = criterion(meta_train_outputs, labels)\n            eps = torch.rand(meta_train_loss.size(), requires_grad=False, device=args.device).div(1e6)\n            eps.requires_grad = True\n            meta_train_loss = torch.sum(eps * meta_train_loss)\n            meta_opt.step(meta_train_loss)\n            \n            # 2. Compute grads of eps on meta validation data\n            meta_val_outputs = meta_model(meta_inputs)\n            criterion.reduction = 'mean'\n            meta_val_loss = criterion(meta_val_outputs, meta_labels)\n            eps_grads = torch.autograd.grad(meta_val_loss, eps, only_inputs=True)[0].detach()\n            \n        # 3. Compute weights for current training batch\n        w_tilde = torch.clamp(-eps_grads, min=0)\n        l1_norm = torch.sum(w_tilde)\n        if l1_norm != 0:\n            w = w_tilde \/ l1_norm\n        else:\n            w = w_tilde\n            \n        # 4. Train model on weighted batch\n        outputs = model(inputs)\n        criterion.reduction = 'none'\n        minibatch_loss = criterion(outputs, labels)\n        minibatch_loss = torch.sum(w * minibatch_loss)\n        minibatch_loss.backward()\n        \n        torch.nn.utils.clip_grad_norm_(model.parameters(), max_norm=5)\n        opt.step()\n        \n        # keep track of epoch loss\/accuracy\n        train_loss += minibatch_loss.item()*outputs.shape[0]\n        _, pred_labels = torch.max(outputs, 1)\n        train_acc += torch.sum(torch.eq(pred_labels.view(-1), labels)).item()\n        \n    # inference after epoch\n    with torch.no_grad():\n        train_loss, train_acc = train_loss\/len(train_dataset), train_acc\/len(train_dataset)       \n        val_loss, (val_acc, val_per_class) = utils.get_loss_n_accuracy(model, criterion, val_loader, args)                                  \n        scheduler.step(val_loss)\n        # log\/print data\n        #writer.add_scalar('Validation\/Loss', val_loss, rnd)\n        #writer.add_scalar('Validation\/Accuracy', val_acc, rnd)\n        #writer.add_scalar('Training\/Loss', train_loss, rnd)\n        #writer.add_scalar('Training\/Accuracy', train_acc, rnd)\n        print(f'|Train\/Valid Loss: {train_loss:.3f} \/ {val_loss:.3f}|', end='--')\n        print(f'|Train\/Valid Acc: {train_acc:.3f} \/ {val_acc:.3f}|', end='\\r')\n<\/code>\nJust in case, full code:\n\n\ngithub.com with link \"https:\/\/github.com\/TinfoilHat0\/Learning-to-Reweight-Examples-for-Robust-Deep-Learning-with-PyTorch-Higher\/blob\/master\/Learning%20to%20Reweight%20Examples%20for%20Robust%20Deep%20Learning%20with%20PyTorch%20Higher.ipynb\"\n\n\nTinfoilHat0\/Learning-to-Reweight-Examples-for-Robust-Deep-Learning-with-PyTorch-Higher\/blob\/master\/Learning to Reweight Examples for Robust Deep Learning with PyTorch Higher.ipynb with link \"https:\/\/github.com\/TinfoilHat0\/Learning-to-Reweight-Examples-for-Robust-Deep-Learning-with-PyTorch-Higher\/blob\/master\/Learning%20to%20Reweight%20Examples%20for%20Robust%20Deep%20Learning%20with%20PyTorch%20Higher.ipynb\"\n<code class=\"lang-ipynb\">{\n \"cells\": [\n  {\n   \"cell_type\": \"code\",\n   \"execution_count\": 1,\n   \"metadata\": {},\n   \"outputs\": [],\n   \"source\": [\n    \"import sys\\n\",\n    \"sys.path.insert(1, '..\/src\/')\\n\",\n    \"import torch\\n\",\n    \"from tqdm.notebook import tqdm\\n\",\n    \"import torch.nn as nn\\n\",\n    \"import torch.optim as optim\\n\",\n    \"from torch.optim import lr_scheduler\\n\",\n    \"import numpy as np\\n\",\n    \"import torchvision\\n\",\n    \"from torchvision import datasets,transforms\\n\",\n    \"from torch.utils.tensorboard import SummaryWriter\\n\",\n    \"from torch.utils.data import DataLoader\\n\",\n<\/code>\n\n  This file has been truncated. show original with link \"https:\/\/github.com\/TinfoilHat0\/Learning-to-Reweight-Examples-for-Robust-Deep-Learning-with-PyTorch-Higher\/blob\/master\/Learning%20to%20Reweight%20Examples%20for%20Robust%20Deep%20Learning%20with%20PyTorch%20Higher.ipynb\"\n\n\n\n\n\nYour old code was not running any backward. So it was expected that this will be slower \nThe code looks ok to me."},{"x":"So I have the following code:\n<code class=\"lang-auto\">import torch\nimport torch.nn as nn\nimport math\n\ntorch.autograd.set_detect_anomaly(True)\n\ndef sigmoid(x):\n    return 1 \/ (1 + torch.exp(-x))\n\ndtype = torch.float\ndevice = torch.device(\"cpu\")\n# device = torch.device(\"cuda:0\") # Uncomment this to run on the cpu\n\n# N = batch size\n# D_in = input size\n# H = hidden dimension\n# D_out = output dimension\nTime, N, D_in, H, D_out = 7, 3, 10, 5, 2\n\n# Create random Tensors to hold input and outputs.\n# Setting requires_grad=False indicates that we do not need to compute gradients\n# with respect to thes Tensors during the backward pass.\nX = torch.randn(Time, N, D_in, device=device, dtype=dtype, requires_grad=False)\nY = torch.randn(Time, D_out, D_in, device=device, dtype=dtype, requires_grad=False)\nCell = torch.zeros(Time, H, D_in, device=device, dtype=dtype, requires_grad=False)\nHidden = torch.zeros(Time, H, D_in, device=device, dtype=dtype, requires_grad=False)\nyHat = torch.zeros(Time, D_out, D_in, device=device, dtype=dtype, requires_grad=False)\n\n# Create random Tensors for weights.\n# Setting requires_grad=True indicates that we want to compute gradients with \n# respect to these Tensors during the backward pass.\nWeightsForget = torch.randn(H, H+N, device=device, dtype=dtype, requires_grad=True)\nWeightsUpdate = torch.randn(H, H+N, device=device, dtype=dtype, requires_grad=True)\nWeightsCell = torch.randn(H, H+N, device=device, dtype=dtype, requires_grad=True)\nWeightsOutput = torch.randn(H, H+N, device=device, dtype=dtype, requires_grad=True)\n\n# Create random Tensors for Bias.\nBiasForget = torch.randn(H, 1, device=device, dtype=dtype, requires_grad=True)\nBiasUpdate = torch.randn(H, 1, device=device, dtype=dtype, requires_grad=True)\nBiasCell = torch.randn(H, 1, device=device, dtype=dtype, requires_grad=True)\nBiasOutput = torch.randn(H, 1, device=device, dtype=dtype, requires_grad=True)\n\n# Weights and Bias relating the hidden state to the output.\nWeights_HiddenToOutput = torch.randn(D_out, H, device=device, dtype=dtype, requires_grad=True)\nBias_HiddenToOutput = torch.randn(D_out, 1, device=device, dtype=dtype, requires_grad=True)\n\n# Initialize aNext and cNext\nhLast = Hidden[0,:,:]\ncLast = torch.zeros(hLast.shape)\n\nlearning_rate = 1e-6\nfor t in range(Time):\n\n    # Concat hLast and X(t)\n    concat = torch.zeros(H+N, D_in, device=device, dtype=dtype)\n    concat[: H, :] = hLast\n    concat[H :, :] = X[t,:,:] \n\n    forget = sigmoid(torch.mm(WeightsForget, concat) + BiasForget) \n    update = sigmoid(torch.mm(WeightsUpdate, concat) + BiasUpdate)\n    candCell = torch.tanh(torch.mm(WeightsCell, concat) + BiasCell)\n    cNext = forget * cLast + update * candCell\n    output = sigmoid(torch.mm(WeightsOutput, concat) + BiasOutput)\n    hNext = output * torch.tanh(cNext)\n\n    # Compute prediction of the LSTM Cell\n    prediction = torch.mm(Weights_HiddenToOutput, hNext) + Bias_HiddenToOutput\n\n    # Save the value of the next hidden state\n    Hidden[t,:,:] = hNext\n\n    # Save the value of the Prediction in Y\n    yHat[t,:,:] = prediction\n\n    # Save the value of the next Cell State\n    Cell[t,:,:] = cNext\n  \n    # Compute and print loss using operations on Tensors.\n    # Now loss is a Tensor of shape(1,)\n    loss = (-Y[t,:,:] * torch.log(prediction) - (1-Y[t,:,:]) * torch.log(1-prediction)).sum()\n    print('t, loss.item = ', t, loss.item()) \n    loss.backward(retain_graph=True)\n\n    with torch.no_grad():\n        # Update hLast and cLast\n        hLast = hNext\n        cLast = cNext\n        \n        # Update Weights\n        WeightsCell -= learning_rate * WeightsCell.grad\n        WeightsOutput -= learning_rate * WeightsOutput.grad\n        WeightsUpdate -= learning_rate * WeightsUpdate.grad\n        Weights_HiddenToOutput -= learning_rate * Weights_HiddenToOutput.grad\n\n        # Update Bias\n        BiasCell -= learning_rate * BiasCell.grad\n        BiasForget -= learning_rate * BiasForget.grad\n        BiasOutput -= learning_rate * BiasOutput.grad\n        BiasUpdate -= learning_rate * BiasUpdate.grad\n        Bias_HiddenToOutput -= learning_rate * Bias_HiddenToOutput.grad\n        \n        # Zero out the gradients\n        WeightsCell.grad.zero_()\n        WeightsForget.grad.zero_()\n        WeightsOutput.grad.zero_()\n        WeightsUpdate.grad.zero_()\n        Weights_HiddenToOutput.grad.zero_()\n        BiasCell.grad.zero_()\n        BiasForget.grad.zero_()\n        BiasOutput.grad.zero_()\n        BiasUpdate.grad.zero_()\n        print(Bias_HiddenToOutput.dtype)\n        Bias_HiddenToOutput.grad.zero_()\n<\/code>\nWhen I run it I get:\nRuntimeError: one of the variables needed for gradient computation has been modified by an inplace operation: [torch.FloatTensor [5, 8]] is at version 1; expected version 0 instead. Hint: the backtrace further above shows the operation that failed to compute its gradient. The variable in question was changed in there or anywhere later. Good luck!\nCommenting out the updates of the weights and biases will let the code run but doesn\u2019t update the weights and biases.  I\u2019m new to pytorch and I understand that this error has something to do with the inplace operation incrementing the version of the Tensor during the loss.backward() call.  But I thought the with torch.no_grad(): was supposed to let me update the Tensors without incrementing their version.\nSo I guess I have 2 questions\u2026  First, how do I get my code running??  Second, what am I missing about using with torch.no_grad()??  Because it doesn\u2019t work the way I thought.\nThanks!\nJP","y":"Hi,\nThe torch.no_grad() hides these ops from the autograd engine so that when you call .backward(), they won\u2019t be taken into account. So this is the right use here.\nThe error you\u2019re seeing is different: when the forward pass needs some Tensor to be able to compute the backward pass, this Tensor is saved. But if the user overrides the content of that Tensor we saved, then we can\u2019t compute the backward pass anymore. (the fact that you override it in a differentiable manner or not does not matter here. The value that we need does not exist anymore).\nI think the issue here is that you\u2019re missing a detach() when you do hLast = hNext.\nUnfortunately, because there is no operation done here, this does not work nicely with the torch.no_grad() \nIn python, doing this only assigns the same python object to a new name. So no pytorch op is called and we cannot detect this automatically\u2026\nYou will need to explicitly .detach() here or do any op like hLast = hNext.view_as(hNext) or hLast = hNext.clone() .","z":"Hi,\nThe torch.no_grad() hides these ops from the autograd engine so that when you call .backward(), they won\u2019t be taken into account. So this is the right use here.\nThe error you\u2019re seeing is different: when the forward pass needs some Tensor to be able to compute the backward pass, this Tensor is saved. But if the user overrides the content of that Tensor we saved, then we can\u2019t compute the backward pass anymore. (the fact that you override it in a differentiable manner or not does not matter here. The value that we need does not exist anymore).\nI think the issue here is that you\u2019re missing a detach() when you do hLast = hNext.\nUnfortunately, because there is no operation done here, this does not work nicely with the torch.no_grad() \nIn python, doing this only assigns the same python object to a new name. So no pytorch op is called and we cannot detect this automatically\u2026\nYou will need to explicitly .detach() here or do any op like hLast = hNext.view_as(hNext) or hLast = hNext.clone() .\nThanks!!  That was correct for that one.  But I still don\u2019t understand the \u2018with torch.no_grad()\u2019 function works.  On line 64 in the code below I do an inplace operation under the no_grad() function an it increments Weights_HiddenToOutput._version from 0 to 1.  I thought it wasn\u2019t supposed to do that under the no_grad() function.  Why does that happen??\n(Sorry about the strange code formatting.  I\u2019m new at this and I can\u2019t figure out how to put all the code in one block quote.)\n<code class=\"lang-python\">import torch\ntorch.autograd.set_detect_anomaly(True)\ndef sigmoid(x):\n    return 1\/(1+torch.exp(-x))\n\ndtype = torch.float\ndevice = torch.device(\"cpu\")\nTime, N, D_in, H, D_out = 7, 3, 10, 5, 2\nX = torch.randn(Time, N, D_in, device=device, dtype=dtype, requires_grad=False)\nY = torch.randn(Time, D_out, D_in, device=device, dtype=dtype, requires_grad=False)\nyHat = torch.randn(Time, D_out, D_in, device=device, dtype=dtype, requires_grad=False)\nHidden = 5\nWeightsForget = torch.randn(H, H+N, device=device, dtype=dtype, requires_grad=True)\nWeightsForget = torch.randn(H, H+N, device=device, dtype=dtype, requires_grad=True)\nWeightsUpdate = torch.randn(H, H+N, device=device, dtype=dtype, requires_grad=True)\nWeightsCell = torch.randn(H, H+N, device=device, dtype=dtype, requires_grad=True)\nWeightsOutput = torch.randn(H, H+N, device=device, dtype=dtype, requires_grad=True)\nBiasForget = torch.randn(H, 1, device=device, dtype=dtype, requires_grad=True)\nBiasUpdate = torch.randn(H, 1, device=device, dtype=dtype, requires_grad=True)\nBiasCell = torch.randn(H, 1, device=device, dtype=dtype, requires_grad=True)\nBiasOutput = torch.randn(H, 1, device=device, dtype=dtype, requires_grad=True)\nWeights_HiddenToOutput = torch.randn(D_out, H, device=device, dtype=dtype, requires_grad=True)\nBias_HiddenToOutput = torch.randn(D_out, 1, device=device, dtype=dtype, requires_grad=True)\nHidden = torch.zeros(Time, H, D_in, device=device, dtype=dtype, requires_grad=False)\nCell = torch.zeros(Time, H, D_in, device=device, dtype=dtype, requires_grad=False)\nhLast = torch.zeros(H, D_in, device=device, dtype=dtype, requires_grad=False)\nHidden[-1] = hLast\ncLast = torch.zeros(hLast.shape)\nCell[-1] = cLast\nlearning_rate = 1e-6\n\nfor e in range(100):\n    for t in range(len(X)):\n        forget = sigmoid(torch.mm(WeightsForget, torch.cat((X[t,:,:],hLast),0)) + BiasForget)\n        update = sigmoid(torch.mm(WeightsUpdate, torch.cat((X[t,:,:],hLast),0)) + BiasUpdate)\n        candCell = torch.tanh(torch.mm(WeightsCell, torch.cat((X[t,:,:],hLast),0)) + BiasCell)\n        cNext = forget * cLast + update * candCell\n        output = sigmoid(torch.mm(WeightsOutput, torch.cat((X[t,:,:],hLast),0)) + BiasOutput)\n        hNext = output * torch.tanh(cNext)\n\n        prediction = torch.softmax((torch.mm(Weights_HiddenToOutput, hNext) + Bias_HiddenToOutput),0)\n\n        Hidden[t] = hNext\n        hLast = hNext.clone()\n        Cell[t] = cNext\n        cLast = cNext.clone()\n        yHat[t] = prediction\n\n    loss = (yHat - Y).pow(2).sum()\n    loss.backward(retain_graph=True)\n\n    with torch.no_grad():\n        # Update Weights\n        WeightsCell.data -= learning_rate * WeightsCell.grad\n        WeightsOutput.data -= learning_rate * WeightsOutput.grad\n        print('WeightsUpdate._version = ', WeightsUpdate._version)\n        WeightsUpdate.data -= learning_rate * WeightsUpdate.grad\n        print('WeightsUpdate._version = ', WeightsUpdate._version)\n        WeightsForget.data -= learning_rate * WeightsForget.grad\n        print('Weights_HiddenToOutput.shape = ', Weights_HiddenToOutput.shape)\n        print('Weights_HiddenToOutput._version = ', Weights_HiddenToOutput._version)\n        Weights_HiddenToOutput -= learning_rate * Weights_HiddenToOutput.grad\n        print('Weights_HiddenToOutput._version = ', Weights_HiddenToOutput._version)\n        \n        # Update Bias\n        BiasCell -= learning_rate * BiasCell.grad\n        BiasForget -= learning_rate * BiasForget.grad\n        BiasOutput -= learning_rate * BiasOutput.grad\n        BiasUpdate -= learning_rate * BiasUpdate.grad\n        Bias_HiddenToOutput -= learning_rate * Bias_HiddenToOutput.grad\n        \n        # Zero out the gradients\n        WeightsCell.grad.zero_()\n        WeightsForget.grad.zero_()\n        WeightsOutput.grad.zero_()\n        WeightsUpdate.grad.zero_()\n        Weights_HiddenToOutput.grad.zero_()\n        BiasCell.grad.zero_()\n        BiasForget.grad.zero_()\n        BiasOutput.grad.zero_()\n        BiasUpdate.grad.zero_()\n        Bias_HiddenToOutput.grad.zero_()\n<\/code>\nHi,\nFor code formatting you can use triple backticks ``` before and after. I edited your post, you can take a look by editing again to see what the raw text looks like.\n\nI thought it wasn\u2019t supposed to do that under the no_grad() function. Why does that happen??\n\nIt depends on what else you\u2019re doing. If you don\u2019t do any op that saves the value of that Tensor, it is fine to modify it inplace (this is the case here).\nIf you do require the original value (like in your original post), then the autograd engine will prevent you from doing that.\nNote that torch.no_grad is irrelevant here. And the check happens the same way with or without it (even though the computed gradients will be different of course).\nSo if it\u2019s irrelevant here,  Why am I still getting the error:\nRuntimeError: one of the variables needed for gradient computation has been modified by an inplace operation: [torch.FloatTensor [2, 5]] is at version 1; expected version 0 instead. Hint: the backtrace further above shows the operation that failed to compute its gradient. The variable in question was changed in there or anywhere later. Good luck!\nBecause you modify inplace a Tensor that is needed for backward.\nSo you should use anomaly mode to get more informations. And remove the corresponding inplace (or add a clone() to make sure the saved Tensor itself is not modified).\nForget it I just figured out what my problem is.  Line 64 should be:\n<code class=\"lang-auto\">Weights_HiddenToOutput.data -= learning_rate * Weights_HiddenToOutput.grad\n<\/code>\nNot:\n<code class=\"lang-auto\">Weights_HiddenToOutput -= learning_rate * Weights_HiddenToOutput.grad\n<\/code>\nThanks for your help though."},{"x":"Hello,\nSometimes my program crashes with the following stack trace:\n<code class=\"lang-auto\">File \"\/Users\/GeorgesKanaan\/Documents\/Development\/Python\/KNet\/src\/main_scripts\/train.py\", line 101, in l1\n    penalty += param.norm(1)\n  File \"\/Users\/GeorgesKanaan\/Documents\/Development\/Python\/KNet\/venv\/lib\/python3.8\/site-packages\/torch\/tensor.py\", line 342, in norm\n    return torch.norm(self, p, dim, keepdim, dtype=dtype)\n  File \"\/Users\/GeorgesKanaan\/Documents\/Development\/Python\/KNet\/venv\/lib\/python3.8\/site-packages\/torch\/functional.py\", line 838, in norm\n    return _VF.norm(input, p)\n (print_stack at ..\/torch\/csrc\/autograd\/python_anomaly_mode.cpp:60)\nTraceback (most recent call last):\n  File \"\/Users\/GeorgesKanaan\/Documents\/Development\/Python\/KNet\/src\/main_scripts\/train.py\", line 58, in train\n    total_loss.backward()\n  File \"\/Users\/GeorgesKanaan\/Documents\/Development\/Python\/KNet\/venv\/lib\/python3.8\/site-packages\/torch\/tensor.py\", line 198, in backward\n    torch.autograd.backward(self, gradient, retain_graph, create_graph)\n  File \"\/Users\/GeorgesKanaan\/Documents\/Development\/Python\/KNet\/venv\/lib\/python3.8\/site-packages\/torch\/autograd\/__init__.py\", line 98, in backward\n    Variable._execution_engine.run_backward(\nRuntimeError: 'ActiveGradsHook' object has no attribute '__name__'\n<\/code>\nActiveGradsHook is below:\n<code class=\"lang-auto\">class ActiveGradsHook:\n    \"\"\"\n    Resets the gradient according to the passed masks.\n    \"\"\"\n\n    def __init__(self, previously_active: [bool], currently_active: [bool], bias=False):\n\n        # Could be None for biases\n        if previously_active is not None:\n            self.previously_active = torch.BoolTensor(previously_active).long().nonzero().view(-1).numpy()\n\n        # Should never be None\n        self.currently_active = torch.BoolTensor(currently_active).long().nonzero().view(-1).numpy()\n\n        self.is_bias = bias\n\n    def __call__(self, grad):\n        print(\"called hook\")\n        grad_clone = grad.clone().detach()\n        print(\"cloned correctly\")\n\n        if self.is_bias:\n            grad_clone[self.currently_active] = 0\n\n        else:\n            grad_clone[self.previously_active, :] = 0\n            grad_clone[:, self.currently_active] = 0\n\n        return grad_clone\n<\/code>\nIt is passed as an initialized object as so on params:\n<code class=\"lang-auto\">hook = ActiveGradsHook(previously_active_weights, active_weights, bias=False)\nparam.register_hook(hook)\n<\/code>\nWhy does pytorch expect the class to have an attribute __name__?","y":"So testing the code and changing the hook to:\n<code class=\"lang-auto\">    def __call__(self, grad):\n        try:\n            grad_clone = grad.clone().detach()\n\n            if self.is_bias:\n                grad_clone[self.currently_active] = 0\n\n            else:\n                grad_clone[self.currently_active, :] = 0\n                grad_clone[:, self.previously_active] = 0\n\n            return grad_clone\n        except Exception as e:\n            print(\"bad hook\")\n            print(e)\n<\/code>\nThen the issue comes from your hook raising errors.\nI am not sure why these errors are not properly propagated but you can fix your issue by simply fixing the issues printed above.","z":"Hi,\nWe only ever call the hooks. So this is most likely something else.\nCan you do a small code sample that reproduces this error please? I would be interested to investigate it further.\nHello, I can work on getting you a small sample. You should know that when I add self.__name__ = None this error as well as any subsequent errors go away, most directly when I remove that attribute I get:\n<code class=\"lang-auto\">  File \"\/home\/gio\/Documents\/KNet\/venv\/lib\/python3.7\/site-packages\/torch\/autograd\/__init__.py\", line 100, in backward\n    allow_unreachable=True)  # allow_unreachable flag\nRuntimeError: <method 'clone' of 'torch._C._TensorBase' objects> returned a result with an error set\n<\/code>\nUnsure what modifications I made got it to not call name anymore, or if that error is getting silenced.\nHere\u2019s a script that reproduces the bug \n<code class=\"lang-auto\">import torch.nn as nn\nimport torch\nimport numpy as np\nimport torchvision.transforms as transforms\nimport torchvision.datasets as datasets\nimport os\n\nfrom torch.utils.data import DataLoader, RandomSampler\n\n# You should be able to ignore this\nclass SimpleModel(nn.Module):\n    def __init__(self, sizes, oldWeights=None, oldBiases=None):\n        super(SimpleModel, self).__init__()\n        self.phase = 'ACTION'\n\n        self.sizes = sizes\n\n        # Encoder\n        encoder_layers = self.set_module('encoder', oldWeights=oldWeights, oldBiases=oldBiases)\n        encoder_layers.append(nn.Sigmoid())\n        self.encoder = nn.Sequential(*encoder_layers)\n\n        # Make float\n        self.float()\n\n    def forward(self, x):\n        x = self.encoder(x)\n        return x\n\n    def set_module(self, label, oldWeights=None, oldBiases=None):\n        sizes = self.sizes[label]\n\n        if oldWeights:\n            oldWeights = oldWeights[label]\n        if oldBiases:\n            oldBiases = oldBiases[label]\n\n        layers = [\n            self.get_layer(sizes[0], sizes[1], oldWeights, oldBiases, 0)\n        ]\n\n        for i in range(1, len(sizes) - 1):\n            layers.append(nn.LeakyReLU())\n            layers.append(self.get_layer(sizes[i], sizes[i+1], oldWeights, oldBiases, i))\n\n        return layers\n\n    def get_layer(self, input, output, init_weights=None, init_biases=None, index=0):\n        layer = nn.Linear(input, output)\n        torch.nn.init.kaiming_uniform_(layer.weight, mode='fan_in', nonlinearity='leaky_relu')\n\n        if init_weights is not None:\n            weights = init_weights[index]\n\n            # Type checking\n            if isinstance(weights, list):\n                weights = np.asarray(weights, dtype=float)\n\n            if isinstance(weights, np.ndarray):\n                weights = torch.from_numpy(weights)\n\n            if isinstance(weights, torch.Tensor):\n                weights = nn.Parameter(weights)\n\n            if isinstance(weights, torch.nn.Parameter):\n                layer.weight = weights\n\n            # Padding\n            weights = layer.weight.detach()\n\n            if input != weights.shape[1]:\n                kaiming_weights = torch.rand(weights.shape[0], input - weights.shape[1]).to(weights.device)\n                torch.nn.init.kaiming_uniform_(kaiming_weights, mode='fan_in', nonlinearity='leaky_relu')\n\n                weights = torch.cat([weights.float(), kaiming_weights.float()], dim=1)\n\n            if output != weights.shape[0]:\n                kaiming_weights = torch.rand(output - weights.shape[0], input).to(weights.device)\n                torch.nn.init.kaiming_uniform_(kaiming_weights, mode='fan_in', nonlinearity='leaky_relu')\n\n                weights = torch.cat([weights.float(), kaiming_weights.float()], dim=0)\n\n            # Set\n            layer.weight = nn.Parameter(weights)\n\n        if init_biases is not None:\n            biases = init_biases[index]\n\n            # Type checking\n            if isinstance(biases, list):\n                biases = np.asarray(biases, dtype=float)\n\n            if isinstance(biases, np.ndarray):\n                biases = torch.from_numpy(biases)\n\n            if isinstance(biases, torch.Tensor):\n                biases = nn.Parameter(biases)\n\n            if isinstance(biases, torch.nn.Parameter):\n                layer.bias = biases\n\n            # Padding\n            biases = layer.bias.detach()\n\n            if output != biases.shape[0]:\n                rand_biases = torch.rand(output - biases.shape[0]).to(biases.device)\n\n                biases = torch.cat([biases.float(), rand_biases.float()], dim=0)\n\n            # Set\n            layer.bias = nn.Parameter(biases)\n\n            # Update the oldBiases to include padding\n            init_biases[index] = layer.bias.detach()\n\n        return layer\n\ndef get_modules(model):\n    modules = {}\n\n    for name, param in model.named_parameters():\n        module = name[0: name.index('.')]\n\n        if module not in modules.keys():\n            modules[module] = []\n\n        modules[module].append((name, param))\n\n    return modules\n\n# Hook class\nclass ActiveGradsHook:\n    \"\"\"\n    Resets the gradient according to the passed masks.\n    \"\"\"\n\n    def __init__(self, previously_active: [bool], currently_active: [bool], bias=False):\n\n        # Could be None for biases\n        if previously_active is not None:\n            self.previously_active = torch.BoolTensor(previously_active).long().nonzero().view(-1).numpy()\n\n        # Should never be None\n        self.currently_active = torch.BoolTensor(currently_active).long().nonzero().view(-1).numpy()\n\n        self.is_bias = bias\n\n        # self.__name__ = None # THIS WILL FIX YOUR CRASH\n\n    def __call__(self, grad):\n        grad_clone = grad.clone().detach()\n\n        if self.is_bias:\n            grad_clone[self.currently_active] = 0\n\n        else:\n            grad_clone[self.currently_active, :] = 0\n            grad_clone[:, self.previously_active] = 0\n\n        return grad_clone\n\ndef train_new_neurons(old_sizes, new_sizes, model, batch_loader, optimizer):\n    if old_sizes == new_sizes:\n        print(\"No new neurons to train.\")\n        return (None, None)\n\n    print(\"Training new neurons...\")\n\n    # Generate hooks for each layer\n    hooks = []\n    modules = get_modules(model)\n\n    for module_name, parameters in modules.items():\n        previously_active_weights = [False] * new_sizes[module_name][0]\n\n        for param_name, param in parameters:\n            split_param_name = param_name.split(\".\")  # Splits action.0.weights\n            param_index = int(split_param_name[1])\n\n            # Map every two indices to one\n            param_index -= param_index % 2\n            param_index \/= 2\n            param_index = int(param_index)\n\n            old_size = old_sizes[module_name][param_index + 1]\n            new_size = new_sizes[module_name][param_index + 1]\n            neurons_added = new_size - old_size\n\n            # Input\/Output must stay the same\n            if param_index == len(old_sizes[module_name]) - 1:\n                assert old_size == new_size\n                continue\n\n            # Freeze biases\/weights\n            if \"bias\" in param_name:\n                active_biases = [False] * old_size + [True] * neurons_added\n                hook = ActiveGradsHook(None, active_biases, bias=True)\n\n                hook = param.register_hook(hook)\n                hooks.append(hook)\n\n            else:\n\n                active_weights = [False] * old_size + [True] * neurons_added\n                hook = ActiveGradsHook(previously_active_weights, active_weights, bias=False)\n\n                hook = param.register_hook(hook)\n                hooks.append(hook)\n\n                previously_active_weights = active_weights\n\n    # Train simply\n    train(batch_loader, model, optimizer)\n\n    # Remove hooks\n    for hook in hooks:\n        hook.remove()\n\n\ndef train(batch_loader, model, optimizer):\n\n    for batch_idx, (inputs, action_target) in enumerate(batch_loader):\n\n        with torch.autograd.detect_anomaly():\n\n            action_output = model(inputs)\n\n            criterion = torch.nn.BCELoss()\n            action_loss = criterion(action_output, action_target)\n\n            # Compute gradient and do SGD step\n            optimizer.zero_grad()\n            action_loss.backward()\n            optimizer.step()\n\n\ndef get_mnist_loader():\n    dataset = datasets.MNIST\n\n    def one_hot_mnist(targets):\n        targets_onehot = torch.zeros(10)\n        targets_onehot[targets] = 1\n        return targets_onehot\n\n    transform_all = transforms.Compose([\n        transforms.ToTensor(),\n        transforms.Normalize((0.1307,), (0.3081,)),\n        transforms.Lambda(lambda a: a.view(-1))\n    ])\n\n    root = os.path.join(os.path.dirname(__file__), \"..\/..\/data\/MNIST\")\n    dataset = dataset(root=root, train=True, download=True, transform=transform_all, target_transform=one_hot_mnist)\n\n    sampler = RandomSampler(dataset)\n    return DataLoader(dataset, sampler=sampler, batch_size=128, num_workers=0, pin_memory=False)\n\n\nif __name__ == \"__main__\":\n    sizes = {\"encoder\": [784, 312, 128, 10]}\n    new_sizes = {\"encoder\": [784, 624, 256, 10]}\n\n    model = SimpleModel(sizes, None, None)\n    optimizer = torch.optim.SGD(model.parameters(), lr=1, momentum=0)\n    mnist_loader = get_mnist_loader()\n    train(mnist_loader, model, optimizer)\n\n    train_new_neurons(sizes, new_sizes, model, mnist_loader, optimizer)\n<\/code>\nSo testing the code and changing the hook to:\n<code class=\"lang-auto\">    def __call__(self, grad):\n        try:\n            grad_clone = grad.clone().detach()\n\n            if self.is_bias:\n                grad_clone[self.currently_active] = 0\n\n            else:\n                grad_clone[self.currently_active, :] = 0\n                grad_clone[:, self.previously_active] = 0\n\n            return grad_clone\n        except Exception as e:\n            print(\"bad hook\")\n            print(e)\n<\/code>\nThen the issue comes from your hook raising errors.\nI am not sure why these errors are not properly propagated but you can fix your issue by simply fixing the issues printed above.\nNoted thank you. It\u2019s possible that it\u2019s not propagated up because it\u2019s on a different thread? I noticed my breakpoints don\u2019t work either for that reason. Is it worth opening an issue on github about this ?\nHey,\nYes I think that would be worth it.\nCould you try making the code sample even smaller before sending it there? Like a hook that just throws an error and nothing else. And the minimal code around to make it loose the error \nThanks !\nWill do. The underlying error is that clone fails though I don\u2019t understand why that happens to create it yet. I get:`\n<code class=\"lang-auto\">The above exception was the direct cause of the following exception:\n\nTraceback (most recent call last):\n  File \"\/home\/gio\/Documents\/KNet\/src\/main_scripts\/den_trainer.py\", line 428, in __call__\n    grad_clone = grad.clone().detach()\n<\/code>\nClone returned a result with an error set."},{"x":"I have one vector and one 2-D vector. Say embed1= [1, 2, 3] shape is (3) and embed2=[[1, 2, 3], [4,5,6], [7,8,9]] shape is (3, 3). Now I want to expand embed1 to [[1, 2, 3], [1, 2, 3], [1, 2, 3]] so that I can do torch.cat() to concatenate embed1 with embed2 and send them to a MLP. Actually this is to avoid of doing for loop by concatenating embed1 with each vector in embed2 and then send each concatenated vector to MLP.  When I read the document, it says that torch.repeat will copy data so I don\u2019t know if this will affect the backpropagation since data are copied instead of the original one. I hope all the autograd process will only update my original embed1.","y":"Hi,\nBackprop-wise, they will give the exact same result.\nThe difference is that if the original dimension you want to expand is of size 1, you can use torch.expand() to do it without using extra memory.\nIf the dimension you want to expand is of size more than 1, then you actually want to repeat what is at that dimension and you should use torch.repeat(). It will use extra memory (there is no way around that).","z":"Hi,\nBackprop-wise, they will give the exact same result.\nThe difference is that if the original dimension you want to expand is of size 1, you can use torch.expand() to do it without using extra memory.\nIf the dimension you want to expand is of size more than 1, then you actually want to repeat what is at that dimension and you should use torch.repeat(). It will use extra memory (there is no way around that).\nTherefore, when expanding a dimension of size 1, should torch.expand() always be used instead of torch.repeat()?\nYes.\nKeep in mind though that if you plan on changing this expanded tensor inplace, you will need to use .clone() on it before so that it actually is a full tensor (with memory for each element). But even .expand().clone() should be faster than .repeat() I think.\nWhat about .contiguous instead of .clone ? Wouldn\u2019t those 2 operations achieve the same goal before inplace modifications?\nI might be going a bit off-topic, I am new to PyTorch and am trying to get what are best practices.\nHi,\nIn this case yes they will do the same thing. Because the input is allways non contiguous \nThank you for your answer.\nHi,\nA and B are feature maps, in shape of [12, 9, 64] each.\nI have this code to concatenate them 9 times in different cells:\nA = A.repeat(1, 1, 9).view(12, 81, 64)\nB = B.repeat(1, 9, 1)\nC = torch.cat((A, B), dim=2)\nHowever, I faced with lack of memory to run this code. Is it possible to use expand instead of view, here also?\nThanks!\nSo to make this a self-contained example:\n<code class=\"lang-python\">A = torch.randn([12, 9, 64])\nB = torch.randn([12, 9, 64])\nAr = A.repeat(1, 1, 9).view(12, 81, 64)\nBr = B.repeat(1, 9, 1)\nC = torch.cat((Ar, Br), dim=2)\nD = torch.cat([A.unsqueeze(2).expand(-1, -1, 9, -1), \n               B.unsqueeze(1).expand(-1, 9, -1, -1)], dim=-1).view(12, 81, 128)\nprint ((C-D).abs().max().item())\n<\/code>\ngives 0 and from A, B to D only the memory for D is newly allocated.\nBest regards\nThomas\nP.S.: Enclose multiple lines of source code with triple backticks ``` to have them rendered nicely. Single backticks for \u201cin-text\u201d commands.\nThomas thanks for your reply. It was helpful, however, still there is memory problem!!\n\u201ccuda out of memory\u201d\nHi.\nI have a somewhat related query so thought I\u2019d ask here instead of starting a new thread.\nSo, I am trying to use VGG as a feature extractor. My model returns a (grayscale) image tensor of size (1, 512, 512) that I want to feed into VGG which takes in 3 channel RGB images. The idea is to compute a loss based on the output of VGG and backprop that into my model.\nSo, if I repeat the grayscale across the 3 dimensions before feeding into VGG, will .backward() successfully backprop into my model? Some code snippets would be quite helpful.\nThanks.\nHi,\nYe, both repeat and expand are differentiable. So you can use them in your net as any other op."},{"x":"Hi all,\nDo applying Random Transformation (flip, crop rotate or any etc.) with p=1 means it is applied to all tensors\/PIL images that are passed?\nOR\nit is like, if p=1 then any randomly chosen PIL image\/Tensor will be be transformed because it is now selected and those skipped tensors\/PIL images will be preserved as they were before?\nI am confused in this command, please help me out.\nThanks in advance.","y":"The probability p will be used to determine, if the current image passed through the transformation will be transformed or not as seen e.g. here with link \"https:\/\/github.com\/pytorch\/vision\/blob\/74de51d6d478e289135d9274e6af550a9bfba137\/torchvision\/transforms\/transforms.py#L615-L617\".","z":"dear , please guide me through this.\nThe probability p will be used to determine, if the current image passed through the transformation will be transformed or not as seen e.g. here with link \"https:\/\/github.com\/pytorch\/vision\/blob\/74de51d6d478e289135d9274e6af550a9bfba137\/torchvision\/transforms\/transforms.py#L615-L617\"."},{"x":"Hi there. Let\u2019s say we obtain a set of gradients for a set of weights as follows:\n<code class=\"lang-auto\">grad = torch.autograd.grad([output], [m for m in self.parameters()],\n                                    grad_outputs=[grad_input],\n                                    create_graph=True, retain_graph=retain_graph,\n                                    only_inputs=True)\nfor i, m in enumerate(self.parameters()):\n        m.data = m.data - lr * grad[i]\n<\/code>\nLet the model that had its weight updated by net. We then perform a forward pass using the weight updated model as net(input). As I\u2019ve retained the graph in the autograd performed above, when I\u2019m performing loss.backward(), I\u2019d like the gradient to flow through the grad_input (backproped from the grad[i]'s) variable mentioned above. Any idea how to achieve this?\nHere\u2019s a simple flow of the program:\n<code class=\"lang-auto\">gradOut = self.netA(input_data)\nself.netB.paramUpdate(gradOut) # performs the code snippet provided above\nout = self.netB(input_data)\nloss = self.criterion(out, label)\nself.optimizer.zero_grad()\nloss.backward() # I hope that the gradients can flow from netB to netA\n<\/code>","y":"You will have to differentiate between the leant parameter in netB and the ones that are computed at each forward.\nOne thing you cal do is rename parameters like weight into weight_orig. Then del the weight. And in the forward part, set the weight as weight_orig + your_udpate.","z":"HI,\nHi, the problem is that you use .data which breaks that history and prevent gradient computation.\nIn particular, if netB does not have any learnable parameters, you should remove them (del netB.some_mod.weight for example) and then fill it in with the value computed: netB.some_mod.weight = lr * grad[i]\nHi , thanks for the reply.\nThe reason of using .data is because if I don\u2019t use it, I realize the the parameter of netB won\u2019t be updated.\nBoth netA and netB have learnable parameters. The idea is that when I\u2019m doing a forward pass on netB, its parameter is a composition of netB's original parameter plus a gradient provided by netA. So when I\u2019m doing backprop, I\u2019m hoping that the gradient will update both netB and netA as the provided gradient should be part of the computational graph.\nI found that pytorch-meta with link \"https:\/\/github.com\/tristandeleu\/pytorch-meta\" has a similar approach where they pre-compute the updated parameter and re-insert it into the existing model. Is this the most elegant way to accomplish this?\nYou will have to differentiate between the leant parameter in netB and the ones that are computed at each forward.\nOne thing you cal do is rename parameters like weight into weight_orig. Then del the weight. And in the forward part, set the weight as weight_orig + your_udpate.\n Thanks! This seems like the only way to get this done. It\u2019s working fine now using this implementation."},{"x":"Hi there I was trying to use nn.Sequential module in a simple regression problem, just a line. The thing is that I print MSELoss every iteration and it is trying to converge at 1, intead of the values I am wondering.\nHere is the code:\n<code class=\"lang-auto\">NN = nn.Sequential(\n    nn.Linear(1,3),\n    nn.Sigmoid(),\n    nn.Linear(3,4),\n    nn.Sigmoid(),\n    nn.Linear(4,7),\n    nn.Sigmoid(),\n    nn.Linear(7,1),\n    nn.Sigmoid()\n)\n\ncriterion = nn.MSELoss()\noptimizer = optim.Adam(NN.parameters(), lr=0.01)\n\nX = torch.rand(200,1)\ny = 2*X + 3\n\nprint(X.shape,y.shape)\nfor i in range(1000):  \n    NN.train()\n    optimizer.zero_grad()\n    out = NN(X)\n    #print(out,y)\n    loss = criterion(y,out)\n    loss.backward()\n    optimizer.step()\n\n    with torch.no_grad():\n        print (\"#\" + str(i) + \" Loss: \" + str(loss.item()))  \n<\/code>\nIf I try this to check the results:\n<code class=\"lang-auto\">newX = []\nnewY = []\nfor i in range(5):\n    r = random.uniform(0, 1)\n    newX.append(r)\n    newY.append(NN(torch.tensor([r])).item())\n<\/code>\nI get [0.9996629953384399, 0.9996600151062012, 0.9996635913848877, 0.9996637105941772, 0.9996615648269653].\nI have tryed lot of things changing every on the network but don\u2019t know what is happening.","y":"Hi,\nThere is an issue with your model architecture, your target values are all greater than one (because of 2x+3 operation) and last operation in your network is nn.Sigmoid() which will return values in the range 0 to 1 so your model would never be able to converge.","z":"Hi,\nThere is an issue with your model architecture, your target values are all greater than one (because of 2x+3 operation) and last operation in your network is nn.Sigmoid() which will return values in the range 0 to 1 so your model would never be able to converge.\nOh,I feel so silly its true. And what should I do, change the structure of the network or normalize inputs and outputs values?\nJust remove last nn.Sigmoid() operation from your model.\nThat works. Thank you really much!!"},{"x":"I have to apply the following operations on a video (frame-wise)\n\nRGB to grayscale\nCenter Crop\nColor Normalize\nGive the video (the sequence of the frames) as input to a model.\n\nMoreover, I have to compute gradients over these operations later. I asked this Question with link \"https:\/\/discuss.pytorch.org\/t\/gradients-over-transformation-operations\/73934\" and was suggested to use Kornia with link \"https:\/\/github.com\/kornia\/kornia\" which supports gradients over these operations, which works well for me.\nThere is another problem though. The model expects an input of the shape (1, #frames, height, width) whereas the above operations expect the shape (#frames, #channels, height, width). So I need to apply reshape() operation, due to which I am unable to compute the gradients. Kindly provide me with a workaround or point out what am I doing wrong.","y":"Hi,\nThe reshape operation is differentiable in pytorch. So it should not prevent you from computing any gradient.\nAlso I think you want to permute dimensions, not reshape them: input = input.permute(1, 0, 2, 3).","z":"Hi,\nThe reshape operation is differentiable in pytorch. So it should not prevent you from computing any gradient.\nAlso I think you want to permute dimensions, not reshape them: input = input.permute(1, 0, 2, 3).\nThanks a lot. Is the permute operation differentiable too?\nYes. In general, all the operations in pytorch that take continuous numbers and return continuous numbers are differentiable."},{"x":"Hi,\nI\u2019m having a weird problem with autograd in recent PyTorch. I\u2019m trying to make a simple example following the official documentation, but instead of in Python, I want to do it in C++ for basically having a test case. Here\u2019s my code:\n<code class=\"lang-auto\">#include <iostream>\n#include <torch\/torch.h>\n\nint main() {\n  if (torch::cuda::is_available()) {\n    std::cout << \"CUDA device is available.\\n\";\n  } else {\n    std::cout << \"CUDA device is not available.\\n\";\n  }\n  try {\n    auto test = torch::ones({2, 2}, at::requires_grad()).cuda();\n    if (test.device().type() == torch::kCUDA) {\n      std::cout << \"We are using CUDA\\n\";\n    } else if (test.device().type() == torch::kCPU) {\n      std::cout << \"Oops... we are supposed to run on CUDA GPU, but we are running on CPU.\";\n    } else {\n      std::cout << \"Uuuhm, we are supposed to run on CUDA GPU, but we are not even running on CPU.\";\n    }\n    auto a = torch::ones({2, 2}, torch::TensorOptions().dtype(torch::kFloat).requires_grad(true)).cuda();\n    auto b = torch::randn({2, 2}, torch::TensorOptions().dtype(torch::kFloat)).cuda();\n    std::cout << \"a: \" << a << \"\\n\";\n    auto c = a + b;\n    std::cout << \"c: \" << c << \"\\n\";\n    auto cgrad = c.grad();\n    std::cout << \"cgrad : \" << cgrad << \"\\n\";\n    \/\/ tried this - doesn't work either\n    \/\/ torch::Tensor a_grad = torch::tensor({1.0, 0.1, 1.0, 0.01}, torch::TensorOptions().dtype(torch::kFloat)).cuda().view({2,2});\n    torch::Tensor a_grad = torch::tensor({1.0, 1.0, 1.0, 1.0}, torch::TensorOptions().dtype(torch::kFloat)).cuda().view({2,2});\n    std::cout << \"a_grad: \" << a_grad << \"\\n\";\n    \/\/ tried this as well, also doesn't work\n    \/\/ c.sum().backward(a_grad);  \/\/ a.grad() will now hold the gradient of c w.r.t. a.\n    c.sum().backward();  \/\/ a.grad() will now hold the gradient of c w.r.t. a.\n    std::cout << \"a.grad(): \"<< a.grad() << \"\\n\";\n    std::cout << \"a_grad: \"<< a_grad << \"\\n\";\n    torch::Tensor g_loss = torch::binary_cross_entropy(a, b);\n    g_loss.backward();\n    std::cout << \"g_loss: \" << g_loss << \"\\n\";\n    std::cout << \"a.grad(): \" << a.grad() << \"\\n\";\n  } catch (const c10::Error&amp; e) {\n    \/\/ this was the original code (for CPU) that worked fine on PyTorch 1.2\n    std::cout << e.what() << \"\\n\";\n    auto test = torch::ones({2, 2}, at::requires_grad());\n    if (test.device().type() == torch::kCPU) {\n      std::cout << \"We are using CPU\\n\";\n    } else if (test.device().type() == torch::kCUDA) {\n      std::cout << \"Oops... we are supposed to run on CPU, but we are running on CUDA GPU.\";\n    } else {\n      std::cout << \"Uuuhm, we are supposed to run on CPU, but we are not even running on CUDA GPU.\";\n    }\n    auto a = torch::ones({2, 2}, at::requires_grad());\n    auto b = torch::randn({2, 2});\n    auto c = a + b;\n    c.backward(a); \/\/ a.grad() will now hold the gradient of c w.r.t. a.\n    std::cout << c << \"\\n\";\n  }\n  return 0;\n}\n<\/code>\nI\u2019ve played around with different versions of it. You can see the CPU specific code section is showing the previous structure I used before PyTorch 1.2 (it\u2019s showing CPU variants, but was basically equivalent for GPU as well). The new style I\u2019m trying to write currently only sits in the GPU section. This is done for illustration purposes here, but I\u2019m actually trying the same for both. This is to account for the change in how autograd deals with passing the dispatch &amp; Jacobian to construct the gradients (link with link \"https:\/\/github.com\/ROCmSoftwarePlatform\/pytorch\/commit\/6e3687341ef211b5ee5c5a3680b49d02475d5377\").\nThat\u2019s all good, but since then I\u2019ve not been able to get any gradients to work. I am probably doing something wrong, but can\u2019t put my finger on it (or found a weird bug, no idea). The output I get is this:\n<code class=\"lang-auto\">$ make -C pytorch\/autograd run\nmake[1]: Entering directory '\/home\/gizdov\/Git\/arch-package-tests\/pytorch\/autograd'\ng++ autograd.cpp -I\/usr\/include\/torch\/csrc\/api\/include -I\/usr\/include\/python3.8 -I\/usr\/include\/python3.8m -L\/usr\/lib\/pytorch -L\/opt\/cuda\/lib -lc10 -ltorch -lnvrtc -lcuda -o autograd\n.\/autograd\nCUDA device is available.\nWe are using CUDA\na:  1  1\n 1  1\n[ CUDAFloatType{2,2} ]\nc:  2.8679  0.9870\n 0.8103  1.9954\n[ CUDAFloatType{2,2} ]\ncgrad : [ Tensor (undefined) ]\na_grad:  1  1\n 1  1\n[ CUDAFloatType{2,2} ]\na.grad(): [ Tensor (undefined) ]\na_grad:  1  1\n 1  1\n[ CUDAFloatType{2,2} ]\ng_loss: 9.25194\n[ CUDAFloatType{} ]\na.grad(): [ Tensor (undefined) ]\nmake[1]: Leaving directory '\/home\/gizdov\/Git\/arch-package-tests\/pytorch\/autograd'\n<\/code>\nMy gradients are always undefined or I get a runtime error from c10 for passing the wrong dispatch tensor if a_grad is missing or malformed.\nCould someone please help?\nThanks.\nEDIT: This is on PyTorch 1.4.1, CUDA 10.2 Compute Capability 5.2, GCC 8.4.0, Python 3.8.2","y":"Please see the comment here: ValueError: can't optimize a non-leaf Tensor? with link \"https:\/\/discuss.pytorch.org\/t\/valueerror-cant-optimize-a-non-leaf-tensor\/21751\/2\". The main problem you are running into is that\nauto a = torch::ones({2, 2}, torch::TensorOptions().dtype(torch::kFloat).requires_grad(true)).cuda(); \nis NOT a leaf node, so autograd does not accumulate gradients into a.grad. This is because you\u2019re calling .cuda() on the actual leaf node, returning a new tensor. If you directly construct the tensor on CUDA, then you can get gradients on it:\n<code class=\"lang-auto\">auto a = torch::ones({2, 2}, torch::TensorOptions().dtype(torch::kFloat).requires_grad(true).device(torch::kCUDA));\nauto c = (a + a).sum();\nc.backward()\na.grad() \/\/ now has something\n<\/code>","z":"Please see the comment here: ValueError: can't optimize a non-leaf Tensor? with link \"https:\/\/discuss.pytorch.org\/t\/valueerror-cant-optimize-a-non-leaf-tensor\/21751\/2\". The main problem you are running into is that\nauto a = torch::ones({2, 2}, torch::TensorOptions().dtype(torch::kFloat).requires_grad(true)).cuda(); \nis NOT a leaf node, so autograd does not accumulate gradients into a.grad. This is because you\u2019re calling .cuda() on the actual leaf node, returning a new tensor. If you directly construct the tensor on CUDA, then you can get gradients on it:\n<code class=\"lang-auto\">auto a = torch::ones({2, 2}, torch::TensorOptions().dtype(torch::kFloat).requires_grad(true).device(torch::kCUDA));\nauto c = (a + a).sum();\nc.backward()\na.grad() \/\/ now has something\n<\/code>\nHi,\nThanks. Indeed that solved part of the issue. However, does that mean I cannot get intermediate gradients? For example, I cannot seem to get c = a + b to get anything for c.grad().\nIn python, you can use c.retain_grad() for this to be populated for non-leafs.\nNot sure if it is available on the cpp API yet? cc \nOtherwise, you can use autograd::grad and provide the inputs you want gradient for, where you can specify any Tensor that requires gradients (no need for them to be leafs).\nThanks.\nWell, I did indeed get it working to a point, but the C++ API does not seem to implement this. Firstly, retain_grad() is not defined. Secondly, when using auto cgrad = torch::autograd::grad({c}, {a, b}, {a_dispatch});, I get the following:\n<code class=\"lang-auto\">terminate called after throwing an instance of 'std::runtime_error'\n  what():  the derivative for 'target' is not implemented\n<\/code>\nSo this is also not implemented. I guess for now I will stick with what works.\nCheers.\nEDIT: wait, no, this seems to be caused by requiring a gradient on b as well to satisfy autograd::grad() call, which then breaks torch::binary_cross_entropy(a, b). So that is what is not implemented. Not sure how to work around it - maybe give it another const for the time being.\nretain_grad() should already work on torch::Tensor, we have a test here: https:\/\/github.com\/pytorch\/pytorch\/blob\/64a6faa2c8435a9e96743b078d9f2ae2b7ef1cc3\/test\/cpp\/api\/autograd.cpp#L129-L153"},{"x":"Can you please explain a difference between Tensor.clone() and Tensor.new_tensor()? According to the documentation, Tensor.new_tensor(x) = x.clone().detach(). Additionally, according to this post on the PyTorch forum with link \"https:\/\/discuss.pytorch.org\/t\/how-to-copy-a-variable-in-a-network-graph\/1603\" and this documentation page with link \"https:\/\/pytorch.org\/docs\/stable\/tensors.html\", x.clone() still maintains a connection with the computation graph of the original tensor (namely x). However, I am new to PyTorch and don\u2019t quite understand how x.clone() interacts with the computation graph of x.","y":"So, s you said, x.clone() maintains the connection with the computation graph. That means, if you use the new cloned tensor, and derive the loss from the new one, the gradients of that loss can be computed all the way back even beyond the point where the new tensor was created. However, if you detach the new tensor, as it is done in the case of .new_tensor(), then the gradients will only be computed from loss backward up to that new tensor but not further than that.\nI hope this helps!","z":"So, s you said, x.clone() maintains the connection with the computation graph. That means, if you use the new cloned tensor, and derive the loss from the new one, the gradients of that loss can be computed all the way back even beyond the point where the new tensor was created. However, if you detach the new tensor, as it is done in the case of .new_tensor(), then the gradients will only be computed from loss backward up to that new tensor but not further than that.\nI hope this helps!\nI see. It makes a lot of sense. Thank you very much.\nHi, Vahid! Say if have a tensor \u2018A\u2019, then I use B=A.clone(); finally, I get my loss=Lossfunc([g(B);A]) where \u2018;\u2019 denotes concatenation. Do the gradients of A include two parts where the first part is directly from the A in the concatenation and the second part is from B? The function g(.) will not affect A. So basically I mean there is a residual style connection in the computation graph. Is that right?"},{"x":"Hi everybody ! I\u2019m currently try some experimentation on times series data (1D sequences).\nSo I built a kind of autoregressive model using lstm cell !\nMy code is running well but the model doesn\u2019t converge.\nSome clues :\n\nI checked my model parameters on many epochs and it didn\u2019t change.\nI checked the grad of layers it set at None so my loss is constant.\nI get this kind of error :\n\n\nstack(): functions with out=\u2026 arguments don\u2019t support automatic differentiation, but one of the arguments requires grad\n\nhere the model code :\n<code class=\"lang-auto\">class LstmModel(nn.Module):\n    def __init__(\n        self, hidden_dim, batch_size, target_size, sequence_length, n_layers, device\n    ):\n        super(LstmModel, self).__init__()\n\n        self.hidden_dim = hidden_dim\n\n        self.target_size = target_size\n\n        self.batch_size = batch_size\n\n        self.device = device\n\n        self.fc = nn.Sequential(\n            nn.Linear(in_features=hidden_dim, out_features=hidden_dim \/\/ 2),\n            nn.ReLU(inplace=False),\n            nn.Linear(in_features=hidden_dim \/\/ 2, out_features=1),\n        )\n        self.sequence_length = sequence_length\n\n        self.lstm_cell = nn.LSTMCell(input_size=1, hidden_size=hidden_dim, bias=True)\n\n    def init_hidden(self, device, batch_size):\n        # Always give batch_size and device information when we create new tensor\n        # initialize the hidden state and the cell state to zeros\n        return (\n            torch.zeros(batch_size, self.hidden_dim, requires_grad=True).to(\n                device=self.device\n            ),\n            torch.zeros(batch_size, self.hidden_dim, requires_grad=True).to(\n                device=self.device\n            ),\n        )\n\n    def forward(self, sequence):\n\n        batch_size = sequence.size(0)\n        hidden, cell = self.init_hidden(batch_size=batch_size, device=self.device)\n\n        for i in range(self.sequence_length):\n            hidden, cell = self.lstm_cell(sequence[:, i], (hidden, cell))\n\n        outputs = torch.zeros(self.target_size, batch_size, 1, requires_grad=True).to(\n            device=self.device\n        )\n\n        out = self.fc(hidden)\n\n        outputs[0] = out\n\n        for target_idx in range(1, self.target_size):\n            hidden, cell = self.lstm_cell(out, (hidden, cell))\n            out = self.fc(hidden)\n            outputs[target_idx] = out\n        outputs = outputs.view(outputs.shape[1], outputs.shape[0], outputs.shape[2])\n        # Outputs have to be shapped like [batch_size,sequence_len,dim_of_xi]\n        return outputs\n\n<\/code>\nMaybe some operations are not differentiable.\nThanks everybody ! Have a nice day !","y":"Could you try to initialize outputs as a list, and append all out tensors to it.\nOnce you are done, create a tensor via outputs = torch.stack(outputs) and return it.\nI assume the creation of outputs as a tensor, which requires gradients might break the computation graph.\nI would assume you\u2019ll get a RuntimeError such as:\n<code class=\"lang-python\">RuntimeError: leaf variable has been moved into the graph interior\n<\/code>","z":"Could you try to initialize outputs as a list, and append all out tensors to it.\nOnce you are done, create a tensor via outputs = torch.stack(outputs) and return it.\nI assume the creation of outputs as a tensor, which requires gradients might break the computation graph.\nI would assume you\u2019ll get a RuntimeError such as:\n<code class=\"lang-python\">RuntimeError: leaf variable has been moved into the graph interior\n<\/code>\nIt\u2019s working ! Thanks you !!"},{"x":"Hello,\nI am breaking a tensor into chunks so that I can individually change the requires_grad argument based on which values I want to optimize\n<code class=\"lang-auto\">ones = torch.ones(3)\nchunks = torch.chunk(ones, 3, 0)\n# using chunks as input to optimizer\nk = torch.cat(chunks)\nprint(k)\nOutput: tensor([1., 1., 1.])\nk[0]=2\nprint(k)\nOutput: tensor([2., 1., 1.])\n\nprint(chunks)\nOutput: (tensor([1.]), tensor([1.]), tensor([1.]))\n<\/code>\nI am doing this so that I don\u2019t have to change most of my code, so I am first using the chunk function to break the tensor and then using cat function. Changing the values of variable \u2018k\u2019 doesn\u2019t seem to reflect on variable \u2018chunks\u2019. I don\u2019t understand where the flaw in my intuition is.\nThank you.","y":"torch.chunk creates views of the original ones tensor. torch.cat does not create a view; it creates a branch new tensor by copying data. This is why modifying the output of torch.cat (k) doesn\u2019t change chunks; their data storage is completely unrelated.","z":"torch.chunk creates views of the original ones tensor. torch.cat does not create a view; it creates a branch new tensor by copying data. This is why modifying the output of torch.cat (k) doesn\u2019t change chunks; their data storage is completely unrelated.\nThank for the reply \nI am trying to understand the difference better\n<code class=\"lang-auto\">x = torch.cat([x1, x2, x3])\nout = f(x)\n<\/code>\nwould the gradients propage to x1, x2, x3 as expected or do they not get any gradients.\nAlso is there a good way of merging all the chunks together, so that I can perform operations on them as if they are a single tensor?\nThank you.\nThe gradients will propagate as long as x1, x2, ... require grad.\n\nAlso is there a good way of merging all the chunks together, so that I can perform operations on them as if they are a single tensor?\n\nYou could use x = torch.cat([x1, x2, x3]), apply some in-place operations, and then split it back up via y1, y2, y3 = x.chunk(3)."},{"x":"I have some questions regarding the theory of autograd:\n1.) Are y and x the input and output respectively?\n2.) Where does v come from? and how is it computed?\nScreenshot 2020-03-24 at 23.31.421662\u00d7760 92.8 KB with link \"https:\/\/discuss.pytorch.org\/uploads\/default\/original\/3X\/c\/8\/c8429ff18a289d060a86c64f2e4988dd6b803149.png\"","y":"Hi,\n\nWhen you write y = f(x), that means that x is the input. And y is the output.\nv can be any Tensor you want. A special example is if you function has a single output (like a loss function in NN) then J will be a simple row and by setting v = 1, you get the gradient of your function.\n","z":"Hi,\n\nWhen you write y = f(x), that means that x is the input. And y is the output.\nv can be any Tensor you want. A special example is if you function has a single output (like a loss function in NN) then J will be a simple row and by setting v = 1, you get the gradient of your function.\n"},{"x":"Hi,\nI am training an RNN, and computing a loss for each step of the output sequence. Is calling loss.backward(retain_graph=True) for each step of the output more memory efficient\/faster than summing loss = loss_step1 + \u2026 + loss_stepn and then calling loss.backward()? I have tried both, and they seem to give the same accuracy. But I\u2019m not sure what is different behind the scenes.","y":"Hi,\nsumming the losses and calling a single backward is going to be more efficient unless each loss work with independent networks and it will be the same.\nBut basically you can\u2019t go wrong with doing the sum and a single backward !.","z":"Hi,\nsumming the losses and calling a single backward is going to be more efficient unless each loss work with independent networks and it will be the same.\nBut basically you can\u2019t go wrong with doing the sum and a single backward !."},{"x":"While reading a pytorch code there was comment where it said we have to once again move the model trained in the GPU to device using .to('CUDA'). So I was wondering the scope of .to('CUDA) ? Will it persist in the cuda memory until the program is terminated or the scope of function is terminated? what happens if we pass the model variable to another function? will it be affected?\nIs transferring the model to run in GPU just a one time activity?","y":"Hi,\nYes it is a one time thing. And the weights will stay on the GPU until you delete the model, or move it back to the cpu().\nThis comment might be related to loading of a saved model maybe? With the saved model being on CPU.","z":"Hi,\nYes it is a one time thing. And the weights will stay on the GPU until you delete the model, or move it back to the cpu().\nThis comment might be related to loading of a saved model maybe? With the saved model being on CPU."},{"x":"Hi guys, when using torch.where, it works as a threshold thus unable to pass the grad by torch.autograd.grad(\u2026). Instead, is there a way to keep to same graph and pass through to next differentiable operation?\nFor example, in a very simple code below\n<code class=\"lang-auto\">import torch\n\na = torch.tensor([[1.0, 2.0], [3.0, 4.0]], requires_grad=True)\nb = torch.tensor([[0.1],[0.9]])\nc = torch.mm(a, b)\n\n# ** position2 **: grad couldn't pass back here\nc = torch.where(c>2, torch.tensor(1.0, requires_grad=True), torch.tensor(0.0, requires_grad=True))\n# ** position1 **: the most place grad could arrive\nloss = c.sum()\n\nprint(torch.autograd.grad(loss, a, allow_unused=True))\n<\/code>\nThe c uses torch.where as a threshold to get binary values, of course the grad cannot arrive at position2. I just wonder is there some methods that we can keep c\u2019s grad in position1 and directly paste to position2, then keep calculating grads until arrive to a.\nAbove is only a hand-written example, when using build-in optimizer, after calculating loss and doing loss.backward(), is there any methods to let the backward overpass torch.where() like example above?","y":"Hi,\nYou can. write a custom Function whose backward will be the identity while the forward is your torch.where.\nYou can find here with link \"https:\/\/pytorch.org\/docs\/stable\/notes\/extending.html\" how to write such Function.","z":"Hi,\nYou can. write a custom Function whose backward will be the identity while the forward is your torch.where.\nYou can find here with link \"https:\/\/pytorch.org\/docs\/stable\/notes\/extending.html\" how to write such Function.\nProblem solved! Thanks so much"},{"x":"I\u2019m currently implementing a network architecture where I have to pass my data through a certain layer for multiple times.\nThe network can be simplified as:\n<code class=\"lang-auto\">Input ->  Layer A -> Layer B -> Layer A -> Output\n<\/code>\nHowever I dont want Layer A to be updated by the gradient, i.e. freezed, on the latter call, while still be updated by the first call. Is this possible with pytorch? I\u2019ve tried to come up with something using no_grad, requires_grad or detach() but I couldn\u2019t.\nA hacky workaround Im thinking of is to do gradient scaling during the backward pass:\n<code class=\"lang-auto\">Input Gradients <- Layer A <- Layer B <- Scale up gradient by \u03bb <- Layer A <- Scale down gradient by \u03bb <- Output Gradients\n<\/code>\nIs there a cleaner implementation? Thanks.","y":"In my case, my layerA is much more complicated than a nn.Linear layer, so simply detaching weightings would not be practical. I am thinking of deepcopying layerA instead, however im not familiar with the interactions of it with pytorch. Would you know if this will work?\n<code class=\"lang-auto\">from copy import deepcopy\nimport torch\n\nx = torch.randn(1, 10)\nlayerA = ...\nlayerB = ...\nlayerA_2 = deepcopy(layerA)\n\nout = layerA(x)\nout = layerB(out)\nout = layerA_2(out)\n\nout.mean().backward()\n\n\n<\/code>","z":"Wouldn\u2019t torch.no_grad() work?\n<code class=\"lang-python\">x = layerA(x)\nx = layerB(x)\nwith torch.no_grad():\n    x = layerA(x)\n\nloss = criterion(x, target)\n<\/code>\nThis would make sure to not track the second call on layerA.\nIf this is the case, I may have misunderstood how no_grad works. I was under the impression that with no_grad, the gradient would not be computed for the second call of layerA and thus the gradient cannot be propagated to layerB and also the first call of LayerA. Can you confirm this is not the case? Thanks for the quick reply.\nNo, sorry, I\u2019m wrong and the output won\u2019t require gradients.\nIn that case, you could detach the parameters of layerA and use the functional API, which should work:\n<code class=\"lang-python\">torch.manual_seed(2809)\n\nlayerA = nn.Linear(10, 10)\nlayerB = nn.Linear(10, 10)\n\nx = torch.randn(1, 10)\n\nout = layerA(x)\nout = layerB(out)\n\nout = F.linear(out, layerA.weight.detach(), layerA.bias.detach())\n\nout.mean().backward()\n\nprint(layerA.weight.grad)\n<\/code>\nIn my case, my layerA is much more complicated than a nn.Linear layer, so simply detaching weightings would not be practical. I am thinking of deepcopying layerA instead, however im not familiar with the interactions of it with pytorch. Would you know if this will work?\n<code class=\"lang-auto\">from copy import deepcopy\nimport torch\n\nx = torch.randn(1, 10)\nlayerA = ...\nlayerB = ...\nlayerA_2 = deepcopy(layerA)\n\nout = layerA(x)\nout = layerB(out)\nout = layerA_2(out)\n\nout.mean().backward()\n\n\n<\/code>\nYes, your approach should also work.\nI compared it to the functional API run and get the same gradients for the copied module."},{"x":"Context:\nThe output of my network has 6 channels. In my custom loss function, I break the 6 channels into two groups of 3 channels each. Process the first group using l1_loss (got a scalar output), the second group using bce_loss (got a scalar output), &amp; add the two scalars obtained to get the final loss. Then I use loss.backward().\nQuestion:\nWould autograd handle the gradient flow automatically in above case? If yes, how is autograd doing this?\nCode:\n<code class=\"lang-auto\">class MyLoss(torch.nn.Module):\n    \n    def __init__(self):\n        super(MyLoss, self).__init__()\n\t\t.\n\t\t.\n\tdef forward(self, device, output_batch, target_batch):\n\t\t\n\t\toutput_batch_part_1 = output_batch[:,0:3,:,:] # [batch, channel, W, H]\n\t\toutput_batch_part_2 = output_batch[:,3:6,:,:]\n\t\t\n\t\ttarget_batch_part_1 = target_batch[:,0:3,:,:]\n\t\ttarget_batch_part_2 = target_batch[:,3:6,:,:]\n\t\t\n\t\tloss_term_1 = self.l1_loss_calculation(output_batch_part_1, target_batch_part_1)\n\t\t\n\t\t#... some processing on output_batch_part_2 ...\n\t\t.\n\t\t.\n\t\tloss_term_2 = self.bce_loss_calulation(output_batch_part_2, target_batch_part_2)\n\t\t\n\t\t#... some normalizations on loss_term_2...\n\t\t.\n\t\t.\n\t\tfinal_loss = loss_term_1 + loss_term_2\n\t\t\n\t\treturn final_loss\n<\/code>","y":"Yes, Autograd tracks the indexing operation as well as additions, so your model should get valid gradients as shown in this example:\n<code class=\"lang-python\">x = torch.randn(1, 6, 4, 4, requires_grad=True)\nx0 = x[:, :3]\nx1 = x[:, 3:]\n\nx0.mean().backward()\nprint(x.grad)\nx1.mean().backward()\nprint(x.grad)\n<\/code>","z":"Yes, Autograd tracks the indexing operation as well as additions, so your model should get valid gradients as shown in this example:\n<code class=\"lang-python\">x = torch.randn(1, 6, 4, 4, requires_grad=True)\nx0 = x[:, :3]\nx1 = x[:, 3:]\n\nx0.mean().backward()\nprint(x.grad)\nx1.mean().backward()\nprint(x.grad)\n<\/code>"},{"x":"I have re-implemented ResNet 18. I verified if the architecture is correct by printing my model and the actual pytorch resent implementation. However my resnet model does not seem to learn, what could be the reason for this eventhough printing the model shows that the archs are identical?","y":"Printing the model via print(model) shows only all created modules, not the forward implementation and thus neither how the modules are used.\nAlso all functional calls are missing from the output, which could be used in the forward.\nYou could either compare the source codes directly (your vs. torchvision implementation) or load the same state_dict into both models and compare the output of each layer to narrow down where a difference might be coming from.","z":"Printing the model via print(model) shows only all created modules, not the forward implementation and thus neither how the modules are used.\nAlso all functional calls are missing from the output, which could be used in the forward.\nYou could either compare the source codes directly (your vs. torchvision implementation) or load the same state_dict into both models and compare the output of each layer to narrow down where a difference might be coming from.\nit could be much easier to help if you could provide a minimal repreducible example.\napart from what ptrblk said, you could be missing the proper weight initialization. check that as well.\nI just knew the first answer would come from ptrblck!\nThanks for the insight, the problem was in my forward block. Naive of me to not consider that!"},{"x":"Hi,\nIs there a specific scheduling capability where I can define forward pass and backward pass to run on specific GPU devices? With model parallelism, it is clear that we can schedule the layers to be in specific GPU devices. But could we go further into the details? Without overriding, the backward function is it possible to give a config and train a neural network? This could involve a weight copy or activation copy to a separate GPU device to do the backward. Is this something possible with Autograd definitions and GPU scheduling in Pytorch?","y":"This overhead is mainly the discovery of what needs to be done to compute gradients. So it needs to traverse all the graph of computation, which takes a bit of time.\nNote that if you\u2019re simply experimenting, this overhead won\u2019t kill you. But it won\u2019t be 0.","z":"Hi,\nNo this is not possible at the moment I\u2019m afraid.\nWhat the engine does is to run the backward of a function on the same device (and cuda stream if it was on GPU) as the forward pass. This is a good heuristic as the backward pass is usually as expensive as the forward. So the splitting used in the forward is a good indication of how the backward should be split.\n\nExactly. But if we not use backward and do all in manual passion, this is possible with\nmultiple cuda streams? Is this possible?\n\nBut if we not use backward and do all in manual passion,\n\nNot sure what you mean by this.\nIf you don\u2019t use the autograd engine and write everything by hand, you can do whatever you want as you write it by hand.\nYes that is true.\nWhat I meant was using autograd function, on a given tensor and get it\u2019s gradient update value.\nGenerally what we do is, we have this leaf node or loss value and call backward().\nSo, it traverse back and calculate all gradients for us. Is this right?\nIf so, I can access w.grad and get the weight updates.\nWhat if I manually go back and just go layer by layer by calling corresponding grad_function of each tensor (contributing to weights) by passing shape-matching inputs and get the grad value.\nThis way, I get more control over it. I don\u2019t want to entirely loose the autograd functionality, but use it at node level to just get the corresponding weight vector update. Can this be possible?\nI guess if you save every input\/output pair for every function, you can do this by doing. If you have b = foo(a) and c = bar(b)\ngrad_b = autograd.grad(c, b, grad_c)\ngrad_a = autograd.grad(b, a, grad_b)\nAnd do that for every op if you have more. That will work, but might not be super efficient.\nYour function representation is exactly what I explained.\nOne point to understand, why this could be very inefficient?\nInefficient in a sense of more time to write code or execution time will be slow because of not using\nsome of the internal optimizations in Autograd?\nInefficient at execution time because the autograd engine has some overhead (to discover the work that needs to be done) that is going to be repeated many times here. Also usually, the backward pass runs only in cpp and without holding the python GIL, here, you will come back to python after each operation.\nGreat!. So if I look into a neat CPP implementation, I could not worry on the Python overhead, but still I have to deal with the overhead of autograd engine.\nCould you please elaborate a bit on the overhead?\nThis overhead is mainly the discovery of what needs to be done to compute gradients. So it needs to traverse all the graph of computation, which takes a bit of time.\nNote that if you\u2019re simply experimenting, this overhead won\u2019t kill you. But it won\u2019t be 0.\n I have a follow up question to this. So, AFAIU forward and backward pass happens inside the GPU device without holding the python GIL.\nBut how about the weight update operation done by an optimizer? Would optimizer access grads and params of each layer in the python environment, update the weights and push them back to the GPU? or would you translate the python logic to a cuda kernel and update the weights inside the GPU itself?\nThese are quite orthogonal things.\nIf you\u2019re using python, the Tensor is a python object. But if the Tensor is a GPU tensor, then the memory it works with is on the GPU. So when you perform operations on it, they will run on the GPU.\nThis discussion here about the GIL is unrelated. You can use the underlying cpp Tensor object and use it without touching the python object as well.\n I think this clears things. \nHi ,\nHow much of work would it be to separate out forward and backward passes? \nLike run the forward on one GPU and the backward on another?\nYes, on separate GPUs.\nThat would be a LOT of work \nWhat would be the benefit of doing this?\n\nFor the sake of the argument of reducing the computation overhead in the backward,\nIf we can divide this into parts, the forward and the backward, can\u2019t we get better performance.\n\n\n\n albanD:\n\nThat would be a LOT of work \n\n\nWhich APIs would be involved in such a task? How complex could this task be?\nCould you give insight with respect to the existing APIs?\nI don\u2019t see how you can get better performances.\nThe forward and backward are sequential.\nAnd if you say you have 2 device and you want to overlap multiple forward backward. Then you can overlap them while keeping each fw\/bw pair on the same device as well.\n\nWhich APIs would be involved in such a task? How complex could this task be?\nCould you give insight with respect to the existing APIs?\n\nThe whole backward pass assumes that intermediary gradients match the intermediary results in the forward pass. Also if some buffers were saved, they would need to be moved from different device.\nThis will for sure not be accepted as a PR (unless you can show a use case where it brings massive speed improvement). And hacking it might still be quite hard.\nWhat if we use pipeline mechanism to load micro-batches instead of mini-batches?\nAnd do the final weight vector update at the end of the mini-batch.\nThen that sequential relationship can still be maintained (as it cannot be broken because of weight vector update requirement). But when backward takes more time, more GPUs\u2019 can do backward task and reduce backward timing. Isn\u2019t this a practical use-case where we could improve performance?"},{"x":"Hi pytorch friends!\nI want to optimize per-example gradient computation, and found an interesting behavior when increasing the batch size.   My findings can be reproduced in this notebook with link \"https:\/\/colab.research.google.com\/gist\/jusjusjus\/7c04e2645ba335061f96f45bab79a16b\/scaling-in-per-example-computation.ipynb\".  Essentially I compute per-example losses then to call backward on each one:\n<code class=\"lang-python\">t0 = time()\nlosses = loss_fn(model(samples), labels)\nmodel.zero_grad()\nfor i, l in enumerate(losses):\n    l.backward(retain_graph=True)\n    # do something with the individual gradients\nsampling_rate = batch_size \/ (time() - t0)\nprint(f\"{sampling_rate:.0f} Hz\")\n<\/code>\nI find that the computed sampling rate has a maximum for 20 to 60 samples dependent on the used GPU.\nHow can you (a) explain this behavior, and (b) possibly fix it so that I can get the most of my computations?\nThanks for considering my problem, please AMA ","y":"But the fact that you do as many backwards as there are samples in the batch makes them slower when the batch size increases.\nIn that case, I can buy that there is a sweet spot for a small number of samples.\nAlso note that each call to backward uses the FULL graph. So the more sample, the more expensive the backward for each individual sample.","z":"Hey,\nThis is most likely because of the large number of calls to .backward().\nOut of curiosity, how do you \u201c# do something with the individual gradients\u201d given that you don\u2019t zero_grad() before? You do something with all the gradients accumulated up to now?\ninteresting; what type of overhead is generated by multiple calls to backward?  Regarding curiosity, I omitted most of that (including zero_grad).  It\u2019s related to differential privacy.\n\nWhat type of overhead does multiple calls to backward generate?\n\nWe need to go in cpp, discover the work that needs to be done, start GPU kernels for everything.\nThat takes some time.\n\nomitted most of that (including zero_grad ). It\u2019s related to differential privacy.\n\nOne thing I would say is that if the final transformation that you\u2019re doing on the gradient is linear, then you can apply that same (similar) transformation to the loss before accumulation.\nOtherwise, you can try to use something like this with link \"https:\/\/github.com\/cybertronai\/autograd-hacks\" to get a per-sample gradients in one backward pass (more or less) and then do all the post processing after that. That might be faster.\n\n\n\n albanD:\n\nWe need to go in cpp, discover the work that needs to be done, start GPU kernels for everything.\nThat takes some time.\n\n\nI modified the code like so to investigate your explanantion:\n<code class=\"lang-python\">for i, l in enumerate(losses):\n    t = time()\n    # just to do something with the gradients\n    model.zero_grad()\n    l.backward(retain_graph=True)\n    print(i, time() - t)\n<\/code>\nWhat I get from this is\n<code class=\"lang-auto\">...\n24 0.0007534027099609375\n25 0.0007722377777099609\n26 0.0007855892181396484\n27 0.0024886131286621094\n28 0.0009698867797851562\n29 0.0012204647064208984\n30 0.0010230541229248047\n31 0.0008389949798583984\n32 0.0024178028106689453\n33 0.0007109642028808594\n34 0.0007128715515136719\n35 0.0006940364837646484\n36 0.0009226799011230469\n37 0.0046350955963134766\n38 0.004754304885864258\n39 0.004813194274902344\n40 0.004805564880371094\n41 0.004813432693481445\n...\n<\/code>\nWhy do you think does it take four times as long to go to cpp in iteration 37 compared with iteration 36?\nHere\u2019s an infographics over a bunch of runs on my dedicated nv 2080 (sampling rate ~ 1 \/ (time() - t), iteration: that\u2019s the index into the batch in the loop across losses):\n\nHowever, thanks for the pointer to autograd hacks; I\u2019ll try that out.\nHi,\nI would bet on you having filled the GPU task queue and so you now have to wait for stuff to be finished on the GPU before queuing more work.\nThe GPU api is asynchronous, so here, you only meansure the time to launch the jobs.\nYou can add torch.cuda.synchronize() before your timing if you want to measure the time it actually takes to compute the result.\nThat\u2019s a great idea.  I added that to the linked notebook (not saving it; you\u2019ll have to do it yourself), and now I receive the following numbers on an nvidia T4:\n<code class=\"lang-auto\">at 3-batches, the sample rate is 614 +\/- 122 Hz\nat 24-batches, the sample rate is 883 +\/- 132 Hz\nat 64-batches, the sample rate is 380 +\/- 5 Hz\nat 96-batches, the sample rate is 265 +\/- 2 Hz\n<\/code>\nI\u2019m still a bit puzzled, though, that we observe a maximum sample processing rate at some 20 samples per batch. Wouldn\u2019t one expect the runtime of the for-loop to scale linearly with the batch size, thus giving a nearly constant sample rate?\nThat is indeed interesting.\nI did a quick test and it has nothing to do with the backwards: if you only measure the forward time, you see the same behavior \nSo I would guess that the kernel for larger batch size are more optimized than the ones for small batch size. And this is why you see this.\nNote that if you use cudnn, it might even take completely different algorithms depending on the input shape!\n\n\n\n albanD:\n\nSo I would guess that the kernel for larger batch size are more optimized than the ones for small batch size. And this is why you see this.\n\n\nYeah, though I think it\u2019s the other way around when trying to backprop sample-by-sample.  Maybe it\u2019s possible to somehow fix the algorithm to the version for small batch sizes.\n\n\n\n albanD:\n\nOtherwise, you can try to use something like this with link \"https:\/\/github.com\/cybertronai\/autograd-hacks\" to get a per-sample gradients in one backward pass (more or less) and then do all the post processing after that. That might be faster.\n\n\nI followed your advise, but to make it work for my GAN application I had to make some major adoptions of the code.  I include this link here for other people\u2019s reference that might encounter the same problem.  Thanks again:\n\n\n\nGitHub with link \"https:\/\/github.com\/jusjusjus\/autograd-hacks\"\n\n\n\njusjusjus\/autograd-hacks with link \"https:\/\/github.com\/jusjusjus\/autograd-hacks\"\nContribute to jusjusjus\/autograd-hacks development by creating an account on GitHub.\n\n\n\n\n\nFyi, I also marked it as a solution, even though I think we still have some unresolved questions to the initial observation.\ncc  the author of autograd-hacks if you want to discuss the necessary changes.\nDid that solve your perf problems? Or at least remove any multi-backward fear?\nAlso this is a modified version of your code that prints runtime instead of throughput.\nhttps:\/\/colab.research.google.com\/drive\/1aMCMZaSBak0PssL8nt2ff3mgFRKeNWaP\nThis makes it clear what happens: for small enough batch size, you get a sub-linear scaling as the GPU is under-utilized. At some point, you fully use the GPU and you start scaling linearly.\nThat would explain why the throughput looks this way.\n glad to see it was useful! I had a similar issue with GANs \u2013 saving forward quantities as module attribute means generator\/discriminator passes got mixed together. Also, keeping activations around took too much memory, so I ended up using lower level autograd-lib with link \"https:\/\/github.com\/cybertronai\/autograd-lib\" for my application. Instead of storing activations in global variable, store them in closure-local variable, this way the memory gets released as you are done with the layer \u2013 example with link \"https:\/\/colab.research.google.com\/drive\/1mUWfuydS7KqoPcFnFtrg4uCLMO-4nHOj#scrollTo=WMDbKbLtrxwG\"\nThe slowness has to do with batch size not saturating GPU compute units, so it could be called a hardware issue. However, autograd-hacks just restructures the computation to do a single large matmul instead of concatting together a bunch of small matmuls. Why couldn\u2019t this be done automatically? In this sense it\u2019s a software issue.\n\n\n\n albanD:\n\nThat would explain why the throughput looks this way.\n\n\nAre you sure about that?  In a linear scaling, one expects what I call sample rate above (batch_size \/ (t1-t0)) to saturate.  However, it starts to decrease with increasing batch sizes (see post above).  Larger batches, it seems, are computed less efficiently than running multiple computes on smaller batches.\nBut the fact that you do as many backwards as there are samples in the batch makes them slower when the batch size increases.\nIn that case, I can buy that there is a sweet spot for a small number of samples.\nAlso note that each call to backward uses the FULL graph. So the more sample, the more expensive the backward for each individual sample."},{"x":"I was wondering if things that show up in pytorch as deprecated or duplicates are going to be removed at some point in order to have a cleaner and more consistent interface?","y":"Hi,\nYes they are. But because Variable what so widely used by the users, we are leaving it for now.\nIt is only a no-op that gives you a Tensor right now.\nBut for deprecation:\n\nWe just removed old style autograd.Function code: they will be actual errors in 1.5\nWe are removing .data internally to prepare for adding warning and deleting it (or making it a no-op like Variable is right now)\n","z":"Hi,\nYes they are. But because Variable what so widely used by the users, we are leaving it for now.\nIt is only a no-op that gives you a Tensor right now.\nBut for deprecation:\n\nWe just removed old style autograd.Function code: they will be actual errors in 1.5\nWe are removing .data internally to prepare for adding warning and deleting it (or making it a no-op like Variable is right now)\n"},{"x":"Hi,\nI\u2019m trying to manipulate a matrix by multiplying each column by a matrix.\nMy code looks like this:\nfor i in range(s.shape[0]):\n\ts[i,:] = torch.matmul(s[i,:],M)\nreturn s\n\nM and s are m*m Tensors.\nWhen I try to backward, I get this error:\n\nRuntimeError: one of the variables needed for gradient computation has been modified by an inplace operation: [torch.FloatTensor [1, 81]], which is output 0 of UnsqueezeBackward0, is at version 81; expected version 80 instead. Hint: the backtrace further above shows the operation that failed to compute its gradient. The variable in question was changed in there or anywhere later. Good luck!\n\nand\n\n\u2026\\torch\\csrc\\autograd\\python_anomaly_mode.cpp:57: UserWarning: Traceback of forward call that caused the error:\n\npoint to this block of code.\nWhat am i doing wrong, and how can I modify this ?","y":"Two possible ways:\n\nUse batch matrix multiplication (or einsum or whatever).\nInstead of overwriting s[i], collect the results in a list and do torch.stack on it after the for loop.\n\nBest regards\nThomas","z":"Two possible ways:\n\nUse batch matrix multiplication (or einsum or whatever).\nInstead of overwriting s[i], collect the results in a list and do torch.stack on it after the for loop.\n\nBest regards\nThomas\nWhat would the einsum look like ? I\u2019m struggling a bit understanding the syntax\nWhat are your input shapes?\nBest regards\nThomas"},{"x":"Just a quick question: the following are exactly equivalent, correct?\nwith torch.no_grad():\nblock of code\nwith torch.set_grad_enabled(False):\nblock of code","y":"There is no difference.\nJust that one takes a bool as input and not the other.\nAlso  torch.set_grad_enabled(False) can be used as just a function to set the grad mode forever.","z":"It seems there is no difference: https:\/\/stackoverflow.com\/a\/53447634\/6888630\nThere is no difference.\nJust that one takes a bool as input and not the other.\nAlso  torch.set_grad_enabled(False) can be used as just a function to set the grad mode forever.\nGreat, thanks a lot. Very convenient to use the same piece of code for training and evaluating."},{"x":"Hi, I have two tensors tensor1 and tensor2, with shape (16, 768, 64, 64) and I want to stack them such that I have a tensor output with shape (16,2,768,64,64), BUT I also want the channel dimension 768 to be an alternating zip of tensor1 and tensor2.\ntensor1[:, 0, :, :] and tensor2[:, 0, :, :] are stacked together as a (16, 2, 64, 64) tensor and so on and then we stack that list of stacked tensors again\u2026\nI don\u2019t think there is a way to do this with torch.cat? So I wrote this little func that should do it for me\n<code class=\"lang-auto\">def zip_and_stack_tensors(tensor1, tensor2):\n    num_of_channels = len(tensor1[0, :, 0, 0])\n    imagelist = []\n    for i in range(0, num_of_channels):\n        tensorimage = torch.stack((tensor1[:, i, :, :], tensor2[:, i, :, :]), dim=1)\n        imagelist.append(tensorimage)\n    output = torch.stack(imagelist, dim=2)\nreturn output\n<\/code>\nHowever, the backwardpass now takes about x8 times longer if I call this function; the forwardpass time does not change considerably. I guess because currently torch.stack gets called 768 + 1 times?\nIs there a way to prevent this\/write my function more efficient?","y":"Hi,\nI\u2019m not sure to understand what you\u2019re trying to do here. You can get the same result as your zip_and_stack_tensors() using a single torch.stack(). Am I missing something?\n<code class=\"lang-python\">import torch\nfrom torch import nn, optim\n\ndef zip_and_stack_tensors(tensor1, tensor2):\n    num_of_channels = len(tensor1[0, :, 0, 0])\n    imagelist = []\n    for i in range(0, num_of_channels):\n        tensorimage = torch.stack((tensor1[:, i, :, :], tensor2[:, i, :, :]), dim=1)\n        imagelist.append(tensorimage)\n    output = torch.stack(imagelist, dim=2)\n    return output\n\nt1 = torch.rand(16, 768, 64, 64)\nt2 = torch.rand(16, 768, 64, 64)\n\nout = zip_and_stack_tensors(t1, t2)\nout_opt = torch.stack([t1, t2], 1)\n\nprint(out.size() == out_opt.size())\nprint((out - out_opt).abs().max())\n<\/code>","z":"Hi,\nI\u2019m not sure to understand what you\u2019re trying to do here. You can get the same result as your zip_and_stack_tensors() using a single torch.stack(). Am I missing something?\n<code class=\"lang-python\">import torch\nfrom torch import nn, optim\n\ndef zip_and_stack_tensors(tensor1, tensor2):\n    num_of_channels = len(tensor1[0, :, 0, 0])\n    imagelist = []\n    for i in range(0, num_of_channels):\n        tensorimage = torch.stack((tensor1[:, i, :, :], tensor2[:, i, :, :]), dim=1)\n        imagelist.append(tensorimage)\n    output = torch.stack(imagelist, dim=2)\n    return output\n\nt1 = torch.rand(16, 768, 64, 64)\nt2 = torch.rand(16, 768, 64, 64)\n\nout = zip_and_stack_tensors(t1, t2)\nout_opt = torch.stack([t1, t2], 1)\n\nprint(out.size() == out_opt.size())\nprint((out - out_opt).abs().max())\n<\/code>\nYes, the shape is the same in both cases, but the order of the depth layer is different as only using stack does not zip the tensors alternatingly:\nLets say I have\nt1 = torch.135(16, 3, 64, 64), where:\nt1[:, 0, :, :] = all 1s\nt1[:, 1, :, :] = all 3s\nt1[:, 2, :, :] = all 5s\nand\nt2 = torch.246(16, 3, 64, 64), where:\nt2[:, 0, :, :] = all 2s\nt2[:, 1, :, :] = all 4s\nt2[:, 2, :, :] = all 6s\nmy function creates a tensor t3 where the depth is in the order 1,2,3,4,5,6, whereas only using stack creates a tensor t3 where the depth is in the order 1,3,5,2,4,6 or not?\nIn the code sample, I print the difference: print((out - out_opt).abs().max()) and it gives 0 for me. So the stack does the same thing as the function you shared.\nIs the function you shared doing what you want?\nYou\u2019re absolutely right. I just had a massive error in my conception of how the dim parameter works in torch.stack.\nBut yeah, this makes total sense now \nThanks for the help! "},{"x":"Hey all,\nI am trying to utilise BCELoss with weights, but I am struggling to understand. I currently am using  LSTM model to detect an event in time-series data. My output from the model and true_output are as follows[batch_size, seq_length].\nCurrently, I think I have managed to do hard code it but it\u2019s not the best way to achieve this.\n<code class=\"lang-auto\">         loss_get = self.criterion(predictions.float(), target.float())\n            # Weighted Binary Corss_entropy Function\n            loss_flat = loss_get.flatten()\n            target_flat = target.flatten()\n            loss_flat[target_flat == 1] *= self.pos_weight_factor\n            loss = loss_flat.mean()\n            loss.backward()\n<\/code>\nMy datasets are imbalance, meaning that I do not have a constant length of the dataset as well as there are more 0\u2019s than 1\u2019s, approximately 100:1, hence I need to penalise the 0\u2019s by multiplying it with an arbitrary number. I understand that there are a few topics on this, but I cannot quite get my head around it.How to apply a weighted BCE loss to an imbalanced dataset? What will the weight tensor contain? with link \"https:\/\/discuss.pytorch.org\/t\/how-to-apply-a-weighted-bce-loss-to-an-imbalanced-dataset-what-will-the-weight-tensor-contain\/56823\"\nTherefore If I wanted to apply weights with this using the built-in function or solution has suggested by this post. https:\/\/discuss.pytorch.org\/t\/solved-class-weight-for-bceloss\/3114?u=ykukkim with link \"https:\/\/discuss.pytorch.org\/t\/solved-class-weight-for-bceloss\/3114\"\nCan anyone guide me through this?\nThanks!","y":"Hello Yong Kuk!\n\n\n\n ykukkim:\n\nI am trying to utilise BCELoss with weights, but I am struggling to understand.\n\u2026\nMy datasets are imbalance, meaning that I do not have a constant length of the dataset as well as there are more 0\u2019s than 1\u2019s, approximately 100:1,\n\n\nThe most straightforward way to do this (and also better for numerical\nreasons) is to adjust your network so that it outputs raw-score logits\nfor its predictions, rather than probabilities.  (For example, if the last\nlayer of your network is a Sigmoid \u2013 that converts a logit to a\nprobability \u2013 just get rid of the Sigmoid layer.)\nThen use BCEWithLogitsLoss instead of BCELoss.  This is because\nBCEWithLogitsLoss offers a pos_weight argument that it uses to\nreweight positive samples in the loss function.  In your case you would\nset pos_weight to something like 100.  (BCELoss does not have a\npos_weight argument \u2013 probably just an oversight, rather than for\nany particular reason.)\nFor some further details, please take a look at this recent thread:\n\n\n\n\nBinary segmentation BCEWithLogitsLoss pos_weight? with link \"https:\/\/discuss.pytorch.org\/t\/binary-segmentation-bcewithlogitsloss-pos-weight\/71667\"\n\n\n    I\u2019m segmenting foreground vs background using unet and there are many more 0s than 1s due to this. It looks like    i should be using BCEWithLogitsLoss as my loss function? \nIt looks like this function takes as an argument the proportion of class imbalance - \u201cpos_weight (Tensor, optional) \u2013 a weight of positive examples. Must be a vector with length equal to the number of classes.\u201d \nDoes that mean i need to know across all my images how many pixels are 1 v 0  If so is there any best\u2026\n  \n\nGood luck!\nK. Frank","z":"Hello Yong Kuk!\n\n\n\n ykukkim:\n\nI am trying to utilise BCELoss with weights, but I am struggling to understand.\n\u2026\nMy datasets are imbalance, meaning that I do not have a constant length of the dataset as well as there are more 0\u2019s than 1\u2019s, approximately 100:1,\n\n\nThe most straightforward way to do this (and also better for numerical\nreasons) is to adjust your network so that it outputs raw-score logits\nfor its predictions, rather than probabilities.  (For example, if the last\nlayer of your network is a Sigmoid \u2013 that converts a logit to a\nprobability \u2013 just get rid of the Sigmoid layer.)\nThen use BCEWithLogitsLoss instead of BCELoss.  This is because\nBCEWithLogitsLoss offers a pos_weight argument that it uses to\nreweight positive samples in the loss function.  In your case you would\nset pos_weight to something like 100.  (BCELoss does not have a\npos_weight argument \u2013 probably just an oversight, rather than for\nany particular reason.)\nFor some further details, please take a look at this recent thread:\n\n\n\n\nBinary segmentation BCEWithLogitsLoss pos_weight? with link \"https:\/\/discuss.pytorch.org\/t\/binary-segmentation-bcewithlogitsloss-pos-weight\/71667\"\n\n\n    I\u2019m segmenting foreground vs background using unet and there are many more 0s than 1s due to this. It looks like    i should be using BCEWithLogitsLoss as my loss function? \nIt looks like this function takes as an argument the proportion of class imbalance - \u201cpos_weight (Tensor, optional) \u2013 a weight of positive examples. Must be a vector with length equal to the number of classes.\u201d \nDoes that mean i need to know across all my images how many pixels are 1 v 0  If so is there any best\u2026\n  \n\nGood luck!\nK. Frank\nHey Frank,\nThank you for your reply! Your reply has cleared some aspects. Furthermore, reading through your thread it helped me even more.\nHowever, I am very new to machine learning, and I am slightly confused with the following terms:\n\nmulti-label, multi-class  classification\n\nWould you care to explain this for me?\nFurthermore, your method seems to me that what I already have done is pretty much the same? as the sigmoid function is internally performed with BCEWithLogitsloss Have I understood correctly?\nThank you!\nHi Yong Kuk!\n\n\n\n ykukkim:\n\n\nmulti-label, multi-class  classification\n\nWould you care to explain this for me?\n\n\nBy way of example, in a conventional three-class (\u201ccat,\u201d \u201cdog,\u201d \u201cbird\u201d)\nclassification problem, given an image, you would say that it is an\nimage of exactly one of a cat or a dog or a bird.  (And you wouldn\u2019t say\nit was \u201cnone of the above\u201d unless you explicitly had a fourth, \u201cnone of\nthe above\u201d class.)\nIn a multi-label (and in this case, three-class) classification problem\nyou would say that an images does or does not contain a cat, and\nalso does or does not contain a dog, and also does or does not contain\na bird.  It can contain any combination, and it might not contain any of\nthe above, and it might contain all three.  You can see that such a\nmulti-label problem is three binary problems (cat: yes or no, dog:\nyes or no, bird: yes or no) run at the same time with the same network.\n\n\nthe sigmoid function is internally performed with BCEWithLogitsloss Have I understood correctly?\n\n\nYes, BCEWithLogitsLoss calculates LogSigmoid internally (in effect\ncalculating Sigmoid internally).  This is numerically more stable than\npassing your logits through Sigmoid and then passing them to\nBCELoss.  (Unless you have specific reason why you need to use\nBCELoss \u2013 and understand it \u2013 you should always use\nBCEWithLogitsLoss instead.)\nBest.\nK. Frank"},{"x":"Hello,\nI am really sorry for this question which is probably trivial to answer but I am struggling to find an answer and I am a beginner.\nI am trying to implement a simple autoencoder following the RedNet architecture designed for denoising (I know there exists many implementations but it is just a way for me to understand autoencoder). The encoder and decoder are symetric (stride: 2 for the encoder \/ upsample X2 for  the decoder) and this architecture uses skip layers: the output of some layers of the encoder are directly summed with the corresponding output of the decoder (i.e the output of the first deconvolution will be summed with the output of the second last deconvolution layer of the decoder).\nHere is my code (for the encoder decoder):\ndef encoder(self, features):\n\n    x = self.enc_conv_1(features)\n    fMap1 = F.leaky_relu(x) \n   \n    x = self.enc_conv_2(fMap1)\n    fMap2 = F.leaky_relu(x)\n    \n    x = self.enc_conv_3(fMap2)\n    fMap3 = F.leaky_relu(x)\n\n    encoded = self.enc_linear(fMap3)\n\n    return encoded, fMap1, fMap2\n\ndef decoder(self, encoded, fMap1, fMap2):\n    x = self.dec_linear_1(encoded)\n    x = x.view(-1, 64, 64, 64)\n\n    x = self.dec_deconv_1(x)\n    x = x + fMap2\n    x = F.leaky_relu(x)\n\n    x = self.dec_deconv_2(x)\n    x = x + fMap1\n    x = F.leaky_relu(fx)\n\n    x = self.dec_deconv_3(x)\n    x = F.leaky_relu(x)\n\n    decoded = torch.sigmoid(x)\n    return decoded\n\ndef forward(self, features):\n    \n    encoded, fMap1, fMap2 = self.encoder(features)\n    decoded = self.decoder(encoded,fMap1, fMap2)\n     \n    return decoded\n\nMy question is, when gradients are backpropagated will the autograd mechanism also backpropate gradients directly through the \u201cskip path\u201d, for instance, will the gradient backpropagate through all the network but also diretly from the second last layer of the decoder back to the first layer of the encoder?\nIf not, how can this be done, do I need to merge the encoder and decoder function within the same function?\nI hope my question is clear. I thank you for your help.\nKind regards\nFrancois","y":"Hi Francois,\nAutograd will track all operations in the forward pass in a computation graph and use it in the backward pass to calculate the gradients for all parameters.\nYou can pass the output of one module\/model to another one. As long as you don\u2019t detach the activations e.g. via tensor = tensor.detach(), it should work out of the box.\nLet us know, if you need more information.","z":"Hi Francois,\nAutograd will track all operations in the forward pass in a computation graph and use it in the backward pass to calculate the gradients for all parameters.\nYou can pass the output of one module\/model to another one. As long as you don\u2019t detach the activations e.g. via tensor = tensor.detach(), it should work out of the box.\nLet us know, if you need more information.\nThank you very much for your quick reply. I  just checked if there were any difference by merging the encoder and decoder functions but no \u2026 it produces the same result.\nI also realised that basically using this \u201cskip layer\u201d technique probably imply that you need to have a fair amount of noise on your input data otherwise the network will not learning anything due to the skip layer mechanism.\nWhat I don\u2019t understand though with my test is that I train my network with only one image (again, for testing) that I blurred. The network seems to learn how to go from the blurred image to the unblurred one. However, during my evaluation, if I input any other image my network will always output the sharp version of the image used for training\u2026 I guess I have a massive bug in my code because it does not make any sense.\nIf you have any idea or extra explanation ?\nThank you again for your answer.\nFrancois\nIf you are using a single image during training, your model overfits on this particular sample.\nIt\u2019s not learning to deblur images, but to output your desired target.\nIt\u2019s useful as a sanity test to check if your code has some hidden bugs, but eventually you would have to scale up your experiment.\nOk, thank you. It just sounds a bit curious to me as during the evaluation the weight are fixed and the code generated by the encoder will obviously be different than the one generated during the training (again using a single image different from the evaluation image). As such, I was expecting a really strange output which would not make any sense.\nYeah, that sounds also reasonable.\nHowever, some of my models were just killing the complete input signal (making it negative, so that relu will set it to zero), while the last output bias was creating my prediction. \nOh. Ok that definitely makes sense now. I will try this with a larger dataset and see what will be learned by the network. My hope is that, by using different size of blur kernel, the network will still able to learn useful features rather that the process of debluring images per se.\nThank you a lot for your answer, it is really helpful.\nFrancois"},{"x":"Hi,\nI am solving a non-linear optimization problem and use torch.autograd.grad to provide Jacobian that an optimizer requires.\nPlease see the code snippet below:\n<code class=\"lang-python\">_stateplus = torch.stack([x[0], x[1], _ar1_plus]).numpy()\n_controls_plus[z_idx, epsilon_idx, :] = policy_plus[z_idx].evaluate(\n   _stateplus)\n<\/code>\nwhere x is a tensor variable with respect to which I want to compute the gradient.\nI add the .numpy() argument, because the .evaluate() accepts only numpy array.\nBut in this case, I encounter the following error as expected:\n<code class=\"lang-python\">RuntimeError: Can't call numpy() on Variable that requires grad. Use var.detach().numpy() instead.\n<\/code>\nI think the detach().numpy() argument breaks the computational graph, but I need a gradient with respect to x.\nHow can I avoid this situation?","y":"Hi,\nUnfortunately, you will only be able to get gradients using autograd if you use pytorch\u2019s Tensor and operators to do all the computations.\nIf you have an operation that you cannot do this way, you will need to provide the gradient formula yourself for that step. You can see how to extend the autograd in this Note with link \"https:\/\/pytorch.org\/docs\/stable\/notes\/extending.html#extending-torch-autograd\".","z":"Hi,\nUnfortunately, you will only be able to get gradients using autograd if you use pytorch\u2019s Tensor and operators to do all the computations.\nIf you have an operation that you cannot do this way, you will need to provide the gradient formula yourself for that step. You can see how to extend the autograd in this Note with link \"https:\/\/pytorch.org\/docs\/stable\/notes\/extending.html#extending-torch-autograd\".\nHi,\nThank you for the link.\nSo you meant var.detach().numpy() breaks the computational graph and we cannot compute the gradient correctly anymore by using torch.autograd.grad(). Is it correct?\n\nSo you meant var.detach().numpy() breaks the computational graph and we cannot compute the gradient correctly anymore by using torch.autograd.grad() . Is it correct?\n\nIt is correct. Because you convert the Tensor into a numpy array, we cannot track gradients anymore.\nThank you. It is a bit sad news but I try to come up with another route.\nUnfortunately we cannot do magic  For AD to work, we need to know everything that you do with the data. If we cannot know what you function did, we cannot compute gradients for it.\nYes you are definitely correct. Thank you again."},{"x":"Hi,\nwhat is the simplest way to compute full Jacobian and Hessian of Loss w.r.to neural network biases only.\nI want to do this layer wise but considering one layer at a time and I am not worried about speed for now.\nThanks","y":"The y should be a single Tensor right?\nSo flat_y here should be a 1D Tensor. And flat_y[i] will be a single number. So it is a scalar output.\nIs the error coming from this function?","z":"Hi,\nYou can check the implementation here: https:\/\/gist.github.com\/apaszke\/226abdf867c4e9d6698bd198f3b45fb7 where you give only the bias as inputs.\nThis gives me error:\none of the variables needed for gradient computation has been modified by an inplace operation: [torch.FloatTensor []] is at version 2; expected version 1 instead. Hint: enable anomaly detection to find the operation that failed to compute its gradient, with torch.autograd.set_detect_anomaly(True).\nIf I do recursive grad(grad), I don\u2019t get the full hessian as we all already know.\nDoes replacing this:\n<code class=\"lang-auto\">grad_x, = torch.autograd.grad(flat_y, x, grad_y, retain_graph=True, create_graph=create_graph)\n<\/code>\nby\n<code class=\"lang-auto\">grad_x, = torch.autograd.grad(flat_y[i], x, retain_graph=True, create_graph=create_graph)\n<\/code>\nsolves the error?\nNo, now I get:\ngrad can be implicitly created only for scalar outputs\nThe y should be a single Tensor right?\nSo flat_y here should be a 1D Tensor. And flat_y[i] will be a single number. So it is a scalar output.\nIs the error coming from this function?\nThanks, got it\u2026\nIt works now."},{"x":"Given a model (e.g. a cnn) with two losses, with the first loss (loss1) computed halfway through the cnn and the second loss (loss2) computed at the end of the cnn. Then, these two losses are summed and we do total_loss.backward() (total_loss = loss1 + loss2). Will the gradients of the second half of the cnn be impacted by loss1 (i.e. would the gradients of the second half of the cnn be same if we did instead loss2.backward() )?","y":"Hi,\nNo it won\u2019t because for a parameter w in the second half of your net, dloss1\/dw = 0. So the contribution of these will just be 0 (ignored in practice).","z":"Hi,\nNo it won\u2019t because for a parameter w in the second half of your net, dloss1\/dw = 0. So the contribution of these will just be 0 (ignored in practice)."},{"x":"Hi there,\nIn the training phase, then I run loss.backward(), it raised this error:\n<code class=\"lang-auto\">  File \"\/home\/jingweipeng\/anaconda3\/envs\/pytorch\/lib\/python3.7\/site-packages\/torch\/tensor.py\", line 107, in backward\n    torch.autograd.backward(self, gradient, retain_graph, create_graph)\n  File \"\/home\/jingweipeng\/anaconda3\/envs\/pytorch\/lib\/python3.7\/site-packages\/torch\/autograd\/__init__.py\", line 93, in backward\n    allow_unreachable=True)  # allow_unreachable flag\nRuntimeError: Function MulBackward0 returned an invalid gradient at index 1 - expected type torch.FloatTensor but got torch.cuda.FloatTensor\n<\/code>\nI have checked device attribute of all parameters, using the code snippet like this with link \"https:\/\/discuss.pytorch.org\/t\/unction-cudnnconvolutionbackward-returned-an-invalid-gradient-at-index-1-expected-type-torch-floattensor-but-got-torch-cuda-floattensor\/51845\/2\".\nThe Loss Function is simple nn.CrossEntropyLoss().\nHow can I trace such an error, using hooks? Could you give me some hints or advice? Thanks in advance!","y":"I have solved this error. Some variables initialized in forward were not put deployed on cuda.","z":"I think there must be some tensors on CPU and some tensors on GPU, right?\nHow can I find such invalid gradient \nI have solved this error. Some variables initialized in forward were not put deployed on cuda.\nHi,\nI\u2019m getting the same error, can you share how you found out which variables were not deployed on cuda. I\u2019m using a large model , so it is difficult for me to print out the type of every model and check. Is there any other way to go about it?"},{"x":"I am using a boolean mask in a network that perform some attention mechanisms. The boolean mask is updated in-place in some loop which causes autograd to crash. Here is an example of the kind of stuff that I do and that does NOT work:\n<code class=\"lang-auto\">import torch\nimport torch.nn.functional as F \n\nd = 4\nx = torch.rand(d, requires_grad=True)\nmask = torch.zeros(d).bool()\n\n# iteration 1\nlabel = 1\nmask[0] = True\ny = x.masked_fill( mask , float('-inf') )\np = F.softmax(y,dim=0)\nloss = - torch.log( p[label] )\n\n# compute by hand grad of the loss at iteration 1\nindicator = torch.zeros(d)\nindicator[label] = 1\ngradloss1 = p-indicator\n\n# iteration 2\nlabel = 1\nmask[3] = True\ny = x.masked_fill( mask , float('-inf') )\np = F.softmax(y,dim=0)\nloss += - torch.log( p[label] )\n\n# compute by hand grad of the loss at iteration 2\nindicator = torch.zeros(d)\nindicator[label] = 1\ngradloss2 = p-indicator\n\n# backprop\nloss.backward()\n\n# check we got correct gradient\nprint(x.grad)\nprint(gradloss1+gradloss2)\n<\/code>\nThis can be fixed by replacing the above code by the following code, in which the boolean mask is not updated in-place:\n<code class=\"lang-auto\">import torch\nimport torch.nn.functional as F \n\nd = 4\nx = torch.rand(d, requires_grad=True)\n\n# iteration 1\nlabel = 1\nmask = torch.zeros(d).bool()\nmask[0] = True\ny = x.masked_fill( mask , float('-inf') )\np = F.softmax(y,dim=0)\nloss = - torch.log( p[label] )\n\n# compute by hand grad of the loss at iteration 1\nindicator = torch.zeros(d)\nindicator[label] = 1\ngradloss1 = p-indicator\n\n# iteration 2\nlabel = 1\nmask = torch.zeros(d).bool()\nmask[0] = True\nmask[3] = True\ny = x.masked_fill( mask , float('-inf') )\np = F.softmax(y,dim=0)\nloss += - torch.log( p[label] )\n\n# compute by hand grad of the loss at iteration 2\nindicator = torch.zeros(d)\nindicator[label] = 1\ngradloss2 = p-indicator\n\n# backprop\nloss.backward()\n\n# check we got correct gradient\nprint(x.grad)\nprint(gradloss1+gradloss2)\n<\/code>\nI am a little confused because the boolean mask has always a requires_grad flag set to False. Why does it matter if the mask is updated in-place or not? Can someone provide a little more explanations on how autograd work in this case? Thanks!","y":"The thing is that the backward operation of masked_fill() needs to know where the input was written into. So it needs the value of the mask. So if you modify it inplace, the autograd engine will fail to compute gradients.","z":"The thing is that the backward operation of masked_fill() needs to know where the input was written into. So it needs the value of the mask. So if you modify it inplace, the autograd engine will fail to compute gradients.\nThanks,  that makes complete sense!"},{"x":"After creating my trainset as below\ntrainset = torchvision.datasets.ImageFolder('data\/train\/', transform=transform)\nrunning trainset.classes returns this list ['.ipynb_checkpoints', '0', '1'].\nLater when I try to train my model I get the error\nIndexError: Target 2 is out of bounds\nwhich I believe is due to my classes being length 3 instead of 2.\nHow do I fix this safely?","y":"Was able to fix the issue by deleting the .ipynb_checkpoints folder using shutil.rmtree('data\/train\/.ipynb_checkpoints')","z":"Was able to fix the issue by deleting the .ipynb_checkpoints folder using shutil.rmtree('data\/train\/.ipynb_checkpoints')"},{"x":"Let\u2019s assume the following situation.\n<code class=\"lang-auto\">logits = model(inputs)\nlogits_x = logits[:batch_size]\nlogits_u_w, logits_u_s = logits[batch_size:].chunk(2)\ndel logits\n\ne.g.1\npseudo_label = torch.softmax(logits_u_w, dim=-1).detach()\ne.g.2\npseudo_label = torch.softmax(logits_u_w.detach(), dim=-1)\n<\/code>\nQ. I want the operation between \u201clogits_u_w\u201d and \u201csoftmax\u201d not to be included in autograd. What is correct? What is the reason?","y":"The two will actually give you the same result if you write them this way.\nI would recommend the 2nd one like  because in the first one, you first create the graph for the softmax then discard it when you detach. In the second one, you just never create that part of the graph.\nBut the difference is going to be very small.","z":"I would say that your second equation (e.g.2) is the correct one.\nAccording to the doc (https:\/\/pytorch.org\/tutorials\/beginner\/blitz\/autograd_tutorial.html#sphx-glr-beginner-blitz-autograd-tutorial-py):\n\nTo stop a tensor from tracking history, you can call  .detach()  to detach it from the computation history, and to prevent future computation from being tracked.\n\nHence, your tensor in e.g.1 would be tracked until the softmax while e.g.2 would stop tracking the gradient at logits_u_w\nThe two will actually give you the same result if you write them this way.\nI would recommend the 2nd one like  because in the first one, you first create the graph for the softmax then discard it when you detach. In the second one, you just never create that part of the graph.\nBut the difference is going to be very small.\nThank you for your answer. I find the doc a bit confusing then. From the doc:\n\nTo stop a tensor from tracking history, you can call  .detach()  to detach it from the computation history, and to prevent future computation from being tracked.\n\n.detach() just detach the gradient from the graph and all the futur operations will not be tracked.\nBut your answer seems to say that it does stop tracking, as the doc says, AND  delete the last operation from the graph (here the softmax operation). Could you please clarify ?\nThe only thing .detach() does is return a new Tensor without gradient history.\nBut if in one line you create the result then detach it. The result that was tracking history goes out of scope. And so this part of the history is not referenced by anything and is deleted."},{"x":"I am trying to use the code for IIC (Invariant Information Clustering) : https:\/\/github.com\/xu-ji\/IIC\nIt basically calls the network twice on original input and modified input.\nx_out = net(sample)    # Softmax output for original sample\nx_tf_out = net(noisy_sample)   # Softmax output for noisy sample\nloss = IIC_loss(x_out, x_tf_out)\nAnd then the loss wants the original sample and the noisy sample to be labeled to the same class. It is computed by the IIC loss :\ndef IIC_loss(x_out, x_tf_out, EPS=sys.float_info.epsilon):\n# has had softmax applied\n_, k = x_out.size()\np_i_j = compute_joint(x_out, x_tf_out)\nassert (p_i_j.size() == (k, k))\np_i = p_i_j.sum(dim=1).view(k, 1).expand(k, k)\np_j = p_i_j.sum(dim=0).view(1, k).expand(k, k)  # but should be same, symmetric\n# avoid NaN losses. Effect will get cancelled out by p_i_j tiny anyway\np_i_j[(p_i_j < EPS).data] = EPS\np_j[(p_j < EPS).data] = EPS\np_i[(p_i < EPS).data] = EPS\nloss = - p_i_j * (torch.log(p_i_j) \n- torch.log(p_j) \n- torch.log(p_i))\nloss = loss.sum()\nreturn loss\ndef compute_joint(x_out, x_tf_out):\n# produces variable that requires grad (since args require grad)\nbn, k = x_out.size()\nassert (x_tf_out.size(0) == bn and x_tf_out.size(1) == k)\np_i_j = x_out.unsqueeze(2) * x_tf_out.unsqueeze(1)  # bn, k, k\np_i_j = p_i_j.sum(dim=0)  # k, k\np_i_j = (p_i_j + p_i_j.t()) \/ 2.  # symmetrise\np_i_j = p_i_j \/ p_i_j.sum()  # normalise\nreturn p_i_j\nWhen I now call backward() on the loss, it leads to the following error :\nloss.backward()\noptimizer.step()\nin backward\n#allow_unreachable=True)  # allow_unreachable flag\nRuntimeError: unsupported operation: more than one element of the written-to tensor refers to a single memory location. Please clone() the tensor before performing the operation.\nWhat changes in the code are needed in new version of Pytorch ? The code ran perfectly even for PyTorch 1.0.","y":"Hi albanD,\nI figured out the issue yesterday. The inplace operations inside the function IIC_loss created the problem.\nInstead of the original lines,\np_i_j[(p_i_j < EPS).data] = EPS\np_j[(p_j < EPS).data] = EPS\np_i[(p_i < EPS).data] = EPS\nThe correct version for new PyTorch (>= 1.3) should be as follows :\np_i_j = torch.where(p_i_j < EPS, torch.tensor([EPS], device = p_i_j.device), p_i_j)\np_j = torch.where(p_j < EPS, torch.tensor([EPS], device = p_j.device), p_j)\np_i = torch.where(p_i < EPS, torch.tensor([EPS], device = p_i.device), p_i)","z":"Hi,\nYes we added more checks for operations that were potentially invalid.\nCould you run your code with anomaly mode with link \"https:\/\/pytorch.org\/docs\/stable\/autograd.html#anomaly-detection\" by setting autograd.set_detect_anomaly(True) at the beginning of your script. That will let us know what is causing the error.\nHi albanD,\nI figured out the issue yesterday. The inplace operations inside the function IIC_loss created the problem.\nInstead of the original lines,\np_i_j[(p_i_j < EPS).data] = EPS\np_j[(p_j < EPS).data] = EPS\np_i[(p_i < EPS).data] = EPS\nThe correct version for new PyTorch (>= 1.3) should be as follows :\np_i_j = torch.where(p_i_j < EPS, torch.tensor([EPS], device = p_i_j.device), p_i_j)\np_j = torch.where(p_j < EPS, torch.tensor([EPS], device = p_j.device), p_j)\np_i = torch.where(p_i < EPS, torch.tensor([EPS], device = p_i.device), p_i)"},{"x":"My own loss function  is special and can not be solved by the \u2018autograd\u2019 function. so I want to define the backward()  in my loss function. But I don\u2019t know if I can get the gradient of loss by hand and make it back throughout the network automatically\uff1f\n<code class=\"lang-auto\">import torch\n\nclass MyLoss(torch.autograd.Function):\n\n    \n    def forward(self, input, label):\n        ctx.save_for_backward(input, label)\n        #my code\n        return loss\n\n    \n    def backward(ctx, grad_output):\n        input, label = ctx.saved_tensors\n        # my code\n        return grad_input, None\n\n\nmodel = MyNetwork()#this is my network\nmyloss = MyLoss()\n\nfor t in range(500):\n    output = model(input)\n    loss = myloss(output,label)\n    loss.backward()\n\n<\/code>\nIs the above code feasible? I am not sure if the gradient can automatic back throughout the network.\nThank you.","y":"Yes you can do that. See more details in the doc with link \"https:\/\/pytorch.org\/docs\/stable\/notes\/extending.html#extending-torch-autograd\".\nBe careful:\n\nYou should not instantiate an instance of the Function. You should do loss = MyLoss.apply(output, label). If you prefer to instantiate an instance, you can simply create a new module like:\n\n<code class=\"lang-auto\">class MyLossMod(nn.Module):\n    def forward(self, input, target):\n        return MyLoss.apply(input, target)\n\n# Then you can use any of the two\nloss = MyLoss.apply(input, target)\n# or\nmylossmod = MyLossMod()\nloss = mylossmod(input, target)\n<\/code>\n\nYour arguments for forward should be ctx not self but I guess that\u2019s just a typo\n","z":"Yes you can do that. See more details in the doc with link \"https:\/\/pytorch.org\/docs\/stable\/notes\/extending.html#extending-torch-autograd\".\nBe careful:\n\nYou should not instantiate an instance of the Function. You should do loss = MyLoss.apply(output, label). If you prefer to instantiate an instance, you can simply create a new module like:\n\n<code class=\"lang-auto\">class MyLossMod(nn.Module):\n    def forward(self, input, target):\n        return MyLoss.apply(input, target)\n\n# Then you can use any of the two\nloss = MyLoss.apply(input, target)\n# or\nmylossmod = MyLossMod()\nloss = mylossmod(input, target)\n<\/code>\n\nYour arguments for forward should be ctx not self but I guess that\u2019s just a typo\n\nThank you for your reply!\nPlease, when our loss function cannot be solved by autograd ? and if it is the case, do we need then to write two classes, one extending the Function class and the other extending nn.module ? (like in the tutorial https:\/\/pytorch.org\/docs\/master\/notes\/extending.html)\nThank you!\nYou will need to define a custom Function.\nThe custom Module presented there is just for convenience. You can use YourFunction.apply() in your code if you don\u2019t want to use Modules."},{"x":"I am trying to find the value of loss for a temporary set of variables in a neural network so that I can have their gradients but I do not want these variables in the computational graph. I have a matrix of weights, I want to know their gradients but this matrix is not in the neural network\u2019s model parameters.","y":"If these are used during the computation of the loss in a differentiable manner, you can get:\n\ncall .retain_grad() on these intermediary elements so that their .grad field will be populated when you call loss.backward().\nGet the grad only for these with grads = autograd.grad(loss, interm_tensors).\n","z":"If these are used during the computation of the loss in a differentiable manner, you can get:\n\ncall .retain_grad() on these intermediary elements so that their .grad field will be populated when you call loss.backward().\nGet the grad only for these with grads = autograd.grad(loss, interm_tensors).\n\nWhat should be the format of interm_tensors to do that?\nYou can do that on any Tensor that has requires_grad=True. Note that this field is set automatically when a Tensor is computed in a differentiable manner."},{"x":"Hi all,\nI\u2019m relatively new to PyTorch and have been having an issue with performing back-propagation that I just don\u2019t know how to solve.\nFirst of all, my code for calculating the loss and then performing back-propagation:\n<code class=\"lang-auto\">    def discriminator_loss(self, real_samples, fake_samples):\n        criterion = BCEWithLogitsLoss()\n\n        real_loss = criterion(self.discriminator(real_samples), ones((1, 1)).to(self.gpu_id))\n        fake_loss = criterion(self.discriminator(fake_samples), zeros((1, 1)).to(self.gpu_id))\n\n        return (real_loss + fake_loss) \/ 2\n\n    ...\n\n        def optimise_discriminator(self, input, targets):\n\n        # Load input and targets into GPU memory.\n        input = input.cuda()\n        for target in targets:\n            target.cuda()\n\n        # Pass input through the generator.\n        outputs = self.generator.forward(input)\n\n        # Get the loss.\n        discriminator_loss = discriminator_loss(targets, outputs)\n\n        # Optimise the discriminator.\n        self.optimiser_discriminator.zero_grad()\n        discriminator_loss.backward()\n        self.optimiser_discriminator.step()\n\n        # Return the loss.\n        return discriminator_loss.item()\n<\/code>\nThe problem occurs when I call the backward method on the output of discriminator_loss function. I get the following error:\n<code class=\"lang-auto\">Traceback (most recent call last):\n  File \"train.py\", line 84, in <module>\n    main()\n  File \"train.py\", line 74, in main\n    generator_loss = model.optimise_discriminator(input, targets)\n  File \"\/nobackup\/sccmho\/projects\/DBT2FFDM\/models\/dbt2ffdm_model.py\", line 101, in optimise_discriminator\n    discriminator_loss.backward()\n  File \"\/nobackup\/sccmho\/.conda\/envs\/pix2pix\/lib\/python3.7\/site-packages\/torch\/tensor.py\", line 166, in backward\n    torch.autograd.backward(self, gradient, retain_graph, create_graph)\n  File \"\/nobackup\/sccmho\/.conda\/envs\/pix2pix\/lib\/python3.7\/site-packages\/torch\/autograd\/__init__.py\", line 99, in backward\n    allow_unreachable=True)  # allow_unreachable flag\n  File \"\/nobackup\/sccmho\/.conda\/envs\/pix2pix\/lib\/python3.7\/site-packages\/torch\/autograd\/function.py\", line 77, in apply\n    return self._forward_cls.backward(self, *args)\n  File \"\/nobackup\/sccmho\/.conda\/envs\/pix2pix\/lib\/python3.7\/site-packages\/torch\/autograd\/function.py\", line 181, in backward\n    raise NotImplementedError\n<\/code>\nThis occurs when only returning real_loss or fake_loss in the discriminator_loss function as well. I\u2019m at a loss () as to what is causing this, as I\u2019m not using a custom loss function. Any guidance would be appreciated.","y":"Hi,\nAre you using a custom autograd.Function ? This error is because you try to use a custom autograd Function without implementing the backward.","z":"Hi,\nAre you using a custom autograd.Function ? This error is because you try to use a custom autograd Function without implementing the backward.\nHi Alban,\nTurns out that was the issue, thanks a lot for your prompt help! "},{"x":"I\u2019m working on some cuda extension for pytorch. When I received the grad_output, printing it yields correct values.\n<code class=\"lang-auto\">std::cout << upstreamGrad << std::endl;\n\noutput \n1 1 1 1 1 1\n[ CUDAFloatType{1,6} ]\n<\/code>\nHowever, accessing it with a pointer returns wrong values.\n<code class=\"lang-auto\">std::vector<float> tmp(6);\ncudaMemcpy(tmp.data(), upstreamGrad.data_ptr(), 6 * sizeof(float), cudaMemcpyDeviceToHost);\nfor (int i = 0; i < 6; i++) {\n  std::cout << tmp[i] << std::endl;\n}\n\noutput\n1\n20\n12\n0\n27\n22\n<\/code>\nI was wondering if I\u2019m missing something here.","y":"You are not checking the storage_offset and stride informations.\nGiven that the first value is correct but not the other one, I would guess that this is a non-contiguous Tensor and you should use the stride to read it properly.\nDoing upstreamGrad.contiguous().data_ptr() should give you the right result.","z":"You are not checking the storage_offset and stride informations.\nGiven that the first value is correct but not the other one, I would guess that this is a non-contiguous Tensor and you should use the stride to read it properly.\nDoing upstreamGrad.contiguous().data_ptr() should give you the right result."},{"x":"Hi,\nI want to stack some tensors keeping their automatic gradient information but screw up.\nI have the following script where I want to compute the derivative with respect to x:\n<code class=\"lang-python\">_k_plusplus = []\nfor aplus_idx, aplus_val in enumerate(aplus):\n    state_plus = torch.tensor([x[0], aplus_val])[None, :]\n    pred_mean = likelihood(gp_kplus(state_plus)).mean\n    _k_plusplus.append(pred_mean)\n    k_plusplus = torch.stack(_k_plusplus).flatten()\n<\/code>\nwhere print(k_plusplus) returns:\n<code class=\"lang-python\">tensor([0.3481, 0.3481, 0.3481, 0.3481, 0.3481], dtype=torch.float64,\n       grad_fn=<ViewBackward>)\n<\/code>\nIt seems to be OK for me, but with this syntax, torch.autograd.gradcheck retunrs False.\nHere note that pred_mean = likelihood(gp_kplus(state_plus)).mean is coming from GPyTorch.\nprint(pred_mean) retuns:\n<code class=\"lang-python\">tensor([0.2027], dtype=torch.float64, grad_fn=<ViewBackward>)\n<\/code>\nagain it seems to be reasonable for me.\nMy guess is when I stack the tensors by torch.stack or maybe at some points, I lose the automatic gradient information.\nWhen I simplify the script:\n<code class=\"lang-python\">k_plusplus = torch.empty(aplus.shape)\nfor aplus_idx, aplus_val in enumerate(aplus):\n   k_plusplus[aplus_idx] = torch.tensor(0.1)\n<\/code>\nIt passes the gradient check without changing the rest of my code.\nThank you in advance for your kind help.","y":"Sorry this is my bad.\nIn the above snippet, type(x) is torch.tensor and I overwrite the tensor object by torch.tensor(x[0]...) where I lost the grad_fn information, I guess.\nWhen I modify\n<code class=\"lang-python\">state_plus = torch.stack([x[0], aplus_val])[None, :]\n<\/code>\neverything works well.","z":"Sorry this is my bad.\nIn the above snippet, type(x) is torch.tensor and I overwrite the tensor object by torch.tensor(x[0]...) where I lost the grad_fn information, I guess.\nWhen I modify\n<code class=\"lang-python\">state_plus = torch.stack([x[0], aplus_val])[None, :]\n<\/code>\neverything works well."},{"x":"In previous versions we did something like:\n<code class=\"lang-auto\">for p in model.parameters():\n    p.data.add_(-lr, p.grad.data)\n<\/code>\nMigration guide says that now using .data is unsafe, so how to rewrite this using .detach()?","y":"Hi,\n<code class=\"lang-auto\">for p in model.parameters():\n    p = (p - lr * p.grad).detach()\n<\/code>\nNo! Don\u2019t do that! Apologies for the bold font. If you assign p, you will just overwrite the name and the model parameter will be unchanged.\nNow, if you look at the source code of an optimizer, say SGD with link \"https:\/\/github.com\/pytorch\/pytorch\/blob\/52e4d3c4a2ca48558f218fd4f09aa9c5aa06450c\/torch\/optim\/sgd.py#L107\", you find that the update rule is still more or less what your original post uses:\n<code class=\"lang-auto\">for p in model.parameters():\n    p.data.add_(-lr, p.grad.data)\n<\/code>\nso that will work because everyone still uses it.\nThat said, I personally think that using torch.no_grad()\n<code class=\"lang-auto\">with torch.no_grad()\n    for p in model.parameters():\n        p.add_(-lr, p.grad)\n<\/code>\nis a better way to achieve the same.\nBut take this last advice with a grain of salt: I\u2019m biased because I\u2019m trying to convince people that there should be an inplace hook for some caching style applications.\nAs a general rule, I try to consult the PyTorch source code as an example as much as I can, and so far, I seem to be doing OK with that strategy.\nBest regards\nThomas","z":"Is it correct to do the following:\n<code class=\"lang-auto\">for p in model.parameters():\n    p = (p - lr * p.grad).detach()\n<\/code>\nHi,\n<code class=\"lang-auto\">for p in model.parameters():\n    p = (p - lr * p.grad).detach()\n<\/code>\nNo! Don\u2019t do that! Apologies for the bold font. If you assign p, you will just overwrite the name and the model parameter will be unchanged.\nNow, if you look at the source code of an optimizer, say SGD with link \"https:\/\/github.com\/pytorch\/pytorch\/blob\/52e4d3c4a2ca48558f218fd4f09aa9c5aa06450c\/torch\/optim\/sgd.py#L107\", you find that the update rule is still more or less what your original post uses:\n<code class=\"lang-auto\">for p in model.parameters():\n    p.data.add_(-lr, p.grad.data)\n<\/code>\nso that will work because everyone still uses it.\nThat said, I personally think that using torch.no_grad()\n<code class=\"lang-auto\">with torch.no_grad()\n    for p in model.parameters():\n        p.add_(-lr, p.grad)\n<\/code>\nis a better way to achieve the same.\nBut take this last advice with a grain of salt: I\u2019m biased because I\u2019m trying to convince people that there should be an inplace hook for some caching style applications.\nAs a general rule, I try to consult the PyTorch source code as an example as much as I can, and so far, I seem to be doing OK with that strategy.\nBest regards\nThomas\n\nNo! Don\u2019t do that!**  Apologies for the bold font. If you assign p, you will just overwrite the name and the model parameter will be unchanged.\n\nMau I ask you why is that?  Thanks!\nFundamentally that is how Python variables work.\n\nA plain assignment = as in p = foo just means \"take whatever is on the right side and give it the name on the left side (p). There isn\u2019t anything (__something__) called when\nThis is in stark contrast to the superficially similar p[something] = or x.p = which call p.__setitem__ and x.__setattr__ under the hood.\n\nBest regards\nThomas\nOK,that\u2019s really clear,Thanks !"},{"x":"consider following code will this part of computation graph correctly and gradients are computed correctly.I mean can distributions be part of graph\n\nimport torch\nimport torch.distributions.normal as tdn\ndist = tdn.Normal(mean,std) #assume network predicts mean,std\nloss = -1 * dist.log_prob(y)\n","y":"Hi,\nI think the log_prob is differentiable for Normal yes. You can check that by verifying that if mean or std has requires_grad=True, then dist.log_prob(y).requires_grad == True.\nIn general, the output requires grad, that means that the function is differentiable using the autograd.","z":"Hi,\nI think the log_prob is differentiable for Normal yes. You can check that by verifying that if mean or std has requires_grad=True, then dist.log_prob(y).requires_grad == True.\nIn general, the output requires grad, that means that the function is differentiable using the autograd.\nAnother question, tdn.cdf(y) will also differentiable right?\nI was thinking if dist = tdn.Normal(mean,std) would mess computational graph.It will not write right?"},{"x":"Hi, I\u2019m new to Pytorch and I\u2019m sorry I can\u2019t explain it because English is not native.\nI am doing GAN implementation now and am confused by weight initialization.\nIn the code I am referring to, initialize the weights as follows:\n<code class=\"lang-auto\">def weights_init(m):\n    classname = m.__class__.__name__\n    if classname.find('Conv') != -1:\n        nn.init.normal_(m.weight.data, 0.0, 0.02)\n        nn.init.constant_(m.bias.data, 0)\n        print(nn.init.constant_(m.bias.data, 0).requires_grad)\n        \n    elif classname.find('BatchNorm') != -1:\n        nn.init.normal_(m.weight.data, 1.0, 0.02)\n        nn.init.constant_(m.bias.data, 0)\n\nG.apply(weights_init)\nD.apply(weights_init)\n<\/code>\nresults is here:\n<code class=\"lang-auto\">False\nFalse\nFalse\nFalse\nFalse\nFalse\nFalse\nFalse\nFalse\nFalse\n<\/code>\nBut according to this with link \"https:\/\/discuss.pytorch.org\/t\/initializing-parameters-with-weight-or-weight-data\/39881\" using \u201cwith no_grad()\u201d is recommended.So ,\n<code class=\"lang-auto\">def weights_init(m):\n    classname = m.__class__.__name__\n    \n    with torch.no_grad():\n        if classname.find('Conv') != -1:\n            nn.init.normal_(m.weight, 0.0, 0.02)\n            nn.init.constant_(m.bias, 0)\n            print(nn.init.constant_(m.bias, 0).requires_grad)\n            \n        elif classname.find('BatchNorm') != -1:\n            nn.init.normal_(m.weight, 1.0, 0.02)\n            nn.init.constant_(m.bias, 0)\n        \nG.apply(weights_init)\nD.apply(weights_init)\n<\/code>\nbut this results are:\n<code class=\"lang-auto\">True\nTrue\nTrue\nTrue\nTrue\nTrue\nTrue\nTrue\nTrue\nTrue\n<\/code>\nI am confused\nWhat are these differences? And how does this difference affect the calculation results?\nThanks.","y":"Stick to the second approach, as the first one is deprecated and might yield unwanted side effects.\nIn a torch.no_grad() block all new operations won\u2019t be tracked. The requires_grad attribute of your parameters will not be changed:\n<code class=\"lang-python\">model = models.resnet50()\n\nwith torch.no_grad():\n    print(model.fc.weight.requires_grad)    \n    > True\n    out = model(torch.randn(1, 3, 224, 224))\n    print(out.requires_grad)\n    > False\n\nout = model(torch.randn(1, 3, 224, 224))\nprint(out.requires_grad)\n> True\n<\/code>","z":"Stick to the second approach, as the first one is deprecated and might yield unwanted side effects.\nIn a torch.no_grad() block all new operations won\u2019t be tracked. The requires_grad attribute of your parameters will not be changed:\n<code class=\"lang-python\">model = models.resnet50()\n\nwith torch.no_grad():\n    print(model.fc.weight.requires_grad)    \n    > True\n    out = model(torch.randn(1, 3, 224, 224))\n    print(out.requires_grad)\n    > False\n\nout = model(torch.randn(1, 3, 224, 224))\nprint(out.requires_grad)\n> True\n<\/code>"},{"x":"<code class=\"lang-auto\">x = Variable(torch.randn(1,3),requires_grad=True)\nz,_ = torch.max(x,1)\nz.backward()\nprint(x.grad)\n\nVariable containing:\n 1  0  0\n[torch.FloatTensor of size 1x3]\n<\/code>\nI understand the max operation is a not differentiable operation. So why can I still get the gradient here?","y":"max simply selects the greatest value and ignores the others, so max is the identity operation for that one element. Therefore the gradient can flow backwards through it for just that one element.","z":"max simply selects the greatest value and ignores the others, so max is the identity operation for that one element. Therefore the gradient can flow backwards through it for just that one element.\nAlso, argmax is not continuous almost everywhere. But max is continuous everywhere.\n<code class=\"lang-auto\">z,y = torch.max(x,1)\n<\/code>\nSo that is the reason y doesn\u2019t have a gradient function?\nWhat do you mean by \u201cargmax is not continuous almost everywhere. But max is continuous everywhere\u201d ?\nDo you mean that we can do backpropagation with max operation but not argmax operation ?\nTo be precise, I should have said that argmax is not differentiable, but max is.\nIs there a method to make index having gradient function?\ni.e.\n\nimport torch\nh = torch.randn(1,2,5, requires_grad=True); print(h)\nval,idx = h.max(1, keepdim=True)\nprint(val)\nprint(idx)\nprint(val)\n\noutputs are:\n\ntensor([[[-0.5372, -0.4683,  0.4891, -0.1686, -0.4147],\n[-1.4412,  1.2837, -0.4467,  0.1731,  1.3256]]], requires_grad=True)\ntensor([[[-0.5372,  1.2837,  0.4891,  0.1731,  1.3256]]],\ngrad_fn=)\ntensor([[[0, 1, 0, 1, 1]]])\n\nI want the tensor([[[0, 1, 0, 1, 1]]]) to have gradient function.\nIt is mathematically not differentiable, so no.\nIn this paper with link \"http:\/\/openaccess.thecvf.com\/content_ECCV_2018\/papers\/Mrigank_Rochan_Video_Summarization_Using_ECCV_2018_paper.pdf\" section 3.3\n\nWe first select Y frames (i.e. keyframes) based on the prediction scores from the\ndecoder.\n\nThe decoder output is [2,320], which means non-keyframe score and key frame score of the 320 frames. We want to find a 0\/1 vector according to the decoder output but the process of [2,320] -> 0\/1 vector seems not differentiable\u2026\nHow to implement this in pytorch?\nThank you very much.\ni guess it means that we can say the same thing to the minimum operation?\nand also, can you explain it a little bit more?\nwhy the fact that it is identity operation for the max elememt changes the situation?\nand if so, why they invented the softmax?\nActually, softmax is more like softargmax\u2026 I have to say softmax is a terrible name."},{"x":"I want to update the weights (theta) of my neural network using the update rule (t+1:next epoch, A is a constant, f and g are function of theta):\n\ntheta_{t+1} = theta_t + A f(theta) \\nabla [ g(theta) + f(theta) ]\n\nwhere nabla gives the derivative of the g and f with respect to theta.\nCurrently, the way I am implementing this update rule is (assume f and g are torch tensors with just one element and which depend of theta, the weights of the neural network):\n<code class=\"lang-auto\">optimizer.zero_grad()\nloss = A * f.detach() * (g + f)\nloss.backward()\noptimizer.step()\n<\/code>\nIs this the correct way of implementing this weight update rule in PyTorch?","y":"That would depend what your optimizer is.\nIt might be clearer to have a custom optimizer that does the scaling by A * f?\n<code class=\"lang-auto\">out = g + f\nout.backward() # Compute \\nable [ g + f]\noptimizer.step(A * f)\n<\/code>\nBut both should do the same step in the end (if your optimizer above is like SGD and has a lr of 1).","z":"That would depend what your optimizer is.\nIt might be clearer to have a custom optimizer that does the scaling by A * f?\n<code class=\"lang-auto\">out = g + f\nout.backward() # Compute \\nable [ g + f]\noptimizer.step(A * f)\n<\/code>\nBut both should do the same step in the end (if your optimizer above is like SGD and has a lr of 1)."},{"x":"Hi,\nSuppose I\u2019ve a code block wrapped with torch no grad and I call a function that does inference in it, i.e,\n<code class=\"lang-auto\">\nwith torch.no_grad():\n    my_infer(my_model, my_data)\n<\/code>\nshould I use with\n\ntorch.no_grad()\n\nat the beginning of my_infer() too or is the outer no_grad  sufficient?","y":"Hi,\nIt is a context manager. So it will enable the no_grad mode when you enter it and disable it when you leave the indentation.\nSo no need to do it again inside the function, it will still be activated.","z":"Hi,\nIt is a context manager. So it will enable the no_grad mode when you enter it and disable it when you leave the indentation.\nSo no need to do it again inside the function, it will still be activated."},{"x":"Hi guys i\u2019ve extracted the features from resnet50 before the layer of Average pooling.\nI achieve a tensor of 1x100352, but i have to respect the structure of the image, so what is the exact shape of the tensor? 2048x7x7 or 7x7x2048?","y":"In PyTorch the default is channels first, so the Tensor should be of shape [B, C, H, W] -> [1, 2048, 7, 7]","z":"In PyTorch the default is channels first, so the Tensor should be of shape [B, C, H, W] -> [1, 2048, 7, 7]"},{"x":"Hello,\n<code class=\"lang-auto\">class Subspace_model(nn.Module):\n    \"\"\"\n    Wraps a model in order to train it in a subspace\n    \"\"\"\n    def __init__(self, model, E):\n        super(Subspace_model, self).__init__()\n        self.model = model\n        self.register_buffer(\"E\", E)\n        self.d_dim = E.shape[1]\n        self.params_d = nn.Parameter(torch.zeros(self.d_dim))\n        self.register_buffer(\"params_0\", parameters_to_vector(model.parameters()))\n\n    def forward(self, x):\n        params_D = self.E @ self.params_d + self.params_0\n        vector_to_parameters(params_D, self.model.parameters())\n        return self.model.forward(x)\n<\/code>\nI want to train a model in an affine-linear subspace of the full-parameter space. For that, I created an embedding E that takes vectors from R^d to R^D. Here, d is the dimensionality of the affine-linear subspace, D is the dimensionality of the full parameter space.\nparams_0 are the initial D-dimensional parameters of the model.\nOriginally, my d-dimensional parameter-vector params_d is the zero vector. It is what I try to learn.\nA forward pass works like this: First, I do E @ params_d + params_0 in order to get the new parameters of the model. Then I apply it to the input.\nFor the backward pass, I want to do the backward of the model in order to get the gradients with respect to the D-dimensional weight vector. Then I want to backpropagate this further through the matrix E in order to update the d-dimensional Vector params_d. params_d is the only vector I want to be updated in the optimizer step!\nI think my code doesn\u2019t work, and I\u2019m not quite sure why. params_d doesn\u2019t receive any gradient.\nPotential reasons:\n\nMaybe I need to register self.model somehow as a buffer as well, since I don\u2019t want it to be updated. However, pytorch doesn\u2019t allow that.\nMaybe the gradient is not able to pass through the \u201cvector_to_parameters\u201d step. If so, how could this be solved?\n\nBest regards,\nLL","y":"Hi Leon!\n\n\n\n Leon_Lang:\n\nI want to train a model in an affine-linear subspace of the full-parameter space. For that, I created an embedding E that takes vectors from R^d to R^D.\n\n\nI don\u2019t really follow your embedding scheme, and I can\u2019t comment\non your code.\nBut if I understand goal correctly, perhaps you can project your\ngradients onto your desired subspace after an unmodified\nbackward() step.\nThe common optimizers use some variant of gradient descent\nwhere params -= learning_rate * gradient (where gradient\nmight have some momentum history in it, but this doesn\u2019t change\nthe idea).\nSo just do the same update step, but replace gradient with a\nprojected_gradient that lies in your subspace.  (You would\nhave tweak or write your own optimizer to do this.)\nAlternatively, you could perform the standard update (and not have\nto modify the optimizer), and then project your new parameters to\nlie in your subspace.  (These two approaches are equivalent*, but I\nthink in terms of projecting the gradient rather than the parameters\nfor some reason.)\n*)  \u201cProject gradient\u201d and \u201cproject (updated) parameters\u201d are equivalent\nif your subspace is understood to contain the zero vector.  From your\nstated initial condition, I understand this to be the case.\nBest.\nK. Frank","z":"Hi,\nCould you show us the vector_to_parameters function please?\nHi albanD,\nit is this one:\nhttps:\/\/pytorch.org\/docs\/stable\/_modules\/torch\/nn\/utils\/convert_parameters.html\nHere is the source code:\n<code class=\"lang-auto\">def vector_to_parameters(vec, parameters):\n    r\"\"\"Convert one vector to the parameters\n\n    Arguments:\n        vec (Tensor): a single vector represents the parameters of a model.\n        parameters (Iterable[Tensor]): an iterator of Tensors that are the\n            parameters of a model.\n    \"\"\"\n    # Ensure vec of type Tensor\n    if not isinstance(vec, torch.Tensor):\n        raise TypeError('expected torch.Tensor, but got: {}'\n                        .format(torch.typename(vec)))\n    # Flag for the device where the parameter is located\n    param_device = None\n\n    # Pointer for slicing the vector for each parameter\n    pointer = 0\n    for param in parameters:\n        # Ensure the parameters are located in the same device\n        param_device = _check_param_device(param, param_device)\n\n        # The length of the parameter\n        num_param = param.numel()\n        # Slice the vector, reshape it, and replace the old data of the parameter\n        param.data = vec[pointer:pointer + num_param].view_as(param).data\n\n        # Increment the pointer\n        pointer += num_param\n<\/code>\nAs you can see, this function is using .data which means that it break the computational graph. This is not expected to work with differentiable operations.\nnn.Parameters() are built only to be trainable parameters, not intermediary Tensors that require gradients.\nOne way to go around this is to manually delete the old parameters and just put your new weights there:\n<code class=\"lang-auto\">del mod.weight # Remove it from the Parameter list\nmod.weight = your_new weights\n# mod.weight is not just a Tensor, not a Parameter anymore.<\/code>\nHi Leon!\n\n\n\n Leon_Lang:\n\nI want to train a model in an affine-linear subspace of the full-parameter space. For that, I created an embedding E that takes vectors from R^d to R^D.\n\n\nI don\u2019t really follow your embedding scheme, and I can\u2019t comment\non your code.\nBut if I understand goal correctly, perhaps you can project your\ngradients onto your desired subspace after an unmodified\nbackward() step.\nThe common optimizers use some variant of gradient descent\nwhere params -= learning_rate * gradient (where gradient\nmight have some momentum history in it, but this doesn\u2019t change\nthe idea).\nSo just do the same update step, but replace gradient with a\nprojected_gradient that lies in your subspace.  (You would\nhave tweak or write your own optimizer to do this.)\nAlternatively, you could perform the standard update (and not have\nto modify the optimizer), and then project your new parameters to\nlie in your subspace.  (These two approaches are equivalent*, but I\nthink in terms of projecting the gradient rather than the parameters\nfor some reason.)\n*)  \u201cProject gradient\u201d and \u201cproject (updated) parameters\u201d are equivalent\nif your subspace is understood to contain the zero vector.  From your\nstated initial condition, I understand this to be the case.\nBest.\nK. Frank\nDear albanD and KFrank,\nthanks for your answers!\n: The idea to change the optimizer (in my case SGD) worked in my case.\nIn order to do so, I split up the matrix E into several components, one for each array in the parameters of the model. Then, I could manually backpropagate the gradients of all the arrays through the embedding. I then re-embedded this into the full parameter space.\nMathematically, that corresponds to multiplying the gradient with E*E.T, which is probably what you meant by \u201cprojection\u201d.\nIn case you\u2019re interested: I\u2019m reproducing the results from this paper on the intrinsic dimension of objective landscapes: https:\/\/arxiv.org\/abs\/1804.08838\nBest regards,\nLeon"},{"x":"Sometimes one needs to manually use the gradient function, because the computed quantity is useful. For example, there is a paper with link \"https:\/\/arxiv.org\/abs\/1904.10619\" that applies reweighting to CTC loss via interpreting it as cross-entropy with some distribution (it happens that CTC\u2019s gradient computes that distribution as an intermediate step).\nIs there a way to compute\/access the CTC loss gradient without resorting to backward hooks? (currently it\u2019s hidden from manual calls if I understand well)\nCan I do (efficiently) sth like:\n<code class=\"lang-auto\"># eq. 5 \ndef ctc_alignment_targets(logits, *args, dim = 1):\n    log_probs = F.log_softmax(logits.detach(), dim = dim)\n    ctc_grad, = torch.autograd.grad(F.ctc_loss(log_probs, *args), (logits,))\n    return log_probs.exp() - ctc_grad\n\n# eq. 13, but have not checked this equivalence in practice\n# alignment_targets = ctc_alignment_targets(logits, ...)\n# ctc_loss_via_cross_entropy = (alignment_targets * F.log_softmax(logits, dim = 1)).mean()\n<\/code>\nI can imagine some other ways to accomplish this (especially if modifications need not to be differentiated through), but is there a clean way to manually compute gradient of some known function?","y":"I\u2019ve tried  the following:\n<code class=\"lang-auto\">def ctc_alignment_targets(logits, targets, input_lengths, target_lengths, blank, dim = 1):\n    with torch.enable_grad():\n        logits = logits.detach().requires_grad_()\n        log_probs = F.log_softmax(logits, dim = dim)\n        ctc_loss = F.ctc_loss(log_probs.permute(2, 0, 1), targets, input_lengths, target_lengths, blank, reduction = 'sum')\n        ctc_grad, = torch.autograd.grad(ctc_loss, (logits,))\n        temporal_mask = (torch.arange(log_probs.shape[-1], device = input_lengths.device, dtype = input_lengths.dtype).unsqueeze(0) < input_lengths.unsqueeze(1))[:, None, :]\n        alignment_targets = log_probs.exp() * temporal_mask - ctc_grad\n        return log_probs, ctc_loss, alignment_targets\n\nlog_probs, ctc_loss, alignment_targets = ctc_alignment_targets(logits, ...)\nctc_loss_via_mle = (-alignment_targets * log_probs).sum()\nprint(ctc_loss, ctc_loss_via_mle)\n<\/code>\nSomething it not quite right, the two quantities don\u2019t match. Maybe did sth wrong, but the autograd.grad approach worked (even without a custom autograd function, though not sure if it\u2019s same speed as the custom autograd function approach you suggested)","z":"There isn\u2019t a way to call ctc\u2019s backward independently of the forward (and not in general, either).\nThis limits you to not manipulating the inputs to the forward that are passed to the grad.\nYou can wrap the call to ctc loss into a custom autograd function (detaching + enable_grad + storing the result as intermediate and returning a detached) and then call that the backward on that during the backward of the autograd function.\nPersonally, I\u2019d think that it\u2019d be cool to expose all backwards in a standard way (and also get rid of sticking them in the Functions.cpp template), but that never gained traction with anyone else.\nBest regards\nThomas\nI\u2019ve tried  the following:\n<code class=\"lang-auto\">def ctc_alignment_targets(logits, targets, input_lengths, target_lengths, blank, dim = 1):\n    with torch.enable_grad():\n        logits = logits.detach().requires_grad_()\n        log_probs = F.log_softmax(logits, dim = dim)\n        ctc_loss = F.ctc_loss(log_probs.permute(2, 0, 1), targets, input_lengths, target_lengths, blank, reduction = 'sum')\n        ctc_grad, = torch.autograd.grad(ctc_loss, (logits,))\n        temporal_mask = (torch.arange(log_probs.shape[-1], device = input_lengths.device, dtype = input_lengths.dtype).unsqueeze(0) < input_lengths.unsqueeze(1))[:, None, :]\n        alignment_targets = log_probs.exp() * temporal_mask - ctc_grad\n        return log_probs, ctc_loss, alignment_targets\n\nlog_probs, ctc_loss, alignment_targets = ctc_alignment_targets(logits, ...)\nctc_loss_via_mle = (-alignment_targets * log_probs).sum()\nprint(ctc_loss, ctc_loss_via_mle)\n<\/code>\nSomething it not quite right, the two quantities don\u2019t match. Maybe did sth wrong, but the autograd.grad approach worked (even without a custom autograd function, though not sure if it\u2019s same speed as the custom autograd function approach you suggested)\n If we figure this out, it\u2019s possible to do label smoothing \/ class weighting in CTC - rather cool  Gradient of NLL wrt softmax logits indeed seems to be like the paper claims, but for CTC grad wrt activations - needs checking. If you give it a look, let me know \nSo I didn\u2019t look at this in great detail, but from a cursory look, it seems that you need to change the formulae inside the reductions, which would mean that you would need custom kernels.\nThat said, I think you can mostly copy-paste  the native CTC code in your own module (in part because I never got around to switching the pointwise part to TensorIterator which would it make more efficient).\nBest regards\nThomas\nI was thinking of a trick. If existing CTC gradient (wrt logits if the paper is correct) is of form log_probs.exp() - somequantity, then we can retrieve somequantity (by undoing the subtraction) and use it in NLL loss. Like that it should be possible to sidestep modifying the original CTC. But something is not quite right, my code snippet does not produce the same loss value via NLL.\nI did a test, and the snippet above actually works! modulo this bug: https:\/\/github.com\/pytorch\/pytorch\/issues\/31557\nThis is cool and opens a way towards easy experimentation of CTC modifications! It would be cool if there was a switch of ctc_loss so that we don\u2019t have to do lot_probs.exp() (also not sure if it\u2019s best to do F.log_softmax(...).exp() or do F.softmax(...) again)\nMy test and impl: https:\/\/gist.github.com\/vadimkantorov\/73e1915178f444b64f9ef01a1e96c1e4\nAlso, what would be your thoughts about factoring CTC Viterbi path aglorithm out of existing CTC implementation? Should that be doable? Or easier to recode it from scratch?"},{"x":"Hi, everyone!\nI am writing a neural classifier and its output is two classes, with a batch size of 5, so output is a tensor of size (5, 2).\nAlso, I am using BCEWithLogitsLoss as the loss function.\nAs you know, BCEWithLogitsLoss accepts a vector of integers (one for each element in the batch) and I have a one-hot vector of two elements as the output of my network.\nIn order to convert from one-hot to scalar class index, I am using the max method, taking the indices, but, when I do so, I receive the following error at the .backprop method:\nRuntimeError: element 0 of tensors does not require grad and does not have a grad_fn\nPlease, is there a different way in Pytorch to convert from one-hot to scalar class index?\nHere goes the relevant part of the code:\n<code class=\"lang-auto\">class ChoamNet(nn.Module):\n\n    def __init__(self):\n        super(ChoamNet, self).__init__()\n\n        # Model Architecture.\n        self.adapt_in = nn.Conv1d(in_channels=21, out_channels=256, kernel_size=1)\n        self.act_in = nn.PReLU(256)\n        \n        self.block01_01 = ResNextBlock(256, 32, 256, 32)\n        self.block01_02 = ResNextBlock(256, 32, 256, 32)\n        self.block01_03 = ResNextBlock(256, 32, 256, 32)\n        self.block01_04 = ResNextBlock(256, 32, 256, 32)\n        \n        self.adapt_01_02 = nn.Conv1d(in_channels=256, out_channels=512, kernel_size=1)\n        self.act_01_02 = nn.PReLU(512)\n        \n        self.block02_01 = ResNextBlock(512, 64, 512, 32)\n        self.block02_02 = ResNextBlock(512, 64, 512, 32)\n        self.block02_03 = ResNextBlock(512, 64, 512, 32)\n        self.block02_04 = ResNextBlock(512, 64, 512, 32)\n        \n        self.adapt_02_03 = nn.Conv1d(in_channels=512, out_channels=1024, kernel_size=1)\n        self.act_02_03 = nn.PReLU(1024)\n        \n        self.block03_01 = ResNextBlock(1024, 128, 1024, 64)\n        self.block03_02 = ResNextBlock(1024, 128, 1024, 64)\n        self.block03_03 = ResNextBlock(1024, 128, 1024, 64)\n        self.block03_04 = ResNextBlock(1024, 128, 1024, 64)\n        self.block03_05 = ResNextBlock(1024, 128, 1024, 64)\n        self.block03_06 = ResNextBlock(1024, 128, 1024, 64)\n        self.block03_07 = ResNextBlock(1024, 128, 1024, 64)\n        self.block03_08 = ResNextBlock(1024, 128, 1024, 64)\n        self.block03_09 = ResNextBlock(1024, 128, 1024, 64)\n        self.block03_10 = ResNextBlock(1024, 128, 1024, 64)\n        self.block03_11 = ResNextBlock(1024, 128, 1024, 64)\n        \n        self.fc1 = nn.Linear(14 * 1024, 2)\n        \n    def forward(self, x):\n\n        x = self.adapt_in(x)\n        x = self.act_in(x)\n        \n        x = self.block01_01(x)\n        x = self.block01_02(x)\n        x = self.block01_03(x)\n        x = self.block01_04(x)\n        \n        x = self.adapt_01_02(x)\n        x = self.act_01_02(x)\n\n        x = self.block02_01(x)\n        x = self.block02_02(x)\n        x = self.block02_03(x)\n        x = self.block02_04(x)\n        \n        x = self.adapt_02_03(x)\n        x = self.act_02_03(x)\n\n        x = self.block03_01(x)\n        x = self.block03_02(x)\n        x = self.block03_03(x)\n        x = self.block03_04(x)\n        x = self.block03_05(x)\n        x = self.block03_06(x)\n        x = self.block03_07(x)\n        x = self.block03_08(x)\n        x = self.block03_09(x)\n        x = self.block03_10(x)\n        x = self.block03_11(x)\n        \n        x = x.view(-1, 14 * 1024)\n        \n        x = F.softmax(self.fc1(x), 1)\n        \n        return x\n<\/code>\n<code class=\"lang-auto\">def perform_training(model, trn_data_i, trn_data_o, tst_data_i, tst_data_o):\n    \n    model.apply(init_weights)\n    \n    #loss = nn.CrossEntropyLoss()\n    loss = nn.BCEWithLogitsLoss()\n    optim = opt.Adam(model.parameters(), lr=1e-3)\n    \n    tmp_trn_loss = []\n    tmp_trn_acc = []\n    \n    tmp_tst_loss = []\n    tmp_tst_acc = []\n    \n    tmp_duration = []\n    \n    tmp_epoch = []\n    found = False\n    fnum = 1\n    file = r'D:\\Project CHOAM\\Results\\Epoch 001.mdl'\n    found_file = file\n    while os.path.exists(file) and os.path.isfile(file):\n        found_file = r'D:\\Project CHOAM\\Results\\Epoch ' + \"{:03d}\".format(fnum) + '.mdl'\n        fnum += 1\n        file = r'D:\\Project CHOAM\\Results\\Epoch ' + \"{:03d}\".format(fnum) + '.mdl'\n        found = True\n        \n    if found:\n        stats = read_csv(r'D:\\Project CHOAM\\Results\\Progress.csv')\n        \n        tmp_trn_loss = list(stats['Training Loss'])\n        tmp_trn_acc = list(stats['Training Precision'])\n        \n        tmp_tst_loss = list(stats['Test Loss'])\n        tmp_tst_acc = list(stats['Test Precision'])\n        \n        tmp_duration = list(stats['Duration'])\n        \n        tmp_epoch = list(stats['Epoch'])\n        \n        model.load_state_dict(torch.load(found_file))\n        model.train()\n    \n    for e in list(range(10000000))[fnum:]:\n        y_true = []\n        y_pred = []\n\n        start = datetime.now()\n        \n        print('Performing training epoch number ' + str(e))\n        \n        stat_trn_loss = 0\n        stat_trn_corr = 0\n        stat_trn_tot = 0\n        \n        stat_tst_loss = 0\n        stat_tst_corr = 0\n        stat_tst_tot = 0\n        \n        # Run the training minibatches.\n        for b in track(range(len(trn_data_i))):\n            model.apply(check_nan)\n            \n            batch_i = trn_data_i[b]\n            batch_o = trn_data_o[b]\n\n            model.zero_grad()\n\n            trn_out = model(batch_i)\n            outputs = trn_out.max(1).indices.float()\n\n            trn_loss = loss(outputs, batch_o.float())\n\n            # Backpropagate errors.\n            trn_loss.backward()\n            optim.step()\n\n            # Calculate training statistics.\n            with torch.no_grad():\n                stat_trn_loss += trn_loss \/ batch_o.size()[0]\n\n                stat_trn_corr += float((trn_out.max(1).indices == batch_o).sum())\n                stat_trn_tot += float(batch_o.size()[0])\n\n                for p in trn_out.max(1).indices:\n                    y_pred.append(p.item())\n\n                for r in batch_o:\n                    y_true.append(r.item())\n\n        # Run the test minibatches.\n        with torch.no_grad():\n            model.eval()\n\n            # Please, don't bother with this part... I will use the same strategy that works above.\n            for b in track(range(len(tst_data_i))):\n                batch_i = tst_data_i[b]\n                batch_o = tst_data_o[b]\n\n                tst_out = model(batch_i)\n\n                #output = tst_out\n                output = tst_out.max(2).indices\n                labels = batch_o\n\n                tst_loss = loss(output, labels)\n\n                # Calculate test statistics.\n                with torch.no_grad():\n                    stat_tst_loss += tst_loss \/ batch_o.size()[0]\n\n                    stat_tst_corr += float((tst_out.max(1).indices == batch_o).sum())\n                    stat_tst_tot += float(batch_o.size()[0])\n\n            model.train()\n\n        # Calculate epoch duration.\n        end = datetime.now()\n        duration = end-start\n\n        with torch.no_grad():\n            tmp_trn_loss.append(stat_trn_loss.item())\n            tmp_trn_acc.append((float(stat_trn_corr) \/ float(stat_trn_tot)))\n\n            tmp_tst_loss.append(stat_tst_loss.item())\n            tmp_tst_acc.append((float(stat_tst_corr) \/ float(stat_tst_tot)))\n            print((stat_tst_corr, stat_tst_tot))\n\n            dur = str(duration)\n            dur = dur[0:dur.index('.')]\n            tmp_duration.append(dur)\n\n            tmp_epoch.append(e)\n\n            df = DataFrame()\n\n            df['Training Loss'] = tmp_trn_loss\n            df['Training Precision'] = tmp_trn_acc\n\n            df['Test Loss'] = tmp_tst_loss\n            df['Test Precision'] = tmp_tst_acc\n\n            df['Duration'] = tmp_duration\n\n            df['Epoch'] = tmp_epoch\n\n            df.to_csv(r'D:\\Project CHOAM\\Results\\Progress.csv')\n\n            torch.save(model.state_dict(), r'D:\\Project CHOAM\\Results\\Epoch ' + \"{:03d}\".format(e) + '.mdl')\n\n            clear_output()\n\n            conf = confusion_matrix(y_true, y_pred)\n\n            fig, ax = plt.subplots()\n            heatmap(conf, annot=True, cbar=True, ax=ax)\n            plt.ylabel('True')\n            plt.xlabel('Predicted')\n            plt.title('Training')\n            plt.show()\n\n<\/code>\nThanks in advance!","y":"\n\n\n Julio_Marco_A_Silva:\n\nAs you know, BCEWithLogitsLoss accepts a vector of integers (one for each element in the batch) and I have a one-hot vector of two elements as the output of my network.\n\n\nThat\u2019s not correct, as nn.BCEWithLogitsLoss expects raw logits as the model output (FloatTensors) and a target tensor with the same shape and type as the output containing values in [0, 1].\nI\u2019m a bit confused about your use case at the moment.\nIf you are dealing with a multi-label classification (each sample can belong to more than a single class), remove the softmax in your model and just pass the logits to the criterion without any max operations.\nOn the other hand, if you are dealing with a multi-class classification (each sample belongs to one class only), still remove the softmax, use nn.CrossEntropyLoss as the criterion, and pass the targets as class indices (e.g. by using torch.argmax(target).","z":"\n\n\n Julio_Marco_A_Silva:\n\nAs you know, BCEWithLogitsLoss accepts a vector of integers (one for each element in the batch) and I have a one-hot vector of two elements as the output of my network.\n\n\nThat\u2019s not correct, as nn.BCEWithLogitsLoss expects raw logits as the model output (FloatTensors) and a target tensor with the same shape and type as the output containing values in [0, 1].\nI\u2019m a bit confused about your use case at the moment.\nIf you are dealing with a multi-label classification (each sample can belong to more than a single class), remove the softmax in your model and just pass the logits to the criterion without any max operations.\nOn the other hand, if you are dealing with a multi-class classification (each sample belongs to one class only), still remove the softmax, use nn.CrossEntropyLoss as the criterion, and pass the targets as class indices (e.g. by using torch.argmax(target).\nThank you very much! That solved it!\nThe code I posted was a mess, due to several failed attempts at correction, but you gave me the answer in the \u201ctorch.argmax\u201d function.\nCheers!"},{"x":"Hello,\nI am struggling with implementing reversible residual network.\nWhen I implement this, the computational graph is consisted of some nodes, and this nodes are constructed using Tensor.\nThe strength of reversible network is that it can construct earlier activation with current activation. However, the computational graph use Tensor as nodes, which makes them always store the earlier activation.\nI want to use gpu memory as efficient as I could. So, I want to retain the structure of graph while the tensor data is freed. Can I do this? If I can, how to do this?\nOne possible solution is to delete tensor.data. However, I worried that it makes unexpected result.\nThanks in advance.","y":"Hi,\nIf you don\u2019t want any Tensor to be saved, just don\u2019t save anything in the ctx and no Tensor will be saved for that Function.\nEvery Tensor is deleted as soon as it is not referenced by anything. So the input data will be deleted as soon as you don\u2019t use it in your forward function.","z":"I found out that deleting tensor.data is impossible.\nMy test code:\n<code class=\"lang-auto\">a = torch.zeros([32, 64], requires_grad=True).cuda()\nb = torch.zeros([64, 128], requires_grad=True).cuda()\nc = torch.zeros([128, 256], requires_grad=True).cuda()\nprint(\"Calculate result four times: \", torch.cuda.memory_allocated())\nd = torch.matmul(a, b)\ne = torch.matmul(d, c)\nprint(\"Calculate result four times: \", torch.cuda.memory_allocated())\ndel d.data\nprint(\"Calculate result four times: \", torch.cuda.memory_allocated())\ne.mean().backward()\nprint(\"Calculate result four times: \", torch.cuda.memory_allocated())\nprint(a.grad)\nprint(b.grad)\n<\/code>\nHi,\n.data is not a thing anymore. It has been removed from the doc and will be deleted.\nThe graph is actually composed of Node (the function that needs to run in the backward) and not Tensor. Only the Tensors required to compute the backward are saved by the Nodes that needs them.\nCan I ask how to deal with this situation??\n\nI construct a computational graph with tensor.\nI don\u2019t want to hold data, i.e. activation, in the intermediate nodes.\nWhen I call backward, I want to calculate the gradient of intermediate nodes with my custom backward.\n\nClearly, the first and third requirements can be accomplished by using normal tensor operation and the custom autograd.Function, respectively.\nThe main concern is how to remove only data, not the computation node itself.\nIs there any suggestion about implementing this?? I think I might to hack ctx in the autograd.Function, but there is lack of documentation\u2026 \nSome code template I want is to this.\n<code class=\"lang-auto\">class CustomFunction(autograd.Function):\n  def forward(ctx, x):\n    output = f(input)\n    # TODO: remove input data or deallocate gpu memory for input data. but how?\n    return output\n\n  def backward(ctx, grad_output):\n    return torch.ones_like(grad_output)\n<\/code>\nHi,\nIf you don\u2019t want any Tensor to be saved, just don\u2019t save anything in the ctx and no Tensor will be saved for that Function.\nEvery Tensor is deleted as soon as it is not referenced by anything. So the input data will be deleted as soon as you don\u2019t use it in your forward function.\nThanks for the answer. \nI tested it by my custom code.\n<code class=\"lang-python\">class CustomMM(autograd.Function):\n  def forward(ctx, x, y):\n    out = torch.mm(x, y)\n    return out\n\n  def backward(ctx, grad_out):\n    return magic_grad_for_x(grad_out), magic_grad_for_y(grad_out)\n\nprint(torch.cuda.memory_allocated())\nx = torch.rand(10000, 10, requires_grad=True).cuda()\ny = torch.rand(10, 10000).cuda()\nz = torch.rand(10000, 100).cuda()\nprint(torch.cuda.memory_allocated())\n\noutput = CustomMM.apply(CustomMM.apply(x, y), z)\nprint(torch.cuda.memory_allocated())\n\n# output.backward()  # It doesn't work because there is no magic function :)\n<\/code>"},{"x":"I understand that the tensors with requires_grad=False do not calculate gradients. I wonder if I can change the requires_grad flag during training so as to train\/freeze parts of network during training. I want to train some parameters if some conditions are met and freeze them if conditions are not matched. An example code is like below. When I change this flag, the optimizer would reflect it automatically? or do I have to do something to let the optimizer know about it?\n....\noptimizer = optim.Adam(model.parameters(), lr = lr)\n....\nfor epoch in range(total_epochs):\n    .... training goes on ....\n\n    if certain_condition:\n        certain_parameter.requires_grad=False \n        # 'certain_parameter' is model parameter in model.parameters(). \n    else:\n        certain_parameter.requires_grad=True\n\n    .... training goes on ....","y":"\n\n\n albanD:\n\nThis will work to freeze the gradients to 0.\nBe careful though that many optimizer will still update the weights even for a gradient of 0! If you have regularization or momentum for example.\n\n\n, I add the same issue and when setting param.grad = None it force the optimizer to skip this param update at step. Thus, also optimizers with momentum will keep those layers params fixed.","z":"Hi,\nThis will work to freeze the gradients to 0.\nBe careful though that many optimizer will still update the weights even for a gradient of 0! If you have regularization or momentum for example.\nthanks! I had the same question.\nwhat do you suggest in case one wants to completely freeze the parameters?\nThe best way is not to give them to the optimizer in the first place I think.\nOr if you want to update them sometimes, have two optimizers, one that you step() at every iteration and one that you step() only when you want to update these special elements.\nHowever, Is there an easier way When encountering the following situation?\nmodel has three parameters (parameter_1, parameter_2  and parameter_3),\nfirst, model need update all parameters.\nDuring the training process, parameter_1 and parameter_2 will be updated sometimes, sometimes parameter_1 and parameter_3.\nSo, we need three optimizers:\noptimizer_1\u2014>parameter_1, parameter_2  and parameter_3\noptimizer_2\u2014>parameter_1 and parameter_2\noptimizer_3\u2014>parameter_1 and parameter_3\nBut how to deal with when there are more parameters ?\nI hope my expression is clear.\nHi,\nYou don\u2019t want to have parameters shared in multiple optimizers as internal states kept for example by Adam or momentum will be wrong.\nYou can create one optimizer for each set of paremeters and .step() all the ones that need to be updated at that iterations.\n\n\n\n albanD:\n\nYou can create one optimizer for each set of paremeters and .step() all the ones that need to be updated at that iterations.\n\n\nI know your means.\nHowever,\nthe number of optimizers will increase when the set of paremeters is too large, which will cause some complexity.\n\n\n\n albanD:\n\nThis will work to freeze the gradients to 0.\nBe careful though that many optimizer will still update the weights even for a gradient of 0! If you have regularization or momentum for example.\n\n\n, I add the same issue and when setting param.grad = None it force the optimizer to skip this param update at step. Thus, also optimizers with momentum will keep those layers params fixed."},{"x":"Hi, when I start to train a simple version StyleGAN2 in PyTorch 1.0.1.\nSince I do not want to use retain_graph = True, A Runtime Error Trying to backward through the graph a second time, but the buffers have already been freed. Specify retain_graph=True when calling backward the first time. raised here.\nThe code of mine is below, really appreciate for your help, thank you!\n<code class=\"lang-python\">...\n# G &amp; D definition.\nG = G_stylegan2(fmap_base=opts.fmap_base,\n                    resolution=opts.resolution,\n                    mapping_layers=opts.mapping_layers,\n                    opts=opts,\n                    return_dlatents=opts.return_latents)\nD = D_stylegan2(fmap_base=opts.fmap_base,\n                    resolution=opts.resolution,\n                    structure='resnet')\n...\nLoss_D_list = [0.0]\nLoss_G_list = [0.0]\nsoftplus = torch.nn.Softplus()\n...\n# Training.\nfor epoch in range(0, 100):\n     bar = tqdm(loader)\n     for i, (real_img,) in enumerate(bar):\n\n            real_img = real_img.to(opts.device)\n            latents = torch.randn([real_img.size(0), 512]).to(opts.device)\n\n            # =======================================================================================================\n            #   (1) Update D network: D_logistic_r1(default)\n            # =======================================================================================================\n            # Compute adversarial loss toward discriminator\n            real_logit = D(real_img)\n            fake_img = G(latents)\n            fake_logit = D(fake_img.detach())\n\n            d_loss = softplus(fake_logit).mean()\n            d_loss = d_loss + softplus(-real_logit).mean()\n\n            # original\n            r1_penalty = D_logistic_r1(real_img.detach(), D)\n            d_loss = d_loss + r1_penalty\n            # lite\n            # d_loss = d_loss.mean()\n\n            loss_D_list.append(d_loss.item())\n\n            # Update discriminator\n            optim_D.zero_grad()\n            d_loss.backward()\n            optim_D.step()\n\n\n            # =======================================================================================================\n            #   (2) Update G network: G_logistic_ns_pathreg(default)\n            # =======================================================================================================\n            if i % 3 == 0:\n                G.zero_grad()\n                fake_scores_out = D(fake_img)\n                g_loss = softplus(-fake_scores_out).mean()\n\n                loss_G_list.append(g_loss.item())\n\n                # Update generator\n                # Note: g_loss.backward(retain_graph=True) is ok, but this version is not I want!\n                g_loss.backward()\n                optim_G.step()\n<\/code>","y":"It seems one BiasAdd module is reused somewhere in the code, but I cannot spot the line.\nThis should be unrelated to your current issue, but I would recommend to setup the bias tensor and wrap it into nn.Parameter as the final step to create a leaf variable.\nCurrently you are creating a non-leaf variable by calling .to() on the parameter.","z":"I\u2019m not sure why you are seeing this error. I tried to reproduce it using your code snippet (and removing some steps) which works fine:\n<code class=\"lang-python\">D = nn.Linear(1, 1)\nG = nn.Linear(1, 1)\n\noptim_D = torch.optim.SGD(D.parameters(), lr=1e-3)\noptim_G = torch.optim.SGD(G.parameters(), lr=1e-3)\n\n\nreal_img = torch.randn(1, 1)\nlatents = torch.randn(1, 1)\n# =======================================================================================================\n#   (1) Update D network: D_logistic_r1(default)\n# =======================================================================================================\n# Compute adversarial loss toward discriminator\nreal_logit = D(real_img)\nfake_img = G(latents)\nfake_logit = D(fake_img.detach())\n\nd_loss = fake_logit.mean()\nd_loss = d_loss + real_logit.mean()\n\n# Update discriminator\noptim_D.zero_grad()\nd_loss.backward()\noptim_D.step()\n\n# =======================================================================================================\n#   (2) Update G network: G_logistic_ns_pathreg(default)\n# =======================================================================================================\nG.zero_grad()\nfake_scores_out = D(fake_img)\ng_loss = fake_scores_out.mean()\n\n# Update generator\n# Note: g_loss.backward(retain_graph=True) is ok, but this version is not I want!\ng_loss.backward()\noptim_G.step()\n<\/code>\nThat being said, the general training routing works fine.\nHowever, the penalty calculation might still produce this error (which isn\u2019t defined unfortunately, so I had to remove it for my code snippet).\nThank you very much. I think it may be some sneaky point in definition of my G? Because the error only occur in g_loss.backward()?\nMy Generator is like this (re-implement StyleGAN2 in PyTorch 1.x):\n<code class=\"lang-python\">class G_stylegan2(nn.Module):\n    def __init__(self,\n                 opts,\n                 return_dlatents=True,\n                 fmap_base=8 << 10,  # stylegan1 8192 (8 << 10), stylegan2 16384 (16 << 10)\n                 num_channels=3,  # Number of output color channels.\n                 mapping_fmaps=512,\n                 dlatent_size=512,  # Disentangled latent (W) dimensionality.\n                 resolution=1024,  # Output resolution.\n                 mapping_layers=8,  # Number of mapping layers.\n                 randomize_noise=True,\n                 fmap_decay=1.0,  # log2 feature map reduction when doubling the resolution.\n                 fmap_min=1,  # Minimum number of feature maps in any layer.\n                 fmap_max=512,  # Maximum number of feature maps in any layer.\n                 architecture='skip',  # Architecture: 'orig', 'skip'.\n                 act='lrelu',  # Activation function: 'linear', 'lrelu'.\n                 lrmul=0.01,  # Learning rate multiplier for the mapping layers.\n                 gain=1,  # original gain in tensorflow.\n                 truncation_psi = 0.7,  # Style strength multiplier for the truncation trick. None = disable.\n                 truncation_cutoff = 8,  # Number of layers for which to apply the truncation trick. None = disable.\n                 ):\n        super().__init__()\n        assert architecture in ['orig', 'skip']\n\n        self.return_dlatents = return_dlatents\n        self.num_channels = num_channels\n\n        self.g_mapping = G_mapping(mapping_fmaps=mapping_fmaps,\n                                   dlatent_size=dlatent_size,\n                                   resolution=resolution,\n                                   mapping_layers=mapping_layers,\n                                   lrmul=lrmul,\n                                   gain=gain)\n\n        self.g_synthesis = G_synthesis_stylegan2(resolution=resolution,\n                                                 architecture=architecture,\n                                                 randomize_noise=randomize_noise,\n                                                 fmap_base=fmap_base,\n                                                 fmap_min=fmap_min,\n                                                 fmap_max=fmap_max,\n                                                 fmap_decay=fmap_decay,\n                                                 act=act,\n                                                 opts=opts)\n\n        self.truncation_cutoff = truncation_cutoff\n        self.truncation_psi = truncation_psi\n\n    def forward(self, x):\n        dlatents1 = self.g_mapping(x)\n        out = self.g_synthesis(dlatents1)\n\n        if self.return_dlatents:\n            return out, dlatents1\n        else:\n            return out\n<\/code>\nIt might be related to the generator.\nDo you still see this issue, if you remove the penalty calculation?\nAlso, would it be possible to post the complete generator code or is it still unpublished work in progress?\nIn that case, could you try to come up with a small code snippet to reproduce this issue?\nThanks for you kindness !  I certainly like to post my code(a little big somehow)! I remove the penalty calculation and it\u2019s still the same error occurs.\nThe definition of the Generator is in stylegan2.py, it has two main class: G_mapping and G_synthesis_stylegan2. Thank you very much and if you are interest in fullfill this, it would be great!\n\n\ngithub.com with link \"https:\/\/github.com\/tomguluson92\/StyleGAN2_PyTorch\/blob\/master\/network\/stylegan2.py\"\n\n\ntomguluson92\/StyleGAN2_PyTorch\/blob\/master\/network\/stylegan2.py with link \"https:\/\/github.com\/tomguluson92\/StyleGAN2_PyTorch\/blob\/master\/network\/stylegan2.py\"\n<code class=\"lang-py\"># -*- coding: utf-8 -*-\n\n\"\"\"\n    StyleGAN2 pytorch\n\n    : samuel ko\n    :   2019.12.13\n\n    : 1) fused_conv: unsupport\n             2) 4x4 upfirdn kernel: (transfer to 3x3 upfirdn kernel)\n\n    :   2019.12.18\n\n    : 1) fix upfirdn2d [1, 3, 1] to original [1, 3, 3, 1] in D_stylegan2 and Upsample2d.\n             2) use_wscale = True (default), gain = 1 (default).\n             3) update he_std calculation method to coordinate with the original repo.\n\n\n    :   2019.12.20\n\n<\/code>\n\n  This file has been truncated. show original with link \"https:\/\/github.com\/tomguluson92\/StyleGAN2_PyTorch\/blob\/master\/network\/stylegan2.py\"\n\n\n\n\n\nHi, I found in BiasAdd module, if I do not wrap self.bias with nn.Parameter, then I do not need to worry about retain_graph, but if I wrap it with nn.Parameer, it obligatory requires me to add retain_graph=True.\nDo you know why? Thanks a lot!\n<code class=\"lang-auto\">class BiasAdd(nn.Module):\n\n    def __init__(self,\n                 channels,\n                 opts,\n                 act='linear', alpha=None, gain=None, lrmul=1):\n        \"\"\"\n            BiasAdd\n        \"\"\"\n        super(BiasAdd, self).__init__()\n\n        self.opts = opts\n        # fixme:\n        # self.bias = nn.Parameter(torch.zeros(channels, 1, 1) * lrmul).to(opts.device) # error!\n        self.bias = (torch.zeros(channels, 1, 1) * lrmul).to(opts.device) # ok\n\n        self.act = act\n        self.alpha = alpha if alpha is not None else 0.2\n        self.gain = gain if gain is not None else 1.0\n\n    def forward(self, x):\n        # Pass Add bias.\n        # x += self.bias (if self.bias wrap with nn.Parameter) ok!\n        x = x + self.bias\n\n        # Evaluate activation function.\n        if self.act == \"linear\":\n            pass\n        elif self.act == 'lrelu':\n            x = F.leaky_relu(x, self.alpha, inplace=True)\n            x = x * np.sqrt(2)  # original repo def_gain=np.sqrt(2).\n\n        # Scale by gain.\n        if self.gain != 1:\n            x = x * self.gain\n\n        return x\n<\/code>\nIt seems one BiasAdd module is reused somewhere in the code, but I cannot spot the line.\nThis should be unrelated to your current issue, but I would recommend to setup the bias tensor and wrap it into nn.Parameter as the final step to create a leaf variable.\nCurrently you are creating a non-leaf variable by calling .to() on the parameter.\nReally appreciate for your help!\nDid the change in creating the parameter solve the issue, since you\u2019ve marked the post as the solution?\nNow I let BiasAdd like below and it works well without retain_graph = True.\n<code class=\"lang-python\">class BiasAdd(nn.Module):\n\n    def __init__(self,\n                 channels,\n                 opts,\n                 act='linear', alpha=None, gain=None, lrmul=1):\n        \"\"\"\n            BiasAdd\n        \"\"\"\n        super(BiasAdd, self).__init__()\n\n        self.opts = opts\n        self.bias = torch.nn.Parameter((torch.zeros(channels, 1, 1) * lrmul))\n\n    def forward(self, x):\n       x += self.bias\n       ...\n       return x\n<\/code>"},{"x":"The forward function of my torch.nn module initially looked like this:\n<code class=\"lang-auto\">def forward(self, a, b, c):\n   #blah blah blah\n   inds = (c<0)\n   c_aug = c\n   c_aug[inds] = a.shape[0]\n   #more blah blah blah\n<\/code>\nThis was giving me an error:\nRuntimeError: one of the variables needed for gradient computation has been modified by an inplace operation: [torch.LongTensor [82, 4]] is at version 2; expected version 1 instead.\nAfter some debugging using anomaly detection and looking at this with link \"https:\/\/discuss.pytorch.org\/t\/setting-part-of-the-training-value-into-zero\/52044\" question, I changed the forward function as follows:\n<code class=\"lang-auto\">def forward(self, a, b, c):\n   #blah blah blah\n   inds = (c<0)\n   c_aug = torch.clone(c)\n   c_aug[inds] = a.shape[0]\n   #more blah blah blah\n<\/code>\nThis does not give me that error. Is it the correct workaround for the error or there are better methods to handle it?","y":"Hi,\nYes this is the correct workaround. You just want to make sure that you don\u2019t modify the c you get as input inplace. So you clone it before changing it inplace.\nNote that we usually do c_aug = c.clone() instead of using torch.clone.","z":"Hi,\nYes this is the correct workaround. You just want to make sure that you don\u2019t modify the c you get as input inplace. So you clone it before changing it inplace.\nNote that we usually do c_aug = c.clone() instead of using torch.clone."},{"x":"I have a custom module, which I am adding after VGG-16 feature module\n<code class=\"lang-auto\">class Last_layers_mod(nn.Module):\n  def __init__(self):\n    super(Last_layers_mod, self).__init__()  \n    self.gap = nn.AdaptiveAvgPool2d((1,1))\n\n  def forward(self, x):\n    if self.train:\n      x.register_hook(lambda x: print(\"gap reg_hook: \", x.size()))\n    x = self.gap(x)\n    \n    x = x.view(-1, self.num_flat_features(x))\n    return x\n\n  def num_flat_features(self, x):\n        size = x.size()[1:]  # all dimensions except the batch dimension\n        num_features = 1\n        for s in size:\n            num_features *= s\n        return num_features\n<\/code>\nThe model works perfectly during training but as soon as the first epoch ends and the model is in model.eval() phase, it throws\n\nRuntimeError: cannot register a hook on a tensor that doesn\u2019t require gradient\n\nHow to fix this issue?","y":"You can replace if self.train: by if self.train and x.requires_grad:","z":"You can replace if self.train: by if self.train and x.requires_grad:"},{"x":"Hi, I\u2019m trying to define an object inside the autograd.Function, but when i\u2019m calling the object inside the backward, it\u2019s saying the object is not defined\u2026\ne.g.\n<code class=\"lang-auto\">class PermutationLayer(autograd.Function):\n    \n    def forward(ctx, x, x1, x2):\n        ctx.save_for_backward(x, x1, x2)\n    \n    \n    def backward(ctx, grad_output, ...):\n        x, x1, x2 = ctx.saved_tensors\n\n        grad_x1 = want_to_add(x1)\n\n        return None, grad_x, grad_x1, grad_x2\n\n    def want_to_add(ctx, x_in):\n        return...\n<\/code>\nCould someone tell me the reason and a way to implement an intended def object to be used inside the autograd.Function?\nThank you for reading!","y":"Hi,\nYou want to double check how python classes work.\nBut because we use static method, we don\u2019t create instances of this class.\nTo use this function, you can either define it outside of the class. Or as another static method and call it with PermutationLayer. want_to_add().","z":"Hi,\nYou want to double check how python classes work.\nBut because we use static method, we don\u2019t create instances of this class.\nTo use this function, you can either define it outside of the class. Or as another static method and call it with PermutationLayer. want_to_add()."},{"x":"Say I have a 2D signal which is basically 4 channels of 1d signals and each 1d signal is of shape 100. So my input is of shape 100x4. Now I have a single kernel, torch.nn.Conv1d() and I want to apply the same kernel to each of the channels individually. What is the most efficient way to do this?\nThe method I have come up is to use list, but I feel there should be more elegant way to do the same. My code is\n<code class=\"lang-auto\">out_list = []\nfor i in channels:\n    # inp is of shape batch_size x channels (4) x vector len (100)\n   out = conv(inp[:, [i], :])\n    out_list.append(out)\no1 = torch.cat(out_list,0)\no1 = o1.view(num_channels, batch_size, vector_len)\no1 = o1.permute(1,0,2).contiguous()\nreturn o1\n<\/code>\nWould be glad if there is a better implementation for the same (or if there is any bug in the presented code). Thank you","y":"You can initialize it with the same weights for each input channel. The vanilla weight updates will change the weights without the constraint to have the same values.\nIf you need to backpropagate, the better method would be to change the view of the input channels, so that the channels are stored in the batch dimension, and you only use a single kernel.\nThis should work:\n<code class=\"lang-auto\">x = Variable(torch.randn(batch_size, in_channels, 100))\n\n# Gaussian kernel\nkernel = Variable(torch.FloatTensor([[[0.06136, 0.24477, 0.38774, 0.24477, 0.06136]]]))\noutput = F.conv1d(x.view(-1, 1, 100), kernel).view(batch_size, in_channels, 96)\n<\/code>","z":"You could repeat the single kernel and use the functional API:\n<code class=\"lang-auto\">x = Variable(torch.randn(1, in_channels, 100))\n\n# Gaussian kernel\nkernel = Variable(torch.FloatTensor([[[0.06136, 0.24477, 0.38774, 0.24477, 0.06136]]]))\nweights = kernel.repeat(1, in_channels, 1)\noutput = F.conv1d(x, weights)\n<\/code>\nWould this work for your use case?\nWill this also work if I want to learn the kernels as well? That is during the backward() method and updating weights with the optimizer, will this ensure that the kernels weights get updated by the average of the gradients in all the channels?\nYou can initialize it with the same weights for each input channel. The vanilla weight updates will change the weights without the constraint to have the same values.\nIf you need to backpropagate, the better method would be to change the view of the input channels, so that the channels are stored in the batch dimension, and you only use a single kernel.\nThis should work:\n<code class=\"lang-auto\">x = Variable(torch.randn(batch_size, in_channels, 100))\n\n# Gaussian kernel\nkernel = Variable(torch.FloatTensor([[[0.06136, 0.24477, 0.38774, 0.24477, 0.06136]]]))\noutput = F.conv1d(x.view(-1, 1, 100), kernel).view(batch_size, in_channels, 96)\n<\/code>\nSweet. This should. work."},{"x":"Hi All,\nI have what I hope to be a simple question - when mu and variance are calculated in the batchnorm layer, are the gradients propagated to the scaling?  I.e., are the mu and var in y = (x - mu) \/ sqrt(var + eps) simple numbers or the gradient tracked tensors?\nI\u2019m asking because I want to implement a modified version of batchnorm using the variance of the dimension before dropout is applied.  I need to know if I need to call .detach() or not on mu and var.\nThanks!\nAndy","y":"I wasn\u2019t sure, but based on this small code snippet, it seems the latter approach is used:\n<code class=\"lang-python\">\n# Manual without detach\ntorch.manual_seed(2809)\nx = torch.randn(10, 3, 4, 4, requires_grad=True)\nmean = x.mean(dim=[0, 2, 3], keepdim=True)\ninvstd = torch.sqrt(x.var([0, 2, 3], unbiased=False, keepdim=True) + 1e-5)\ny = (x - mean) \/ invstd \ny.abs().sum().backward()\nprint(x.grad)\nx1_grad = x.grad.clone()\ny1 = y.clone()\n\n# Manual with detach\ntorch.manual_seed(2809)\nx = torch.randn(10, 3, 4, 4, requires_grad=True)\nmean = x.mean(dim=[0, 2, 3], keepdim=True).detach()\ninvstd = torch.sqrt(x.var([0, 2, 3], unbiased=False, keepdim=True).detach() + 1e-5)\ny = (x - mean) \/ invstd\ny.abs().sum().backward()\nprint(x.grad)\nx2_grad = x.grad.clone()\ny2 = y.clone()\n\n# BN\ntorch.manual_seed(2809)\nx = torch.randn(10, 3, 4, 4, requires_grad=True)\nbn = nn.BatchNorm2d(3, affine=False)\ny = bn(x)\ny.abs().sum().backward()\nprint(x.grad)\nx3_grad = x.grad.clone()\ny3 = y.clone()\n\n# Compare\nprint((y3-y1).abs().max())\n> tensor(2.3842e-07, grad_fn=<MaxBackward1>)\nprint((y3-y2).abs().max())\n> tensor(2.3842e-07, grad_fn=<MaxBackward1>)\n\nprint((x3_grad - x1_grad).abs().max())\n> tensor(8.3447e-07)\nprint((x3_grad - x2_grad).abs().max())\n> tensor(2.6167)\n<\/code>","z":"The running stats are registered as buffers and do not require gradients, so you shouldn\u2019t need to call .detach() on them.\nDoes batch norm in training mode normalize to the current batch statistics, or to the current running stats?  I assume the former because I have a case were batch statistics are actually tied to a bag of instanced so they aren\u2019t comparable across bags (so I\u2019m using running stats = False).\nYes, during training the batch statistics will be used to normalize the current input.\nThanks for the responses so far.\nHopefully this makes my initial question clear:  Say you just want to normalize to 0 mean.  The operation is y = (x - mu).\nIs the \u2018batchnorm equivalent\u2019 approach like:\n<code class=\"lang-auto\">y = x - x.mean(dim=0).detach()\n<\/code>\nor\n<code class=\"lang-auto\">y = x - x.mean(dim=0)\n<\/code>\nWhile the value of y is the same, the gradient propagation is clearly different.\nI wasn\u2019t sure, but based on this small code snippet, it seems the latter approach is used:\n<code class=\"lang-python\">\n# Manual without detach\ntorch.manual_seed(2809)\nx = torch.randn(10, 3, 4, 4, requires_grad=True)\nmean = x.mean(dim=[0, 2, 3], keepdim=True)\ninvstd = torch.sqrt(x.var([0, 2, 3], unbiased=False, keepdim=True) + 1e-5)\ny = (x - mean) \/ invstd \ny.abs().sum().backward()\nprint(x.grad)\nx1_grad = x.grad.clone()\ny1 = y.clone()\n\n# Manual with detach\ntorch.manual_seed(2809)\nx = torch.randn(10, 3, 4, 4, requires_grad=True)\nmean = x.mean(dim=[0, 2, 3], keepdim=True).detach()\ninvstd = torch.sqrt(x.var([0, 2, 3], unbiased=False, keepdim=True).detach() + 1e-5)\ny = (x - mean) \/ invstd\ny.abs().sum().backward()\nprint(x.grad)\nx2_grad = x.grad.clone()\ny2 = y.clone()\n\n# BN\ntorch.manual_seed(2809)\nx = torch.randn(10, 3, 4, 4, requires_grad=True)\nbn = nn.BatchNorm2d(3, affine=False)\ny = bn(x)\ny.abs().sum().backward()\nprint(x.grad)\nx3_grad = x.grad.clone()\ny3 = y.clone()\n\n# Compare\nprint((y3-y1).abs().max())\n> tensor(2.3842e-07, grad_fn=<MaxBackward1>)\nprint((y3-y2).abs().max())\n> tensor(2.3842e-07, grad_fn=<MaxBackward1>)\n\nprint((x3_grad - x1_grad).abs().max())\n> tensor(8.3447e-07)\nprint((x3_grad - x2_grad).abs().max())\n> tensor(2.6167)\n<\/code>\nAwesome! This makes sense to me, and figured this was the case, and glad to know for sure.  Thank you so much for helping clear this up!"},{"x":"Hi,\nI am implementing custom autograd functions on Sparse Tensors. As I use COO format to encode sparse tensors, the input of my auto grad functions is a pair of tensors, one containing the indices of type torch(.cuda).IntTensor and one containing the values of type torch(.cuda).FloatTensor.\nThe indice tensor do not have gradient (None) but is used to compute the gradient with respect to the value tensor. It seems that there has been an update in PyTorch 1.3 that do not allow this type of autograd function anymore. Indeed, my custom functions used to work under PyTorch 1.2 but it now raise the following error:\n<code class=\"lang-auto\">RuntimeError: Expected isFloatingType(grads[i].type().scalarType()) to be true, but got false.  (Could this error message be improved?  If so, please report an enhancement request to PyTorch.)\n<\/code>\nHere is a small reproducible exemple:\n<code class=\"lang-auto\">import torch\nimport torch.nn \nimport torch.nn.functional as F\n\n\n\nclass test_discrete_grad_1(torch.autograd.Function):\n    \n    \n    def forward(ctx, ind, val):\n        size_table = ind.max(dim=1)[1]\n        ctx.ind = ind\n        new_ind = ind\n        new_ind[1] = (new_ind[1] + 5)%(size_table[1].item()) \n        new_val = val * 3\n        return new_ind, new_val\n\n    \n    def backward(ctx, grad_ind, grad_val):\n        ### grad_ind is supposed to be None\n        return None, 3 * grad_val\n\n\nclass test_discrete_grad_2(torch.autograd.Function):\n    \n    \n    def forward(ctx, ind, val):\n        ctx.ind = ind \n        res = torch.zeros((2,))\n        res[0] = 2 * val[(ind[1]%2)==0].sum()\n        res[1] = 5 * val[(ind[1]%2)==1].sum()\n        return res\n\n    \n    def backward(ctx, grad):\n        ind = ctx.ind\n        val = torch.zeros((ind.size(1),))\n        val[(ind[1]%2)==0] = 2 * grad[0]\n        val[(ind[1]%2)==1] = 5 * grad[1]\n        return None, val\n\n\ndef test():\n\n    ind = torch.randint(low=0, high=30, size=(3, 20))\n    val = torch.rand(20)\n    val.requires_grad = True\n    opp_1 = test_discrete_grad_1.apply\n    opp_2 = test_discrete_grad_2.apply\n    new_ind, new_val = opp_1(ind, val)\n    res = opp_2(new_ind, new_val)\n    print(res)\n    out = res.sum()\n    out.backward()\n    print(val.grad)\n\ntest()\n<\/code>\nThis script runs properly on PyTorch 1.2 and raises the previous error on Pytorch >1.3.\nIs there a way to solve this without casting the indice tensor to Float type? (in my application it would requires A LOT of casting operations).\nThank you in advance.\nSamuel","y":"Hi,\nThe wasn\u2019t very clear about this, but the latest version here with link \"https:\/\/pytorch.org\/docs\/master\/notes\/extending.html\" should be better. In particular, you have to mark non-differentiable outputs explicitly in your custom Functions.\nIn you case, you should add at the end of test_discrete_grad_1's forward function:\n<code class=\"lang-python\">ctx.mark_non_differentiable(new_ind)\n<\/code>","z":"Hi,\nThe wasn\u2019t very clear about this, but the latest version here with link \"https:\/\/pytorch.org\/docs\/master\/notes\/extending.html\" should be better. In particular, you have to mark non-differentiable outputs explicitly in your custom Functions.\nIn you case, you should add at the end of test_discrete_grad_1's forward function:\n<code class=\"lang-python\">ctx.mark_non_differentiable(new_ind)\n<\/code>\nAlright I will test with the latest version. Thank you for you answer!\nAdding this will fix your issue for all pytorch\u2019s versions.\nIt\u2019s only the latest doc version that explain the use of this function properly.\nOk got it. Thank you"},{"x":"Hi, I\u2019m trying to get a hessian vector product of a network.\nI try to follow the hvp implemented in Tensorflow. But the following codes don\u2019t work as expected. Does anybody know how to solve it?\nThank you in advance.\n<code class=\"lang-auto\">\ninput = torch.randn(1, 3, 32, 32)  # Batch_size is 1.\nout = net(input).sum()  # net is a neural network\n\npara_list = [x for x in net.parameters()]\n\ngrads = autograd.grad([out], para_list, retain_graph=True, create_graph=True)\n\nelem_prod = [g * v for g, v in zip(grads, list_of_v)]  # Here list_of_v is a list of vector v. Each v is corresponding to a parameter in para_list.\n\nhvps = autograd.grad(elem_prod, para_list, create_graph=True)\n\n<\/code>\nThe error says:\nRuntimeError: grad can be implicitly created only for scalar outputs.\nIf my implementation is totally wrong, what is the correct way of doing this?\nThanks again.","y":"Hi,\nI think the problem is that you do an element-wise product when you do g * v instead of a dot product.\nDoes elem_prod = sum([(g * v).sum() for g, v in zip(grads, list_of_v)]) compute what you want?","z":"Hi,\nI think the problem is that you do an element-wise product when you do g * v instead of a dot product.\nDoes elem_prod = sum([(g * v).sum() for g, v in zip(grads, list_of_v)]) compute what you want?"},{"x":"I have 2 train sets: one with label and one with no label.\nWhen training, i\u2019m simultaneously load  one batch from labelled set, calculate loss in 1 way; one batch from unlabeled set, calculate loss in other way. Finally I sum them (2 loss) and loss.backward() .\nIs this way ok ? it\u2019s quite uncommon in my mind so just ask if the engine truthfully know how to back-propagate  ?\nThank you.","y":"Semi-supervised learning is a relatively common technique to deal with situations in which labeled data isn\u2019t abundant and might look like this.\nPyTorch\u2019s autograd it is much the same if the loss has 1, 2, or 10 terms - losses of minibatches typically are just sums or means of the individual items under the hood, too.\nIf you have very separate computations, you could also forward with the first, backward, forward for the second and backward again. PyTorch will accumulate the gradients of the two (which is why you need to zero gradients), and it might be much easier on GPU memory \/ you can afford larger batch sizes.\nBest regards\nThomas","z":"Semi-supervised learning is a relatively common technique to deal with situations in which labeled data isn\u2019t abundant and might look like this.\nPyTorch\u2019s autograd it is much the same if the loss has 1, 2, or 10 terms - losses of minibatches typically are just sums or means of the individual items under the hood, too.\nIf you have very separate computations, you could also forward with the first, backward, forward for the second and backward again. PyTorch will accumulate the gradients of the two (which is why you need to zero gradients), and it might be much easier on GPU memory \/ you can afford larger batch sizes.\nBest regards\nThomas"},{"x":"When we define the forward process, do we need to setup autograd for any tensors, such as hidden\/cell state?\nFor example,\n    h0 = torch.zeros(self.layer_dim, x.size(0), self.hidden_dim).requires_grad_()\n    c0 = torch.zeros(self.layer_dim, x.size(0), self.hidden_dim).requires_grad_()\n\nIn some tutorials, I found out from the beginning to the end not even one torch is set as requires_grad.\nIs it because in nn.LSTM module it already handles this problem?","y":"Hi,\nThis is because only the Tensors for which you want the .grad field to be populated needs the user to call requires_grad_().\nWhen working with nn, this is done automatically for nn.Parameters.\nSo if you use nn.Parameters, you never need to call this explicitely.\nUnless you want extra gradients to be computed for things that are not nn.Parameters.","z":"Hi,\nThis is because only the Tensors for which you want the .grad field to be populated needs the user to call requires_grad_().\nWhen working with nn, this is done automatically for nn.Parameters.\nSo if you use nn.Parameters, you never need to call this explicitely.\nUnless you want extra gradients to be computed for things that are not nn.Parameters.\nThank you for your help!"},{"x":"Hi everyone,\nI would like to implement a module that I can perform some operations to intermediate layers such that:\n(1) requires_gradient = False for these operations a, b\n(2) requires_gradient = True for out\n(3) force PyTorch to build Autograd graph such that the order is: input -> out (conv1) -> out (relu1). It means that PyTorch won\u2019t need to take care of computing gradients for any function_a and function_b in the backward pass. The reason is: function_a and function_b were not provided by PyTorch, and it is very hard to compute their derivatives.\nI am not sure whether it is feasible or not. Do you have any ideas how to proceed?\nThank you so much for reading.\nA simple network can be seen as follows:\n<code class=\"lang-auto\">class CustomModule(nn.Module):\n    def __init__(self, module):\n        super(CustomModule, self).__init__()\n        self.module = module\n\n    def forward(self, input):\n        a = function_a(input)       # no need gradient in backward\n        out = self.module(input)    # need gradient in backward\n        b = function_b(out)         # no need gradient in backward\n        return b\n\n\nclass Net(nn.Module):\n    def __init__(self):\n        super(Net, self).__init__()\n        self.conv1 = CustomModule(nn.Conv3d(1, 100, kernel_size=1))\n        self.relu1 = CustomModule(nn.ReLU())\n\n    def forward(self, input):\n        conv1 = self.conv1(input)\n        relu1 = self.relu1(conv1)\n        return relu1\n<\/code>","y":"\nno if you chain them, then func1 will also need to be differentiable to get gradients for module1.\nYou can use the information in this part of the doc with link \"https:\/\/pytorch.org\/docs\/stable\/notes\/extending.html#extending-torch-autograd\" to create your own autograd Function. In your case, you can use func1 and func2 for the forwards and you will need to implement the backward for them.\n","z":"Hi,\nThe gradients are computed using backpropagation. So to get the gradients for a module, you need all use of its output to be done by differentiable functions.\nIf you do:\n<code class=\"lang-auto\">a = func1(input)\nb = module(a)\nc = func2(b)\nloss = crit(c)\n<\/code>\nAnd your learnable parameters are in module.\nYou will need func2 and crit to be differentiable. But not func1.\nHi ,\nThank you so much for your response.\nIs it true that if my learnable parameters are in module1 and module2, and I do:\n<code class=\"lang-auto\">a1 = func1(input)\nb1 = module1(a1)\nc1 = func2(b1)\n\na2 = func1(c1)\nb2 = module2(a2)\nc2 = func2(b2)\n\nloss = crit(c2)\n<\/code>\n(1) I will need only critand func2 to be differentiable? How does backward-pass work in the aforementioned scenario?\n(2) As I understand, my problem is a kind of disconnected graph since in the backward pass I just want to compute the gradients of loss -> b2 -> b1. Is there any ways to avoid Autograd and do the back-propagation manually?\nBest!\n\nno if you chain them, then func1 will also need to be differentiable to get gradients for module1.\nYou can use the information in this part of the doc with link \"https:\/\/pytorch.org\/docs\/stable\/notes\/extending.html#extending-torch-autograd\" to create your own autograd Function. In your case, you can use func1 and func2 for the forwards and you will need to implement the backward for them.\n\nThank you so much "},{"x":"Hi all,\nI met a problem with scatter_.\nHere is the code(it was posted here: Differentiable argmax with link \"https:\/\/discuss.pytorch.org\/t\/differentiable-argmax\/33020\/6\"):\n<code class=\"lang-auto\">class ArgMax(torch.autograd.Function):\n    \n    def forward(ctx, input):\n        idx = torch.argmax(input, 1)\n        output = torch.zeros_like(input)\n        output.scatter_(1, idx, 1)\n        return output\n\n    \n    def backward(ctx, grad_output):\n        return grad_output\n\nif __name__ == '__main__':\n    a = torch.rand((3, 2, 4, 2), requires_grad=True)\n    print(a)\n    argmax = ArgMax()\n    b = argmax.apply(a)\n    b.backward()\n<\/code>\nBut it seems scatter_ doesn\u2019t work here, and I got the error:\n<code class=\"lang-auto\">RuntimeError: invalid argument 3: Index tensor must either be empty or have same dimensions as output tensor at \/Users\/distiller\/project\/conda\/conda-bld\/pytorch_1570710797334\/work\/aten\/src\/TH\/generic\/THTensorEvenMoreMath.cpp:133\n<\/code>\nIs there anything wrong in the usage of scatter_ here?\nThanks in advance!","y":"\n\n\n xiaohan:\n\nidx = torch.argmax(input, 1)\n\n\nI think you want this to read\n<code class=\"lang-auto\">idx = torch.argmax(input, 1, keepdim=True)\n<\/code>\nBest regards\nThomas","z":"\n\n\n xiaohan:\n\nidx = torch.argmax(input, 1)\n\n\nI think you want this to read\n<code class=\"lang-auto\">idx = torch.argmax(input, 1, keepdim=True)\n<\/code>\nBest regards\nThomas\nThanks so much, Thomas! This fixed the problem."},{"x":"Hi,\nI custom a Function, in forward method. I have a numpy array x and a cuda tensor input.How to copy value of x to input tensor ? I have try, in forward method, torch.to(device) don\u2019t work.\nThanks\n<code class=\"lang-auto\">class MyFunction(torch.autograd.Function):\n\n    \n    def forward(ctx, input):\n        x = np.zeros(shape)\n        # do something for x\n        # and how to copy data of x to gpu tensor input\n        \n        return ...\n\n    \n    def backward(ctx, grad_output):\n          '''\n<\/code>","y":"At first you should check if CUDA devices are available.\nThen set the device variable with some value (e.g. 'cpu', 'cuda:0') and pass it to your_tensor.to() function.\nNote: set a constant string value for the device is not an only option (if you want use tensor.to() for transfering to device), you may pass there a device value of some other tensor","z":"Here is an answer, that might work for you.\nYou should create Tensor from NumPy array and then transfer it to device\n\n\n\n\nConverting numpy array to tensor on GPU with link \"https:\/\/discuss.pytorch.org\/t\/converting-numpy-array-to-tensor-on-gpu\/19423\/2\"\n\n\n    You should transform numpy arrays to PyTorch tensors with torch.from_numpy. \nOtherwise some weird issues might occur. \nimg = torch.from_numpy(img).float().to(device)\n  \n\n\n\n\n\n zetyquickly:\n\nHere is an answer, that might work for you.\nYou should create Tensor from NumPy array and then transfer it to device\nConverting numpy array to tensor on GPU with link \"https:\/\/discuss.pytorch.org\/t\/converting-numpy-array-to-tensor-on-gpu\/19423\/2\"\n\n\nHi, I create a tensor from numpy array. But the tensor to cuda device don\u2019t work.\nAt first you should check if CUDA devices are available.\nThen set the device variable with some value (e.g. 'cpu', 'cuda:0') and pass it to your_tensor.to() function.\nNote: set a constant string value for the device is not an only option (if you want use tensor.to() for transfering to device), you may pass there a device value of some other tensor"},{"x":"Hi. I have a network f that take a batch of input pair of image and label (x,y) and compute a cross entropy loss. I understand that if I use nn.CrossEntropyLoss(), the loss will be averaged over every input of the batch. If I access any weight (let say module.conv1.weight), I can obtain the gradient of the loss that is the average of the cross entropy over the batch with respect to the parameter  module.conv1.weight by accessing  module.conv1.weight.grad.\nMy question is the following. I would like to be able to compute the mean of the squared gradient of the loss with respect to the parameter over the batch instead of the mean of the gradient itself.\nIn other words, instead of having module.conv1.weight.grad that contains the mean of the gradient of the loss with respect to every element of the batch, I would like to extract a variable that would contain  the mean of the squared gradient of the loss, i.e. a vector g whose entries g_i are (module.conv1.weight.grad_i)^2.\nIs it possible to do that?\nThanks","y":"There\u2019s a trick from Ian Goodfellow to do this in a single pass: https:\/\/arxiv.org\/abs\/1510.01799\nThis post gives PyTorch recipe \u2013 Efficient Per-Example Gradient Computations with link \"https:\/\/discuss.pytorch.org\/t\/efficient-per-example-gradient-computations\/17204\/3\"","z":"Hi,\nPytorch computes the gradients wrt the loss you computed. So if your loss is the average, then the gradients will be the average as well.\nIf you don\u2019t average the loss, then you will get as many losses as the batch size. And you will need to run as many backwards as the batch size to get all these gradients.\nThere\u2019s a trick from Ian Goodfellow to do this in a single pass: https:\/\/arxiv.org\/abs\/1510.01799\nThis post gives PyTorch recipe \u2013 Efficient Per-Example Gradient Computations with link \"https:\/\/discuss.pytorch.org\/t\/efficient-per-example-gradient-computations\/17204\/3\"\nThanks a lot for this answer!"},{"x":"I\u2019m trying to visualize an already trained CNN model according to these instructions with link \"https:\/\/towardsdatascience.com\/how-to-visualize-convolutional-features-in-40-lines-of-code-70b7d87b0030\", which involve finding an input image that maximizes a feature map of interest. Unfortunately, the input variable is stuck with a None grad after back-propagation, so learning is impossible. How can I get the gradients to be computed?\n<code class=\"lang-auto\">device = torch.device('cpu')\n\nclass M7(nn.Module):\n    # Zhang. M7-1\n    def __init__(self):\n        super().__init__()\n        self.conv1 = nn.Conv2d(1, 64, 3, padding=1)\n        self.conv2 = nn.Conv2d(64, 128, 3, padding=1)\n        self.conv3 = nn.Conv2d(128, 256, 3, padding=1)\n        self.conv4 = nn.Conv2d(256, 512, 3, padding=1)\n        self.conv5 = nn.Conv2d(512, 512, 3, padding=1)\n        self.fc1 = nn.Linear(512 * 4 * 4, 1024) # 4 = 64\/(2^4)\n        self.fc2 = nn.Linear(1024, 200)\n\u200b\n    def forward(self, X):\n        X = F.max_pool2d(F.relu(self.conv1(X)), (2, 2), (2, 2))\n        X = F.max_pool2d(F.relu(self.conv2(X)), (2, 2), (2, 2))\n        X = F.max_pool2d(F.relu(self.conv3(X)), (2, 2), (2, 2))\n        X = F.relu(self.conv4(X))\n        X = F.max_pool2d(F.relu(self.conv5(X)), (2, 2), (2, 2))\n        \n        X = F.dropout(torch.flatten(X, start_dim=1))\n        X = F.dropout(F.relu(self.fc1(X)))\n        X = self.fc2(X)\n        return F.log_softmax(X, dim=1) # log-probabilities\n\u200b\nm7 = M7()\n\nfrom torchvision import transforms\ndata_dir = '..\/input\/casia-hwdb11-200-most-common\/train\/'\nmean_image = 255 - np.load(os.path.join(data_dir, 'mean_image.npy'))\ntransform = transforms.Compose([\n    transforms.Lambda(cv2.bitwise_not),\n    transforms.Lambda(lambda img: img - mean_image),\n    transforms.Lambda(torch.from_numpy),\n    transforms.Lambda(lambda x: x.float()),\n    transforms.Lambda(lambda x: x[None][None])\n ])\n\u200b\nclass SavedFeatures():\n    def __init__(self, module):\n        self.hook = module.register_forward_hook(self.hook_fn)\n    def hook_fn(self, module, input, output):\n        self.features = torch.tensor(output, requires_grad=True, device=device)\n    def close(self):\n        self.hook.remove()\n\nimg = np.uint8(np.random.uniform(40 - 15, 40 + 15, (64, 64)))\nimg = transform(img)\nimg.requires_grad_()\nm7.to(device).eval()\nactivations = SavedFeatures(list(m7.children())[1])\n_ = m7(img)\nloss = -activations.features[0, 0].mean()\nloss.backward()\n\nprint('requires grad:', img.requires_grad) \nprint('is leaf:', img.is_leaf) \nprint()\nprint(loss)\nprint(img.grad)\n<\/code>\nHere\u2019s the output, which always includes a warning for copying construct a tensor. I haven\u2019t been able to identify what triggers this warning, but it may be related to my grad problem.\n<code class=\"lang-auto\">requires grad: True\nis leaf: True\n\ntensor(14.2372, grad_fn=<NegBackward>)\nNone\n\n\/opt\/conda\/lib\/python3.6\/site-packages\/ipykernel_launcher.py:16: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n  app.launch_new_instance()\n<\/code>","y":"By rewrapping your intermediate activation in a new tensor, you are detaching it from the computation graph:\n<code class=\"lang-python\">self.features = torch.tensor(output, requires_grad=True, device=device)\n<\/code>\nYou could assign self.features directly to output, which will give you a valid gradient in your input tensors.","z":"By rewrapping your intermediate activation in a new tensor, you are detaching it from the computation graph:\n<code class=\"lang-python\">self.features = torch.tensor(output, requires_grad=True, device=device)\n<\/code>\nYou could assign self.features directly to output, which will give you a valid gradient in your input tensors.\nThank you. Your solution works. But how come creating a new tensor seems to work in the original code with link \"https:\/\/github.com\/fg91\/visualizing-cnn-feature-maps\/blob\/master\/filter_visualizer.ipynb\"?\n<code class=\"lang-auto\">class SaveFeatures():\n    def __init__(self, module):\n        self.hook = module.register_forward_hook(self.hook_fn)\n    def hook_fn(self, module, input, output):\n        self.features = torch.tensor(output,requires_grad=True).cuda()\n    def close(self):\n        self.hook.remove()\n<\/code>\nI\u2019m not familiar with the code and just a bit with FastAI.\nAre you getting valid gradients for the input image using this code snippet?\nNo, but the code apparently worked for the author.\nI\u2019m not sure, as it shouldn\u2019t have worked before (creating a new tensor has always detached the output from the computation graph), and there is also an open issue with link \"https:\/\/github.com\/fg91\/visualizing-cnn-feature-maps\/issues\/7\", which describes the same error.\nI\u2019m really not sure, if the code was working at some point or if the author used another code base to write the blog post.\nIn the comments one user seems to have forked his code (link with link \"https:\/\/nbviewer.jupyter.org\/github\/anhquan0412\/animation-classification\/blob\/master\/convolutional-feature-visualization.ipynb\"), which does not contain the recreation of the tensor.\nThank you. Now I\u2019ve learned something about how to use Github as well."},{"x":"Hi,\nI want to create a new module that implements a custom convolution layer that, given one input channel, produces two output channels using e.g. the following two filters:\n\nThose two filters share the weights w0, \u2026, w7 and the center weight is fixed to zero. Optimally, I want to use the filters together in one F.conv2d(inputs, filters) call.\nIs this possible? If yes, how do I get the shared weights and the weights fixed to zero?\nThanks in advance!","y":"Hi,\nThe simplest solution to get these is for you only to store a 1D Tensor that contains [w0, w1\u2026, w7] and then reconstruct the filters on every forward before giving them to F.conv2d.","z":"Hi,\nThe simplest solution to get these is for you only to store a 1D Tensor that contains [w0, w1\u2026, w7] and then reconstruct the filters on every forward before giving them to F.conv2d.\nOk, I understand.\nBut is there a way to construct this in the constructor and not the forward pass?  Because if there are many input- and output-channels, this might be a lot of work for every forward pass.\nWell not really. Because this needs to be recomputed at every forward like any other operation you perform in your forward pass."},{"x":"Hi. While training with CIFAR-10 dataset and ResNet-20, I want to change the activation value in backpropagation, but I got the following error.\nRuntimeError: one of the variables needed for gradient computation has been modified by an inplace operation: [torch.cuda.FloatTensor [128, 16, 32, 32]], which is output 0 of ReluBackward1, is at version 2; expected version 1 instead. Hint: enable anomaly detection to find the operation that failed to compute its gradient, with torch.autograd.set_detect_anomaly(True).\nCan\u2019t I change the activation value saved in the forward path during the backward path?\nThank you.","y":"There is no unified way to compute these.\nBut even if you do this, you won\u2019t computes gradients anymore.","z":"You can change it. But if you change it inplace and something else needed the original value (the relu in this case), then the relu cannot compute its backward anymore. Hence the error.\nThank you for your reply. Then, is there no way to use the changed activation instead of the existing activation in calculating the gradient?\nBut then you won\u2019t compute gradients anymore. That is fine?\nI want to see the code how to calculate the gradient from an existing built-in function(for example, torch.nn.Conv2d). If that\u2019s possible, Maybe i can change the activation value to whatever i want in calculating the gradient\nThere is no unified way to compute these.\nBut even if you do this, you won\u2019t computes gradients anymore.\nThat\u2019s too bad. Okay. Thank you for your answer."},{"x":"Consider the following (made up) example snippet:\n<code class=\"lang-python\">import torch\n\nmeasure = torch.nn.MSELoss()\n\nx = torch.tensor([1, 0], dtype=torch.float64)\nt = torch.eye(2, dtype=torch.float64, requires_grad=True)\na = torch.ones(2, dtype=torch.float64, requires_grad=True)\ny = t @ a\n\nfor __ in range(2):\n    x_out = t @ (x + y)  # Raises `RuntimeError` later on.\n    # x_out = t @ (x + t @ a)  # Works fine.\n    loss = measure(x_out, x)\n    loss.backward()\n<\/code>\nDuring the second iteration of the for loop the loss.backward() raises the following exception:\n<code class=\"lang-auto\">RuntimeError: Trying to backward through the graph a second time, but the buffers have already been freed. Specify retain_graph=True when calling backward the first time.\n<\/code>\nI am not sure why this happens since in every iteration I create a new computational graph by redefining x_out and loss. What is reused in every iteration is the y tensor and when I replace y with its original expression t @ a then the code runs without error.\nSo I suppose that by reusing y among the iterations that part of the graph is shared and reused. However the underlying mechanism is not really clear to me. I thought that the first call to loss.backward() should detach the graph and so subsequent iterations should build a new one? But apparently the sub-graph y = t @ a is reused in the following iterations (which makes sense since I don\u2019t provide information about that part anymore). So for the above example, which graph is actually being built and what parts of it are freed again upon calling loss.backward()?\nMy second question is, In order to make the above example work without recomputing t @ a at every iteration, what is the preferred way of dealing with the problem? Should I specify retain_graph=True? However I don\u2019t want to retain the full graph, since the \u201ctail\u201d is rebuilt on every iteration. Also I read in this topic with link \"https:\/\/discuss.pytorch.org\/t\/how-to-free-graph-manually\/9255\/2\" that a graph will be freed when the corresponding output variables run out of scope (when their reference count drops to zero) and since for loss that happens at every iteration I would expect the whole graph to be rebuilt on every iteration. According to the error message that doesn\u2019t seem the case though. The y variable never runs out of scope, so does that mean that the sub-graph corresponding to y = t @ a is not freed? And how can I free this (sub-) graph manually then (after the loop)?\nMy third question is if someone could explain the details about graph creation and graph (buffer) freeing. According to the error message torch is aware of the fact that I want to reuse a (part of the) graph however it already freed the relevant resources. So what information does torch use in order to evaluate the structure of graphs (I suppose it\u2019s .grad_fn) and what resources are allocated and then freed during backpropagation?","y":"\n\n\n Dominik:\n\nTo what parts of the graph does this refer?\n\n\nThis refers to every part of the graph that were visited during the backward pass.\n\n\n\n Dominik:\n\nAnd what does \u201canything\u201d mean in that context? What else than gradient computation would \/ could you use a graph for?\n\n\nYes only gradient computation in this case.\nBe careful if playing with super simple example, you can create simple graphs that don\u2019t have any buffer or intermediary result and so you will be able to backprop through them multiple times. But this is an artefact of the fact that these graphs are super simple.\n\n\n\n Dominik:\n\nBased on the above statement that .backward() frees any resources \/ buffers \/ intermediary results of the graph, I would expect the computation of d and e not to work\n\n\nIt does free ressources of the graph. Not the Tensors that the user created during the forward.\nYou don\u2019t have a strong link between Tensors from the forward pass and nodes in the graph. The only thing is that computing a new tensor creates a new node in the graph. But you can remove the Tensor without impacting the graph and you can remove the graph\u2019s buffers (by doing backward) without impacting the Tensor that created it.\nIn your example, b and c are not part of the graph.\nSo even after calling backprop, you can still use them.\nThe same way, you could del b and the c.backward() call will still work without any problem.","z":"Hi,\nThere are many questions here, I\u2019ll try and answer in the order you asked them even if they overlap.\nYes the error is because here the operation t @ a is reused multiple times. I am not sure what you mean by \" loss.backward() should detach the graph\". backward does not change the graph, it just traverses it (and free un necessary buffers if retain_graph is not set).\nIn you case, the graph up to y is built once. Then each iteration extend this graph. The problem is that the first backward will free even the common part of the graph and so the second one will fail.\n\u201cI would expect the whole graph to be rebuilt on every iteration\u201d: keep in mind that the computational graph is built ONLY buy doing forward operations in your python script. The backend will never change\/built it.\nIn you case you indeed can use retain_graph=True to keep the whole graph during the backward. And the \u201clower\u201d part (that was created inside the loop) will be freed when the loop exit. The upper part (t @ a) will still remain as the y variable still refers to it.\nTo free the upper part after the loop, simply delete everything that references it: del y in your case.\nThe graph is created every time your perform an operation on tensors (when gradient mode is enabled).\nThe graph contains both the functions that need to be applied to perform the backward pass and the buffers and temporary variable they need.\nWhen you call backward, the graph is traversed in reverse order and each function is applied one after the other. After each function execution, all its buffers and temporary variables are deleted (if retain_graph=False).\nIf during the backward, a buffer does not exist, then the above error is raised as the only way this buffer does not exist anymore is because your already backproped.\nHi,\nthanks for the prompt and thorough reply, it\u2019s a lot clearer now  I have a few follow-up questions though:\n\n\nSo that basically means that y = t @ a creates a graph and each iteration in the loop extends this graph by attaching to it? Consequently when the variables in the loop are re-assigned (x_out and loss) then the previously attached part is removed again, since nothing refers to it anymore? Is that interpretation correct?\n\n\nThen the keyword argument retain_graph actually only refers to the buffers (of the graph\u2019s vertices) that are needed for performing the backpropagation? But the actual graph (i.e. the relations between tensors) are retained in any way (no matter the value of the retain_graph argument)? In other topics I read that it was called retain_variables before, why the change to retain_graph (retain_buffers would be clearer maybe)? At least to me it was confusing until I read your reply.\n\n\nThe following statements are not completely clear to me:\n\n\n\nThe graph contains both the functions that need to be applied to perform the backward pass [\u2026]\n[\u2026] each function is applied one after the other\n\nWhat are these functions actually (are they related to the .grad_fn attribute)? And how are the backprop buffers related to them?\n\nYes, here is a small sample. I use y = a*2 here because that way the op has a single input and single output and it is easier to show the graph.\n\n<code class=\"lang-auto\">a = torch.rand(10, requires_grad=True)\n# no graph at this point\n\ny = a * 2\n# The backward graph linked to y is: y -> a\n\n## Iterations 0\nx_out0 = 2 * y\n# The backward graph linked to x_out0 is: x_out0 -> y -> a\n# Note that the backward graph linked to y is still: y -> a (the arrow here is the same as the one between y and a above, meaning that deleting the buffers in one will delete them in the other)\n\nloss0 = measure(x_out0)\n# The backward graph linked to loss0 is: loss0 -> x_out0 -> y -> a\nloss0.backward()\n\n## Iteration 1\nx_out1 = 2 * y\n# The backward graph linked to x_out1 is: x_out1 -> y -> a (the arrow between y and a here is the same as the one between y and a in iteration 0, meaning that deleting the buffers in one will delete them in the other)\n\nloss1 = measure(x_out1)\n# The backward graph linked to loss1 is: loss1 -> x_out1 -> y -> a\nloss1.backward()\n# This one will fail as the buffers for the array between y and a are already deleted.\n\n# If you were printing the all the graphs linked to both loss0 and loss1, you would get:\n# loss1 -> x_out1 -> y -> a\n#    loss0 -> x_out0 \u2191\n# As you can see some part of the graph is shared.\n<\/code>\n\n\nIt refers to both buffers and intermediary results. Without these, the \u201cgraph\u201d cannot be used for anything and so it does not really exist anymore. We only use it to detect double usage.\n\n\nThese functions are autograd.Function that contain both the code to run for the backward pass and some intermediate variables needed to perform these computations. Each function handle it\u2019s buffers and temporary variables in the way it wants.\n\n\nDoes that make things clearer?\nLet me know if you have other doubts.\nYour 2. point is still confusing me. You say that\n\n[\u2026] and intermediary results. Without these, the \u201cgraph\u201d cannot be used for anything [\u2026]\n\nTo what parts of the graph does this refer? Does it refer to the entire graph that was tracked through backpropagation? And what does \u201canything\u201d mean in that context? What else than gradient computation would \/ could you use a graph for?\nI tried the following snippet where a graph is constructed (up to c), then backproped and then the graph\u2019s upper part (by d, or even the whole graph by e) is reused for construction of another graph. Based on the above statement that .backward() frees any resources \/ buffers \/ intermediary results of the graph, I would expect the computation of d and e not to work (i.e. not only backprop), but apparently it does:\n<code class=\"lang-python\">a = torch.tensor(1, requires_grad=True)\nb = a * torch.tensor(2)\nc = b * torch.tensor(3)\n\nc.backward()\n\nd = b * torch.tensor(4)\ne = c * torch.tensor(5)\nprint('d = ', d)\nprint('e = ', e)\n<\/code>\nSo it still remains unclear to me what things, after performing .backward(), the graph (or parts of it) can be used for and what things it cannot be used for anymore (another .backward() for example). Like a list of do\u2019s and don\u2019t\u2019s after a graph has been backproped (considering operations involving that graph).\n\n\n\n Dominik:\n\nTo what parts of the graph does this refer?\n\n\nThis refers to every part of the graph that were visited during the backward pass.\n\n\n\n Dominik:\n\nAnd what does \u201canything\u201d mean in that context? What else than gradient computation would \/ could you use a graph for?\n\n\nYes only gradient computation in this case.\nBe careful if playing with super simple example, you can create simple graphs that don\u2019t have any buffer or intermediary result and so you will be able to backprop through them multiple times. But this is an artefact of the fact that these graphs are super simple.\n\n\n\n Dominik:\n\nBased on the above statement that .backward() frees any resources \/ buffers \/ intermediary results of the graph, I would expect the computation of d and e not to work\n\n\nIt does free ressources of the graph. Not the Tensors that the user created during the forward.\nYou don\u2019t have a strong link between Tensors from the forward pass and nodes in the graph. The only thing is that computing a new tensor creates a new node in the graph. But you can remove the Tensor without impacting the graph and you can remove the graph\u2019s buffers (by doing backward) without impacting the Tensor that created it.\nIn your example, b and c are not part of the graph.\nSo even after calling backprop, you can still use them.\nThe same way, you could del b and the c.backward() call will still work without any problem.\n<code class=\"lang-auto\">In your example, b and c are not part of the graph.\n<\/code>\nHI,I have a question that how to know if the b and c are not part of graph?\nBecause the graph we were talking about above is the graph to compute gradients. Which is composed only of backward Nodes and their buffers, not the user Tensors.\n\nBecause the graph we were talking about above is the graph to compute gradients.\n\nI still have a hard time getting my head around this \u2026 So I write a easy example as below and hope can get some feedback from you!\n<code class=\"lang-auto\">a = torch.tensor(1., requires_grad=True)\nb = torch.tensor(2.,requires_grad= True)\nc = a*b \n<\/code>\nTo my understanding , the graph would be created since c =a*b \n<code class=\"lang-auto\">for _ in range(5):\n    print(c.backward(retain_graph=True))\n    print(a.grad,b.grad)\n<\/code>\nThe result\n<code class=\"lang-auto\">None\ntensor(2.) tensor(1.)\nNone\ntensor(4.) tensor(2.)\nNone\ntensor(6.) tensor(3.)\nNone\ntensor(8.) tensor(4.)\nNone\ntensor(10.) tensor(5.)\n<\/code>\nQ1:\nWhy does print(c.backward(retain_graph=True))gives the None??\nAs for the explanation you give above:\nIf the tensor is not part of computing gradient graph ,then we can delete it and backward should work fine  .So,here\u2019s what I do:\n<code class=\"lang-auto\">del b \nfor _ in range(3):\n     c.backward(retain_graph = True)\n     print(a.grad)\n<\/code>\nAnd the result gives \uff1a\n<code class=\"lang-auto\">tensor(12.)\ntensor(14.)\ntensor(16.)\n<\/code>\nThe result does\u2019t make sense to me since to my understanding, c =a*b\nWhat I expect is that the c.backward(retain_graph = True) should fail because the b has already been deleted ,what do I miss here?\nI think the problem is that I do not know which tensor or function would be considered as the graph to compute gradients,Maybe b here is not ,but I think it should be part of it\u2026\nTwo questions for you ,I am a beginner of Pytorch and thanks in advance!\nHi, I read this blog here: https:\/\/blog.paperspace.com\/pytorch-101-understanding-graphs-and-automatic-differentiation\/\nWhat I got is:\n1\/ when you want a tensor to be part of the graph, you need to specify: requires_grad  =True\nSo, for your first  example, x,t,a, y and x_out (requires_grad being contagious) are all going to be in the dynamic computation graph once you do the forward pass (ie y = t @ a also x_out=t @ (x + y) and loss = measure())\nIf we define a leaf node as a tensor you create without the need of a function( say add, multiply etc) then t and a are leaf nodes\n2\/All the non-leafs nodes will be erased once you do the backward pass. So, y and x_out\u2019s values will be erased from the graph as well as the other gradients say dy\/dt , dy\/da, dx_out\/dt, dx_out\/dy etc\n3\/For there to not be an error, one has to do forward() then backward(). But since you only recalculated x_out and loss but not y again in the loop, you didn\u2019t do a complete forward(). Thus the error happened.\nHope this helps!\nHi,\n\n\n.backward() does not return anything. So it prints None. You can check the doc for more details.\nI mention ackward Nodes and their buffers. In your case, the content of b becomes one of the buffers needed to backward the multiplication. So even if you delete b itself. The corresponding buffer will still be there.\n\nOk,Thanks for your clear explanation !"},{"x":"Hello,\nI wonder if it is possible to set learning rates for each channel of the weight in a certain conv layer? I wrote an example but it would raise an error.\n<code class=\"lang-auto\"># e.g.\nclass Model(nn.Module):\n    def __init__(self):\n        super(Model, self).__init__()\n        self.conv = nn.Conv2d(3, 8, kernel_size=1, padding=0, stride=1, bias=False)\n\n    def forward(self, x):\n        return self.conv(x)\n\nmodel = Model()\nprint(model.conv.weight.shape) # torch.Size([8, 3, 1, 1])\n\nlr = [0.01* i for i in range(1, model.conv.weight.shape[0] + 1)] # [0.01, 0.02, ..., 0.08]\n\ntorch.optim.Adam(([{'params': p, 'lr': l} for p,l in zip(model.conv.weight, lr)])\n# ValueError: can't optimize a non-leaf Tensor\n<\/code>\nThanks!","y":"A possible solution is to split the weight parameter in a layer (e.g. nn.Conv2d) into multiple parameters,\nand then concat the outputs together.\nFor example, for a layer with 2 input channels and 4 output channels, a pseudo code works as bellow:\n<code class=\"lang-auto\">class MyConv(nn.Module):\n\n    def __init__(self, channel_in, channel_out, kernel_size):\n        self.weight1 = Parameter(torch.zeros(1, channel_in, kernel_size, kernel_size))\n        self.weight2 = Parameter(torch.zeros(1, channel_in, kernel_size, kernel_size))\n        ...\n        self.weight4 = Parameter(torch.zeros(1, channel_in, kernel_size, kernel_size))\n\n        ...\n    def forward(self, input):\n        output1 = torch.nn.functional.conv2d(input, self.weight1, ...)\n        ...\n        output4 = torch.nn.functional.conv2d(input, self.weight1)\n        return torch.cat((output1, ..., output4), dim=1)\n<\/code>\nAnd then you can apply channel-wise learning rate tuning by adjusting the learning rates of parameter1, \u2026, parameter4.","z":"A possible solution is to split the weight parameter in a layer (e.g. nn.Conv2d) into multiple parameters,\nand then concat the outputs together.\nFor example, for a layer with 2 input channels and 4 output channels, a pseudo code works as bellow:\n<code class=\"lang-auto\">class MyConv(nn.Module):\n\n    def __init__(self, channel_in, channel_out, kernel_size):\n        self.weight1 = Parameter(torch.zeros(1, channel_in, kernel_size, kernel_size))\n        self.weight2 = Parameter(torch.zeros(1, channel_in, kernel_size, kernel_size))\n        ...\n        self.weight4 = Parameter(torch.zeros(1, channel_in, kernel_size, kernel_size))\n\n        ...\n    def forward(self, input):\n        output1 = torch.nn.functional.conv2d(input, self.weight1, ...)\n        ...\n        output4 = torch.nn.functional.conv2d(input, self.weight1)\n        return torch.cat((output1, ..., output4), dim=1)\n<\/code>\nAnd then you can apply channel-wise learning rate tuning by adjusting the learning rates of parameter1, \u2026, parameter4."},{"x":"I got this error when doing the backward pass:\n<code class=\"lang-auto\">loss.backward()\n  File \"\/usr\/local\/lib\/python3.6\/dist-packages\/torch\/tensor.py\", line 166, in backward\n    torch.autograd.backward(self, gradient, retain_graph, create_graph)\n  File \"\/usr\/local\/lib\/python3.6\/dist-packages\/torch\/autograd\/__init__.py\", line 99, in backward\n    allow_unreachable=True)  # allow_unreachable flag\nRuntimeError: Function DivBackward0 returned an invalid gradient at index 1 - expected type torch.FloatTensor but got torch.cuda.FloatTensor\n<\/code>\nThis occurs when executing loss.backward().\nAny idea is appreciated. Thanks in advance!","y":"I was able to solve the error. Some of the tensors are placed on cpu others on gpu. To solve the issue I have moved all the tensors to gpu.\n<code class=\"lang-auto\">torch.tensor(1, dtype=torch.float, requires_grad=True) \n#changed to \ntorch.tensor(1, dtype=torch.float, requires_grad=True).cuda()\n<\/code>","z":"Hi,\nThis is quite unexpected. Do you have custom autograd.Function that you defined? Do you use hooks to change some gradients during the backward?\nI was able to solve the error. Some of the tensors are placed on cpu others on gpu. To solve the issue I have moved all the tensors to gpu.\n<code class=\"lang-auto\">torch.tensor(1, dtype=torch.float, requires_grad=True) \n#changed to \ntorch.tensor(1, dtype=torch.float, requires_grad=True).cuda()\n<\/code>\nHi there,\nThe solution adressed above is absolutely correct. Some of the tensors are at gpu and some are at cpu.\nAs suggested by the above solutions as you can move the tensor to gpu by .cuda() call, similarly you can move the tensor\/model on the cpu by .cpu() call. It will come handy.\nEither you can use the solution above, but defining the device on which the neural networks and tensors is the right and more informative approch. Define the device by :-\nDevice = torch.device(\u2018cuda\u2019 if torch.cuda.is_available() else \u2018cpu\u2019)\ntorch.cuda.is_available() checks if cuda(or gpu) is available to you or not\nNow I can already see that you have cuda on your system, but still its an additive line to check which device you can have while working with pytorch.\nThen the model can be moved on the said device as :\nmodel = model.to(Device)\nand the tensors(t) can be put on the device the same way :\nt = t.to(Device)\nHope this helps, this is just for further info.\nThanks\nGood to hear it\u2019s working.\nHowever, note that you are creating a non-leaf tensor with the cuda call as described by  here with link \"https:\/\/discuss.pytorch.org\/t\/valueerror-cant-optimize-a-non-leaf-tensor\/21751\/2\", so you would probably want to create the tensor as:\n<code class=\"lang-python\">torch.tensor(1, dtype=torch.float, requires_grad=True, device='cuda:0')\n<\/code>\n\n\n\n rustagiadi95:\n\nt = t.to(Device)\n\n\nI have used this, but the error here is strange. Eventhough using the above statement (t.to(device)) some of the tensors are on gpu and others are on cpu. It is not showing any error during forward propagation. The error is occurring during backprop. I need some clarification why it is not throwing any error during forward prop. TIA.\nCould you post a minimal code snippet to reproduce this error?"},{"x":"How can we assign an external tensor to the nn.Parameters of a network and keep the gradient flowing out of the network back to the external source of the parameters? The problem is that when assigning the tensor to the weights of a layer, I can\u2019t find a way to make autograd see that the assignment to the weight is a slice from a larger tensor of parameters.\nCreate parameters\n\nmodel = nn.Sequential( nn.Linear(1, 1) ).to(device)\ntheta = torch.tensor([[1.0,2.0]], requires_grad=True).to(device)\n\nSelect slice of theta to be assign as weight\n\nw = torch.reshape(theta[0][0:1], model._modules[\u20180\u2019]._parameters[\u2018weight\u2019].shape)\n\nThis returns a tensor with the right grad_fn:\ntensor([[1.]], device=\u2018cuda:0\u2019, grad_fn=< AsStridedBackward >)\nAssignment with nn.Parameter does not work since I think there is clone() when constructing\n\nmodel._modules[\u20180\u2019]._parameters[\u2018weight\u2019] = nn.Parameter(w)\n\nThis returns a parameter with no grad_fn:\nParameter containing:\ntensor([[1.]], device=\u2018cuda:0\u2019, requires_grad=True)\nAssignment with .data does not work either\n\nmodel._modules[\u20180\u2019]._parameters[\u2018weight\u2019].data = w\n\nThis returns a parameter with no grad_fn:\nParameter containing:\ntensor([[1.]], device=\u2018cuda:0\u2019, requires_grad=True)\nI need to find a way to keep the Backward assignment when passing the new tensor into the Parameter","y":"Since you won\u2019t learn weight anymore, you don\u2019t need it to be a parameter anymore. So you can do the following\n<code class=\"lang-auto\">mod = model._modules['0']\ndel mod.weight\nmod.weight = w\n<\/code>\nYou need to be careful though to delete .weight before it is passed to the optimizer.\nAnd to populate it with a Tensor of the right size after that before calling the forward.","z":"Since you won\u2019t learn weight anymore, you don\u2019t need it to be a parameter anymore. So you can do the following\n<code class=\"lang-auto\">mod = model._modules['0']\ndel mod.weight\nmod.weight = w\n<\/code>\nYou need to be careful though to delete .weight before it is passed to the optimizer.\nAnd to populate it with a Tensor of the right size after that before calling the forward.\nThis solution seems to work so far. The idea is to first remove all Parameters in the Module, and replace them by tensors under object attributes with the same name. Here is what I used:\ndef flip_parameters_to_tensors(module):\n    attr = []\n    while bool(module._parameters):\n        attr.append( module._parameters.popitem() )\n    setattr(module, 'registered_parameters_name', [])\n\n    for i in attr:\n        setattr(module, i[0], torch.zeros(i[1].shape,requires_grad=True))\n        module.registered_parameters_name.append(i[0])\n\n    module_name = [k for k,v in module._modules.items()]\n\n    for name in module_name:\n        flip_parameters_to_tensors(module._modules[name])\n\nThen, we can used the saved list of previously active attributes to assign the tensors:\ndef set_all_parameters(module, theta):\n    count = 0  \n\n    for name in module.registered_parameters_name:\n        a = count\n        b = a + getattr(module, name).numel()\n        t = torch.reshape(theta[0,a:b], getattr(module, name).shape)\n        setattr(module, name, t)\n\n        count += getattr(module, name).numel()\n\n    module_name = [k for k,v in module._modules.items()]\n    for name in module_name:\n        count += set_all_parameters(module._modules[name], theta)\n    return count\n\nThis way, the flattened vector is assigned to all tensors of the NN for evaluation. The backward() gets back to the parameter vector outside the NN.\nThanks\nShameless self-promotion of this helper package of mine. https:\/\/github.com\/SsnL\/PyTorch-Reparam-Module\nBasically it allows you to activate a network with arbitrary parameter, and minimal code change."},{"x":"From this thread with link \"https:\/\/discuss.pytorch.org\/t\/memory-leak-when-appending-tensors-to-a-list\/25937\" i learned that I always store the whole computational graph when I append a tensor to a list.\nTo fix this I could detach the tensor before storing it, however in my case I need them for gradient computation. For each pixel of an image I have to store one state tensor, as soon as all state tensors are computed I can compute the output. If I use more layers I need to store n times the amount of tensors.\nCurrently I use 64x84 images and with two layers of 256 units I already run out of memory on my universities cluster.\nIs there some way of reducing the storage needed or am I just not able to use larger numbers with my architecture?","y":"If your want to backprop through all these, then you need to have them.\nAn alternative is to trade memory for compute using the torch.utils.checkpoint with link \"https:\/\/pytorch.org\/docs\/stable\/checkpoint.html?highlight=checkpoint\" module.","z":"If your want to backprop through all these, then you need to have them.\nAn alternative is to trade memory for compute using the torch.utils.checkpoint with link \"https:\/\/pytorch.org\/docs\/stable\/checkpoint.html?highlight=checkpoint\" module."},{"x":"\nthis is a part of the network i\u2019m creating where the numbres presente the numbre on feature maps. This is the code i proposed:\n<code class=\"lang-auto\">        self.last_one_down_1=nn.Sequential(\n          nn.BatchNorm2d(num_features=512),\n          nn.ReLU()\n        )\n        self.last_one_down_2_p1=nn.Sequential(\n          nn.Conv2d(in_channels=512,out_channels=1024,kernel_size=3),\n          nn.ReLU()\n        )\n        self.last_one_down_2_p2=nn.Sequential(\n          nn.Conv2d(in_channels=1024,out_channels=512,kernel_size=3),\n          nn.ReLU(),\n          nn.PixelShuffle(upscale_factor=2)\n        )\n<\/code>\nthe outpute i\u2019ve gote :\n<code class=\"lang-auto\">after last_one_down_1 level ,input shape =  torch.Size([1, 512, 21, 31])\nafter last_one_down_2_p1 level ,input shape =  torch.Size([1, 1024, 19, 29])\nafter last_one_down_2_p2 level ,input shape =  torch.Size([1, 128, 34, 54]\n<\/code>\nso the probleme is the output of Conv2d of self.last_one_down_2_p2 suppose to be 512 but i\u2019ve got 128\ncan anone help me to solve it?","y":"nn.PixelShuffle will change the number of output channels as described in the docs with link \"https:\/\/pytorch.org\/docs\/stable\/generated\/torch.nn.PixelShuffle.html#torch.nn.PixelShuffle\":\n\nInput: (N, L, H_in, W_in) where L= C * upscale_factor**2\nOutput: (N, C, H_out, W_out) where H_out = H_in * upscale_factor and W_out = W_in * upscale_factor\n\nIn your case, upscale_factor=2, so the input channels are defined as L = 512 = 128 * 2**2, and the output channels thus C = 128.","z":"nn.PixelShuffle will change the number of output channels as described in the docs with link \"https:\/\/pytorch.org\/docs\/stable\/generated\/torch.nn.PixelShuffle.html#torch.nn.PixelShuffle\":\n\nInput: (N, L, H_in, W_in) where L= C * upscale_factor**2\nOutput: (N, C, H_out, W_out) where H_out = H_in * upscale_factor and W_out = W_in * upscale_factor\n\nIn your case, upscale_factor=2, so the input channels are defined as L = 512 = 128 * 2**2, and the output channels thus C = 128.\n  Thank you for your response. It was of great help.  \nIn fact, I am trying to recreate the RUNet architecture for super image resolution and as a beginner it causes me a lot of problems and a terrible headache. \nRU-Net887\u00d7414 41.2 KB with link \"https:\/\/discuss.pytorch.org\/uploads\/default\/original\/3X\/2\/b\/2b6c5a093837a80cc1fa0ddf467f92cec30b06b0.png\""},{"x":"Hi,\nIn this tutorial with link \"https:\/\/pytorch.org\/tutorials\/beginner\/examples_autograd\/two_layer_net_custom_function.html\",  it is mentioned that backward receives a tensor (grad_output) but when we run loss.backward(), loss is a scalar. I see scalar is also a tensor but if grad_output is something other than the loss then my question is how I can  have access to the grad_output that is computed based on loss then?\n<code class=\"lang-auto\">\n    def backward(ctx, grad_output):\n        \"\"\"\n        In the backward pass we receive a Tensor containing the gradient of the loss\n        with respect to the output, and we need to compute the gradient of the loss\n        with respect to the input.\n        \"\"\"\n        input, = ctx.saved_tensors\n        grad_input = grad_output.clone()\n        grad_input[input < 0] = 0\n        return grad_input\n<\/code>","y":"I could successfully bypass loss.backward() in a simple network for both MSE loss and CrossEntropy loss and I replaced it with a backward network. For future reference, I am sharing the code. Thanks albanD!\n<code class=\"lang-auto\">import torch.nn as nn\nimport torch.optim as optim\nimport torchvision\n\nimport numpy as np\nimport scipy.stats as ss\nimport scipy\nimport torch\nfrom torchvision import datasets, transforms\nimport scipy.stats as ss\nimport matplotlib.pylab as plt\n\nimagesetdir = '.\/'\n\nuse_cuda = True\nbatch_size = 1024\nkwargs = {'num_workers': 0, 'pin_memory': True, 'drop_last':True} if use_cuda else {}\ntrain_loader = torch.utils.data.DataLoader(\n    datasets.MNIST(imagesetdir, train=True, download=True,\n                    transform=transforms.Compose([\n                        transforms.Resize(32),\n                        transforms.ToTensor(),\n                       \n                    ])),\n    batch_size=batch_size, shuffle=True, **kwargs)\ntest_loader = torch.utils.data.DataLoader(\n    datasets.MNIST(imagesetdir, train=False, transform=transforms.Compose([\n                        transforms.ToTensor(),\n                        transforms.Normalize((0.1307,), (0.3081,))\n                    ])),\n    batch_size=batch_size, shuffle=False, **kwargs)\n\n\n\nclass Forward(nn.Module):\n    def __init__(self):\n        super(Forward, self).__init__()\n        self.fc_0 = nn.Linear(1024, 40, bias=False)\n        self.fc_1 = nn.Linear(40, 10, bias=False)\n\n    def forward(self, x):\n        x0 = self.fc_0(x)\n        x1 = self.fc_1(x0)\n\n        return x1, [x, x0, x1]\n\nclass Backward(nn.Module):\n    def __init__(self):\n        super(Backward, self).__init__()\n        self.fc_1 = nn.Linear(10, 40, bias=False)\n        self.fc_0 = nn.Linear(40, 1024, bias=False)\n\n    def forward(self, x):\n        x1 = self.fc_1(x)\n        x0 = self.fc_0(x1)\n\n        return x0, [x1, x]\n\ndef transpose_weights(state_dict):\n\n    state_dict_new = {}\n    for k, item in state_dict.items():\n        state_dict_new.update({k: item.t()})\n    return state_dict_new\n\ndef corr(t0, t1):\n    return ss.pearsonr(t0.view(-1).detach().cpu().numpy(),t1.view( -1).detach().cpu().numpy() )\n\n# A simple hook class that returns the input and output of a layer during forward\/backward pass\nclass Hook():\n    def __init__(self, module, backward=False):\n        if backward==False:\n            self.hook = module.register_forward_hook(self.hook_fn)\n        else:\n            self.hook = module.register_backward_hook(self.hook_fn)\n    def hook_fn(self, module, input, output):\n        self.input = input\n        self.output = output\n    def close(self):\n        self.hook.remove()\n\nmodelF = Forward().cuda() # main model\nmodelB = Backward().cuda() # backward network to compute gradients for modelF\n\nmodelC = Forward().cuda() # Forward Control model to compare to BP\nmodelC.load_state_dict(modelF.state_dict())\nmodelB.load_state_dict(transpose_weights(modelF.state_dict()) )\n\noptimizerC = optim.Adam(modelC.parameters(), lr=0.0001)\noptimizer = optim.Adam(modelF.parameters(),  lr=0.0001)\ncriterion = nn.CrossEntropyLoss() #nn.MSELoss() #\n\n# -------Implementing BP without using loss.backward() ----------\nhookC = [Hook(layer[1], backward=True) for layer in list(modelC._modules.items())]\n\nn_classes = 10\nonehot = torch.zeros(train_loader.batch_size, n_classes).cuda()\nSoftmax = nn.Softmax(dim=1)\nfor epoch in range(20):\n\n    loss_running = 0\n    lossC_running = 0\n    for i, (inputs, target) in enumerate(train_loader):\n\n        inputs = inputs.view(train_loader.batch_size, -1).cuda()\n        target = target.cuda()\n        onehot.zero_()\n        onehot.scatter_(1, target.view(train_loader.batch_size,-1), 1)\n        \n        # ------------- BP Control ------------------------------------------\n        outputsC, activationsC = modelC(inputs)\n        lossC = criterion(outputsC, target)\n        optimizerC.zero_grad()\n        lossC.backward()\n        optimizerC.step()\n\n        lossC_running += lossC.item()\n        ParamsC = [p for p in modelC.parameters()]\n\n        # -------------Implementing BP bypassing loss.backward()-------------\n        modelB.load_state_dict(transpose_weights(modelF.state_dict()) )\n\n        outputs, activationsF = modelF(inputs)\n        probs = Softmax(outputs.detach())\n        # the gradient of CrossEntropy is pi-yi (pi: softmax of the output, yi:onehot label)\n        grad_input = onehot - probs # for CrossEntropyLoss\n        \n        #grad_input = (2\/n_classes) * (onehot-outputs) # for MSEloss\n        \n        recons, activationsB = modelB(grad_input)\n\n        loss = criterion(outputs, target)\n        optimizer.zero_grad()\n        \n\n        ParamsF = [p for p in modelF.parameters() if p.requires_grad]\n\n        # copy the backward gradients into the parameter grads.\n        for ip, pF in enumerate(ParamsF):\n            # parameters of the control model\n            pC = ParamsC[ip]\n            hC = hookC[::-1][ip].output[0]\n\n            aF = activationsF[ip] # forward activations\n            aB = activationsB[ip] # backward activations\n            \n            pF.grad = -torch.matmul(aB.t().detach(), aF.clone().detach())\n            # pF.grad should be close to pC.grad\n            \n        optimizer.step()\n        loss_running += loss.item()\n    \n    print('Epoch %d: Loss= %.3f, Loss_Control= %.3f'%(epoch, loss_running\/(i+1), lossC_running\/(i+1)))\n<\/code>","z":"Hi,\nWhen you call t.backward(), if t is not a tensor with a single element, it will actually complain and ask the user to provide the first grad_output (as a Tensor of the same size as t).\nIn the particular case where t has a single element, grad_output defaults to torch.Tensor([1]) because that way what is computed are gradients.\nDoes that answer your question?\nThank you for your reply.\nSo backward function always assumes a 1D input. I thought backward network can be considered as the transpose of the forward network in the linear case (fully connected and conv), so I thought backward function should accept an input the size of the output of the layer. That\u2019s why I am puzzled why backward accepts 1D input.\nHere is what I want to implement:\nAccording to the chain rule, we can view the process of computing the gradients of loss w.r.t. parameters as the dLoss\/dy * dy\/dparams. I am building a network that computes the second part i.e. dy\/dparams (similar to what happens in the backward function of each module in Pytorch). Now, my question is what would be the input to that network so that it computes the same gradients as loss.backward() calculates for each layer. This way, I would be able to bypass loss.backward() and use my own backward function. But first, for a sanity check, I want to implement backpropagation using this method (feeding gradients computed in loss to a forward network instead of using loss.backward())\nThanks\nHi,\nThere is a small difference between theory and practice here:\n\nIn theory yes, backward should work only with 1D Tensors and vector Jacobian product. Where the Jacobian assumes 1D input and 1D output.\nIn practice, your input is not a 1D and the output is not either. So you will get a dLoss\/dy which is not 1D but the same shape as y. and you should return something that is the same shape as params. But it should be computed as if these were 1D * 2D matrices.\n\nThat being said, keep in mind that if your net is a function f with input x and output y (assumed 1D here). Then what you will compute by doing y.backward(v) is v.t() J where J is the jacobian of f of size (len(y), len(x)).\nThank you for clarification.\nI am still having a hard time replicating Pytorch\u2019s loss.backward() from scratch.\nI computed dLoss\/dy when Loss is a CorssEntropy with link \"https:\/\/deepnotes.io\/softmax-crossentropy\" loss and fed it to my backward network. I compared the loss and also the gradients computed by my network to the same network being trained with loss.backward(). But, gradients are not the same the ones computed by loss.backward() and I am puzzled why. I would appreciate it if you could point me to where my logic is wrong. Here is the code:\n<code class=\"lang-auto\">import torch.nn as nn\nimport torch.optim as optim\nimport torchvision\n\nimport numpy as np\nimport scipy.stats as ss\nimport scipy\nimport torch\nfrom torchvision import datasets, transforms\n\nimagesetdir = '\/.' #path_prefix+'\/Data\/'\n\nuse_cuda = True\nbatch_size = 64\nkwargs = {'num_workers': 0, 'pin_memory': True, 'drop_last':True} if use_cuda else {}\ntrain_loader = torch.utils.data.DataLoader(\n    datasets.MNIST(imagesetdir, train=True, download=True,\n                    transform=transforms.Compose([\n                        transforms.Resize(32),\n                        transforms.ToTensor(),\n                       \n                    ])),\n    batch_size=batch_size, shuffle=True, **kwargs)\ntest_loader = torch.utils.data.DataLoader(\n    datasets.MNIST(imagesetdir, train=False, transform=transforms.Compose([\n                        transforms.ToTensor(),\n                        transforms.Normalize((0.1307,), (0.3081,))\n                    ])),\n    batch_size=batch_size, shuffle=False, **kwargs)\n\n\n\nclass Forward(nn.Module):\n    def __init__(self):\n        super(Forward, self).__init__()\n        self.fc_0 = nn.Linear(1024, 40, bias=False)\n        self.fc_1 = nn.Linear(40, 10, bias=False)\n\n    def forward(self, x):\n        x0 = self.fc_0(x)\n        x1 = self.fc_1(x0)\n\n        return x1, [x0, x1]\n\nclass Backward(nn.Module):\n    def __init__(self):\n        super(Backward, self).__init__()\n        self.fc_1 = nn.Linear(10, 40, bias=False)\n        self.fc_0 = nn.Linear(40, 1024, bias=False)\n\n    def forward(self, x):\n        x1 = self.fc_1(x)\n        x0 = self.fc_0(x1)\n\n        return [x0, x1]\n\ndef transpose_weights(state_dict):\n\n    state_dict_new = {}\n    for k, item in state_dict.items():\n        state_dict_new.update({k: item.t()})\n    return state_dict_new\n\n\nmodelF = Forward().cuda() # main model\nmodelB = Backward().cuda() # backward network to compute gradients for modelF\n\nmodelC = Forward().cuda() # Forward Control model to compare to BP\n\noptimizerC = optim.Adam(modelC.parameters(), lr=0.0001)\noptimizer = optim.Adam(modelF.parameters(), lr=0.0001)\ncriterion = nn.CrossEntropyLoss()\n\n# -------Implementing BP without using loss.backward() ----------\n\nn_classes = 10\nonehot = torch.zeros(train_loader.batch_size, n_classes).cuda()\nSoftmax = nn.Softmax(dim=1)\nfor epoch in range(20):\n\n    loss_running = 0\n    lossC_running = 0\n    for i, (inputs, target) in enumerate(train_loader):\n\n        inputs = inputs.view(train_loader.batch_size, -1).cuda()\n        target = target.cuda()\n        onehot.zero_()\n        onehot.scatter_(1, target.view(train_loader.batch_size,-1), 1)\n        \n        # ------------- BP Control ------------------------------------------\n        outputsC, _ = modelC(inputs)\n        lossC = criterion(outputsC, target)\n        paramsC = [p for p in modelC.parameters()]\n        optimizerC.zero_grad()\n        lossC.backward()\n        optimizerC.step()\n\n        lossC_running += lossC.item()\n\n        # -------------Implementing BP bypassing loss.backward()-------------\n        modelB.load_state_dict(transpose_weights(modelF.state_dict()) )\n\n        outputs, activationsF = modelF(inputs)\n        probs = Softmax(outputs.detach())\n        # the gradient of CrossEntropy is pi-yi (pi: softmax of the output, yi:onehot label)\n        activationsB = modelB(probs - onehot)\n\n        loss = criterion(outputs, target)\n        optimizer.zero_grad()\n        \n\n        ParamsF = [p for p in modelF.parameters() if p.requires_grad]\n\n        # copy the backward gradients into the parameter grads.\n        for ia, a in enumerate(activationsB):\n            pF = ParamsF[ia] # parameters of the forward model\n            h = activationsF[ia] # forward activations\n\n            # computing the gradients by multiplying forward activations and backward activations\n            pF.grad = torch.matmul(h.t().detach(), a.clone().detach())\n                \n        optimizer.step()\n        loss_running += loss.item()\n    \n    print(epoch, loss_running\/(i+1), lossC_running\/(i+1))\n<\/code>\nDon\u2019t you want to make sure modelF and modelC have the same weights?\nmodelC is only for control, so ideally I want to train modelF (using modelB instead of loss.backward()) and to train modelC using the conventional loss.backward() and then compare the parameters (weights) of modelF to modelC to see if I could replicate the logic behind loss.backward() when training modelF by modelB.\nUnfortunately there is no guaranty that two models with the same architecture will converge to exactly the same solution.\nAlso keep in mind that to train your network, you want the gradients wrt the weights. Not the input or activations. So your use of modelB might not be right. In particular, the gradients wrt the weights are given by dL\/dy dy\/dparams = dL\/dy * input for a Linear layer.\nA simpler check for this could be:\n<code class=\"lang-auto\">initialize modelF\ninitialize modelB with transpose of modelF\ninitialize modelC with modelF\n\nGet a single sample\ndo a forward \/ backward in modelC\ndo forward in modelF, modelB\ncheck the values computed by modelB and the .grad in modelC\n<\/code>\nThank you for the suggestions.\nI changed the loss function to MSELoss and the loss for my modelF starts to decrease. However, it does not give me the same gradients as modelC (computed by loss.backward()). To be able to check what the input to my backward model (modelB) should be, I need to have access to the inputs to backward function within nn.Linear module to be able to compare the input to my backward model (modelB) and the input to the backward function in nn.Module. Is there any way to access to grad_input for my control model (modelC)?\nYou can register a Tensor hook on the output of the layer. That way you will have access to the gradient for that Tensor.\nI could successfully bypass loss.backward() in a simple network for both MSE loss and CrossEntropy loss and I replaced it with a backward network. For future reference, I am sharing the code. Thanks albanD!\n<code class=\"lang-auto\">import torch.nn as nn\nimport torch.optim as optim\nimport torchvision\n\nimport numpy as np\nimport scipy.stats as ss\nimport scipy\nimport torch\nfrom torchvision import datasets, transforms\nimport scipy.stats as ss\nimport matplotlib.pylab as plt\n\nimagesetdir = '.\/'\n\nuse_cuda = True\nbatch_size = 1024\nkwargs = {'num_workers': 0, 'pin_memory': True, 'drop_last':True} if use_cuda else {}\ntrain_loader = torch.utils.data.DataLoader(\n    datasets.MNIST(imagesetdir, train=True, download=True,\n                    transform=transforms.Compose([\n                        transforms.Resize(32),\n                        transforms.ToTensor(),\n                       \n                    ])),\n    batch_size=batch_size, shuffle=True, **kwargs)\ntest_loader = torch.utils.data.DataLoader(\n    datasets.MNIST(imagesetdir, train=False, transform=transforms.Compose([\n                        transforms.ToTensor(),\n                        transforms.Normalize((0.1307,), (0.3081,))\n                    ])),\n    batch_size=batch_size, shuffle=False, **kwargs)\n\n\n\nclass Forward(nn.Module):\n    def __init__(self):\n        super(Forward, self).__init__()\n        self.fc_0 = nn.Linear(1024, 40, bias=False)\n        self.fc_1 = nn.Linear(40, 10, bias=False)\n\n    def forward(self, x):\n        x0 = self.fc_0(x)\n        x1 = self.fc_1(x0)\n\n        return x1, [x, x0, x1]\n\nclass Backward(nn.Module):\n    def __init__(self):\n        super(Backward, self).__init__()\n        self.fc_1 = nn.Linear(10, 40, bias=False)\n        self.fc_0 = nn.Linear(40, 1024, bias=False)\n\n    def forward(self, x):\n        x1 = self.fc_1(x)\n        x0 = self.fc_0(x1)\n\n        return x0, [x1, x]\n\ndef transpose_weights(state_dict):\n\n    state_dict_new = {}\n    for k, item in state_dict.items():\n        state_dict_new.update({k: item.t()})\n    return state_dict_new\n\ndef corr(t0, t1):\n    return ss.pearsonr(t0.view(-1).detach().cpu().numpy(),t1.view( -1).detach().cpu().numpy() )\n\n# A simple hook class that returns the input and output of a layer during forward\/backward pass\nclass Hook():\n    def __init__(self, module, backward=False):\n        if backward==False:\n            self.hook = module.register_forward_hook(self.hook_fn)\n        else:\n            self.hook = module.register_backward_hook(self.hook_fn)\n    def hook_fn(self, module, input, output):\n        self.input = input\n        self.output = output\n    def close(self):\n        self.hook.remove()\n\nmodelF = Forward().cuda() # main model\nmodelB = Backward().cuda() # backward network to compute gradients for modelF\n\nmodelC = Forward().cuda() # Forward Control model to compare to BP\nmodelC.load_state_dict(modelF.state_dict())\nmodelB.load_state_dict(transpose_weights(modelF.state_dict()) )\n\noptimizerC = optim.Adam(modelC.parameters(), lr=0.0001)\noptimizer = optim.Adam(modelF.parameters(),  lr=0.0001)\ncriterion = nn.CrossEntropyLoss() #nn.MSELoss() #\n\n# -------Implementing BP without using loss.backward() ----------\nhookC = [Hook(layer[1], backward=True) for layer in list(modelC._modules.items())]\n\nn_classes = 10\nonehot = torch.zeros(train_loader.batch_size, n_classes).cuda()\nSoftmax = nn.Softmax(dim=1)\nfor epoch in range(20):\n\n    loss_running = 0\n    lossC_running = 0\n    for i, (inputs, target) in enumerate(train_loader):\n\n        inputs = inputs.view(train_loader.batch_size, -1).cuda()\n        target = target.cuda()\n        onehot.zero_()\n        onehot.scatter_(1, target.view(train_loader.batch_size,-1), 1)\n        \n        # ------------- BP Control ------------------------------------------\n        outputsC, activationsC = modelC(inputs)\n        lossC = criterion(outputsC, target)\n        optimizerC.zero_grad()\n        lossC.backward()\n        optimizerC.step()\n\n        lossC_running += lossC.item()\n        ParamsC = [p for p in modelC.parameters()]\n\n        # -------------Implementing BP bypassing loss.backward()-------------\n        modelB.load_state_dict(transpose_weights(modelF.state_dict()) )\n\n        outputs, activationsF = modelF(inputs)\n        probs = Softmax(outputs.detach())\n        # the gradient of CrossEntropy is pi-yi (pi: softmax of the output, yi:onehot label)\n        grad_input = onehot - probs # for CrossEntropyLoss\n        \n        #grad_input = (2\/n_classes) * (onehot-outputs) # for MSEloss\n        \n        recons, activationsB = modelB(grad_input)\n\n        loss = criterion(outputs, target)\n        optimizer.zero_grad()\n        \n\n        ParamsF = [p for p in modelF.parameters() if p.requires_grad]\n\n        # copy the backward gradients into the parameter grads.\n        for ip, pF in enumerate(ParamsF):\n            # parameters of the control model\n            pC = ParamsC[ip]\n            hC = hookC[::-1][ip].output[0]\n\n            aF = activationsF[ip] # forward activations\n            aB = activationsB[ip] # backward activations\n            \n            pF.grad = -torch.matmul(aB.t().detach(), aF.clone().detach())\n            # pF.grad should be close to pC.grad\n            \n        optimizer.step()\n        loss_running += loss.item()\n    \n    print('Epoch %d: Loss= %.3f, Loss_Control= %.3f'%(epoch, loss_running\/(i+1), lossC_running\/(i+1)))\n<\/code>"},{"x":"This happens in a GRU model, specifically with the GRU layer processing the hidden input, judging by the error report.\nGRU layer has two inputs: data (embedding layer, size [sequence length, batch size, embedding features]) and hidden features from the previous step, size [1, batch size, hidden features].\noutput, hidden = self.gru(x, hidden)\nThe error trace stack is\n<code class=\"lang-auto\">RuntimeError: one of the variables needed for gradient computation has been modified by an inplace operation: [torch.cuda.FloatTensor [21, 3, 256]], which is output 0 of CudnnRnnBackward, is at version 1; \nexpected version 0 instead. Hint: the backtrace further above shows the operation that failed to compute its gradient. The variable in question was changed in there or anywhere later. Good luck!\n<\/code>\nThe size of the float tensor points to the hidden state of the GRU. I definitely didn\u2019t use an inplace operation on it anywhere. Other tensors and variables seem to be fine.\nSo I don\u2019t quite understand what to do here.","y":"Versions are a way to track inplace operations. every time a tensor is modified inplace, its version counter is incremented by 1.\nYou might be modifying the output or hidden inplace after it is returned by the encoder gru. In the rest of your forward pass.","z":"Hi,\nGiven that it is output 0, isn\u2019t it the output?\nDid you make sure that you don\u2019t modify the output inplace either?\nCould you share a small code sample that reproduces the issue if you can\u2019t find the problem?\nhi , there are two networks here, both  GRU: encoder and decoder. It is the decoder that returns this error. Here\u2019s the code for the encoder:\n<code class=\"lang-auto\">class EncoderGRU(nn.Module):\n\n     # seq2seq model\n     def __init__(self, vocab_size, embedding_size, hidden_size, device, num_layers=1):\n         super(EncoderGRU, self).__init__()\n         self.embedding = nn.Embedding(vocab_size, embedding_size)\n         self.gru = nn.GRU(embedding_size, hidden_size, num_layers, batch_first=False)\n         self.device = device\n\n     def forward(self, input_dialogue, hidden=None):\n         x = self.embedding(input_dialogue)\n         x = x.view(x.size()[1], x.size()[0],-1)\n         output, hidden = self.gru(x, hidden)\n         return output, hidden\n<\/code>\nThe error refers to the third line in the forward method, because [torch.cuda.FloatTensor [21, 3, 256]] refers to the hidden state (sequence length:21, batch size: 3, number of features: 256), not the embedding vector. Where do I modify hidden inplace?\nAlso, looking at the error report, I\u2019m not sure I understand what sort of \u2018version\u2019 the exception refers to: \u2018is at version 1, expected version 0\u2019. What does this mean?\nVersions are a way to track inplace operations. every time a tensor is modified inplace, its version counter is incremented by 1.\nYou might be modifying the output or hidden inplace after it is returned by the encoder gru. In the rest of your forward pass.\nRight! It was modified (unsqueeze) before input inthe decoder, and I forgot to remove the _ inplace symbol! So given all these problem with autograd handling inplace operations, when can I actually use them? Just for the dataset manipulation?\nYou can use them in the autograd. But they are not all be allowed. Unfortunately, it is a bit hard to track down what caused the error in general.\nwell unsqueeze_ is definitely one of the more uself in-place operations, and if it is not allowed, I\u2019m not sure when to use it. The one I had a problem with was not the hidden state, as it seemed from the error stack trace: it was the output of the encoder, the full history of the feedforward, size sequence_length x batch_size x hidden_state_size.\nThe thing is that a single operation is never disallowed, it is a combinations of operations that can be.\n<code class=\"lang-auto\">c = a + b\na.div_(2)\n(c + a).sum().backward()\n<\/code>\nThis will work, because the value of a is not needed to compute the gradients in the sum operation.\n<code class=\"lang-auto\">c = a * b\na.div_(2)\n(c + a).sum().backward()\n<\/code>\nThis won\u2019t work because the value of a is needed to compute the gradient wrt b in the multiplication.\nSo a given inplace operation can be allowed or not, depending on the surrounding code."},{"x":"I have been trying to get the gradients for input in this thread with link \"https:\/\/discuss.pytorch.org\/t\/how-to-use-autograd-to-get-gradients-with-respect-to-the-input\/57093\" for more than a week now.\nLooking at the official tutorial here with link \"https:\/\/pytorch.org\/tutorials\/beginner\/blitz\/autograd_tutorial.html\", for getting gradients with respect to the input where the tensor used for backward is not a scaler, it says,\n\nNow in this case y is no longer a scalar. torch.autograd could not compute the full Jacobian directly, but if we just want the vector-Jacobian product, simply pass the vector to backward as argument:\nx = torch.randn(3, requires_grad=True)\n\n\n<code class=\"lang-auto\">y = x * 2\nwhile y.data.norm() < 1000:\n    y = y * 2\n\nprint(y)\nv = torch.tensor([0.1, 1.0, 0.0001], dtype=torch.float)\ny.backward(v)\n\nprint(x.grad)\n<\/code>\n\nHowever, when I try to sth like  this the input gradient is always None!\nThe input will only have a gradient if I backprop using the loss1! which doesnt make any sense to me!\n<code class=\"lang-auto\">def fc_batchnorm_act(in_, out_, use_bn=True, act=nn.ReLU()):\n    return nn.Sequential(nn.Linear(in_,out_),\n                         act,\n                         nn.BatchNorm1d(out_) if use_bn else nn.Identity())\n\nclass Reshape(nn.Module):\n    def __init__(self, shape):\n        super().__init__()\n        self.shape = shape \n\n    def forward(self, input):\n        return input.view(self.shape)\n\nclass Contractive_AutoEncoder(nn.Module):\n    def __init__(self):\n        super().__init__()\n        self.encoder = nn.Sequential(Reshape(shape=(-1, 28*28)),\n                                     fc_batchnorm_act(28*28, 400, False))\n\n        self.decoder = nn.Sequential(fc_batchnorm_act(400, 28*28, False, nn.Sigmoid()),\n                                     Reshape(shape=(-1, 1, 28, 28)))                             \n        \n    def forward(self, input):\n        outputs_e = self.encoder(input)\n        outputs = self.decoder(outputs_e)\n        return outputs_e, outputs\n\ndef loss_function(output_e, outputs, imgs, device):\n \n    criterion = nn.MSELoss()\n    assert outputs.shape == imgs.shape ,f'outputs.shape : {outputs.shape} != imgs.shape : {imgs.shape}'\n    \n    imgs.requires_grad = True \n    loss1 = criterion(outputs, imgs)\n    # loss1.backward(retain_graph=True)\n    output_e.backward(torch.ones(outputs_e.size()).to(device), retain_graph=True)\n    print(imgs.grad)\n    loss2 = torch.mean(pow(imgs.grad,2))\n    imgs.requires_grad = False \n    imgs.grad.data.zero_()\n    loss = loss1 + loss2 \n    return loss\n\n<\/code>\nand this is how it is used :\n<code class=\"lang-auto\">for e in range(epochs):\n    for i, (imgs, labels) in enumerate(dataloader_train):\n        imgs = imgs.to(device)\n        labels = labels.to(device)\n\n        outputs_e, outputs = model(imgs)\n        loss = loss_function(outputs_e, outputs, imgs,device)\n        optimizer.zero_grad()\n        loss.backward()\n        optimizer.step()\n\n    print(f'epoch\/epoechs: {e}\/{epochs} loss : {loss.item():.4f} ')\n<\/code>\nWhat am I missing here? I\u2019d grately appreciate this","y":"Hoo,\nThe thing is that the requires_grad flag is not retroactive. If you forwarded through your model with the requires_grad flag being False, then the you cannot get gradients. Your loss1 gives you some gradients because you use imgs again after setting the flag.","z":"For some weird reason if I set requires_grad outside of the loss_function, the backward works while setting the requires_grad inside loss function fails completely and not even a single iteration happens! This however wont happen if I doloss1.backward()! at all!\nHi,\nIsn\u2019t the problem that imgs is not a leaf tensor? You can check with imgs.is_leaf.\nIn particular, .grad field is only populated for leaf Tensors. If you want it for other Tensors, you can use the imgs.retain_grad() with link \"https:\/\/pytorch.org\/docs\/stable\/tensors.html#torch.Tensor.retain_grad\" function to get the .grad field populated for non-leaf Tensors.\nThanks a lot I really appreciate your kind help. but why does it work when I backprop from loss1 !?\nfor example\n<code class=\"lang-auto\">def loss_function(output_e, outputs, imgs, device):\n \n    criterion = nn.MSELoss()\n    assert outputs.shape == imgs.shape ,f'outputs.shape : {outputs.shape} != imgs.shape : {imgs.shape}'\n    \n    imgs.requires_grad = True \n    loss1 = criterion(outputs, imgs)\n    loss1.backward(retain_graph=True)\n    #output_e.backward(torch.ones(outputs_e.size()).to(device), retain_graph=True)\n    print(imgs.grad)\n    loss2 = torch.mean(pow(imgs.grad,2))\n    imgs.requires_grad = False \n    imgs.grad.data.zero_()\n    loss = loss1 + loss2 \n    return loss\n<\/code>\nIf I use loss1.backward(retain_graph=True) I dont get any errors at all! but when I comment this and instead try  to do output_e.backward(torch.ones(outputs_e.size()).to(device), retain_graph=True) it fails.\nThe only way it works is that I set the imgs.requires_grad inside the training loop that is :\nthe training loop :\n<code class=\"lang-auto\">for e in range(epochs):\n    for i, (imgs, labels) in enumerate(dataloader_train):\n        imgs = imgs.to(device)\n        labels = labels.to(device)\n\n        imgs.requires_grad_(True)\n\n        outputs_e, outputs = model(imgs)\n\n        loss = loss_function(outputs_e, outputs, imgs, lam,device)\n        imgs.requires_grad_(False)\n\n        optimizer.zero_grad()\n        loss.backward()\n        optimizer.step()\n    print(f'epoch\/epochs: {e}\/{epochs} loss: {loss.item():.4f}')\n<\/code>\nand the loss :\n<code class=\"lang-auto\">def loss_function(output_e, outputs, imgs, device):\n \n    criterion = nn.MSELoss()\n    assert outputs.shape == imgs.shape ,f'outputs.shape : {outputs.shape} != imgs.shape : {imgs.shape}'\n    \n    loss1 = criterion(outputs, imgs)\n    #loss1.backward(retain_graph=True)\n    output_e.backward(torch.ones(outputs_e.size()).to(device), retain_graph=True)\n    loss2 = torch.mean(pow(imgs.grad,2))\n    imgs.grad.data.zero_()\n    loss = loss1 + loss2 \n    return loss\n<\/code>\nand now both loss.backward and outputs_e.backward() work without a hitch!\nHoo,\nThe thing is that the requires_grad flag is not retroactive. If you forwarded through your model with the requires_grad flag being False, then the you cannot get gradients. Your loss1 gives you some gradients because you use imgs again after setting the flag.\nThanks a gazillion times "},{"x":"Hi,\nIs there have any method to save a tensor for statistics?\nExample\n<code class=\"lang-python\">class Exp(Function):\n    \n     def forward(ctx, i):\n         i_max = i.max()\n         return i\n\n    \n    def backward(ctx, grad_output):\n        return grad_output\n<\/code>\nThanks!","y":"Hi,\nIf you want to save it for the backward pass, then you can save it within the context as ctx.i_max = i.max().\nIf you want to save it for external use, then you can use any pythonic way you want: use a global variable, save it in a list, have a stats object where you can record what happens etc.","z":"Hi,\nIf you want to save it for the backward pass, then you can save it within the context as ctx.i_max = i.max().\nIf you want to save it for external use, then you can use any pythonic way you want: use a global variable, save it in a list, have a stats object where you can record what happens etc."},{"x":"Hello everyone, I hope you are having a great time.\nI\u2019m trying to create a contractive autoencoder in Pytorch. I found this thread with link \"https:\/\/discuss.pytorch.org\/t\/deep-contractive-autoencoder\/12124\" and tried according to that .\nThis is the snippet I wrote :\n<code class=\"lang-auto\">\nclass Contractive_AutoEncoder(nn.Module):\n    def __init__(self):\n        super().__init__()\n        self.encoder = nn.Linear(784, 512)\n        self.decoder = nn.Linear(512, 784)\n\n    def forward(self, input):\n        # flatten the input\n        shape = input.shape\n        input = input.view(input.size(0), -1)\n        output_e = F.relu(self.encoder(input))\n        output = F.sigmoid(self.decoder(output_e))\n        output = output.view(*shape)\n        return output_e, output\n\ndef loss_function(output_e, outputs, imgs, device):\n    output_e.backward(torch.ones(output_e.size()).to(device), retain_graph=True)\n    criterion = nn.MSELoss()\n    assert outputs.shape == imgs.shape ,f'outputs.shape : {outputs.shape} != imgs.shape : {imgs.shape}'\n    \n    imgs.grad.requires_grad = True \n    loss1 = criterion(outputs, imgs)\n    print(imgs.grad)\n    loss2 = torch.mean(pow(imgs.grad,2))\n    loss = loss1 + loss2 \n    return loss \n\nepochs = 50 \ninterval = 2000\ndevice = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\nmodel = Contractive_AutoEncoder().to(device)\noptimizer = optim.Adam(model.parameters(), lr =0.001)\n\nfor e in range(epochs):\n    for i, (imgs, labels) in enumerate(dataloader_train):\n        imgs = imgs.to(device)\n        labels = labels.to(device)\n\n        outputs_e, outputs = model(imgs)\n        loss = loss_function(outputs_e, outputs, imgs,device)\n        optimizer.zero_grad()\n        loss.backward()\n        optimizer.step()\n        if i%interval: \n            print('')\n\n    print(f'epoch\/epoechs: {e}\/{epochs} loss : {loss.item():.4f} ')\n<\/code>\nfor the sake of brevity I just used one layer for encoder and one layer for decoder. it should work regardless of number of layers in each decoder and encoder obviously!\nbut the catch here is, aside from the fact that I dont knwo if this is the correct way of doing this, (calculating gradients with respect to the input), I get an error which makes the former solution wrong\/not applicable. That is,\nimgs.grad.requires_grad = True\nproduces the error :\n\nAttributeError : \u2018NoneType\u2019 object has no attribute \u2018requires_grad\u2019\n\nI also tried the second method suggested in that thread which is as follows :\n<code class=\"lang-auto\">class Contractive_Encoder(nn.Module):\n    def __init__(self):\n        super().__init__()\n        self.encoder = nn.Linear(784, 512)\n        \n    def forward(self, input):\n        # flatten the input\n        input = input.view(input.size(0), -1)\n        output_e = F.relu(self.encoder(input))\n        return output_e\n\nclass Contractive_Decoder(nn.Module):\n    def __init__(self):\n        super().__init__()\n        self.decoder = nn.Linear(512, 784)\n\n    def forward(self, input):\n        # flatten the input\n        output = F.sigmoid(self.decoder(input))\n        output = output.view(-1,1,28,28)\n        return output\n\n\nepochs = 50 \ninterval = 2000\ndevice = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n\nmodel_enc = Contractive_Encoder().to(device)\nmodel_dec = Contractive_Decoder().to(device)\n\noptimizer = optim.Adam([{\"params\":model_enc.parameters()},\n                        {\"params\":model_dec.parameters()}], lr =0.001)\n\noptimizer_cond = optim.Adam(model_enc.parameters(), lr = 0.001)\n\ncriterion = nn.MSELoss()\n\nfor e in range(epochs):\n    for i, (imgs, labels) in enumerate(dataloader_train):\n        imgs = imgs.to(device)\n        labels = labels.to(device)\n\n        outputs_e = model_enc(imgs)\n        outputs = model_dec(outputs_e)\n        loss_rec = criterion(outputs, imgs)\n        optimizer.zero_grad()\n        loss_rec.backward()\n        optimizer.step()\n\n        imgs.requires_grad_(True)\n        y = model_enc(imgs)\n        optimizer_cond.zero_grad()\n        y.backward(torch.ones(imgs.view(-1,28*28).size()))\n\n        imgs.grad.requires_grad = True\n        loss = torch.mean([pow(imgs.grad,2)])\n        optimizer_cond.zero_grad()\n        loss.backward()\n        optimizer_cond.step()\n        \n        if i%interval: \n            print('')\n\n    print(f'epoch\/epoechs: {e}\/{epochs} loss : {loss.item():.4f} ')\n<\/code>\nbut I face the error :\nRuntimeError: invalid gradient at index 0 - got [128, 784] but expected shape compatible with [128, 512]\n\nHow should I go about this?\nI really appreciate any kind of help in this .\nThanks a lot in advance","y":"OK, I ended up changing the loss function as follows, How ever is it correct? or did I mess something up here again?\n<code class=\"lang-auto\">def loss_function(output_e, outputs, imgs, device):\n    criterion = nn.MSELoss()\n    assert outputs.shape == imgs.shape ,f'outputs.shape : {outputs.shape} != imgs.shape : {imgs.shape}'\n    \n    loss1 = criterion(outputs, imgs)\n    output_e.backward(torch.ones(output_e.size()).to(device), retain_graph=True)    \n    loss2 = torch.mean(pow(imgs.grad,2))\n    imgs.grad.data.zero_()\n    loss = loss1 + loss2 \n    return loss \n<\/code>\nthe notable changes are\n\n\ninstead ofimgs.grad.requires_grad=True, I used imgs.requires_grad=True as it doesn\u2019t make any sense to me, to make grads require grads! while imgs doesn\u2019t require it in first place and it will always result in error (NoneType doesnt have require_grads!)\n\n\nin order for imgs to have gradients, you need to remember:\nFirst imgs is a non-leaf node. autograd wont store grads for non-leaf nodes. in order to make them have gradients, you should use imgs.retain_grad().  retain_grad() must be called before doing forward().\nSecond.requires_grad is not retroactive, which means it must be set prior to running forward()\n\n\nSo the training loop should look like this ultimately :\n<code class=\"lang-auto\">for e in range(epochs):\n    for i, (imgs, labels) in enumerate(dataloader_train):\n        imgs = imgs.to(device)\n        labels = labels.to(device)\n\n        imgs.retain_grad()\n        imgs.requires_grad_(True)\n        \n        outputs_e, outputs = model(imgs)\n        loss = loss_function(outputs_e, outputs, imgs, lam,device)\n\n        imgs.requires_grad_(False)\n\n        optimizer.zero_grad()\n        loss.backward()\n        optimizer.step()\n    print(f'epoch\/epochs: {e}\/{epochs} loss: {loss.item():.4f}')\n<\/code>","z":"Any help in  this is greatly appreciated guys!  ,\n\n\n\n Shisho_Sama:\n\nimgs.grad.requires_grad = True\n\n\nimgs is a tensor right? so I think it should be\nimgs.requires_grad = True\n\nMaybe this FSGM Tutorial with link \"https:\/\/pytorch.org\/tutorials\/beginner\/fgsm_tutorial.html\" is helpful since it also relies on getting the gradient with respect to the input.\nThanks a lot. really appreciate it. looking in to this right now \nOK, I ended up changing the loss function as follows, How ever is it correct? or did I mess something up here again?\n<code class=\"lang-auto\">def loss_function(output_e, outputs, imgs, device):\n    criterion = nn.MSELoss()\n    assert outputs.shape == imgs.shape ,f'outputs.shape : {outputs.shape} != imgs.shape : {imgs.shape}'\n    \n    loss1 = criterion(outputs, imgs)\n    output_e.backward(torch.ones(output_e.size()).to(device), retain_graph=True)    \n    loss2 = torch.mean(pow(imgs.grad,2))\n    imgs.grad.data.zero_()\n    loss = loss1 + loss2 \n    return loss \n<\/code>\nthe notable changes are\n\n\ninstead ofimgs.grad.requires_grad=True, I used imgs.requires_grad=True as it doesn\u2019t make any sense to me, to make grads require grads! while imgs doesn\u2019t require it in first place and it will always result in error (NoneType doesnt have require_grads!)\n\n\nin order for imgs to have gradients, you need to remember:\nFirst imgs is a non-leaf node. autograd wont store grads for non-leaf nodes. in order to make them have gradients, you should use imgs.retain_grad().  retain_grad() must be called before doing forward().\nSecond.requires_grad is not retroactive, which means it must be set prior to running forward()\n\n\nSo the training loop should look like this ultimately :\n<code class=\"lang-auto\">for e in range(epochs):\n    for i, (imgs, labels) in enumerate(dataloader_train):\n        imgs = imgs.to(device)\n        labels = labels.to(device)\n\n        imgs.retain_grad()\n        imgs.requires_grad_(True)\n        \n        outputs_e, outputs = model(imgs)\n        loss = loss_function(outputs_e, outputs, imgs, lam,device)\n\n        imgs.requires_grad_(False)\n\n        optimizer.zero_grad()\n        loss.backward()\n        optimizer.step()\n    print(f'epoch\/epochs: {e}\/{epochs} loss: {loss.item():.4f}')\n<\/code>"},{"x":"Hello\uff0cit really made me confused when reading the  refered source code in AutoGrad about the Conv2d with link \"https:\/\/discuss.pytorch.org\/t\/autograd-about-the-conv2d\/12130\".\nWhen i read the detail of  function THNN_(SpatialConvolutionMM_updateGradInput) with link \"https:\/\/github.com\/pytorch\/pytorch\/blob\/bc7a41af7d541e64f8b8f7318a7a2248c0119632\/aten\/src\/THCUNN\/generic\/SpatialConvolutionMM.cu#L211-L486\", Could any warm-kinded people tell me the meaning of gradInput, gradOutput and gradColums and the correct order of reading Autograd mechanism used in Conv2d?\nThanks a lot, Really need your help!","y":"Yes, if you\u2019re in the backward of the layer l,\ngrad_input = di,l\ngrad_output = di,l+1","z":"Hi,\nThe argument grad_output is the gradient with respect to the output given by the autograd engine.\ngrad_input is the tensor where the result should be written.\ngrad_columns is a temporary buffer given here for efficiency.\nThe backward functions always take grad_output and return grad_input.\nThx a lot! Could i regard the grad_input as \u03b4i,l defined below? \nthe superscript l reprensents the layer , W is conv2d\u2019s weight kernel and \u03c3(z) has a universal format like\n\nYes, if you\u2019re in the backward of the layer l,\ngrad_input = di,l\ngrad_output = di,l+1\nI am confused that why     gradColumns = weights\u2019 * gradOutput_n ?\nCould any help be provided?"},{"x":"Hi,\nI was just experimenting with pytorch. I implemented a cross-entropy loss function and softmax function as below\n<code class=\"lang-auto\">def xent(z,y):\n    y = torch.Tensor(to_one_hot(y,3))    #to_one_hot converts a numpy 1D array to one hot encoded 2D array\n    y_hat = pt_softmax(z)\n    loss = -y*torch.log(y_hat)\n    loss = loss.mean()\n    return loss\n\ndef pt_softmax(x):\n    exps = torch.exp(x - torch.max(x,dim=1)[0].unsqueeze(1))\n    return exps \/ torch.sum(exps,dim=1).unsqueeze(1)\n<\/code>\nAnd I was comparing this loss with nn.CrossEntropyLoss and found that nn.CrossEntropyLoss converges faster on wine dataset with link \"https:\/\/archive.ics.uci.edu\/ml\/machine-learning-databases\/wine\/\" on UCI repo. Also, the weights and gradients obtained after each epoch was different for both the losses. I am using batch gradient descent and not getting any nan values.\nCan anyone please let me know why this is happening? Is it because of unstable implementation of xent or due to some other reason?","y":"Thanks  for replying. You are correct the values obtained are exactly similar for same z and y. My implementation of xent was wrong. Here is correct implementation.\n<code class=\"lang-auto\">def xent(z,y):\n    y = torch.Tensor(to_one_hot(y,3))\n    y_hat = pt_softmax(z)\n    loss = -y*torch.log(y_hat)\n    loss = loss.sum()\/y.shape[0]\n    return loss\n<\/code>\nPreviously I was taking mean over all the elements but I have to sum the elements first then take the average w.r.t batch size.","z":"If you test with a given z and y, you should get very similar values between your version and the official one.\nYou should check that this is the case first.\nThen if this the true and the training behavior are still different, you can try with different random seeds. Sometimes, the random initialization can have a significant impact on the speed of convergence of the network.\nThanks  for replying. You are correct the values obtained are exactly similar for same z and y. My implementation of xent was wrong. Here is correct implementation.\n<code class=\"lang-auto\">def xent(z,y):\n    y = torch.Tensor(to_one_hot(y,3))\n    y_hat = pt_softmax(z)\n    loss = -y*torch.log(y_hat)\n    loss = loss.sum()\/y.shape[0]\n    return loss\n<\/code>\nPreviously I was taking mean over all the elements but I have to sum the elements first then take the average w.r.t batch size."},{"x":"Hello, I am attempting to recreate the CURE regularizer (https:\/\/arxiv.org\/abs\/1811.09716) and have a question about how to compute the gradient of the regularization term. Specifically, the regularizer is\n\nand is used in conjunction with the standard cross entropy loss of a DNN classifier. I am following the implementation (from snippet) and may be misunderstanding the key line for the gradient computation. Here is how the repo computes the regularizer:\n\n\ngithub.com with link \"https:\/\/github.com\/F-Salehi\/CURE_robustness\/blob\/daeef24ea443c37a55f87f93679a44d8504d88e7\/CURE\/CURE.py#L200-L217\"\n\n\nF-Salehi\/CURE_robustness\/blob\/daeef24ea443c37a55f87f93679a44d8504d88e7\/CURE\/CURE.py#L200-L217 with link \"https:\/\/github.com\/F-Salehi\/CURE_robustness\/blob\/daeef24ea443c37a55f87f93679a44d8504d88e7\/CURE\/CURE.py#L200-L217\"\n<code class=\"lang-py\">\ndef regularizer(self, inputs, targets, h = 3., lambda_ = 4):\n    '''\n    Regularizer term in CURE\n    '''\n    z, norm_grad = self._find_z(inputs, targets, h)\n    \n    inputs.requires_grad_()\n    outputs_pos = self.net.eval()(inputs + z)\n    outputs_orig = self.net.eval()(inputs)\n\n\n    loss_pos = self.criterion(outputs_pos, targets)\n    loss_orig = self.criterion(outputs_orig, targets)\n    grad_diff = torch.autograd.grad((loss_pos-loss_orig), inputs, grad_outputs=torch.ones(targets.size()).to(self.device),\n                                    create_graph=True)[0]\n    reg = grad_diff.reshape(grad_diff.size(0), -1).norm(dim=1)\n    self.net.zero_grad()\n\n\n    return torch.sum(self.lambda_ * reg) \/ float(inputs.size(0)), norm_grad\n<\/code>\n\n\n\n\n\nMy question is, is this:\n<code class=\"lang-auto\">grad_diff = torch.autograd.grad((loss_pos-loss_orig), inputs, grad_outputs=torch.ones(targets.size()).to(self.device), create_graph=True)[0]\n<\/code>\nthe correct statement for the gradient computation involved in the regularization term? At first glance, it appears to be allowing for the computation of the gradient of the difference of the losses, rather than the difference of the gradients. But, I may be misinterpreting this line completely.\nAny comments on if this is the correct\/incorrect way to compute the regularization term and potential solutions or improvements would be greatly appreciated.\nThanks,\nNate","y":"\n\n\n Nate:\n\nthe correct statement for the gradient computation involved in the regularization term? At first glance, it appears to be allowing for the computation of the gradient of the difference of the losses, rather than the difference of the gradients. But, I may be misinterpreting this line completely.\n\n\nThe gradient operator \u2207 is in itself linear as an operator. This means that \u2207a - \u2207b = \u2207(a-b).\nIn fact, if you write a minus function m(x1, x2) = x1 - x2 and some loss l(m) calculated from that, then the backpropagation step for this calculation is just feeding dl\/dm as grad_out into the backward of x1 and -dl\/dm as grad_out into the backward of x2. So really, the two not only agree mathematically but also computationally, with some efficiency gain if the two have common intermediates, so that the graph of the calculation of the intermediates has to be only backpropagated through once.\nBest regards\nThomas","z":"\n\n\n Nate:\n\nthe correct statement for the gradient computation involved in the regularization term? At first glance, it appears to be allowing for the computation of the gradient of the difference of the losses, rather than the difference of the gradients. But, I may be misinterpreting this line completely.\n\n\nThe gradient operator \u2207 is in itself linear as an operator. This means that \u2207a - \u2207b = \u2207(a-b).\nIn fact, if you write a minus function m(x1, x2) = x1 - x2 and some loss l(m) calculated from that, then the backpropagation step for this calculation is just feeding dl\/dm as grad_out into the backward of x1 and -dl\/dm as grad_out into the backward of x2. So really, the two not only agree mathematically but also computationally, with some efficiency gain if the two have common intermediates, so that the graph of the calculation of the intermediates has to be only backpropagated through once.\nBest regards\nThomas"},{"x":"I want to use different learning rates for different input samples to carry out model training. Any good suggestions?","y":"I second \u2019s suggestions. Do a nn.LossFn(reduction='none'), weigh the different samples to your liking and then do your own reduction (e.g. mean).","z":"The way I think of is:\nAssuming using nn.BCELoss() as loss function.\nAs here with link \"https:\/\/pytorch.org\/docs\/master\/nn.functional.html#loss-functions\" said,\n\n'none' : no reduction will be applied\n\nSo I can get the loss value of each sample by:\nloss_reduction_none = nn.BCELoss(reduction='none')\nThen I can adjust the loss value of each sample like:\n<code class=\"lang-auto\">loss_reduction_none[1] = criterion_reduction_none[1] * scaling_factor_1,\nloss_reduction_none[0] = criterion_reduction_none[2] * scaling_factor_2,\n...\nloss_reduction_none[n] = criterion_reduction_none[n] * scaling_factor_n,\n<\/code>\nBecause the learning rate works in this way.\n\nw_new = w_old - learning_rate * (\u2202 loss \/ \u2202 weight)\n\nAs I mentioned above, I adjust the loss function value of each sample.\nThat indirectly solves the problem of learning rate adjustment for each sample.\nThen I can use backward and step function:\nloss_reduction_none.backward()\noptimizer.step()\nBut I am not sure if PyTorch will use the loss function value which I have modified.\nActually, I don\u2019t know the parameter reduction is designed for what kind of demand.\nIs this the right answer?\nMaybe this question is something like below:\n\n\n\n\nHow are optimizer.step() and loss.backward() related? with link \"https:\/\/discuss.pytorch.org\/t\/how-are-optimizer-step-and-loss-backward-related\/7350\"\n\n\n    Hi. \nI am pretty new to Pytorch and keep surprised with the performance of Pytorch  \nI have followed tutorials and there\u2019s one thing that is not clear. \nHow the optimizer.step() and loss.backward() related? \nDoes optimzer.step() function optimize based on the closest loss.backward() function? \nWhen I check the loss calculated by the loss function, it is just a Tensor and seems it isn\u2019t related with the optimizer. \nHere\u2019s my questions: \n(1) Does optimzer.step() function optimize bas\u2026\n  \n\n\nI second \u2019s suggestions. Do a nn.LossFn(reduction='none'), weigh the different samples to your liking and then do your own reduction (e.g. mean).\nThanks for your reply.\n\nDo a  nn.LossFn(reduction='none') , weigh the different samples to your liking\n\nIf the method above is correct (In fact, I think that\u2019s correct),\nbut why I need to do reduction (e.g. mean)?\nIf I don\u2019t do that, what will happen?\nFrom mathematical theory\n\n\nBatch Gradient Descent\nWhen the number of samples is large, the training process will be slow.\n\n\nStochastic Gradient Descent\nDecreased accuracy, not globally optimal.\n\n\nMini-batch Gradient Descen\nUse a subset of samples to update each parameter.\n\n\nUsually we use Mini-batch Gradient Descen-like to update the parameters. Which means we get the average value of the loss function to update parameters.\nIs the process of averaging (do reduction (e.g. mean)) equivalent to this process?\nOr the process of averaging (do reduction (e.g. mean)) is just for PyTorch operating mechanism?\n\n\n\n shirui-japina:\n\nUsually we use Mini-batch Gradient Descen-like to update the parameters. Which means we get the average value of the loss function to update parameters.\nIs the process of averaging (do reduction (e.g. mean)) equivalent to this process?\n\n\nYup   We have a loss for each sample in your batch, then mean\/average that to get one loss-float which we can use to backpropagate. (edit: doesn\u2019t need to be mean, but other reduction techniques such a sum)\nThanks for your reply.\nI think I thoroughly understood this problem.\nI have reconsidered it. And found some problems.\n\n\n\n\nHow are optimizer.step() and loss.backward() related? with link \"https:\/\/discuss.pytorch.org\/t\/how-are-optimizer-step-and-loss-backward-related\/7350\/2\"\n\n\n    optimizer.step is performs a parameter update based on the current gradient (stored in .grad attribute of a parameter) and the update rule. As an example, the update rule for SGD is defined here: \nhttps:\/\/github.com\/pytorch\/pytorch\/blob\/cd9b27231b51633e76e28b6a34002ab83b0660fc\/torch\/optim\/sgd.py#L63. \n\n\nCalling .backward() mutiple times accumulates the gradient (by addition) for each parameter. This is why you should call optimizer.zero_grad() after each .step() call. Note that following the f\u2026\n  \n\n\nTopic above describes the operating mechanism of .backward() and optimizer.step() in PyTorch.\nSimple understanding is:\n\n\n.backward()\nGet the gradient for the parameters in model.\n\n\noptimizer.step()\nperforms a parameter update based on the  current  gradient (stored in .grad attribute of a parameter) and the update rule.\n\n\nI mean, the function  optimizer.step() considers the update method ,and actually update the parameters, but not PyTorch users to consider the update method. (We don\u2019t have to do reduction (e.g. mean) or something by manual)\nWhat we should do is,\n\n\nBefore the function .backward(), adjust the loss function value of each sample.\n\n\nThen get gradient for the parameters in model by .backward().\n\n\nAt the end, update the parameters based on the  current  gradient by optimizer.step().\n\n\nThe parameter in LossFn() reduction can set as: 'none'  |  'mean'  |  'sum'.\nBut not here to get the average of loss function value for the parameters updating.\n(Although I don\u2019t know what these 'mean'  |  'sum' are for.)\nIf you set the reduction flag to \u2018none\u2019 and try to do the backward() on the un-reduced loss, you get this error message\nRuntimeError: grad can be implicitly created only for scalar outputs\n"},{"x":"Hi Guys,\nI am trying to run out.backward(torch.Tensor([2.0])) and I get a error shape mismatch, (which makes sense) but I am try to run the same in Pytorch 1.0.2 it is working if I print grad post this operation elements in the matrix get multiplied by 2.0.\nout.backward(torch.tensor([2.0])) throws error\nRuntimeError: Mismatch in shape: grad_output[0] has a shape of torch.Size([1]) and output[0] has a shape of torch.Size([]).\nInfact, out.backward(torch.tensor([1.0])) doesn\u2019t work as well in the latest version.","y":"Your full function here is:\nf(x) = ( sum_i (xi+2)^2 * 3 ) \/ size(x) = (sum_i (xi+2)^2 * 3 ) \/ 4\ndf(x)\/dxj = 2 * (xj + 2) *3 \/ 4\ngiven that you start with xi = 1 for all i.\nthe gradients you expect with no scaling is: 2 * (1 + 2) *3 \/ 4 = 4.5\nSo if you multiply the result by 2, you get 9 as expected.\nI am not sure why you get 6 for your pytorch 1.2 code, but I would say this is most likely a small bug in your test code? do your start with torch.zeros() for example instead of torch.ones()?","z":"0 dimensional tensors have been introduced to represent tensors with no dimension and a single input value.\nYou can use torch.tensor(2.0) to get a 0 dimensional Tensor that contains the value 2.\nThanks , it works now but I get different output for x.grad if I use\nOutput 1: (out.backward(torch.tensor([2.0])) in pytorch version 1.2)\nA 2x2 square matrix where each values is 6\nOutput 2: (out.backward(torch.tensor(2.0)) in pytorch version 1.3)\nA 2x2 square matrix where each values is 9\nMay you please explain the difference in results?\nValues of x,y and z are as follows:\nx = torch.ones(2, 2)\nx.requires_grad_(True)\ny = x+2\nz = y * y * 3\nout = z.mean()\nout.backward(torch.tensor(2.0))\nprint(x.grad)\nOUTPUT - tensor([[9., 9.],\n[9., 9.]])\nor\ntensor([[6., 6.],\n[6., 6.]])\nYour full function here is:\nf(x) = ( sum_i (xi+2)^2 * 3 ) \/ size(x) = (sum_i (xi+2)^2 * 3 ) \/ 4\ndf(x)\/dxj = 2 * (xj + 2) *3 \/ 4\ngiven that you start with xi = 1 for all i.\nthe gradients you expect with no scaling is: 2 * (1 + 2) *3 \/ 4 = 4.5\nSo if you multiply the result by 2, you get 9 as expected.\nI am not sure why you get 6 for your pytorch 1.2 code, but I would say this is most likely a small bug in your test code? do your start with torch.zeros() for example instead of torch.ones()?\nSorry  it was mistake from my end. That should return 9.0 as expected, I am using different function for whose derivative is 3.0 and hence when multiplied by 2.0 = 6.0\nThanks again for your help!"},{"x":"The output of our CNN network is a non-negative tensor named D which dimension is [B,4,H,W]. B is batch size. For every sample, the output is a [4,H,W] tensor named Di. We want minimize the image structure similarity between the four channels of Di, so we define a custom loss function using SSIM. We calculate the SSIM values between every two channels of Di, and take the sum as the final loss.\nthe SSIM function code is from https:\/\/github.com\/VainF\/pytorch-msssim.\nIn the beginning, we did not concern about the different of value distribution between each channel, and the code is :\n<code class=\"lang-auto\">criterionSSIM = ssim.SSIM(data_range=1, channel=4)  \/\/Construct the SSIM criterion\nT1 = D.clone().detach()\nl1 = T1[:, 0, :, :]\nl2 = T1[:, 1, :, :]\nl3 = T1[:, 2, :, :]\nl4 = T1[:, 3, :, :]\ntmp1 = torch.stack([l2, l3, l4, l1], 1)\nloss1 = criterionSSIM(fusion_out, tmp1)\ntmp2 = torch.stack([l3, l4, l1, l2], 1)\nloss2 = criterionSSIM(fusion_out, tmp2)\ntmp3 = torch.stack([l4, l1, l2, l3], 1)\nloss3 = criterionSSIM(fusion_out, tmp3)\nlossSSIM = (loss1+loss2+loss3)\n<\/code>\nBut we found that the SSIM loss go down below zero quickly. To avoid negative SSIM, we normalize every channel of Di to [0, 1], and the code changes to :\n<code class=\"lang-auto\">criterionSSIM = ssim.SSIM(data_range=1, channel=4)  \/\/Construct the SSIM criterion\nB, C, H, W = D.shape\n    for b in range(0, B):\n    \tfor c in range(0, C):\n        \tD[b][c] = D[b][c] \/ torch.max(D[b][c])       \/\/ normalize every channel to [0, 1]\nT1 = D.clone().detach()\nl1 = T1[:, 0, :, :]\nl2 = T1[:, 1, :, :]\nl3 = T1[:, 2, :, :]\nl4 = T1[:, 3, :, :]\ntmp1 = torch.stack([l2, l3, l4, l1], 1)\nloss1 = criterionSSIM(fusion_out, tmp1)\ntmp2 = torch.stack([l3, l4, l1, l2], 1)\nloss2 = criterionSSIM(fusion_out, tmp2)\ntmp3 = torch.stack([l4, l1, l2, l3], 1)\nloss3 = criterionSSIM(fusion_out, tmp3)\nlossSSIM = (loss1+loss2+loss3)\n<\/code>\nThen the complier reports:\n\nRuntimeError: one of the variables needed for gradient computation has been modified by an inplace operation: [torch.cuda.FloatTensor [224, 224]], which is output 0 of SelectBackward, is at version 128; expected version 127 instead. Hint: enable anomaly detection to find the operation that failed to compute its gradient, with torch.autograd.set_detect_anomaly(True).\n\nWe think this error is caused by the normalization action:\n<code class=\"lang-auto\">  for b in range(0, B):\n    \tfor c in range(0, C):\n        \tD[b][c] = D[b][c] \/ torch.max(D[b][c])       \/\/ normalize every channel to [0, 1]\n<\/code>\nBut as a rookie, we don\u2019t know how to fix it. I checked out#6934 with link \"https:\/\/github.com\/pytorch\/pytorch\/issues\/6934\"but got no clue. If anybody here can help us, that will be very appreciated and thankful.","y":"I would recommend you to do the following to avoid for loops and in-place ops:\n<code class=\"lang-auto\">D_ = D.view(B,C,-1)\nD_max = D_.max(dim=2)[0].unsqueeze(2).unsqueeze(2)\nD_norm = (D\/D_max).view(*D.shape)\n<\/code>\nAnyway if you want to follow the for-loop approach you just need to use\n<code class=\"lang-auto\">D[b][c] = D[b][c].clone()\n<\/code>\nAs you are modifying tensor\u2019s value iteratively, which affects backprop.","z":"I would recommend you to do the following to avoid for loops and in-place ops:\n<code class=\"lang-auto\">D_ = D.view(B,C,-1)\nD_max = D_.max(dim=2)[0].unsqueeze(2).unsqueeze(2)\nD_norm = (D\/D_max).view(*D.shape)\n<\/code>\nAnyway if you want to follow the for-loop approach you just need to use\n<code class=\"lang-auto\">D[b][c] = D[b][c].clone()\n<\/code>\nAs you are modifying tensor\u2019s value iteratively, which affects backprop.\nTHANK YOU\uff01\uff01\nI will try that and report the result\u3002\nAnd I don\u2019t prefer the for-loop approach \uff0c I just do not know the elegant way \uff01 \uff1a\uff09\nThe code works, and loss.backwards() reports no error now.\nTHANK YOU!\nThe SSIM values still go below zero after batchs.  I have to find a way to  fix that now. "},{"x":"Hi!\nI\u2019m trying to run a dataset distillation algorithm (see paper with link \"https:\/\/arxiv.org\/pdf\/1811.10959.pdf\" here) and I\u2019ve encountered an implementation problem. In case you\u2019re not familiar with this paper, I will expalin its main idea briefly. Basically, dataset distillation aims at synthesizing a small number of data on which a model is trained can achieve performance of model trained on original data. And this synthesizing procedure is done by 2nd order derivatives. Note that practice training on synthesized data can be extended to multiple steps and epochs.\nBelow is a minimal example of how to synthesize data\n<code class=\"lang-auto\"># rdata and rlabel denote the real data and real label\n# steps contain  synthesized data, label and its corresponding lr\ndef forward(model, rdata, rlabel, steps):\n        model.train()\n        w = model.get_param()\n        params = [w]\n        gws = []\n\n        # first we train model on synthesized data\n        for step_i, (data, label, lr) in enumerate(steps):\n            with torch.enable_grad():\n                output = model.forward_with_param(data, w)\n                loss = F.cross_entropy(output, label)\n            gw, = torch.autograd.grad(loss, w, lr, create_graph=True)\n\n            with torch.no_grad():\n                new_w = w.sub(gw).requires_grad_()\n                params.append(new_w)\n                gws.append(gw)\n                w = new_w\n\n        # final L, evaluated on real data\n        model.eval()\n        output = model.forward_with_param(rdata, params[-1])\n        ll = F.cross_entropy(output, rlabel)\n        return (ll, params, gws)\n\ndef backward(model, rdata, rlabel, steps, saved_for_backward):\n        l, params, gws = saved_for_backward\n        gdatas = []\n        glrs = []\n        # d denotes derivative\n        dw, = torch.autograd.grad(l, (params[-1],))\n\n        # backward\n        model.train()\n        for (data, label, lr), w, gw in reversed(list(zip(steps, params, gws))):\n            hvp_in = [w]\n            hvp_in.append(data)\n            hvp_in.append(lr)\n            dgw = dw.neg()  # gw is already weighted by lr, so simple negation\n            hvp_grad = torch.autograd.grad(\n                outputs=(gw,),\n                inputs=hvp_in,\n                grad_outputs=(dgw,)\n            )\n            # Update for next iteration, i.e., previous step\n            with torch.no_grad():\n                # Save the computed gdata and glrs\n                gdatas.append(hvp_grad[1])\n                glrs.append(hvp_grad[2])\n\n                # Update dw\n                # dw becomes the gradients w.r.t. the updated w for previous step\n                dw.add_(hvp_grad[0])\n\n        return gdatas, glrs\n\nsaved = forward(model, rdata, rlabel, steps)\ngrad_infos = backward(model, rdata, rlabel, steps, saved)\n<\/code>\nSo here is my problem,during synthesizing multiple steps and multiple epochs data, those buffer( e.g. running mean\/var in bn) should be tracked and added into the computation graph. Or it should stay a constant. Otherwise the backprop computation will be in correct since wrong buffer will be used.\nMy question is, how to add those buffers into computation graph while keep the bn layer normal? Or should I restore buffers of each step and epoch at the appropriate time?\nThanks a lot!\nEdit: in case this example is too complex to understand, the algorithm is presented as follows:\nScreenshot from 2019-10-13 14-34-41.png857\u00d7372 67.1 KB with link \"https:\/\/discuss.pytorch.org\/uploads\/default\/original\/3X\/b\/1\/b1dbcc08739ca14966a4a3775b6e18b562cba828.png\"","y":"You will need to subclass batchnorm to make that happen.\nHere is an example for the 2d version:\n<code class=\"lang-python\">Class MyLearntBatchnorm(nn. BatchNorm2d):\n  def __init__(self, *args, **kwargs):\n    # Initialize the regular batchnorm\n    super().__init__(*args, **kwargs)\n    # Get the size of the runnning_* buffers\n    stats_size = self.running_mean.size()\n    # Unregister them as buffer by deleting them\n    del self.running_mean\n    del self.running_var\n    # Make them learnable parameters now\n    self.running_mean = nn.Parameter(torch.Tensor(stats_size))\n    self.running_var = nn.Parameter(torch.Tensor(stats_size))\n    # Initialize them whichever way you want (I don't think uniform is a good idea here but not sure what to use)\n    with torch.no_grad():\n      self.running_mean.uniform_()\n      self.running_var.uniform_()\n\n  def forward(self, *args, **kwargs):\n    # Our version behaves the same during train and eval and mimicks the original eval mode\n    orig_training = self.training\n    self.training = False\n    # Use the regular batchnorm forward\n    super().forward(*args, **kwargs)\n    # Restore the original training flag\n    self.training = orig_training\n<\/code>\nI haven\u2019t tried the code so there might be some typos but that should clearly present the idea.\nWhat can block this approach would be that the gradients for running_mean and running_var are not implemented in the provided batchnorm.\nIn that case, you will have to reimplement the batchnorm by hand using python functions.","z":"Hi,\nI\u2019m not sure what you mean with \u201chow to add those buffers into computation graph\u201d.\nDo you want them to have a fixed value and never be updated?\nI mean, if the model is trained on synthesized data with multiple steps \/ multiple epochs, and we just backprop like this forward and backward example, then the gradient computation would be incorrect. Because after multiple steps \/ multiple epochs, the buffer value (running mean\/var) is updated. Since the buffer value is not in the computation graph, the updated buffer value is used during backprop instead of real buffer value, especially when computing gradients of starting steps\/epochs.\nAlso, I do hope they can be updated, otherwise the bn layer is not fully functioning.\nSorry, I am a bit confused with your namings. Could you define properly what you mean by updated buffer and real bufferand the buffer value is not in the computation graph.\nAlso when you use batchnorm in train mode, the running_mean and running_var are not used, only updated to be used in .eval() mode.\nI\u2019m sorry, perhaps those namings are not 100% accurate. Also, I\u2019m sorry for your confusion. Perhaps this expression is easier to follow.\nThe problem is that, dataset distillation algorithm assumes that model weight (and running mean\/var) stays unchanged during distillation (multiple running of forward  and  backward). But after a run of  forward  and  backward, the running mean\/var is changed, leading to incorrect gradient computation in the next run of  forward  and  backward. Also, the number of synthesized images is very small (10-100 images), so we cannot simply use the statistics of synthesized images. The naive solution would be using batch norm always in eval mode ( track_running_stats=False ), which will harm model power. A more clever solution would be to track and add computation graphs for buffers, and then use gradient to update running mean\/var, just like  synthesizing images in backward function.\nAnd my question is, how to add running mean\/var into computation graph while keep the bn layer behavior normal?\nSo you want to change the batchnorm layer to behave just like a batchnorm in eval mode with the running_mean and running_var` being parameters that you train jointly with the rest of the model?\nThat\u2019s right!\nDuring training, I want the bn layer to use batch mean\/var, while running_mean  and  running_var is trained like common model parameters. During evaluation, I want the trained running_mean  and  running_var to be used like common running_mean  and  running_var in common bn layer.\nIs there any way to do so?\nYou will need to subclass batchnorm to make that happen.\nHere is an example for the 2d version:\n<code class=\"lang-python\">Class MyLearntBatchnorm(nn. BatchNorm2d):\n  def __init__(self, *args, **kwargs):\n    # Initialize the regular batchnorm\n    super().__init__(*args, **kwargs)\n    # Get the size of the runnning_* buffers\n    stats_size = self.running_mean.size()\n    # Unregister them as buffer by deleting them\n    del self.running_mean\n    del self.running_var\n    # Make them learnable parameters now\n    self.running_mean = nn.Parameter(torch.Tensor(stats_size))\n    self.running_var = nn.Parameter(torch.Tensor(stats_size))\n    # Initialize them whichever way you want (I don't think uniform is a good idea here but not sure what to use)\n    with torch.no_grad():\n      self.running_mean.uniform_()\n      self.running_var.uniform_()\n\n  def forward(self, *args, **kwargs):\n    # Our version behaves the same during train and eval and mimicks the original eval mode\n    orig_training = self.training\n    self.training = False\n    # Use the regular batchnorm forward\n    super().forward(*args, **kwargs)\n    # Restore the original training flag\n    self.training = orig_training\n<\/code>\nI haven\u2019t tried the code so there might be some typos but that should clearly present the idea.\nWhat can block this approach would be that the gradients for running_mean and running_var are not implemented in the provided batchnorm.\nIn that case, you will have to reimplement the batchnorm by hand using python functions.\nSo in addition to the batchnorm in your example code, I have to implement a backward method for batchnorm, to manually compute the gradients for running_mean and running_var?\nYou can use this new batchnorm as a drop in replacement.\nIt it runs fine, then you\u2019re done.\nIf it complains that it cannot compute gradients for running_mean and running_var, then you will have to change the forward() function to implement batchnorm:\n<code class=\"lang-auto\"># This is not the exact code you want, just a quick version to give you an example\ndef forward(self, input):\n    out = (input - self.running_mean) \/ self.running_var\n    out = # The linear transformation from batchnorm\n    return out\n<\/code>\nThis will be a bit slower \/ consume a bit more memory than the fully optimized one but will do what you want \nAh, I see! Thank you, I think that solves my problem.\nAlso, I\u2019m sorry for any inconvenience or confusion I\u2019ve caused. I don\u2019t know much about how to ask a question properly since I\u2019m new here.\nThanks again!\nNo problem, happy to help! Glad that this works for you!"},{"x":"Hi,\nI use autograd.grad function with create_graph=True. After I finish, I want to release the GPU memory the created backward graph. How can I do it?","y":"Hi,\nI don\u2019t think this is what he wants.\nretain_graph can be used if you want to call backward() multiple times on the same graph.\ncreate_graph is used if you want to backward() through the backward(). But since this second backward() will go through the current graph, then retain_graph has to be set as well. If you set it to False, you will get an error.\n the graph is deleted as soon as all references to it are gone. This would be the Tensor that contains the loss for example.\nNote that pytorch uses a custom gpu memory allocator. So when objects are freed, they are not returned to the OS directly. And so you won\u2019t see in nvidia-smi that objects have been freed. This memory is available to be reused by pytorch though. So the next forward will actually not allocate more memory on your GPU. You can find the functions here in the doc with link \"https:\/\/pytorch.org\/docs\/stable\/cuda.html#memory-management\" to be able to see exactly how much memory is used and how much is cached to be re-used later.","z":"Easy, just check out the official docs with link \"https:\/\/pytorch.org\/docs\/master\/autograd.html#torch.autograd.backward\":\n\n\nretain_graph (bool with link \"https:\/\/docs.python.org\/3\/library\/functions.html#bool\" , optional ) \u2013 If False , the graph used to compute the grad will be freed. Note that in nearly all cases setting this option to True is not needed and often can be worked around in a much more efficient way. Defaults to the value of create_graph .\n\ncreate_graph (bool with link \"https:\/\/docs.python.org\/3\/library\/functions.html#bool\" , optional ) \u2013 If True , graph of the derivative will be constructed, allowing to compute higher order derivative products. Defaults to False .\n\nSo if you set retain_graph=False this looks promising.\nDo you mean to cal autograd.grad function again this time with retain_graph=False??? This seems nasty!\nIf you check the docs, when you do create_graph=True this means retain_graph will take this True, since it defaults to the value of create_graph.\nIf you do create_graph=True, you should set also retain_graph=False to release the memory.\nHi,\nI don\u2019t think this is what he wants.\nretain_graph can be used if you want to call backward() multiple times on the same graph.\ncreate_graph is used if you want to backward() through the backward(). But since this second backward() will go through the current graph, then retain_graph has to be set as well. If you set it to False, you will get an error.\n the graph is deleted as soon as all references to it are gone. This would be the Tensor that contains the loss for example.\nNote that pytorch uses a custom gpu memory allocator. So when objects are freed, they are not returned to the OS directly. And so you won\u2019t see in nvidia-smi that objects have been freed. This memory is available to be reused by pytorch though. So the next forward will actually not allocate more memory on your GPU. You can find the functions here in the doc with link \"https:\/\/pytorch.org\/docs\/stable\/cuda.html#memory-management\" to be able to see exactly how much memory is used and how much is cached to be re-used later.\n\n\n\n albanD:\n\nI don\u2019t think this is what he wants.\n\n\nTrue but if in some case he doesn\u2019t plan to backward() multiple times on the same graph, which I think is most likely the case it would be great to do:\n<code class=\"lang-auto\">backward(create_graph=True, retain_graph=False, ...)\n<\/code>\nto free the graph.\nAs per this post with link \"https:\/\/discuss.pytorch.org\/t\/runtimeerror-trying-to-backward-through-the-graph-a-second-time-but-the-buffers-have-already-been-freed-specify-retain-graph-true-when-calling-backward-the-first-time\/6795\/2\".\nHi,\nBut if you create the graph, you\u2019re planning on calling backward on it no?\nBasically the existing graph will be a subset of the new graph created by create_graph=True. So if you ever want to use this newly created graph, you must pass retain_graph=True when you create it.\nOr am I misunderstanding your point?"},{"x":"Hi everyone,\nI was wondering if it is possible to obtain the original  name or the full path of the image after using torch.utils.data.random_split(). Achieving this would allow to save train and test split images with their original names to local directories. Thanks.","y":"random_split wraps the Dataset into Subsets as seen here with link \"https:\/\/github.com\/pytorch\/pytorch\/blob\/0444bac940695ccfa43854f9241839f759ba5f28\/torch\/utils\/data\/dataset.py#L278-L296\". The used indices can be access via subset_dataset.indices and could be used to get the image paths.\nIt depends what kind of Dataset you are using, but e.g. ImageFolder should contains the paths in its dataset.samples attribute.","z":"random_split wraps the Dataset into Subsets as seen here with link \"https:\/\/github.com\/pytorch\/pytorch\/blob\/0444bac940695ccfa43854f9241839f759ba5f28\/torch\/utils\/data\/dataset.py#L278-L296\". The used indices can be access via subset_dataset.indices and could be used to get the image paths.\nIt depends what kind of Dataset you are using, but e.g. ImageFolder should contains the paths in its dataset.samples attribute.\nThanks a lot   for the reply."},{"x":"Hi,\nI am trying to perform autograd in the following example, but unable to calculate gradients way back to the theta variable.\n<code class=\"lang-auto\">import torch\nfrom torch.autograd import Variable\n    theta = Variable(torch.randn(1)*3.1414)\n    theta.requires_grad_(True)\n    cosval = Variable(torch.cos(theta))\n    cosval.requires_grad_(True)\n    mat = Variable(torch.Tensor([1,cosval,cosval]))\n    mat.requires_grad_(True)\n    inpt = torch.Tensor([3,3,3])\n    y = torch.dot(mat,inpt)\n    y.backward(torch.ones(3))\n<\/code>\nwhen I ran the above code I am able to see the gradients of \u2018mat\u2019 but not for \u2018cosval\u2019 and \u2018theta\u2019. Here gradients for cosval and theta are none.","y":"Hi ,\nHere the graph built by autograd will represent only \" y = mat . inpt\".\nThat\u2019s because mat = Variable(torch.Tensor([1,cosval,cosval])) is a leaf node (think that mat creation torch.Tensor([1,cosval,cosval] is not a differentiable operation).\nAlso Variable is deprecated with link \"https:\/\/pytorch.org\/docs\/stable\/autograd.html#variable-deprecated\" and you should use Tensor instead.\nIf you replace them by Tensor and use torch.cat to create mat:\n<code class=\"lang-auto\">mat = torch.cat((torch.Tensor([1]), cosval, cosval))\n<\/code>\nit should do.\nYou will also need to call retain_grad on Tensors you want to access the gradient after the backward pass. So you should end up with something like:\n<code class=\"lang-auto\">import torch\n\ntheta = torch.randn(1, requires_grad=True)*3.1414\ntheta.retain_grad()\n\ncosval = torch.cos(theta)\ncosval.retain_grad()\n\nmat = torch.cat((torch.Tensor([1]), cosval, cosval))\nmat.retain_grad()\n\ninpt = torch.Tensor([3,3,3])\ny = torch.dot(mat,inpt)\ny.backward(torch.ones(3))\n<\/code>","z":"Hi ,\nHere the graph built by autograd will represent only \" y = mat . inpt\".\nThat\u2019s because mat = Variable(torch.Tensor([1,cosval,cosval])) is a leaf node (think that mat creation torch.Tensor([1,cosval,cosval] is not a differentiable operation).\nAlso Variable is deprecated with link \"https:\/\/pytorch.org\/docs\/stable\/autograd.html#variable-deprecated\" and you should use Tensor instead.\nIf you replace them by Tensor and use torch.cat to create mat:\n<code class=\"lang-auto\">mat = torch.cat((torch.Tensor([1]), cosval, cosval))\n<\/code>\nit should do.\nYou will also need to call retain_grad on Tensors you want to access the gradient after the backward pass. So you should end up with something like:\n<code class=\"lang-auto\">import torch\n\ntheta = torch.randn(1, requires_grad=True)*3.1414\ntheta.retain_grad()\n\ncosval = torch.cos(theta)\ncosval.retain_grad()\n\nmat = torch.cat((torch.Tensor([1]), cosval, cosval))\nmat.retain_grad()\n\ninpt = torch.Tensor([3,3,3])\ny = torch.dot(mat,inpt)\ny.backward(torch.ones(3))\n<\/code>\nThank you. Your solution solved my problem. But I am just wondering how come torch.Tensor([1,cosval,cosval]) is not differentiable and torch.cat((torch.Tensor([1]),cosval,cosval)) is differentiable?. Is it because torch.Tensor() is a leaf node where as torch.cat() is a torch op and makes it as a part of the graph?\nNp, happy to read that! \nThat\u2019s correct, torch.Tensor() is not an op but will be and will be a leaf node in your graph, but torch.cat with link \"https:\/\/pytorch.org\/docs\/stable\/torch.html#torch.cat\" is one.\nGot it. Thank you Serge.!!"},{"x":"In a simple linear network:\n4(input)\u2014>100\n100\u2014>3(output)\nWith a tensor X for the input and Y for output. Where should I use requires_grad=True in X,Y or the nn.Linear creates weights tensors that have the requires_grad=True automatically set?","y":"Usually you don\u2019t need to set this attribute manually.\nThe parameters inside the layers will be created as nn.Parameter, which will automatically have requires_grad=True.\nA use case for setting this attribute manually, would be e.g. if you use some extra parameters in your model (for your fancy operations) or e.g. if you need gradients in the input (adversarial training).","z":"Usually you don\u2019t need to set this attribute manually.\nThe parameters inside the layers will be created as nn.Parameter, which will automatically have requires_grad=True.\nA use case for setting this attribute manually, would be e.g. if you use some extra parameters in your model (for your fancy operations) or e.g. if you need gradients in the input (adversarial training)."},{"x":"I am trying to write a custom function that computes the dominant eigenvector and its derivative of a symmetric matrix using Eq. (68) in the matrix cookbook with link \"http:\/\/www2.imm.dtu.dk\/pubdb\/views\/edoc_download.php\/3274\/pdf\/imm3274.pdf\" and numpy.\nHere is my code:\n<code class=\"lang-python\">import torch\nfrom torch.autograd import Function, Variable, gradcheck\nimport numpy as np\n\n\ndef to_tensor(x):\n    return torch.from_numpy(x).float()\n\nclass TopEigen(Function):\n\n    \n    def forward(ctx, matrix):\n        cov = matrix.numpy()\n        lams, vecs = np.linalg.eigh(cov)\n        ctx.intrm = (lams, vecs, cov)\n        return to_tensor(vecs[:, -1, None])\n\n    \n    def backward(ctx, grad_output):\n        lams, vecs, cov = ctx.intrm\n        output = grad_output.data.numpy()\n        pinv = np.linalg.pinv(lams[-1]*np.eye(*cov.shape) - cov)\n        grad_np = np.dot(np.dot(output.T, pinv).T, vecs[None, :, -1])\n        return Variable(to_tensor(grad_np))\n\n\n# Testing\ntopeig = TopEigen.apply\n\np, q = 5, 3\nin_tensor = Variable(torch.rand(p, q), requires_grad=True)\ncov_in = torch.mm(in_tensor.t(), in_tensor)\n\nout = topeig(cov_in).mean()\nout.backward(retain_graph=True)\n\ntest = gradcheck(topeig, (cov_in, ), eps=1e-6, atol=1e-4)\nprint(test)\n<\/code>\nWhen I run this script, I get the following error message:\n<code class=\"lang-auto\">RuntimeError: for output no. 0,\n numerical:(\n-0.0680  0.0404  0.0532\n 0.0000  0.0000  0.0000\n 0.0000  0.0000  0.0000\n-0.0015 -0.1016  0.0791\n 0.0249 -0.0779  0.0285\n 0.0000  0.0000  0.0000\n-0.0019  0.0790 -0.0578\n 0.0655 -0.0740 -0.0246\n 0.0431  0.0375 -0.0817\n[torch.FloatTensor of size 9x3]\n,)\nanalytical:(\n    0     0     0\n    0     0     0\n    0     0     0\n    0     0     0\n    0     0     0\n    0     0     0\n    0     0     0\n    0     0     0\n    0     0     0\n[torch.FloatTensor of size 9x3]\n,)\n<\/code>\nWhy is the analytical output all 0?  I check the code with double() tensors but the result was the same.\nCan anyone help me with spotting the error in my code?","y":"Hi . I found the issue.  Eigen-decomposition requires a symmetric matrix as its input.  But the perturbations by gradcheck() makes the input asymmetric and eigen-decomposition fails.  I wrote my own numerical gradient checking function and the above function worked.","z":"Hi,\nI think you have the same problem as this with link \"https:\/\/discuss.pytorch.org\/t\/embeddings-doing-wierd-things-with-gradients\/16345\" post.\nBasically, add a cov_in.retain_grad() before giving it to gradcheck.\nThank you.  It resolved the issue.\nIt seems that the tutorial should be updated, right?  Because I followed the exact steps of the tutorial.\nI just made a PR in the master branch so that this issue does not happen anymore \n,  : I am still not able to get this function right.  Here is my new code:\n<code class=\"lang-python\">import torch\nfrom torch.autograd import Function, gradcheck\nfrom torch import from_numpy\nimport numpy as np\n\n\nclass TopEigen(Function):\n    \"\"\"Copmpute the top eigenvector of a matrix.\"\"\"\n\n    \n    def forward(ctx, matrix):\n        cov = matrix.detach().numpy()\n        lams, vecs = np.linalg.eigh(cov)\n        ctx.intrm = lams, vecs\n        return from_numpy(vecs[:, -1, None])\n\n    \n    def backward(ctx, grad_output):\n        lams, vecs, cov = ctx.intrm\n        output = grad_output.data.numpy()\n        pinv = np.linalg.pinv(lams[-1]*np.eye(cov.shape[0]) - cov)\n        return from_numpy(np.dot(np.dot(output.T, pinv).T, vecs[None, :, -1]))\n\n\n# Create a random symmetric matrix\np, q = 5, 3\ntorch.manual_seed(0)\nin_tensor = torch.rand(p, q, dtype=torch.float64, requires_grad=True)\ncov_in = torch.mm(in_tensor.t(), in_tensor)\n\n# Testing\ntopeig = TopEigen.apply\nout = topeig(cov_in).mean()\ntest = gradcheck(topeig, (cov_in, ), eps=1e-10, atol=1e-3)\nprint(test)\n<\/code>\nHere is the output in pytorch 0.4.1:\n<code class=\"lang-nohighlight\">Traceback (most recent call last):\n  File \"eigen.py\", line 41, in <module>\n    test = gradcheck(topeig, (cov_in, ), eps=1e-10, atol=1e-3)\n  File \"...\/anaconda3\/lib\/python3.6\/site-packages\/torch\/autograd\/gradcheck.py\", line 214, in gradcheck\n    'numerical:%s\\nanalytical:%s\\n' % (i, j, n, a))\n  File \"...\/anaconda3\/lib\/python3.6\/site-packages\/torch\/autograd\/gradcheck.py\", line 194, in fail_test\n    raise RuntimeError(msg)\nRuntimeError: Jacobian mismatch for output 0 with respect to input 0,\nnumerical:tensor([[-0.0822,  0.0487,  0.0460],\n        [ 0.0000,  0.0000,  0.0000],\n        [ 0.0000,  0.0000,  0.0000],\n        [-0.0256, -0.0462,  0.0814],\n        [ 0.0441, -0.0817,  0.0360],\n        [ 0.0000,  0.0000,  0.0000],\n        [-0.0219,  0.0801, -0.0611],\n        [ 0.0819, -0.0387, -0.0568],\n        [ 0.0381,  0.0329, -0.0820]], dtype=torch.float64)\nanalytical:tensor([[-0.0822,  0.0487,  0.0460],\n        [-0.0743,  0.0441,  0.0416],\n        [-0.0680,  0.0403,  0.0381],\n        [ 0.0487, -0.0903,  0.0398],\n        [ 0.0441, -0.0817,  0.0360],\n        [ 0.0403, -0.0747,  0.0329],\n        [ 0.0460,  0.0398, -0.0992],\n        [ 0.0416,  0.0360, -0.0897],\n        [ 0.0381,  0.0329, -0.0820]], dtype=torch.float64)\n<\/code>\nInterestingly, some rows are similar.  Would you please spot my mistake?\nSomething\u2019s funny in your formula. You could either use PyTorch master, which includes a derivative for symeig or compare your own calculation with that which has been implemented in PyTorch:\n\n\ngithub.com with link \"https:\/\/github.com\/pytorch\/pytorch\/blob\/506142ac8aebe0b3794ba66d6d6c4019fc2182c8\/tools\/autograd\/templates\/Functions.cpp#L1544\"\n\n\npytorch\/pytorch\/blob\/506142ac8aebe0b3794ba66d6d6c4019fc2182c8\/tools\/autograd\/templates\/Functions.cpp#L1544 with link \"https:\/\/github.com\/pytorch\/pytorch\/blob\/506142ac8aebe0b3794ba66d6d6c4019fc2182c8\/tools\/autograd\/templates\/Functions.cpp#L1544\"\n<code class=\"lang-cpp\">\n  }\n  v_term = u.mm(v_term);\n} else {\n  v_term = at::zeros({1}, self.type()).expand_as(self);\n}\n\n\nreturn u_term + sigma_term + v_term;\n}\n\n\n\/\/ http:\/\/eprints.maths.ox.ac.uk\/1079\/1\/NA-08-01.pdf\nTensor symeig_backward(const std::vector<torch::autograd::Variable> &amp;grads, const Tensor&amp; self,\n                  bool eigenvectors, bool upper, const Tensor&amp; lambda, const Tensor&amp; v) {\n  auto glambda = grads[0];\n  auto gv = grads[1];\n\n\n  auto vt = v.t();\n\n\n  if (!eigenvectors) {\n      throw std::runtime_error(std::string(\"cannot compute backward without \"\n                                           \"computing eigenvectors in forward pass\"));\n  }\n<\/code>\n\n\n\n\n\n\nBest regards\nThomas\n:  I understand that the current functionality is available in torch, but this piece of code is only a simplified version of what I want to implement with the goal of finding the bugs.\nMy main goal is to compute the top-1 eigenvector and its derivative of a batch of matrices.\nCan you take a brief look and see my mistake in it?\nHi . I found the issue.  Eigen-decomposition requires a symmetric matrix as its input.  But the perturbations by gradcheck() makes the input asymmetric and eigen-decomposition fails.  I wrote my own numerical gradient checking function and the above function worked.\nHa cool. Glad you found it.\nCool, could you please show the final full code?\n\n\n\n taha:\n\ngrad_np = np.dot(np.dot(output.T, pinv).T, vecs[None, :, -1])\n\n\nHi, why do you compute gradients in this way?\n   should it be the kronecker product of pinv with vecs[None, :, -1]?\nHere I was interested in the top eigenvector only using Eq. (68) in the matrix cookbook with link \"http:\/\/www2.imm.dtu.dk\/pubdb\/views\/edoc_download.php\/3274\/pdf\/imm3274.pdf\".\nHere is my final code. However, I strongly recommend you to NOT use it; it is very slow. Please use the batch eigenvalue operations that are available starting from torch.1.2.\n<code class=\"lang-python\">import torch\nfrom torch.autograd import Function, gradcheck\nfrom torch import from_numpy\nimport numpy as np\n\nclass TopEigen(Function):\n    \"\"\"Copmpute the top eigenvector of a matrix.\"\"\"\n\n    \n    def flatten(bag, i):\n        return tuple([item for sublist in bag for item in sublist[i]])\n\n    \n    def _bdiag(matrix):\n        b, d = matrix.shape\n        dlams = np.zeros((b, d, d))\n        bag_ind = [[list(range(b)), [i]*b, [i]*b] for i in range(d)]\n        index = tuple([TopEigen.flatten(bag_ind, i) for i in range(d)])\n        dlams[index] = matrix.T.flatten()\n        return dlams\n\n    \n    def forward(ctx, tensor):\n        cov = tensor.detach().numpy()\n        lams, vecs = np.linalg.eigh(cov)\n        ctx.intrm = lams, vecs\n        return from_numpy(vecs[:, :, -1, None])\n\n    \n    def backward(ctx, grad_output):\n        lams, vecs = ctx.intrm\n        output = grad_output.data.numpy()\n        lams = lams[:, -1, None] - lams\n        index = np.where(abs(lams) > 1e-6)\n        lams[index] = 1\/lams[index]\n        dlams = TopEigen._bdiag(lams)\n        pinv = np.matmul(vecs, np.matmul(dlams, vecs.transpose((0, 2, 1))))\n        return from_numpy(np.matmul(np.matmul(output.transpose((0, 2, 1)),\n                          pinv).transpose((0, 2, 1)), vecs[:, None, :, -1]))\n<\/code>"},{"x":"I stumbled upon a rather weird issue while debugging a larger code in PyTorch. A simple instance to reproduce the issue is here: https:\/\/colab.research.google.com\/drive\/11kBaxMOxN9i0X1vtaX47Yz7rkkHz7Kpu\nAs you can see, the variable x gets updated in every epoch (see that their norms are different), however, the norm of their difference (rel_diff) turns out to be zero. Any help is deeply appreciated.","y":"Use x_old = x.detach().clone(), since .data will be a reference and point to the same location in x_old and x_new.","z":"Use x_old = x.detach().clone(), since .data will be a reference and point to the same location in x_old and x_new."},{"x":"Hi,\nI have two loss variables, A and B.\nWhen I do A.backward() everything works fine, but when I do B.backward() i get the following error:\nTypeError: backward() takes 2 positional arguments but 3 were given\nI expect that the difference is in the history of the variables but I cannot find it.\nAny ideas or directions will be appreciated.","y":"Hi,\nThe error in the second function is that since it has two outputs from the forward, it will get two grad_ inputs in the backward method. Here you expect only one hence the error.","z":"Hi,\nCould you give more details on the model you\u2019re using? If you have custon Module or Function in it?\nI have an architecture that consists largely of regular Modules and Functions, but also uses two custom Functions, each one associated with a different loss (I add their code below).\nThe loss functions themselves are also non-trivial but are all a series of torch functions and are probably okay so for now I will only add the code for the custom Functions to maintain brevity.\nThe custom Function for loss A is simply adding some noise to the input:\n<code class=\"lang-auto\">    \n    # bias is an optional argument\n    def forward(ctx, input, stdev):\n        normal_sample = torch.normal(torch.zeros(input.size()), torch.zeros(input.size()) + stdev).cuda()\n\n        # re-center the means to the input\n        output = input + normal_sample\n\n        ctx.stdev = stdev\n        ctx.mark_dirty(input)\n        ctx.save_for_backward(input, output)\n\n        return output\n\n    \n    def backward(ctx, grad_output):\n        input, output = ctx.saved_variables\n        stdev = ctx.stdev\n\n        tensor_output = output.data\n        tensor_input = input.data\n        tensor_output.normal_(0, stdev[0][0])\n\n        # re-center the means to the input\n        tensor_output.add(tensor_input)\n\n        del ctx.stdev\n        return Variable(tensor_output), None\n<\/code>\nThe custom function for loss B uses its input to weigh multinomial sampling and outputs the resulting indexing:\n<code class=\"lang-auto\">    \n    def forward(ctx, input):\n        one = torch.ones(1).cuda()\n\n        if len(input.size()) == 1:\n            input = torch.unsqueeze(input, 0)\n\n        output = torch.zeros(input.size())\n        if input.is_cuda:\n            output = output.cuda()\n\n        # sample from categorical with p = input\n        _index = torch.multinomial(input + constants.epsilon, 1, False)\n\n        output.scatter_(1, _index, torch.unsqueeze(one.repeat(_index.size()[0]),1))\n\n        ctx.mark_dirty(input)\n\n        ctx.save_for_backward(input, output)\n\n        return _index.float(), output\n\n    \n    def backward(ctx, grad_output):\n        input, output = ctx.saved_variables\n        gradInput = torch.zeros(input.size()).cuda()\n        gradInput.copy_(output)\n        gradInput.div_(input)\n        gradInput.mul_(grad_output)\n        return gradInput\n<\/code>\nHi,\nThe error in the second function is that since it has two outputs from the forward, it will get two grad_ inputs in the backward method. Here you expect only one hence the error.\nThis might be a dumb question, but I have a similar scenario and want to give multiple outputs from my custom forward function similar to what mordith was doing. One of these will need to be differentiated but the others will not. For example,\ndef forward(ctx, input)\n\u2026\nreturn differentiated_output, non_differentiated_output_for_diagnostics, another_one\nYour forward should mark the non differentiable outputs and the backward should return None for them."},{"x":"Hello!\nI\u2019m an experienced programmer, familiar to neural nets (theoretically) but new to pytorch. I\u2019m trying out pytorch in C++ and am currently writing the learning stage of my simple network (1 hidden layer).\nI have an optimizer and have calculated a loss tensor, all according to the tutorial found here with link \"https:\/\/pytorch.org\/tutorials\/advanced\/cpp_frontend.html\". The tutorial then calls d_loss_fake.backward(). I did some digging and this apparently updates the gradients of the network. I am confused however about how it achieves this, as I could not find any relations between the loss tensor and the network. Concretely: how does .backward() know which weights to update if there is no connection between the loss tensor and the network?\nSorry for the beginner question. I hope this isn\u2019t a duplicate, I\u2019ve been googling for a couple of hours now. Thanks in advance!","y":"Maybe a starting point is https:\/\/pytorch.org\/docs\/stable\/notes\/autograd.html\nBehind the scenes PyTorch tracks all operations on tensors with requires_grad == true and builds a computation graph during the forward pass. It knowns how the loss value was calculated and can automatically back-propagate the gradient step by step from the loss (or any scalar model output) to the model parameters.","z":"Maybe a starting point is https:\/\/pytorch.org\/docs\/stable\/notes\/autograd.html\nBehind the scenes PyTorch tracks all operations on tensors with requires_grad == true and builds a computation graph during the forward pass. It knowns how the loss value was calculated and can automatically back-propagate the gradient step by step from the loss (or any scalar model output) to the model parameters."},{"x":"I define my custom loss as follows:\n<code class=\"lang-auto\">def custom loss(target,output,mask):\n    output = torch.neg(output)\n    loss = torch.add(target,output)\n    loss = torch.pow(loss,2)\n    loss = torch.mul(loss,mask)\n    loss = torch.mean(loss)\n<\/code>\nI\u2019ve seen many people use a class. But I also read a function would be fine. Would that loss work? target,output and mask are Variable.\nSubsequently, in my training loop, I call loss.backward(). The program runs but I just want to confirm my implementation of the loss is correct.\nThanks","y":"Yes, your approach should work just fine.\nThe advantage of using a class is that it can hold internal attributes you would have to pass otherwise to your functional approach.\nE.g. using nn.CrossEntropyLoss, you could define the weight or reduction argument while creating an instance of this class. If you are using the functional method (F.cross_entropy), you would need to pass these arguments in each call.\nWhile this might be cumbersome in some cases, it might make the code cleaner e.g. if you need to change the weight argument often.\nBoth APIs have some advantages and disadvantages and you can chose whatever feels right and makes the code clean. ","z":"Yes, your approach should work just fine.\nThe advantage of using a class is that it can hold internal attributes you would have to pass otherwise to your functional approach.\nE.g. using nn.CrossEntropyLoss, you could define the weight or reduction argument while creating an instance of this class. If you are using the functional method (F.cross_entropy), you would need to pass these arguments in each call.\nWhile this might be cumbersome in some cases, it might make the code cleaner e.g. if you need to change the weight argument often.\nBoth APIs have some advantages and disadvantages and you can chose whatever feels right and makes the code clean. "},{"x":"I have trained a CNN model with softmax classification layer. Now, I want to get the class activation maps (CAM) from these trained model on some test-samples. I have found a code that does this using Keras, but cannot do the same thing in PyTorch.\nIn this Keras code, they compute the gradients of the predicted output with respect to the last convolutional layer. So, I wonder how I should compute these gradients in PyTorch.\nHere is the full Keras code:\n<code class=\"lang-auto\">    preds = model.predict(image)\n    class_idx = np.argmax(preds[0])\n    class_output = model.output[:, class_idx]\n    last_conv_layer = model.get_layer(\"mixed10\")\n\n    grads = K.gradients(class_output, last_conv_layer.output)[0]\n    pooled_grads = K.mean(grads, axis=(0, 1, 2))\n    iterate = K.function([model.input], [pooled_grads, last_conv_layer.output[0]])\n    pooled_grads_value, conv_layer_output_value = iterate([image])\n\n    for i in range(2048):\n        conv_layer_output_value[:, :, i] *= pooled_grads_value[i]\n\n    heatmap = np.mean(conv_layer_output_value, axis = -1)\n    heatmap = np.maximum(heatmap, 0)\n    heatmap \/= np.max(heatmap)\n<\/code>\nAny help is appreciate!","y":"How about defining a hook? Would that solve your problem? E.g., something like\n<code class=\"lang-python\">class MyModel(nn.Module):\n    def __init__(self):\n        super(MyModel, self).__init__()\n \n        self.gradients = []\n\n        ## your layers\n        self.some_conv_layer.register_backward_hook(self.save_gradients)\n        \n    def forward(self, x):\n        ## your forward pass\n        return F.softmax(logits)\n\n    def save_gradients(self, module, grad_input, grad_output):\n        self.gradients.append(grad_output)\n\nmodel = MyModel()\noutput = model(x)\noutput.backward()\n\nprint(model.gradients)\n<\/code>","z":"How about defining a hook? Would that solve your problem? E.g., something like\n<code class=\"lang-python\">class MyModel(nn.Module):\n    def __init__(self):\n        super(MyModel, self).__init__()\n \n        self.gradients = []\n\n        ## your layers\n        self.some_conv_layer.register_backward_hook(self.save_gradients)\n        \n    def forward(self, x):\n        ## your forward pass\n        return F.softmax(logits)\n\n    def save_gradients(self, module, grad_input, grad_output):\n        self.gradients.append(grad_output)\n\nmodel = MyModel()\noutput = model(x)\noutput.backward()\n\nprint(model.gradients)\n<\/code>\nThank you very much! I think that solved the problem, and with that I should be able to get the activation maps.\nHi everyone , Is there a way to achieve the same using c++ frontend ?\nThank you."},{"x":"Are these operations fundamentally different?","y":"reshape tries to return a view if possible, otherwise copies to data to a contiguous tensor and returns the view on it. From the docs with link \"https:\/\/pytorch.org\/docs\/stable\/torch.html#torch.reshape\":\n\nReturns a tensor with the same data and number of elements as input , but with the specified shape. When possible, the returned tensor will be a view of input . Otherwise, it will be a copy. Contiguous inputs and inputs with compatible strides can be reshaped without copying, but you should not depend on the copying vs. viewing behavior.\nSee torch.Tensor.view() with link \"https:\/\/pytorch.org\/docs\/stable\/tensors.html#torch.Tensor.view\" on when it is possible to return a view.\nA single dimension may be -1, in which case it\u2019s inferred from the remaining dimensions and the number of elements in input .\n\nHave a look at this example to demonstrate this behavior:\n<code class=\"lang-python\">x = torch.arange(4*10*2).view(4, 10, 2)\ny = x.permute(2, 0, 1)\n\n# View works on contiguous tensors\nprint(x.is_contiguous())\nprint(x.view(-1))\n\n# Reshape works on non-contugous tensors (contiguous() + view)\nprint(y.is_contiguous())\ntry: \n    print(y.view(-1))\nexcept RuntimeError as e:\n    print(e)\nprint(y.reshape(-1))\nprint(y.contiguous().view(-1))\n<\/code>\npermute is quite different to view and reshape:\n<code class=\"lang-python\"># View vs. permute\nx = torch.arange(2*4).view(2, 4)\nprint(x.view(4, 2))\n> tensor([[0, 1],\n          [2, 3],\n          [4, 5],\n          [6, 7]])\nprint(x.permute(1, 0))\n> tensor([[0, 4],\n          [1, 5],\n          [2, 6],\n          [3, 7]])\n<\/code>","z":"reshape tries to return a view if possible, otherwise copies to data to a contiguous tensor and returns the view on it. From the docs with link \"https:\/\/pytorch.org\/docs\/stable\/torch.html#torch.reshape\":\n\nReturns a tensor with the same data and number of elements as input , but with the specified shape. When possible, the returned tensor will be a view of input . Otherwise, it will be a copy. Contiguous inputs and inputs with compatible strides can be reshaped without copying, but you should not depend on the copying vs. viewing behavior.\nSee torch.Tensor.view() with link \"https:\/\/pytorch.org\/docs\/stable\/tensors.html#torch.Tensor.view\" on when it is possible to return a view.\nA single dimension may be -1, in which case it\u2019s inferred from the remaining dimensions and the number of elements in input .\n\nHave a look at this example to demonstrate this behavior:\n<code class=\"lang-python\">x = torch.arange(4*10*2).view(4, 10, 2)\ny = x.permute(2, 0, 1)\n\n# View works on contiguous tensors\nprint(x.is_contiguous())\nprint(x.view(-1))\n\n# Reshape works on non-contugous tensors (contiguous() + view)\nprint(y.is_contiguous())\ntry: \n    print(y.view(-1))\nexcept RuntimeError as e:\n    print(e)\nprint(y.reshape(-1))\nprint(y.contiguous().view(-1))\n<\/code>\npermute is quite different to view and reshape:\n<code class=\"lang-python\"># View vs. permute\nx = torch.arange(2*4).view(2, 4)\nprint(x.view(4, 2))\n> tensor([[0, 1],\n          [2, 3],\n          [4, 5],\n          [6, 7]])\nprint(x.permute(1, 0))\n> tensor([[0, 4],\n          [1, 5],\n          [2, 6],\n          [3, 7]])\n<\/code>\n\nso when training a model, is it best to use view?\nIs using reshape a source of possible bugs in terms of gradient flow?\n\nAnything to be careful otherwise as well?\nThanks!\n\n\nNot necessarily. The usage of view and reshape does not depend on training \/ not-training.\nI personally use view whenever possible and add a contiguous call to it, if necessary. This will make sure I see, where a copy is done in my code. reshape on the other hand does this automatically, so your code might look cleaner.\n\n\nNo, it should not be a bug regarding the gradient flow. However, as shown with the reshape vs. permute example, the wrong operator might of course cause problems in your training. E.g. if you would like to swap some axes of an image tensor from NCHW to NHWC, you should use permute.\n\n"},{"x":"Hello guys!\nI\u2019m getting the following error when calling .backward():\nEncounter the RuntimeError: one of the variables needed for gradient computation has been modified by an inplace operation\n<code class=\"lang-auto\">for i, j, k in zip(X, Y, Z):\n    A[:, i, j] = A[:, i, j] + k\n<\/code>\nX and Y are leaf tensors. Z is a non-leaf tensor resulting from the previous layer.\nX, Y pairs are non-unique: Ex. (3, 9, 0.5), (3, 9, 0.4) should result in (3, 9, 0.9)\nI\u2019ve tried .clone(), torch.add(), torch.sum(), and so on.\nPlease help! ","y":"Hi  ,\nIf you don\u2019t absolutely need these operations to be in-place, this should work:\n<code class=\"lang-auto\">B = A.clone()\nfor i, j, k in zip(X, Y, Z):\n    B[:, i, j] = A[:, i, j] + k\n<\/code>\nor this what you are referring to when you say \u201cI\u2019ve tried .clone()\u201d?","z":"Hi  ,\nIf you don\u2019t absolutely need these operations to be in-place, this should work:\n<code class=\"lang-auto\">B = A.clone()\nfor i, j, k in zip(X, Y, Z):\n    B[:, i, j] = A[:, i, j] + k\n<\/code>\nor this what you are referring to when you say \u201cI\u2019ve tried .clone()\u201d?\nThanks Serge!  I was using it wrong "},{"x":"Hi,\nI have a simple question that I fail to understand, how does the loss which is usually a single value, propagate correctly in a network with multiple output nodes.\nI\u2019ll give an example, assuming I have a network with X inputs and 2 outputs. The network get pictures of cats or dogs or something else. And should output high value in output node 1 for dog (low value if no dog), and high value for output node 2 for cats. (And low values for no cat).\nAssuming I use MSE for my loss function, or some other loss function (CrossEntropyLoss) which outputs a single value, how does the loss propagate correctly for each node?\nFor example, let say I gave the network a batch with 1 pictures of a cat, and I got a vector of 2 (2 output nodes) which was (1,1) - therefor for node-2 I do not need to propagate any error but for node 2 I do need to propagate error. But when I do MSE I get a single loss value, how does this single loss value propagate correctly to each node.\nMy intuition would be that I\u2019ll have a loss which is a vector a loss for node-1 and a loss for node-2 and that each loss will propagate to his node. But as I said the MSE gives back a single loss value.\nThanks,","y":"Hi ,\nFirst, in autograd terms, leaf nodes are the input of the forward (the last  on which backpropagation will be applied) and the root the output of the forward network.\nRemember that when you use MSELoss, the last operation to be applied is a reduction (by default Mean but you can also choose sum with the reduction with link \"https:\/\/pytorch.org\/docs\/stable\/nn.html#torch.nn.MSELoss\" arg ).\nSo the first backward operation to be computed will the mean, as you can see here:\n<code class=\"lang-auto\">>>> loss(y, t)\ntensor(0.1328, grad_fn=<MeanBackward0>) # <==\n<\/code>\nan operation which will propagate the gradient to every output node contributing to this mean.\nI hope everything is clear. ","z":"Hi ,\nFirst, in autograd terms, leaf nodes are the input of the forward (the last  on which backpropagation will be applied) and the root the output of the forward network.\nRemember that when you use MSELoss, the last operation to be applied is a reduction (by default Mean but you can also choose sum with the reduction with link \"https:\/\/pytorch.org\/docs\/stable\/nn.html#torch.nn.MSELoss\" arg ).\nSo the first backward operation to be computed will the mean, as you can see here:\n<code class=\"lang-auto\">>>> loss(y, t)\ntensor(0.1328, grad_fn=<MeanBackward0>) # <==\n<\/code>\nan operation which will propagate the gradient to every output node contributing to this mean.\nI hope everything is clear. "},{"x":"Hello, I am trying to compute the std for an image, but I want a different std for each channel. Suppose we have an image with the shape  [channel,x,y], in numpy we can achieve this by using multi-axis, like:  img.std(axis=(1,2))\nUnfortunately, the torch std implementation does not support multi-axis, so I tryed  to implement my own function:\n<code class=\"lang-python\">def std_for_channel(img):\n   if (torch.typename(img) == 'torch.autograd.variable.Variable'):\n      num_channels = img.size()[0]\n   else:\n      num_channels = img.shape[0]\n   out = torch.zeros(num_channels)\n   for i in range(num_channels):\n      out[i] = img[i,:,:].std(unbiased=False)\n   return out\n<\/code>\nwhile this works when passing a tensor, I keep getting error when using this on a Variable (getting \u2018RuntimeError: can\u2019t assign a Variable to a scalar value of type float\u2019). Since I want to use this std information in my custom loss, how can I implement it correctly so backpropagation will be able to train the weights of the network?","y":"img.contiguous().view(img.size(0), -1).std(-1)","z":"img.contiguous().view(img.size(0), -1).std(-1)\nThank you very much.\nSorry for bumping this, but for anyone who comes across this and it seems like magic\u2026\nimg.contiguous() makes sure the tensor is in a contiguous chunk of memory for efficient access.\n.view(img.size(0), -1) resizes from [c, x, y, z\u2026etc] to [c, n] where n = xyz\u2026\n.std(-1) takes the standard deviation when you collapse the last channel so you end up with the channel mean.\nIf the last command is confusing, it\u2019s because pytorch dims can be confusing, here is a nice explanation\nUnderstanding dimensions in pytorch with link \"https:\/\/towardsdatascience.com\/understanding-dimensions-in-pytorch-6edf9972d3be\""},{"x":"In the following program, b.grad is None but a.grad is not None. Why is that?\n<code class=\"lang-auto\">    a = torch.tensor([[[[1.0]]]]).requires_grad_()     \n    b = torch.nn.functional.interpolate(a, size=(2, 2))\n    loss = torch.sum(b)           \n    loss.backward()               \n    b.grad                        \n    a.grad\n<\/code>","y":"\nAlso you can use:\n<code class=\"lang-auto\">b.retain_grad()\n<\/code>\nto keep the grad.\nPlease find more info here with link \"https:\/\/pytorch.org\/docs\/stable\/autograd.html#torch.Tensor.is_leaf\"","z":"Hi ,\nb grad is None since it is a non-leaf Tensor.\nIn your example:\n<code class=\"lang-auto\">>>> a.is_leaf\nTrue\n>>> b.is_leaf\nFalse\n<\/code>\nThe gradient will be calculated in the during the backward phase (since it is needed by a) but it won\u2019t kept in grad.\n\nAlso you can use:\n<code class=\"lang-auto\">b.retain_grad()\n<\/code>\nto keep the grad.\nPlease find more info here with link \"https:\/\/pytorch.org\/docs\/stable\/autograd.html#torch.Tensor.is_leaf\"\nAs  said, the grad won\u2019t be kept by default, so you might want to call b.retain_grad() before calling backward."},{"x":"Hi. I am reproducing a model, depicted in the following.\nmodel1180\u00d7465 83.6 KB with link \"https:\/\/discuss.pytorch.org\/uploads\/default\/original\/3X\/9\/3\/932601d2a6039d45f95e5621de04a01f0844c590.jpeg\"\nI have created a simple model for it as shown below, but i need help in \u201cHow to iterate the convolutional layer twice or thrice as shown in above figure\u201d\nCapture-1546\u00d7737 15.6 KB with link \"https:\/\/discuss.pytorch.org\/uploads\/default\/original\/3X\/6\/d\/6d7c91b3740561688f6222fd845499087818cacc.png\"","y":"Hi,\nIn case of implementing repetitive blocks, you can set up an entry block and sequential repetitive blocks after the entry block. Implementing it as a module would be :\n<code class=\"lang-auto\">class Conv_rep(nn.Module):\n      def __init__(self, in_channels, out_channels,num_reps, kernel_size=3, stride=1,\n                 padding=1, dilation=1, bias=False):\n               \n           super(Conv_rep,self).__init__(in_channels, out_channels,num_reps, kernel_size, \n                                       stride, padding, dilation,bias)\n\n           self.conv_entry=nn.Conv2d(in_channels=in_channels,out_channels=out_channels,\n                           kernel_size=kernel_size,stride=stride,padding=padding)\n           self.conv_rep=nn.Conv2d(in_channels=out_channels,out_channels=out_channels,\n                           kernel_size=kernel_size,stride=stride,padding=padding)\n           self.num_reps=num_reps\n      \n      def forward(self,x):\n           \n           x_rep=self.conv_entry(x)\n           \n           for i in range(1,self.num_rep):\n                   x_rep=self.conv_rep(x_rep)\n           return x_rep\n<\/code>\nNote that I haven\u2019t added activations or Batch Normalization as nothing of that sort was mentioned, but you should add them according to your model and use-case [as I see you haven\u2019t used activations in your \u2018Net\u2019 module]","z":"Hi,\nI think by the repetition, they mean having same blocks connected to each other sequentially rather than passing input to a conv layer multiple times.\nIn the latter case, for instance, if you have a 3-channel input, then after first conv block, you will get 64-channel tensor. Then how can one use same conv block which only accepts 3-channel input for your current tensor which has 64-channels after first iteration? Do they provided any information about it?\nThe reason I think they just meant to have same block is that they wanted to simply model structure in the given table. Altought you can make sure if they provided information about model size or number of parameters.\nBests\nHi,\nIn case of implementing repetitive blocks, you can set up an entry block and sequential repetitive blocks after the entry block. Implementing it as a module would be :\n<code class=\"lang-auto\">class Conv_rep(nn.Module):\n      def __init__(self, in_channels, out_channels,num_reps, kernel_size=3, stride=1,\n                 padding=1, dilation=1, bias=False):\n               \n           super(Conv_rep,self).__init__(in_channels, out_channels,num_reps, kernel_size, \n                                       stride, padding, dilation,bias)\n\n           self.conv_entry=nn.Conv2d(in_channels=in_channels,out_channels=out_channels,\n                           kernel_size=kernel_size,stride=stride,padding=padding)\n           self.conv_rep=nn.Conv2d(in_channels=out_channels,out_channels=out_channels,\n                           kernel_size=kernel_size,stride=stride,padding=padding)\n           self.num_reps=num_reps\n      \n      def forward(self,x):\n           \n           x_rep=self.conv_entry(x)\n           \n           for i in range(1,self.num_rep):\n                   x_rep=self.conv_rep(x_rep)\n           return x_rep\n<\/code>\nNote that I haven\u2019t added activations or Batch Normalization as nothing of that sort was mentioned, but you should add them according to your model and use-case [as I see you haven\u2019t used activations in your \u2018Net\u2019 module]\nThank Dear \nThanks Dear "},{"x":"Hello,\nI\u2019m trying to implement a custom autograd function where the output of the forward pass is the solution of an optimization problem. I\u2019d like to solve this optimization problem with a new sub graph defined inside the autograd function, during the forward pass.\nHowever, it seems impossible to create a sub graph in the forward. (requires_grad is automatically disabled for the child, but I don\u2019t have any warning)\nWould you know how I could fix this issue ?\nThanks a lot for your help\nBruno","y":"For a similar problem, have a look at https:\/\/github.com\/leowalkling\/sparsemap\/blob\/master\/python\/sparsemap\/layers_pt\/base.py (a fork from vene\/sparsemap with link \"https:\/\/github.com\/vene\/sparsemap\/blob\/master\/python\/sparsemap\/layers_pt\/base.py\")","z":"I think the answer depends on your exact problem, maybe you could give more details on your optimization procedure.\n\nWhy does your optimization procedure need to be in the forward() staticmethod of some Function anyway?\nYou could also precompute the Jacobian of your operation in the forward pass and use a custom Function to create an output tensor (result of your optimization) which would apply that Jacobian in its backward() staticmethod.\ntorch.autograd.enable_grad would be necessary within your forward() staticmethod.\n\nHi,\nThank you very much !\nAbout the optimization procedure, I\u2019d like to solve the problem with a gradient descent inside my Function.\nI\u2019d like to do something like this:\nforward(X,D)\n(nested SGD solver)\nreturn A* = argmin_A ||X-DA||^2\n(Of course, my nested problem is less trivial so I can\u2019t find gradients by hand)\n\n\nI am not sure I fully understand this solution. Or I am not sure it could work this way.\nActually I am doing some bi-level optimization. The gradient return by the backward method is not obtained with the graph. (It is computed based on some optimality KKT conditions).\n\n\nI\u2019ll take a look at this thanks a lot\n\n\nFor a similar problem, have a look at https:\/\/github.com\/leowalkling\/sparsemap\/blob\/master\/python\/sparsemap\/layers_pt\/base.py (a fork from vene\/sparsemap with link \"https:\/\/github.com\/vene\/sparsemap\/blob\/master\/python\/sparsemap\/layers_pt\/base.py\")"},{"x":"I was looking for some information on affine transforms and came across post on a thread: Affine transformation matrix paramters conversion with link \"https:\/\/discuss.pytorch.org\/t\/affine-transformation-matrix-paramters-conversion\/19522\/2\"\nWhen I try to print(x_trans.grad_fn) at the end, it prints None. Can someone help me understand why no gradients are calculated for this operation?","y":"If no parameters in the operations require a gradient, you won\u2019t get a grad_fn.\nYou could set requires_grad=True e.g. for x, which will give you a valid grad_fn in x_trans.","z":"If no parameters in the operations require a gradient, you won\u2019t get a grad_fn.\nYou could set requires_grad=True e.g. for x, which will give you a valid grad_fn in x_trans."},{"x":"Hello,\ni write a toy code to check SGD weight_decay.\nbut it seems to have no effect to the gradient update.\nam i misunderstand the meaning of weight_decay?\nthank you very much.\nPyTorch 1.0\n\n<code class=\"lang-auto\">import torch\nimport numpy as np\n\nnp.random.seed(123)\nnp.set_printoptions(8, suppress=True)\n\nx_numpy = np.random.random((3, 4)).astype(np.double)\nw_numpy = np.random.random((4, 5)).astype(np.double)\nx_torch = torch.tensor(x_numpy, requires_grad=True)\nw_torch = torch.tensor(w_numpy, requires_grad=True)\n\nlr = 0.1\nsgd = torch.optim.SGD([w_torch], lr=lr, weight_decay=0.9)\n\ny_torch = torch.matmul(x_torch, w_torch)\nloss = y_torch.sum()\n\nprint(\"w_torch before SGD\")\nprint(w_torch.data.numpy())\n\nsgd.zero_grad()\nloss.backward()\nsgd.step()\n\nw_grad = w_torch.grad.data.numpy()\n\nprint(\"w_torch after SGD\")\nprint(w_torch.data.numpy())\n\nprint(\"check_weight_decay\")\nprint(w_numpy - lr * w_grad)\n<\/code>\n<code class=\"lang-auto\">\"\"\"\ncode output :\n\nw_torch before SGD\n[[ 0.43857224  0.0596779   0.39804426  0.73799541  0.18249173]\n [ 0.17545176  0.53155137  0.53182759  0.63440096  0.84943179]\n [ 0.72445532  0.61102351  0.72244338  0.32295891  0.36178866]\n [ 0.22826323  0.29371405  0.63097612  0.09210494  0.43370117]]\nw_torch after SGD\n[[ 0.20941374 -0.13538012  0.17253327  0.48188881 -0.02361953]\n [ 0.04952477  0.37357542  0.37382677  0.46716854  0.6628466 ]\n [ 0.50417498  0.40095203  0.50234411  0.13881324  0.17414831]\n [ 0.01120012  0.07076036  0.37766885 -0.11270393  0.19814865]]\ncheck_weight_decay\n[[ 0.20941374 -0.13538012  0.17253327  0.48188881 -0.02361953]\n [ 0.04952477  0.37357542  0.37382677  0.46716854  0.6628466 ]\n [ 0.50417498  0.40095203  0.50234411  0.13881324  0.17414831]\n [ 0.01120012  0.07076036  0.37766885 -0.11270393  0.19814865]]\n\"\"\"\n<\/code>","y":"The weight_decay parameter adds a L2 penalty to the cost which can effectively lead to to smaller model weights. It seems to work in my case:\n<code class=\"lang-python\">import torch\nimport numpy as np\n\nnp.random.seed(123)\nnp.set_printoptions(8, suppress=True)\n\nx_numpy = np.random.random((3, 4)).astype(np.double)\nw_numpy = np.random.random((4, 5)).astype(np.double)\nx_torch = torch.tensor(x_numpy, requires_grad=True)\nw_torch = torch.tensor(w_numpy, requires_grad=True)\n\n#######################################################\n\nprint('Original weights', w_torch)\n\nlr = 0.1\nsgd = torch.optim.SGD([w_torch], lr=lr, weight_decay=0)\n\ny_torch = torch.matmul(x_torch, w_torch)\nloss = y_torch.sum()\n\nsgd.zero_grad()\nloss.backward()\nsgd.step()\n\nw_grad = w_torch.grad.data.numpy()\nprint('0 weight decay', w_torch)\n\n\n#######################################################\n\nw_torch = torch.tensor(w_numpy, requires_grad=True)\n\nprint('Reset Original weights', w_torch)\n\nsgd = torch.optim.SGD([w_torch], lr=lr, weight_decay=1)\n\ny_torch = torch.matmul(x_torch, w_torch)\nloss = y_torch.sum()\n\nsgd.zero_grad()\nloss.backward()\nsgd.step()\n\nw_grad = w_torch.grad.data.numpy()\nprint('1 weight decay', w_torch)\n<\/code>\nThis returns\n<code class=\"lang-auto\">Original weights tensor([[0.4386, 0.0597, 0.3980, 0.7380, 0.1825],\n        [0.1755, 0.5316, 0.5318, 0.6344, 0.8494],\n        [0.7245, 0.6110, 0.7224, 0.3230, 0.3618],\n        [0.2283, 0.2937, 0.6310, 0.0921, 0.4337]],\n       dtype=torch.float64, requires_grad=True)\n0 weight decay tensor([[ 0.2489, -0.1300,  0.2084,  0.5483, -0.0072],\n        [ 0.0653,  0.4214,  0.4217,  0.5243,  0.7393],\n        [ 0.5694,  0.4559,  0.5674,  0.1679,  0.2067],\n        [ 0.0317,  0.0972,  0.4345, -0.1044,  0.2372]],\n       dtype=torch.float64, requires_grad=True)\nReset Original weights tensor([[0.4386, 0.0597, 0.3980, 0.7380, 0.1825],\n        [0.1755, 0.5316, 0.5318, 0.6344, 0.8494],\n        [0.7245, 0.6110, 0.7224, 0.3230, 0.3618],\n        [0.2283, 0.2937, 0.6310, 0.0921, 0.4337]],\n       dtype=torch.float64, requires_grad=True)\n1 weight decay tensor([[ 0.2050, -0.1360,  0.1686,  0.4745, -0.0254],\n        [ 0.0478,  0.3683,  0.3685,  0.4608,  0.6544],\n        [ 0.4969,  0.3948,  0.4951,  0.1356,  0.1705],\n        [ 0.0089,  0.0678,  0.3714, -0.1136,  0.1938]],\n       dtype=torch.float64, requires_grad=True)\n<\/code>\nAs you can see, the weights are smaller when I use weight_decay=1 compared to weight_decay=0","z":"The weight_decay parameter adds a L2 penalty to the cost which can effectively lead to to smaller model weights. It seems to work in my case:\n<code class=\"lang-python\">import torch\nimport numpy as np\n\nnp.random.seed(123)\nnp.set_printoptions(8, suppress=True)\n\nx_numpy = np.random.random((3, 4)).astype(np.double)\nw_numpy = np.random.random((4, 5)).astype(np.double)\nx_torch = torch.tensor(x_numpy, requires_grad=True)\nw_torch = torch.tensor(w_numpy, requires_grad=True)\n\n#######################################################\n\nprint('Original weights', w_torch)\n\nlr = 0.1\nsgd = torch.optim.SGD([w_torch], lr=lr, weight_decay=0)\n\ny_torch = torch.matmul(x_torch, w_torch)\nloss = y_torch.sum()\n\nsgd.zero_grad()\nloss.backward()\nsgd.step()\n\nw_grad = w_torch.grad.data.numpy()\nprint('0 weight decay', w_torch)\n\n\n#######################################################\n\nw_torch = torch.tensor(w_numpy, requires_grad=True)\n\nprint('Reset Original weights', w_torch)\n\nsgd = torch.optim.SGD([w_torch], lr=lr, weight_decay=1)\n\ny_torch = torch.matmul(x_torch, w_torch)\nloss = y_torch.sum()\n\nsgd.zero_grad()\nloss.backward()\nsgd.step()\n\nw_grad = w_torch.grad.data.numpy()\nprint('1 weight decay', w_torch)\n<\/code>\nThis returns\n<code class=\"lang-auto\">Original weights tensor([[0.4386, 0.0597, 0.3980, 0.7380, 0.1825],\n        [0.1755, 0.5316, 0.5318, 0.6344, 0.8494],\n        [0.7245, 0.6110, 0.7224, 0.3230, 0.3618],\n        [0.2283, 0.2937, 0.6310, 0.0921, 0.4337]],\n       dtype=torch.float64, requires_grad=True)\n0 weight decay tensor([[ 0.2489, -0.1300,  0.2084,  0.5483, -0.0072],\n        [ 0.0653,  0.4214,  0.4217,  0.5243,  0.7393],\n        [ 0.5694,  0.4559,  0.5674,  0.1679,  0.2067],\n        [ 0.0317,  0.0972,  0.4345, -0.1044,  0.2372]],\n       dtype=torch.float64, requires_grad=True)\nReset Original weights tensor([[0.4386, 0.0597, 0.3980, 0.7380, 0.1825],\n        [0.1755, 0.5316, 0.5318, 0.6344, 0.8494],\n        [0.7245, 0.6110, 0.7224, 0.3230, 0.3618],\n        [0.2283, 0.2937, 0.6310, 0.0921, 0.4337]],\n       dtype=torch.float64, requires_grad=True)\n1 weight decay tensor([[ 0.2050, -0.1360,  0.1686,  0.4745, -0.0254],\n        [ 0.0478,  0.3683,  0.3685,  0.4608,  0.6544],\n        [ 0.4969,  0.3948,  0.4951,  0.1356,  0.1705],\n        [ 0.0089,  0.0678,  0.3714, -0.1136,  0.1938]],\n       dtype=torch.float64, requires_grad=True)\n<\/code>\nAs you can see, the weights are smaller when I use weight_decay=1 compared to weight_decay=0\nHello, rasbt\nreply.png1379\u00d7734 53.2 KB with link \"https:\/\/discuss.pytorch.org\/uploads\/default\/original\/2X\/3\/39a9f417cb836820174d71e9a9318d5ccc6191f3.png\"\nhere is another check code :\n<code class=\"lang-auto\">import torch\nimport numpy as np\n\nnp.random.seed(123)\nnp.set_printoptions(8, suppress=True)\n\nx_numpy = np.random.random((3, 4)).astype(np.double)\nx_torch = torch.tensor(x_numpy, requires_grad=True)\nx_torch2 = torch.tensor(x_numpy, requires_grad=True)\n\nw_numpy = np.random.random((4, 5)).astype(np.double)\nw_torch = torch.tensor(w_numpy, requires_grad=True)\nw_torch2 = torch.tensor(w_numpy, requires_grad=True)\n\nlr = 0.1\nweight_decay = 0.9\nsgd = torch.optim.SGD([w_torch], lr=lr, weight_decay=0)\nsgd2 = torch.optim.SGD([w_torch2], lr=lr, weight_decay=weight_decay)\n\ny_torch = torch.matmul(x_torch, w_torch)\ny_torch2 = torch.matmul(x_torch2, w_torch2)\n\nloss = y_torch.sum()\nloss2 = y_torch2.sum()\n\nsgd.zero_grad()\nsgd2.zero_grad()\n\nloss.backward()\nloss2.backward()\n\nsgd.step()\nsgd2.step()\n\nw_grad = w_torch.grad.data.numpy()\nw_grad2 = w_torch2.grad.data.numpy()\n\nprint(\"check_grad\")\nprint(w_grad + weight_decay * w_numpy)\nprint(w_grad2)\n\n\"\"\"\ncheck_grad\n[[ 2.29158508  1.95058016  2.25510989  2.56106592  2.06111261]\n [ 1.25926989  1.57975955  1.58000814  1.67232418  1.86585193]\n [ 2.20280346  2.10071483  2.20099271  1.84145669  1.87640346]\n [ 2.17063112  2.22953686  2.53307273  2.04808866  2.35552527]]\n[[ 2.29158508  1.95058016  2.25510989  2.56106592  2.06111261]\n [ 1.25926989  1.57975955  1.58000814  1.67232418  1.86585193]\n [ 2.20280346  2.10071483  2.20099271  1.84145669  1.87640346]\n [ 2.17063112  2.22953686  2.53307273  2.04808866  2.35552527]]\n \"\"\"\n<\/code>\nThe part that I circled doesn\u2019t seem right to me:\n39a9f417cb836820174d71e9a9318d5ccc6191f3.png1379\u00d7734 65.7 KB with link \"https:\/\/discuss.pytorch.org\/uploads\/default\/original\/2X\/e\/e1761b0ce81dd7cff5679379168d2e5f1b6bec70.png\"\nIn L2 regularization, you modify the cost as follows\ntext_Regularized.png2262\u00d7325 21.5 KB with link \"https:\/\/discuss.pytorch.org\/uploads\/default\/original\/2X\/4\/44ad32d9502e88846fc36bb31c06c7df8ef97c69.png\"\nThe weight update should be then\ntext_updatedWeig.png2954\u00d7154 25.4 KB with link \"https:\/\/discuss.pytorch.org\/uploads\/default\/original\/2X\/e\/e831c3e6a017fcf2fd3ae177c71c928274772d57.png\"\nThe way PyTorch applied the weight decay seems correct to me (you can drop the factor 2)\nIf you look closely,  the formula that you circled is just a rearrangement of the usual SGD weight decay formula I guess.\nOoops, you are right, they are exactly the same.\nI have a doubt here. In pytorch the weigh decay,  is it only applied to the weighs or to all the parameters that requires gradient? I mean for instance if I use this piece of code:\noptimizer = torch.optim.Adam(model.parameters(), lr=args.lr, betas=args.betas, weight_decay=args.wd)\nWill be the weight decay applied to all the parameters of the model including bias and batchnorm parameters?\nThanks\n<code class=\"lang-auto\">   def step(self, closure=None):\n        \"\"\"Performs a single optimization step.\n\n        Arguments:\n            closure (callable, optional): A closure that reevaluates the model\n                and returns the loss.\n        \"\"\"\n        loss = None\n        if closure is not None:\n            loss = closure()\n\n        for group in self.param_groups:\n            weight_decay = group['weight_decay']\n            momentum = group['momentum']\n            dampening = group['dampening']\n            nesterov = group['nesterov']\n\n            for p in group['params']:\n                if p.grad is None:\n                    continue\n                d_p = p.grad.data\n                if weight_decay != 0:\n                    d_p.add_(weight_decay, p.data)\n                if momentum != 0:\n                    param_state = self.state[p]\n                    if 'momentum_buffer' not in param_state:\n                        buf = param_state['momentum_buffer'] = torch.clone(d_p).detach()\n                    else:\n                        buf = param_state['momentum_buffer']\n                        buf.mul_(momentum).add_(1 - dampening, d_p)\n                    if nesterov:\n                        d_p = d_p.add(momentum, buf)\n                    else:\n                        d_p = buf\n\n                p.data.add_(-group['lr'], d_p)\n\n        return loss\n<\/code>\n<code class=\"lang-python\"> d_p = p.grad.data\n if weight_decay != 0:\n     d_p.add_(weight_decay, p.data)\n<\/code>\nso\u2026 weight_decay * w is already included in w_grad\nThanks for your reply but it is not answering my question  . I will try to explain better. My concern is to know if the weigh_decay functionality is able to distinguish between weighs, bias and learning parameters of batchnorm. Because, Normally weight decay is only applied to the weights and not to the bias and batchnorm parameters (do not make sense to apply a weight decay to the batchnorm parameters). For this reason I am asking if the weigh decay is able to distinguish between this kind of parameters. My feelings after see the code is that weight_decay functionality is not able to distinguis between these parameters, but I would like to have the confirmation."},{"x":"Hi,\nI\u2019m working on a custom layer which uses the unfold operator.\nI would like to compute the second order derivatives (for some hessian vector products\u2026). However I have the following error:\n\nderivative for _thnn_im2col_backward is not implemented\n\nCan anyone tell me how to solve this? Or are there any workaround\/hacks to solve this?\nthanks for your help","y":"Hi,\nWhen implemented in python, a forward\/backward pair is defined by a Function. You can see here with link \"https:\/\/pytorch.org\/docs\/stable\/notes\/extending.html\" in the doc how in details.\nBy default, if higher order derivatives are required, the autograd is applied to the backward function that is defined.\nYou can manually defined second derivatives by using 2 Functions, where the backward of the first one is simply calling the forward of the second one.\nYou also have a decorator  to notify the engine that a backward is implemented in a non differentiable manner. If you ask for second derivatives for such a function, the error you see is raised.\nFor the cpp implementations, most of them are not implemented in a differentiable manner hence the error you see.","z":"Hi,\nThe problem is that the second derivative of the unfold operation is not implemented.\nYou would need a function that computes this second derivative or the first derivative to be implemented in a differentiable way with the autograd engine.\nHello,\nThanks a lot!\n\nYou would need a function that computes this second derivative\n\nHow does it work exactly ? Should I write a backward for the backward ? I didn\u2019t find many implementations of such layers.\n\nthe first derivative to be implemented in a differentiable way with the autograd engine.\n\nDo you mean that as long as the backward is implemented in a differentiable way, it will automatically handle double diff ?\nthank you\nHi,\nWhen implemented in python, a forward\/backward pair is defined by a Function. You can see here with link \"https:\/\/pytorch.org\/docs\/stable\/notes\/extending.html\" in the doc how in details.\nBy default, if higher order derivatives are required, the autograd is applied to the backward function that is defined.\nYou can manually defined second derivatives by using 2 Functions, where the backward of the first one is simply calling the forward of the second one.\nYou also have a decorator  to notify the engine that a backward is implemented in a non differentiable manner. If you ask for second derivatives for such a function, the error you see is raised.\nFor the cpp implementations, most of them are not implemented in a differentiable manner hence the error you see.\nAwesome, it really helps ! Thank you\nBruno"},{"x":"What kind of operations can I perform on tensors while still being able to compute gradients? For instance when I try\nx = torch.rand(1, requires_grad=True)\ny = x+2\nprint(y.grad_fn)\nI get <AddBackward0 object at 0x7fd459552320>\nbut when I try something like\nx = torch.rand(1, requires_grad=True)\ny = torch.tensor([x , 0])\nprint(y.grad_fn)\nI get None.","y":"Recreating a tensor will break the computation graph.\nIf you would like to concatenate two tensors, you should use torch.cat or torch.stack instead:\n<code class=\"lang-python\">x = torch.rand(1, requires_grad=True)\ny = torch.cat([x , torch.tensor([0.])])\nprint(y.grad_fn)\n<CatBackward object at 0x7fdb08dca6d8>\n<\/code>","z":"Recreating a tensor will break the computation graph.\nIf you would like to concatenate two tensors, you should use torch.cat or torch.stack instead:\n<code class=\"lang-python\">x = torch.rand(1, requires_grad=True)\ny = torch.cat([x , torch.tensor([0.])])\nprint(y.grad_fn)\n<CatBackward object at 0x7fdb08dca6d8>\n<\/code>"},{"x":"i understood that\n\neval() changes the bn and dropout layer\u2019s behaviour\ntorch.no_grad() deals with the autograd engine and stops it from calculating the gradients, which is not compulsory but the recommended way of doing validation\n\nBUT, I didn\u2019t understand the use of  with torch.set_grad_enabled()\nCan somebody pls explain what is its use and where exactly can it be used.","y":"\n\n\n\n'model.eval()' vs 'with torch.no_grad()' with link \"https:\/\/discuss.pytorch.org\/t\/model-eval-vs-with-torch-no-grad\/19615\/29\"\n\n\n    are you saying that torch.no_grad  and torch.set_grad_enabled(False) are the same ?\n  \n\n","z":"\n\n\n\n'model.eval()' vs 'with torch.no_grad()' with link \"https:\/\/discuss.pytorch.org\/t\/model-eval-vs-with-torch-no-grad\/19615\/29\"\n\n\n    are you saying that torch.no_grad  and torch.set_grad_enabled(False) are the same ?\n  \n\n"},{"x":"how to backward if my loss is something about the derivative of the net output?\nfor example:\nmy input is \u2018u\u2019, then my net output can be seemed as f(u).\nand my target is target = a*( f \u2019 (u)|u ) + b,        \" f \u2019 (u)|u\" means the derivative of \u2019 f \u2019 w.r.t \u2018u\u2019\nso my loss may will be \u2018| ground truth - target |\u2019.\nIs this kind of loss can be backwarded in pytorch?\nI tried to backward something about the derivative, but it can\u2019t work.\nhere is my code :\nimport torch\nfrom torch.autograd import Variable\nx = Variable(torch.ones(2, 2), requires_grad=True)\ny = x + 2\nout = y\nout.backward(torch.ones(2, 2))\nk = 2*x.grad\nk.backward()\nthe error is :\nk.backward()\nRuntimeError: element 0 of variables tuple is volatile\nit seems like pytorch will not save the parameters when it calculates the derivative.\nHas there any method to solve this kind of questions?\nThank you.","y":"Hi,\nFor this to work you need to pass the create_graph=True option to the first .backward() to let it know that you need to be able to call .backward() on the grad itself.\nAlso if you need the gradient wrt a specific variable, you can use the autograd.grad(outputs, inputs) with link \"https:\/\/pytorch.org\/docs\/stable\/autograd.html#torch.autograd.grad\" function to get the derivative of the output(s) wrt the input(s). For example out.backward() is equivalent to autograd.grad(out, model.parameters()).","z":"Hi,\nFor this to work you need to pass the create_graph=True option to the first .backward() to let it know that you need to be able to call .backward() on the grad itself.\nAlso if you need the gradient wrt a specific variable, you can use the autograd.grad(outputs, inputs) with link \"https:\/\/pytorch.org\/docs\/stable\/autograd.html#torch.autograd.grad\" function to get the derivative of the output(s) wrt the input(s). For example out.backward() is equivalent to autograd.grad(out, model.parameters()).\nIt works !!!\nSorry for I haven\u2019t read the document completely.\nThank you !\nThank you, I have been looking for an example on how to use autograd api. IMO, a small example of using autograd.grad api should be added to pytorch doc. Furthermore, examples of higher-order derivatives would help as well. Say, for example Double Backpropagation, especially since loss.backward(create_graph=True) no longer works, causing memory leak due to some changes in C++ backend(weak to strong pointers), issue page with link \"https:\/\/github.com\/pytorch\/pytorch\/issues\/4661\"."},{"x":"Hi\nI am trying to understand\/instrument autograd in PyTorch. I\u2019ve put a pdb.set_trace() before backward and traced the code until Variable._execution_engine.run_backward() is called which prevents pdb to step into. I presume this is where the code calls C++ extensions.\nIf so, is there a way to continue debugging and step into this function?\nThanks.","y":"The cpp engine is based on Function with link \"https:\/\/github.com\/pytorch\/pytorch\/blob\/master\/torch\/csrc\/autograd\/function.h\" which are similar to the python ones. They are the elementary operations that are considered.\nThe forward pass attaches a grad_fn to the Tensors during the forward pass. Then you have a graph of Functions stored in cpp. Accessed from the next_functions field.\nWhen computing a backward, the engine traverses this graph. The entry point in cpp is here with link \"https:\/\/github.com\/pytorch\/pytorch\/blob\/master\/torch\/csrc\/autograd\/engine.cpp#L536\" and you can look in the same file for all the code.","z":"Hi,\nThis function and the whole autograd engine is implemented in cpp.\nYou will need a cpp debugger if you really want to step into it. It might not be very helpful though depending on what you\u2019re trying to achieve?\nYou are right. Following yours and others advice in this thread with link \"https:\/\/discuss.pytorch.org\/t\/how-to-use-pdb-or-gdb-debug-from-python-into-c-c-code\/8152\", I cloned PyTorch\u2019s source, built it with debugging flags, and set a pdb_trace point before backward. However I don\u2019t actually know what happens after Variable._execution_engine.run_backward(), therefore I can\u2019t put a breakpoint using gdb on called C++ function, so that it would stop when it reaches the underlying code.\nAs for my purpose, while I know autograd\u2019s high level functionality,  I am trying to study\/tweak\/understand how it is implemented.\nThe cpp engine is based on Function with link \"https:\/\/github.com\/pytorch\/pytorch\/blob\/master\/torch\/csrc\/autograd\/function.h\" which are similar to the python ones. They are the elementary operations that are considered.\nThe forward pass attaches a grad_fn to the Tensors during the forward pass. Then you have a graph of Functions stored in cpp. Accessed from the next_functions field.\nWhen computing a backward, the engine traverses this graph. The entry point in cpp is here with link \"https:\/\/github.com\/pytorch\/pytorch\/blob\/master\/torch\/csrc\/autograd\/engine.cpp#L536\" and you can look in the same file for all the code.\nWhere can I find the implementaion for add-edge, sub-edge, multiple-edge and so on ??\nHi,\nWhere did you see these functions?\nI didnt see these function, but I know pytorch can do any differential operator, so it might be some files that implement these differentiation\nI\u2019m not sure what you expect these functions to be \/ do?\nThere are no functions to manipulate the graph explicitly. It is built when performing operations but cannot be changed.\nThe <a>file<\/a> you told me is actually what I expected to find, but in cpp files.\nwhat is like \u2026\ny=x**4\nand we can get gradient of y wrt x as 4x^3\nso the code must return something like nx^(n-1)\nimport torch\nx = torch.autograd.Variable(torch.Tensor([2]),requires_grad=True)\ny = 5*x**2\ny.backward()\nx.grad\nwe get 20 because 5x^2 -> 10x = 10 * 2 = 20\nAnd I would like to know the code inside backward() ,  there might be some function return 10 * input , right?\nHi,\nThis is actually never built explicitly.\nWhat happens is that when the functions of the forward are seen, they corresponding backward ops are recorded.\nAnd so if you say y = f( g(x) ) where g is the square function and f is time 5, noting z = g(x), what the backward computes is dy\/dx as dy\/dz * dz\/dx = df(z)\/dz * dg(x)\/dx = 5 * 2x. Note that the 2x is computed first by the backward of g then 5 times that result for the backward of f.\nBasically pytorch is just doing backprop, each elementary functions one at a time. Nothing more fancy.\ng(x)=x^2 ,but where pytorch compute dg(x)\/dx as 2x or any those elementary function backward.\nThis is what I want to find out.\nAlso, thanks you for answer my question!!\nYou can find here with link \"https:\/\/github.com\/pytorch\/pytorch\/blob\/d71501259b759066e39024e0b2c6a86d975a0bb5\/tools\/autograd\/derivatives.yaml#L664-L665\" the lines that define tha backward of the pow functions as the pow_backward functions.\nIf you look for this function, you will find it here with link \"https:\/\/github.com\/pytorch\/pytorch\/blob\/6615797837917ab7f0cd956779ad702994c0479c\/tools\/autograd\/templates\/Functions.cpp#L136-L143\" with a slightly more general formula for any power.\nAlso remember that these always compute the backward pass. So if we have o = f(i)and then o is used to compute some loss L. it computes dL\/di = dL\/do * do\/di. dL\/do is given in the grad argument of the function.\nThis ->  templates\/Functions.cpp is what I want !!!\nTHX A LOT "},{"x":"Hello everyone,\nI was trying to reproduce a model where the activation functions is defined as:\nx \/ sqrt(1 + x**2)\nwere x are vectors\nI was wondering how can I define this operation and still benefit from autograd magic.\nThank you very much in advance.","y":"Hi,\nI think that if you write a simple function like\nimport torch\ndef sqrt_act_fun(x):\nnum = x\nden = torch.sqrt(1.0 + torch.mul(x,x))\nreturn torch.div(num, dev)\nProvided that \u201cx\u201d is a torch tensor and that you use the output to compute (directly or indirectly) the loss value L, then the gradients will be automatically handled by the autograd package when you call L.backward().\nAlternatively you can define a custom nn.Module, overriding its forward() method with the same code as in the function above, and then use it as a new layer in a DNN (for example).\nHope this helps ","z":"Hi,\nI think that if you write a simple function like\nimport torch\ndef sqrt_act_fun(x):\nnum = x\nden = torch.sqrt(1.0 + torch.mul(x,x))\nreturn torch.div(num, dev)\nProvided that \u201cx\u201d is a torch tensor and that you use the output to compute (directly or indirectly) the loss value L, then the gradients will be automatically handled by the autograd package when you call L.backward().\nAlternatively you can define a custom nn.Module, overriding its forward() method with the same code as in the function above, and then use it as a new layer in a DNN (for example).\nHope this helps \nThank you very much.\nI tried the first solution you propose and worked perfectly!!"},{"x":"Hi , thanks for your explanation about the definition of  _convolution_double_backward() with link \"https:\/\/github.com\/pytorch\/pytorch\/blob\/master\/aten\/src\/ATen\/native\/Convolution.cpp\". When reading this code, I was confused about the parameter like ggI, ggW_r, ggb and gO_r and output_mask and the target of calculating ggW, gI and gW.\nUnder my superficial understanding of all these, ggW_r means the gradient of convolution kernel? xx_r means a requirement, ggb is the gradient of bias.\nReally hope your advice on how to realize these parameters and the calculation of ggW, gI and gW.\nThank you very much!","y":"Hi,\nI\u2019m not sure about the *_r variables. Most likely some some cpp reasons.\nThe gg* variables contain the gradient of the gradient of something.\nSince the forward is I, W, b -> O\nthe backward is gO, I, W -> gI, gW, gb\nand the double backward ggI, ggW, ggb -> ggO, gI, gW","z":"Hi,\nI\u2019m not sure about the *_r variables. Most likely some some cpp reasons.\nThe gg* variables contain the gradient of the gradient of something.\nSince the forward is I, W, b -> O\nthe backward is gO, I, W -> gI, gW, gb\nand the double backward ggI, ggW, ggb -> ggO, gI, gW\nvery appreciate to your answer, thank you !"},{"x":"Hi, can someone tell me what I need to do to back-propagate when I am working with stacked tensors? For instance, when I try a simple matrix multiplication of two 3 x 3 matrices and work back from there, it works. But for the same case, if I split one of the matrices into 3 individual 3 x 1 matrices and stack them and then do the multiplication, it doesn\u2019t seem to work.\n<code class=\"lang-auto\">import torch\n\ndevice = torch.device('cpu')\n\nx = torch.tensor([[1,2,3],[4,5,6],[7,8,9]], dtype=torch.float)\ny = torch.tensor([[ 2.8191,  4.3610,  3.7487],\n        [ 3.8279, 12.9571, 10.9117],\n        [ 4.8367, 21.5532, 18.0747]])\n\nw1 = torch.randn(3, 1, device=device, requires_grad=True)\nw2 = torch.randn(3, 1, device=device, requires_grad=True)\nw3 = torch.randn(3, 1, device=device, requires_grad=True)\nw = torch.stack((w1, w2, w3))\nw.squeeze_(-1)\nprint(w)\n\n\nlearning_rate = 1e-5\nfor t in range(20000):\n  y_pred = x.mm(w)\n  loss = (y_pred - y).pow(2).sum()\n  if t%5000==0:\n    print(t, loss.item())\n  loss.backward()\n  with torch.no_grad():\n#     print(\"Grad=\",w1.grad)\n    w1 -= learning_rate * w1.grad\n#     print(w1, learning_rate * w1.grad)\n    w2 -= learning_rate * w2.grad\n    w3 -= learning_rate * w3.grad\n\n    # Manually zero the gradients after running the backward pass\n    w1.grad.zero_()\n    w2.grad.zero_()\n    w3.grad.zero_()\n    \nprint(y_pred)\n<\/code>","y":"I think your problem is you are not updating w\u2026\n<code class=\"lang-auto\">import torch\ndevice = torch.device('cpu')\nx = torch.tensor([[1, 2, 3], [4, 5, 6], [7, 8, 9]], dtype=torch.float).requires_grad_()\ny = torch.tensor([[2.8191, 4.3610, 3.7487],\n                  [3.8279, 12.9571, 10.9117],\n                  [4.8367, 21.5532, 18.0747]])\nw1 = torch.randn(3, 1, device=device, requires_grad=True)\nw2 = torch.randn(3, 1, device=device, requires_grad=True)\nw3 = torch.randn(3, 1, device=device, requires_grad=True)\nprint(w)\nlearning_rate = 1e-5\nfor t in range(20000):\n    w = torch.stack((w1, w2, w3))\n    w.squeeze_(-1)\n    y_pred = x.mm(w)\n    loss = (y_pred - y).pow(2).sum()\n    if t % 5000 == 0:\n        print(t, loss.item())\n        print(w1.grad)\n    loss.backward()\n    with torch.no_grad():\n        #     print(\"Grad=\",w1.grad)\n        w1 -= learning_rate * w1.grad\n        #     print(w1, learning_rate * w1.grad)\n        w2 -= learning_rate * w2.grad\n        w3 -= learning_rate * w3.grad\n        # Manually zero the gradients after running the backward pass\n        w1.grad.zero_()\n        w2.grad.zero_()\n        w3.grad.zero_()\nprint(y_pred)\ntensor([[-0.5458, -0.4896, -0.2024],\n        [-1.2095,  0.0565, -2.5812],\n        [ 0.4733,  0.0300, -1.0872]], grad_fn=<SqueezeBackward3>)\n0 3298.955322265625\nNone\n5000 4.48732852935791\ntensor([[0.],\n        [0.],\n        [0.]])\n10000 3.5715510845184326\ntensor([[0.],\n        [0.],\n        [0.]])\n15000 2.842688798904419\ntensor([[0.],\n        [0.],\n        [0.]])\ntensor([[ 2.1616,  5.4135,  4.2396],\n        [ 3.6429, 13.2533, 11.0498],\n        [ 5.1242, 21.0930, 17.8600]], grad_fn=<MmBackward>)\n<\/code>\nYou stacked once at the begining but you have to keep doing it.\nIn addition you should use different name variables for w1 after backprop, saving original names or you loose variable scope.","z":"I think your problem is you are not updating w\u2026\n<code class=\"lang-auto\">import torch\ndevice = torch.device('cpu')\nx = torch.tensor([[1, 2, 3], [4, 5, 6], [7, 8, 9]], dtype=torch.float).requires_grad_()\ny = torch.tensor([[2.8191, 4.3610, 3.7487],\n                  [3.8279, 12.9571, 10.9117],\n                  [4.8367, 21.5532, 18.0747]])\nw1 = torch.randn(3, 1, device=device, requires_grad=True)\nw2 = torch.randn(3, 1, device=device, requires_grad=True)\nw3 = torch.randn(3, 1, device=device, requires_grad=True)\nprint(w)\nlearning_rate = 1e-5\nfor t in range(20000):\n    w = torch.stack((w1, w2, w3))\n    w.squeeze_(-1)\n    y_pred = x.mm(w)\n    loss = (y_pred - y).pow(2).sum()\n    if t % 5000 == 0:\n        print(t, loss.item())\n        print(w1.grad)\n    loss.backward()\n    with torch.no_grad():\n        #     print(\"Grad=\",w1.grad)\n        w1 -= learning_rate * w1.grad\n        #     print(w1, learning_rate * w1.grad)\n        w2 -= learning_rate * w2.grad\n        w3 -= learning_rate * w3.grad\n        # Manually zero the gradients after running the backward pass\n        w1.grad.zero_()\n        w2.grad.zero_()\n        w3.grad.zero_()\nprint(y_pred)\ntensor([[-0.5458, -0.4896, -0.2024],\n        [-1.2095,  0.0565, -2.5812],\n        [ 0.4733,  0.0300, -1.0872]], grad_fn=<SqueezeBackward3>)\n0 3298.955322265625\nNone\n5000 4.48732852935791\ntensor([[0.],\n        [0.],\n        [0.]])\n10000 3.5715510845184326\ntensor([[0.],\n        [0.],\n        [0.]])\n15000 2.842688798904419\ntensor([[0.],\n        [0.],\n        [0.]])\ntensor([[ 2.1616,  5.4135,  4.2396],\n        [ 3.6429, 13.2533, 11.0498],\n        [ 5.1242, 21.0930, 17.8600]], grad_fn=<MmBackward>)\n<\/code>\nYou stacked once at the begining but you have to keep doing it.\nIn addition you should use different name variables for w1 after backprop, saving original names or you loose variable scope.\nThat should have been painfully obvious  Thank you for pointing it out!"},{"x":"I\u2019m trying to understand how autograd works. Suppose I have the following code:\n<code class=\"lang-auto\">x = Variable(torch.arange(0, 4).double(), requires_grad=True)\n\ny = Variable(x * 2, requires_grad=True)\n\nz = y * x\n\nz.backward(torch.ones(y.size()).double())\ny.sum().backward()\n\nprint(x.grad, 2*x)\n<\/code>\nIt seems to be working correctly. The problem is that if I change the 3rd line to z = Variable(y*x, requires_grad=True), then it does not work anymore. What\u2019s the difference? Thank you.","y":"Is your error something along the lines of?\nRuntimeError: Trying to backward through the graph a second time ...\n\n?\nThat\u2019s because the graph gets deleted after it\u2019s been evaluated via .backward(). If you want to backpropate through the graph a second time. You can set the argument \"retain_graph=True in your first backward call.\nAlso, please note that Variable has been deprecated since PyTorch 0.4 (multiple versions ago) and it\u2019s better to use the torch.tensor constructor. E.g., the following should work:\n<code class=\"lang-python\">x = torch.tensor(torch.arange(0, 4).double(), requires_grad=True)\n\ny = x.detach().requires_grad_(True)*2\n\nz = y * x\n\nz.backward(torch.ones(y.size()).double(), retain_graph=True)\n\ny.sum().backward()\n\nprint(x.grad, 2*x)\n<\/code>","z":"Is your error something along the lines of?\nRuntimeError: Trying to backward through the graph a second time ...\n\n?\nThat\u2019s because the graph gets deleted after it\u2019s been evaluated via .backward(). If you want to backpropate through the graph a second time. You can set the argument \"retain_graph=True in your first backward call.\nAlso, please note that Variable has been deprecated since PyTorch 0.4 (multiple versions ago) and it\u2019s better to use the torch.tensor constructor. E.g., the following should work:\n<code class=\"lang-python\">x = torch.tensor(torch.arange(0, 4).double(), requires_grad=True)\n\ny = x.detach().requires_grad_(True)*2\n\nz = y * x\n\nz.backward(torch.ones(y.size()).double(), retain_graph=True)\n\ny.sum().backward()\n\nprint(x.grad, 2*x)\n<\/code>"},{"x":"For the sake of the example, let\u2019s say I don\u2019t use Dropout, BatchNorm etc, just a plain CNN.\nAccording to the docs (in PyTorch 0.4),\nwith torch.set_grad_enabled(is_train)\n\nprevents tracking via autograd, which would make the inference mode more efficient (I assume). Now, if I would use model.eval(), would this have the same effect. E.g., does the following track gradients after model.eval()\n<code class=\"lang-python\">model = CNN()\nfor e in num_epochs:\n    # do training\n\n# evaluate model:\nmodel = model.eval()\nlogits, probas = model(testset_features)\n<\/code>\nor is it recommended, in addition, to do the following:\n<code class=\"lang-python\">model = CNN()\nfor e in num_epochs:\n    # do training\n\n# evaluate model:\nmodel = model.eval()\nwith torch.set_grad_enabled(False):\n    logits, probas = model(testset_features)\n<\/code>","y":"eval doesn\u2019t turn off history tracking.","z":"I think it is the latter. model.eval() has effect on dropout, batchnorm etc. You can use model.eval() in combination with with torch.no_grad() during inference phase.\nfrom the docs:\nDisabling gradient calculation is useful for inference, when you are sure that you will not call Tensor.backward(). It will reduce memory consumption for computations that would otherwise have requires_grad=True. In this mode, the result of every computation will have requires_grad=False, even when the inputs have requires_grad=True.\neval doesn\u2019t turn off history tracking.\nthanks  and \nI also assumed that eval() mode automatically turns off gradient computation. Hopefully you can see why this might be confusing for us newcomers. I would request to emphasize this point in docs at nn.Module\u2019s eval() function. Actually apart from FAQ, an article pointing out common mistakes and confusions would be great. Thanks.\nExactly! Emm, so is this article available now?\n\n\n\n Shihab_Shahriar:\n\nActually apart from FAQ, an article pointing out common mistakes and confusions would be great.\n\n"},{"x":"I have been trying to use grad-cam for a custom model I made in pytorch but can\u2019t figure out how to do it. The model I made is a classifier model using ResNet50. Any help on how I can use gradcam to create a heatmap on images with based on my model would be really appreaciated.\nThank you!","y":"Captum provides a Guided GradCam implementation here with link \"https:\/\/captum.ai\/api\/guided_grad_cam.html\", which might be useful. ","z":"Captum provides a Guided GradCam implementation here with link \"https:\/\/captum.ai\/api\/guided_grad_cam.html\", which might be useful. \nThanks! I\u2019ll have a look"},{"x":"Hi,\nI am optimizing a tensor called z in my training loop. z is a leaf tensor. I want to do some experiments how my model behaves when each epoch a little noise is added to z after the gradients were backpropagated.\nWhen I do this\n<code class=\"lang-python\">my_adam = optim.Adam([z], lr=self.lr_shape)\n\n# and this comes in the training loop:\nloss.backward()\nmy_adam.step()\nz[some_indices] = z[some_indices] + my_noise_tensor\n<\/code>\nI get\n\nRuntimeError: a leaf Variable that requires grad has been used in an in-place operation.\n\nHowever, when I do this instead\nz.data[some_indices] = z[some_indices] + my_noise_tensor\nit works without an error. Is this a safe way to do it or can this cause some problems with autograd or other problems that I might be missing? I mean is it safe to write to .data directly?\nWhat is the recommended way to modify a leaf variable manually in the training loop?","y":"Yeah, that\u2019s fine here. The new style to express this is to use torch.no_grad() to signify that you don\u2019t want to track the gradient of this operation (but z.data is OK too):\n<code class=\"lang-auto\">with torch.no_grad():\n  z[some_indices] += my_noise_tensor\n<\/code>","z":"Yeah, that\u2019s fine here. The new style to express this is to use torch.no_grad() to signify that you don\u2019t want to track the gradient of this operation (but z.data is OK too):\n<code class=\"lang-auto\">with torch.no_grad():\n  z[some_indices] += my_noise_tensor\n<\/code>"},{"x":"I\u2019m implementing an RBF network by using some beginner examples from Pytorch Website. I have a problem when implementing the kernel bandwidth differentiation for the network. Also, I would like to know whether my attempt to implement the idea is fine. This is a code sample to reproduce the issue. Thanks\n<code class=\"lang-auto\"># -*- coding: utf-8 -*-\nimport torch\nfrom torch.autograd import Variable\n\n\ndef kernel_product(x,y, mode = \"gaussian\", s = 1.):\n    x_i = x.unsqueeze(1)\n    y_j = y.unsqueeze(0)\n    xmy = ((x_i-y_j)**2).sum(2)\n\n    if   mode == \"gaussian\" : K = torch.exp( - xmy\/s**2) )\n    elif mode == \"laplace\"  : K = torch.exp( - torch.sqrt(xmy + (s**2)))\n    elif mode == \"energy\"   : K = torch.pow(   xmy + (s**2), -.25 )\n\n    return torch.t(K)\n\n\nclass MyReLU(torch.autograd.Function):\n    \"\"\"\n    We can implement our own custom autograd Functions by subclassing\n    torch.autograd.Function and implementing the forward and backward passes\n    which operate on Tensors.\n    \"\"\"\n\n    \n    def forward(ctx, input):\n        \"\"\"\n        In the forward pass we receive a Tensor containing the input and return\n        a Tensor containing the output. ctx is a context object that can be used\n        to stash information for backward computation. You can cache arbitrary\n        objects for use in the backward pass using the ctx.save_for_backward method.\n        \"\"\"\n        ctx.save_for_backward(input)\n        return input.clamp(min=0)\n\n    \n    def backward(ctx, grad_output):\n        \"\"\"\n        In the backward pass we receive a Tensor containing the gradient of the loss\n        with respect to the output, and we need to compute the gradient of the loss\n        with respect to the input.\n        \"\"\"\n        input, = ctx.saved_tensors\n        grad_input = grad_output.clone()\n        grad_input[input < 0] = 0\n        return grad_input\n\n\ndtype = torch.cuda.FloatTensor\nN, D_in, H, D_out = 64, 1000, 100, 10\n\n# Create random Tensors to hold input and outputs, and wrap them in Variables.\nx = Variable(torch.randn(N, D_in).type(dtype), requires_grad=False)\ny = Variable(torch.randn(N, D_out).type(dtype), requires_grad=False)\n\n# Create random Tensors for weights, and wrap them in Variables.\nw1 = Variable(torch.randn(H, D_in).type(dtype), requires_grad=True)\nw2 = Variable(torch.randn(H, D_out).type(dtype), requires_grad=True)\n\n# I've created this scalar variable (the kernel bandwidth)\ns = Variable(torch.randn(1).type(dtype), requires_grad=True)\n\nlearning_rate = 1e-6\nfor t in range(500):\n    # To apply our Function, we use Function.apply method. We alias this as 'relu'.\n    relu = MyReLU.apply\n\n    # Forward pass: compute predicted y using operations on Variables; we compute\n    # ReLU using our custom autograd operation.\n#    y_pred = relu(x.mm(w1)).mm(w2)\n    y_pred = relu(kernel_product(w1, x, s)).mm(w2)\n\n    # Compute and print loss\n    loss = (y_pred - y).pow(2).sum()\n    print(t, loss.data[0])\n\n\n    # Use autograd to compute the backward pass.\n    loss.backward()\n\n    # Update weights using gradient descent\n    w1.data -= learning_rate * w1.grad.data\n    w2.data -= learning_rate * w2.grad.data\n\n    # Manually zero the gradients after updating weights\n    w1.grad.data.zero_()\n    w2.grad.data.zero_()\n<\/code>\nHowever I get this error, which disappears when I simply use a fixed scalar in the default input parameter of kernel_product():\n<code class=\"lang-auto\">RuntimeError: eq() received an invalid combination of arguments - got (str), but expected one of:\n * (float other)\n      didn't match because some of the arguments have invalid types: (str)\n * (Variable other)\n      didn't match because some of the arguments have invalid types: (str)\n<\/code>\nThank you for your help","y":"In the following line you are missing the mode parameter to kernel_product.\n<code class=\"lang-auto\">y_pred = relu(kernel_product(w1, x, s)).mm(w2)\n<\/code>\nThis would be better\n<code class=\"lang-auto\">y_pred = relu(kernel_product(w1, x, \"gaussian\", s)).mm(w2)\n<\/code>","z":"In the following line you are missing the mode parameter to kernel_product.\n<code class=\"lang-auto\">y_pred = relu(kernel_product(w1, x, s)).mm(w2)\n<\/code>\nThis would be better\n<code class=\"lang-auto\">y_pred = relu(kernel_product(w1, x, \"gaussian\", s)).mm(w2)\n<\/code>\nI don\u2019t see how do you update the bandwidth in this case?"},{"x":"I\u2019m trying to implement a model which is similar to the Unet based the attached architecture. (Supplementary materials for:\nDeepLearningforSegmentationusinganOpenLarge-ScaleDatasetin2DEchocardiography)\nPicture1.png608\u00d71125 155 KB with link \"https:\/\/discuss.pytorch.org\/uploads\/default\/original\/2X\/8\/8dc5c8ceb82f5f1e11f3e5c2d3920013488afb2e.png\"\nI used this implementation with link \"https:\/\/github.com\/ptrblck\/pytorch_misc\/blob\/master\/unet_demo.py\" but I did change it based on the attached pic.\nhere is the Unet 2 implementation:\n<code class=\"lang-auto\">class BaseConv(nn.Module):\n    def __init__(self, in_channels, out_channels, kernel_size, padding, stride, droup_rate = False):\n        super(BaseConv, self).__init__()\n        self.act = nn.ReLU()\n        self.conv1 = nn.Conv2d(in_channels, out_channels, kernel_size, padding, stride)\n        self.b1 = nn.BatchNorm2d(out_channels, eps=1e-05, momentum=0.1, affine=True)\n        self.conv2 = nn.Conv2d(out_channels, out_channels, kernel_size, padding, stride)\n        self.b2 = nn.BatchNorm2d(out_channels, eps=1e-05, momentum=0.1, affine=True)\n                \n    def forward(self, x):\n        x = self.conv1(x)\n        x = self.act(self.b1(x))\n        x = self.conv2(x)\n        x = self.act(self.b2(x))\n\n        return x\n    \n    \nclass DownConv(nn.Module):\n    def __init__(self, in_channels, out_channels, kernel_size, padding, stride):\n        super(DownConv, self).__init__()\n        self.pool1 = nn.MaxPool2d(kernel_size=2)\n        self.conv_block = BaseConv(in_channels, out_channels, kernel_size, padding, stride)\n    \n    def forward(self, x):\n        x = self.pool1(x)\n        x = self.conv_block(x)\n        return x\n\n\nclass UpConv(nn.Module):\n    def __init__ (self, in_channels, in_channels_skip, out_channels, kernel_size, padding, stride):\n        super(UpConv, self).__init__()\n        self.act = nn.ReLU()\n        self.conv_trans1 = nn.ConvTranspose2d(in_channels, in_channels, kernel_size=2, padding=0, stride=2)\n        self.b3 = nn.BatchNorm2d(in_channels, eps=1e-05, momentum=0.1, affine=True)\n        self.conv3 = nn.Conv2d(in_channels=in_channels + in_channels_skip, out_channels= out_channels, kernel_size=kernel_size, padding=padding, stride=stride)\n        self.b4 = nn.BatchNorm2d(out_channels, eps=1e-05, momentum=0.1, affine=True)\n        self.conv4 = nn.Conv2d(out_channels, out_channels, kernel_size=kernel_size, padding=padding, stride=stride)\n        self.b5 = nn.BatchNorm2d(out_channels, eps=1e-05, momentum=0.1, affine=True)\n\n\n    def forward(self, x, x_skip):\n        x = self.conv_trans1(x)\n        x = self.act(self.b3(x))\n        x = self.conv3(x)\n        x = torch.cat((x, x_skip), dim=1)\n        x = self.act(self.b4(x))\n        x = self.conv4(x)\n        x = self.act(self.b5(x))\n       \n        return x\n    \n     \n        \nclass UNet(nn.Module):\n    def __init__(self, in_channels, out_channels, n_class, kernel_size, padding, stride, droup_rate = False):\n        super(UNet, self).__init__()\n\n        self.down1 = DownConv(in_channels, out_channels, kernel_size, padding, stride)#48\n        \n        self.down2 = DownConv(out_channels, 2 * out_channels, kernel_size, padding, stride)#96\n\n        self.down3 = DownConv(2 * out_channels, 4 * out_channels, kernel_size, padding, stride)#192\n\n        self.down4 = DownConv(4 * out_channels, 8 * out_channels, kernel_size, padding, stride)#384\n        \n        self.down5 = BaseConv(8 * out_channels, 16 * out_channels, kernel_size, padding, stride)#768\n               \n\n        \n        self.up4 = UpConv(16 * out_channels, 8 * out_channels, 8 * out_channels, kernel_size, padding, stride)\n        \n        self.up3 = UpConv(8 * out_channels, 4 * out_channels, 4 * out_channels,  kernel_size, padding, stride)\n\n        self.up2 = UpConv(4 * out_channels, 2 * out_channels, 2 * out_channels, kernel_size, padding, stride)\n\n        self.up1 = UpConv(2 * out_channels, out_channels, out_channels, kernel_size, padding, stride)\n        \n        self.out = nn.Conv2d(out_channels, n_class, kernel_size, padding, stride)\n\n        \n    def forward(self, x):\n        # Encoder\n        x1 = self.down1(x)\n        x2 = self.down2(x1)\n        x3 = self.down3(x2)\n        x4 = self.down4(x3)\n        x5 = self.down5(x4)\n        \n\n        # Decoder\n        x_up1 = self.up4(x5, x4)\n        x_up2 = self.up3(x_up1, x3)\n        x_up3 = self.up2(x_up2, x2)\n        x_up4 = self.up1(x_up3, x1)\n\n        \n        x_out = F.log_softmax(self.out(x_up4), 1)\n        print(x_out.size())\n        return x_out\n \n    \nmodel = UNet(in_channels=1,\n             out_channels=48,\n             n_class=2,\n             kernel_size=3,\n             padding=1,\n             stride=1)\n\nmodel = model.to(device)\n#print(model)\nprint(\"UNet model created\")\n\n\n#Print model's state_dict\nprint(\"Model's state_dict:\")\nfor param_tensor in model.state_dict():\n     print(param_tensor, \"\\t\", model.state_dict()[param_tensor].size())\n<\/code>\nCurrently, x_up1 causing an error RuntimeError: Given groups=1, weight of size [384, 1152, 3, 3], expected input[1, 768, 30, 40] to have 1152 channels, but got 768 channels instead\nI\u2019m not sure where I am doing wrong. any comments would be appreciated.","y":"Edit: Think I got it wrong the first time. This time I\u2019m concating asap in the UpConv and changed the out_channels in all the UpConv layers so it matches the kernel size from the picture (Kernel \/ Pool size). Also changed the order in the UpConv forward function. I haven\u2019t doublechecked but let me know if this is what you\u2019re looking for.\n<code class=\"lang-auto\">import torch\nimport torch.nn as nn\nimport torch.nn.functional as F\n\nclass BaseConv(nn.Module):\n    def __init__(self, in_channels, out_channels, kernel_size, padding, stride, droup_rate = False):\n        super(BaseConv, self).__init__()\n        self.act = nn.ReLU()\n        self.conv1 = nn.Conv2d(in_channels, out_channels, kernel_size, padding, stride)\n        self.b1 = nn.BatchNorm2d(out_channels, eps=1e-05, momentum=0.1, affine=True)\n        self.conv2 = nn.Conv2d(out_channels, out_channels, kernel_size, padding, stride)\n        self.b2 = nn.BatchNorm2d(out_channels, eps=1e-05, momentum=0.1, affine=True)\n                \n    def forward(self, x):\n        x = self.conv1(x)\n        x = self.act(self.b1(x))\n        x = self.conv2(x)\n        x = self.act(self.b2(x))\n\n        return x\n    \n    \nclass DownConv(nn.Module):\n    def __init__(self, in_channels, out_channels, kernel_size, padding, stride):\n        super(DownConv, self).__init__()\n        self.pool1 = nn.MaxPool2d(kernel_size=2)\n        self.conv_block = BaseConv(in_channels, out_channels, kernel_size, padding, stride)\n    \n    def forward(self, x):\n        x = self.pool1(x)\n        x = self.conv_block(x)\n        return x\n\n\nclass UpConv(nn.Module):\n    def __init__ (self, in_channels, in_channels_skip, out_channels, kernel_size, padding, stride):\n        super(UpConv, self).__init__()\n        self.act = nn.ReLU()\n        self.conv_trans1 = nn.ConvTranspose2d(in_channels + in_channels_skip, out_channels, kernel_size=2, padding=0, stride=2)\n        self.b3 = nn.BatchNorm2d(out_channels, eps=1e-05, momentum=0.1, affine=True)\n        self.conv3 = nn.Conv2d(in_channels=out_channels, out_channels= out_channels, kernel_size=kernel_size, padding=padding, stride=stride)\n        self.b4 = nn.BatchNorm2d(out_channels, eps=1e-05, momentum=0.1, affine=True)\n        self.conv4 = nn.Conv2d(out_channels, out_channels, kernel_size=kernel_size, padding=padding, stride=stride)\n        self.b5 = nn.BatchNorm2d(out_channels, eps=1e-05, momentum=0.1, affine=True)\n\n\n    def forward(self, x, x_skip):\n        x = torch.cat((x, x_skip), dim=1)\n        x = self.conv_trans1(x)\n        x = self.act(self.b3(x))\n        x = self.conv3(x)\n        x = self.act(self.b4(x))\n        x = self.conv4(x)\n        x = self.act(self.b5(x))\n        return x\n    \n     \n        \nclass UNet(nn.Module):\n    def __init__(self, in_channels, out_channels, n_class, kernel_size, padding, stride, droup_rate = False):\n        super(UNet, self).__init__()\n\n        self.down1 = DownConv(in_channels, out_channels, kernel_size, padding, stride)#48\n        \n        self.down2 = DownConv(out_channels, 2 * out_channels, kernel_size, padding, stride)#96\n\n        self.down3 = DownConv(2 * out_channels, 4 * out_channels, kernel_size, padding, stride)#192\n\n        self.down4 = DownConv(4 * out_channels, 8 * out_channels, kernel_size, padding, stride)#384\n        \n        self.down5 = BaseConv(8 * out_channels, 16 * out_channels, kernel_size, padding, stride)#768\n\n        \n        self.up4 = UpConv(16 * out_channels, 8 * out_channels, 8 * out_channels, kernel_size, padding, stride)\n        \n        self.up3 = UpConv(8 * out_channels, 4 * out_channels, 4 * out_channels,  kernel_size, padding, stride)\n\n        self.up2 = UpConv(4 * out_channels, 2 * out_channels, 2 * out_channels, kernel_size, padding, stride)\n\n        self.up1 = UpConv(2 * out_channels, out_channels, out_channels, kernel_size, padding, stride)\n        \n        self.out = nn.Conv2d(out_channels, n_class, kernel_size, padding, stride)\n\n        \n    def forward(self, x):\n        # Encoder\n        x1 = self.down1(x)\n        x2 = self.down2(x1)\n        x3 = self.down3(x2)\n        x4 = self.down4(x3)\n        x5 = self.down5(x4)\n\n        # Decoder\n        x_up1 = self.up4(x5, x4)\n        x_up2 = self.up3(x_up1, x3)\n        x_up3 = self.up2(x_up2, x2)\n        x_up4 = self.up1(x_up3, x1)\n\n        x_out = F.log_softmax(self.out(x_up4), 1)\n        return x_out\n \n    \nmodel = UNet(in_channels=1,\n             out_channels=48,\n             n_class=2,\n             kernel_size=3,\n             padding=1,\n             stride=1)\n\n#print(model)\nprint(\"UNet model created\")\n\n\n#Print model's state_dict\nprint(\"Model's state_dict:\")\n\n\ninp = torch.randn(2, 1, 128, 128)\nout = model(inp)\nprint('input size: {}'.format(inp.size()))\nprint('output size: {}'.format(out.size()))\n\n<\/code>","z":"Edit: Think I got it wrong the first time. This time I\u2019m concating asap in the UpConv and changed the out_channels in all the UpConv layers so it matches the kernel size from the picture (Kernel \/ Pool size). Also changed the order in the UpConv forward function. I haven\u2019t doublechecked but let me know if this is what you\u2019re looking for.\n<code class=\"lang-auto\">import torch\nimport torch.nn as nn\nimport torch.nn.functional as F\n\nclass BaseConv(nn.Module):\n    def __init__(self, in_channels, out_channels, kernel_size, padding, stride, droup_rate = False):\n        super(BaseConv, self).__init__()\n        self.act = nn.ReLU()\n        self.conv1 = nn.Conv2d(in_channels, out_channels, kernel_size, padding, stride)\n        self.b1 = nn.BatchNorm2d(out_channels, eps=1e-05, momentum=0.1, affine=True)\n        self.conv2 = nn.Conv2d(out_channels, out_channels, kernel_size, padding, stride)\n        self.b2 = nn.BatchNorm2d(out_channels, eps=1e-05, momentum=0.1, affine=True)\n                \n    def forward(self, x):\n        x = self.conv1(x)\n        x = self.act(self.b1(x))\n        x = self.conv2(x)\n        x = self.act(self.b2(x))\n\n        return x\n    \n    \nclass DownConv(nn.Module):\n    def __init__(self, in_channels, out_channels, kernel_size, padding, stride):\n        super(DownConv, self).__init__()\n        self.pool1 = nn.MaxPool2d(kernel_size=2)\n        self.conv_block = BaseConv(in_channels, out_channels, kernel_size, padding, stride)\n    \n    def forward(self, x):\n        x = self.pool1(x)\n        x = self.conv_block(x)\n        return x\n\n\nclass UpConv(nn.Module):\n    def __init__ (self, in_channels, in_channels_skip, out_channels, kernel_size, padding, stride):\n        super(UpConv, self).__init__()\n        self.act = nn.ReLU()\n        self.conv_trans1 = nn.ConvTranspose2d(in_channels + in_channels_skip, out_channels, kernel_size=2, padding=0, stride=2)\n        self.b3 = nn.BatchNorm2d(out_channels, eps=1e-05, momentum=0.1, affine=True)\n        self.conv3 = nn.Conv2d(in_channels=out_channels, out_channels= out_channels, kernel_size=kernel_size, padding=padding, stride=stride)\n        self.b4 = nn.BatchNorm2d(out_channels, eps=1e-05, momentum=0.1, affine=True)\n        self.conv4 = nn.Conv2d(out_channels, out_channels, kernel_size=kernel_size, padding=padding, stride=stride)\n        self.b5 = nn.BatchNorm2d(out_channels, eps=1e-05, momentum=0.1, affine=True)\n\n\n    def forward(self, x, x_skip):\n        x = torch.cat((x, x_skip), dim=1)\n        x = self.conv_trans1(x)\n        x = self.act(self.b3(x))\n        x = self.conv3(x)\n        x = self.act(self.b4(x))\n        x = self.conv4(x)\n        x = self.act(self.b5(x))\n        return x\n    \n     \n        \nclass UNet(nn.Module):\n    def __init__(self, in_channels, out_channels, n_class, kernel_size, padding, stride, droup_rate = False):\n        super(UNet, self).__init__()\n\n        self.down1 = DownConv(in_channels, out_channels, kernel_size, padding, stride)#48\n        \n        self.down2 = DownConv(out_channels, 2 * out_channels, kernel_size, padding, stride)#96\n\n        self.down3 = DownConv(2 * out_channels, 4 * out_channels, kernel_size, padding, stride)#192\n\n        self.down4 = DownConv(4 * out_channels, 8 * out_channels, kernel_size, padding, stride)#384\n        \n        self.down5 = BaseConv(8 * out_channels, 16 * out_channels, kernel_size, padding, stride)#768\n\n        \n        self.up4 = UpConv(16 * out_channels, 8 * out_channels, 8 * out_channels, kernel_size, padding, stride)\n        \n        self.up3 = UpConv(8 * out_channels, 4 * out_channels, 4 * out_channels,  kernel_size, padding, stride)\n\n        self.up2 = UpConv(4 * out_channels, 2 * out_channels, 2 * out_channels, kernel_size, padding, stride)\n\n        self.up1 = UpConv(2 * out_channels, out_channels, out_channels, kernel_size, padding, stride)\n        \n        self.out = nn.Conv2d(out_channels, n_class, kernel_size, padding, stride)\n\n        \n    def forward(self, x):\n        # Encoder\n        x1 = self.down1(x)\n        x2 = self.down2(x1)\n        x3 = self.down3(x2)\n        x4 = self.down4(x3)\n        x5 = self.down5(x4)\n\n        # Decoder\n        x_up1 = self.up4(x5, x4)\n        x_up2 = self.up3(x_up1, x3)\n        x_up3 = self.up2(x_up2, x2)\n        x_up4 = self.up1(x_up3, x1)\n\n        x_out = F.log_softmax(self.out(x_up4), 1)\n        return x_out\n \n    \nmodel = UNet(in_channels=1,\n             out_channels=48,\n             n_class=2,\n             kernel_size=3,\n             padding=1,\n             stride=1)\n\n#print(model)\nprint(\"UNet model created\")\n\n\n#Print model's state_dict\nprint(\"Model's state_dict:\")\n\n\ninp = torch.randn(2, 1, 128, 128)\nout = model(inp)\nprint('input size: {}'.format(inp.size()))\nprint('output size: {}'.format(out.size()))\n\n<\/code>\nThe model is based on this paper with link \"https:\/\/www.ncbi.nlm.nih.gov\/pubmed\/30802851\". They provided the supplementary paper which include the above pic I did share. I couldn\u2019t find the link of the supplementary paper to share with you. Shall I share it through Dropbox or one-drive link ?\nAh, I updated my answer since then. I think I got it right now. Could you confirm?\n thanks a lot. I did get print of updated model as follows:\n<code class=\"lang-auto\">\ndown1.conv_block.conv1.weight          torch.Size([48, 1, 3, 3])\ndown1.conv_block.conv2.weight          torch.Size([48, 48, 3, 3])\n\ndown2.conv_block.conv1.weight          torch.Size([96, 48, 3, 3])\ndown2.conv_block.conv2.weight          torch.Size([96, 96, 3, 3])\n\ndown3.conv_block.conv1.weight          torch.Size([192, 96, 3, 3])\ndown3.conv_block.conv2.weight          torch.Size([192, 192, 3, 3])\n\ndown4.conv_block.conv1.weight          torch.Size([384, 192, 3, 3])\ndown4.conv_block.conv2.weight          torch.Size([384, 384, 3, 3])\n\ndown5.conv1.weight          torch.Size([768, 384, 3, 3])\ndown5.conv2.weight          torch.Size([768, 768, 3, 3])\n\nup4.conv_trans1.weight          torch.Size([1152, 384, 2, 2])\nup4.conv3.weight          torch.Size([384, 384, 3, 3])\nup4.conv4.weight          torch.Size([384, 384, 3, 3])\n\nup3.conv_trans1.weight          torch.Size([576, 192, 2, 2])\nup3.conv3.weight          torch.Size([192, 192, 3, 3])\nup3.conv4.weight          torch.Size([192, 192, 3, 3])\n\nup2.conv_trans1.weight          torch.Size([288, 96, 2, 2])\nup2.conv3.weight          torch.Size([96, 96, 3, 3])\nup2.conv4.weight          torch.Size([96, 96, 3, 3])\n\nup1.conv_trans1.weight          torch.Size([144, 48, 2, 2])\nup1.conv3.weight          torch.Size([48, 48, 3, 3])\nup1.conv4.weight          torch.Size([48, 48, 3, 3])\n\nout.weight          torch.Size([2, 48, 3, 3])\n\n<\/code>\napparently the number of channel from up4 to up1 isn\u2019t same as pic.\nNp. Aren\u2019t they? Number of filters in U1=384, U2=192, U3=96, U4=48. Thats the same as in the printout you just posted just the variables are named in reverse (up1 = U4).\nEdit: Ah, the number of channels isn\u2019t specified in the pic. It\u2019s the # filters (kernel in pic) that is specified. I think\u2026 Long time since I learned how conv2d works \n yeah you are right that up1=up4. In the pic number of channel also specified. if you look at the print for up4.conv_trans1.weight torch.Size([1152, 384, 2, 2]) is 1152 channel but in the pic up1 is 384. I think the concatenation shouldn\u2019t be in the self.conv_trans1 . Also in the pic connection specified in front of conv col. is that right?\nI think that the pic doesn\u2019t specify input channels but rather output channels, meaning the number of filters. Then the printout of up4.conv_trans1.weight torch.Size([1152, 384, 2, 2]) actually does match the picture since the second number (output channels) matches the pic up1= 384.\nI don\u2019t understand what you mean by this\n\n\n\n Neda:\n\nI think the concatenation shouldn\u2019t be in the self.conv_trans1 . Also in the pic connection specified in front of conv col. is that right?\n\n\nBut it is very likely that the code I provided is doing something in the wrong order \n if those filter kernels in pic are number of input channels, then how should I change up4 ?\nI don\u2019t know. In that case, maybe someone else can help out?\nBut lets think about the actual input to the network. The first layer is the D1. If that picture would specify the number of input channels somewhere it would have to write 1 for grayscale and 3 for RGB. It only specifies 48, which has to be output channels. So again, I believe the pic is showing output channels.\nOk so this implementation takes the output in the downsample layers with the asterisks *** and concats that with input of the upsample layers with matching ***. Note that I changed to order of the DownConv forward pass to first do the conv, then the maxpooling\n<code class=\"lang-auto\">import torch\nimport torch.nn as nn\nimport torch.nn.functional as F\n\nclass BaseConv(nn.Module):\n    def __init__(self, in_channels, out_channels, kernel_size, padding, stride, droup_rate = False):\n        super(BaseConv, self).__init__()\n        self.act = nn.ReLU()\n        self.conv1 = nn.Conv2d(in_channels, out_channels, kernel_size, padding, stride)\n        self.b1 = nn.BatchNorm2d(out_channels, eps=1e-05, momentum=0.1, affine=True)\n        self.conv2 = nn.Conv2d(out_channels, out_channels, kernel_size, padding, stride)\n        self.b2 = nn.BatchNorm2d(out_channels, eps=1e-05, momentum=0.1, affine=True)\n                \n    def forward(self, x):\n        x = self.conv1(x)\n        x = self.act(self.b1(x))\n        x = self.conv2(x)\n        x = self.act(self.b2(x))\n\n        return x\n    \n    \nclass DownConv(nn.Module):\n    def __init__(self, in_channels, out_channels, kernel_size, padding, stride):\n        super(DownConv, self).__init__()\n        self.pool1 = nn.MaxPool2d(kernel_size=2)\n        self.conv_block = BaseConv(in_channels, out_channels, kernel_size, padding, stride)\n    \n    def forward(self, x):\n        x = self.conv_block(x)\n        down = self.pool1(x)\n        return x, down\n\n\nclass UpConv(nn.Module):\n    def __init__ (self, in_channels, in_channels_skip, out_channels, kernel_size, padding, stride):\n        super(UpConv, self).__init__()\n        self.act = nn.ReLU()\n        self.conv_trans1 = nn.ConvTranspose2d(in_channels, out_channels, kernel_size=2, padding=0, stride=2)\n        self.b3 = nn.BatchNorm2d(out_channels, eps=1e-05, momentum=0.1, affine=True)\n        self.conv3 = nn.Conv2d(in_channels=in_channels, out_channels= out_channels, kernel_size=kernel_size, padding=padding, stride=stride)\n        self.b4 = nn.BatchNorm2d(out_channels, eps=1e-05, momentum=0.1, affine=True)\n        self.conv4 = nn.Conv2d(out_channels, out_channels, kernel_size=kernel_size, padding=padding, stride=stride)\n        self.b5 = nn.BatchNorm2d(out_channels, eps=1e-05, momentum=0.1, affine=True)\n\n\n    def forward(self, x, x_skip):\n        x = self.conv_trans1(x)\n        x = self.act(self.b3(x))\n        x = torch.cat((x, x_skip), dim=1)\n        x = self.conv3(x)\n        x = self.act(self.b4(x))\n        x = self.conv4(x)\n        x = self.act(self.b5(x))\n        return x\n    \n     \n        \nclass UNet(nn.Module):\n    def __init__(self, in_channels, out_channels, n_class, kernel_size, padding, stride, droup_rate = False):\n        super(UNet, self).__init__()\n\n        self.down1 = DownConv(in_channels, out_channels, kernel_size, padding, stride)#48\n        \n        self.down2 = DownConv(out_channels, 2 * out_channels, kernel_size, padding, stride)#96\n\n        self.down3 = DownConv(2 * out_channels, 4 * out_channels, kernel_size, padding, stride)#192\n\n        self.down4 = DownConv(4 * out_channels, 8 * out_channels, kernel_size, padding, stride)#384\n        \n        self.down5 = BaseConv(8 * out_channels, 16 * out_channels, kernel_size, padding, stride)#768\n\n        \n        self.up4 = UpConv(16 * out_channels, 8 * out_channels, 8 * out_channels, kernel_size, padding, stride)\n        \n        self.up3 = UpConv(8 * out_channels, 4 * out_channels, 4 * out_channels,  kernel_size, padding, stride)\n\n        self.up2 = UpConv(4 * out_channels, 2 * out_channels, 2 * out_channels, kernel_size, padding, stride)\n\n        self.up1 = UpConv(2 * out_channels, out_channels, out_channels, kernel_size, padding, stride)\n        \n        self.out = nn.Conv2d(out_channels, n_class, kernel_size, padding, stride)\n\n        \n    def forward(self, x):\n        # Encoder\n        x1, down = self.down1(x)\n        x2, down = self.down2(down)\n        x3, down = self.down3(down)\n        x4, down = self.down4(down)\n        x5 = self.down5(down)\n\n        # Decoder\n        x_up1 = self.up4(x5, x4)\n        x_up2 = self.up3(x_up1, x3)\n        x_up3 = self.up2(x_up2, x2)\n        x_up4 = self.up1(x_up3, x1)\n\n        x_out = F.log_softmax(self.out(x_up4), 1)\n        return x_out\n \n    \nmodel = UNet(in_channels=1,\n             out_channels=48,\n             n_class=2,\n             kernel_size=3,\n             padding=1,\n             stride=1)\n\n#print(model)\nprint(\"UNet model created\")\n\n\n#Print model's state_dict\nprint(\"Model's state_dict:\")\n\n\ninp = torch.randn(2, 1, 128, 128)\nout = model(inp)\nprint('input size: {}'.format(inp.size()))\nprint('output size: {}'.format(out.size()))\n\n\nprint(\"Model's state_dict:\")\nfor param_tensor in model.state_dict():\n     print(param_tensor, \"\\t\", model.state_dict()[param_tensor].size())\n<\/code>\n thank you very much for your help. Yes, you are right. the pic shows the output channel. I read in the paper \u201cPlease note that the kernel size value corresponds to the number of feature maps (dimensionality of the output of the layer)\u201d.\nthe snippet you posted first if we want to keep it as pic, the maxpooling should be after conv. also if we agree that those filter size are output channel, still up-sampling sections up4.conv_trans1.weight torch.Size([1152, 384, 2, 2]) giving the wrong number of channels.\nin the second model you posted is causing an error RuntimeError: Input type (torch.cuda.FloatTensor) and weight type (torch.FloatTensor) should be the same which is about x1, down = self.down1(x)\nHey, I\u2019ll just reply quickly now but the second posting I did is running for me. Perhaps you mixed some of my code with your own? The error message you are getting is because the model and input are not on the same device, meaning they arent both on either the CPU or the GPU. Try adding the model = model.to(device) as in your original code\n I\u2019m trying to implement a model similar to what I posted here before with some differences. The model should be same as this attached pic. Please note that the kernel size value corresponds to the number of feature maps (dimensionality of the output of the layer).\nCapture.PNG580\u00d7963 101 KB with link \"https:\/\/discuss.pytorch.org\/uploads\/default\/original\/2X\/3\/3413178bbbdc24d43ebdf5ed4d5a87d9c15212a5.png\"\nhere is the code:\n<code class=\"lang-auto\">\nclass BaseConv(nn.Module):\n    def __init__(self, in_channels, out_channels, kernel_size, padding, stride):\n        super(BaseConv, self).__init__()\n        self.act = nn.ReLU()\n        self.conv1 = nn.Conv2d(in_channels, out_channels, kernel_size, padding, stride)\n        self.conv2 = nn.Conv2d(out_channels, out_channels, kernel_size, padding, stride)\n     \n                \n    def forward(self, x):\n        x = self.act(self.conv1(x))\n        x = self.act(self.conv2(x))\n\n        return x\n    \nclass DownConv(nn.Module):\n    def __init__(self, in_channels, out_channels, kernel_size, padding, stride):\n        super(DownConv, self).__init__()\n        self.pool1 = nn.MaxPool2d(kernel_size=2)\n        self.conv_block = BaseConv(in_channels, out_channels, kernel_size, padding, stride)\n    \n    def forward(self, x):\n        x = self.conv_block(x)\n        x = self.pool1(x)\n        return x\n\n    \n    \nclass UpConv(nn.Module):\n    def __init__ (self, in_channels, in_channels_skip, out_channels, kernel_size, padding, stride):\n        super(UpConv, self).__init__()\n        self.conv_trans1 = nn.Upsample(size=(in_channels, in_channels), scale_factor=(2))\n        self.conv_block = BaseConv(in_channels=in_channels + in_channels_skip, out_channels= out_channels, kernel_size=kernel_size, padding=padding, stride=stride)\n        \n    def forward(self, x, x_skip):\n        x = torch.cat((x, x_skip), dim=1)\n        x = self.conv_trans1(x)\n        x= self.conv_block(x)\n        return x\n\n\nclass UNet(nn.Module):\n    def __init__(self, in_channels, out_channels, n_class, kernel_size, padding, stride):\n        super(UNet, self).__init__()\n\n        self.down1 = DownConv(in_channels, out_channels, kernel_size, padding, stride)\n\n        self.down2 = DownConv(out_channels, out_channels, kernel_size, padding, stride)\n\n        self.down3 = DownConv(out_channels, 2 * out_channels, kernel_size, padding, stride)\n\n        self.down4 = DownConv(2 * out_channels, 4 * out_channels, kernel_size, padding, stride)\n        \n        self.down5 = DownConv(4 * out_channels, 4 * out_channels, kernel_size, padding, stride)\n        \n        \n        self.down6 = BaseConv( 4 * out_channels, 4 * out_channels, kernel_size, padding, stride)\n        \n\n        self.up1 = UpConv(4 * out_channels, 4 * out_channels, 4 * out_channels, kernel_size, padding, stride)\n\n        self.up2 = UpConv(4 * out_channels, 4 * out_channels, 4 * out_channels, kernel_size, padding, stride)\n        \n        self.up3 = UpConv(4 * out_channels, 2 * out_channels, 2 * out_channels, kernel_size, padding, stride)\n\n        self.up4 = UpConv(2 * out_channels, out_channels, out_channels, kernel_size, padding, stride)\n\n        self.up5 = UpConv(out_channels, out_channels, out_channels, kernel_size, padding, stride)\n        \n      \n        self.out = nn.Conv2d(out_channels, n_class, kernel_size, padding, stride)\n\n\n    def forward(self, x):\n        # Encoder\n        x1 = self.down1(x)\n        x2 = self.down2(x1)\n        x3 = self.down3(x2)\n        x4 = self.down4(x3)\n        x5 = self.down5(x4)\n        \n        x6 = self.down6(x5)\n\n        # Decoder\n        x_up1 = self.up1(x6, x5)\n        x_up2 = self.up2(x_up1, x4)\n        x_up3 = self.up3(x_up2, x3)\n        x_up4 = self.up4(x_up3, x2)\n        x_up5 = self.up5(x_up4, x1)\n        \n        x_out = F.log_softmax(self.out(x_up5), 1)\n        return x_out\n \n    \nmodel = UNet(in_channels=1,\n             out_channels=32,\n             n_class=2,\n             kernel_size=3,\n             padding=1,\n             stride=1)\n\nmodel = model.to(device)\n\n\n#Print model's state_dict\nprint(\"Model's state_dict:\")\nfor param_tensor in model.state_dict():\n     print(param_tensor, \"\\t\", model.state_dict()[param_tensor].size())\n\n<\/code>\nI think down layers are Ok, but the problem is up5 which is not correspond with the pic as it doesn\u2019t have 32 output. Could you please point me to the right direction?\nHere is the print model tate_dict\n<code class=\"lang-auto\">Model's state_dict:\ndown1.conv_block.conv1.weight    torch.Size([32, 1, 3, 3])\ndown1.conv_block.conv1.bias      torch.Size([32])\ndown1.conv_block.conv2.weight    torch.Size([32, 32, 3, 3])\ndown1.conv_block.conv2.bias      torch.Size([32])\ndown2.conv_block.conv1.weight    torch.Size([32, 32, 3, 3])\ndown2.conv_block.conv1.bias      torch.Size([32])\ndown2.conv_block.conv2.weight    torch.Size([32, 32, 3, 3])\ndown2.conv_block.conv2.bias      torch.Size([32])\ndown3.conv_block.conv1.weight    torch.Size([64, 32, 3, 3])\ndown3.conv_block.conv1.bias      torch.Size([64])\ndown3.conv_block.conv2.weight    torch.Size([64, 64, 3, 3])\ndown3.conv_block.conv2.bias      torch.Size([64])\ndown4.conv_block.conv1.weight    torch.Size([128, 64, 3, 3])\ndown4.conv_block.conv1.bias      torch.Size([128])\ndown4.conv_block.conv2.weight    torch.Size([128, 128, 3, 3])\ndown4.conv_block.conv2.bias      torch.Size([128])\ndown5.conv_block.conv1.weight    torch.Size([128, 128, 3, 3])\ndown5.conv_block.conv1.bias      torch.Size([128])\ndown5.conv_block.conv2.weight    torch.Size([128, 128, 3, 3])\ndown5.conv_block.conv2.bias      torch.Size([128])\ndown6.conv1.weight       torch.Size([128, 128, 3, 3])\ndown6.conv1.bias         torch.Size([128])\ndown6.conv2.weight       torch.Size([128, 128, 3, 3])\ndown6.conv2.bias         torch.Size([128])\nup1.conv_block.conv1.weight      torch.Size([128, 256, 3, 3])\nup1.conv_block.conv1.bias        torch.Size([128])\nup1.conv_block.conv2.weight      torch.Size([128, 128, 3, 3])\nup1.conv_block.conv2.bias        torch.Size([128])\nup2.conv_block.conv1.weight      torch.Size([128, 256, 3, 3])\nup2.conv_block.conv1.bias        torch.Size([128])\nup2.conv_block.conv2.weight      torch.Size([128, 128, 3, 3])\nup2.conv_block.conv2.bias        torch.Size([128])\nup3.conv_block.conv1.weight      torch.Size([64, 192, 3, 3])\nup3.conv_block.conv1.bias        torch.Size([64])\nup3.conv_block.conv2.weight      torch.Size([64, 64, 3, 3])\nup3.conv_block.conv2.bias        torch.Size([64])\nup4.conv_block.conv1.weight      torch.Size([32, 96, 3, 3])\nup4.conv_block.conv1.bias        torch.Size([32])\nup4.conv_block.conv2.weight      torch.Size([32, 32, 3, 3])\nup4.conv_block.conv2.bias        torch.Size([32])\nup5.conv_block.conv1.weight      torch.Size([32, 64, 3, 3])\nup5.conv_block.conv1.bias        torch.Size([32])\nup5.conv_block.conv2.weight      torch.Size([32, 32, 3, 3])\nup5.conv_block.conv2.bias        torch.Size([32])\nout.weight       torch.Size([2, 32, 3, 3])\nout.bias         torch.Size([2])\n<\/code>\nDoes the code you posted run for you? It doesn\u2019t work for me. It never gets to up5. The up1 concatenates the input so the number of channels are 256, whilst the number of input channels in the first upsample is 128. Perhaps thats something?\nBtw, what pytorch version are you running?\n Yes it does run for me.\nWhat pytorch version? And have you put any input through the network?\n it is Torch version: 0.4.1  . Yes, you are right, when give it input it will stop at  x_up1 = self.up1(x6, x5) the error is about upsample\nreturn torch._C._nn.upsample_nearest2d(input, _output_size(2)) ValueError: only one of size or scale_factor should be defined\nTry replacing these functions in the code below. I didn\u2019t get the x_out = F.log_softmax(self.out(x_up5), 1) to work but I bet you got that one \n<code class=\"lang-auto\">class UpConv(nn.Module):\n    def __init__ (self, in_channels, in_channels_skip, out_channels, kernel_size, padding, stride):\n        super(UpConv, self).__init__()\n        self.conv_trans1 = nn.Upsample(scale_factor=2)\n        self.conv_block = BaseConv(in_channels=in_channels+in_channels_skip, out_channels=out_channels, kernel_size=kernel_size, padding=padding, stride=stride)\n        \n    def forward(self, x, x_skip):\n        x = torch.cat((x, x_skip), dim=1)\n        x = self.conv_trans1(x)\n        x = self.conv_block(x)\n        return x\n\n\nclass UNet(nn.Module):\n    def __init__(self, in_channels, out_channels, n_class, kernel_size, padding, stride):\n        super(UNet, self).__init__()\n\n        self.down1 = DownConv(in_channels, out_channels, kernel_size, padding, stride)\n\n        self.down2 = DownConv(out_channels, out_channels, kernel_size, padding, stride)\n\n        self.down3 = DownConv(out_channels, 2 * out_channels, kernel_size, padding, stride)\n\n        self.down4 = DownConv(2 * out_channels, 4 * out_channels, kernel_size, padding, stride)\n        \n        self.down5 = DownConv(4 * out_channels, 4 * out_channels, kernel_size, padding, stride)\n        \n        \n        self.down6 = BaseConv( 4 * out_channels, 4 * out_channels, kernel_size, padding, stride)\n        \n\n        self.up1 = UpConv(4 * out_channels, 4 * out_channels, 4 * out_channels, kernel_size, padding, stride)\n\n        self.up2 = UpConv(4 * out_channels, 4 * out_channels, 4 * out_channels, kernel_size, padding, stride)\n        \n        self.up3 = UpConv(4 * out_channels, 2 * out_channels, 2 * out_channels, kernel_size, padding, stride)\n\n        self.up4 = UpConv(2 * out_channels, out_channels, out_channels, kernel_size, padding, stride)\n\n        self.up5 = UpConv(out_channels, out_channels, int(out_channels\/2), kernel_size, padding, stride)\n        \n        self.out = nn.Conv2d(int(out_channels\/2), n_class, kernel_size, padding, stride)\n<\/code>\n thank you for this. when I print the model it seems fine in terms of output, but when give it an input It throwing an error\nx = torch.cat((x, x_skip), dim=1) RuntimeError: invalid argument 0: Sizes of tensors must match except in dimension 1. Got 7 and 14 in dimension 2 at c:\\programdata\\miniconda3\\conda-bld\\pytorch_1533100020551\\work\\aten\\src\\thc\\generic\/THCTensorMath.cu:87"},{"x":"In the following tutorial, why do we need \u201cwith torch.set_grad_enabled(phase == \u2018train\u2019)\u201d because we aren\u2019t calling backward on \u201ctest\u201d phase anyways so no gradients will be calculated.\nhttps:\/\/pytorch.org\/tutorials\/beginner\/transfer_learning_tutorial.html#training-the-model","y":"You are right that we aren\u2019t calling backward on \u201ctest\u201d.\nHowever, during \u201ctest\u201d, the model layers will create the computation graph for forward propagation, which can only be cleaned-up by a backward propagation. To avoid this, with torch.set_grad_enabled(False) informs all the layers to not to create any computational graph, because we do not wish to backpropagate for current computations.","z":"You are right that we aren\u2019t calling backward on \u201ctest\u201d.\nHowever, during \u201ctest\u201d, the model layers will create the computation graph for forward propagation, which can only be cleaned-up by a backward propagation. To avoid this, with torch.set_grad_enabled(False) informs all the layers to not to create any computational graph, because we do not wish to backpropagate for current computations.\ngreat! thank you for the response. makes sense. So is it just to due to memory issues that clean up of computational graph is required? what would happen if we don\u2019t clean the computational graph? any effect on next iteration gradient calculations?\nI think, the gradient calculations of next iterations won\u2019t be affected because every forward call would be working on their own set of data. But if the computational graphs are not cleaned up, you would eventually face the Out of memory error such as:\n\n\n\n\nCPU RAM explodes after 30 minibatch iterations with link \"https:\/\/discuss.pytorch.org\/t\/cpu-ram-explodes-after-30-minibatch-iterations\/41014\"\n\n\n    I have read over 10 different reports of a similar problem with exploding RAM on a CPU and none of them have worked. \nI can run my forward pass using just the encoder network without any problems so I know my DataLoader and Encoder are fine. If I use the Decoder just for forward passes, without even storing or computing any losses, the RAM explodes. \nTherefore, what is it about my decoder that causes this? I tried to follow this https:\/\/pytorch.org\/tutorials\/intermediate\/seq2seq_translation_tuto\u2026 with link \"https:\/\/pytorch.org\/tutorials\/intermediate\/seq2seq_translation_tutorial.html\"\n\n\n"},{"x":"If I have two networks with the same architecture, say A and B. For security reason, Net B cannot get access to the training data. So I need to train A and update B with A\u2019s gradient.\nNote we cannot always copy A to B because A will be updated when training.","y":"I think you should also clone the gradients of A&amp;B. Can you change like below. Without clone data is not explicitly copied to destination I guess.\n<code class=\"lang-auto\">for net1,net2 in zip(A.named_parameters(),B.named_parameters()):\n                net2[1].data.grad = net1[1].data.grad.clone\n<\/code>","z":"you can select model parameter with model.named_parameters(). This can be something like below. I dont try it but shows general syntax\n<code class=\"lang-auto\">for net1,net2 in zip(A.named_parameters(),B.named_parameters()):\n                net2[1].data.grad = net1[1].data.grad\n<\/code>\nThank you for your quick reply.\nI tried and it works. But another problem occurs. After I copied the gradient from A to B, how can I apply the gradient to B so it will be updated with the gradient? I tried opt_B.step() but it doesn\u2019t work.\nyou need to first create an optimizer for model B. then apply optimizer.step\n<code class=\"lang-auto\">optim.Adam(B.parameters(), lr=args.lr)\n<\/code>\nYes, that\u2019s what I exactly do for model B. But it doesn\u2019t work.\n<code class=\"lang-auto\">        for batch_idx, data in enumerate(self.dataset_s[0]):\n            img = data['img']\n            label = data['label']\n            img = Variable(img.cuda())\n            label = Variable(label.long().cuda())\n            self.opt_g_s[0].zero_grad()\n            self.opt_c_s[0].zero_grad()\n            self.opt_g_t.zero_grad()\n            self.opt_c_t.zero_grad()\n            # output = self.C_s[0](self.G_s[0](img))\n            f = self.G_s[0](img)\n            output = self.C_s[0](f)\n\n            f_t = self.G_t(img)\n            output_t = self.C_t(f_t)\n\n            loss = criterion(output, label)\n            loss_t = criterion(output_t, label)\n            loss.backward()\n            for net1, net2 in zip(self.G_s[0].named_parameters(), self.G_t.named_parameters()):\n                # print(net1)\n                net2[1].grad = net1[1].grad\n            for net1, net2 in zip(self.C_s[0].named_parameters(), self.C_t.named_parameters()):\n                net2[1].grad = net1[1].grad\n            self.opt_g_t.step()\n            self.opt_c_t.step()\n            self.opt_g_s[0].step()\n            self.opt_c_s[0].step()\n            print(loss_t.data[0], loss.data[0])\n<\/code>\nHere is the code, G_s, C_s belong to net A, G_t, C_t belong to net B.  output, loss are net A\u2019s output and loss, output_t, loss_t are net B\u2019s loss and output.\nIdeally,  I would expect the loss_t will decrease in the same way with loss. But unfortunately, this is what I got:\n<code class=\"lang-auto\">2.325899600982666 2.311429977416992\n2.302957773208618 2.259328603744507\n2.338597059249878 2.1615893840789795\n2.310314178466797 1.9996391534805298\n2.3391530513763428 1.930885672569275\n2.3019471168518066 1.79799485206604\n2.312781810760498 1.7417033910751343\n2.315269708633423 1.7081165313720703\n2.3183462619781494 1.5497541427612305\n2.3130362033843994 1.5103309154510498\n2.3282203674316406 1.3914222717285156\n2.311227560043335 1.425149917602539\n2.32726788520813 1.3116666078567505\n2.3288776874542236 1.175093412399292\n2.3100202083587646 1.1519544124603271\n2.3509089946746826 1.1114654541015625\n2.3917856216430664 1.0374433994293213\n2.3640432357788086 0.9599748253822327\n2.3335037231445312 0.9961658716201782\n2.336228847503662 1.1022270917892456\n2.3434970378875732 0.9420759677886963\n2.314120292663574 0.9514802694320679\n2.3666858673095703 0.9194179773330688\n2.312129497528076 0.9283604621887207\n2.3348450660705566 0.8091476559638977\n2.396702766418457 0.8768067359924316\n2.3209187984466553 0.8434578776359558\n2.4188976287841797 0.645444393157959\n2.420340061187744 0.7303286194801331\n2.3238790035247803 0.831498384475708\n2.3971896171569824 0.8141204714775085\n2.3836135864257812 0.6898840069770813\n2.33423113822937 0.6100970506668091\n2.361891269683838 0.6768393516540527\n2.3814330101013184 0.7510046362876892\n2.3523271083831787 0.6501622200012207\n2.335498094558716 0.6775341033935547\n2.3763034343719482 0.5295536518096924\n2.3889424800872803 0.5690212845802307\n2.4591455459594727 0.5898173451423645\n2.369767665863037 0.7039562463760376\n2.4159860610961914 0.5748851299285889\n2.3444278240203857 0.37794822454452515\n2.394176483154297 0.5638140439987183\n2.426819324493408 0.43249520659446716\n2.357060432434082 0.5665342211723328\n2.391160488128662 0.47172683477401733\n2.4093570709228516 0.43452709913253784\n2.3887462615966797 0.40173453092575073\n2.471668243408203 0.4635242223739624\n2.3329155445098877 0.5089845657348633\n2.344878911972046 0.45018270611763\n2.377624273300171 0.5257216095924377\n<\/code>\nI think you should also clone the gradients of A&amp;B. Can you change like below. Without clone data is not explicitly copied to destination I guess.\n<code class=\"lang-auto\">for net1,net2 in zip(A.named_parameters(),B.named_parameters()):\n                net2[1].data.grad = net1[1].data.grad.clone\n<\/code>\nWeird requirement , did you solve the problem?\nThanks.\nThis works:\n<code class=\"lang-auto\">for net1,net2 in zip(A.named_parameters(),B.named_parameters()):\n                net2[1].grad = net1[1].grad.clone()\n<\/code>\nYeah, I finally find the reason.\nThe network B is updated correctly with network A.\nThe reason why the loss of network B is not decreasing is that the initialization of net B is different from net A. After I initialize them with same parameters, the loss of net B is synchronized with net A.\nGlad to know!\nWhat about remove the .clone().\nI mean just use net2[1].grad = net1[1].grad and they will point to the same address in memory, and I am not sure if it will use less memory than the present implementation, you could have a try\nYeah, I think it saves more memory as it\u2019s not deep copy without clone()\nwhy not accept the solution and close the topic as solved."},{"x":"I have a following piece of code (part of a train function):\n<code class=\"lang-auto\">loss = criterion(output, target)\nloss.backward()\noptimizer.step()\nloss_scalar = loss.item()\n<\/code>\nI run it on GPU without DataParallel. I have the latest version of PyTorch 1.0.1.post2 with CUDA 10.\nIf I run it in this order, execution time is 40 seconds. However, if I put loss.item() before loss.backward(), my execution time blows up to 200 seconds. Most of the time is then spent in .backward() as I can see from my profiler.\nWhat would be the reason for that? Is there any preferred order of converting loss to scalar?","y":"I think the loss.item() op creates a synchronization point so that your script would have to wait for the CUDA kernels to finish their execution.\nApparently if you call it directly after loss.backward() some kernels cannot run in background and you see a worse performance.","z":"I think the loss.item() op creates a synchronization point so that your script would have to wait for the CUDA kernels to finish their execution.\nApparently if you call it directly after loss.backward() some kernels cannot run in background and you see a worse performance."},{"x":"Suppose we have following system:\n\nWe keep the weights of both the models constant and find out gradient of combined loss with respect to input X.\nNow, how the gradient will be calculated for X, given that we have two routes to reach X?","y":"If your loss is L=L1+L2 then the gradients at the input will sum.  Consider:\nL1=f1(x)\nL2=f2(x)\nL=f1(x)+f2(x)\nThe derivative w.r.t x will be the derivative through f1 + the derivative through f2. This is because the derivative of a sum is the sum of the derivatives. If your combined loss is:\nL=f1(x)*f2(x)\nThen apply the chain rule and  you will obtain something similar. And so on.\nHope it helps!","z":"If your loss is L=L1+L2 then the gradients at the input will sum.  Consider:\nL1=f1(x)\nL2=f2(x)\nL=f1(x)+f2(x)\nThe derivative w.r.t x will be the derivative through f1 + the derivative through f2. This is because the derivative of a sum is the sum of the derivatives. If your combined loss is:\nL=f1(x)*f2(x)\nThen apply the chain rule and  you will obtain something similar. And so on.\nHope it helps!\nSo, when combined loss = loss1 + loss2, it is similar to something like following:\noptimizer.zero_grad()\nloss1.backward(retain_graph=True)\nloss2.backward()\noptimizer.step()\nAm I right?\nYes it is! But I\u2019d recommend you to stick to the first way, because retain_graph can easily lead to errors due to memory consumption etc.\nexactly, you save memory if you do it directly instead of calling backward two times."},{"x":"I was looking at the code of batchnorm\n<code class=\"lang-auto\">    def __init__(self, num_features, eps=1e-5, momentum=0.1, affine=True,\n                 track_running_stats=True):\n        super(_BatchNorm, self).__init__()\n        self.num_features = num_features\n        self.eps = eps\n        self.momentum = momentum\n        self.affine = affine\n        self.track_running_stats = track_running_stats\n        if self.affine:\n            self.weight = Parameter(torch.Tensor(num_features))\n            self.bias = Parameter(torch.Tensor(num_features))\n        else:\n            self.register_parameter('weight', None)\n            self.register_parameter('bias', None)\n        if self.track_running_stats:\n            self.register_buffer('running_mean', torch.zeros(num_features))\n            self.register_buffer('running_var', torch.ones(num_features))\n            self.register_buffer('num_batches_tracked', torch.tensor(0, dtype=torch.long))\n        else:\n            self.register_parameter('running_mean', None)\n            self.register_parameter('running_var', None)\n            self.register_parameter('num_batches_tracked', None)\n        self.reset_parameters()\n<\/code>\nand I don\u2019t really understand when to use a register_buffer\/ register_parameter vs nn.parameter\nBy doing some test:\n<code class=\"lang-auto\">a = torch.nn.BatchNorm2d(100)\n\na.register_parameter('test',None)\n\na\nOut[34]: BatchNorm2d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n\na.test\n\na.test2 = torch.nn.parameter.Parameter(requires_grad=False)\n\na.test2\nOut[37]: \nParameter containing:\ntensor([])\n<\/code>\nThe behavior is different, in case of a registered parameter there is no return when None is used.\nRegister parameter only can register a parameter or None, so why is it used?\nWith respect to register_buffer docs just says it is used when u want to register something which is not a parameter. So I assume i does not compute gradients. Is there any different between register_buffer and a parameter with requires_grad = false?\nIn the code above, why if self.track:running.stats =True they register a buffer but if False they register a parameter?\nI checked it and register_buffer can also register None","y":"Hi,\nYou have two kinds of Tensors that a Module want to hold to:\n\nSome that are learnable. Represented as nn.Parameter and that should be registered with mod.register_parameter(\"name\", value) where value can be either None or an nn.Parameter\nSome that are not learnable. Represented as regular torch.Tensors and that should be registered with mod.register_buffer(\u201cname\u201d, value) where value can be either None or a torch.Tensor.\n\nNote that for simplicity, when you do mod.name = something. If something is an nn.Parameter, register_parameter() will be called automatically. EDITED: not true for Tensors.\nAs you can see, the only way to set a parameter or a buffer to None is to call the method directly. Otherwise, you can use natural assignment.","z":"Hi,\nYou have two kinds of Tensors that a Module want to hold to:\n\nSome that are learnable. Represented as nn.Parameter and that should be registered with mod.register_parameter(\"name\", value) where value can be either None or an nn.Parameter\nSome that are not learnable. Represented as regular torch.Tensors and that should be registered with mod.register_buffer(\u201cname\u201d, value) where value can be either None or a torch.Tensor.\n\nNote that for simplicity, when you do mod.name = something. If something is an nn.Parameter, register_parameter() will be called automatically. EDITED: not true for Tensors.\nAs you can see, the only way to set a parameter or a buffer to None is to call the method directly. Otherwise, you can use natural assignment.\n<code class=\"lang-auto\">       if self.track_running_stats:\n            self.register_buffer('running_mean', torch.zeros(num_features))\n            self.register_buffer('running_var', torch.ones(num_features))\n            self.register_buffer('num_batches_tracked', torch.tensor(0, dtype=torch.long))\n        else:\n            self.register_parameter('running_mean', None)\n            self.register_parameter('running_var', None)\n            self.register_parameter('num_batches_tracked', None)\n<\/code>\n, a nitpicking: why running_mean, running_var and num_batches_tracked in case of track_running_stats=False are registered as parameters ? I understand that this have no effect but this looks a bit strange\u2026\n\n\n\n albanD:\n\nNote that for simplicity, when you do mod.name = something. If something is an nn.Parameter, register_parameter() will be called automatically. Similarly, if something is a Tensor, register_buffer will be called automatically. Otherwise, mod.name is just assigned as for any other object.\n\n\n, thank you for that explanation.\nI verified this to work with not using register_parameter and assigning nn.Parameter instead.\nBut it doesn\u2019t seem to be the case with torch.Tensor (bolded in the quote above) automatically triggering a call to register_buffer.\ne.g. inside:\n<code class=\"lang-auto\">class MyModule(nn.Module):\n    def __init__(self):\n        super().__init__()\n        self.something = torch.zeros(1)\n<\/code>\ndoesn\u2019t add something to self._buffers\nIt\u2019s empty if I dump the keys, say during forward.\n<code class=\"lang-auto\">print(\"keys\", self._buffers.keys())\n<\/code>\nif I use:\nself.register_buffer('something', torch.zeros(1))\nit does end up in self._buffers.\ntested with pytorch 1.0.0.dev20190405\nAlso is there a magic repr way to get pytorch to show all the registered buffers in a given or all layer? (Other than doing it manually).\nThanks.\nAs far as I know, an setting a tensor as an attribute won\u2019t register it as a buffer, but you would rather need to call self.register_buffer directly.\nYou can call print(list(model.buffers())) or print(dict(model.named_buffers())) to get all buffers (adapted from the .parameters()\/.named_parameters() methods).\n\n\n\n ptrblck:\n\nAs far as I know, an setting a tensor as an attribute won\u2019t register it as a buffer, but you would rather need to call self.register_buffer directly.\n\n\nThank you for validating that - hope  then can edit his \u201caccepted\u201d solution to fix that.\n\nYou can call print(list(model.buffers())) or print(dict(model.named_buffers())) to get all buffers (adapted from the .parameters() \/ .named_parameters() methods).\n\nAwesome, thank you, !\nmodel.named_buffers is what I was after, model.buffers isn\u2019t useful for debug since it\u2019s unnamed data. In particular this gives me the listing of all registered buffers w\/o data:\n<code class=\"lang-auto\">print(dict(learn.model.named_buffers()).keys())\n<\/code>\nSo after doing self.register_buffer('mybuf1', tensor(0.)) the above printout give me:\n<code class=\"lang-auto\">dict_keys(['0.2.mybuf1', '1.2.mybuf1', '2.2.mybuf1'])\n<\/code>\nwhere 0.2 corresponds to group 0 layer 2, 1.2 group 1 layer 2, etc. Cool!"},{"x":"Why y.grad is None over here :\nx=torch.tensor(2.0,requires_grad=True)\ny=xx\nz=yy\nz.backward()\ny.grad \/\/ Gives None","y":"Because it will only retain the grad of tensors with requires_grad=True &amp;&amp; is_leaf=True .\nHere with link \"https:\/\/pytorch.org\/docs\/stable\/autograd.html#torch.Tensor.is_leaf\" for more details.","z":"Because it will only retain the grad of tensors with requires_grad=True &amp;&amp; is_leaf=True .\nHere with link \"https:\/\/pytorch.org\/docs\/stable\/autograd.html#torch.Tensor.is_leaf\" for more details."},{"x":"Hi,\nI want to visualize convolutional features, but when I call backward(),\nthe input variable\u2019s grad still None.\nI have checked the input variable, and is_leaf is True.\n<code class=\"lang-auto\">class SaveFeatures():\n    def __init__(self, module):\n        self.hook = module.register_forward_hook(self.hook_fn)\n    def hook_fn(self, module, input, output):\n        self.features = output.clone().detach().requires_grad_(True)\n    def close(self):\n        self.hook.remove()\n\nclass FilterVisualizer():\n    def __init__(self, model):\n        self.model = model\n        self.model.eval()\n        for param in self.model.parameters():\n            param.requires_grad = False\n\n    def visualize(self, layer, filter, lr=0.1, opt_steps=100):\n        img = np.uint8(np.random.uniform(150, 180, (48, 48, 3))) # generate random image #48, 48, 3\n        img = transform_train(Image.fromarray(img)) # 3, 44, 44\n        img = torch.unsqueeze(img, 0) # 1, 3, 44, 44\n        activations = SaveFeatures(list(self.model.children())[0][layer])  # register hook\n        img_var = Variable(img, requires_grad=True)  # convert image to Variable that requires grad\n        optimizer = torch.optim.Adam([img_var], lr=lr, weight_decay=1e-6)\n        for n in range(opt_steps):  # optimize pixel values for opt_steps times\n            optimizer.zero_grad()\n            self.model(img_var)\n            loss = -activations.features[0, filter].mean() #activations.features.shape=1,64,44,44 ,features[0,filter] = 44,44\n            loss.backward()\n            optimizer.step()\n<\/code>","y":"Hello,\nThere are two points in your snippet do not make sense to me.\n\n\n\n IapCaL:\n\nactivations = SaveFeatures(list(self.model.children())[0][layer])\n\n\nI have got that you want to extract features from intermediate layers, I think the first index [0] already represent the layer index, so what does the [layer] do for?\n\n\n\n IapCaL:\n\noptimizer = torch.optim.Adam([img_var], lr=lr, weight_decay=1e-6)\n\n\nHere the optimizer will only update the input, it seems weird, is there a specific purpose that you only want to \u201cupdate the input\u201d?\nAnd I have reproduced your issue with a dummy ConvNet, I think the problem raises in this line\n<code class=\"lang-auto\">    def hook_fn(self, module, input, output):\n        self.features = output.clone().detach().requires_grad_(True)\n<\/code>\nYou should remove the .detach() so that the input.grad and model.module.weight.grad are not None.","z":"Hello,\nThere are two points in your snippet do not make sense to me.\n\n\n\n IapCaL:\n\nactivations = SaveFeatures(list(self.model.children())[0][layer])\n\n\nI have got that you want to extract features from intermediate layers, I think the first index [0] already represent the layer index, so what does the [layer] do for?\n\n\n\n IapCaL:\n\noptimizer = torch.optim.Adam([img_var], lr=lr, weight_decay=1e-6)\n\n\nHere the optimizer will only update the input, it seems weird, is there a specific purpose that you only want to \u201cupdate the input\u201d?\nAnd I have reproduced your issue with a dummy ConvNet, I think the problem raises in this line\n<code class=\"lang-auto\">    def hook_fn(self, module, input, output):\n        self.features = output.clone().detach().requires_grad_(True)\n<\/code>\nYou should remove the .detach() so that the input.grad and model.module.weight.grad are not None.\nThanks for your reply,\nFirst,\n<code class=\"lang-auto\">print(list(self.model.children()))\n<\/code>\n<code class=\"lang-auto\">[Sequential(\n  (0): Conv2d(3, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n   .....\n  ), Linear(in_features=512, out_features=7, bias=True)]\n<\/code>\nFor two term in the list, it need to choose the Sequential term.\nSecond, the optimizer only update the input because I don\u2019t want to train the model.\nI want to visualize convolutional features, and train the input that can activate the selected filter.\nreference with link \"https:\/\/towardsdatascience.com\/how-to-visualize-convolutional-features-in-40-lines-of-code-70b7d87b0030\"\nI remove .detach() and it work. Thanks a lot!"},{"x":"Hello, everyone! I am trying to write code to achieve the following functions:\nDue to some reasons, during training, my loss can be NONE. So when the loss is NONE, I skip this training step and directly continue the next iteration step. But it seems that I fail to free up the GPU memory. Thus in the next iteration step, I will encounter \u201cout of memory\u201d error. Some part of my code is as follows, and I use two networks in my code. I have tried to delete the output,input,target, but my GPUs are still out of memory.\nI am very grateful if anyone is nice to help me!\n    for i, (input, target) in enumerate(train_loader):\n        actual_step = int(args.start_step + cnt)\n        adjust_learning_rate(optimizer1, actual_step)\n        adjust_learning_rate(optimizer2, actual_step)\n\n        input = input.cuda()\n        target = target.cuda()\n\n        # compute output\n        output1 = model1(input)\n        output2 = model2(input)\n\n        loss1_1, loss2_1 = criterion(output1[0], output2[0], target)\n        loss1_2, loss2_2 = criterion(output1[1], output2[1], target)\n        if loss1_1 is None:\n            print(\"loss1_1 is None!\")\n            # TODO release GPU memory\n            continue # in the next step, I encounter Out of Memory Error.\n        if loss1_2 is None:\n            print(\"loss1_2 is None!\")\n            # TODO release GPU memory\n            continue\n\n        loss1 = loss1_1 + 0.4 * loss1_2\n        loss2 = loss2_1 + 0.4 * loss2_2\n\n        # compute gradient and do SGD step\n        optimizer1.zero_grad()\n        loss1.backward()\n        optimizer1.step()\n\n        optimizer2.zero_grad()\n        loss2.backward()\n        optimizer2.step()","y":"Hello,\nYou could just call .backward() without optimizer.step() if the step is unexpected condition.\n<code class=\"lang-auto\">optimizer.zero_grad()\nif condition:\n    loss.backward()\nelse:\n    loss.backward()\n    optimizer.step()\n<\/code>","z":"Hello,\nYou could just call .backward() without optimizer.step() if the step is unexpected condition.\n<code class=\"lang-auto\">optimizer.zero_grad()\nif condition:\n    loss.backward()\nelse:\n    loss.backward()\n    optimizer.step()\n<\/code>\nHello,Thanks very much for your reply! But I think this way may accumulate the backward gradient. I have tried another similar way. When loss is none, I change to use a normal way to calculate the loss and everything is fine now."},{"x":"Hi!\nI implemented the following class. In the forward step, it works, however, when I execute the backward step, it returns: RuntimeError: one of the variables needed for gradient computation has been modified by an inplace operation\nThe output of the forward method is a sum of the elements in a vector, which is generated after the for loop: for iter in range ( r ). The output vector is v. If the value of r is 1, the backpropagation works. If the value of r is greater than one, which means, the output vector is adjusted at least twice, the backpropagation does not work. It seems that it does not preserve the old values of the output vector into the iterative process.\nI have been trying several configurations, including a list of vectors v. However, none of them works. Any suggestions?\nclass Model_New(nn.Module):\n    def __init__(self, parallel = False):\n        super(Model_New, self).__init__()\n        self.ReLUConv1 = nn.Sequential(\n            nn.Conv2d(in_channels=1, out_channels=256, kernel_size=9, stride=1),\n            nn.ReLU(inplace=True)\n        )\n        self.PrimaryCaps = nn.ModuleList()\n        for _ in range(32):\n            self.PrimaryCaps.append(nn.Conv2d(in_channels=256, out_channels=8, kernel_size=9, stride=2))\n        self.W_ij = nn.Parameter(torch.rand((32, 10, 6*6, 16, 8)))\n        self.decoder = nn.Sequential(\n            nn.Linear(16*10, 512),\n            nn.ReLU(inplace=True),\n            nn.Linear(512, 1024),\n            nn.ReLU(inplace=True),\n            nn.Linear(1024, 784),\n            nn.Sigmoid()\n        )\n\n\n    def forward(self, x, y=None):\n        # local variables definition\n        W_shape = self.W_ij.shape       # (l=32, i=6*6, j=10, v=16, u=8) \n        batch = x.shape[0]\n\n        v_out, reconstructed = [], []\n        for batch_id in range(batch):\n            u = self.ReLUConv1(x[batch_id, ].reshape(1, 1, 28, 28))\n            u = [torch.squeeze(capsules(u)).reshape(8, -1) for capsules in self.PrimaryCaps] \n            u = torch.stack([torch.t(u_tmp).expand(10, 36, 8)[:, :, :, None] for u_tmp in u])\n            u_ji = torch.squeeze(torch.matmul(self.W_ij, u))\n            b = torch.zeros((32, 10, 36), requires_grad = False)\n\n            r = 5\n            for iter in range(r):\n                c = F.softmax(b, dim=1)[:, :, :, None]\n                c = c.repeat(1, 1, 1, 16)\n                s = torch.sum(torch.mul(c, u_ji), dim=(0, 2))\n                s_norm = torch.norm(s, dim=1, keepdim=True)\n                v = torch.div(s, s_norm.repeat(1, 16))\n                v = torch.mul(s_norm**2\/(1 + s_norm**2).repeat(1, 16), v)\n                if (iter + 1) < r:\n                    for j in range(10):\n                        b[:, j,:] += torch.squeeze(torch.matmul(u_ji[:, j, :, :], v[j, :].reshape(16, 1)))\n\n            if not y is None:\n                nz = torch.zeros((10, 10))\n                nz[y[batch_id], y[batch_id]] = 1\n                v = torch.mm(nz, v)\n            v_out.append(v)\n\n        return torch.sum(torch.stack(v_out))\n\nmodel = Model_New(parallel=False)\noutputs = model(torch.rand((3, 1, 28, 28)))\nprint(outputs)\noutputs.backward()\n++++++++++++\nOutput:\ntensor(-101.7659, grad_fn=)\n---------------------------------------------------------------------------\nRuntimeError                              Traceback (most recent call last)\n in \n1 outputs = model(torch.rand((3, 1, 28, 28)))\n2 print(outputs)\n----> 3 outputs.backward()\n4 # outputs[2].grad_fn\n5 # loss = CapsNet_loss(outputs[0], outputs[1], images, labels)\n~\/anaconda3\/lib\/python3.6\/site-packages\/torch\/tensor.py in backward(self, gradient, retain_graph, create_graph)\n    100                 products. Defaults to ``False``.\n    101         \"\"\"\n--> 102         torch.autograd.backward(self, gradient, retain_graph, create_graph)\n    103 \n    104     def register_hook(self, hook):\n\n~\/anaconda3\/lib\/python3.6\/site-packages\/torch\/autograd\/__init__.py in backward(tensors, grad_tensors, retain_graph, create_graph, grad_variables)\n     88     Variable._execution_engine.run_backward(\n     89         tensors, grad_tensors, retain_graph, create_graph,\n---> 90         allow_unreachable=True)  # allow_unreachable flag\n     91 \n     92 \n\nRuntimeError: one of the variables needed for gradient computation has been modified by an inplace operation","y":"You should be able to replace the for loop in j by replacing matmul with broadcasting (or einsum if you can\u2019t help it). Then you can just write b = b + X, eliminating the inplace update.\nBest regards\nThomas","z":"You should be able to replace the for loop in j by replacing matmul with broadcasting (or einsum if you can\u2019t help it). Then you can just write b = b + X, eliminating the inplace update.\nBest regards\nThomas\nHi Thomas:\nThanks for your help. I changed the for loop from this one:\nfor j in range(10):\n  b[:, j,:] += torch.squeeze(torch.matmul(u_ji[:, j, :, :], v[j, :].reshape(16, 1)))\n\nTo this:\nvv = torch.unsqueeze(v, 2).expand(32, 10, 16, 1)\nb = b + torch.squeeze(torch.matmul(u_ji, vv))\n\nand it works! Thank you so much for your help!"},{"x":"I have a combined network with generator and discriminator.\nThe joint training of this network collapses due to the over-fitting of discriminator.\nSo I try to update discriminator occasionally,\ni.e., generator will be updated in every time step but in case of discriminator it will update only once in a hundred step.\nThe pseudo-code of the model is\nclass custom_loss(nn.Module):\ndef __ init__():\ndef forward(self, desired, predicted)\nloss_discriminator = nn.crossEntropyLoss(desired, predicted)\nloss_generator = nn.MSELoss(desired, predicted)\nreturn  aloss_discriminator + bloss_generator\n\u2026\npredicted = model( input )\nloss = custom_loss( desired, predicted )\nloss.backward()\n\u2026\noptimizer.step()\n(where a, b are constant)","y":"Instead of having one loss function and one optimizer for both your discriminator and generator, you could have a loss and optimizer for each of them. This means your training loop would look something like this:\n<code class=\"lang-python\">for i in range(NUM_ITERATIONS):\n    loss_gen = ...\n    loss_gen.backward()\n    optim_gen.step()\n\n    if i % 100 == 0:\n        loss_disc = ...\n        loss_disc.backward()\n        optim_disc.step()\n\n<\/code>\nHope this helps!","z":"Instead of having one loss function and one optimizer for both your discriminator and generator, you could have a loss and optimizer for each of them. This means your training loop would look something like this:\n<code class=\"lang-python\">for i in range(NUM_ITERATIONS):\n    loss_gen = ...\n    loss_gen.backward()\n    optim_gen.step()\n\n    if i % 100 == 0:\n        loss_disc = ...\n        loss_disc.backward()\n        optim_disc.step()\n\n<\/code>\nHope this helps!\nOh\u2026i forget the way of multiple optimizer and loss objects. Now it is solved. thanks!"},{"x":"I created a function that takes input as a \u201clist\u201d and converts it into a \u201cdataloader\u201d input.\n<code class=\"lang-auto\">def load_data(list_):\n    if len(list_[0])==2:\n        img, lab = [], []\n        for i in list_:\n            img.append(i[0]), lab.append(i[1])\n        xs = torch.stack(img)\n        xs = xs.squeeze(1)\n        ys = torch.Tensor(lab)\n        dataset = TensorDataset(xs, ys)\n        del img, lab\n    elif len(list_[0])==3:\n        img, lab, flag = [], [], []\n        for i in list_:\n            img.append(i[0]), lab.append(i[1]), flag.append(i[2])\n        xs = torch.stack(img)\n        xs = xs.squeeze(1)\n        ys = torch.Tensor(lab)\n        flag = torch.Tensor(flag)\n        dataset = TensorDataset(xs, ys, flag)\n        del img, lab, flag\n    return dataset\n\ndataset = load_data(list_) # Coverting the list into dataset\ntrain_data, val_data = torch.utils.data.random_split(dataset, [int(0.90*len(dataset)), int(0.10*len(dataset)+1)] )\ntrain_loader = DataLoader(train_data, batch_size=BATCH_SIZE, shuffle=True)\nval_loader = DataLoader(val_data, batch_size=BATCH_SIZE, shuffle=True)\n<\/code>\nThe error I am getting is because of different sizes which I don\u2019t understand correctly.\n<code class=\"lang-auto\">Traceback (most recent call last):\n  File \"newpipeline.py\", line 242, in <module>\n    dataset = load_data(list_) # Coverting the list into dataset\n  File \"newpipeline.py\", line 26, in load_data\n    xs = torch.stack(img)\nRuntimeError: invalid argument 0: Sizes of tensors must match except in dimension 0. Got 64 and 28 in dimension 1 at ..\/aten\/src\/TH\/generic\/THTensor.cpp:689\n<\/code>\nThe data I dump as a list from the latent dimension of an autoencoder is of the shape - [Batch-size, 256, 2, 2]","y":"Hi,\nError is happening here, which means imgs that you are stacking do not have same shape. Based on error, for some images, you have 64 and another 28 for height.\n\n\n\n _joker:\n\nxs = torch.stack(img)\n\n\nBests","z":"Hi,\nError is happening here, which means imgs that you are stacking do not have same shape. Based on error, for some images, you have 64 and another 28 for height.\n\n\n\n _joker:\n\nxs = torch.stack(img)\n\n\nBests"},{"x":"Hi\nMay I ask what will happen if the batch size is 1 and the dataParallel is used here, will the data still get splited into mini-batches, or nothing will happen?\nBest Regards","y":"Tested on images. And I only have 2 GPUs, for the case of more GPUs the Batch Size should be larger than the number of the GPUs to make all GPUs work.","z":"Just tested it, if the batch_size is 1 and DataParallel is used, only 1 GPU will be used. If the batch_size is larger than 1, 2 GPU will be used.\nMay I ask did you test on images or nlp?\nTested on images. And I only have 2 GPUs, for the case of more GPUs the Batch Size should be larger than the number of the GPUs to make all GPUs work."},{"x":"Say I have a network in which the output of a module M is consumed by two or more successors in the graph. In classical autodiff, when the gradients for M are pushed back through the multiple successors, they are accumulated (summed). Is it possible to access the gradients prior to accumulation?","y":"Here\u2019s a little hack that does the trick: for each path the output of M takes, apply the identity function and call retain_grad on its output.\n<code class=\"lang-auto\">#!\/usr\/bin\/env python\n\n# coding: utf-8\n\nfrom functools import partial\n\nimport torch\nimport torch.nn as nn\n\n\ndef print_tensor_grad(grad, name=None, value=None):\n    print(name, 'value', value, 'grad', grad)\n\n\ndef print_module_grad(module, grad_input, grad_out, name=None):\n    print(name, grad_input)\n\n\nclass Network(nn.Module):\n    def __init__(self, n_in=2, n_out=2):\n        super().__init__()\n        self.layer1 = nn.Linear(n_in, n_out, bias=False)\n        self.layer2 = nn.Linear(n_out, n_out, bias=False)\n        self.layer3 = nn.Linear(n_out*2, 1, bias=False)\n        self.identity = nn.LeakyReLU(negative_slope=1.0)\n\n    def forward(self, input):\n        out1 = self.layer1(input)\n        out1.retain_grad()\n\n        path1 = self.identity(out1)\n        path1.retain_grad()\n        path2 = self.identity(out1)\n        path2.retain_grad()\n\n        out2 = self.layer2(path1)\n        input3 = torch.cat((path2, out2), dim=1)\n        out3 = self.layer3(input3)\n\n        out1.register_hook(\n            partial(print_tensor_grad, name='out1', value=out1))\n        path1.register_hook(\n            partial(print_tensor_grad, name='path1', value=path1))\n        path2.register_hook(\n            partial(print_tensor_grad, name='path2', value=path2))\n\n        return {\n                'out1': out1,\n                'path1': path1,\n                'path2': path2,\n                'y': out3\n        }\n\n\nif __name__ == '__main__':\n    torch.manual_seed(17)\n    network = Network()\n    x = torch.ones(1, 2)\n    out = network(x)\n    out['y'].backward()\n    # Verify that the gradient of the output of the first layer is the\n    # same as the sum of the two paths taken by that output.\n    print('out1', out['out1'].grad)\n    print('path1', out['path1'].grad)\n    print('path2', out['path2'].grad)\n    assert torch.all(\n        out['out1'].grad == out['path1'].grad + out['path2'].grad)\n<\/code>","z":"Here\u2019s a little hack that does the trick: for each path the output of M takes, apply the identity function and call retain_grad on its output.\n<code class=\"lang-auto\">#!\/usr\/bin\/env python\n\n# coding: utf-8\n\nfrom functools import partial\n\nimport torch\nimport torch.nn as nn\n\n\ndef print_tensor_grad(grad, name=None, value=None):\n    print(name, 'value', value, 'grad', grad)\n\n\ndef print_module_grad(module, grad_input, grad_out, name=None):\n    print(name, grad_input)\n\n\nclass Network(nn.Module):\n    def __init__(self, n_in=2, n_out=2):\n        super().__init__()\n        self.layer1 = nn.Linear(n_in, n_out, bias=False)\n        self.layer2 = nn.Linear(n_out, n_out, bias=False)\n        self.layer3 = nn.Linear(n_out*2, 1, bias=False)\n        self.identity = nn.LeakyReLU(negative_slope=1.0)\n\n    def forward(self, input):\n        out1 = self.layer1(input)\n        out1.retain_grad()\n\n        path1 = self.identity(out1)\n        path1.retain_grad()\n        path2 = self.identity(out1)\n        path2.retain_grad()\n\n        out2 = self.layer2(path1)\n        input3 = torch.cat((path2, out2), dim=1)\n        out3 = self.layer3(input3)\n\n        out1.register_hook(\n            partial(print_tensor_grad, name='out1', value=out1))\n        path1.register_hook(\n            partial(print_tensor_grad, name='path1', value=path1))\n        path2.register_hook(\n            partial(print_tensor_grad, name='path2', value=path2))\n\n        return {\n                'out1': out1,\n                'path1': path1,\n                'path2': path2,\n                'y': out3\n        }\n\n\nif __name__ == '__main__':\n    torch.manual_seed(17)\n    network = Network()\n    x = torch.ones(1, 2)\n    out = network(x)\n    out['y'].backward()\n    # Verify that the gradient of the output of the first layer is the\n    # same as the sum of the two paths taken by that output.\n    print('out1', out['out1'].grad)\n    print('path1', out['path1'].grad)\n    print('path2', out['path2'].grad)\n    assert torch.all(\n        out['out1'].grad == out['path1'].grad + out['path2'].grad)\n<\/code>"},{"x":"Hey; I have multiple loss functions, each with respect to a batch of data point. I want to save each gradient to a buffer.Here is my way of doing\n<code class=\"lang-auto\">buffer = []\nfor b in range(num_batch):\n    weights.zero_grad()\n    loss = loss_func(batch[b])\n    loss.backward()\n    buffer.append(weights.grad)\n<\/code>\nHowever, I\u2019m wondering if there is any none-loop way of doing this ?","y":"If you use the same weight you will have to do the for loop I\u2019m afraid.\nNote that in your code, you want to do weights.grad.clone() on the last line. All changes to .grad are inplace and so after you do weight.zero_grad(), your buffer will contain only zeros if you don\u2019t clone.","z":"If you use the same weight you will have to do the for loop I\u2019m afraid.\nNote that in your code, you want to do weights.grad.clone() on the last line. All changes to .grad are inplace and so after you do weight.zero_grad(), your buffer will contain only zeros if you don\u2019t clone.\n\n\n\n albanD:\n\nIf you use the same weight you will have to do the for loop I\u2019m afraid.\n\n\nEach loss_func(batch[b]) is differentiated w.r.t. the same set of parameters.\nThanks for point out the clone() part.\nAlso; I\u2019m wondering that for a deep network parameters. What is the best structure to save the the most recent iteration\u2019s gradients and each gradients can also be access through similar thing as index(list isn\u2019t the best choice here I think)? I don\u2019t need the all history but only the gradient information from last iteration.\nFor more details, I\u2019m trying to implement algorithm 1 in this paper\n\n\narxiv.org with link \"https:\/\/arxiv.org\/pdf\/1309.2388.pdf\"\n\n\n with link \"https:\/\/arxiv.org\/pdf\/1309.2388.pdf\"\n1309.2388.pdf with link \"https:\/\/arxiv.org\/pdf\/1309.2388.pdf\"\n1179.19 KB\n\n\n\n\n\nHi,\nI\u2019m afraid you will have to do the bookeeping by hand and potentially implement a new optimizer.\nAs an example, you can look how rms prop with link \"https:\/\/github.com\/pytorch\/pytorch\/blob\/db82fc7ca6e6bf261f51a6794b400a04eaf55bd0\/torch\/optim\/rmsprop.py#L88-L93\" handles such bookeeping.\nThanks for the link!"},{"x":"Hi,\nCan someone demonstrate how to use the new checkpoint with link \"http:\/\/pytorch.org\/docs\/master\/checkpoint.html\" feature for part of the model. For example how to use checkpoint for the _DenseBlock with link \"https:\/\/github.com\/pytorch\/vision\/blob\/master\/torchvision\/models\/densenet.py#L147\" of the  densenet implementation, since this layer stores intermediate feature maps and becomes memory intensive?\nShould the checkpoint be called in the __init__ of the module or in forward?\nPointers would be really helpful.\nRegards\nNabarun","y":"No it\u2019s the oposite.\nBy default, the autograd saves all intermediary results needed for the backward pass.\nThe checkpointing tool has been added to allow not storing all intermediary results. So you only need it if you don\u2019t have enough memory to store your model, otherwise it will just slow down your model. So for the function\/module that is given to it, no intermediary result will be saved and they will be recomputed during the backward.","z":"Came here to ask the same thing.\nAnd the use of the term \u2018checkpoint\u2019 as \u2018trading compute for memory\u2019 is confusing me.  In common CS parlance, \u2018checkpointing\u2019 refers to the practice of saving a program\u2019s state so that it can be resumed if failure occurs.  But I don\u2019t see any specification of a file path (for saving) in the torch.utils.checkpoint.checkpoint spec.\nHi,\nYou need to modify the forward pass to replace how you use the corresponding submodule:\n<code class=\"lang-auto\"># Original:\nout = self.my_block(inp1. inp2. inp3)\n\n# With checkpointing:\nout = checkpoint(self.my_block, inp1, inp2, inp3)\n<\/code>\nHi,\nIf I understand correctly, the intermediate activations will be stored for the layers which we checkpoint and for the others will be recalculated at the backward pass. So basically we need to checkpoint the block which we want the activations to be stored and not the block for which we dont want to store. Is my understanding correct?\nRegards\nNo it\u2019s the oposite.\nBy default, the autograd saves all intermediary results needed for the backward pass.\nThe checkpointing tool has been added to allow not storing all intermediary results. So you only need it if you don\u2019t have enough memory to store your model, otherwise it will just slow down your model. So for the function\/module that is given to it, no intermediary result will be saved and they will be recomputed during the backward.\nI see. Thanks for the explanation. And thanks for the very useful tool! \nHi,\nParameters with link \"https:\/\/pytorch.org\/docs\/stable\/checkpoint.html\"\n<code class=\"lang-auto\">function \u2013 describes what to run in the forward pass of the model or part of the model. \nIt should also know how to handle the inputs passed as the tuple.\n<\/code>\nFor the function  parameter,\nI have a confusion \u2013 It is forward function or part of forward function ?\nIn other words,\n\n\n\n albanD:\n\nout = self.my_block(inp1. inp2. inp3)\n\n\nout is the return output of the forward function or intermediate result of the forward function.\nRight?"},{"x":"Hi\nI am aware of the fact that when using a list of layers we need to wrap them in nn.ModuleList so that the parameters get registered properly. But is there any chance that they will still get gradients and be trained if I do not wrap them in a ModuleList?\nNote:  This is not a custom layer. They are not being registered manually either.\nEg : self.affine_layers = [nn.Linear(self.affine_layers_dim_in, self.affine_layers_dim_op) for x in range(self.features)]\nThanks in advance.","y":"they might be receiving gradients, as they might be part of computation graph dynamically created during forward() function. But the parameters won\u2019t be updated as the optimizer is not acting on those gradients.\nIs there any specific behavior you see thats not consistent? It would be good to know.","z":"As far as I know,  there is a chance if the parameters of those modules are added manually to the optimizer instead of just using net.parameters().\n\n\n\n InnovArul:\n\nnet.parameters()\n\n\nif you are talking about nn.Parameter() not, they are not being added manually. Again, I think nn.Parameter() is for tensor variables and would probably be used in custom layer creation.\nI have updated the question and given more details.\nThanks!\nStill the same answer holds. Unless you add the parameters of the modules in the list to the optimizer manually,  the optimizer won\u2019t know that the module exists in the model (net.parameters() won\u2019t reveal the parameters of the model).\nI see, so if they are not being revealed in net.parameters() for sure, they are not receiving the gradients and won\u2019t be trained? There are no cases where PyTorch might be giving them gradients in any weird way possible?\nThis would be my last question, sorry if I am being really specific. I do not have the time to check the source code.\nBest\nAB\nthey might be receiving gradients, as they might be part of computation graph dynamically created during forward() function. But the parameters won\u2019t be updated as the optimizer is not acting on those gradients.\nIs there any specific behavior you see thats not consistent? It would be good to know.\nThere might be some inconsistent behavior. But I might not be able to demonstrate that. I just wanted to know if the weights of layers get update if they are not wrapped in a nn.ModuleList(). Seems like they do not (as I though considering the documentation and past experience). This question was a sanity check.\nThanks\nAB"},{"x":"I write a code but it has a error, I can not fix it\nmy environment is : cuda9 + cudnn 7.1  python=3.6.6 pytorch =1.0.1\nmy code:\n<code class=\"lang-auto\">import torch\nfrom torch import nn, optim\nfrom torch.autograd import Variable\nfrom torch.utils.data import DataLoader\nfrom torchvision import transforms\nfrom torchvision import datasets\n\n# \u5b9a\u4e49\u8d85\u53c2\u6570\nbatch_size = 100\nlearning_rate = 1e-3\nnum_epoches = 20\n\n# \u4e0b\u8f7d\u8bad\u7ec3\u96c6 MNIST \u624b\u5199\u6570\u5b57\u8bad\u7ec3\u96c6\ntrain_dataset = datasets.MNIST(\n    root='D:\/data', train=True, transform=transforms.ToTensor(), download=True)\n\ntest_dataset = datasets.MNIST(\n    root='D:\/data', train=False, transform=transforms.ToTensor())\n\ntrain_loader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True)\ntest_loader = DataLoader(test_dataset, batch_size=batch_size, shuffle=False)\n\n# \u5b9a\u4e49 Recurrent Network \u6a21\u578b\n\n\nclass Rnn(nn.Module):\n    def __init__(self, in_dim, hidden_dim, n_layer, n_class):\n        super(Rnn, self).__init__()\n        self.n_layer = n_layer\n        self.hidden_dim = hidden_dim\n        self.lstm = nn.LSTM(in_dim, hidden_dim, n_layer, batch_first=True)\n        self.classifiers = nn.Linear(hidden_dim, n_class)\n\n    def forward(self, x):\n        out, _ = self.lstm(x)\n        out = out[:, -1, :]\n        out = self.classifiers(out)\n        return out\n\n\nmodel = Rnn(28, 128, 2, 10)\nmodel = model.cuda()\n\ncriterion = nn.CrossEntropyLoss()\noptimizer = optim.Adam(model.parameters(), lr=learning_rate)\n\nfor epoch in range(num_epoches):\n    running_loss = 0.\n    running_acc = 0.\n    for i, data in enumerate(train_loader, 1):\n        img, label = data\n        b, c, h, w = img.size()\n        assert c == 1, 'Channel must be 1'\n        img = img.squeeze(1)\n\n        img = Variable(img).cuda()\n        label = Variable(label).cuda()\n\n        # \u5411\u524d\u4f20\u64ad\n        out = model(img)\n        loss = criterion(out, label)\n        running_loss += loss.data.item() * label.size(0)\n        _, pred = torch.max(out, 1)\n        num_correct = (pred == label).sum()\n        running_acc += num_correct.data.item()\n        # \u5411\u540e\u4f20\u64ad\n        optimizer.zero_grad()\n        loss.backward()\n        optimizer.step()\n\n        if i % 300 == 0:\n            print(\"[{} \/ {}] Loss: {:.6f}, Acc: {:.6f}\".format(\n                epoch + 1, num_epoches, running_loss \/ (batch_size * i),\n                running_acc \/ (batch_size * i)\n            ))\n    print(\"Finish {} epoch, Loss: {:.6f}, Acc: {:.6f}\".format(\n        epoch + 1, running_loss \/\n        (len(train_dataset)), running_acc \/ (len(train_dataset))\n    ))\n\n    model.eval()\n    eval_loss = 0. \n    eval_acc = 0. \n    for data in test_loader:\n        img, label = data\n        b, c, h, w = img.size()\n        assert c == 1, \"channel must be 1\"\n        img = img.squeeze(1)\n\n        with torch.no_grad():\n            img = Variable(img).cuda()\n            label = Variable(label).cuda()\n        out = model(img)\n        loss = criterion(out, label)\n        eval_loss += loss.data.item() * label.size(0)\n        _, pred = torch.max(out, 1)\n        num_correct = (pred == label).sum()\n        eval_acc += num_correct.data.item()\n    print('Test Loss: {:.6f}, Acc: {:.6f}'.format(eval_loss \/ (len(\n        test_dataset)), eval_acc \/ (len(test_dataset))))\n\n# \u4fdd\u5b58\u6a21\u578b\ntorch.save(model.state_dict(), '.\/rnn.pth')\n\n<\/code>\nthe error:\n<code class=\"lang-auto\">RuntimeError\ncudnn RNN backward can only be called in training mode\n  File \"D:\\documents\\th_torch\\recurrent_network001.py\", line 68, in <module>\n    loss.backward()\n<\/code>\nerror.jpg1383\u00d7588 92.5 KB with link \"https:\/\/discuss.pytorch.org\/uploads\/default\/original\/2X\/5\/5a508e2230ba1f2d4ef89d197148b5d63ac3d301.jpeg\"","y":"Hi\nYou call model.eval() at line 81 but forget to set the model to train mode again when you run start the second epoch.\nAdding model.train() after line 47 will remove the error.","z":"Hi\nYou call model.eval() at line 81 but forget to set the model to train mode again when you run start the second epoch.\nAdding model.train() after line 47 will remove the error.\nHi\uff0c thanks\uff0c your suggestion has helped me fix this error."},{"x":"This is a very simple example:\n<code class=\"lang-auto\">import torch\n\nx = torch.tensor([1., 2., 3., 4., 5.], requires_grad=True)\ny = torch.tensor([2., 2., 2., 2., 2.], requires_grad=True)\nz = torch.tensor([1., 1., 0., 0., 0.], requires_grad=True)\n\ns = torch.sum(x * y * z)\ns.backward()\n\nprint(x.grad)\n<\/code>\nThis will print,\n<code class=\"lang-auto\">tensor([2., 2., 0., 0., 0.]),\n<\/code>\nsince, of course, ds\/dx is zero for the entries where z is zero.\nMy question is: Is pytorch smart and stop the computations when it reaches a zero? Or does in fact do the calculation \u201c2*5\u201d , only to later do  \u201c10 * 0 = 0\u201d ?\nIn this simple example it doesn\u2019t make a big difference, but in the (bigger) problem I am looking at, this will make a difference.\nThank you for any input.\nPS: question also posted here: https:\/\/stackoverflow.com\/questions\/54781966\/does-pytorch-do-eager-pruning-of-its-computational-graph","y":"Hi,\nNo it does not.\nThe thing is that a tensor full of 0 would be a stop just for multiplication. Other operations might still continue even after a 0 is encountered. So this cannot be done in general I think (even though it might in some very specific cases).","z":"Hi,\nNo it does not.\nThe thing is that a tensor full of 0 would be a stop just for multiplication. Other operations might still continue even after a 0 is encountered. So this cannot be done in general I think (even though it might in some very specific cases)."},{"x":"Hi, I am still very new to PyTorch and after searching google for this doubt I could not find anything. Hopefully, someone could help me.\nI have a forward() method that looks as follows:\n\n  def forward(self, feature_vector):\n      activation_function = {'tanh': torch.tanh, 'relu': F.relu}\n\n      symbol, X = feature_vector\n\n      X = self.backend.from_numpy(X)\n\n      for i, l in enumerate(self.linears[symbol]):\n          if i != self.out_layer_indices[symbol]:\n              X = activation_function[self.activation_function](l(X))\n          else:\n              X = l(X)\n\n       X = (self.slope[symbol] * X) + self.intercept[symbol]\n      return X\n\n\nThe problem I have is with the line before the return. I also have a method called train where I defined self.slope and self.intercept using a python dictionary:\n      self.slope = {}\n      self.intercept = {}\n      \n       \n      for symbol in unique_element_symbols:\n          linears = []\n      \n          intercept = some operation\n          intercept = self.backend.from_numpy(intercept)\n          self.intercept[symbol] = Variable(intercept, requires_grad=True)\n       \n          slope = some operation\n          slope = self.backend.from_numpy(slope)\n          self.slope[symbol] = Variable(slope, requires_grad=True)\n          # I create a neural network here with Linear() + nn.ModuleDict\n\n      # Model is trained in this loop\n      for epoch in range(self.epochs):     \n          self.forward(tensor)\n          #  some more code to have the outputs\n          criterion = nn.MSELoss()\n          loss = torch.sqrt(criterion(outputs, targets))\n          self.optimizer.zero_grad()  # clear previous gradients\n          loss.backward()\n          self.optimizer.step()\n\nIf my understanding of the documentation is correct, the creation of Variable with requires_grad=True should make autograd aware of the existence of those tensors. I can see the weights of the layers are being updated but those variables I created inside the dictionary not. As they are part of the forward() method they have to affect the output and then they have to change according to the gradient. Are those variables not updated because they are in a python dictionary?\nI would be glad if someone could help me to understand what would be the issue with my code.","y":"The core reason of \u201cthose variables not updated\u201d is because they are not registed in the optimizer. If you look at any optimizer\u2019s constructor, there will be a parameter named params, which is expected to hold the parameters you want to automatically updated by grad.\nI\u2019m not sure how you initialized your optimizer. If you do create the optimizer in a most common way, you should make whatever you want to updated automatically as a parameter of the model by using self.register_parameter with link \"https:\/\/pytorch.org\/docs\/stable\/nn.html#torch.nn.Module.register_parameter\" in the model\u2019s constructor. And then, you could do torch.optim.SGD(model.parameters() since now the model.parameters() will return the variables you just registed.\nIf you have a hard time to put slop or intercept in the model\u2019s parameters, you could also register those tensors directly to the optimizer by using self.optimizer.add_param_group with link \"https:\/\/pytorch.org\/docs\/stable\/optim.html#torch.optim.Optimizer.add_param_group\"","z":"Most likely there parameters are not properly registered and thus unknown to the optimizer.\nYou could try to use a nn.ModuleDict with link \"https:\/\/pytorch.org\/docs\/stable\/nn.html#torch.nn.ModuleDict\" instead, but you would need to wrap your parameters in a nn.Module, since plain nn.Parameters won\u2019t be recognized as far as I know.\nPS: Variables are deprecated since 0.4.0. Just use torch.tensor(..., requires_grad=True) instead.\nThe core reason of \u201cthose variables not updated\u201d is because they are not registed in the optimizer. If you look at any optimizer\u2019s constructor, there will be a parameter named params, which is expected to hold the parameters you want to automatically updated by grad.\nI\u2019m not sure how you initialized your optimizer. If you do create the optimizer in a most common way, you should make whatever you want to updated automatically as a parameter of the model by using self.register_parameter with link \"https:\/\/pytorch.org\/docs\/stable\/nn.html#torch.nn.Module.register_parameter\" in the model\u2019s constructor. And then, you could do torch.optim.SGD(model.parameters() since now the model.parameters() will return the variables you just registed.\nIf you have a hard time to put slop or intercept in the model\u2019s parameters, you could also register those tensors directly to the optimizer by using self.optimizer.add_param_group with link \"https:\/\/pytorch.org\/docs\/stable\/optim.html#torch.optim.Optimizer.add_param_group\"\nThank you  and . Both of you were right. The parameters were not correctly registered when using a python dictionary.\nI tried using nn.ModuleDict and nn.Parameters without success. However, the self.register_parameter() did the trick for me.\nI removed the dictionaries for those parameters and added this instead:\nself.register_parameter(intercept_name, intercept)\nself.register_parameter(slope_name, slope)\n\nTo access them in the forward() method I had to do:\nfor name, param in self.named_parameters():\n    if intercept_name == name:\n        intercept = param \n    elif slope_name == name:\n        slope = param \n       \nX = slope * X + intercept \n\nNow intercept and slope are changing according to the gradient. Is there any way to access these parameters without using the loop over name, param shown above?\nI\u2019m glad it\u2019s working!\nYou can just use the name you\u2019ve used to register the parameters, e.g.:\n<code class=\"lang-python\">X = self.slope_name * X + self.intercept_name\n<\/code>\nThanks. That worked.\nThis is me, again\u2026 Something weird is happening. When I register the variables as we discussed above in this class with link \"https:\/\/gist.github.com\/muammar\/6d298f7fa5700efdbd01ae697f63e089\" (I pasted it in a gist because it is long), not all variables are being seen by autograd. There should be a total of 8 tensors but at the end of optimization only 6 of them are shown (relevant part of the output below):\n<code class=\"lang-auto\">outputs\ntensor([-14.5772266388, -14.5772266388])\ntargets\ntensor([-14.5868730545, -14.5640010834])\nNo diff in intercept_Cu\nNo diff in slope_Cu\nDiff in linears.Cu.0.weight\nNo diff in linears.Cu.0.bias\nDiff in linears.Cu.2.weight\nNo diff in linears.Cu.2.bias\nDiff in linears.Cu.4.weight\nNo diff in linears.Cu.4.bias\n\nOptimized parameters for Cu symbol\nIndex 0\nParameter containing:\ntensor([[ 1.5998219169e-05, -1.1084647089e-11,  3.4887983702e-07,\n         -5.9022102505e-05,  1.5358797100e-05, -2.2421262713e-07,\n          3.9578364522e-05, -2.5841705792e-05],\n        [-1.2979450847e-11,  2.4929600051e-11, -4.3761640145e-11,\n          8.7308825414e-07,  6.8003464548e-07, -6.9001464453e-07,\n         -3.0529092328e-05, -1.9285680537e-06],\n        [-3.4112128677e-09, -2.0672181415e-12,  1.0248225879e-12,\n          1.3090937500e-05, -1.9991681199e-08, -1.2244654499e-05,\n          1.1959917501e-09, -1.1793726173e-07],\n        [ 9.3959987360e-12, -2.8132822081e-06, -7.1578106144e-06,\n         -1.5608311514e-06,  7.4273208156e-05, -6.5615221589e-13,\n          1.0243820725e-04,  2.6734230119e-07],\n        [-2.8905316867e-05,  1.7972409978e-06,  2.8471620681e-05,\n          1.1441625247e-06, -4.3263348743e-06,  9.2861837402e-06,\n         -7.3636897469e-08, -6.2427188823e-06],\n        [ 1.8716022510e-08, -4.3462468966e-06, -7.1537678559e-11,\n          4.4766447493e-13, -4.2634189867e-07,  6.2688843006e-10,\n         -1.5413985643e-09, -1.9352362415e-06],\n        [-4.0789027480e-06,  1.7624552484e-08, -5.8772937336e-05,\n          1.3928577259e-12,  1.4477242303e-06, -6.5660731252e-07,\n          1.3057894830e-04,  1.0623334674e-06],\n        [ 2.8627397342e-07,  7.6879496191e-07, -1.5201392500e-07,\n          9.4639290182e-08,  1.7211885250e-09, -3.1544458712e-10,\n         -3.1436915742e-04, -9.5523216004e-09],\n        [ 5.4327131238e-07,  5.3367260989e-05,  3.0272097329e-11,\n         -2.5873794129e-06, -2.5613280741e-07,  4.1264866013e-05,\n          1.3438809527e-12, -5.6481166411e-09],\n        [-6.4899657445e-05, -4.3667625960e-08, -6.4955729684e-10,\n          7.9043999790e-08, -7.7281238191e-06,  1.7655082047e-05,\n         -1.6245309098e-07, -1.7478591019e-08]], requires_grad=True)\nGradient tensor(0.0126342149)\n\nIndex 1\nParameter containing:\ntensor([ 0.0846629143,  0.2052433789,  0.1129320264,  0.1384415329,\n         0.2349925339, -0.1073408127,  0.2195934355,  0.3364700377,\n         0.1929847300, -0.0893238783], requires_grad=True)\nNo gradient?\n\nIndex 2\nParameter containing:\ntensor([[-1.4006408492e-05, -1.3260194009e-06,  1.4346720434e-07,\n         -6.5448512032e-07,  2.9784255275e-06,  4.5995878547e-13,\n          6.7223256337e-05,  6.4453017576e-12,  1.0301571401e-10,\n         -1.2009696349e-08],\n        [-2.2814828071e-07, -5.8791869151e-08, -3.9165245835e-04,\n         -2.5221936539e-06,  1.1180619595e-06, -2.6514657293e-05,\n         -1.4766897038e-07,  2.7023989242e-04, -2.9795790401e-12,\n          3.4368467823e-06],\n        [ 3.6120570712e-06, -3.7223298568e-04,  7.1171717408e-09,\n         -4.0368172449e-06, -1.1812019807e-07, -9.0479334176e-06,\n         -9.7775303479e-12,  3.3027842505e-07, -2.2225761143e-07,\n          1.7060537516e-07],\n        [ 4.7848516260e-05,  1.4109857602e-06, -4.7986867813e-09,\n         -1.1886934145e-11, -1.5743089534e-06, -1.9210867777e-06,\n          2.5946489401e-10,  7.1065740485e-05, -7.2540847214e-06,\n         -2.9720404740e-13],\n        [ 7.8338234744e-07,  2.9897366403e-05,  1.0493286936e-05,\n         -1.2905216806e-07, -5.0532015905e-08, -1.4369081327e-05,\n          5.9140187659e-05,  1.8394788640e-05,  2.8736901004e-04,\n         -7.9514339557e-11],\n        [-3.5491411109e-04,  3.9472433855e-06, -3.6779524635e-06,\n          1.3279050108e-05,  1.0775630388e-09,  2.0076269536e-09,\n          2.2207383154e-05,  1.0671607924e-05,  3.5179223801e-07,\n          8.3256582002e-06],\n        [-4.0831773518e-09,  3.4044984204e-05,  3.9824635678e-07,\n         -5.4254252291e-07, -8.2707781596e-12,  7.9960360555e-10,\n          1.6246242751e-07, -1.5748057303e-09, -4.6191617002e-05,\n          1.4769234986e-04],\n        [ 6.0335892158e-06,  4.0175755203e-06,  2.3420781872e-05,\n         -1.4100555745e-07,  4.3824256863e-06, -1.9676244847e-05,\n         -4.2883926653e-05,  2.6943742341e-05,  1.5044579982e-07,\n          3.4529236359e-08],\n        [-2.4134715204e-05,  3.6303499655e-05, -1.0801615247e-07,\n          8.3609793364e-06,  3.0849619179e-06, -8.6793288574e-06,\n          2.4900288554e-04,  8.5335452355e-14, -3.4220584699e-11,\n         -4.0262357288e-06],\n        [-3.2995540096e-06, -9.5245795251e-08,  2.4340472748e-08,\n         -3.7661432133e-13, -4.4606429661e-09, -7.5562275015e-06,\n         -6.9999718107e-05,  1.4586039470e-04,  1.0552175809e-06,\n         -6.1385714220e-12]], requires_grad=True)\nGradient tensor(-0.0002918996)\n\nIndex 3\nParameter containing:\ntensor([-0.0368886292, -0.1048975587, -0.2438423038, -0.2089971900,\n         0.2615807354,  0.0241439044, -0.1016014665,  0.2302859128,\n        -0.2738550305, -0.2952967882], requires_grad=True)\nNo gradient?\n\nIndex 4\nParameter containing:\ntensor([[-1.6471599520e-04,  5.0920876674e-05,  1.6964193492e-05,\n         -7.2204138633e-06, -7.4410144713e-11,  1.3845928848e-09,\n          2.6772568162e-07,  4.4445322422e-11,  3.0647162930e-05,\n         -4.6163746447e-05]], requires_grad=True)\nGradient tensor(-0.0134047084)\n\nIndex 5\nParameter containing:\ntensor([-0.1564691514], requires_grad=True)\nNo gradient?\n<\/code>\nI understand biases are not counted, but only the weights of the layers are retained. Additionally, the loss seems to be decreasing in error with each epoch but the outputs of the model remain the same.\nDo you see any problem in the class I have built? what would you recommend to check? I would really appreaciate any suggestions. I am lost here.\nCould you just for the sake of debugging set the learning rate quite high, e.g. 100 and run a single update step to check if the parameters get updated?\nI would like to make sure that the gradients are not too small and we are thus not seeing any updates in the parameters even though the code should generally work.\nIf that still doesn\u2019t work, could you point me to some lines of code in your Gist so that debugging would be a bit faster? \nI tried it and this is the output:\n<code class=\"lang-auto\">outputs\ntensor([-14.7144765854, -14.7144765854])\ntargets\ntensor([-14.5868730545, -14.5640010834])\nNo diff in intercept_Cu\nNo diff in slope_Cu\nDiff in linears.Cu.0.weight\nNo diff in linears.Cu.0.bias\nDiff in linears.Cu.2.weight\nNo diff in linears.Cu.2.bias\nDiff in linears.Cu.4.weight\nNo diff in linears.Cu.4.bias\n\nOptimized parameters for Cu symbol\nIndex 0\nParameter containing:\ntensor([[ 8.5844440460e+00,  5.7265067101e-01,  9.7642364502e+00,\n         -6.3340994529e-05, -6.5706312656e-01, -1.0317638516e-01,\n         -8.1990205217e-04,  3.8604885340e-02],\n        [ 5.6350793839e+00, -2.3107304573e+00,  3.0389562016e-04,\n          4.8599953651e+00,  6.9255195558e-03, -1.2411396503e+00,\n         -1.9741505384e-01,  1.5696491755e-04],\n        [-5.0464026572e-05, -3.3354687691e+00,  8.9420490265e+00,\n          3.8780815601e+00,  2.3376888130e-03, -5.5896580219e-02,\n          1.3885598630e-02, -1.0426228866e-02],\n        [ 9.5443532337e-04,  6.6207236052e-01, -8.9323358536e+00,\n          6.4504299164e+00, -5.6406411204e-06, -1.3022724390e+00,\n         -6.7651176453e-01, -4.0060423315e-02],\n        [-7.3771867752e+00, -2.8842912674e+01, -1.2261917114e+01,\n          2.0418181084e-03, -3.8052463531e+00, -4.1274856776e-02,\n          7.0336312056e-02,  7.7507920563e-02],\n        [ 1.1060595512e-01, -3.2884517312e-01,  1.3592758179e+00,\n         -3.1894344091e-01, -1.7296176404e-02, -6.2223523855e-02,\n         -1.0991756916e+00, -1.0611775797e-03],\n        [-1.0529378653e+00,  8.7992340326e-02, -3.9956837893e-02,\n          5.1080572605e-01,  7.3425645828e+00, -1.4347046090e-05,\n          4.9415111542e-02,  1.4767070770e+01],\n        [-6.3522720337e-01, -3.3820127137e-03,  1.0707162857e+01,\n          1.5198823530e-03, -5.2807319164e-01,  5.2644854784e-01,\n         -1.2110622600e-02, -2.9190010391e-03],\n        [ 3.0526014045e-02, -1.3536047190e-03, -3.8478989154e-04,\n          2.9252339154e-03,  5.4483871460e+00,  7.9564154148e-03,\n         -1.8055616617e+00, -6.4464583993e-03],\n        [-4.6962329745e-01,  8.6185136752e-06,  2.4837136269e-02,\n         -4.1209143092e-05,  4.2492513657e+00,  8.4312686920e+00,\n          1.9236560433e-07,  3.2226529717e-01]], requires_grad=True)\nGradient tensor(-0.0052632340)\n\nIndex 1\nParameter containing:\ntensor([ 0.3178096116,  0.0436611772, -0.2040621340, -0.0848887563,\n         0.2899220884, -0.2525188029,  0.3507566750, -0.1945837736,\n         0.1707542241, -0.0507352650], requires_grad=True)\nNo gradient?\n\nIndex 2\nParameter containing:\ntensor([[ 9.4454865903e-03, -3.8685989380e-01,  2.8510479927e+00,\n          1.6451107513e-05,  7.2450813605e-05, -2.4000716209e-01,\n         -1.0067681968e-01, -6.8808451295e-02, -1.3941727579e-02,\n         -1.1572503299e-01],\n        [-4.0232582251e-05,  1.5237447619e-01, -8.4863287952e-08,\n         -3.3062148094e-01,  3.1492298841e-01,  7.1657931805e-01,\n          6.5576374531e-02,  5.8732334524e-02, -1.4156305790e-01,\n         -1.1431868374e-01],\n        [ 3.6709681153e-01, -5.6241098791e-03, -1.8890530029e-08,\n          2.2205217101e-04, -3.8731803894e+00, -8.4317040443e-01,\n         -3.5567022860e-03,  7.6645493507e-02, -1.7931096554e+00,\n         -2.0117998123e+00],\n        [-2.6692817919e-03,  2.3045387268e+00, -1.9369858503e-01,\n          1.1653967202e-02, -1.5044789314e+00,  2.6386910677e-01,\n         -1.8918566406e-02,  2.4579927325e-02, -8.7022192020e-05,\n          1.4020656636e-07],\n        [ 2.8100815415e-01, -1.4995394740e-03,  3.7862854004e+00,\n          2.3118360519e+01, -1.2707098722e+00, -8.9394124225e-03,\n         -7.3012824942e-06,  6.0733418650e-07, -6.8714976311e-02,\n         -1.7940466932e-04],\n        [-1.1861760616e+00, -1.7072351277e-01,  7.4709236622e-02,\n         -1.6057054698e-01,  1.0028474033e-01,  4.4707970619e+00,\n         -3.2747825980e-01,  1.8114055820e-06, -6.0276460648e-01,\n         -2.9894538879e+01],\n        [-2.0331549644e-01, -9.2998981476e-01, -2.3422073573e-03,\n          6.5794992447e-01, -4.0772670507e-01, -1.7908929586e+00,\n         -4.3703973293e-02, -2.3664340377e-02,  3.4835241735e-02,\n          7.3881530762e-01],\n        [ 5.4340696335e-01,  1.3241521083e-04, -3.2028186321e-01,\n         -1.6411489248e-01, -8.0035102367e-01, -1.0085972399e-01,\n         -2.3231016099e-01,  9.6048679352e+00, -1.3925330162e+01,\n         -8.2148885727e-01],\n        [-6.0046720505e-01,  1.0296676308e-02, -5.9643266723e-03,\n         -2.2244569845e-04,  1.5874393284e-03,  9.7708535194e-01,\n         -7.4371069670e-02,  8.7442662334e-05, -2.0362114906e-01,\n          1.5027550697e+01],\n        [ 1.2441553175e-02,  3.5354614258e+00, -4.9783945084e-01,\n          1.0338279605e-01,  2.9940547943e+00, -1.0266765952e-01,\n          1.2045311928e-01, -3.1238024235e+00,  3.3330893517e+00,\n         -4.7617787123e-01]], requires_grad=True)\nGradient tensor(0.0078644780)\n\nIndex 3\nParameter containing:\ntensor([-0.0413947999,  0.2711434066,  0.0748769045,  0.1031675935,\n         0.0756872594,  0.3022760451,  0.2172745764, -0.2653046250,\n         0.2037093341, -0.0445466638], requires_grad=True)\nNo gradient?\n\nIndex 4\nParameter containing:\ntensor([[-4.1193764657e-02, -3.5373184830e-02,  1.8808110617e-03,\n         -3.8154840004e-03, -2.7028546333e+00, -1.3087383270e+01,\n         -3.1675234437e-02, -7.3683762550e-01, -4.4051003456e-01,\n         -1.4208417851e-03]], requires_grad=True)\nGradient tensor(0.0099373152)\n\nIndex 5\nParameter containing:\ntensor([-0.1635424048], requires_grad=True)\nNo gradient?\n<\/code>\nNow, by downloading the gist the changes can be made in L-342 with link \"https:\/\/gist.github.com\/muammar\/6d298f7fa5700efdbd01ae697f63e089#file-neuralnetwork-py-L342\". Thank you very much for your help. I am embarrassed but honestly lost.\nThanks for the info!\nI tried to debug your code and stumbled upon this line of code with link \"https:\/\/gist.github.com\/muammar\/6d298f7fa5700efdbd01ae697f63e089#file-neuralnetwork-py-L200\". You are detaching image_energy by calling .item(). Later in your get_loss method, you are using outputs to calculate the gradients.\nHowever, since image_energy was detached, the computation graph shouldn\u2019t compute any valid gradients before this point.\nI\u2019m not completely understanding the code, so let me know, if I\u2019m on the wrong path. \n\nThanks for the info!\nI tried to debug your code and stumbled upon this line of code with link \"https:\/\/gist.github.com\/muammar\/6d298f7fa5700efdbd01ae697f63e089#file-neuralnetwork-py-L200\". You are detaching  image_energy  by calling  .item() . Later in your  get_loss  method, you are using  outputs  to calculate the gradients.\nHowever, since  image_energy  was detached, the computation graph shouldn\u2019t compute any valid gradients before this point.\n\nI think I understand what you mean about the problem I create when detaching the image_energy with .item(). I proceeded to change with link \"https:\/\/gist.github.com\/muammar\/6d298f7fa5700efdbd01ae697f63e089\/revisions#diff-cb70bad5acc04a28940d30ec97742299\" the line of code you referred above removing the .item() and changed L-202 with link \"https:\/\/gist.github.com\/muammar\/6d298f7fa5700efdbd01ae697f63e089#file-neuralnetwork-py-L202\" to outputs = torch.tensor(outputs, requires_grad=True). I still get the same problem. Then it means that the tensors I am using to compute the loss function are broken. Is that correct?\n\nI\u2019m not completely understanding the code, so let me know, if I\u2019m on the wrong path. \n\nYour analysis is right. Let me just give a brief idea about what is going on.  These targets are two energies (scalars) of two molecules with 4 atoms each. The inputs, in this case, are 4 vectors for each each molecule because there are 4 atoms (see here with link \"https:\/\/gist.github.com\/muammar\/6d298f7fa5700efdbd01ae697f63e089#file-neuralnetwork-py-L304\"). Applying forward to those features will return atomic energies. That is why I sum them, add their sum (image_energy) to the outputs array, and then pass them to the loss. I probably am failing to see how to avoid summing those 4 atomic energies to recover the total energy and avoid detaching and breaking the graph calculation.\nThanks for the explanation!\nYour experiment sounds really interesting.\nRewrapping output in a tensor also detaches the graph.\nYour code should work with the following changes:\n<code class=\"lang-python\">...\n     outputs.append(image_energy)\noutputs = torch.stack(outputs)\nloss, rmse = self.get_loss(outputs, targets, 4)\n<\/code>\nCould you check for valid results?\nI just checked my commits and I was using  torch.stack but dropped it at some point. With that change, now I am getting this (thanks for spotting that!);\n<code class=\"lang-auto\">outputs\ntensor([[-14.5754413605],\n        [-14.5754413605]], grad_fn=<StackBackward>)\ntargets\ntensor([-14.5868730545, -14.5640010834])\nDiff in intercept_Cu\nDiff in slope_Cu\nDiff in linears.Cu.0.weight\nDiff in linears.Cu.0.bias\nDiff in linears.Cu.2.weight\nDiff in linears.Cu.2.bias\nDiff in linears.Cu.4.weight\nDiff in linears.Cu.4.bias\n\nOptimized parameters for Cu symbol\nIndex 0\nParameter containing:\ntensor([[-3.2861328236e-06,  2.6473844628e-05,  3.5650995045e-08,\n          1.4864326658e-06, -1.0720622959e-04,  3.0406141605e-08,\n          1.3593539488e-11, -1.9859398570e-09],\n        [-5.8556226534e-10, -1.1646034137e-10, -3.3600910682e-12,\n          6.7106892265e-10,  1.8430428624e-13,  1.9599950107e-08,\n          2.4708364435e-05,  3.2043487863e-07],\n        [ 1.9733222143e-04,  1.3596744564e-10,  1.6436105810e-08,\n         -3.0531379647e-09, -6.3785437305e-06,  7.4555270811e-13,\n          8.1752958067e-05, -1.1181630725e-05],\n        [-1.2528111881e-09,  4.6580535127e-06,  2.3549859979e-12,\n          2.8091712984e-11,  1.7993905931e-04, -1.4886735508e-12,\n         -9.2567507479e-12,  2.8859590202e-06],\n        [ 3.4168685943e-06, -5.6807679357e-05,  1.2368669559e-05,\n         -2.1798576399e-12, -1.4500128600e-05, -3.1362407071e-07,\n          1.7807322283e-09,  9.7959136838e-06],\n        [ 8.9927290503e-08, -5.2266013739e-09, -9.1957379753e-14,\n         -2.9821003977e-07, -1.7513568764e-06,  1.0372443600e-13,\n          1.2319574694e-07, -3.6574114347e-05],\n        [ 5.3554540500e-06, -1.8524660845e-05,  1.1853338037e-05,\n         -2.1492420638e-04,  2.3621556466e-05, -8.0939061009e-11,\n         -6.9240194023e-08,  1.5314364646e-05],\n        [-2.0754782781e-08, -1.9774879547e-05, -1.3601642422e-05,\n          5.2368657634e-05,  3.3497635741e-05, -5.8766081565e-06,\n         -6.5623047703e-05,  3.9108752389e-05],\n        [-1.5761332861e-06, -7.3087621786e-06, -2.9493878628e-07,\n         -4.5463502829e-07, -3.2682427786e-07, -1.4819252101e-05,\n          2.6041425372e-05, -1.0358776308e-06],\n        [-6.6475655558e-07, -1.3438479496e-10,  1.8068027430e-07,\n         -2.5042306007e-09, -5.2879945542e-06, -6.9557786446e-06,\n          2.9763690179e-08, -2.8302894425e-14]], requires_grad=True)\nGradient tensor(-6.9187924964e-05)\n\nIndex 1\nParameter containing:\ntensor([ 0.0316366255,  0.2617242932,  0.3015798032, -0.0021922502,\n        -0.0615932010, -0.2602513433, -0.0311477333, -0.2361671627,\n         0.1662444025, -0.0660640150], requires_grad=True)\nGradient tensor(2.4240243301e-16)\n\nIndex 2\nParameter containing:\ntensor([[ 9.7365699503e-08,  4.2661922635e-06,  1.9631031591e-06,\n         -4.2697833123e-05,  6.2208728195e-06,  1.1655485604e-12,\n         -6.4603467763e-05, -1.3209832117e-09,  1.1756450391e-07,\n         -7.9867913882e-07],\n        [ 6.2096376041e-07, -6.1568898673e-06, -7.3711348136e-10,\n          3.8170369088e-09, -1.4567660855e-06, -1.9914123186e-06,\n          2.3581033020e-06, -1.4400919781e-06,  1.2830110308e-09,\n         -9.6331113753e-08],\n        [-1.5564822888e-06,  2.5510303203e-06,  1.5743958670e-07,\n         -1.4166996607e-06, -1.3405845323e-07, -9.9951203083e-06,\n          1.0537170965e-05,  7.7086369856e-06,  2.7015998813e-11,\n          4.8960191457e-10],\n        [-1.7156789909e-06, -2.3564821277e-06,  1.5615292262e-13,\n          3.7418202217e-13, -4.3808788178e-05, -1.6505031454e-05,\n         -8.4225684986e-06,  4.8483889259e-06, -5.0767212656e-08,\n          2.3069132737e-07],\n        [ 2.8512771678e-05, -7.5202997323e-06,  1.7333918549e-06,\n          1.2562820473e-07,  5.2780844271e-05, -1.1339360562e-09,\n          3.8166854659e-13, -1.2022780993e-07,  7.6206299127e-05,\n          1.7692066194e-06],\n        [-1.8082403130e-07,  1.8067876226e-06,  1.5731624337e-10,\n         -3.9476603410e-12, -1.6683844706e-07,  1.6850806333e-06,\n         -2.5483970489e-10, -1.8325088604e-05, -3.7972899918e-06,\n          1.8083280651e-08],\n        [ 1.2292147552e-12,  9.3183753052e-06, -1.0420450280e-07,\n          3.0822411645e-07,  6.6431852019e-07,  9.7349062145e-10,\n          3.3600823372e-05, -2.3434172908e-04, -4.7051515462e-11,\n          3.7607719605e-08],\n        [ 2.9142852020e-10,  1.6880323983e-07,  3.4797506032e-06,\n          2.2227823138e-08, -8.4504938513e-07, -1.0985943663e-04,\n          2.5039498723e-05,  8.6511966211e-13,  1.6281462740e-04,\n         -2.8856720746e-07],\n        [-2.9593747968e-06, -5.8458951457e-08, -5.6971380502e-08,\n         -1.2519759184e-04, -7.4558295735e-13, -2.9341401842e-07,\n          4.2668673927e-08,  4.3059226300e-06, -2.8244965478e-08,\n         -5.0044291129e-06],\n        [ 3.0511776004e-06, -1.0126113713e-15,  5.8472587625e-06,\n         -7.7287486420e-06, -1.2084972241e-06, -4.0337028162e-10,\n          7.3831834015e-05,  1.5755430240e-06,  1.3774927379e-12,\n          1.3072969159e-04]], requires_grad=True)\nGradient tensor(-0.0001017772)\n\nIndex 3\nParameter containing:\ntensor([ 0.1047996879, -0.2729528248,  0.2065724581,  0.0899153724,\n        -0.1029036865, -0.3049356043, -0.2874263823,  0.1054596379,\n        -0.2320392430, -0.0505143851], requires_grad=True)\nGradient tensor(-1.4201264947e-12)\n\nIndex 4\nParameter containing:\ntensor([[ 1.3515332284e-06,  1.9205540269e-08,  1.9853887352e-05,\n          2.2419371817e-06,  1.0494016323e-08, -4.0517086745e-05,\n          1.4675807324e-04,  5.4397496285e-08, -6.9757788879e-08,\n          3.3906806038e-06]], requires_grad=True)\nGradient tensor(0.0001229179)\n\nIndex 5\nParameter containing:\ntensor([-0.2859740555], requires_grad=True)\nGradient tensor(-7.3175310256e-09)\n\n<\/code>\nThe two variables that were not changing before now they are changing. However, outputs remained the same even though the loss function value at epoch 1000 was 1.643620e-05. Do you know any idea why? Doesn\u2019t that mean that outputs are very near to the targets? Meanwhile, I am playing with learning rate and weight decay and see what happens.\nSo while the loss decreases the outputs stay approx. the same? Let me know if that\u2019s the case and I do some more debugging.\nOne issue that comes to my mind is that we\u2019ve recently had similar problems using nn.MSELoss when the model output and target had a shape mismatch and we\u2019re silently broadcasted. Could you check the shapes of all tensors used in nn.MSELoss?\n\n\n\n ptrblck:\n\nSo while the loss decreases the outputs stay approx. the same? Let me know if that\u2019s the case and I do some more debugging.\n\n\nThat is exactly the case. While the loss function is decreasing the outputs remain the same.\n\n\n\n ptrblck:\n\nOne issue that comes to my mind is that we\u2019ve recently had similar problems using nn.MSELoss when the model output and target had a shape mismatch and we\u2019re silently broadcasted. Could you check the shapes of all tensors used in nn.MSELoss ?\n\n\nOK! I checked, and the shapes are not the same!\n<code class=\"lang-auto\">output = torch.Size([2, 2])\ntarget = torch.Size([2])\n<\/code>\nWhy stack is changing the shape of the output tensor??? What can I use instead of stack? I really appreciate all your help!!\nGreat! If you don\u2019t want an additional dimension, you could use torch.cat, but you would end up with 4 values nevertheless so there is a shape mismatch indeed.\nIf you are dealing with energies for 4 atoms, I would expect the target to also have 4 values. Does it make sense or what am I still missing something? \n\n\n\n ptrblck:\n\nGreat! If you don\u2019t want an additional dimension, you could use torch.cat , but you would end up with 4 values nevertheless so there is a shape mismatch indeed.\nIf you are dealing with energies for 4 atoms, I would expect the target to also have 4 values. Does it make sense or what am I still missing something? \n\n\nIt makes sense, but I don\u2019t have access to energies per atom :(. The energy of a molecular system comes from something we call the wave function and solving the Schr\u00f6dinger equation using that function returns only total energies and not atomic ones.\nNow with cat sizes match:\n<code class=\"lang-auto\">torch.Size([2])\ntorch.Size([2])\n<\/code>\nBut the loss keeps decreasing without the outputs changing. Do you think I should change the forward method?\nThanks for the explanation. I remember some things from high school about the energy levels etc.\nI\u2019m not completely sure what\u2019s going on as I would assume your output should now contain 4 elements.\nI would like to debug it a bit later this evening as I currently don\u2019t have access to my machine.\n thanks! I will keep  changing things here and there. I am very interested in understanding what is causing this issue.\nI tried to debug the code a bit more and in my code the shapes of the output and target were [2, 1] and [2], respectively.\nTo fix this, I just created the target as:\n<code class=\"lang-python\">targets = [[-14.586873530850994], [-14.56400104603344]]\n<\/code>\nAlso, it seems you are passing the input in a shape of [8]. A linear layer would expect an input of [batch_size, nb_features], so I unsqueezed the tensor in dim0:\n<code class=\"lang-python\">X = X.unsqueeze(0)\nX = self.linears[symbol](X)\n<\/code>\nThe output values are really close to each other and sometimes have even the same values.\nIf I just change the targets randomly to have a larger distance, the output values also diverge a bit, but generally your input and target seem to contain the \u201csignal\u201d in a low precision range.\nHave you thought about some normalization scheme?\nI\u2019m a bit afraid that we are currently running into floating point precision issues (~1e-6 would be the limit).\nHowever, even with float64 precision, I couldn\u2019t really fit the data.\n<code class=\"lang-python\">loss 9.11804408893418e-6, rmse 0.011436155542466114\n<\/code>"},{"x":"Hi, I am trying to implement parts of the Class Activation Map algorithm that requires computing the gradients of the output logit with respect to the last convolutional activation; I have come across some issues and I don\u2019t think I understand how to do it.\nI understand that I need to register a hook since the activations are intermediate variables, but my registered hook doesn\u2019t seem to be triggered on backward. Here is a simple code snippet with the VGG19 network:\n<code class=\"lang-auto\">def hook(grad):\n    print(\"I am called\")\n    print(grad)\n\nclass VGG(nn.Module):\n    def __init__(self):\n        super(VGG, self).__init__()\n        \n        # get the pretrained VGG19 network\n        self.vgg = vgg19(pretrained=True)\n        \n        # disect the network to access its last convolutional layer\n        self.features_conv = self.vgg.features[:35]\n        \n        # get the relu and the max pool of the features stem\n        self.relu_max_pool_features = self.vgg.features[35:37]\n        \n        # get the classifier of the vgg19\n        self.classifier = self.vgg.classifier\n        \n    def forward(self, x):\n        x = self.features_conv(x)\n        x = self.relu_max_pool_features(x)\n        x = x.view((1, -1))\n        x = self.classifier(x)\n        return x\n    \n    def get_activations(self, x):\n        return self.features_conv(x)\n\n# - - - - -\nvgg = VGG()\nimg, label = next(iter(dataloader))\npred = vgg(img)\n\n# get the activations of the last conv layer in the features stem\nactivations = vgg.get_activations(img)\n\n# register the hook\nactivations.register_hook(hook)\n\n# calculate the gradients of the logit wrt to all the parameters\npred[:, 805].backward()\n<\/code>\nNo gradient is then saved neither in .grad (which is understandable); however, the hook is never triggered either.\nAlso, if there is an easier way to get to the activations of pretrained networks, I would love to learn it. Any help is appreciated.","y":"Ok, I have been able to solve the problem. It seems that the way my model class is written when I call get_activations() a new tensor is returned, hence the backward() doesn\u2019t compute the gradient of the outputs with respect to the activations. I fix it by attaching a hook to the activation tensor within the forward method like this:\n<code class=\"lang-auto\">class VGG(nn.Module):\n    def __init__(self):\n        super(VGG, self).__init__()\n        \n        # get the pretrained VGG19 network\n        self.vgg = vgg19(pretrained=True)\n        \n        # disect the network to access its last convolutional layer\n        self.features_conv = self.vgg.features[:35]\n        \n        # get the relu and the max pool of the features stem\n        self.relu_max_pool_features = self.vgg.features[35:37]\n        \n        # get the classifier of the vgg19\n        self.classifier = self.vgg.classifier\n        \n        # placeholder for the gradients\n        self.gradients = None\n        \n    def activations_hook(self, grad):\n        self.gradients = grad\n        \n    def forward(self, x):\n        x = self.features_conv(x)\n        \n        # register the hook\n        x.register_hook(self.activations_hook)\n        \n        x = self.relu_max_pool_features(x)\n        x = x.view((1, -1))\n        x = self.classifier(x)\n        return x\n    \n    def get_activations_gradient(self):\n        return self.gradients\n<\/code>\nThis way the gradient is returned as expected when called with vgg.get_activations_gradient().","z":"Ok, I have been able to solve the problem. It seems that the way my model class is written when I call get_activations() a new tensor is returned, hence the backward() doesn\u2019t compute the gradient of the outputs with respect to the activations. I fix it by attaching a hook to the activation tensor within the forward method like this:\n<code class=\"lang-auto\">class VGG(nn.Module):\n    def __init__(self):\n        super(VGG, self).__init__()\n        \n        # get the pretrained VGG19 network\n        self.vgg = vgg19(pretrained=True)\n        \n        # disect the network to access its last convolutional layer\n        self.features_conv = self.vgg.features[:35]\n        \n        # get the relu and the max pool of the features stem\n        self.relu_max_pool_features = self.vgg.features[35:37]\n        \n        # get the classifier of the vgg19\n        self.classifier = self.vgg.classifier\n        \n        # placeholder for the gradients\n        self.gradients = None\n        \n    def activations_hook(self, grad):\n        self.gradients = grad\n        \n    def forward(self, x):\n        x = self.features_conv(x)\n        \n        # register the hook\n        x.register_hook(self.activations_hook)\n        \n        x = self.relu_max_pool_features(x)\n        x = x.view((1, -1))\n        x = self.classifier(x)\n        return x\n    \n    def get_activations_gradient(self):\n        return self.gradients\n<\/code>\nThis way the gradient is returned as expected when called with vgg.get_activations_gradient()."},{"x":"Pseudo code\uff1a\ndef forward(self,x):\nout = torch.zeros(10,10,10,10)\n\u2026\nout[:,:,:,col][:,:,row,:] = x  # col row are indices\nI feel like this is weird\uff0cbecause I did not operate on x itself. Instead, x is assigned to a part of another leaf tensor. Does this have a influence on backward\uff1f","y":"Hi,\nYou can do this, and if the backward does not raise an error you will get the correct gradients.\nWhat can happen though is that you get Tensor needed for backward has been changed by an inplace operation. As inplace operations can change a tensor needed during the backward pass. The engine is over-restrictive and so if it works, will always give the right answer. If it does not, you will have to remove the inplace ops. To create a vector of results, you can for example store them in a list while you create them and then cat them at the end.","z":"Hi,\nYou can do this, and if the backward does not raise an error you will get the correct gradients.\nWhat can happen though is that you get Tensor needed for backward has been changed by an inplace operation. As inplace operations can change a tensor needed during the backward pass. The engine is over-restrictive and so if it works, will always give the right answer. If it does not, you will have to remove the inplace ops. To create a vector of results, you can for example store them in a list while you create them and then cat them at the end.\nThanks for your nice reply\uff01\uff01\uff01"},{"x":"Hi,\nAs far as my understanding, the attribute \u2018\u2018requires_grad\u2019\u2019 of  a parameter should be True if the parameter needs to be updated. But in my code, I find that a \u201cConv2d.weight.data.requires_grad\u201d  is False.\nI just added the conv layer as the normal way: self.conv1 = nn.Conv2d(\u2026)\nIs there something wrong with my understanding or with my code?","y":"requires_grad is always False for the data attribute. This is a legacy attribute for performing operations without getting them tracked by autograd. You should not use .data. The recommended ways to achieve the same are \u1e81ith torch.no_grad(): or .detach().\nHave a look at the following example:\n<code class=\"lang-auto\">In[1]: x = torch.nn.Conv2d(2, 2, 1)\nIn[2]: x.weight.requires_grad\nOut[2]: True\nIn[3]: x.weight.data.requires_grad\nOut[3]: False\n<\/code>","z":"requires_grad is always False for the data attribute. This is a legacy attribute for performing operations without getting them tracked by autograd. You should not use .data. The recommended ways to achieve the same are \u1e81ith torch.no_grad(): or .detach().\nHave a look at the following example:\n<code class=\"lang-auto\">In[1]: x = torch.nn.Conv2d(2, 2, 1)\nIn[2]: x.weight.requires_grad\nOut[2]: True\nIn[3]: x.weight.data.requires_grad\nOut[3]: False\n<\/code>"},{"x":"How can I minimize quadratic function? I thought the code below would find the solution x = 2, but it doesn\u2019t.\n<code class=\"lang-auto\">import torch\n\nx = torch.tensor(.0, requires_grad=True)\ny = (x-2)**2\n\n# optimizer = torch.optim.SGD([x], lr=0.0001)\noptimizer = torch.optim.Adam([x], lr=0.0001)\n\n# initilizae\nprint(x,y)\n\nfor i in range(30000):\n    optimizer.zero_grad()\n    y.backward(retain_graph=True)\n    optimizer.step()\n\n    if (i + 1) % 1000 == 0:\n        print(i + 1, x, y)\n<\/code>\nHere is the result.\n<code class=\"lang-auto\">tensor(0., requires_grad=True) tensor(4., grad_fn=<PowBackward0>)\n1000 tensor(0.1000, requires_grad=True) tensor(4., grad_fn=<PowBackward0>)\n2000 tensor(0.2000, requires_grad=True) tensor(4., grad_fn=<PowBackward0>)\n3000 tensor(0.3000, requires_grad=True) tensor(4., grad_fn=<PowBackward0>)\n4000 tensor(0.4000, requires_grad=True) tensor(4., grad_fn=<PowBackward0>)\n5000 tensor(0.5000, requires_grad=True) tensor(4., grad_fn=<PowBackward0>)\n6000 tensor(0.6000, requires_grad=True) tensor(4., grad_fn=<PowBackward0>)\n7000 tensor(0.7000, requires_grad=True) tensor(4., grad_fn=<PowBackward0>)\n8000 tensor(0.8000, requires_grad=True) tensor(4., grad_fn=<PowBackward0>)\n9000 tensor(0.9000, requires_grad=True) tensor(4., grad_fn=<PowBackward0>)\n10000 tensor(1.0001, requires_grad=True) tensor(4., grad_fn=<PowBackward0>)\n11000 tensor(1.1001, requires_grad=True) tensor(4., grad_fn=<PowBackward0>)\n12000 tensor(1.2001, requires_grad=True) tensor(4., grad_fn=<PowBackward0>)\n13000 tensor(1.3001, requires_grad=True) tensor(4., grad_fn=<PowBackward0>)\n14000 tensor(1.4001, requires_grad=True) tensor(4., grad_fn=<PowBackward0>)\n15000 tensor(1.5001, requires_grad=True) tensor(4., grad_fn=<PowBackward0>)\n16000 tensor(1.6002, requires_grad=True) tensor(4., grad_fn=<PowBackward0>)\n17000 tensor(1.7002, requires_grad=True) tensor(4., grad_fn=<PowBackward0>)\n18000 tensor(1.8002, requires_grad=True) tensor(4., grad_fn=<PowBackward0>)\n19000 tensor(1.9002, requires_grad=True) tensor(4., grad_fn=<PowBackward0>)\n20000 tensor(2.0002, requires_grad=True) tensor(4., grad_fn=<PowBackward0>)\n21000 tensor(2.1001, requires_grad=True) tensor(4., grad_fn=<PowBackward0>)\n22000 tensor(2.2000, requires_grad=True) tensor(4., grad_fn=<PowBackward0>)\n23000 tensor(2.2999, requires_grad=True) tensor(4., grad_fn=<PowBackward0>)\n24000 tensor(2.3998, requires_grad=True) tensor(4., grad_fn=<PowBackward0>)\n25000 tensor(2.4997, requires_grad=True) tensor(4., grad_fn=<PowBackward0>)\n26000 tensor(2.5996, requires_grad=True) tensor(4., grad_fn=<PowBackward0>)\n27000 tensor(2.6995, requires_grad=True) tensor(4., grad_fn=<PowBackward0>)\n28000 tensor(2.7994, requires_grad=True) tensor(4., grad_fn=<PowBackward0>)\n29000 tensor(2.8993, requires_grad=True) tensor(4., grad_fn=<PowBackward0>)\n30000 tensor(2.9992, requires_grad=True) tensor(4., grad_fn=<PowBackward0>)\n<\/code>\nThe equivalent TF code will be like the blow and it works.\n<code class=\"lang-auto\">  \nimport tensorflow as tf\n\nx = tf.Variable(0.0)\ny = (x-2)**2\n\nstep = tf.train.AdamOptimizer(0.01).minimize(y)\n\nwith tf.Session() as sess:\n    sess.run([tf.global_variables_initializer()])\n    _x, _y = sess.run([x, y])\n    print(0, _x, _y)\n    for i in range(10000):\n        _, _x, _y = sess.run([step, x, y])\n        if (i + 1) % 100 == 0:\n            print(i + 1, _x, _y)\n<\/code>","y":"<code class=\"lang-auto\">for i in range(30000):\n    optimizer.zero_grad()\n    y = (x-2)**2\n    y.backward(retain_graph=True)\n    optimizer.step()\n<\/code>\n\n19000 tensor(1.7238) tensor(1.00000e-02 *7.6316)\n20000 tensor(1.7971) tensor(1.00000e-02 *4.1186)\n21000 tensor(1.8638) tensor(1.00000e-02 *1.8556)\n22000 tensor(1.9210) tensor(1.00000e-03 *6.2550)\n23000 tensor(1.9642) tensor(1.00000e-03 *1.2822)\n24000 tensor(1.9896) tensor(1.00000e-04 *1.0800)\n25000 tensor(1.9986) tensor(1.00000e-06 *1.8781)\n26000 tensor(2.0000) tensor(1.00000e-09 *2.1949)\n27000 tensor(2.0000) tensor(1.00000e-11 *7.3669)\n28000 tensor(2.0000) tensor(1.00000e-11 *2.7512)\n29000 tensor(2.0000) tensor(1.00000e-12 *9.6065)\n30000 tensor(2.0000) tensor(1.00000e-12 *3.6380)\n","z":"<code class=\"lang-auto\">for i in range(30000):\n    optimizer.zero_grad()\n    y = (x-2)**2\n    y.backward(retain_graph=True)\n    optimizer.step()\n<\/code>\n\n19000 tensor(1.7238) tensor(1.00000e-02 *7.6316)\n20000 tensor(1.7971) tensor(1.00000e-02 *4.1186)\n21000 tensor(1.8638) tensor(1.00000e-02 *1.8556)\n22000 tensor(1.9210) tensor(1.00000e-03 *6.2550)\n23000 tensor(1.9642) tensor(1.00000e-03 *1.2822)\n24000 tensor(1.9896) tensor(1.00000e-04 *1.0800)\n25000 tensor(1.9986) tensor(1.00000e-06 *1.8781)\n26000 tensor(2.0000) tensor(1.00000e-09 *2.1949)\n27000 tensor(2.0000) tensor(1.00000e-11 *7.3669)\n28000 tensor(2.0000) tensor(1.00000e-11 *2.7512)\n29000 tensor(2.0000) tensor(1.00000e-12 *9.6065)\n30000 tensor(2.0000) tensor(1.00000e-12 *3.6380)\n"},{"x":"Hello,\nI am running in the backpropagation error that states that \u201cTrying to backward through the graph a second time, but the buffers have already been freed. Specify retain_graph=True when calling backward the first time\u201d.\nHowever, I don\u2019t see where is my code trying to run through the graph a second time. Let me show you two examples, the first which works fine, and the second which does not, and I do not understand why it fails.\n<code class=\"lang-auto\">x = torch.randn( (5, 4), requires_grad=True )  \ntheta = torch.randn( (4, 3), requires_grad=False )\n\noptimizer = optim.Adam( [x] )\n\nxx = x  # Apparently this is the conflicting line\n\nfor k in range(10):\n    m = torch.matmul( xx, theta )\n    \n    optimizer.zero_grad()\n    m.sum().backward()\n    optimizer.step()\n    \n<\/code>\nThis works fine. However, if I change the xx declaration to simply\n<code class=\"lang-auto\">xx = x\/10\n<\/code>\nthen I get the backpropagation error. Also setting \u201cretain_graph=True\u201d as suggested simply makes \u201cx\u201d not optimise at all, it behaves as if \u201crequires_gradient\u201d were set to \u201cFalse\u201d, which is not the behaviour I need.\nI don\u2019t understand what is going on.","y":"Part of your computation graph is outside the loop (xx = x\/10).\nAfter the first optimizer.step() call, the intermediate values will be cleared and thus x cannot be optimized anymore. If you move xx = x\/10 into the loop, the code should work.","z":"Part of your computation graph is outside the loop (xx = x\/10).\nAfter the first optimizer.step() call, the intermediate values will be cleared and thus x cannot be optimized anymore. If you move xx = x\/10 into the loop, the code should work.\nI see, thanks a lot.\nWith that division I was trying to reduce the variance of the initial random sampling in X, so the logical way for me to do so was simply once outside the loop. Ideally I would first have declared XX with the reduced variance and then added it to the optimiser, but if done like that pytorch complains that XX is not a leaf variable; ie\n<code class=\"lang-auto\">x = torch.randn( (5, 4), requires_grad=True ) \nxx = x\/10\noptimizer = optim.Adam( [xx] )  # ValueError: can't optimize a non-leaf Tensor\n<\/code>\nIs there any other way to have a tensor in the optimiser that is the result of some previous computations?\nThanks!\nNevermind, found the solution:\n<code class=\"lang-auto\">x = torch.randn( (5, 4) ) \nxx = x\/10\nxx.requires_grad=True\noptimizer = optim.Adam( [xx] )  # Works as expected!\n<\/code>"},{"x":"What are your preferences and the reasoning behind them?\nI saw this transform in my Udacity class, and it seemed redundant in the use of both:\n<code class=\"lang-auto\">transform = transforms.Compose([transforms.Resize(255),\n                                 transforms.CenterCrop(224),\n                                 transforms.ToTensor()])\n<\/code>\nI was thinking Resize keeps the amount of information the same, but distorts it. It seems like CenterCrop risks cutting out important bits, but what it does keep isn\u2019t overly distorted.\nJust a newb question! ","y":"non uniform as in the size of the image.\nAnd most likely the subject of interest in at the center of the image, hence the center crop.","z":"it seems to me that whomever created this transform function is expecting non-uniform input image. the Resize functions is there only to ensure uniformity, and center crop reduce background noise.\nWhat do you mean non-uniform? As in the size is non-uniform (which is true), or something else?\nWhat sort of background would one expect? This is the Cat and Dog dataset, by the way.\nnon uniform as in the size of the image.\nAnd most likely the subject of interest in at the center of the image, hence the center crop."},{"x":"<code class=\"lang-auto\">class MyLayer(nn.module):\n    def __init__(self, num_units):\n        self.weight = Parameter(torch.rand(num_units, num_units)) \n<\/code>\nI want to optimize all of the weights in the tensor except for the ones across the diagonal (i.e. the weights across the diagonal should stay fixed). What is the simplest way to exclude these weights across the diagonal from being changed when I perform backpropagation (with loss.backward())?","y":"After loss.backward() and before optimizer.step(), you can do\nmodel.layer.weight.grad.diagonal().zero_().\nHaving gradient entries constant to zero on every invocation should leave the values fixed with most optimizers.\nSophisticated people might mention backward hooks, but I am not one of them.\nBest regards\nThomas","z":"After loss.backward() and before optimizer.step(), you can do\nmodel.layer.weight.grad.diagonal().zero_().\nHaving gradient entries constant to zero on every invocation should leave the values fixed with most optimizers.\nSophisticated people might mention backward hooks, but I am not one of them.\nBest regards\nThomas"},{"x":"I\u2019m using torch version 1.0.0.\nI am trying to perform a gradient descent optimization using two different optimizers: SGD and Adam from the optim package. Strangely, while the loss function obtained by Adam converges and is steadily decreasing, the loss with SGD increases dramatically from the start and contains nan after just a few iterations.\nThis is true for different learning rates, momenta and also after adding regularisation terms.\nWhy is this??\nBelow is the code. You will not be able to run it as is as it imports some of my classes but I will be happy to provide anyone with the rest of the code if need be. The important part is the gradient descent loop at the end:\n<code class=\"lang-auto\">import torch\nimport torch.nn as nn\nimport torch.optim as optim\nimport HiddenStatesModel as HSM\nimport numpy as np\nimport matplotlib.pyplot as plt\n\ndef S(x):\n    return 1\/(1+torch.exp(-x))\n\nsft = nn.functional.softmax\n\ndef forward(StatesVec, kappa, rho, L, dt):\n    lbd = 1\/8\n    StatesVec = StatesVec + (kappa*(-lbd*StatesVec -rho.mm(S(StatesVec)) + torch.ones(N,1,dtype=torch.float64)))*dt\n    # Use factor 2 in sft() to have a \"stronger\" softmax function\n    pos = L.mm(sft(StatesVec*2, dim=0))\n    return StatesVec, pos\n\ntimesteps = 1100\nN = 6\ndt = 0.1\n\n\"\"\"Create connectivity matrix rho\"\"\"\nrho = torch.zeros(N,N,dtype=torch.float64);\nfor i in range(N):\n    for j in range(N):\n        if i == j:\n            rho[i, j] = 0\n        elif j == i + 1:\n            rho[i, j] = 1.5\n        elif j == i - 1:\n            rho[i, j] = 0.5\n        else:\n            rho[i, j] = 1\n\nrho[-1, 0] = 1.5\nrho[0, -1] = 0.5\n\nprocess = HSM.GenerativeProcess()\n\nLtrue = np.array([[1,1.1,1,1.2,1.4,1.3],\n                  [1,1.2,0.4,1.3,0.9,0.6]]);\n    \nOBStrajectory, HSPtrajectory = process.run(N, 2, timesteps, Ltrue[:, 0:N], dt=dt)\n\ntarget = torch.tensor(OBStrajectory, requires_grad = False, dtype = torch.float64)\n\n\"\"\"L and kappa will be inferred. Randomly initialise L, set initial kappa to 5.5\"\"\"\nL = torch.tensor(np.random.randn(2,N),dtype = torch.float64, requires_grad = True)\nkappa = torch.tensor(5.5, requires_grad = True, dtype = torch.float64)\n\n\niterations = 100\nlosses = []\n\noptimizer = optim.SGD([L, kappa], lr = 0.05, momentum = 0.0)\n#optimizer = optim.Adam([L, kappa], lr = 0.05)\nfor _ in range(iterations):\n    \n    \"\"\"Initialise States Vector\"\"\"\n    StatesVec = torch.tensor(HSPtrajectory[:, 0]).view([6, 1])\n    \n    \"\"\"HS stands for Hidden States\"\"\"\n    HSHistory = np.zeros([6, timesteps])\n    loss = 0\n    for t in range(timesteps):\n        StatesVec, pos = forward(StatesVec, kappa, rho, L, dt)\n        HSHistory[:, t] = StatesVec.clone().detach().numpy()[:, 0]\n        loss += torch.sum((pos - target[:, t].view(2,1))**2)\n    \n    losses.append(loss)\n    optimizer.zero_grad()\n    loss.backward()\n    optimizer.step()\n    \n    print(\"Iteration no %d\"%_)\n    \n\nplt.figure()\nplt.plot(losses)\nplt.ylabel('Loss')\nplt.xlabel('# iterations')\nplt.show()\n<\/code>\n\n","y":"Due to the normalization implicit in Adam, the learning rates aren\u2019t comparable to SGD, and you probably use a much too high learning rate for SGD.\nBest regards\nThomas","z":"Due to the normalization implicit in Adam, the learning rates aren\u2019t comparable to SGD, and you probably use a much too high learning rate for SGD.\nBest regards\nThomas\nThank you, that resolves the problem."},{"x":"I find some codes in this with link \"https:\/\/github.com\/facebookresearch\/maskrcnn-benchmark\/blob\/55d3ab44aa9ff5261afb8244ab5e265266d8436f\/maskrcnn_benchmark\/layers\/roi_align.py#L25\" and become curious about the function of .","y":"Yes.\nYou should use it when you write a backward that cannot be used for high-order gradients.","z":"If you wrap your Function's backward method with this wrapper, the Tensors that you will get as input will never require gradients and you don\u2019t have to write a backward function that computes the gradients in a differentiable manner. For example, you can use other libraries to do the computation.\nIf you try to backward through the backward pass of such Function, an error will be raised stating that this Function is only differentiable once.\nThx for your reply but it\u2019s a bit confusing\u2026Is that meaning we cannot compute high-order gradients from this function?\nYes.\nYou should use it when you write a backward that cannot be used for high-order gradients.\nGot it!\nVery helpful!"},{"x":"Does the nn.funtional.grid_sample backwards grad to grid??\ntorch.nn.functional. grid_sample ( input ,  grid ,  mode=\u2018bilinear\u2019 ,  padding_mode=\u2018zeros\u2019 )\nhttps:\/\/pytorch.org\/docs\/stable\/nn.html#torch-nn-functional\nHello, I just wonder that the bilinear interpolation operation based \u2018grid_sample\u2019 is often used to compute affine transformation or in the spatial transformaiton network\nI already know that this function does propagate the grad to the input data\nbut does this function propagate the grad to the input grid?","y":"yes it computes gradient wrt grid.","z":"   could you plz help me a little?\nyes it computes gradient wrt grid.\nalthough you will get 0 grad grid if you are using it with nearest neighbor sampling\nwhy 0 grad with nearest mode but not     \u2018bilinear\u2019 mode,    big thx~\n I am little bit confused after hours of thinking, how actually did the back end kernel calculate the grad for each grid point?   I just cant figure out, much thx\nBecause the gradient is zero almost everywhere and undefined otherwise. It\u2019s the same reason why ceil(x) gives all zero gradients.\n\n\n\n Zichun_Zhang:\n\nhow actually did the back end kernel calculate the grad for each grid point? I just cant figure out, much thx\n\n\nIn forward, each output pixel is a linear interpolation (weighted sum) of input pixels, where the interpolation weights are computed using the mode and grid values. In most cases, this computation is simple and differentiable, e.g., bilinear. So on a high level, it goes like grad_output -> grad_interpolation_weights -> grad grid. (In real code there are a lot of other detailed optimizations.) If you are interested, CPU kernel is at https:\/\/github.com\/pytorch\/pytorch\/blob\/b039a715ce4e9cca82ae3bf72cb84652957b2844\/aten\/src\/ATen\/native\/cpu\/GridSamplerKernel.cpp and GPU kernel is at https:\/\/github.com\/pytorch\/pytorch\/blob\/b039a715ce4e9cca82ae3bf72cb84652957b2844\/aten\/src\/ATen\/native\/cuda\/GridSampler.cu.\nWhat a amazing reply, hope it didnt waste your time , \nI understand it a little bit more now, and I will step further into the source code you post\nIf I got further questions I\u2019d like to have your advice, much thx"},{"x":"Hi!\nI was wondering if it\u2019s possible to train partially with fp16.\nI\u2019m dealing with a very big network. I would like to use fp16 in a submodule of the network.\nDoes autograd deal with casting from float16 to float32 during the backprop?\nIn other post i saw someone who recommends to use fp32 on batch normalization not to have convergence issues. So there should be no problem right?","y":"Yes, it\u2019s possible.\nHowever, using FP16, you should take care of some possible pitfalls, e.g. imprecise weight updates or gradient underflow. NVIDIA recently published their mixed precision utilities for PyTorch named apex with link \"https:\/\/github.com\/NVIDIA\/apex\".\nOn the one side you have the automatic mixed precision training using amp, while on the other side the FP16_Optimizer gives you a bit more control over your workflow.\nHave a look at this small tutorial with link \"https:\/\/github.com\/ptrblck\/apex\/blob\/apex_tutorial\/tutorials\/apex_mixed_precision_intro.ipynb\" I\u2019ve written a few days ago. It\u2019s still not finished and  is currently reviewing it, but should be a good starter I hope.","z":"Yes, it\u2019s possible.\nHowever, using FP16, you should take care of some possible pitfalls, e.g. imprecise weight updates or gradient underflow. NVIDIA recently published their mixed precision utilities for PyTorch named apex with link \"https:\/\/github.com\/NVIDIA\/apex\".\nOn the one side you have the automatic mixed precision training using amp, while on the other side the FP16_Optimizer gives you a bit more control over your workflow.\nHave a look at this small tutorial with link \"https:\/\/github.com\/ptrblck\/apex\/blob\/apex_tutorial\/tutorials\/apex_mixed_precision_intro.ipynb\" I\u2019ve written a few days ago. It\u2019s still not finished and  is currently reviewing it, but should be a good starter I hope."},{"x":"Hi all,\nI need a bit of help with my PyTorch code. What I am trying to do is to share the weights in the last layer that connects to the output layer while the bias should still be independent.\nI.e., what I am trying to do is to duplicate the weights in one row of the weight matrix of the last fully connected layer over the number of output units. E.g., suppose the last hidden layer and the output layer look like this:\n\nWhat I want to achieve is that these weights are the same:\n\nSuppose I have a convolutional neural network like this:\n<code class=\"lang-python\">class ConvNet(torch.nn.Module):\n\n    def __init__(self, num_classes):\n        super(ConvNet, self).__init__()\n        \n\n        self.conv_1 = torch.nn.Conv2d(in_channels=3,\n                                      out_channels=20,\n                                      kernel_size=(5, 5),\n                                      stride=(1, 1))\n\n        self.conv_2 = torch.nn.Conv2d(in_channels=20,\n                                      out_channels=40,\n                                      kernel_size=(7, 7),\n                                      stride=(1, 1),\n                                      padding=1)                                  \n\n        self.conv_3 = torch.nn.Conv2d(in_channels=40,\n                                      out_channels=80,\n                                      kernel_size=(11, 11),\n                                      stride=(1, 1),\n                                      padding=0)                                 \n        ###############################################\n        \n        self.linear_1 = torch.nn.Linear(1*1*80, num_classes)\n        \n        # Weight sharing\n        self.linear_1.weight[1:] = self.linear_1.weight[0]\n\n        \n    def forward(self, x):\n        out = self.conv_1(x)\n        out = F.relu(out)\n        out = F.max_pool2d(out, kernel_size=(2, 2), stride=(2, 2))\n        \n        out = self.conv_2(out)\n        out = F.relu(out)\n        out = F.max_pool2d(out, kernel_size=(2, 2), stride=(2, 2))\n\n        out = self.conv_3(out)\n        out = F.relu(out)\n        out = F.max_pool2d(out, kernel_size=(2, 2), stride=(2, 2))\n        \n        logits = self.linear_1(out.view(-1, 1*1*80))\n        probas = F.softmax(logits, dim=1)\n\n        return logits, probas\n<\/code>\nI thought I could maybe achieve this by setting\n<code class=\"lang-python\">        # Weight sharing\n        self.linear_1.weight[1:].requires_grad = False\n        self.linear_1.weight[1:] = self.linear_1.weight[0]\n<\/code>\nor\n<code class=\"lang-python\">        # Weight sharing\n        self.linear_1.weight[1:] = self.linear_1.weight[0]\n<\/code>\nas shown in the code example above. Unfortunately, this throws an ValueError: can't optimize a non-leaf Tensor.\nAnother thing I tried was\n<code class=\"lang-python\">        # Weight sharing\n        self.linear_1.weight[1:] = self.linear_1.weight[1:].detach()\n        self.linear_1.weight[1:]= self.linear_1.weight[0].detach()\n<\/code>\nBut this yields the same error.\nDoes anyone have an idea how I could achieve this weight sharing in the last layer? I would really appreciate it!","y":"So, for example if the input to the FC layer has 200 units, and we want output of 100, a single linear is called that takes input of size 200, and an output of size 1. Then, the bias vector of size 100 is added to the output of linear layer using broadcasting operation.","z":"Would it work to define a single linear layer that has output of size 1 and disable the bias using self.linear = torch.nn.Linear(1*1*80, 1, bias=False)? Then you perform the linear FC layer only once and add the biases manually? You can define the bias tensors b1 and b2 as nn.Parameter and add the bias to the output of the final FC layer to get the two different outputs. Therefore, the weights of the FC layer are shared, and biases are defined separately and added independently to each output unit.\nThanks, but I think this would not be an ideal work around, because if I have ~100 output units, I would have to do 100 separate (matrix * sharedvector + bias_i) operations. So, I thought that instead of doing that I want to have a weight matrix with sharedvector as columns so that the GPU can do the regular matrix-matrix multiplication.\nSo, for example if the input to the FC layer has 200 units, and we want output of 100, a single linear is called that takes input of size 200, and an output of size 1. Then, the bias vector of size 100 is added to the output of linear layer using broadcasting operation.\nThanks a lot Vahid. That\u2019s a good point. Doing the matrix-matrix multiplication with the shared weights is wasteful. It\u2019s much more efficient to only have a weight vector and then duplicate the outputs, and then add the bias to it. For future reference, the modification (which seems to work) is:\n\nHave only a weight vector and define the bias manually\n\n<code class=\"lang-python\">        self.linear_1 = torch.nn.Linear(1*1*80, 1, bias=False)\n        self.linear_1_bias = nn.Parameter(torch.tensor(torch.zeros(num_classes),\n                                                       dtype=self.linear_1.weight.dtype))\n<\/code>\n\nThen duplicate the outputs over all output units and add the bias vector:\n\n<code class=\"lang-python\">        logits = self.linear_1(out.view(-1, 1*1*80))\n        ones = torch.ones(num_classes, dtype=logits.dtype)\n        ones = logits\n        logits = logits + self.linear_1_bias\n<\/code>"},{"x":"I\u2019ve tried to run WGAN model and meet some strange behavior. I\u2019m not sure is it my bug, or pytorch bug. I made a minimal example with link \"https:\/\/gist.github.com\/RomanSteinberg\/9ca64be01ff8c8d02a225bd56c41fb5d\" to discuss it here.\nSo, we have an error\n\nRuntimeError: Trying to backward through the graph a second time, but the buffers have already been freed. Specify retain_graph=True when calling backward the first time.\n\nBut if you look into the gist you can find my comments on how to change it to run successfully. For example, I don\u2019t understand how inplace ReLu operations infuence on this error (note http:\/\/i.imgur.com\/6H26H6o.png).\nMain questions: can you explain why it works this way? Why those errors occur?\nDetails: pytorch 0.4.1, ubuntu 16.04, GeForce 1080TI\nNote 1. Module BrokenBlock has no parameters. You can check it using the folowing code:\n<code class=\"lang-python\">b = BrokenBlock(3)\nfor p in b.parameters():\n    print(p.data.size())\n<\/code>\nAnd if it has no parameters then backward should not walk through it. But changing ReLu(inplace=False) in BrokenBlock somehow influences the backward to walk successfully.","y":"Hi,\nYes this comment was more to explain the details of the issue without writing too long stuff on the github PR.\nYou can either use inplace=False or use the version of relu that I gave you above with inplace=True.","z":"Hi,\nThe thing is that during .backward() call, to reduce the peak memory usage, pytorch frees all the memory buffers kept by all the functions in the graph.\nThis means that you cannot call .backward() twice on the same graph otherwise you will get the error you see above. Note that for graphs that do not have any memory buffer, this will work properly as no missing buffers will be encountered during the backward pass.\nI think each of the different scenario you show will actually change which functions keep buffers or not.\nWhat you need to set is gradient_penalty.backward(retain_graph=True) and loss_D_real.backward(retain_graph=True).\n you are right in general and it is obvious. But I think you are wrong in the conclusions.\nConsider the folowing changes:\n<code class=\"lang-python\">        # gradient_penalty.backward()  # comment this line and error disappears\n        # loss_D_real.backward()\n        # loss_D_fake.backward()\n        loss_D = loss_D_fake - loss_D_real + gradient_penalty\n        loss_D.backward()\n<\/code>\nThe error still occurs. It means that some vertices (nodes) of the graph were searched twice, but I don\u2019t call .backward() twice as you said. So, I think that graph is backwarded twice by single call. It is obviously caused by autograd.grad routine but why error dessapears when Relu set not inplace?\nOn more thing. I haven\u2019t found full description about retain_graph option and buffers it frees. What those buffers contain? Why I can or cann\u2019t free them? Can you llink something for this topic to me?\nFor the buffer part. Every op will store anything that it will need for backward. For example the op a * b will need to store both a and b to be able to backward. These buffers are freed just after the backward call. To reduce memory usage by not keeping them around when they\u2019re not needed. If you plan on calling this backward function twice, then you should tell the graph not to free them by using retain_graph so that the second call to backward still has the buffers.\nIn your case I agree that this should not happend with a single call to backward().\nYour code is quite big so it may take a bit of time for us to check it. If you have a smaller version it would be welcome \nI reduced it as much as I could, but I\u2019ll try to do more. I think visualization of graph and buffers for its vertices can help to understand what is going on. Can I visualize it using tensorboard? Any link will be very helpfull.\n\nIn your case I agree that this should not happend with a single call to  backward() .\n\nNotice one more thing. Let\u2019s conssider the folowwing changes:\n<code class=\"lang-python\">        # gradient_penalty.backward()  # comment this line and error disappears\n        loss_D_real.backward()\n        loss_D_fake.backward()\n<\/code>\nThe error dissapears, but we have run backward twice! Tensors loss_D_real and loss_D_fake are obtained by the same model netD. So, to understand the problem we need to verify what is going on during backward pass. I need more documentation.\nHere is a more minimal example. No more cuda\/data parallel\/nn.Module and such \nStill looking into why it fails\n<code class=\"lang-auto\">import torch\nfrom torch import nn, cuda\nfrom torch.autograd import Variable, grad\nfrom torch.nn import functional as F\n\n# Debug stuff\nimport torchviz\ntorch.autograd.set_detect_anomaly(True)\n\ninputs = torch.ones((1, 3, 256, 256), requires_grad=True)\n\ntmp1 = F.instance_norm(inputs)\ntmp2 = F.threshold(tmp1, 0., 0., True)\nprob_interpolated = torch.sigmoid(tmp2)\n\ngradients = grad(outputs=prob_interpolated, inputs=inputs,\n                 grad_outputs=torch.ones(prob_interpolated.size()),\n                 create_graph=True, retain_graph=True)[0]\n\ngradient_penalty = gradients.sum()\n\n# Debug graph\ntorchviz.make_dot(gradient_penalty).view()\ngradient_penalty.backward()\n\n<\/code>\nOk !\nAfter 2h of bughunting, I found the problem and it\u2019s a bug on our side.\nFrom your side, you will have to use a workaround for now.\nFor your current code, replace the ReLU you\u2019re using with:\n<code class=\"lang-auto\">class ReLU(nn.Module):\n    def __init__(self, inplace=False):\n        super(ReLU, self).__init__()\n        self.inplace = inplace\n\n    def forward(self, input):\n        if self.inplace:\n            return torch.relu_(input)\n        else:\n            return torch.relu(input)\n<\/code>\nAnd it should all be fine \nIf you see the same problem again you can post here and we\u2019ll find a way around it.\nI\u2019ll open an issue on github for the bug and edit here when it\u2019s done.\nThanks a lot for the bug report and the repro code.\nI\u2019ll add more informations here for future reference.\nThe corresponding PR to fix it is here with link \"https:\/\/github.com\/pytorch\/pytorch\/pull\/15219\".\nSmallest repro code:\n<code class=\"lang-python\">import torch\nfrom torch import nn, cuda\nfrom torch.autograd import Variable, grad\nfrom torch.nn import functional as F\n\n# Debug stuff\nimport torchviz\ntorch.autograd.set_detect_anomaly(True)\n\ninputs = torch.ones((1, 3, 256, 256), requires_grad=True)\n\ntmp1 = (inputs+1).view_as(inputs)\ntmp2 = F.threshold(tmp1, 0., 0., True)\nprob_interpolated = torch.sigmoid(tmp2)\n\ngradients = grad(outputs=prob_interpolated, inputs=inputs,\n                 grad_outputs=torch.ones(prob_interpolated.size()),\n                 create_graph=True, retain_graph=True)[0]\n\ngradient_penalty = gradients.sum()\n\n# Debug graph\ntorchviz.make_dot(gradient_penalty).view()\ngradient_penalty.backward()\n<\/code>\nThe computational graph generated is:\nimage.png428\u00d71258 69.2 KB with link \"https:\/\/discuss.pytorch.org\/uploads\/default\/original\/2X\/b\/bde9bc1e7d94e038fd340b68931fc19a6feb5af2.png\"\nThe interesting part is the branch on the right that links ThresholdBackwardBackward directly to ThresholdBackward while ThresholdBackward is already wrapped inside the first CopySlices.\nThe thing is that part of the threshold_ function code is:\n<code class=\"lang-cpp\">  baseType->threshold_(self_, threshold, value);\n  increment_version(self);\n  rebase_history(flatten_tensor_args( self ), grad_fn);\n  if (tracer_state) {\n    jit::tracer::setTracingState(std::move(tracer_state));\n    jit::tracer::addOutput(node, self);\n  }\n  if (grad_fn) {\n    grad_fn->result_ = SavedVariable(self, true);\n  }\n<\/code>\nAs you can see, self is considered as an output of grad_fn when saved. And so when ThresholdBackward is called to generate ThresholdBackwardBackward, self is associated to ThresholdBackward and thus the graph above.\nThe thing is that after the rebase_history, self is not an output of grad_fn anymore, it\u2019s an output of the rewritten graph.\nChanging the save to\n<code class=\"lang-auto\">grad_fn->result_ = SavedVariable(self, !as_variable_ref(self).is_view());\n<\/code>\nMake sure that in the case where self's history is rewritten, we don\u2019t consider it as an output of grad_fn anymore.\nAfter the fix in the PR, the new graph is as expected:\nimage.png332\u00d71248 60.7 KB with link \"https:\/\/discuss.pytorch.org\/uploads\/default\/original\/2X\/6\/6fcd86058c3488ee58acb8faae812a673e2a49f1.png\"\nHi  !\nActually I\u2019m not familar Torch C++ code. So, I understand std::move  but not rebase_history, jit::tracer, SavedVariable, setTracingState. Also, It is not clear why there should be no ThresholdBackward before ThresholdBackwardBackward, because I don\u2019t know how backward pass should be accomplished. That\u2019s why I asked links for detailed description of backward pass.\nI think, your comments will be very usefull for people who are deep in Torch code, but not for regular users.\nI trust you fixed it . And also I understand that I can use inplace=False to get the correct code with slight additional memory usage. Thank you!\nHi,\nYes this comment was more to explain the details of the issue without writing too long stuff on the github PR.\nYou can either use inplace=False or use the version of relu that I gave you above with inplace=True."},{"x":"Hi guys, I am in what I believe to be a tricky situation. I am using an VAE based net to generate the mu and sigma parameters (both 2D, that is (x, y)), and from this values I am creating a distribution:\n<code class=\"lang-python\">m = MultivariateNormal(mu, torch.diag(sigma))\n<\/code>\nSo to get the probability of any tuple value (x,y) I do this\n<code class=\"lang-python\">prob = m.log_prob((x,y)).exp()\n<\/code>\nSuppose I have a batch B (N x Channels x W x H) of images, I pass through the VAE and get the parameters mu and sigma. Then I want to create N tensors of size W x H of prob values. For example, suppose for image i of size W x H in the batch, I create a tensor T of same size (W x H), but in position (i,j) of T (0 <= j < W, 0 <= i < H) I have prob = m.log_prob((i,j)).exp(). And this goes for each pair of position for each image in the batch. Then I multiply T and each channel of image i. And T\u2019 for image i\u2019 and so on, that is, a different T for a different image in the batch.\nThe resulting batch (B x Tensor of T\u2019s) I pass to other nets to do other stuffs. How do I do this in order to backprop the values from the following nets to the VAE?\nJust a reminder, as I am working with batches, m.sample() returns a sample of shape (Batch size, 2), and therefore m.log_prob() receives a tensor in the same shape.","y":"I solved the problem with the code bellow\n<code class=\"lang-python\">def generate_grid(h, w):\n    x = torch.arange(0, h)\n    y = torch.arange(0, w)\n\n    grid = torch.stack([y.expand(h,-1).t().flatten(), x.repeat(w)]).t()\n    return grid.float()\n\nevaluate_mn_dist = lambda mu, sigma, tensor: MultivariateNormal(mu, torch.diag(sigma)).log_prob(tensor).exp()\n\nw, h = imgs[0].shape[1:] # imgs is the 4-D batch (B, C, W, H)\ngrid = generate_grid(h, w)\n\n# for each image in the batch, get the probabilities from a gaussian distributions with\n# parameters given from the VAE\nmaps = torch.stack([evaluate_mn_dist(mean, std, grid) for mean, std in zip(mu_final, sigma_final)])\n\n# reshape the attention maps to get shape (Batchs, 1 , W, H)\nmaps = torch.unsqueeze(maps,1).view(-1,1,w,h)\n# multiply the images with the maps\nimgs_ = imgs*maps\n<\/code>","z":"I solved the problem with the code bellow\n<code class=\"lang-python\">def generate_grid(h, w):\n    x = torch.arange(0, h)\n    y = torch.arange(0, w)\n\n    grid = torch.stack([y.expand(h,-1).t().flatten(), x.repeat(w)]).t()\n    return grid.float()\n\nevaluate_mn_dist = lambda mu, sigma, tensor: MultivariateNormal(mu, torch.diag(sigma)).log_prob(tensor).exp()\n\nw, h = imgs[0].shape[1:] # imgs is the 4-D batch (B, C, W, H)\ngrid = generate_grid(h, w)\n\n# for each image in the batch, get the probabilities from a gaussian distributions with\n# parameters given from the VAE\nmaps = torch.stack([evaluate_mn_dist(mean, std, grid) for mean, std in zip(mu_final, sigma_final)])\n\n# reshape the attention maps to get shape (Batchs, 1 , W, H)\nmaps = torch.unsqueeze(maps,1).view(-1,1,w,h)\n# multiply the images with the maps\nimgs_ = imgs*maps\n<\/code>"},{"x":"I\u2019d like to comment or ask about this issue that I realized yesterday. I was migrating a big script into a Class version of it, but by doing this I forgot to add that when training, the input tensor has to require grad (I usually do this by using inputTensor.requires_grad_()).\nDespite this, the model managed to converge to the same error as it would converge with grad enabled on that input.\nMy question is then, why should I use requires_grad = True in the input vectors if by doing so it won\u2019t change anything? Maybe I\u2019m missing something?.\nI checked if requires_grad was true by printing it. Obviously I make loss.backward(), optimizer.step() and optimizer.zero_grad().\nSorry if my question (and my english) is very basic. Thanks!","y":"Your input doesn\u2019t need to require gradients, if you just would like to train your model.\nIt might require gradients, if you would like to use the gradients in the input itself.\nSome use cases could be e.g. to generate adversarial examples or visualize some activations in the input using the gradients.","z":"Your input doesn\u2019t need to require gradients, if you just would like to train your model.\nIt might require gradients, if you would like to use the gradients in the input itself.\nSome use cases could be e.g. to generate adversarial examples or visualize some activations in the input using the gradients."},{"x":"I would like to do some gradient hacking on a linear layer. The most elegant way I found to do this is to write a new gradient function as follows:\n<code class=\"lang-auto\">def custom_grad(s1=1, s2=1):\n    class Custom(Function):\n\n        \n        def forward(ctx, inputs, weights):\n            ctx.save_for_backward(inputs, weights)\n\n            outputs = inputs @ weights.t()\n            return s1 * outputs\n\n        \n        def backward(ctx, grad_output):\n            inputs, weights = ctx.saved_tensors\n            grad_input = grad_output.clone()\n\n            dx = grad_input @ weights\n            dw = grad_input.unsqueeze(-1) @ inputs.unsqueeze(1)\n\n            return s2 * dx, dw.sum(0)\n\n    return Custom.apply\n<\/code>\nThe problem with this approach, however, is that it is so extremely slow (it takes twice as long) due to the two matrix multiplications. I tried using something like dx, dw = torch.autograd.grad(outputs, [inputs, weights], grad_input) to speed things up, but for some reason outputs is a leaf variable and therefore does not allow gradients to flow through.\nIn the end, I only need to rescale some gradients, so it would be nice if I could just use the default autograd functionality, but it seems to be nearly impossible.\nPS: I\u2019m working with pytorch 0.4.0 for now","y":"This seems to be working, thanks! The only issue is when the inputs do not require gradients (i.e. in the input layer). However, this can easily be resolved by using something like:\n<code class=\"lang-auto\">def forward(self, x):\n    if x.requires_grad:\n         x = x.clone()\n         x.register_hook(lambda grad: grad * self.s2 \/ self.s1)\n\n    raw = x @ self.weight.t()\n    return self.s1 * raw\n<\/code>","z":"Hi,\nYou can use hooks with link \"https:\/\/pytorch.org\/docs\/stable\/autograd.html#torch.Tensor.register_hook\" for that.\nRegister a hook on the tensor whose gradient you want to scale, and return the scaled gradient.\nI was hoping I could avoid the hooks, since it blurs the actual idea behind the gradient hacking. Seems like I don\u2019t have much of a choice here then\u2026\nWell the hook are supposed to be the right way to do gradient hacking \n Any ideas on how I can keep the effect of the gradient hacking local in a forward pass of a module?\n<code class=\"lang-auto\">def forward(self, x):\n    raw = x @ self.weight.t()\n    y = self.s1 * raw\n    x.register_hook(lambda grad: grad * self.s2 \/ self.s1)\n    return y\n<\/code>\nHow can I prevent other gradients w.r.t. the tensor passed to x to be affected by this hook? I assume something like\n<code class=\"lang-auto\">h = x.register_hook(lambda grad: grad * self.s2 \/ self.s1)\nh.remove()\n<\/code>\nwould be something like a NO-OP.\nHi,\nYou can add x = x.clone() at the beginning of your forward function.\nThis seems to be working, thanks! The only issue is when the inputs do not require gradients (i.e. in the input layer). However, this can easily be resolved by using something like:\n<code class=\"lang-auto\">def forward(self, x):\n    if x.requires_grad:\n         x = x.clone()\n         x.register_hook(lambda grad: grad * self.s2 \/ self.s1)\n\n    raw = x @ self.weight.t()\n    return self.s1 * raw\n<\/code>"},{"x":"I am trying to backpropagate the loss of my model towards the input, for the purpose of calculating adversarial examples. The idea is to train an adversarial sticker that can be added to an input image to cause an object detection system to fail. However, when trying to backpropagate my loss towards the input of my model by doing torch.autograd.grad(loss,sticker), I get the following RuntimeError: One of the differentiated Tensors appears to not have been used in the graph.\nMy graph looks something like this:\nsticker, images -> apply sticker -> object detection -> loss\nThe loss is defined by how well the object detector succeeds in recognizing the objects.\nI suspect I get this error because somewhere along my calculations, i used an operation that caused the input Sticker to get disconnected from the graph. I\u2019m not sure how to determine where that happens. Is there any way to check for this?","y":"did you use .detach() or .data or .numpy() somewhere in the chain? those are the main suspects for a disconnected graph","z":"did you use .detach() or .data or .numpy() somewhere in the chain? those are the main suspects for a disconnected graph\nThank you for your attention. I used .detach() like you said."},{"x":"Hello, everyone !\nMy demand is a optical-flow-generating problem. I have two raw images and a optical flow data as ground truth, now my algorithm is to generate optical flow using raw images, and the euclidean distance between generating optical flow and ground truth could be defined as a loss value, so it can implement a backpropagation to update parameters.\nI take it as a regression problem, and I have to ideas now:\n\n\nI can set every parameters as (required_grad = true), and compute a loss, then I can loss.backward() to acquire the gradient, but I don\u2019t know how to add these parameters in optimizer to update those.\n\n\nI write my algorithm as a model. If I design a \u201ccustom\u201d model, I can initilize several layers such as nn.Con2d(), nn.Linear() in def init() and I can update parameters in methods like (torch.optim.Adam(model.parameters())), but if I define new layers by myself, how should I add this layer\u2019s parameters in updating parameter collection???\n\n\nThis problem has confused me several days. ,are there any good methods to update user-defined parameters ? I would be very grateful if you could give me some advice !","y":"When you define a new custom layer in a class, you can inherit the properties of nn.Module and define the learnable parameters as nn.Parameter(). Then, these parameters can be updated by the autograd module. I have written a small example for a custom defined layer:\n<code class=\"lang-auto\">import torch\nimport torch.nn as nn\nimport torch.optim as optim\n\nclass CustomLayer(nn.Module):\n    def __init__(self):\n        super(CustomLayer, self).__init__()\n        weight = torch.randn(10, 2)\n        bias = torch.zeros(2)\n        print(weight.requires_grad)\n        self.weight = nn.Parameter(weight)\n        print(self.weight.requires_grad)\n        self.bias = nn.Parameter(bias)\n\n    def forward(self, x):\n        return torch.matmul(x, self.weight) + self.bias\n\nnet = CustomLayer()\noptimizer = optim.Adam(net.parameters(), lr=0.1)\n<\/code>\nSo, since I have defined the self.weight and self.bias as nn.Parameter, then they will be included in net.parameters(). For example, let\u2019s print the first parameter (in this case weights).\n<code class=\"lang-auto\">print(next(net.parameters()))\nParameter containing:\ntensor([[-0.8969,  0.0836],\n        [ 2.7248, -0.2516],\n        [-0.8740,  0.8217],\n        [-0.5867, -0.8351],\n        [-0.3588, -0.0523],\n        [ 0.2368,  1.6558],\n        [ 0.8367,  2.5776],\n        [ 1.5905,  0.1696],\n        [-0.3271,  0.3540],\n        [ 0.5066,  0.2650]], requires_grad=True)\n<\/code>\nSo these are the initial values of self.weight. Now, we define a loss function and call the backward() and update the parameters:\n<code class=\"lang-auto\">x = torch.randn(4, 10)\ny = torch.tensor([[1, 0], [0, 1], [1, 1], [0, 1]], dtype=torch.float)\nh = net(x)\n\nloss = torch.sum(torch.pow(h-y, 2))\nloss.backward()\noptimizer.step()\nprint(next(net.parameters()))\nParameter containing:\ntensor([[-0.7969, -0.0164],\n        [ 2.6248, -0.1516],\n        [-0.7740,  0.7217],\n        [-0.6867, -0.7351],\n        [-0.2588,  0.0477],\n        [ 0.3368,  1.5558],\n        [ 0.9367,  2.4776],\n        [ 1.4905,  0.2696],\n        [-0.4271,  0.2540],\n        [ 0.4066,  0.1650]], requires_grad=True)\n<\/code>\nAs you can see, the parameters of this custom-layer is updated by optim.step().","z":"When you define a new custom layer in a class, you can inherit the properties of nn.Module and define the learnable parameters as nn.Parameter(). Then, these parameters can be updated by the autograd module. I have written a small example for a custom defined layer:\n<code class=\"lang-auto\">import torch\nimport torch.nn as nn\nimport torch.optim as optim\n\nclass CustomLayer(nn.Module):\n    def __init__(self):\n        super(CustomLayer, self).__init__()\n        weight = torch.randn(10, 2)\n        bias = torch.zeros(2)\n        print(weight.requires_grad)\n        self.weight = nn.Parameter(weight)\n        print(self.weight.requires_grad)\n        self.bias = nn.Parameter(bias)\n\n    def forward(self, x):\n        return torch.matmul(x, self.weight) + self.bias\n\nnet = CustomLayer()\noptimizer = optim.Adam(net.parameters(), lr=0.1)\n<\/code>\nSo, since I have defined the self.weight and self.bias as nn.Parameter, then they will be included in net.parameters(). For example, let\u2019s print the first parameter (in this case weights).\n<code class=\"lang-auto\">print(next(net.parameters()))\nParameter containing:\ntensor([[-0.8969,  0.0836],\n        [ 2.7248, -0.2516],\n        [-0.8740,  0.8217],\n        [-0.5867, -0.8351],\n        [-0.3588, -0.0523],\n        [ 0.2368,  1.6558],\n        [ 0.8367,  2.5776],\n        [ 1.5905,  0.1696],\n        [-0.3271,  0.3540],\n        [ 0.5066,  0.2650]], requires_grad=True)\n<\/code>\nSo these are the initial values of self.weight. Now, we define a loss function and call the backward() and update the parameters:\n<code class=\"lang-auto\">x = torch.randn(4, 10)\ny = torch.tensor([[1, 0], [0, 1], [1, 1], [0, 1]], dtype=torch.float)\nh = net(x)\n\nloss = torch.sum(torch.pow(h-y, 2))\nloss.backward()\noptimizer.step()\nprint(next(net.parameters()))\nParameter containing:\ntensor([[-0.7969, -0.0164],\n        [ 2.6248, -0.1516],\n        [-0.7740,  0.7217],\n        [-0.6867, -0.7351],\n        [-0.2588,  0.0477],\n        [ 0.3368,  1.5558],\n        [ 0.9367,  2.4776],\n        [ 1.4905,  0.2696],\n        [-0.4271,  0.2540],\n        [ 0.4066,  0.1650]], requires_grad=True)\n<\/code>\nAs you can see, the parameters of this custom-layer is updated by optim.step().\nThank you ! It worked."},{"x":"Hello,\nI am trying to implement gradient checkpointing in my code to circumvent GPU memory limitations, and I found a Pytorch implementation with link \"https:\/\/pytorch.org\/docs\/stable\/checkpoint.html#torch.utils.checkpoint.checkpoint\" . However I could not find any examples anywhere online. All I see right now is:\n<code class=\"lang-auto\">>>> model = nn.Sequential(...)\n>>> input_var = checkpoint_sequential(model, chunks, input_var)\n<\/code>\nThis is for sequential models - I could not find anything for a non-sequential model, even though it is implemented in pytorch. Also, I am not sure what these input vars and chunks are and how the whole thing gels with the rest of the code. It would be a very useful resource if someone can provide a very simple self contained example of how this can be used to train a model. Thanks!","y":"I used this tutorial with link \"https:\/\/github.com\/prigoyal\/pytorch_memonger\/blob\/master\/tutorial\/Checkpointing_for_PyTorch_models.ipynb\" from  to see how different models should be used for checkpointing.","z":"I used this tutorial with link \"https:\/\/github.com\/prigoyal\/pytorch_memonger\/blob\/master\/tutorial\/Checkpointing_for_PyTorch_models.ipynb\" from  to see how different models should be used for checkpointing."},{"x":"I have an ImageFolder. I see that the  __getitem__(index)  method of ImageFolder can get both the tensor and the class of an index. But what if I want to use ImageFolder to sample the images from only a particular class of my choice.\nHow would this be done?","y":"Internally ImageFolder creates these paths, so the one approach would be to only have one subfolder inside root containing your desired class images.\nEvery other approach would make the usage of ImageFolder not really useful, as you would have to filter out the other unwanted classes.\nA kind of dirty hack would be to use ImageFolder, and increase the index inside __getitem__ until you sample an image from the class you want.","z":"If could provide a WeightedRandomSampler with all weights set to zero for elements of the unwanted classes.\nAlternatively, you could write your custom Dataset and just read all images from a particular folder, which will also result in a single class being sampled.\n\n\n\n ptrblck:\n\nWeightedRandomSampler\n\n\nSo currently I am using custom dataset and the class looks like this:\n<code class=\"lang-auto\">class CustomData(Dataset):\n    \"\"\"\n    CustomData dataset\n    \"\"\"\n\n    def __init__(self, name, dirpath, transform=None, should_invert=False):\n        super(Dataset, self).__init__()\n        self.dirpath = dirpath\n        self.imageFolderDataset = dset.ImageFolder(root=self.dirpath)\n        self.transform = transform\n        self.should_invert = should_invert\n        self.to_tensor = transforms.ToTensor()\n\n    def __getitem__(self, index):\n        # Training images\n        images = self.imageFolderDataset.imgs\n        img = cv2.imread(images[index][0])\n\n        if self.should_invert:\n            img = PIL.ImageOps.invert(img)\n\n        if self.transform is not None:\n            img = self.transform(img)\n\n        img = np.array(img, dtype='uint8')\n        img = cv2.cvtColor(img, cv2.COLOR_BGR2RGB)\n        img_as_tensor = self.to_tensor(img)\n        return img_as_tensor, images[index][1]\n\n    def __len__(self):\n        return len(self.imageFolderDataset.imgs)\n<\/code>\nHow can I incorporate reading from particular folder because ImageFolder gives error if there is no directory inside?\nAn easy solution would be to just copy one folder into the root directory, but that would probably include some data moving, which you might want to avoid.\nI would suggest to pass the image paths of the single class you want and to just sample from them:\n<code class=\"lang-python\">def __init__(...):\n    self.image_paths = image_paths  # Should contain a list of image paths of your desired class: e.g. ['.\/data\/class0\/img0.png', '.\/data\/class0\/img1.png', ...]\n    ...\n\ndef __getitem__(self, index):\n    img = cv2.imread(self.image_paths[index])\n    ...\n    return img_as_tensor, torch.tensor([0])\n<\/code>\nBut would it not be possible without using passing image_paths. I was kind of avoiding this approach.\nIn that case, I would just use a SubsetRandomSampler based on the class indices.\nHere is a small example getting the class indices for class0 from an ImageFolder dataset and creating the SubsetRandomSampler:\n<code class=\"lang-python\">targets = torch.tensor(dataset.targets)\ntarget_idx = (targets==0).nonzero()\n\nsampler = torch.utils.data.sampler.SubsetRandomSampler(target_idx)\n\nloader = DataLoader(\n    dataset,\n    sampler=sampler)\n\nfor data, target in loader:\n    print(target)  # should only print zeros\n<\/code>\nYou could create a member method inside CustomData to return the class indices and pass it to the sampler.\n\nAgain I have to specify dataset.targets somewhere. I just want to do something like passing the directory path and then loading images from there. Is it possible to do this without using a list if paths like ImageFolder does it.\nInternally ImageFolder creates these paths, so the one approach would be to only have one subfolder inside root containing your desired class images.\nEvery other approach would make the usage of ImageFolder not really useful, as you would have to filter out the other unwanted classes.\nA kind of dirty hack would be to use ImageFolder, and increase the index inside __getitem__ until you sample an image from the class you want.\nGiven a list classes of classes, I did the following:\n<code class=\"lang-auto\">def checkfun(args):\n  return args.split(\"\/\")[-2] in classes and args.endswith(\".jpg\")\n\n\ndef ___find_classes(self, dir):\n  return classes, {c: i for i, c in enumerate(classes)}\n\ntorchvision.datasets.ImageFolder._find_classes = ___find_classes\ndataset = torchvision.datasets.ImageFolder(root=\"root_training_dir\", is_valid_file=checkfun, )\n<\/code>\nBasically,\n\nIv\u2019e created filter function to filter out images with the wrong class.\nTorch _find_classes function just list directory, before the filtering, so Iv\u2019e just replaced it with a new function.  Probably the better thing to do here is to build some context manager to switch back to original function after I done.\n"},{"x":"If I code:\n<code class=\"lang-python\">with torch.no_grad():\n    w -= 0.01 * w.grad\n    w.grad.zero_()\n<\/code>\nEverything works well with no error.\nHowever, if I code:\n<code class=\"lang-auto\">with torch.no_grad():\n    w = w - 0.01 * w.grad\n    w.grad.zero_()\n<\/code>\nI meet the following error:\n<code class=\"lang-auto\">Traceback (most recent call last):\n  File \"\/home\/jihao\/deep_learning\/auto_gradient.py\", line 34, in <module>\n    w.grad.zero_()\nAttributeError: 'NoneType' object has no attribute 'zero_'\n<\/code>\nWhat\u2019s the difference between the two snips of code?","y":"w -= 0.01 * w.grad is an in-place operation, so it performs calculation on existing w and updates the value.\nHowever,  w = w - 0.01 * w.grad is not in-place operation, so it creates a new variable w, which does not have requires_grad set and so the error.\nYou can quickly check this by calling - print(w.requires_grad). In the first case you would get True wheres in the second case it would be False.","z":"w -= 0.01 * w.grad is an in-place operation, so it performs calculation on existing w and updates the value.\nHowever,  w = w - 0.01 * w.grad is not in-place operation, so it creates a new variable w, which does not have requires_grad set and so the error.\nYou can quickly check this by calling - print(w.requires_grad). In the first case you would get True wheres in the second case it would be False.\nI understand. Thank you so much!\nCould you please mark the thread as solved for the benefit of other members.\nAlready marked. Thank you very much for solving my first question here.\nAh great. Thanks "},{"x":"Hi,\nI\u2019ve been trying many things without success, unfortunately. Basically, this is what I want:\nX --> PyTorch model --> X* --> TF model\nThen, I can obtain the gradient w.r.t X* through the TF model, but I want to propagate it through the PyTorch model to obtain an X.grad.\nThis is how far I\u2019ve come:\nimage.png1510\u00d71010 152 KB with link \"https:\/\/discuss.pytorch.org\/uploads\/default\/original\/2X\/c\/c5460d5c3ee5b77fe19244c63bc5af574f16fbe3.png\"\nI\u2019ve been using this with link \"https:\/\/discuss.pytorch.org\/t\/how-to-use-torch-autograd-backward-when-variables-are-non-scalar\/4191\" and this with link \"https:\/\/pytorch.org\/tutorials\/advanced\/numpy_extensions_tutorial.html\" to get so far.\nI\u2019m new to writing custom gradient functions, so my code probably doesn\u2019t make much sense.  Hope anyone can help out.\nThanks","y":"Hi, I managed to figure it out myself!  It\u2019s actually much simpler than I thought. For any future readers:\nimage.png1772\u00d7266 45.2 KB with link \"https:\/\/discuss.pytorch.org\/uploads\/default\/original\/2X\/d\/d4dfaa688432108bac96d673ff59717a091aa100.png\"","z":"Hi, I managed to figure it out myself!  It\u2019s actually much simpler than I thought. For any future readers:\nimage.png1772\u00d7266 45.2 KB with link \"https:\/\/discuss.pytorch.org\/uploads\/default\/original\/2X\/d\/d4dfaa688432108bac96d673ff59717a091aa100.png\""},{"x":"Suppose that F(x) is a NN with 3 inputs and 1 output and I try to compute the gradient of F wrt the first input. But I got the error message: \u201cOne of the differentiated Tensors appears to not have been used in the graph. Set allow_unused=True if this is the desired behavior.\u201d What is the problem?\nMy code is as follows:\nimport torch\nimport torch.nn as nn\nfrom torch.autograd import Variable,grad\nNN = nn.Sequential(\ntorch.nn.Linear(3, 64),\ntorch.nn.ReLU(),\ntorch.nn.Linear(64, 64),\ntorch.nn.ReLU(),\ntorch.nn.Linear(64, 64),\ntorch.nn.ReLU(),\ntorch.nn.Linear(64, 1)\n)\nX = Variable(torch.rand(5,3), requires_grad=True)\nF = NN(X)\nG = grad(torch.sum(F),X[:,0],create_graph=True)[0]\nprint(G)","y":"Hi,\nThe problem here is that your function takes a single Tensor as input. The autograd works with Tensors as a unit and so it does not recognize X[:,0] as being a subset of X, but as a different Tensor. So for him, this new Tensor was not used during the backward.\nYou will have to either give each entry separately as a different tensor (and use torch.cat in your forward to recreate a single Tensor) to be able to ask gradients for a single one.\nOtherwise, you could ask the gradients for X and then slice the gradients to get the part you want.","z":"Hi,\nThe problem here is that your function takes a single Tensor as input. The autograd works with Tensors as a unit and so it does not recognize X[:,0] as being a subset of X, but as a different Tensor. So for him, this new Tensor was not used during the backward.\nYou will have to either give each entry separately as a different tensor (and use torch.cat in your forward to recreate a single Tensor) to be able to ask gradients for a single one.\nOtherwise, you could ask the gradients for X and then slice the gradients to get the part you want."},{"x":"Dear community members,\nDescription of the problem:\nI am writing and using a custom cpp extension using Pybind11 and Aten (python3 only). Everything was working well with pytorch<=0.4 (under Linux and MacOs with both gcc and clang). I use my own cmake routine to compile my extension.\nUnfortunately, the last version of pytorch introduced a problem : the extension still compiles fine, but crash when I import the created module (which is in fact a shared object called libKeOpstorch6698ab2e06.cpython-36m-x86_64-linux-gnu.so). Under python 3 it gives:\n<code class=\"lang-auto\">import libKeOpstorch6698ab2e06\n[...]\nImportError: [...]\/pykeops\/build\/libKeOpstorch6698ab2e06.cpython-36m-x86_64-linux-gnu.so: \nundefined symbol: _ZN2at5ErrorC1ENS_14SourceLocationENSt7__cxx1112basic_stringIcSt11char_traitsIcESaIcEEE\n<\/code>\nThe error was reproduced on a debian testing (python3.6) and a Ubuntu16.04 LTS (python3.5).\nComments:\nAfter de-mangling the missing symbol reads :\n<code class=\"lang-auto\">at::Error::Error(at::SourceLocation, std::__cxx11::basic_string<char, std::char_traits<char>, std::allocator<char> >)\n<\/code>\nLooking for a solution I end up reading the he file [...]\/lib\/python3.5\/site-packages\/torch\/lib\/include\/ATen\/Error.h. It has evolved a lot between 0.4 and  0.4.1. In the v0.4.1, this header contains a method at::Error::Error(SourceLocation source_location, std::string err) which is close to the undefined symbol\u2026\nI suspect an unfortunate cast between std::string and std::__cxx11::basic_string (maybe by pybind11) \u2026 But I am currently stuck with this.\nI will appreciate any idea\/comment.\nBest,\nb.\n\n\n\nGitLab with link \"https:\/\/plmlab.math.cnrs.fr\/benjamin.charlier\/libkeops\"\n\n\n\nBenjamin Charlier \/ libkeops with link \"https:\/\/plmlab.math.cnrs.fr\/benjamin.charlier\/libkeops\"\nKErnel OPerationS, on CPUs and GPUs, with autodiff and without memory overflows\n\n\n\n\n\n","y":"Indeed, it works with the built version (branch: master commit: 0d5e4a2c6)\u2026 I\u2019m not sure it is really a good news though. Asking an end-user to rebuild pytorch from source is, for our project, a bit too much.\nI hope next python package will not suffer the same problem\u2026\nAnyway. Thank you for your kind and quick answer","z":"cc  who knows the best about c++ extension\nThe __cxx11 superficially looks related to the C++ ABI switch in gcc 5. You could either try to compile both PyTorch from source and your extension with the same compiler or share which version you used for your extension.\nIf you have a very recent cuda, gcc 6 is a great choice, I have used gcc 5 successfully since PyTorch 0.1.2, but I always self-compiled so I didn\u2019t have consistency issues.\nBest regards\nThomas\nHi tom,\nI have tested on Debian testing (no cuda) with g++5, 6 and 7, as well as with clang++ 6 and on Ubuntu 18.04 (with cuda) with g++5. The same symbol remains undefined.\nI will try asap to rebuild v0.4.1 from source, to test if the symbol is recovered.\nIndeed, it works with the built version (branch: master commit: 0d5e4a2c6)\u2026 I\u2019m not sure it is really a good news though. Asking an end-user to rebuild pytorch from source is, for our project, a bit too much.\nI hope next python package will not suffer the same problem\u2026\nAnyway. Thank you for your kind and quick answer\nAt some point of time, PyTorch was compiled with g++ 4.8 or somesuch (before the C++ abi switch) and g++ 5,6,7 are all after (I don\u2019t know whether you can use a switch for compatibility).\nPersonally, I\u2019d hope that one could move to a newer gcc, but even the\"manylinux 2010\" seems to decree gcc 4 or something like that. \nBest regards\nThomas"},{"x":"I find a mistake about tensors on GPU.\n\nimport torch\ndevice = torch.device(\u201ccuda\u201d)\nx = torch.rand(3, requires_grad=True)\nx = x.to(device)\nm = x.mean()\nm.backward()\nprint(x)\nprint(m)\nprint(x.grad)\n\nThese codes output\n\ntensor([0.6155, 0.2922, 0.6875], device=\u2018cuda:0\u2019, grad_fn=<CopyBackwards>)\ntensor(0.5317, device=\u2018cuda:0\u2019, grad_fn=<MeanBackward1>)\nNone\n\nx.grad=None, I wonder why?","y":"Hi,\nWhen you do x = x.to(device), you change the tensor x.\nOnly the tensor that you created with requires_grad=True will have gradients computed.\nThe x that you check is an intermediary Tensor and so will not have gradients.\nNote that you can do: x = torch.rand(3, device=device, requires_grad=True).","z":"Hi,\nWhen you do x = x.to(device), you change the tensor x.\nOnly the tensor that you created with requires_grad=True will have gradients computed.\nThe x that you check is an intermediary Tensor and so will not have gradients.\nNote that you can do: x = torch.rand(3, device=device, requires_grad=True)."},{"x":"Here is code in pytorch 0.4.1\n<code class=\"lang-auto\">import torch\nfrom torch.autograd import grad\nx = torch.ones(10,2, dtype=torch.float, requires_grad=True)\nw = torch.rand(10,2, dtype=torch.float, requires_grad=True)\ny=x*w\ny0=y[:,0]\nx0=x[:,0]\ngy0=torch.ones(10, dtype=torch.float)\n \n#test1:\n#this gives none\n#g0=grad(y0, x0, gy0, allow_unused=True)\n\n#test2:\n#this works, g0 is not none\ng0=grad(y0, x, gy0)\n<\/code>\nif I remove allow_unused=True, then the error shows:\nRuntimeError: One of the differentiated Tensors appears to not have been used in the graph. Set allow_unused=True if this is the desired behavior\nOK\u2026 it is not bug\u2026\nHere is the graph\nx0<- x -> x*w  ->y ->y0","y":"Yes exactly !\nThere is no link between x0 and y0 in that case.","z":"Yes exactly !\nThere is no link between x0 and y0 in that case."},{"x":"I\u2019ve read through a lot of other posts over the last week trying to solve this and I\u2019m still not making any headway, so here goes. Let me know if there\u2019s any other info I can give you.\nBasically I have the following situation,\n\nBuilt a custom NN layer (below)\nThen included it as part of a neural network (also see below).\nThis can successfully train and test, however, as soon as I try the following,\n\n<code class=\"lang-auto\">            zero_gradients(x)\n            out = model(x)\n\n            y.data = out.data.max(1)[1]\n            _loss = loss(out, y)\n            _loss.backward()\n            normed_grad = step_alpha * torch.sign(x.grad.data)\n<\/code>\nI get the following error for \u201cnormed_grad = step_alpha * torch.sign(x.grad.data)\u201d,\n<code class=\"lang-auto\">AttributeError: 'NoneType' object has no attribute 'data'\n<\/code>\n\u201cgrad\u201d is what is NoneType and I can\u2019t seem to figure out why. I\u2019ve tried this with non-custom neural networks and it works fine. I do model saving\/loading and I\u2019ve tried doing it right after training instead of after having saved and loaded the model.\nCustom Neural Network Layer (partial)\n<code class=\"lang-auto\">class CustomLayer(nn.Module):\n\n    def __init__(self, input_features, output_features, num_vectors=64, bias=True):\n        super(CustomLayer, self).__init__()\n        self.input_features = input_features\n        self.output_features = output_features\n        self.vector_count = num_vectors\n\n        self.weight = nn.Parameter(torch.Tensor(output_features, input_features))\n        if bias:\n            self.bias = nn.Parameter(torch.Tensor(output_features))\n        else:\n            self.register_parameter('bias', None)\n        self.reset_parameters()\n\n    def reset_parameters(self):\n        stdv = 1. \/ math.sqrt(self.weight.size(1))\n        self.weight.data.uniform_(-stdv, stdv)\n        if self.bias is not None:\n            self.bias.data.uniform_(-stdv, stdv)\n\n    def forward(self, x):\n        generated_vectors = []\n        for rx in x:\n            # stuff gets appended to generated_vectors\n\n        x = numpy.array(generated_vectors)\n        x = torch.from_numpy(x).float()\n        x = x.view(-1, len(rx) * self.vector_count)\n        x = Variable(x, requires_grad=True)\n\n        return F.linear(x, self.weight, self.bias)\n\n    def __repr__(self):\n        return self.__class__.__name__ + '(' \\\n            + 'in_features=' + str(self.input_features) \\\n            + ', out_features=' + str(self.output_features) \\\n            + ', bias=' + str(self.bias is not None) + ')'\n<\/code>\nThen I used the custom layer as part of the following neural network.\n<code class=\"lang-auto\">class Net(nn.Module):\n    def __init__(self):\n        super(Net, self).__init__()\n        self.conv1 = nn.Conv2d(1, 10, kernel_size=5)\n        self.fc1b = nn.Linear(640, 50)\n        self.fc2b = nn.Linear(50, 10)\n        self.custom= custom_layer.CustomLayer(640, 640)\n\n    def forward(self, x):\n        x = F.relu(F.max_pool2d(self.conv1(x), 2))\n        x = self.custom(x)\n        x = self.fc1b(x)\n        x = self.fc2b(x)\n\n        return F.log_softmax(x)\n<\/code>","y":"\n\n\n IsaacGS:\n\ngenerated_vectors = []\nfor rx in x:\n# stuff gets appended to generated_vectors\n    x = numpy.array(generated_vectors)\n    x = torch.from_numpy(x).float()\n    x = x.view(-1, len(rx) * self.vector_count)\n    x = Variable(x, requires_grad=True)\n\n\n\nHere you seem to work outside the Variable interface. If generated_vectors is generated using tensors and\/or numpy, then no history will be tracked. So pytorch cannot backward pass it.","z":"Only variables that require grad will receive .grad in a backward. So you will want to make sure that your x has requires_grad=True.\nAlso a better way to calculate y is y = out.max(1)[1].detach().\nYeah, my x has requires_grad=True. Thanks for the tip about \u201cy\u201d\n\n\n\n IsaacGS:\n\ngenerated_vectors = []\nfor rx in x:\n# stuff gets appended to generated_vectors\n    x = numpy.array(generated_vectors)\n    x = torch.from_numpy(x).float()\n    x = x.view(-1, len(rx) * self.vector_count)\n    x = Variable(x, requires_grad=True)\n\n\n\nHere you seem to work outside the Variable interface. If generated_vectors is generated using tensors and\/or numpy, then no history will be tracked. So pytorch cannot backward pass it.\nDarn\u2026 Yeah that\u2019s a problem. Okay thank you!\nHey Simon, so I rewrote my custom layer. But I still seem to be \u201ccorrupting\u201d the gradient somewhere. Is there a particular way you would suggest looking for this mistake? Something like the following code. I\u2019m just trying to track it down now \n<code class=\"lang-auto\">\n    def forward(self, x):\n        # Regular layer\n        x = F.relu(F.max_pool2d(self.conv1(x), 2))\n\n        # Me trying to get a gradient\n        z = torch.add(x, 1)\n        s = torch.mul(z, z)\n        out = s.mean()\n        out.backward()\n        print(x.grad)\n<\/code>\nTo anyone looking in the future. See this with link \"https:\/\/discuss.pytorch.org\/t\/solved-debugging-nans-in-gradients\/10532\/2?u=isaacgs\" answer and this with link \"http:\/\/pytorch.org\/tutorials\/beginner\/former_torchies\/nn_tutorial.html#forward-and-backward-function-hooks\" page.\nno hx\/unhx about it. doesn\u2019t matter. and any is ok"},{"x":"Hi, I\u2019m confused about autograd.Function recently. I think most of the common mathematical operation can be implemented by pytorch functions, and pytorch can automatically build computation graph for us to backward.\nDo we need to use this class only when we need to define our own derivation or integrate some operations in one class?\nIf I don\u2019t need some special derivation operation, can I just define a normal function containing several math functions and use it in my forward procedure?","y":"Ideally, yes. you are right. If you do not need any custom functions for your model, you need not use autograd.Function. You could just use modules from torch.nn packages, APIs from  torch, torch.nn.functional packages. As you said, you can create normal python function with these predefined APIs from torch and torch will take care of calculating the gradient automatically for you.","z":"Ideally, yes. you are right. If you do not need any custom functions for your model, you need not use autograd.Function. You could just use modules from torch.nn packages, APIs from  torch, torch.nn.functional packages. As you said, you can create normal python function with these predefined APIs from torch and torch will take care of calculating the gradient automatically for you.\nThanks a lot\uff01I figured it out now."},{"x":"I\u2019ve been around this problem for the whole day.\ntorch.autograd.backward(loss_seq, grad_seq)  will get an error.\nError log(Pytorch 0.4.1):\n<code class=\"lang-auto\">Traceback (most recent call last):\n  File \"train_vgg.py\", line 272, in <module>\n    torch.autograd.backward(loss_seq, grad_seq)\n  File \"\/root\/anaconda3\/lib\/python3.6\/site- \npackages\/torch\/autograd\/__init__.py\", line 90, in backward\n    allow_unreachable=True)  # allow_unreachable flag\nRuntimeError: invalid gradient at index 0 - expected shape [] but got [1]\n<\/code>\nInput:\n<code class=\"lang-auto\">loss_seq:[tensor(7.3761, device='cuda:1', grad_fn=<ThAddBackward>), tensor(4.3005, device='cuda:1', grad_fn=<ThAddBackward>), tensor(4.2209, device='cuda:1', grad_fn=<ThAddBackward>)]\ngrad_seq:[tensor([1.], device='cuda:1'), tensor([1.], device='cuda:1'), tensor([1.], device='cuda:1')]\n<\/code>\nRelete code:\n<code class=\"lang-auto\">images = Variable(images).cuda(gpu)\n\nlabel_yaw = Variable(labels[:,0]).cuda(gpu)\nlabel_pitch = Variable(labels[:,1]).cuda(gpu)\nlabel_roll = Variable(labels[:,2]).cuda(gpu)\n\npre_yaw, pre_pitch, pre_roll = model(images)\n\n# Cross entropy loss\nloss_yaw = criterion(pre_yaw, label_yaw)\nloss_pitch = criterion(pre_pitch, label_pitch)\nloss_roll = criterion(pre_roll, label_roll)\n\nloss_yaw += 0.005 * loss_reg_yaw\nloss_pitch += 0.005 * loss_reg_pitch\nloss_roll += 0.005 * loss_reg_roll\n\nloss_seq = [loss_yaw, loss_pitch, loss_roll]\ngrad_seq = [torch.ones(1).cuda(gpu) for _ in range(len(loss_seq))]\n\n# crash here\ntorch.autograd.backward(loss_seq, grad_seq)\n<\/code>\nCan someone tell how to fix it?","y":"use\n<code class=\"lang-auto\">grad_seq = [torch.tensor(1.0).cuda(gpu) for _ in range(len(loss_seq))]\n<\/code>\nThe whole idea is to use torch.tensor to create a zero-dimensional tensor containing 1.0.","z":"\n\n\n noone:\n\ngrad_seq = [torch.ones(1).cuda(gpu) for _ in range(len(loss_seq))]\n\n\n<code class=\"lang-auto\">grad_seq = [torch.tensor(1).cuda(gpu) for _ in range(len(loss_seq))]\n<\/code>\nThank you for your reply. I changed to your code, but get a another error:\n<code class=\"lang-auto\">Traceback (most recent call last):\n  File \"train_vgg.py\", line 363, in <module>\n    torch.autograd.backward(loss_seq, grad_seq)\n  File \"\/root\/anaconda3\/lib\/python3.6\/site-packages\/torch\/autograd\/__init__.py\", line 90, in backward\n    allow_unreachable=True)  # allow_unreachable flag\nRuntimeError: invalid gradient at index 0 - expected type torch.cuda.FloatTensor but got torch.cuda.LongTensor\n<\/code>\nSo. I change code like this:grad_seq = [torch.FloatTensor(1).cuda(gpu) for _ in range(len(loss_seq))], but I got a same error:\n<code class=\"lang-auto\">Traceback (most recent call last):\n  File \"train_vgg.py\", line 272, in <module>\n    torch.autograd.backward(loss_seq, grad_seq)\n  File \"\/root\/anaconda3\/lib\/python3.6\/site- \npackages\/torch\/autograd\/__init__.py\", line 90, in backward\n    allow_unreachable=True)  # allow_unreachable flag\nRuntimeError: invalid gradient at index 0 - expected shape [] but got [1]\n<\/code>\nuse\n<code class=\"lang-auto\">grad_seq = [torch.tensor(1.0).cuda(gpu) for _ in range(len(loss_seq))]\n<\/code>\nThe whole idea is to use torch.tensor to create a zero-dimensional tensor containing 1.0.\n Thanks. It\u2019s work now."},{"x":"In my model, the weight of conv is determined by another tensor(let\u2019s call it weight_real) on the fly while running model, the actual parameter to be optimized is weight_real but not the weight itself.\nSo I create a new module that calls F.conv2d in forward. What I expected is the grad propagates first from the output to the weight, then from the weight to weight_real.\nBut I met a trouble that, if I set the weight tensor as a parameter, then the weight will be a leaf node, weight_real can\u2019t get any grad, if I set the weight tensor as a normal tensor, the weight tensor itself can\u2019t get any grad. It seems that only if weight is a parameter but not a normal tensor then its grad will be calculated.\nIs there any way to solve this problem?  Thanks in advance!","y":"Hi,\nThe only gradients you actually care about are for weight_real no?\nIn that case you can just make weight_real a Parameter of your module and create weight in a differentiable manner during the forward.","z":"Hi,\nThe only gradients you actually care about are for weight_real no?\nIn that case you can just make weight_real a Parameter of your module and create weight in a differentiable manner during the forward.\nHi,\nMany thanks. Surprisingly it works. However I\u2019m still confused, how can weight_real get the correct grad when weight has no grad? (I set a breakpoint on optimizer.step() and the grad of weight is None)\nHi,\nthe .grad field is only populated for leaf tensors. That is a tensor created by the user where requires_grad=True is specified.\nIf you want the .grad value for an intermediate result to inspect it, you can call .retain_grad() with link \"https:\/\/pytorch.org\/docs\/stable\/autograd.html#torch.Tensor.retain_grad\" on it during the forward pass.\nThanks for your great help!"},{"x":"I would like to mean a unknown word by 0.\n<code class=\"lang-auto\">m.embedding.weight[1:] = index2vector\nm.embedding.weight[0] = index2vector.mean(dim=0)\nm.embedding.weight = Parameter(m.embedding.weight)\n\nembedding_params = [id(p) for p in m.embedding.parameters()]\nparams = [p for p in m.parameters() if id(p) not in embedding_params and id(p) not in user_bias_params and p.requires_grad]\nembedding_params = [p for p in m.parameters() if id(p) in embedding_params and p.requires_grad]\n\nparams_dict = [{'params': params, 'lr': LR},\n            {'params': embedding_params, 'weight_decay': 1E-6},\n           ]\n<\/code>\nindex2vector is pretrained word vector.\nbut I want to just freeze m.embedding.weight[1:].\nor any advice of method for representing unknown words.","y":"A dirty but working hack would be to zero out the gradient of these parts right before the optimizer step.","z":"A dirty but working hack would be to zero out the gradient of these parts right before the optimizer step.\nthanks a lot.it works."},{"x":"The following code snippet produces the RuntimeError \u201cone of the variables needed for gradient computation has been modified by an inplace operation\u201d\n<code class=\"lang-auto\">for i in range(n):\narr[i] = torch.cross(vec[i], vec[i+1])\narr[i] = arr[i] \/ torch.norm(arr[i])\n<\/code>\nWhereas if I refactor it to the following:\n<code class=\"lang-auto\">for i in range(n):\ntmp = torch.cross(vec[i], vec[i+1])\narr[i] = tmp \/ torch.norm(tmp)\n<\/code>\nIt runs without any issue. My question is, is the former example an inplace operation? Does reassigning an array not create a new variable?","y":"if arr is a torch tensor, then assiging into arr[i] will copy the data inplace in the tensor.\nSo yes this is an inplace operation.","z":"if arr is a torch tensor, then assiging into arr[i] will copy the data inplace in the tensor.\nSo yes this is an inplace operation."},{"x":"Hi everybody,\nI\u2019m currently trying to figure out how to use PyTorch to optimize an angle representing the angular part of an axis\/angle rotation of one vector set into another. That is, I have two sets of vectors of, let\u2019s say, shape (100, 3). One is the input, the other is the target. The target is equal to the input, rotated by the given axis\/angle rotation. The angle for each sample in the sets includes some unknown gaussian noise. Due to that unknown noise a simple matrix operation to solve the equation system is impossible.\nWhat I have so far is an nn.Module that executes the rotation with the current angle on the input. Code looks as follows:\n<code class=\"lang-auto\">class AngleModel(torch.nn.Module):\n    def __init__(self):\n        super(AngleModel, self).__init__()\n        self.angle = nn.Parameter(torch.tensor(0.0, requires_grad = True))\n        self.qw = torch.cos(self.angle \/ 2.)\n        self.qx = torch.zeros(1)\n        self.qy = torch.zeros(1)\n        self.qz = torch.sin(self.angle \/ 2.)\n\n    def forward(self, input):\n        m11 = 1. - 2. * torch.pow(self.qy, 2) - 2. * torch.pow(self.qz, 2).requires_grad_()\n        m22 = 1. - 2. * torch.pow(self.qx, 2) - 2. * torch.pow(self.qz, 2).requires_grad_()\n        m33 = 1. - 2. * torch.pow(self.qx, 2) - 2. * torch.pow(self.qy, 2).requires_grad_()\n\n        m21 = 2. * self.qx * self.qy - 2. * self.qz * self.qw\n        m12 = 2. * self.qx * self.qy + 2. * self.qz * self.qw\n\n        m31 = 2. * self.qx * self.qz + 2 * self.qy * self.qw\n        m13 = 2. * self.qx * self.qz - 2 * self.qy * self.qw\n\n        m32 = 2. * self.qy * self.qz - 2. * self.qx * self.qw\n        m23 = 2. * self.qy * self.qz + 2. * self.qx * self.qw\n\n        matrix = torch.Tensor([\n            [m11, m21, m31],\n            [m12, m22, m32],\n            [m13, m23, m33],\n        ])\n\n        output = torch.matmul(input.float(), matrix.float())\n        return output\n<\/code>\nI initialize the model and optimizer like this:\n<code class=\"lang-auto\">        model = AngleModel()\n        crit = torch.nn.MSELoss()\n        l_rate = 0.01\n        optim = torch.optim.SGD(model.parameters(), lr = l_rate)\n        epochs = 100\n<\/code>\nand execute the training like this:\n<code class=\"lang-auto\">        for epoch in range(epochs):\n            _x = torch.tensor(input, requires_grad = True)\n            _y = torch.tensor(target)\n\n            optim.zero_grad()\n            outputs = model.forward(_x)\n            loss = crit(outputs, _y)\n            loss.backward()\n            optim.step()\n\n            print(\"loss %05.3f; %s\" % (loss.item(), model.angle.item()))\n<\/code>\nI suspect the trigonometric and pow functions to be the culprit of why angle isn\u2019t getting updated. Am I right? angle.grad is None after loss.backward().\nHow would I go on about this? What do I need to do in order to get this working?\nThank you very much!","y":"You need to declare your parameters in the __init__, but put all computation (except for constants) in the forward pass.\nYou shouldn\u2019t have to do requires_grad_ in the forward.\nYou should not use tensor or Tensor within your calculation. This will break the graph.\nIt isn\u2019t entirely clear to me whether you want qx and qy to be learnable in some way, too (otherwise you could just write them as 0 rather than using torch.zeros. You could leave those in the __init__ if they\u2019re constant.\n<code class=\"lang-auto\">class AngleModel(torch.nn.Module):\n    def __init__(self):\n        super(AngleModel, self).__init__()\n        self.angle = nn.Parameter(torch.tensor(0.0)) # parameter will have requires_grad by default, so no need to pass in a tensor requiring grad.\n\n    def forward(self, input):\n        # make qx...qz locals\n        qx = torch.zeros(1)\n        qy = torch.zeros(1)\n        # qw and qz are dependent on the angle, so we need to recompute them here\n        qw = torch.cos(self.angle \/ 2.)\n        qz = torch.sin(self.angle \/ 2.)\n        # there isn't anything wrong with using torch.pow, but personally, I like **\n        m11 = 1. - 2. * qy**2 - 2. * qz**2\n        m22 = 1. - 2. * qx**2 - 2. * qz**2\n        m33 = 1. - 2. * qx**2 - 2. * qy**2\n\n        m21 = 2. * self.qx * self.qy - 2. * self.qz * self.qw\n        m12 = 2. * self.qx * self.qy + 2. * self.qz * self.qw\n\n        m31 = 2. * self.qx * self.qz + 2 * self.qy * self.qw\n        m13 = 2. * self.qx * self.qz - 2 * self.qy * self.qw\n\n        m32 = 2. * self.qy * self.qz - 2. * self.qx * self.qw\n        m23 = 2. * self.qy * self.qz + 2. * self.qx * self.qw\n\n        # noone uses Tensor anymore, and tensor isn't right here either, so you have to cat your way to the matrix or do something differently elsewhere\n        matrix = torch.stack([\n            torch.cat([m11, m21, m31], dim=0),\n            torch.cat([[m12, m22, m32], dim=0),\n            torch.cat([[m13, m23, m33], dim=0),\n        ], dim=0)\n\n        output = torch.matmul(input, matrix)\n        return output\n<\/code>\nor something similar (I didn\u2019t run your code because your example isn\u2019t completely self-contained nor did I check the maths) should work.\nBest regards\nThomas","z":"You need to declare your parameters in the __init__, but put all computation (except for constants) in the forward pass.\nYou shouldn\u2019t have to do requires_grad_ in the forward.\nYou should not use tensor or Tensor within your calculation. This will break the graph.\nIt isn\u2019t entirely clear to me whether you want qx and qy to be learnable in some way, too (otherwise you could just write them as 0 rather than using torch.zeros. You could leave those in the __init__ if they\u2019re constant.\n<code class=\"lang-auto\">class AngleModel(torch.nn.Module):\n    def __init__(self):\n        super(AngleModel, self).__init__()\n        self.angle = nn.Parameter(torch.tensor(0.0)) # parameter will have requires_grad by default, so no need to pass in a tensor requiring grad.\n\n    def forward(self, input):\n        # make qx...qz locals\n        qx = torch.zeros(1)\n        qy = torch.zeros(1)\n        # qw and qz are dependent on the angle, so we need to recompute them here\n        qw = torch.cos(self.angle \/ 2.)\n        qz = torch.sin(self.angle \/ 2.)\n        # there isn't anything wrong with using torch.pow, but personally, I like **\n        m11 = 1. - 2. * qy**2 - 2. * qz**2\n        m22 = 1. - 2. * qx**2 - 2. * qz**2\n        m33 = 1. - 2. * qx**2 - 2. * qy**2\n\n        m21 = 2. * self.qx * self.qy - 2. * self.qz * self.qw\n        m12 = 2. * self.qx * self.qy + 2. * self.qz * self.qw\n\n        m31 = 2. * self.qx * self.qz + 2 * self.qy * self.qw\n        m13 = 2. * self.qx * self.qz - 2 * self.qy * self.qw\n\n        m32 = 2. * self.qy * self.qz - 2. * self.qx * self.qw\n        m23 = 2. * self.qy * self.qz + 2. * self.qx * self.qw\n\n        # noone uses Tensor anymore, and tensor isn't right here either, so you have to cat your way to the matrix or do something differently elsewhere\n        matrix = torch.stack([\n            torch.cat([m11, m21, m31], dim=0),\n            torch.cat([[m12, m22, m32], dim=0),\n            torch.cat([[m13, m23, m33], dim=0),\n        ], dim=0)\n\n        output = torch.matmul(input, matrix)\n        return output\n<\/code>\nor something similar (I didn\u2019t run your code because your example isn\u2019t completely self-contained nor did I check the maths) should work.\nBest regards\nThomas\nWorks like a charm! Thank you very much! That\u2019s what I figured that it had gotten something to do with a broken graph\u2026\nSince I know the axis of the rotation and it is one of the three main axes of the coordinate system, the structure of the corresponding quaternion is already given. That\u2019s why I don\u2019t need to learn qx and qy, hence they can both be 0.\nFor the sake of completeness, the working final code of the nn.Module looks like this now:\n<code class=\"lang-auto\">class AngleModel(nn.Module):\n    def __init__(self):\n        super(AngleModel, self).__init__()\n        self.angle = nn.Parameter(torch.tensor(0.0))\n\n    def forward(self, input):\n        qw = torch.cos(self.angle \/ 2.)\n        qx = 0.0\n        qy = 0.0\n        qz = torch.sin(self.angle \/ 2.)\n\n        matrix = torch.zeros(3, 3)\n\n        matrix[0, 0] = 1. - 2. * qy ** 2 - 2. * qz ** 2\n        matrix[1, 1] = 1. - 2. * qx ** 2 - 2. * qz ** 2\n        matrix[2, 2] = 1. - 2. * qx ** 2 - 2. * qy ** 2\n\n        matrix[0, 1] = 2. * qx * qy - 2. * qz * qw\n        matrix[1, 0] = 2. * qx * qy + 2. * qz * qw\n\n        matrix[0, 2] = 2. * qx * qz + 2 * qy * qw\n        matrix[2, 0] = 2. * qx * qz - 2 * qy * qw\n\n        matrix[1, 2] = 2. * qy * qz - 2. * qx * qw\n        matrix[2, 1] = 2. * qy * qz + 2. * qx * qw\n\n        output = torch.matmul(input, matrix)\n        return output\n<\/code>"},{"x":"I used qfgaohao \/ pytorch-ssd with link \"https:\/\/github.com\/qfgaohao\/pytorch-ssd\" (which has mobilenet-1-ssd, mobilenet-2-ssd-lite networks) to train a custom dataset with 2 classes (one as background and another class for the object type as the dataset is only for a single class). I first modified the dataloader to load my custom dataset. I trained upto 30 epochs and got an mAP of about 85% and the results seemed good for baseline. The output from softmax are good as for the background class its giving high probabilities and the object class its giving low probabilities for most boxes. In this particular network, we get output from 6 different confidence heads.\nEg: while debugging I did F.softmax(confidences[3], dim=2) . The output was :\n\ntensor([[[0.9768, 0.0232],\n[0.9870, 0.0130],\n[0.9799, 0.0201],\n[0.9844, 0.0156],\n[0.9764, 0.0236],\n[0.9813, 0.0187],\n[0.9867, 0.0133],\n[0.9884, 0.0116],\n[0.9872, 0.0128],\n[0.9907, 0.0093],\n[0.9801, 0.0199],\n[0.9821, 0.0179],\n[0.9889, 0.0111],\n[0.9872, 0.0128],\n[0.9912, 0.0088],\n[0.9825, 0.0175],\n[0.9857, 0.0143],\n[0.9703, 0.0297],\n[0.9604, 0.0396],\n[0.9536, 0.0464],\n[0.9770, 0.0230],\n[0.9749, 0.0251],\n[0.9900, 0.0100],\n[0.9703, 0.0297],\n[0.9703, 0.0297],\n[0.9674, 0.0326],\n[0.9892, 0.0108],\n[0.9872, 0.0128],\n[0.9875, 0.0125],\n[0.9742, 0.0258],\n[0.9825, 0.0175],\n[0.9739, 0.0261],\n[0.9940, 0.0060],\n[0.9780, 0.0220],\n[0.9919, 0.0081],\n[0.9521, 0.0479],\n[0.9629, 0.0371],\n[0.9741, 0.0259],\n[0.9720, 0.0280],\n[0.9738, 0.0262],\n[0.9778, 0.0222],\n[0.9709, 0.0291],\n[0.9773, 0.0227],\n[0.9692, 0.0308],\n[0.9769, 0.0231],\n[0.9823, 0.0177],\n[0.9821, 0.0179],\n[0.9762, 0.0238],\n[0.9783, 0.0217],\n[0.9781, 0.0219],\n[0.9793, 0.0207],\n[0.9798, 0.0202],\n[0.9805, 0.0195],\n[0.9638, 0.0362]]])\n\nThe output for object class scores sorted  ```torch.sort(F.softmax(confidences[0], dim=2) [0][:,1])````  is:\n\ntorch.return_types.sort(\nvalues=tensor([6.7742e-04, 7.2989e-04, 7.3594e-04,  \u2026, 8.8332e-01, 9.0915e-01,\n9.1045e-01]),\nindices=tensor([ 121,  103,  367,  \u2026, 1083, 1080,  969]))\n\nIn the output of the first layer it detects the object class in most test images and the distance between the scores is very high as well. Here it got the output with .91, .90 where the object is and scores in rest of the bounding boxes being very low (in range of e-03 or e-04) where the object is not there.\nThis output is reasonable as its able to detect background in most anchor locations. Similar was the case in other outputs of confidence heads. It was getting high score for the a few anchor boxes for the object class and the output made sense.\nTo train and experiment more, I\u2019ve integrated the repository into my custom training pipeline by modularizing the code in such a way that the class has functions to build_net, train and eval the model.I have not made any change to the files for building the model, preprocessing or losses , except for a few functional changes. I\u2019m able to properly load the models trained in original implementation into my pipeline without problem. But now that I have trained mobilenet2-ssd-lite in my integrated code for 30 epochs, the softmax outputs from my integrated pipeline is giving out values that are high for all the anchor boxes in object class. I again did F.softmax(confidences[3], dim=2), this is the output:\n\ntensor([[[0.6760, 0.3240],\n[0.6753, 0.3247],\n[0.6753, 0.3247],\n[0.6748, 0.3252],\n[0.6757, 0.3243],\n[0.6758, 0.3242],\n[0.6747, 0.3253],\n[0.6755, 0.3245],\n[0.6742, 0.3258],\n[0.6738, 0.3262],\n[0.6728, 0.3272],\n[0.6743, 0.3257],\n[0.6751, 0.3249],\n[0.6749, 0.3251],\n[0.6743, 0.3257],\n[0.6743, 0.3257],\n[0.6742, 0.3258],\n[0.6739, 0.3261],\n[0.6752, 0.3248],\n[0.6737, 0.3263],\n[0.6746, 0.3254],\n[0.6749, 0.3251],\n[0.6749, 0.3251],\n[0.6750, 0.3250],\n[0.6765, 0.3235],\n[0.6748, 0.3252],\n[0.6748, 0.3252],\n[0.6724, 0.3276],\n[0.6756, 0.3244],\n[0.6727, 0.3273],\n[0.6747, 0.3253],\n[0.6749, 0.3251],\n[0.6747, 0.3253],\n[0.6739, 0.3261],\n[0.6741, 0.3259],\n[0.6739, 0.3261],\n[0.6767, 0.3233],\n[0.6770, 0.3230],\n[0.6768, 0.3232],\n[0.6769, 0.3231],\n[0.6775, 0.3225],\n[0.6766, 0.3234],\n[0.6770, 0.3230],\n[0.6755, 0.3245],\n[0.6751, 0.3249],\n[0.6752, 0.3248],\n[0.6752, 0.3248],\n[0.6753, 0.3247],\n[0.6756, 0.3244],\n[0.6764, 0.3236],\n[0.6754, 0.3246],\n[0.6755, 0.3245],\n[0.6751, 0.3249],\n[0.6762, 0.3238]]])\n\nThe output for torch.sort(F.softmax(confidences[0], dim=2) [0][:,1]) is\n\ntorch.return_types.sort(\nvalues=tensor([0.1053, 0.1532, 0.1655,  \u2026, 0.6778,0.7770, 0.8186]),\nindices=tensor([1084,  970,  968,  \u2026,  969, 1080, 1083]))\n\nThus it means its able to learn where the object is with high confidence but not able to properly tell where its not in the layers ahead. In confidence[0], compared to the model trained in original implementation above, the scores of the other anchors in the one trained in my integration are not that distant. In confidence[1] - confidence[5], the confidence outputs among the object class are very close in each layers separately but in higher range. It should ideally be able to output the confidence for the anchors to be in range of 0.0xxx as its doing from original implementation.  Eg: in confidence[1] all the values for class score might be 0.29xx, in confidence[3] as given above, all the values are in the range 0.32xx  and it is a pattern I was seeing. Sometime it can go in the range of 0.4xxx as well for all the anchor boxes. They only differ in 3rd and 4th decimal place.\nI\u2019m getting mAP of 71% on the integrated version as well. The consequence of this is when I run inference on the test images nms ends up outputting a lot of bounding boxes all over the image. If I increase the probability threshold to eg 0.5, all the bounding boxes in close range go away if they are below 0.5 and do end up having the object class detected. But it does miss out cases it was able to detect with the model trained in the original implementation.\nThe loss output from the original seem to lower in  original code. But when I\u2019m training from my integration the losses go down as well but not as much. I used cosine lr schelduler in both the runs. I trained mobilenet-1-ssd as well from my implementation but the results are the same.\nMy implementation losses:\n1817\u00d7743 with link \"https:\/\/user-images.githubusercontent.com\/31937185\/83347901-ddf34500-a345-11ea-996e-a65b2ae3c882.png\"\nTo me it looks like the the layers for confidence headers from confidence[1] - confidence[5] are not learning and even confidence[0] is not able to learn that well in my pipeline.I\u2019m not able to clearly tell where I\u2019m going wrong. Any suggestions will be welcome to debug such a problem. A pytorch forum thread with link \"https:\/\/discuss.pytorch.org\/t\/same-softmax-values-no-matter-what-is-the-input-problem\/8480\" is the closest I\u2019ve found to the current problem.","y":"Weight decay was 0.1. It should have been 0.0005. My bad.","z":"\n\n\n Chaitanya_C:\n\nTo train and experiment more, I\u2019ve integrated the repository into my custom training pipeline by modularizing the code in such a way that the class has functions to build_net, train and eval the model. I have not made any change to the files for building the model, preprocessing or losses , except for a few functional changes .\n\n\nI don\u2019t know which changes were made, but you could try to isolate each change and check, if the change is applying the expected change.\nCould you explain from a high-level perspective, what your changes were supposed to do?\nAlso, I assume you are using the softmax for debugging?\n\n\nI did step through the code in debugger and it looks like its able to follow the similar structure to the original code loading the model and data while training. I\u2019m able to load the models trained in original implementation into my pipeline and run some eval operations on it. Also able to load the pretrained weights provided for mobilenet1 and mobilelet2 were loading for training. But I can try doing it once again by isolating and checking them as you have suggested.\n\n\nThe main vision folder with link \"https:\/\/github.com\/qfgaohao\/pytorch-ssd\/tree\/master\/vision\" which contains various modules for model, data and losses is almost same in my integration. The only changes that I made were :\n\n\n\n\nChanged the data loading part to load images and labels from my custom dataset file. The prepossessing applied on the images and labels loaded from my dataset is the same as the original implementation.\n\n\nMoved some of the tensors to \u2018cpu\u2019 device  as it was causing problem in dataloader it tried to do multiprocessing in cuda device when num_workers>0. After moving the tesnor to cpu it prevented the error as cpu processes were spawned for dataloading and preprocessing. Below is the diff from this file with link \"https:\/\/github.com\/qfgaohao\/pytorch-ssd\/blob\/master\/vision\/ssd\/ssd.py\"\n\n\n<code class=\"lang-auto\">@@ -141,7 +143,7 @@\n class MatchPrior(object):\n     def __init__(self, center_form_priors, center_variance, size_variance, iou_threshold):\n         self.center_form_priors = center_form_priors\n-        self.corner_form_priors = box_utils.center_form_to_corner_form(center_form_priors)\n+        self.corner_form_priors = box_utils.center_form_to_corner_form(center_form_priors).to('cpu')\n         self.center_variance = center_variance\n         self.size_variance = size_variance\n         self.iou_threshold = iou_threshold\n@@ -149,11 +151,13 @@\n     def __call__(self, gt_boxes, gt_labels):\n         if type(gt_boxes) is np.ndarray:\n             gt_boxes = torch.from_numpy(gt_boxes)\n+            gt_boxes = gt_boxes.to('cpu')\n         if type(gt_labels) is np.ndarray:\n             gt_labels = torch.from_numpy(gt_labels)\n+            gt_labels = gt_labels.to('cpu')\n         boxes, labels = box_utils.assign_priors(gt_boxes, gt_labels,\n                                                 self.corner_form_priors, self.iou_threshold)\n-        boxes = box_utils.corner_form_to_center_form(boxes)\n+        boxes = box_utils.corner_form_to_center_form(boxes).to('cpu')\n         locations = box_utils.convert_boxes_to_locations(boxes, self.center_form_priors, self.center_variance, self.size_variance)\n         return locations, labels\n<\/code>\n\nAnother change for this was made to this file: \/vision\/ssd\/config\/mobilenetv1_ssd_config.py\n\n<code class=\"lang-auto\">@@ -20,4 +20,4 @@\n ]\n \n \n-priors = generate_ssd_priors(specs, image_size)\n\\ No newline at end of file\n+priors = generate_ssd_priors(specs, image_size).to('cpu')\n\\ No newline at end of file\n<\/code>\n\nYes, the softmax function was used in debugging. The softmax is applied only when the model is run in test mode to get the probability outputs. I applied softmax on the individual feature maps which were output from confidence headers of various layers, which is when I discovered this discrepancy between the outputs of the model trained in the original implementation and the one trained in my pipeline.\n\nWeight decay was 0.1. It should have been 0.0005. My bad."},{"x":"I\u2019d like to implement distributed model parallel at the module level such as nn.Linear. The following snippet divides the module and distributes it to multiple GPUs, but parallel computation is not done.\n<code class=\"lang-python\">class DistLinear(nn.Module):\n\n    def __init__(self, input_size, output_size, bias=True, io_gpu=0, num_gpus=2):\n        super().__init__()\n        self.input_size = input_size\n        self.output_size = output_size\n        self.bias = bias\n        self.io_gpu = io_gpu\n        self.num_gpus = num_gpus\n\n        self.setup_modules()\n\n    def setup_modules(self):\n        self.bucket_size = math.ceil(self.output_size \/ self.num_gpus)\n        last_bucket_size = self.output_size - self.bucket_size*(self.num_gpus-1)\n        self.modular = nn.ModuleList([self.get_module(self.bucket_size)\n                                        for _ in range(self.num_gpus-1)])\n        self.modular.append(self.get_module(last_bucket_size))\n        for i in range(self.num_gpus):\n            self.modular[i].cuda(i)\n\n    def get_module(self, output_size):\n        return nn.Linear(self.input_size, output_size, bias=self.bias)\n\n    def forward(self, x): # x: batch x hidden\n        xs = []\n        for i in range(self.num_gpus):\n            _x = x.cuda(i)\n            _x = self.modular[i](_x)\n            _x = _x.cuda(self.io_gpu)\n            xs.append(_x)\n        return torch.cat(xs, dim=-1)\n<\/code>","y":"Hi , for distributed model parallel across different machines, it requires inter-node communication, right now we don\u2019t have good support for it like DistributedDataParallel wrapper.\nFor the single node multiple GPU model parallel( like DataParallel wrapper), yes parallel_apply should work in your case.","z":"Are you looking for something like DataParallel?\nhttps:\/\/pytorch.org\/tutorials\/beginner\/blitz\/data_parallel_tutorial.html\nNo, I want to do model parallel. In the above snippet I used a for loop, so I think that it is not parallel processing.\nI noticed that nn.parallel.parallel_apply(modulelist, inputs) is suitable, is this correct?\nWhen actually comparing the speed of for loop and parallel_apply, I feel that parallel_apply is a little faster.\nHi , for distributed model parallel across different machines, it requires inter-node communication, right now we don\u2019t have good support for it like DistributedDataParallel wrapper.\nFor the single node multiple GPU model parallel( like DataParallel wrapper), yes parallel_apply should work in your case.\nThanks  , my problem seems to be solved."},{"x":"I try to implement the disconnection of weights, i.e., the specific connection is always 0. It sounds like masked_scatter_, but I found it could not be autograded.\nHere is my code:\n<code class=\"lang-auto\">import torch\nimport numpy as np\n\nx = torch.rand((3, 1))\n# tensor([[ 0.8525],\n#         [ 0.1509],\n#         [ 0.9724]])\n\nweights = torch.rand((2, 3), requires_grad=True)\n# tensor([[ 0.3240,  0.0792,  0.6858],\n#         [ 0.5248,  0.4565,  0.3625]])\n\nmask = torch.Tensor([[0,1,0],[1,0,1]])\n# tensor([[ 0.,  1.,  0.],\n#         [ 1.,  0.,  1.]])\n\nmask_weights = weights * mask\n# tensor([[ 0.0000,  0.0792,  0.0000],\n#         [ 0.5248,  0.0000,  0.3625]])\n\ny = torch.mm(mask_weights, x)\n# tensor([[ 0.0120],\n#         [ 0.7999]])\n\nweights.grad_fn\n# None\n\nmask_weights.grad_fn\n# MulBackward1 object\n<\/code>\nThe above code implements this graph. \nI want some function which is similar to masked_scatter_ and can apply auto-gradient. Thank you in advance.","y":"While I am not clear about the picture you have posted - (why is the shape of x (3,2) if there are just three input nodes). One clear issue with your code though is that none of the variables have a requires_grad=True. For autograd to track stuff at least one of the inputs should have requires_grad=True.\nAs far as the implementation of the diagram is concerned I would do something like this (assuming each node yields a scalar in the first layer):\n<code class=\"lang-auto\"># Layer 1\nx = torch.randn((3,1), requires_grad=True)\n# tensor([ 1.4079,  1.5536,  0.2737])\nmask = torch.Tensor([[0,1,0],[1,0,1]])\n# tensor([[ 0,  1,  0],\n#         [ 1,  0,  1]])\ny = mask * x.expand_as(mask)\n# tensor([[ 0.0000,  1.5536,  0.0000],\n#      [ 1.4079,  0.0000,  0.2737]])\ny.grad_fn\n# <MulBackward1 at 0x7f4b7a6725c0>\n<\/code>\nIf I am correct about what you are trying to do you don\u2019t need masked_scatter but simple boolean masking.\nIn any case, masked_scatter_ seems to be working for me and calculating the gradients correctly.","z":"The question isn\u2019t very clear. You to seem to be using y but it hasn\u2019t been defined in the code above?\nThank you for your kind reply. I have edited my question.\nWhile I am not clear about the picture you have posted - (why is the shape of x (3,2) if there are just three input nodes). One clear issue with your code though is that none of the variables have a requires_grad=True. For autograd to track stuff at least one of the inputs should have requires_grad=True.\nAs far as the implementation of the diagram is concerned I would do something like this (assuming each node yields a scalar in the first layer):\n<code class=\"lang-auto\"># Layer 1\nx = torch.randn((3,1), requires_grad=True)\n# tensor([ 1.4079,  1.5536,  0.2737])\nmask = torch.Tensor([[0,1,0],[1,0,1]])\n# tensor([[ 0,  1,  0],\n#         [ 1,  0,  1]])\ny = mask * x.expand_as(mask)\n# tensor([[ 0.0000,  1.5536,  0.0000],\n#      [ 1.4079,  0.0000,  0.2737]])\ny.grad_fn\n# <MulBackward1 at 0x7f4b7a6725c0>\n<\/code>\nIf I am correct about what you are trying to do you don\u2019t need masked_scatter but simple boolean masking.\nIn any case, masked_scatter_ seems to be working for me and calculating the gradients correctly.\nThank you so much for your kind help. I edited my post again. Sincere apologize for the confusion .\nI am wondering under this implementation, will mask_weights keep tasked?\nThank you so much.\nCan you clarify what do you mean by \u201ckeep tasked\u201d?\nweights.grad_fn  is none because the gradients would be computed wrt weights. At the end when you finally get a scalar and call .backward the right gradients will be calculated in weights.grad. You can use an optimizer or manually update the weights from there.\nI assume this is solved now?\nYes, sure, just one more little question. For the \u201ckeep tasked\u201d, I actually refer to \u201cdisconnected\u201d. So if I use auto back-propogation, will the disconnection still be disconnected please?\nSir, I just did an experiment, the gradients of mask_weights are no longer masked.\nCare to share the code for your experiment\nYes, sure, thank you.\nContinue my codes on the original post.\n<code class=\"lang-auto\">mask_weights.register_hook(print)\n\nz = torch.Tensor([[1], [1]])\n# tensor([[ 1.],\n#         [ 1.]])\n\nout = (y-z).mean()\n# tensor(-0.6595)\n\nout.backward()\n# tensor([[ 0.1920,  0.1757,  0.0046],\n#         [ 0.1920,  0.1757,  0.0046]])\n\nweights.grad\n# tensor([[ 0.0000,  0.1757,  0.0000],\n#         [ 0.1920,  0.0000,  0.0046]])\n<\/code>\nAs you can see, the value of gradients of mask_weights are not masked."},{"x":"Repost of this Stackoverflow question with link \"https:\/\/stackoverflow.com\/questions\/50826045\/pytorch-0-4-0-broadcasting-doesnt-work-in-optimizer\"\nI can\u2019t seem to get broadcasting to work with autograd in pytorch 0.4.0! Below is a minimal code example that reproduces my problem. I would like to find a single value \u201cbias\u201d, which minimizes the loss over the dataset. The understand the error message as it wants to backpropagate a vector with 5 entries into a scalar, which it cannot figure out. However, this is the whole idea of broadcasting. The behavior I expected was that it would propagate the mean of the error back to the broadcasted scalar value (here bias).\nPlease advice.\nCode:\n<code class=\"lang-auto\">import numpy as np\nimport torch\nfrom torch import nn\nimport torch.nn.functional as F\nfrom torch.utils.data import Dataset\n\nprint(torch.__version__)\n\nclass AddBias(torch.autograd.Function):\n    \n    def forward(ctx, input, bias):\n        ctx.save_for_backward(input, bias)\n        return input - bias\n    \n    def backward(ctx, grad_out):\n        input, bias = ctx.saved_tensors\n        grad_in = grad_bias = None\n        len_grad = len(ctx.needs_input_grad)\n        assert len_grad in {0, 1, 2}\n        if ctx.needs_input_grad[0]: grad_in = grad_out\n        if len_grad == 2: grad_bias = -1 * grad_out \n        return grad_in, grad_bias\n\nclass BiasModel(nn.Module):\n    def __init__(self, size):\n        super(BiasModel, self).__init__()\n        self.bias_model = AddBias.apply\n        self.bias = nn.Parameter(torch.tensor(0.5, dtype=torch.float, requires_grad=True))\n    def forward(self, arr): return self.bias_model(arr[:], self.bias).unsqueeze(-1)\n\nclass MyData(Dataset):\n    def __init__(self, data): self.data = data\n    def __len__(self): return len(self.data)\n    def __getitem__(self, i): \n        arr = torch.tensor(data[i], dtype=torch.float)\n        target = torch.tensor(arr > 0.5, dtype=torch.float).unsqueeze(-1)\n        return arr, target\n\nm = 5\ndata = np.random.random((100, m))\nmodel = BiasModel(m)\nmy_data = MyData(data)\n\nloss_func = F.binary_cross_entropy_with_logits\nwith torch.no_grad():\n    loss = 0.\n    for arr, target in my_data: loss += loss_func(model(arr), target)\n    print('loss before', loss \/ len(my_data))\n\noptimizer = torch.optim.SGD(model.parameters(), lr=0.1)\n\nloss_tot = 0.\nfor arr, target in my_data:\n    model.zero_grad()\n    loss = loss_func(model(arr), target)\n    loss_tot += loss\n    loss.backward()\n    optimizer.step()\n<\/code>\nOutput:\n<code class=\"lang-auto\">0.4.0\nloss before tensor(0.5735)\n---------------------------------------------------------------------------\nRuntimeError                              Traceback (most recent call last)\n<ipython-input-4-27bce65b553b> in <module>()\n     56     loss_tot += loss\n     57     loss.backward()\n---> 58     optimizer.step()\n\n~\/miniconda3\/envs\/myproject\/lib\/python3.6\/site-packages\/torch\/optim\/sgd.py in step(self, closure)\n    105                         d_p = buf\n    106 \n--> 107                 p.data.add_(-group['lr'], d_p)\n    108 \n    109         return loss\n<\/code>\nRuntimeError: expand(torch.FloatTensor{[5]}, size=[]): the number of sizes provided (0) must be greater or equal to the number of dimensions in the tensor (1)","y":"This line in your forward input - bias broadcasts, but in backward you didn\u2019t revert the broadcasting.\nI think you\u2019d be better off just use the provided subtraction rather than writing a new autograd.Function\u2026 It will be faster and won\u2019t be error-prune.","z":"This line in your forward input - bias broadcasts, but in backward you didn\u2019t revert the broadcasting.\nI think you\u2019d be better off just use the provided subtraction rather than writing a new autograd.Function\u2026 It will be faster and won\u2019t be error-prune.\nThanks, Siman!\nThat makes a lot of sense now.\nResolved it by changing a line in the backward pass in AdBias to\nif len_grad == 2: grad_bias = -1 * torch.mean(grad_out)\nThe example above was only meant to illustrate my issue with broadcasting doing the learning phase. Thanks for the suggestion though."},{"x":"Hello all, if my code is\n<code class=\"lang-auto\">n=10\nlearning_rate=1e-2\ncriterion= nn.BCEWithLogitsLoss()\noptimizer = optim.SGD(net.parameters(), lr=learning_rate, momentum=0.9, weight_decay=1e-6)\nloss= criterion(outputs, targets)\nloss=loss \/ n\nloss.backward()\n<\/code>\nDoes the loss  equivalent with the below by using learning rate \/ n?\n<code class=\"lang-auto\">n=10\nlearning_rate=1e-3  #1e-2 \/n\ncriterion= nn.BCEWithLogitsLoss()\noptimizer = optim.SGD(net.parameters(), lr=learning_rate, momentum=0.9, weight_decay=1e-6)\nloss= criterion(outputs, targets)\nloss.backward()\n<\/code>\nI am using SGD. IF not, what should I change in the first code without using loss=loss\/n? Thanks","y":"If you\u2019re using momentum and\/or weight decay (or another optimizer) it won\u2019t work.\nFor example SGD+weight decay (with learning rate lr and weight_decay wd will do the following update: w = w - lr * (dL\/dw + wd*w). You can see that scaling your loss: dL\/dw -> 1\/n*dL\/dw will not have the same effect as changing the learning rate lr -> 1\/n*lr: the weight decay term will not be scaled the same way.\nFor other optimizer, you will need to check the formula of the update and see if scaling of the gradient and of the learning rate have the same effect on the update or not (it is really unlikely).","z":"IF you\u2019re using sgd, yes.\nThanks for your reply. I have updated more detail in the question. Actually, I used SGD+momentum.  Let  me know if you want to change your answer with the updated question\nIf you\u2019re using momentum and\/or weight decay (or another optimizer) it won\u2019t work.\nFor example SGD+weight decay (with learning rate lr and weight_decay wd will do the following update: w = w - lr * (dL\/dw + wd*w). You can see that scaling your loss: dL\/dw -> 1\/n*dL\/dw will not have the same effect as changing the learning rate lr -> 1\/n*lr: the weight decay term will not be scaled the same way.\nFor other optimizer, you will need to check the formula of the update and see if scaling of the gradient and of the learning rate have the same effect on the update or not (it is really unlikely).\n Please correct me if I am wrong. it seems like this is because of the way PyTorch\u2019s SGD with link \"https:\/\/pytorch.org\/docs\/master\/optim.html#torch.optim.SGD\" is different from other frameworks (e.g. caffe)\nI am porting a network from caffe and am trying to understand why after\/if I increase the lr (after certain epoch), network always becomes unstable (inf weights and nan loss).\n~<s>It seems like PyTorch\u2019s SGD is more sensitive to lr changes because it is applied to velocity instead of gradients. Is there any particular reason for this choice?<\/s>~\nEDIT: I added a SGD that is more like other frameworks with link \"https:\/\/discuss.pytorch.org\/t\/sgd-compatibility-with-other-frameworks\/19960\" and still if I increase lr, the network becomes unstable. Decreasing lr is always fine."},{"x":"import torch\nfrom torch.autograd import Variable\nfrom common.constants import Constants\nimport torch.nn.functional as F\nclass Cluster_Assignment_Hardening_Loss(torch.nn.Module):\ndef __init__(self):\n    super(Cluster_Assignment_Hardening_Loss,self).__init__()\n\ndef forward(self,encode_output, centroids):\n\n    # Calculate q_ij (Eqn-4 of the paper)\n    q_temp = Variable(torch.zeros(size = (len(encode_output),len(centroids)), requires_grad=True))\n\n    for i in range(len(encode_output)):\n        for j in range(len(centroids)):\n            a = encode_output[i] - centroids[j]\n            b = torch.pow(a.norm(2),2)\n            q_temp[i][j] = torch.pow((1+b\/Constants.MU),-(Constants.MU+1)\/2)\n\n\n    col_sum = torch.sum(q_temp,dim=1)  # --\n    for i in range(len(encode_output)):\n        q_temp[i] = q_temp[i]\/col_sum[i]\n\n    # Calculate p_ij (Eqn-5 of paper)\n    p_temp = Variable(torch.zeros(size = (len(encode_output),len(centroids)), requires_grad=True))\n    temp = torch.pow(p_temp,2)\n    row_sum = torch.sum(q_temp,dim=0)     # ||\n\n    for j in range(len(centroids)):\n        p_temp[:,j] = q_temp[:,j]\/row_sum[j]\n\n    col_sum = torch.sum(p_temp, dim=1)  # --\n    for i in range(len(encode_output)):\n        p_temp[i] = p_temp[i]\/col_sum[i]\n\n    kl_div = F.kl_div(p_temp, q_temp)\n    return kl_div\n\nI have created a custom loss function which ultimately calculates KL-diveregence between p_temp and q_temp. Both the encode_input and centroids are tensors.\nBut I am getting the following error:\nkl_div = F.kl_div(p_temp, q_temp)\nRuntimeError: the derivative for \u2018target\u2019 is not implemented\nWhat could be the issue?","y":"I write the dimensions in the comments. Given:\n<code class=\"lang-auto\">z = torch.randn(7,5)   # i, d use torch.stack([list of z_i], 0) if you don't know how  to get this otherwise.\nmu = torch.randn(6,5)  # j, d\nnu = 1.2\n<\/code>\nyou do\n<code class=\"lang-auto\"># I don't use norm. Norm is more memory-efficient, but possibly less numerically stable in backward\nq_raw = (1 + ((z.unsqueeze(1)-mu.unsqueeze(0))**2).sum(2) \/ nu)**(-(nu+1) \/ 2)  # i, j\nq_sum = q_raw.sum(1, keepdim=True) # i, 1 --> will be broadcast\nq = q_raw \/ q_sum # i, j\n\np_raw = q**2 \/ q.sum(0, keepdim=True) # i, j\np_sum = p_raw.sum(1, keepdim=True)    # 1, j --> will be broadcast\np = p_raw \/ p_sum\n\nkl_div = (p * (p.clamp(min=1e-7).log()  - q.clamp(min=1e-7).log())).sum()\n<\/code>\nKindly send your invoicing address by PM. \nBest regards\nThomas","z":"\n\n\n shivangi:\n\nBut I am getting the following error:\nkl_div = F.kl_div(p_temp, q_temp)\nRuntimeError: the derivative for \u2018target\u2019 is not implemented\n\n\nThis is quite literally what it says: F.kl_div does not support taking gradients w.r.t. the second (target) argument and you q_temp asks for gradients (requires_grad=True). If you want the derivative for the second argument as well, you would need to spell out the kl_div, too.\nIf I may say so, you likely want to replace the for loops over coordinates with clever broadcasting with link \"https:\/\/pytorch.org\/docs\/master\/notes\/broadcasting.html\", this is usually much, much faster to the point where it is an error to do for loops over coordinates (very narrow exceptions apply when you would have unreasonably large intermediates or somesuch).\nBest regards\nThomas\n\n\nThis is what I am trying to implement\nBut what do you mean \u201cspell out\u201d.\nHow should I handle this case? Can you please explain in detail\nCan you suggest a cleaner way to do this?\nI will clean the code for broadcasting as you said once it works\nAssuming p and q are probability distributions and you want kl_div to be D(p||q) (which is not what F.kl_div does because it has log probabilities on p and I think it does D(q||p)) ),\n<code class=\"lang-auto\">  kl_div = (p_temp*(p_temp.log()-q_temp.log())).sum()\n<\/code>\nwould give D(p||q). For stability, you could use p_temp.clamp(min=1e-7).log() or somesuch for the and similarly for q. Also, directly computing the log can enhance stability.\nBest regards\nThomas\n\nThis is my custom loss class:\nimport torch\nfrom torch.autograd import Variable\nfrom common.constants import Constants\nimport torch.nn.functional as F\nimport torch.nn as nn\nclass Cluster_Assignment_Hardening_Loss(torch.nn.Module):\ndef __init__(self):\n    super(Cluster_Assignment_Hardening_Loss,self).__init__()\n\ndef forward(self,encode_output, centroids):\n\n    # Calculate q_ij (Eqn-4 of the paper)\n    q_temp = Variable(torch.zeros(size = (len(encode_output),len(centroids)), requires_grad=True).cuda())\n\n    for i in range(len(encode_output)):\n        for j in range(len(centroids)):\n            a = encode_output[i] - centroids[j]\n            b = torch.pow(a.norm(2),2)\n            q_temp[i][j] = torch.pow((1+b\/Constants.MU),-(Constants.MU+1)\/2)\n\n\n    col_sum = torch.sum(q_temp,dim=1)  # --\n    for i in range(len(encode_output)):\n        q_temp[i] = q_temp[i].clone()\/col_sum[i]\n\n    # Calculate p_ij (Eqn-5 of paper)\n    p_temp = Variable(q_temp.data.clone(), requires_grad=True).cuda()\n    #p_temp = Variable(torch.zeros(size = (len(encode_output),len(centroids)), requires_grad=True).cuda())\n    temp = torch.pow(p_temp,2)\n    row_sum = torch.sum(q_temp,dim=0)     # ||\n\n    for j in range(len(centroids)):\n        p_temp[:,j] = q_temp[:,j]\/row_sum[j]\n\n    col_sum = torch.sum(p_temp, dim=1)  # --\n    for i in range(len(encode_output)):\n        p_temp[i] = p_temp[i].clone()\/col_sum[i]\n\n    kl_div = (p_temp * (p_temp.clamp(min=1e-7).log()  - q_temp.clamp(min=1e-7).log())).sum()\n    return kl_div\n\nBut I am unable to train my network.\nI am getting the following error:\ntorch.autograd.backward(self, gradient, retain_graph, create_graph)\nFile \u201c\/usr\/prakt\/python3.5\/site-packages\/torch\/autograd\/init.py\u201d, line 89, in backward\nallow_unreachable=True)  # allow_unreachable flag\nRuntimeError: leaf variable has been moved into the graph interior\nNow is a good time to vectorize your code \nWhat the error means is that autograd doesn\u2019t like how you assign to bits of your variable (which makes it hard to tell what\u2019s going on if things get overwritten). Most of the need to do so will go away anyway when you write the code better.\n<code class=\"lang-auto\">p_temp = Variable(q_temp.data.clone(), requires_grad=True).cuda()\n<\/code>\nthis will also trip you, as .cuda() counts as computation and you won\u2019t get a grad on p_temp. If you\u2019re on 0.4, use p_temp = q_temp.clone().detach().cuda().requires_grad_() or so.\nBest regards\nThomas\n\n\n\n tom:\n\np_temp = q_temp.clone().detach().cuda().requires_grad_()\n\n\n\nI have changed the code accordingly, but still getting the same error. Even after broadcasting I am getting the same error\ndef forward(self,encode_output, centroids):\n\n    # Calculate q_ij (Eqn-4 of the paper)\n    q_temp = Variable(torch.zeros(size=(len(encode_output),len(centroids))).cuda())\n\n    for j in range(len(centroids)):\n        val = encode_output - centroids[j]\n        nom = torch.pow(val.norm(2,dim=1),2)\n        q_temp[:,j] = torch.pow((1+nom\/Constants.MU),-(Constants.MU+1)\/2)\n\n    q = Variable(q_temp.data.clone(),requires_grad=True).cuda()\n\n    col_sum = torch.sum(q_temp,dim=1)  # --\n    for i in range(len(encode_output)):\n        q[i] = q[i].clone()\/col_sum[i]\n\n\n    # Calculate p_ij (Eqn-5 of paper)\n    p_temp = q.clone().detach().cuda().requires_grad_()\n    p_temp = torch.pow(p_temp,2)\n    row_sum = torch.sum(q,dim=0)     # ||\n\n    p_temp \/= row_sum\n\n    p = p_temp.clone().detach().cuda().requires_grad_()\n    col_sum = torch.sum(p_temp, dim=1)  # --\n    for i in range(len(encode_output)):\n        p[i] = p[i].clone()\/col_sum[i]\n\n    kl_div = (p * (p.clamp(min=1e-7).log()  - q.clamp(min=1e-7).log())).sum()\n    return kl_div\n\nWhat am I missing?  I could not really understand (I am new to pytorch)\nThe assignments q[i] =, q_temp[:, j], p[i] = are likely the source of your problem. If you manage to write them as one operation filling everything, it will probably go away.\nThe snippet you post has too many \u201cfree\u201d variables to try to execute it, but it vaguely looks like it should be easy to eliminate the for loops.\nBest regards\nThomas\n\nI tried to eliminate as many loops a I could. And I am not able to think of any way of eliminating these ones that are left.\nCan you suggest some way on how can I do this?\nThis is what I am trying to calculate.\n($ z_i $ is a one-dim tensor(Encoder output) , $ \\mu_j $ is also a on-dim tensor after doing some manipulations on $$z_i$$ )\nAny help is appreciated\nI write the dimensions in the comments. Given:\n<code class=\"lang-auto\">z = torch.randn(7,5)   # i, d use torch.stack([list of z_i], 0) if you don't know how  to get this otherwise.\nmu = torch.randn(6,5)  # j, d\nnu = 1.2\n<\/code>\nyou do\n<code class=\"lang-auto\"># I don't use norm. Norm is more memory-efficient, but possibly less numerically stable in backward\nq_raw = (1 + ((z.unsqueeze(1)-mu.unsqueeze(0))**2).sum(2) \/ nu)**(-(nu+1) \/ 2)  # i, j\nq_sum = q_raw.sum(1, keepdim=True) # i, 1 --> will be broadcast\nq = q_raw \/ q_sum # i, j\n\np_raw = q**2 \/ q.sum(0, keepdim=True) # i, j\np_sum = p_raw.sum(1, keepdim=True)    # 1, j --> will be broadcast\np = p_raw \/ p_sum\n\nkl_div = (p * (p.clamp(min=1e-7).log()  - q.clamp(min=1e-7).log())).sum()\n<\/code>\nKindly send your invoicing address by PM. \nBest regards\nThomas\n\nThanks. It works now\nI have one more question to ask.\nI want to use this loss combined with MSE error loss to optimize my network\n            loss = alpha * loss_mse + (1 - alpha) * loss_kl_div\n            error += loss.item()\n            loss.backward()\n            optimizer.step()\n\nWill this work for what I wish to achieve?\n\nPM sent.\nPlease check\nThat should work, just remember to zero the grads in your training loop.\nBest regards\nThomas"},{"x":"I run my pytorch code well on mac and even on windows system but the same code seems stuck on CentOS6.3.\nI debug with ipdb, and found the code was stuck at F.conv2d function:\nEFA833B36E022034CEA4593EA27F6342.jpg1490\u00d7204 44.2 KB with link \"https:\/\/discuss.pytorch.org\/uploads\/default\/original\/2X\/2\/298a53a343f90bd370b61ca43e7197a9dd74a668.jpg\"\nPID USER      PR  NI  VIRT  RES  SHR S %CPU %MEM    TIME+  COMMAND\n19285 work      20   0 2106m 906m  22m S  0.3  0.6   0:31.14 python\nThe running env was created with anaconda(python 2.7\/3.6), pytorch version is 0.4.0.\nI tried for a long time to resolve this problem and i tried. Do you have a suggestion? Thank you so much!","y":"I reinstall CentOS6.3, and then upgrade glibc2.14, glibc2.17 due to the pytorch0.4.0 running error info.\nNow everything is ok.\nBy the way, the pytorch0.3.1 perform well before i upgrade the glibc(up to 2.12). So i think the lastest pytorch0.4.0 may haven\u2019t deal very well with glibc, leave running deadlock appearance and doesn\u2019t tell any error and warning info, just stuck at F.conv2d in torch\/nn\/modules\/conv.py(301).\nThank you all the same ","z":"Are you running your code on CPU or GPU, and multiprocessing?\n\n for ii, (data, label) in tqdm(enumerate(train_dataloader)):\n     input = Variable(data)\n     target = Variable(label)\n     optimizer.zero_grad()\n     score = model(input) # stuck here \n     loss = criterion(score, target)\n     loss.backward()\n     optimizer.step()\n\n\nOn CPU, no multiprocessing i think\u2026\nI reinstall CentOS6.3, and then upgrade glibc2.14, glibc2.17 due to the pytorch0.4.0 running error info.\nNow everything is ok.\nBy the way, the pytorch0.3.1 perform well before i upgrade the glibc(up to 2.12). So i think the lastest pytorch0.4.0 may haven\u2019t deal very well with glibc, leave running deadlock appearance and doesn\u2019t tell any error and warning info, just stuck at F.conv2d in torch\/nn\/modules\/conv.py(301).\nThank you all the same "},{"x":"I\u2019m using detach_ to cut off part of a retained graph:\n<code class=\"lang-auto\">import torch\nfrom torch import nn\n\nidx = 0\nclass M(nn.Module):\n\tdef __init__(self):\n\t\tsuper().__init__()\n\t\tself.w = nn.Parameter(torch.tensor(2, dtype = torch.float32))\n\t\n\tdef forward(self, h, x):\n\t\tglobal idx\n\t\tnew_h = h + x * self.w\n\t\tdef get_pr(idx_val):\n\t\t\tdef pr(*_): print(\"<-- {}\".format(idx_val))\n\t\t\treturn pr\n\t\tnew_h.register_hook(get_pr(idx))\n\t\tprint(\"--> {}\".format(idx))\n\t\tidx += 1\n\t\treturn new_h\n\nm = M()\nz = torch.tensor([0], dtype = torch.float32)\na1 = torch.tensor([1], dtype = torch.float32)\na2 = torch.tensor([2], dtype = torch.float32)\nb1 = torch.tensor([1], dtype = torch.float32)\nb2 = torch.tensor([3], dtype = torch.float32)\nb3 = torch.tensor([2], dtype = torch.float32)\nc1 = torch.tensor([2], dtype = torch.float32)\nc2 = torch.tensor([3], dtype = torch.float32)\n\nh0 = torch.cat([z, z], dim = 0)\ni0 = torch.cat([a1, b1], dim = 0)\nh1 = m(h0, i0)\ni1 = torch.cat([a2, b2], dim = 0)\nh2 = m(h1, i1)\nh2.backward(torch.tensor([3-h2[0],0]), retain_graph = True)\n\ni2 = torch.cat([b3, c1], dim = 0)\nh3 = m(torch.cat([h2[[1]], z], dim = 0), i2)\nh3.backward(torch.tensor([6-h3[0],0]), retain_graph = True)\n\n#h2.detach_()\ni3 = torch.cat([c2], dim = 0)\nh4 = m(torch.cat([h3[[1]]], dim = 0), i3)\nh4.backward(torch.tensor([5-h4[0]]), retain_graph = True)\n<\/code>\nThis prints --> for forwards and <-- for backwards to see what\u2019s going on. With no detach, the last few lines are (correct):\n<code class=\"lang-nohighlight\">--> 3\n<-- 3\n<-- 2\n<-- 1\n<-- 0\n<\/code>\nIf h3 is detached, it\u2019s (also correct):\n<code class=\"lang-nohighlight\">--> 3\n<-- 3\n<\/code>\nIf h1 or h2 are detached, it prints out the same line as for no detach (incorrect!). The correct output (e.g. for h2) should be:\n<code class=\"lang-nohighlight\">--> 3\n<-- 3\n<-- 2\n<\/code>\nI\u2019m pretty sure this is a bug. But I\u2019ve only been using pytorch for two days and don\u2019t know the internals. Maybe I\u2019m doing (or expecting) something wrong?","y":"I\u2019m afraid you cannot modify the graph like that at the moment.\nYou have to redo the forward.\n is that a feature we would like to add in the future? Not sure if it\u2019s even feasible with the current backend.","z":"Hi,\nKeep in mind that detach() or detach_() won\u2019t modify an existing graph. It will stop tracking operations for the current Tensor for every new operations you\u2019re going to perform on them.\nI am not sure to understand 100% your code but it looks like you\u2019re trying to modify an existing graph no?\nYeah, trying to modify the existing graph. At the detach point, I know all gradients that flow backward through h2 will be zero, so I want to prune it from the graph.\nYou will need to detach the h2 variable before using it in the rest of the computations.\nDetaching h2 earlier (e.g. right before i2) means gradients from h3.backward won\u2019t flow through it, which I do want.\n\nTo make things less mysterious, this is a dynamic batching POC for RNNs: batch size of 2 (initially), and the three sequences are [a1, a2], [b1, b2, b3], [c1, c2] batched like this (each column is a timestep, first row is t labels):\n<code class=\"lang-nohighlight\">0--1--2--3--4\n|a1|a2|c1|c2|\n|b1|b2|b3|\n<\/code>\nSequence a gets backpropped at t = 2 (h2.backward), but graph can\u2019t be pruned for t < 2 because b isn\u2019t complete so non-zero gradients still need to flow there. b gets backpropped at t = 3, so now the graph can be pruned.\n\nFrom what I\u2019ve seen so far it looks like it\u2019s not possible to modify an existing graph. Is there any way of creating a new graph, without redoing the forward operations?\nI\u2019m afraid you cannot modify the graph like that at the moment.\nYou have to redo the forward.\n is that a feature we would like to add in the future? Not sure if it\u2019s even feasible with the current backend.\nYou are right. Graph is construct at forward time.\nWhile I see the use, I\u2019m a bit afraid that it would end up being dangerous. How the graph is constructed is really an implementation detail, and graph destruction might have surprising consequences, that affect variables other than the one you detached."},{"x":" (e) .env  In [1]   import torch\n\n (e) .env  In [2]   from torch.autograd import Function\n\n (e) .env  In [3]   from torch.cuda import comm\n\n (e) .env  In [4]   a = [torch.tensor([0], device=torch.device(i), requires_grad=True) for i in range(4)]\n\n (e) .env  In [5]   class Reduce(Function):\n                         \n                         def forward(ctx, inputs):\n                             ctx.target_gpus = [inputs[i].get_device() for i in range(len(inputs))]\n                             inputs = sorted(inputs, key=lambda i: i.get_device())\n                             return comm.reduce_add(inputs)\n                    \n                         \n                         def backward(ctx, gradOutput):\n                             return Broadcast.apply(ctx.target_gpus, gradOutput)\n                    \n\n (e) .env  In [6]   b = Reduce.apply(a)\n\n (e) .env  In [7]   b.requires_grad\n            Out[7]   False\n\n (e) .env  In [8]   class Reduce(Function):\n                         \n                         def forward(ctx, *inputs):\n                             ctx.target_gpus = [inputs[i].get_device() for i in range(len(inputs))]\n                             inputs = sorted(inputs, key=lambda i: i.get_device())\n                             return comm.reduce_add(inputs)\n                    \n                         \n                         def backward(ctx, gradOutput):\n                             return Broadcast.apply(ctx.target_gpus, gradOutput)\n                    \n\n (e) .env  In [9]   c = Reduce.apply(*a)\n\n (e) .env  In [10]   c.requires_grad\n            Out[10]   True\n\n (e) .env  In [11]  \n\nCould anyone explains why are their behaviors different? Thank you.","y":"The inner workings of apply check whether an argument is a Tensor (using THPVariable_Check) and needs grad with link \"https:\/\/github.com\/pytorch\/pytorch\/blob\/master\/torch\/csrc\/autograd\/python_function.cpp#L527\".\nThe list passed in the first version isn\u2019t a Tensor.\nThe second version unpacks the list and passes Tensors as arguments.\nBest regards\nThomas","z":"The inner workings of apply check whether an argument is a Tensor (using THPVariable_Check) and needs grad with link \"https:\/\/github.com\/pytorch\/pytorch\/blob\/master\/torch\/csrc\/autograd\/python_function.cpp#L527\".\nThe list passed in the first version isn\u2019t a Tensor.\nThe second version unpacks the list and passes Tensors as arguments.\nBest regards\nThomas"},{"x":"My code is based on Pytorch 0.3.\nI want to take the features of net1(such as, the last layer but one), then feed them to net2.\nNet1 and net2 should be trained simultaneously.  And of course, the gradients from net2 will\nhave an influence on net1.  (I modified the architecture of net2).\nHere is the toy example:\n<code class=\"lang-python\">import torch\nfrom torch.autograd import Variable\nimport torch.nn\n\nclass my_net(nn.Module):\n    def __init__(self):\n        super(my_net, self).__init__()\n        self.features = torch.nn.Sequential(\n                            torch.nn.Conv2d(3, 5, 3, padding=1),\n                            torch.nn.ReLU(),\n                            torch.nn.Conv2d(5, 10, 3, padding=1),\n                            # Take the output of this layer as the input of net2\n                            torch.nn.ReLU(),\n                            torch.nn.Conv2d(10, 15, 3, padding=1)\n                            )\n    def forward(self, x):\n        x = self.features(x)\n        return x\n\nclass my_net2(nn.Module):\n    def __init__(self):\n        super(my_net2, self).__init__()\n        self.features = torch.nn.Sequential(\n                            torch.nn.Conv2d(10, 25, 3, padding=1)\n                            )\n    def forward(self, x):\n        x = self.features(x)\n        return x\n\nnet1 = my_net().cuda()\nnet2 = my_net2().cuda()\n\n\ninput1 = Variable(torch.ones(1, 3, 10, 10).cuda())\nout1 = net1(input1)\n\n# input of net2 are features got from net1\ninput2 = ????????????\nout2 = net2(input2)\n\n# Create targets\ntarget1 = Variable(torch.ones_like(out1.data))\ntarget2 = Variable(torch.ones(1, 25, 10, 10)*2)\n\n\ncriterion = torch.nn.MSELoss(size_average=False)\nloss1 = criterion(out1, target1)\n\n# Or should I user another instance of MSELoss ?\nloss2 = criterion(out2, target2)\n\nloss = loss1+loss2\nloss.backward()\n<\/code>","y":"If you need to have more flexibility, you could have a look at forward hooks in this post with link \"https:\/\/discuss.pytorch.org\/t\/how-can-l-load-my-best-model-as-a-feature-extractor-evaluator\/17254\/6?u=ptrblck\".\nSince you need the gradients, make sure not to detach the tensor.","z":"<code class=\"lang-auto\">class my_net(nn.Module):\n    def __init__(self):\n        super(my_net, self).__init__()\n        self.features1 = torch.nn.Sequential(\n                            torch.nn.Conv2d(3, 5, 3, padding=1),\n                            torch.nn.ReLU(),\n                            torch.nn.Conv2d(5, 10, 3, padding=1),\n                            )\n                            # Take the output of this layer as the input of net2\n         self.features2 = torch.nn.Sequential(\n                            torch.nn.ReLU(),\n                            torch.nn.Conv2d(10, 15, 3, padding=1)\n                            )\n    def forward(self, x):\n        x1 = self.features1(x)\n        x2 = self.features2(x1)\n        return (x1,x2)\n<\/code>\nShould be enough in my opinion\nI am sorry for the confusion. I have updated my code. Thank you for your reply.\nYou could just use out1 and feed it into net2.\nAlternatively, \u2019s suggestion would also work (creating a new model with both models inside).\nI think you misread the question?\nFeatures just before out1 are required for net2 and not out1\u2026!\nComplete Code :\n<code class=\"lang-auto\">import torch\nfrom torch.autograd import Variable\nimport torch.nn\n\nclass my_net(nn.Module):\n    def __init__(self):\n        super(my_net, self).__init__()\n        self.features1 = torch.nn.Sequential(\n                            torch.nn.Conv2d(3, 5, 3, padding=1),\n                            torch.nn.ReLU(),\n                            torch.nn.Conv2d(5, 10, 3, padding=1),\n                            )\n                            # Take the output of this layer as the input of net2\n         self.features2 = torch.nn.Sequential(\n                            torch.nn.ReLU(),\n                            torch.nn.Conv2d(10, 15, 3, padding=1)\n                            )\n    def forward(self, x):\n        x1 = self.features1(x)\n        x2 = self.features2(x1)\n        return (x1,x2)\n\nclass my_net2(nn.Module):\n    def __init__(self):\n        super(my_net2, self).__init__()\n        self.features = torch.nn.Sequential(\n                            torch.nn.Conv2d(10, 25, 3, padding=1)\n                            )\n    def forward(self, x):\n        x = self.features(x)\n        return x\n\nnet1 = my_net().cuda()\nnet2 = my_net2().cuda()\n\n\ninput1 = Variable(torch.ones(1, 3, 10, 10).cuda())\nfeat_for_net2, out1 = net1(input1)\n\n# input of net2 are features got from net1\ninput2 = feat_for_net2\nout2 = net2(input2)\n\n# Create targets\ntarget1 = Variable(torch.ones_like(out1.data))\ntarget2 = Variable(torch.ones(1, 25, 10, 10)*2)\n\n\ncriterion = torch.nn.MSELoss(size_average=False)\nloss1 = criterion(out1, target1)\n\n# Or should I user another instance of MSELoss ?\nloss2 = criterion(out2, target2)\n\nloss = loss1+loss2\nloss.backward()\n\n<\/code>\nIf  has easier approach would be great \nYes, I misunderstood your question, because I thought you want to use self.features. \nI think your approach is fine. Since you need out1 for loss1, that would be the straightforward approach.\nStill looking for a better way. As I may need to select outputs of multi-layers in net1.  Wonder whether it can be implemented in a nicer way.\nIf you need to have more flexibility, you could have a look at forward hooks in this post with link \"https:\/\/discuss.pytorch.org\/t\/how-can-l-load-my-best-model-as-a-feature-extractor-evaluator\/17254\/6?u=ptrblck\".\nSince you need the gradients, make sure not to detach the tensor.\nPretty cool, even I needed something like this!\nThanks"},{"x":"Similar to this post with link \"https:\/\/discuss.pytorch.org\/t\/multiple-forward-passes-1-backward-pass-instead-of-packed-sequences\/11648\", I would like to do multiple toward pass before backward, but on the same model, Effectively simulating a very large batch-size (like Gradient decent) which would not fit in any GPU.\nmore concretely,\nis it possible to somehow accumulate the gradient information and perform GD on large dataset using pytorch and if it is possible, how can I do it?\nMany thanks!","y":"suppose you forward pass on  6 datapoints (6 images) and you want to perform GD on 36 datapoints (images)\nLet dataloader load input with dimension [6x3x224x224] then\n<code class=\"lang-auto\">for i,(input,target) in enumerate(dataloader):\n    output = model(input)\n    loss = lossfn(output, target)\n    loss.backward() #Only stores the gradients at all nodes\n    if (i+1)%6==0:\n        loss = loss \/ 6 # Since we add up loss for 6 minibatches, we would takes mean of loss at end\n        optim.step() #Uses the gradients to backpropogagte after 6 batches\n        optim.zero_grad()\n\n<\/code>","z":"suppose you forward pass on  6 datapoints (6 images) and you want to perform GD on 36 datapoints (images)\nLet dataloader load input with dimension [6x3x224x224] then\n<code class=\"lang-auto\">for i,(input,target) in enumerate(dataloader):\n    output = model(input)\n    loss = lossfn(output, target)\n    loss.backward() #Only stores the gradients at all nodes\n    if (i+1)%6==0:\n        loss = loss \/ 6 # Since we add up loss for 6 minibatches, we would takes mean of loss at end\n        optim.step() #Uses the gradients to backpropogagte after 6 batches\n        optim.zero_grad()\n\n<\/code>\nTurns out i was searching with the wrong keyword,\nalready discussed\nhere with link \"https:\/\/discuss.pytorch.org\/t\/how-to-implement-accumulated-gradient\/3822\" and here with link \"https:\/\/discuss.pytorch.org\/t\/how-to-implement-accumulated-gradient-in-pytorch-i-e-iter-size-in-caffe-prototxt\/2522\"\n\nAlso read this on other post, a huge thank you!"},{"x":"In April, Tensors and Variables merged with link \"https:\/\/pytorch.org\/2018\/04\/22\/0_4_0-migration-guide.html\". It is my understanding from the example code that the following should work:\n<code class=\"lang-auto\">x = torch.ones(1, requires_grad=True)\ny = torch.ones(1, requires_grad=True)\nz = x+y\nz.backward()\nprint(z.grad)\n<\/code>\nYet z.grad is None. What gives?","y":"z is an intermediate, whose .grad is not usually retained (x,y do have grads).\nIf you want the grad for z as well, use z.retain_grad() after z = x + y.\nBest regards\nThomas","z":"z is an intermediate, whose .grad is not usually retained (x,y do have grads).\nIf you want the grad for z as well, use z.retain_grad() after z = x + y.\nBest regards\nThomas\nAh, thank you! That clarifies things a lot.\nIf any developer comes across this: the Pytorch 0.4.0 migration section dealing with the Tensor-Variable merging does not mention retain_grad."},{"x":"Hi all, I got a question about how to find Min over a dimension of a Variable.  I\u2019ve tried to use torch.min() but it returned a tuple.","y":"If you read the documentation more carefully, you\u2019ll found that the tuple contains the argmin and min value.\n\nReturns the minimum value of each row of the input Tensor in the given dimension dim. The second return value is the index location of each minimum value found (argmin).\n","z":"If you read the documentation more carefully, you\u2019ll found that the tuple contains the argmin and min value.\n\nReturns the minimum value of each row of the input Tensor in the given dimension dim. The second return value is the index location of each minimum value found (argmin).\n\nYou\u2019re correct. However, I would like to use the minimum value for a further operation and it must be compatible with autograd. I\u2019m not sure if I make some operations on the Tensor of the tuple, the connectivity of the layer to my graph would  be changed or not.\nIf you use part of the returned tuple, e.g. result[0], it will not change your computation graph. You could check this by visualizing the grad of previous layers.\nI see. Thank for your help \nTry this:\nmin_val, _ = torch.min(input, dim=1, keepdim=True)"},{"x":"I need to implement my own function, and I\u2019m struggling to understand the documentation with link \"https:\/\/pytorch.org\/docs\/stable\/autograd.html?highlight=function#torch.autograd.Function.backward\". Coming from a mathematical point of view, I think of the setup as follows: I\u2019m implementing a function f. For autograd to work, it needs to know how to compute the derivative of g\u2218f when g is a function whose doman matches the codomain of f, assuming it already knows how to compute the derivative of g. By the chain rule, it suffices to provide the derivative of f. How do I fit this into the language of the PyTorch documentation? To me, the derivative of a function from R^m to R^n at some given point is a linear map, not simply a vector. I\u2019d compute the matrix of this linear map, and multiply it with that for g.\nThe documentation with link \"https:\/\/pytorch.org\/docs\/stable\/autograd.html?highlight=function#torch.autograd.Function.backward\" for backward says:\n\nIt must accept a context ctx as the first argument, followed by as many outputs did forward() return, and it should return as many tensors, as there were inputs to forward(). Each argument is the gradient w.r.t the given output, and each returned value should be the gradient w.r.t. the corresponding input.\n\nWhat, precisely, does \u201ceach argument is the gradient w.r.t the given output\u201d mean? The gradient of the full composition, possibly with some projection onto some of the first map\u2019s variables, I assume? But then that\u2019s a linear map. In this example with link \"https:\/\/pytorch.org\/docs\/master\/notes\/extending.html\" the values returned from backward seem to be vectors, not matrices (represeting linear maps). Can someone help me clear up my confusion, perhaps by expressing in more mathematical terms what backward is supposed to do?","y":"It doesn\u2019t compute the full Jacobian. Instead, for a function y = f(x). Given some dL\/dy tensor, the backward computes dL\/dx, which is dL\/dy * dy\/dx.","z":"It doesn\u2019t compute the full Jacobian. Instead, for a function y = f(x). Given some dL\/dy tensor, the backward computes dL\/dx, which is dL\/dy * dy\/dx.\nSo, to clarify: suppose I have some function f:R^2 -> R^3. Suppose its formula is f(x,y) = (x^2,xy, y^2). It\u2019s implemented with a forward that takes a single Tensor of shape [2] as argument and returns a single Tensor of shape [3] as per the formula. By my (apparently wrong) understanding, backward should take a Tensor A of shape [m, 3] (for some m depending on the function following f in the composition) and return a Tensor of shape [m, 2] representing the gradient of the composition at some point (obtained from the context object). The gradient of f can be represented by a 3x2 matrix, the Jacobian at the given point, and the value returned from backward is simply the matrix product of A and this matrix.\nWhat\u2019s the point of taking the A argument at all? Or, in your formulation: Why does backward need to know dL\/dy, when as you say the result we want is just the product of dL\/dy and dy\/dx? Why isn\u2019t it just the responsibility of backward to compute dy\/dx (at the given point)?\n\n\n\n gspr:\n\nBy my (apparently wrong) understanding, backward should take a Tensor A of shape [m, 3] (for some m depending on the function following f in the composition) and return a Tensor of shape [m, 2] representing the gradient of the composition at some point (obtained from the context object).\n\n\nNo, the backward takes in a tensor of shape [3], representing dL\/d output for some scalar L, and returns a tensor of shape [2], representing [dL\/ dx, dL\/dy].\n\n\n\n gspr:\n\nWhy isn\u2019t it just the responsibility of backward to compute dy\/dx (at the given point)?\n\n\nBecause in most cases, computing dL\/dx given dL\/dy is computationally cheaper and what people want in optimizing a scalar objective.\nAha! This is highly illuminating! Thank you. One thing I don\u2019t understand though: what is the meaning of the argument given to backward when the output leaf (sorry, I don\u2019t know the proper terminology) of the graph is not a scalar? I agree with your comment that having a scalar-valued loss function is what makes sense for deep learning, but it seems to be that the AD framework allows for an output leaf that is vector-valued.\nCurrently that is not supported by pytorch, but I agree that having ways to compute full Jacobian will be great!\nIt\u2019s also not supported by most other popular autodiff libraries.\nOK, I understand. Thanks!"},{"x":"It seems like all three options for padding_mode parameter: 'zeros' ,  'reflect' ,  'replicate' output same 0 paddings. Only 'circular' outputs the padding its name suggests. I have used the following code to test this.\n<code class=\"lang-auto\">import torch.nn as nn\nfrom PIL import Image\nimport matplotlib.pyplot as plt\nimport torchvision.utils as utils\nimport numpy as np\n\ndef imshow(images):\n    img = utils.make_grid(images)\n    npimg = img.numpy()\n    plt.imshow(np.transpose(npimg, (1, 2, 0)))\n    plt.show()\n\nfor images, labels, names in loader_eval:\n    conv1 = nn.Conv2d(1, 1, kernel_size=1, stride=1, padding=10, padding_mode='replicate')\n    conv1.state_dict()['weight'].copy_(torch.FloatTensor([[[[1.0]]]]))\n    conv1.state_dict()['bias'].copy_(torch.FloatTensor([0.0]))\n    img = conv1(images)\n    imshow(img.detach())\n    break\n<\/code>\nAm I doing something wrong or is there a bug in the implementation?\nThanks.","y":"Hi,\nI think I found the issue. My Pytorch version is 1.4.0 and I was refering to 1.5.0 docs.\nThanks again for the help.","z":"Hi,\nI ran your code and everything works just fine. Could you please share the exact input and output that produces wrong padding?\nI changed your visualization method and this may help to depict the cases better.\n<code class=\"lang-auto\">import torch.nn as nn\nfrom PIL import Image\nimport seaborn as sn\nimport pandas as pd\nimport matplotlib.pyplot as plt\nimport torchvision.utils as utils\nimport numpy as np\n\ndef imshow(img):\n    npimg = img.numpy()\n    # npimg = np.transpose(npimg, (1, 2, 0))\n    df_cm = pd.DataFrame(npimg[0, 0])\n    plt.figure(figsize = (10,7))\n    sn.heatmap(df_cm, annot=True)\n\n# I have considered that you have a batch of 1 grayscale image.\nimg = torch.arange(0, 100, dtype=torch.float32).view(1, 1, 10, 10)\nconv1 = nn.Conv2d(1, 1, kernel_size=1, stride=1, padding=(2,2), padding_mode='replicate')\nconv1.state_dict()['weight'].copy_(torch.FloatTensor([[[[1.0]]]]))\nconv1.state_dict()['bias'].copy_(torch.FloatTensor([0.0]))\nimg2 = conv1(img)\nimshow(img2.detach())\n<\/code>\nbests\nHi,\nThanks for the reply. I have ran your code and here are the ouputs:\nreflect\nreflect1000\u00d7700 48.1 KB with link \"https:\/\/discuss.pytorch.org\/uploads\/default\/original\/3X\/f\/d\/fdb8fca8560958b230bcd05944600a9000f11ee4.png\"\nreplicate\nreflect1000\u00d7700 48.1 KB with link \"https:\/\/discuss.pytorch.org\/uploads\/default\/original\/3X\/f\/d\/fdb8fca8560958b230bcd05944600a9000f11ee4.png\"\ncircular\ncircular1000\u00d7700 49.2 KB with link \"https:\/\/discuss.pytorch.org\/uploads\/default\/original\/3X\/1\/2\/120576a82ae16ac00ec2e680fba021f1deceafc0.png\"\nThanks.\nHi,\nI think I found the issue. My Pytorch version is 1.4.0 and I was refering to 1.5.0 docs.\nThanks again for the help.\nOw, that is possible, I lost the track of versions.\nI think I have to mention that libraries like PyTorch are being consistently updated and there are thousands of issues and pull requests on Github. So, I think the best way to keep our codes stable and more reliable (even faster and more optimized) is to update to latest stable version.\nGood luck"},{"x":"Error: save_for_backward can only save input or output tensors\nIt does not allow me to save any intermediate tensors.\nAlso, in backward definition if I do .data will it still work?\nThank you,","y":"Sorry maybe my statement was unclear.\nWhen you will call Name.apply(input), depending on which version of pytorch you\u2019re currently running (this behaviour changed in the latest releases), you\u2019ll get either Tensor or Variable (that depends on the pytorch version, not what you do).","z":"You can use the ctx to save everything you want exept for input or output that should only be saved with save_for_backward (to avoid bugs).\nThe backward definition works with Variable by default. If your backward implementation is not differentiable (meaning you can\u2019t do backward of backward), then you can use the  (can be imported torch.autograd.function) decorator on top of the  for your backward and it will receive directly Tensor and will raise a proper error if you try to differentiate it.\nThank you for the quick response.\nThis is really helpful. However, my gradchecks passes what could be the reason for that.\nAlso, to clarify as I am new to python, saving with ctx means similar to self?\nExample:\nctx.intermediate_result = torch.Tensor(2, 3)\nThanks!\nYes it does mean the same thing.\nThe gradcheck is expected to pass because it only checks the first derivative.\nAlso, this implies that whatever intermediate results we save in forward should be wrapped in Variable inside forward definition itself ?\nThanks!\nDepending on which version of pytorch you\u2019re using \nIf the input is a Variable, then yes, wrap everything in Variables.\nIf the input is a Tensor, then just compute your forward and return a Tensor.\nI have summarized what I am doing with custom operations\nclass Name(Function):\n     \n     def forward(ctx, input):\n         ctx.save_for_backward(input)\n         ctx.intermediate_results = tensor\n         return loss\n\n     \n     def backward(ctx, grad_output):\n        if torch.is_tensor(grad_output):\n            tensor = ctx.intermediate_results\n            inputs = ctx.saved_tensors\n        else:\n            tensor = Variable(ctx.intermediate_results)\n            inputs = ctx.saved_variables\n        # do gradient computation\n        return grad_input\n  \n##OR Case 2\n\nclass Name(Function):\n     \n     def forward(ctx, input):\n         ctx.save_for_backward(input)\n         ctx.intermediate_results = Variable(tensor)\n         return loss\n\n     \n     def backward(ctx, grad_output):\n        tensor = ctx.intermediate_results\n        inputs = ctx.saved_variables\n        # do gradient computation\n        return grad_input\n\nThe second one works fine when I feed Variables?\nwhich one is correct?\nI don\u2019t know how to test the backward of the first one using just tensors?\nWhat is the correct methodology?\nSorry maybe my statement was unclear.\nWhen you will call Name.apply(input), depending on which version of pytorch you\u2019re currently running (this behaviour changed in the latest releases), you\u2019ll get either Tensor or Variable (that depends on the pytorch version, not what you do)."},{"x":"Like the code shown below. It notified me that I was trying to use some graph that had been freed but I could not find why.\n\n\ngist.github.com with link \"https:\/\/gist.github.com\/santisy\/5c3b8e15f13c1c1719fabfd105c970df\"\n\n\nhttps:\/\/gist.github.com\/santisy\/5c3b8e15f13c1c1719fabfd105c970df\nbug_report1.py\n<code class=\"Python\">import torch\nimport torch.nn as nn\nimport torch.nn.functional as F\nfrom torch.autograd import Variable\nfrom torch.nn import Parameter\n\ndef _l2normalize(v, eps=1e-12):\n    return v \/ (torch.norm(v, p=2) + eps)\n\ndef max_singular_value(W, u=None, Ip=1):<\/code>\nThis file has been truncated. show original with link \"https:\/\/gist.github.com\/santisy\/5c3b8e15f13c1c1719fabfd105c970df\"\n\n\n\n\n\n\n\n\nNote: this code is trying to do Spectral Normalization.(https:\/\/arxiv.org\/pdf\/1802.05957.pdf)","y":"Yes, simply detaching u from the graph also make sens\u2026","z":"A weird bug indeed. Your problem comes from the fact that it seems you can\u2019t create a parameter on CPU and then move it on GPU. As a consequence, your parameter (u) is no longer a parameter but becomes a variable. And when you call zero_grad, as u is still a leaf of the graph (requiring no gradient), but is not recognized by the loop that only treats module\u2019s parameters.\nYou have to create the parameter from a tensor already on the GPU.\nMy solution:\n<code class=\"lang-python\">class SNLinear(nn.Linear):\n    def __init__(self, in_features, out_features, bias=True, Ip=1):\n        super(SNLinear, self).__init__(in_features, out_features, bias)\n        self.Ip = Ip\n        self.u = None\n\n    def max_singular_value(self):\n        W = self.weight.view(self.weight.size(0), -1)\n        size = W.size() # n x m\n        if self.u is None:\n            self.u = Parameter(torch.FloatTensor(1, size[0]).normal_().cuda(), requires_grad=False) # 1 x n\n        _u = self.u\n        for _ in range(self.Ip):\n            _v =  _l2normalize(torch.mm(_u, W)) # 1 x m\n            _u = _l2normalize(torch.mm(W, _v.t())) # n x 1\n            _u = _u.view(1, -1)\n        sigma = _u.mm(W).mm(_v.t())\n        return sigma, _u\n\n    def forward(self, input):\n        sigma, _u = self.max_singular_value()\n        self.u.data = _u.data\n        W_bar = self.weight \/ sigma\n        return F.linear(input, self.weight\/sigma, self.bias)\n<\/code>\nActually, I just found a solution. Basically, just add .detach() at https:\/\/gist.github.com\/santisy\/5c3b8e15f13c1c1719fabfd105c970df#file-bug_report1-py-L39\nYes, simply detaching u from the graph also make sens\u2026"},{"x":"I don\u2019t understand since we always use  optimizer.zero_grad() to clear the gradinet saved in the variable, why it keeps the last gradient.\nIs there any other place to use the previous gradient?","y":"You could simulate a larger batch size by accumulating the gradients from a few forward passes and call backward() on these.\nAlso in the DCGAN with link \"https:\/\/github.com\/pytorch\/examples\/blob\/master\/dcgan\/main.py#L240\" example the gradients from the \u201creal\u201d and \u201cfake\u201d loss are accumulated and the optimizer is called just after both backward passes were called.\nIt gives you more flexibility, if you would like to experiment with some crazy stuff! ","z":"You could simulate a larger batch size by accumulating the gradients from a few forward passes and call backward() on these.\nAlso in the DCGAN with link \"https:\/\/github.com\/pytorch\/examples\/blob\/master\/dcgan\/main.py#L240\" example the gradients from the \u201creal\u201d and \u201cfake\u201d loss are accumulated and the optimizer is called just after both backward passes were called.\nIt gives you more flexibility, if you would like to experiment with some crazy stuff! "},{"x":"To my knowledge, the autograd.backward() function is used to determine the gradient of the loss with respect to the output of the network, which ultimately gets propagated back through the network via chain rule.\nIs it possible to manually set the initial gradient (gradient of loss w.r.t. output), and use the backward() function to propagate this artificial gradient back through the network to the inputs?\nIf so, how might I go about doing this?\nThis post with link \"https:\/\/discuss.pytorch.org\/t\/how-to-manually-set-output-gradients\/10363\" asked a similar question, but the answer claims that there is some way to set a gradient param in the backward() function. However, I do not see this in the documentation.\nThanks!","y":"I apologize, it should be output.backward().\nWhat you want to do is:\n<code class=\"lang-auto\">output.backward(custom_grad)\n<\/code>","z":"you can do:\n<code class=\"lang-python\">model.backward(gradient)\n<\/code>\n Thanks for the quick reply!\nI am encountering an issue when I attempt to run the line that you suggested. I assume that when you are referring to model, you are referring to a \u2018Net\u2019 object. I am trying to run the following code segment, in which I am attempting to:\n\nLoad a pre-trained MNIST model\nExecute a forward pass with a single image (batch_size=1) to get activations\nCreate a dummy gradient (custom_grad = 0.5)\nBackpropagate the dummy gradient through the network\nAccess the artificial gradient w.r.t. the input\n\n<code class=\"lang-auto\"># Load model for testing\nmodel = Net()\nSoftmaxWithXent = nn.CrossEntropyLoss()\nmodel = torch.load('.\/mnist_saved_model.pth')\nmodel.eval()\n\n# Construct the testing dataset\ntest_dataset = MNIST_Dataset(mnist_test_data, mnist_test_labels)\n\nfor img,lbl in test_dataset.read(batch_size=1, shuffle=True):\n    # Create the data and label variables so we can use them in the computation\n    img = Variable(torch.FloatTensor(img), requires_grad=True)\n    lbl = Variable(torch.LongTensor(lbl))\n    # Normalize RGB [0,255] to [0,1]\n    img = torch.div(img, 255.0)\n    # Call a forward pass on the data\n    output = model(img)\n    custom_grad = torch.FloatTensor(np.asarray([0.5]))\n    model.backward(custom_grad)\n    print(\"img.grad.data\", img.grad)\n<\/code>\nWhen I run this, I get the following error:\nAttributeError: 'Net' object has no attribute 'backward'\n** As a side note, when I get the gradient w.r.t. a loss function (as usual), and attempt to extract the gradient w.r.t. the input image, I either get None or gradient values that are nearly zero (on order of 10^-30). Why might this be?\nI am new to PyTorch so please excuse my ineptitude. Thanks again!\nI apologize, it should be output.backward().\nWhat you want to do is:\n<code class=\"lang-auto\">output.backward(custom_grad)\n<\/code>"},{"x":"I\u2019ve just implemented a new decorrelation penalty function in Torch, and I\u2019m trying to check the gradient using torch.autograd.gradcheck. It failed, so I checked my previously implemented functions that were passing gradcheck. Those failed too.\nI got curious and started testing torch.inverse, torch.mm, and a few other functions. Every single one of them fails gradcheck.\nIs there something wrong with gradcheck in version 0.3.0.post4?","y":"Looks like you are using FloatTensor\u2019s.  I believe gradcheck is designed only to work with DoubleTensors\nTry adding .double() to the model and the Variable, and you should find it works OK.","z":"gradcheck with link \"https:\/\/github.com\/pytorch\/pytorch\/blob\/70ba50c3d49c0454210ead1ad931aa39520b10f8\/torch\/autograd\/gradcheck.py#L139\" has a precision argument. If your function generates large gradients, then the error will be larger and might pass the precision threshold of 1e-6.\nEven with eps=1e-3, this code is still failing\nimport torch\nx = torch.autograd.Variable(torch.randn(40, 20), requires_grad=True)\ny = torch.autograd.Variable(torch.randn(20, 30), requires_grad=True)\nres = torch.autograd.gradcheck(torch.mm, (x, y), eps=1e-3)\nprint(res)\nLooks like you are using FloatTensor\u2019s.  I believe gradcheck is designed only to work with DoubleTensors\nTry adding .double() to the model and the Variable, and you should find it works OK."},{"x":"I\u2019m grad-checking a model, and seem to have traced a broken gradient to the embeddings I was plugging in.\nHere is a minimal example of my code\u2026 any thoughts welcome.\n<code class=\"lang-auto\">import torch.nn as nn\nfrom torch.nn.utils.rnn import pack_padded_sequence, pad_packed_sequence\nfrom torch.autograd.gradcheck import gradcheck\nfrom torch.autograd import Variable\nimport torch\n\n    def testGradCheckEmbeddingBasic(self):\n        seqs = ['ghatmasala', 'nicela', 'c-pakodas']\n        e = nn.Embedding(10, 3, sparse=False).double()\n        indices = Variable(torch.LongTensor([[1], [4]]))\n        embed = e(indices)\n        print(embed)\n        input = (embed, )\n        model = nn.Linear(3, 3).double()\n        test = gradcheck(model, input, eps=1e-6, atol=1e-4)\n<\/code>\nError I get is\u2026\nVariable containing:\n(0 ,.,.) =\n-1.7624  0.1646 -0.5719\n(1 ,.,.) =\n-0.5188 -0.2282  1.3176\n[torch.DoubleTensor of size 2x1x3]\nRan 1 test in 0.232s\nFAILED (errors=1)\nError\nTraceback (most recent call last):\nFile \u201cC:\\Users\\ZEBEAST\\Anaconda3\\envs\\pytorch\\lib\\unittest\\case.py\u201d, line 59, in testPartExecutor\nyield\nFile \u201cC:\\Users\\ZEBEAST\\Anaconda3\\envs\\pytorch\\lib\\unittest\\case.py\u201d, line 605, in run\ntestMethod()\nFile \u201cC:\\Users\\ZEBEAST\\PycharmProjects\\sauron\\tests\\test_characterEmbedding.py\u201d, line 51, in testGradCheckEmbeddingBasic\ntest = gradcheck(model, input, eps=1e-6, atol=1e-4)\nFile \u201cC:\\Users\\ZEBEAST\\Anaconda3\\envs\\pytorch\\lib\\site-packages\\torch\\autograd\\gradcheck.py\u201d, line 181, in gradcheck\nreturn fail_test(\u2018for output no. %d,\\n numerical:%s\\nanalytical:%s\\n\u2019 % (j, numerical, analytical))\nFile \u201cC:\\Users\\ZEBEAST\\Anaconda3\\envs\\pytorch\\lib\\site-packages\\torch\\autograd\\gradcheck.py\u201d, line 166, in fail_test\nraise RuntimeError(msg)\nRuntimeError: for output no. 0,\nnumerical:(\n0.3081 -0.1945 -0.0135  0.0000  0.0000  0.0000\n-0.0716  0.3882  0.2380  0.0000  0.0000  0.0000\n0.3901 -0.1542  0.5559  0.0000  0.0000  0.0000\n0.0000  0.0000  0.0000  0.3081 -0.1945 -0.0135\n0.0000  0.0000  0.0000 -0.0716  0.3882  0.2380\n0.0000  0.0000  0.0000  0.3901 -0.1542  0.5559\n[torch.FloatTensor of size 6x6]\n,)\nanalytical:(\n0  0  0  0  0  0\n0  0  0  0  0  0\n0  0  0  0  0  0\n0  0  0  0  0  0\n0  0  0  0  0  0\n0  0  0  0  0  0\n[torch.FloatTensor of size 6x6]\n,)","y":"It does require grad, but it is not a leaf Variable (a Variable created by the user). The fact that it requires grad means that its gradient is needed to compute the gradient of other leaf Variables that require grad. So it will be computed but not saved to not increase memory usage for no reason.\nIf you want its gradient to be saved, you need to call .retain_grad() that will make it save its gradients even though it is not a leaf Variable.","z":"Hi,\nThe thing is that the input that you give to the gradcheck should be a Variable for which you want gradients: either a leaf Variable with requires_grad=True or call .retain_grad() in the input Variable before giving it to gradcheck so that it\u2019s gradients are computed.\nHi Alban, thanks for the reply\u2026\nHere\u2019s a screenshot of the inputted \u201cembed\u201d variable from the debugger\u2026\nimage.png968\u00d7387 28 KB with link \"https:\/\/discuss.pytorch.org\/uploads\/default\/original\/2X\/7\/7d8ee12cc73c5729ecc7c73d2239123e50c7604a.png\"\nIt says that the embedding Variable requires a gradient.  Is that what you mean?\nAdding retain_grad() fixes it though!  Kinda odd\u2026\n<code class=\"lang-auto\">    def testGradCheckEmbeddingBasic(self):\n        seqs = ['ghatmasala', 'nicela', 'c-pakodas']\n        e = nn.Embedding(10, 3, sparse=False).double()\n        indices = Variable(torch.LongTensor([[1], [4]]))\n        embed = e(indices)\n        print(embed)\n        embed.retain_grad()\n        input = (embed, )\n        model = nn.Linear(3, 3).double()\n        test = gradcheck(model, input, eps=1e-6, atol=1e-4)\n<\/code>\nVariable containing:\n(0 ,.,.) =\n-1.5149  0.3036 -0.8191\n(1 ,.,.) =\n-0.6803  0.7728  2.4776\n[torch.DoubleTensor of size 2x1x3]\nRan 1 test in 0.246s\nOK\nIt does require grad, but it is not a leaf Variable (a Variable created by the user). The fact that it requires grad means that its gradient is needed to compute the gradient of other leaf Variables that require grad. So it will be computed but not saved to not increase memory usage for no reason.\nIf you want its gradient to be saved, you need to call .retain_grad() that will make it save its gradients even though it is not a leaf Variable.\nCool, thanks for the explanation Alban!"},{"x":"I tried the following standard custom function example from the documentation here with link \"http:\/\/pytorch.org\/docs\/master\/notes\/extending.html\":\n<code class=\"lang-python\">class LinearFunction(Function):\n\n    \n    # bias is an optional argument\n    def forward(ctx, input, weight, bias=None):\n        ctx.save_for_backward(input, weight, bias)\n        output = input.mm(weight.t())\n        if bias is not None:\n            output += bias.unsqueeze(0).expand_as(output)\n        return output\n\n    # This function has only a single output, so it gets only one gradient\n    \n    def backward(ctx, grad_output):\n        input, weight, bias = ctx.saved_tensors\n        grad_input = grad_weight = grad_bias = None\n\n        if ctx.needs_input_grad[0]:\n            grad_input = grad_output.mm(weight)\n        if ctx.needs_input_grad[1]:\n            grad_weight = grad_output.t().mm(input)\n        if bias is not None and ctx.needs_input_grad[2]:\n            grad_bias = grad_output.sum(0).squeeze(0)\n\n        return grad_input, grad_weight, grad_bias\n\nlinear = LinearFunction.apply\n\nb, p, q = 10, 5, 1\nweight = nn.Parameter(torch.rand(q, p), requires_grad=True)\nin_tensor = Variable(torch.rand(b, p), requires_grad=False)\n\nout = linear(in_tensor, weight).mean()\nout.backward()\n<\/code>\nWhen I run, it gives the following error:\nRuntimeError: mm(): argument 'mat2' (position 1) must be Variable, not torch.FloatTensor.\nCan you spot where my mistake is?","y":"For pytorch 0.3.1, the forward uses Tensors, but the backward uses Variables, so use ctx.saved_variables instead of ctx.saved_tensors.\nOn master (the documentation of which you link) it is ctx.saved_tensors again.\nBest regards\nThomas","z":"For pytorch 0.3.1, the forward uses Tensors, but the backward uses Variables, so use ctx.saved_variables instead of ctx.saved_tensors.\nOn master (the documentation of which you link) it is ctx.saved_tensors again.\nBest regards\nThomas\nThank you.  Just resolved the error."},{"x":"I want to make sure I understand the relationship between\nsetting requires_grad = False\nAND\nnot passing the layer\u2019s parameters into the optimizer, as discussed here with link \"https:\/\/discuss.pytorch.org\/t\/how-to-perform-finetuning-in-pytorch\/419\/6\".\nMy question is if requires_grad = False, what will the optimizer do with those parameters?  Does it ever make sense to set requires_grad = False and still pass the frozen parameters to the optimizer?","y":"Nevermind, this question doesn\u2019t even make any sense.  If you try to do this, you will get the error:\nValueError: optimizing a parameter that doesn't require gradients","z":"Nevermind, this question doesn\u2019t even make any sense.  If you try to do this, you will get the error:\nValueError: optimizing a parameter that doesn't require gradients"},{"x":"I\u2019m trying to backward over loss inside a loop.\nThe loss variable is computed afresh each time and so are other variables. The 1st iteration works fine and backward() call works.\nBut it throws the retain_graph error in the consequent iteration. I can\u2019t find where the problem is.","y":"The variables inside model didn\u2019t get updated with parameters, causing a dependency on prev iteration parameters.","z":"The variables inside model didn\u2019t get updated with parameters, causing a dependency on prev iteration parameters."},{"x":"Hi there! I am trying to use nn.AvgPool2d and I am getting the following error: RuntimeError: avg_pool2d(): argument 'input' (position 1) must be Variable, not torch.cuda.FloatTensor\nThe error is generated by the following code:\n<code class=\"lang-auto\">import torch\nimport torch.nn as nn\n\nm = nn.AvgPool2d(3, stride= 2)\ninputs = torch.randn(20, 16, 50, 32)\noutput = m(inputs)\n<\/code>\nThe output is the following:\n<code class=\"lang-auto\">Traceback (most recent call last):\n  File \"<stdin>\", line 1, in <module>\n  File \"\/usr\/local\/lib\/python3.6\/dist-packages\/torch\/nn\/modules\/module.py\", line 357, in __call__\n    result = self.forward(*input, **kwargs)\n  File \"\/usr\/local\/lib\/python3.6\/dist-packages\/torch\/nn\/modules\/pooling.py\", line 506, in forward\n    self.padding, self.ceil_mode, self.count_include_pad)\nRuntimeError: avg_pool2d(): argument 'input' (position 1) must be Variable, not torch.FloatTensor\n<\/code>\nI am using a docker container (Dockerfile with link \"https:\/\/github.com\/sergiuoprea\/MyDockerfiles\/blob\/master\/Cuda90_Python36_PyTorch_TF_Keras\/Dockerfile\") based on ubuntu 16.04 with cuda9.0 and cudnn7.  Python 3.6 installing PyTorch from  http:\/\/download.pytorch.org\/whl\/cu90\/torch-0.3.1-cp36-cp36m-linux_x86_64.whl. I am not sure if this can be related with PyTorch version. Should I update to PyTorch 0.4.0 ?. The code which produces the above error was extracted from the official  PyTorch nn documentation. Thanks in advance for your help! I really don\u2019t know what is the problem","y":"Updating to PyTorch 0.4.0 solved this issue","z":"Updating to PyTorch 0.4.0 solved this issue"},{"x":"I implemented my own loss function using nn.Module. Here\u2019s a simplified example:\n<code class=\"lang-auto\">import torch\nimport torch.nn as nn\nfrom torch.autograd import Variable\n\ndtype = torch.FloatTensor\n\nclass MyLoss(nn.Module):\n    def __init__(self, dim, noise):\n        super(MyLoss, self).__init__()\n        self.W = Variable(torch.randn(dim, 1).type(dtype), requires_grad=True)\n        self.sigmoid = nn.Sigmoid()\n\n    def forward(self, y_hat, y):\n        h = y_hat-y\n        g = self.sigmoid(self.W)\n        m = torch.mul(h,g)\n        return torch.norm(m) # this should go to 0\n<\/code>\nfor each iteration I use:\n<code class=\"lang-auto\">        optimizer.zero_grad()\n        x = Variable(torch.FloatTensor(x),requires_grad=False)\n        y = Variable(torch.FloatTensor(y),requires_grad=False)\n        # Forward + Backward + Optimize\n        y_hat = model(x)\n        loss = criterion(y_hat, y)\n        loss.backward() # prams of MyLoss are not updated\n        optimizer.step()\n<\/code>\nCurrently W, the variable that requires a grad in MyLoss is no updating. I guess I need to implement the backward function as well in MyLoss. Is that correct? If so how can I use pytorch backward functions and not calculate the gradients myself?","y":"To add learnable parameters to an nn.Module, you should use nn.Parameter type:\nself.W = nn.Parameter(torch.randn(dim, 1).type(dtype))\nThen you need to make sure that this is passed to your optimizer with the other parameters of your network:\noptimizer = optim.SGD(model.parameters() + criterion.parameters(), optim_args).\nAnd no you don\u2019t need to implement the backward pass if you do that ","z":"To add learnable parameters to an nn.Module, you should use nn.Parameter type:\nself.W = nn.Parameter(torch.randn(dim, 1).type(dtype))\nThen you need to make sure that this is passed to your optimizer with the other parameters of your network:\noptimizer = optim.SGD(model.parameters() + criterion.parameters(), optim_args).\nAnd no you don\u2019t need to implement the backward pass if you do that "},{"x":"I implemented memory-efficient DenseNet for PyTorch v0.4.0 and it works fine for single GPU. However, it fails in the multi-GPUs case.  The error occurs during the backward process of nn.DataParallel model:\nRuntimeError: one of the variables needed for gradient computation has been modified by an inplace operation\nI guess the modifications of intermediate variables with shared storage cause this error, but I cannot locate the inplace operation since all the inplace operations work fine for single-gpu case.  Does nn.DataParallel do additional gradient checking when backward?\nBesides, I run the similar implementation with Pytorch v0.3.1 and it works well for nn.DataParallel.\nI opened an issue with link \"https:\/\/github.com\/wandering007\/efficient-densenet-pytorch\/issues\/1\" for the project. The code can be tested.","y":"I fix the bug strangely by no longer restoring the running_mean and running_var for backward.\nThe in-place operation like self.running_mean.copy_(self.prev_running_mean) when backward cannot pass the nn.DataParallel case.","z":"I fix the bug strangely by no longer restoring the running_mean and running_var for backward.\nThe in-place operation like self.running_mean.copy_(self.prev_running_mean) when backward cannot pass the nn.DataParallel case."},{"x":"Hi, I\u2019m interested in trying to combine loss criteria, and I made a little function, to try to do that.   Here\u2019s what DOESN\u2019T work\u2026\n<code class=\"lang-auto\">def calc_loss(Y_pred, Y_true, criteria, lambdas):\n    #  Y_pred: the output from the network\n    #  Y_true: the target values\n    #  criteria: an iterable of pytorch loss criteria, e.g. [torch.nn.MSELoss(), torch.nn.L1Loss()]\n    #  lambdas:  a list of regularization parameters\n    assert len(criteria) == len(lambdas), \"Must have the same number of criteria as lambdas\"\n    loss = torch.zeros(1, requires_grad=True)       # not sure how to properly initialize a single pytorch number for autograd\n    for i in range(len(criteria)):\n        loss += lambdas[i] * criteria[i](Y_pred, Y_true)\n    return loss\n<\/code>\nWhen I do that, I get the Runtime error at the \u201closs +=\u201d line\u2026\n<code class=\"lang-auto\">RuntimeError: a leaf Variable that requires grad has been used in an in-place operation.\n<\/code>\nSo instead, here\u2019s the working routine that I have right now, but it\u2019s just not \u2018elegant\u2019 IMHO:\n<code class=\"lang-auto\">def calc_loss(Y_pred, Y_true, criteria, lambdas):\n    assert len(criteria) == len(lambdas), \"Must have the same number of criteria as lambdas\"\n\n    for i in range(len(criteria)):\n        if (0==i):\n            loss = lambdas[i] * criteria[i](Y_pred, Y_true)\n        else:\n            loss += lambdas[i] * criteria[i](Y_pred, Y_true)\n    return loss\n<\/code>\nFor future reference how should you properly initialize a \u2018zero\u2019 loss of this type?\n(I\u2019m using Pytorch 0.4)\nThanks!\nPS- Also tried a simple\u2026\n<code class=\"lang-auto\">    loss = torch.dot(lambdas, criteria(Y_pred, Y_true))\n<\/code>\n\u2026but that was just wishful thinking, because the lambdas are \u2018just numbers\u2019, so you get TypeError: 'list' object is not callable.  Even if you wrap the lambdas in torch.tensor(), you get the same error.","y":"Hi,\nYou can just initialize it with a python 0.\nI haven\u2019t run it, but the following code should work \n<code class=\"lang-auto\">def calc_loss(Y_pred, Y_true, criteria, lambdas):\n    #  Y_pred: the output from the network\n    #  Y_true: the target values\n    #  criteria: an iterable of pytorch loss criteria, e.g. [torch.nn.MSELoss(), torch.nn.L1Loss()]\n    #  lambdas:  a list of regularization parameters\n    assert len(criteria) == len(lambdas), \"Must have the same number of criteria as lambdas\"\n    loss = 0\n    for i in range(len(criteria)):\n        loss += loss lambdas[i] * criteria[i](Y_pred, Y_true)\n    return loss\n<\/code>","z":"Hi,\nYou can just initialize it with a python 0.\nI haven\u2019t run it, but the following code should work \n<code class=\"lang-auto\">def calc_loss(Y_pred, Y_true, criteria, lambdas):\n    #  Y_pred: the output from the network\n    #  Y_true: the target values\n    #  criteria: an iterable of pytorch loss criteria, e.g. [torch.nn.MSELoss(), torch.nn.L1Loss()]\n    #  lambdas:  a list of regularization parameters\n    assert len(criteria) == len(lambdas), \"Must have the same number of criteria as lambdas\"\n    loss = 0\n    for i in range(len(criteria)):\n        loss += loss lambdas[i] * criteria[i](Y_pred, Y_true)\n    return loss\n<\/code>\nWow, that\u2019s so simple. I guess I\u2019d anticipated getting \u2018Expected __Tensor but got ___Tensor\u2019  errors if I tried that!\nThanks.  Yes, it works."},{"x":"Hi everyone\nI am currently working on keyframe extraction from videos.\nCode :\n<code class=\"lang-auto\">import torch\nfrom torch.utils.data import Dataset\nfrom PIL import Image\nimport numpy as np\nimport cv2\n\nclass makeDataset(Dataset):\n    def __init__(self, dataset, labels, spatial_transform, seqLen=20):\n        self.spatial_transform = spatial_transform\n        self.images = dataset\n        self.labels = labels\n        self.seqLen = seqLen\n\n    def __len__(self):\n        return len(self.images)\n\n    def __getitem__(self, idx):\n        vid_name = self.images[idx]\n        label = self.labels[idx]\n        inpSeq = []\n        self.spatial_transform.randomize_parameters()\n        vid=cv2.VideoCapture(vid_name)\n        ret, prev_frame = vid.read()\n        p_frame_thresh = 20\n        count=1\n        index=0\n        while(index<=self.seqLen):\n            # Extract images\n            ret, curr_frame = vid.read()\n            diff = cv2.absdiff(curr_frame, prev_frame)\n            non_zero_count = np.count_nonzero(diff)\n            if non_zero_count > p_frame_thresh:\n                count+=1\n                img=Image.fromarray(curr_frame)\n                inpSeq.append(self.spatial_transform(img.convert('RGB')))\n            prev_frame = curr_frame\n        print (count)\n        inpSeq = torch.stack(inpSeq, 0)\n        return inpSeq, label\n<\/code>\nwhat is the problem here, I don\u2019t understand this error? can any one guide me please ","y":"The two frames in video not having the same size.\ndiff = cv2.absdiff(curr_frame, prev_frame)\nthis operation only performs when the two frames having same size.","z":"The two frames in video not having the same size.\ndiff = cv2.absdiff(curr_frame, prev_frame)\nthis operation only performs when the two frames having same size.\nI am also having the same problem\u2026 How do you fix it ?  I am new to Python and learning now\u2026 I hope that there is a way to make the images of same sizes using code\u2026"},{"x":"Hi,\nIn my model , some parameters\u2019s gradient keeps as zero,  which confused me a lot.  Some necessary information list as below:\nThe main code about the parameters of zero-gradient:\n<code class=\"lang-auto\">###\nsome code\n###\n# about attention\nalpha = F.softmax((bag_embs * self.att_a).mm(self.query_vec), 1)\nbag_embs = (bag_embs * alpha).sum(0)\nbatch_fea.append(bag_embs)\n###\nsome code\n###\n<\/code>\nin the model definition:\n<code class=\"lang-auto\">self.bias_d = nn.Parameter(torch.randn(3,5))\nself.query_vec = nn.Parameter(torch.randn(3,1)\nself.att_a = nn.Parameter(torch.randn(3)\n\n<\/code>\nin the model weight init:\n<code class=\"lang-auto\">nn.init.xavier_uniform(self.query_vec)\nnn.init.uniform(self.att_a)\n<\/code>\nAnd I use the Tensorboad to visualize the model parameters and its gradient:\nimage.png957\u00d7605 42 KB with link \"https:\/\/discuss.pytorch.org\/uploads\/default\/original\/2X\/2\/2a8294e9119e579306fea4eaea69782f4a6c76be.png\"\nimage.png920\u00d7301 12.3 KB with link \"https:\/\/discuss.pytorch.org\/uploads\/default\/original\/2X\/d\/dd984425bd94965f61dd2ade44f882db3ca37a66.png\"\nThe gradient of att_a and query_vec is always zero. while others such as bias_d is ok.\nSo What may cause that problem?\nThanks","y":"Good that you found it !\nThe answer is that a softmax with a single output is a constant function that outputs 1 all the time. And the gradient of a constant function is 0.","z":"Hi,\nYou code sample does not show how bias_d is used so not sure how to answer.\nThat being said, in what you showed, if alpha is 0, then both these gradients will be 0.\nI guess a good way to debug this is to print the different values during the forward to make sure they are what you expect.\nThanks for your reply. The code sample is a part of the whole model.  bias_d is just a bias in one fully connected layer.   And bias_d can be updated normally. But the two vectors att_a, query_vec can not be.\nalpha  is the attention weight , so it cannot be zero.   the other parameters in the model are updated normally except for the two att-a, query_vec.  It\u2019s very strange\u2026 \nHi, I found out the problem:joy:  the dim in  F.softmax() should be 0 rather than 1. After changing it to 0, the gradient is normal.\nBecause (bag_embs * self.att_a).mm(self.query_vec) get a Variable with size K * 1 and the softmax should be applied in the first dim.\nIf F.softmax(..., 1), then the values in alpha are all 1.\nBTW, why  the gradient will become 0 when alpha is all 1.\nGood that you found it !\nThe answer is that a softmax with a single output is a constant function that outputs 1 all the time. And the gradient of a constant function is 0.\nYeah.\nThanks very much."},{"x":"I\u2019m pretty convinced this will be a silly question, and still I\u2019d ask it.\nLook at the example at http:\/\/pytorch.org\/tutorials\/beginner\/blitz\/autograd_tutorial.html\nHow the gradient out with respect to x is calculated is pretty clear and makes sense according to the chain rule.\nWhat I don\u2019t understand is why the other leaves have no gradient. E.g., if you print(y.grad) it equals to None.\nWhy\u2019s that? After all, y  (or z) is another variable (in fact, Variable) involved in the chain rule\u2026","y":"y and z aren\u2019t leaf nodes because their computation depends on x so their gradients aren\u2019t saved.\nIf you want to see their gradients I think you can register a backward() hook: http:\/\/pytorch.org\/docs\/master\/autograd.html#torch.autograd.Variable.register_hook","z":"y and z aren\u2019t leaf nodes because their computation depends on x so their gradients aren\u2019t saved.\nIf you want to see their gradients I think you can register a backward() hook: http:\/\/pytorch.org\/docs\/master\/autograd.html#torch.autograd.Variable.register_hook\nThank you, this answers my question.\nIf I didn\u2019t misunderstand you, for backpropagation purposes the graph is viewed with the output (loss) as its root, and the input layer as composed by leaf nodes.\nThe grad with respect to intermediate nodes has to be computed (otherwise one cannot leverage the chain rule), but once the beckward pass finishes, you just need the gradient with respect to leaf nodes in order to update the weights. So it doesn\u2019t save intermediate steps.\nAm I right?\n yes you are right.\nThanks, both of you have been very kind."},{"x":"Hello.\nI am creating an architecture which uses two networks(N1, N2).\nN1 is already trained, so I want to use the output of N1 as input of N2.\nHowever, I don\u2019t want to update the parameters on N1 in the process.(I want to train only N2)\nHow could it be possible?","y":"You can just give the parameters of N2 to the optimizer that you choose.\nThe code can look like\n<code class=\"lang-auto\">optimizer = torch.optim.Adam(N2.parameters(), blah blah)\ninput = Variable(sth)\nN1_out = N1(input)\nN2_out = N2(N1_out)\nloss = compute_loss(N2_out)\n...\n<\/code>","z":"You can just give the parameters of N2 to the optimizer that you choose.\nThe code can look like\n<code class=\"lang-auto\">optimizer = torch.optim.Adam(N2.parameters(), blah blah)\ninput = Variable(sth)\nN1_out = N1(input)\nN2_out = N2(N1_out)\nloss = compute_loss(N2_out)\n...\n<\/code>"},{"x":"Hi, I read PyTorch with Example with link \"http:\/\/pytorch.org\/tutorials\/beginner\/pytorch_with_examples.html\" and I tried to implement network with learned parameters and custom loss function. Following is abstract of what I did:\n\nLoaded learned model of \u2018model.pth\u2019 which was converted from \u2018.t7\u2019 by using convert_torch_to_pytorch with link \"https:\/\/github.com\/clcarwin\/convert_torch_to_pytorch\"\n\nDefined custom loss function of triplet_loss() which is unsure of working properly\n\nBelow is the code:\n<code class=\"lang-python\"># define custom loss function\n\nfrom torch import exp\nfrom torch.autograd import Variable\n\nclass LambdaBase(nn.Sequential):\n    def __init__(self, fn, *args):\n        super(LambdaBase, self).__init__(*args)\n        self.lambda_func = fn\n\n    def forward_prepare(self, input):\n        output = []\n        for module in self._modules.values():\n            output.append(module(input))\n        return output if output else input\n\nclass Lambda(LambdaBase):\n    def forward(self, input):\n        return self.lambda_func(self.forward_prepare(input))\n\nmodel = nn.Sequential( # Sequential,\n\tnn.Conv2d(3,64,(3, 3),(1, 1),(1, 1)),\n\tnn.ReLU(),\n\tnn.Conv2d(64,64,(3, 3),(1, 1),(1, 1)),\n\tnn.ReLU(),\n\tnn.Dropout(0.25),\n\tnn.MaxPool2d((4, 4),(4, 4)),\n\tnn.BatchNorm2d(64,0.001,0.9,True),\n\tnn.Conv2d(64,128,(3, 3),(1, 1),(1, 1)),\n\tnn.ReLU(),\n\tnn.Conv2d(128,128,(3, 3),(1, 1),(1, 1)),\n\tnn.ReLU(),\n\tnn.Dropout(0.25),\n\tnn.MaxPool2d((4, 4),(4, 4)),\n\tnn.BatchNorm2d(128,0.001,0.9,True),\n\tnn.Conv2d(128,256,(3, 3),(1, 1),(1, 1)),\n\tnn.ReLU(),\n\tnn.Conv2d(256,256,(3, 3),(1, 1),(1, 1)),\n\tnn.ReLU(),\n\tnn.Dropout(0.25),\n\tnn.MaxPool2d((4, 4),(4, 4)),\n\tnn.BatchNorm2d(256,0.001,0.9,True),\n\tnn.Conv2d(256,128,(1, 1)),\n\tnn.ReLU(),\n\tLambda(lambda x: x.view(x.size(0),-1)), # Reshape,\n\tnn.Sequential(Lambda(lambda x: x.view(1,-1) if 1==len(x.size()) else x ),nn.Linear(3072,128)), # Linear,\n)\n\nmodel.load_state_dict(torch.load('model.pth'))\n\ndef triplet_loss(vec_ref, vec_pos, vec_neg):\n    \n    dist_pos = torch.norm(vec_ref - vec_pos, 2)\n    dist_neg = torch.norm(vec_ref - vec_neg, 2)\n    \n    reg_dist_pos = exp(dist_pos)\/(exp(dist_pos) + exp(dist_neg))\n    reg_dist_neg = exp(dist_neg)\/(exp(dist_pos) + exp(dist_neg))\n    \n    loss = reg_dist_pos\n    \n    return loss\n\nlearning_rate = 1e-4\ntriplet_samples = 100\n\nfor t in range(triplet_samples):\n    # extract feature of t th image triplet\n    # img_***[t] is tensor of image\n    y_ref = model(img_ref[t])\n    y_pos = model(img_pos[t])\n    y_neg = model(img_var[t])\n    \n    loss = triplet_loss(y_ref, y_pos, y_neg)\n    \n    print(t, loss.data[0])\n    model.zero_grad()\n    \n    loss.backward()\n    \n    for param in model.parameters():\n        param.data -= learning_rate * param.grad.data\n<\/code>\nI tried this code on one image triplet and worked and  computed loss. But I\u2019m not sure whether the loss.backward() properly works. Could you explain it is properly implemented or not?\nThank you.","y":"You are correct. The dropout layer randomly affects the output during training. After training just do\n<code class=\"lang-auto\">model.train(False) \n# or equivalently\nmodel.eval()\n\n# dropout is now switched off\n\nstable_prediction = model(input)\n<\/code>","z":"PyTorch is pretty good at automatically differentiating stuff. If loss.backward() isn\u2019t throwing any errors then chances are it is working properly.\nI notice that triplet_loss calculates reg_dist_neg but never uses it. Is that intentional?\n Thank you for reply.\n\ntriplet_loss calculates reg_dist_neg but never uses it. Is that intentional?\n\nYes, It\u2019s intentional. I\u2019ll use value of  reg_dist_neg after addtional modification of triplet_loss.\nNow I have another question that:\nWhen I forward() same image Tensor two times, I\u2019ll get two different output. Is this because of the dropout layer?\nWhen I want to use this network as feature extractor, what should I do?\nYou are correct. The dropout layer randomly affects the output during training. After training just do\n<code class=\"lang-auto\">model.train(False) \n# or equivalently\nmodel.eval()\n\n# dropout is now switched off\n\nstable_prediction = model(input)\n<\/code>\nI really thank you!\nI could extract specific value for one image after applied your suggestion."},{"x":"I am working on creating custom loss function in pytorch, below the code:\n<code class=\"lang-auto\">class CustomSoftmaxLoss(torch.nn.Module):\n    def __init__(self):\n        super(CustomSoftmaxLoss, self).__init__()\n\n    def forward(self, predictions, targets):\n        sample_size = predictions.size()[0]\n\n        # Fix numerical instability\n        predictions -= predictions.max(dim=1)[0].view(-1, 1)\n\n        # Run predicitions through softmax\n        softmax = torch.exp(predictions.float())\n        softmax = softmax \/ softmax.sum(dim=1).view(-1, 1)\n\n        # Cross-entropy loss\n        loss = -torch.log(softmax[torch.arange(sample_size).type(torch.LongTensor), targets]).sum()\n        loss \/= sample_size\n\n        return loss\n\n# Test\npredictions = torch.from_numpy(np.array([[1,2,3], [3,15,7], [10,4,2], [2,3,4]]))\ntargets = torch.from_numpy(np.array([0,1,2,2]))\n\npredictions = Variable(predictions)\ntargets = Variable(targets)                          \n\ncriterion = CustomSoftmaxLoss()\na         = criterion(predictions, targets)\nprint(a)\n<\/code>\nThe code above is working. However, if I enable CUDA like below, there is error\n<code class=\"lang-auto\">criterion = CustomSoftmaxLoss().cuda()\na         = criterion(predictions.cuda(), targets.cuda())\n<\/code>\nTypeError: Performing basic indexing on a tensor and encountered an error indexing dim 0 with an object of type torch.LongTensor. The only supported types are integers, slices, numpy scalars, or if indexing with a torch.cuda.LongTensor or torch.cuda.ByteTensor only a single Tensor may be passed.\nHow to fix this please?\nThanks.","y":"You need cuda.LongTensor to index a cuda tensor. You should probably change torch.arange(sample_size).type(torch.LongTensor) to torch.arange(sample_size).type_as(target.data).","z":"You need cuda.LongTensor to index a cuda tensor. You should probably change torch.arange(sample_size).type(torch.LongTensor) to torch.arange(sample_size).type_as(target.data)."},{"x":"Hi all,\nI\u2019m a newbie of pytorch and I like it a lot! But I have a problem with gradient of re-used variable\nhere is my code:\nimport torch as t\nfrom torch.autograd import Variable as v\nstep =2\nx0 = t.FloatTensor([2, 1]).view(1, 2)\nx0 = v(x0, requires_grad=True)\nfor _ in range(step):\nx2 = t.FloatTensor([[1, 2], [1, 2]])\nx2 = v(x2, requires_grad=True)\nx0 = t.mm with link \"http:\/\/t.mm\"(x0, x2)\ny = v(t.FloatTensor([[1, 2], [3, 4]]))\nz = t.mm with link \"http:\/\/t.mm\"(x0, y)\nz.backward(t.FloatTensor([[1, 1]]))\nprint(x2.grad)\nprint(x0.grad)\nis gradient of x2 accumulated by every step? why gradient of x0 is None? and how can we get gradient of x0 for every step? Thanks a lot!","y":"because you redefined x0. the second x0 is not really the x0 you defined the first time. it is not really reusing thje same variable, but redefining a symbol to be a different variable. your new variable here is an intermediate results, which do not retain gradients by default. you can call retain_grad() on it to get grad http:\/\/pytorch.org\/docs\/master\/autograd.html#torch.autograd.Variable.retain_grad.","z":"because you redefined x0. the second x0 is not really the x0 you defined the first time. it is not really reusing thje same variable, but redefining a symbol to be a different variable. your new variable here is an intermediate results, which do not retain gradients by default. you can call retain_grad() on it to get grad http:\/\/pytorch.org\/docs\/master\/autograd.html#torch.autograd.Variable.retain_grad.\nHi SimonW,\nThanks for your reply! Now I know how to obtain gradient for every step.\nBut I\u2019m still wondering, after I changed x0 for every step, when I do backward and apply gradient to variables by step(), still the original x0 (first defined x0) will be updated but just I can\u2019t print out its gradient, right?\nThat\u2019s right. Just think of them as regular Python objects. The symbol name is just a pointer.\nYou solve my confusion, thanks a lot!"},{"x":"I think it is sometimes puzzling to transform between these following data type:\n\nTensor\nVariable\ncuda Variable\n\nSome operations such as torch.index_select does not support Variable, while I think it is better to support them both.\nSeems that the key difference between a Tensor and a Variable is whether to keep the gradient history, however some operations do not need that in most cases.","y":"Good point. And we are in progress of merging Variable and Tensor classes  So in future everything will work on everything (mostly).","z":"Good point. And we are in progress of merging Variable and Tensor classes  So in future everything will work on everything (mostly)."},{"x":"Hi All! I am trying to write a new loss function, which takes infinity norm of the weights.\nI am trying some thing on this line.\n<code class=\"lang-auto\">def l2_regu(mdl):\n        l2_reg = None\n        for W in mdl.parameters():\n                if W.ndimension() < 2:\n                        continue\n                else:\n                        w_tmp = W\n                        if l2_reg is None:\n                                l2_reg = (torch.max(torch.abs(w_tmp)))**2\n                        else:   \n                                l2_reg = l2_reg + (torch.max(torch.abs(w_tmp)))**2\n      \n        return l2_reg\n<\/code>\n<code class=\"lang-auto\">Running this throws an error  saying:\nFile \"train.py\", line 208, in train\n    oloss =  l2_regu(model)\n  File \"train.py\", line 38, in l2_reg_ortho\n    l2_reg = l2_reg + (torch.max(torch.abs(w_tmp)))**2\nRuntimeError: cuda runtime error (2) : out of memory at \/pytorch\/torch\/lib\/THC\/generic\/THCStorage.cu:58\n(torch3) bansa01:~\/pytorch_wideres\/tmp_inf\/WideResNet-pytorch$\n<\/code>\nEvery thing works good if I remove\/replace with l2 loss, but with infinity norm it throws the above error.\nRegards,\nNitin","y":"My guess is that reading the source code for torch.norm might be worthwhile.","z":"Hi,\nThe error is that you run out of memory.\nI guess your script when using the l2 norm is using almost all the memory.\nYour implementation has intermediary results (and so uses a bit more memory).\nI guess reducing your batch size should reduce the memory footprint and make it run nicely.\nThanks !\nThe issue is bit weird, It is actually working fine If I simply replace the \u2018Infinity\u2019 to just calculate the l2 norm, without any  issue. Also I reduced the batch size to as low as 4. But still the same error persists. Do you think this could be due to some other reason?\nHi,\nFrom your code, there is no \u201cinfinity\u201d, you do this norm by doing torch.max(torch.abs(w_tmp))**2. If you were doing w_tmp.norm(2) then one is doing one operation while the other is doing 3.\nWhat is the memory usage for the corresponding code that uses the l2 regularization?\nIf you could provide a small code snippet that reproduce the issue, that could be helpful.\nHello Alan, Somehow the Norm (Infinity) is always returning 1. and giving in-correct value. My code is as pointed out earlier this.\n<code class=\"lang-auto\">def l2_regu(mdl):\n        l2_reg = None\n        for W in mdl.parameters():\n                if W.ndimension() < 2:\n                        continue\n                else:\n                        w_tmp = W\n                        if l2_reg is None:\n                                l2_reg = (torch.max(torch.abs(w_tmp)))**2\n                        else:   \n                                l2_reg = l2_reg + (torch.max(torch.abs(w_tmp)))**2\n      \n        return l2_reg\n<\/code>\nEven though I kept my batch size as 4(very small), within an Epoch, I saw through nvidia-smi command the memory usage went from 100 MB to 12GB, throwing the CUDA MEMORY ERROR. If I simply replace that (torch.max(torch.abs(w_tmp)))**2 with l2 norm every thing works perfectly. I understand that there are 3 operations here as contrast to 1 operation in l2 norm, But I am getting memory error within an Epoch that too  for a very small batch size.\nLet\u2019s experiment a bit\u2026\nGiven a=torch.randn(5,5)\nThen a.norm(p=np.inf) always returns 1.0\nHowever torch.max(torch.abs(a)) correctly returns the abs value of the element with the maximum absolute value.\nSo either my understanding of the infinity norm is wrong, or torch.norm(p=np.inf) is doing something strange.\nMoreover as noted in another thread torch.norm(p=np.inf) throws an error when applied to a Variable.\n Yeah, So If we apply norm to a tensor it returns 1, if applied to a variable returns an error, and when I replace it by torch.max(torch.abs(a)), it return correct valueif written in an standalone fashion, but when I embed it to a running code , it throws the CUDA MEMORY ERROR(Which is validated by the output of nvidia-smi, which shows the memory usage exploding!). where as the same function if I use l2 norm, every thing works fine.\nSo My conclusion is  somehow torch.max(torch.abs(a)) is blowing up things, but why and how should I fix it\u2026 I am not able to understand. My  aim is to add a regularizer which helps in getting a l-infinity norm.\nMy guess is that reading the source code for torch.norm might be worthwhile."},{"x":"Hello ALL,\nI am implementing WideResnet in pytorch, which has perfectly worked with l2-norm, with a batchsize of 64 on 2 GPUs(1080 Ti). But when I replace it with a inifinity norm defined by (torch.max(torch.abs(w_tmp)))**2, it gives an CUDA Memory error within a epoch of training,where memory usage directly shoots up from 100MB to 12GB, with a very batch size of even 4. Function used for regularization is:\n<code class=\"lang-auto\">def l2_regu(mdl):\n        l2_reg = None\n        for W in mdl.parameters():\n                if W.ndimension() < 2:\n                        continue\n                else:\n                        w_tmp = W\n                        if l2_reg is None:\n                                l2_reg = (torch.max(torch.abs(w_tmp)))**2\n                        else:   \n                                l2_reg = l2_reg + (torch.max(torch.abs(w_tmp)))**2\n      \n        return l2_reg\n<\/code>\nIf I simply replaced the line to l2_reg = w_tmp.norm(2)**2 everythigng works fine. Also I am using torch.max(torch.abs(w_tmp)))**2 for infinity norm because it always return 1 or an error while used with a variable.\nRegards,\nNitin","y":"Could you avoid doing multiple posts for the same thing please?","z":"Could you avoid doing multiple posts for the same thing please?"},{"x":"Hi all,\nI\u2019m very new to PyTorch but I\u2019m trying to implement a network which is a bit tricky.\nConcretely speaking, the network has these four particularities:\nFirst, it consists of several similar units, such that each unit takes the input and provides two outputs: one to be fed to the next unit and the other one directly fed to the final loss-function of the network. This means that I cannot use the standard nn.Sequntial to define my network architecture.\nSecond, it has variable number of units. So I want to construct the network in a for-loop somewhere.\nThird, each unit consists of operations not in the nn class, like e.g., nn.Linear or nn.Relu. So I have to define also the units myself.\nForth, I need to initialize my parameters from some other functions. So all the initial parameters of all the  units are gathered in e.g. a list and fed to the network in one place.\nSuppose I want to use torch.optim. So I should find a good way to declare my gradient-requiring variables to it. But this is where things are becoming problematic for me.\nThe way I see this network to be implemented is to define one class for the units that inherits from nn.Module and one class for the the whole network that connects the units together.\nMy current code looks like this:\n<code class=\"lang-auto\">import torch\nfrom torch.nn.parameter import Parameter\nfrom torch.autograd import Variable\nfrom torch import nn\n#####\n\nmyParametersList = [torch.randn(2,2),\n                    torch.randn(2,2),\n                    torch.randn(2,2)]\ninput = Variable(torch.randn(2,2))\n########\nclass myUnit(nn.Module):\n    \"\"\"\n    Defines a generic unit of the network\n    \"\"\"\n    def __init__(self,myParameter):\n        super(myUnit, self).__init__()\n        self.myParameter = Parameter(myParameter,requires_grad=True)\n    def forward(self,input):\n        \"\"\"\n        Whatever operation. Just an example.\n        \"\"\"\n        output_1 = self.myParameter * input - 1\n        output_2 = output_1 - output_1.mean()\n        return output_1,output_2\n#######\nclass myNetwork(nn.Module):\n    \"\"\"\n    Uses myUnit class to build-up the network.\n    \"\"\"\n    def __init__(self,myParametersList,numUnits):\n        super(myNetwork, self).__init__()\n        self.myParametersList = myParametersList\n        self.numUnits = numUnits\n        assert numUnits == len(myParametersList)\n    def forward(self,input):\n        output_final = Variable(torch.zeros(2,2))\n        for u in range(self.numUnits):\n            myParameter = self.myParametersList[u]\n            unitObj = myUnit(myParameter)\n            output_1, output_2 = unitObj.forward(input)\n            input = output_1.clone()  # to be fed to the next unit\n            output_final.add_(output_2)\n\n            return output_final\n\n##################\n#myModel = myUnit(myParametersList[0])\nmyModel = myNetwork(myParametersList,3)\nmyModel.forward(input) # I need this to create the list of my parameters.\n\noptimizer = torch.optim.Adam(myModel.parameters(), lr=1e-2)\n\n\nfor t in range(50):\n    output_final = myModel.forward(input)[0]\n\n    loss = (input - output_final).pow(2).mean()\n    print(t, loss.data[0])\n\n    optimizer.zero_grad()\n\n    loss.backward()\n    optimizer.step<\/code>\nBut I get this error about the list of parameters.\nValueError: optimizer got an empty parameter list\nHowever, when I build the model only from one single myUnit, I don\u2019t get this error anymore.\nI am also aware of this post with link \"https:\/\/discuss.pytorch.org\/t\/error-optimizer-got-an-empty-parameter-list\/1501\/4\", but that didn\u2019t help me.\nAny thoughts? I\u2019d appreciate a lot!","y":"The problem is that the params contained in self.myParametersList are not recognised as being params of myNetwork. There are several possible solutions\u2026\nThe nicest solution is to have myNetwork.__init__ initialise the list of myUnit submodules.\nThis is the cleanest solution and, I imagine, the one that most people would prefer.\nThis way the myNetwork instance has a list of registered submodules thanks to nn.ModuleList and each myUnit instance has its own registered parameter tensor, and PyTorch automatically figures out what parameters need updating.\n<code class=\"lang-auto\">class myUnit(nn.Module):\n    def __init__(self):\n        super(myUnit, self).__init__()\n        self.myParameter = Parameter(torch.randn(2,2), requires_grad=True)\n\n    def forward(self, input):\n        # same as your code\n\nclass myNetwork(nn.Module):\n    def __init__(self, numUnits):\n        super(myNetwork, self).__init__()\n        self.mySubmodulesList = nn.ModuleList([myUnit() for _ in range(numUnits)])\n\n    def forward(self, input):\n        output_final = Variable(torch.zeros(2,2))\n        for u in range(len(self.mySubmodulesList)):\n            output_1, output_2 = self.mySubmodulesList[u](input)\n            input = output_1 # cloning output_1 seems unnecessary as output_1 is never used elsewhere.\n            output_final = output_final + output_2 # I am not certain that an inplace add_ will not cause errors during the backward pass\n            return output_final\n<\/code>\nA hackier solution would involve looping through the list of parameters adding each one explicitly using register_parameter with link \"http:\/\/pytorch.org\/docs\/0.3.0\/nn.html#torch.nn.Module.register_parameter\".\n<code class=\"lang-auto\">self.myParametersList = myParametersList\nfor i, myParam in enumerate(self.myParametersList):\n    self.register_parameter(\"myParam\"+str(i), myParam)\n<\/code>","z":"The problem is that the params contained in self.myParametersList are not recognised as being params of myNetwork. There are several possible solutions\u2026\nThe nicest solution is to have myNetwork.__init__ initialise the list of myUnit submodules.\nThis is the cleanest solution and, I imagine, the one that most people would prefer.\nThis way the myNetwork instance has a list of registered submodules thanks to nn.ModuleList and each myUnit instance has its own registered parameter tensor, and PyTorch automatically figures out what parameters need updating.\n<code class=\"lang-auto\">class myUnit(nn.Module):\n    def __init__(self):\n        super(myUnit, self).__init__()\n        self.myParameter = Parameter(torch.randn(2,2), requires_grad=True)\n\n    def forward(self, input):\n        # same as your code\n\nclass myNetwork(nn.Module):\n    def __init__(self, numUnits):\n        super(myNetwork, self).__init__()\n        self.mySubmodulesList = nn.ModuleList([myUnit() for _ in range(numUnits)])\n\n    def forward(self, input):\n        output_final = Variable(torch.zeros(2,2))\n        for u in range(len(self.mySubmodulesList)):\n            output_1, output_2 = self.mySubmodulesList[u](input)\n            input = output_1 # cloning output_1 seems unnecessary as output_1 is never used elsewhere.\n            output_final = output_final + output_2 # I am not certain that an inplace add_ will not cause errors during the backward pass\n            return output_final\n<\/code>\nA hackier solution would involve looping through the list of parameters adding each one explicitly using register_parameter with link \"http:\/\/pytorch.org\/docs\/0.3.0\/nn.html#torch.nn.Module.register_parameter\".\n<code class=\"lang-auto\">self.myParametersList = myParametersList\nfor i, myParam in enumerate(self.myParametersList):\n    self.register_parameter(\"myParam\"+str(i), myParam)\n<\/code>\n\nProblem solved! Thanks very much!\nI used the first solution that you proposed where this np.ModuleList does the trick."},{"x":"Hello There,\nI have got two networks. The first one is a network initialized with a pre-trained model plus some extra layers defined by me and the second one is the same network but trained for one epoch. It is worthwhile to mention that the pre-trained section of both networks was frozen in order to getting no parameter updating and my optimizer just trains the extra layers. For being sure about no parameter updating on my pre-trained section of both network, I have compared the parameters of them using the following code.\n<code class=\"lang-auto\">model0 = Model(model_path, 425)\nmodel0 = nn.DataParallel(model0)\nmodel0.load_state_dict(torch.load(ckpt0))\nmodel0.train(True)\n\nmodel1 = Model(model_path, 425)\nmodel1 = nn.DataParallel(model1)\nmodel1.train(True)\n\nmodel0_name = [] # layers' name\nfor name, param in model0.named_parameters():\n    model0_name.append(name)\n\nmodel1_name = [] # layers' name\nfor name, param in model1.named_parameters():\n    model1_name.append(name)\n\ndiff = []\nfor p1, p2 in zip(model0.parameters(), model1.parameters()):\n\tdiff.append((p1.data - p2.data).sum())\n\nfor n in zip(model0_name, model1_name, diff):\n\tprint(n[0], n[1], \"{0:.8f}\".format(n[2]))\n<\/code>\nand here is the output:\n<code class=\"lang-auto\">module.stage1.0.weight module.stage1.0.weight 0.00000000\nmodule.stage1.1.weight module.stage1.1.weight 0.00000000\nmodule.stage1.1.bias module.stage1.1.bias 0.00000000\nmodule.stage1.4.weight module.stage1.4.weight 0.00000000\nmodule.stage1.5.weight module.stage1.5.weight 0.00000000\nmodule.stage1.5.bias module.stage1.5.bias 0.00000000\nmodule.stage1.8.weight module.stage1.8.weight 0.00000000\nmodule.stage1.9.weight module.stage1.9.weight 0.00000000\nmodule.stage1.9.bias module.stage1.9.bias 0.00000000\nmodule.stage1.11.weight module.stage1.11.weight 0.00000000\nmodule.stage1.12.weight module.stage1.12.weight 0.00000000\nmodule.stage1.12.bias module.stage1.12.bias 0.00000000\nmodule.stage1.14.weight module.stage1.14.weight 0.00000000\nmodule.stage1.15.weight module.stage1.15.weight 0.00000000\nmodule.stage1.15.bias module.stage1.15.bias 0.00000000\nmodule.stage4.0.weight module.stage4.0.weight 0.00000000\nmodule.stage4.1.weight module.stage4.1.weight 0.00000000\nmodule.stage4.1.bias module.stage4.1.bias 0.00000000\nmodule.stage4.3.weight module.stage4.3.weight 0.00000000\nmodule.stage4.4.weight module.stage4.4.weight 0.00000000\nmodule.stage4.4.bias module.stage4.4.bias 0.00000000\nmodule.stage4.6.weight module.stage4.6.weight 0.00000000\nmodule.stage4.7.weight module.stage4.7.weight 0.00000000\nmodule.stage4.7.bias module.stage4.7.bias 0.00000000\nmodule.stage5.0.weight module.stage5.0.weight 0.00000000\nmodule.stage5.1.weight module.stage5.1.weight 0.00000000\nmodule.stage5.1.bias module.stage5.1.bias 0.00000000\nmodule.stage5.3.weight module.stage5.3.weight 0.00000000\nmodule.stage5.4.weight module.stage5.4.weight 0.00000000\nmodule.stage5.4.bias module.stage5.4.bias 0.00000000\nmodule.stage5.6.weight module.stage5.6.weight 0.00000000\nmodule.stage5.7.weight module.stage5.7.weight 0.00000000\nmodule.stage5.7.bias module.stage5.7.bias 0.00000000\nmodule.stage5.9.weight module.stage5.9.weight 0.00000000\nmodule.stage5.10.weight module.stage5.10.weight 0.00000000\nmodule.stage5.10.bias module.stage5.10.bias 0.00000000\nmodule.stage5.12.weight module.stage5.12.weight 0.00000000\nmodule.stage5.13.weight module.stage5.13.weight 0.00000000\nmodule.stage5.13.bias module.stage5.13.bias 0.00000000\nmodule.parallel1.0.weight module.parallel1.0.weight 0.00000000\nmodule.parallel1.1.weight module.parallel1.1.weight 0.00000000\nmodule.parallel1.1.bias module.parallel1.1.bias 0.00000000\nmodule.parallel1.3.weight module.parallel1.3.weight 0.00000000\nmodule.parallel1.4.weight module.parallel1.4.weight 0.00000000\nmodule.parallel1.4.bias module.parallel1.4.bias 0.00000000\nmodule.parallel1.6.weight module.parallel1.6.weight 0.00000000\nmodule.parallel1.7.weight module.parallel1.7.weight 0.00000000\nmodule.parallel1.7.bias module.parallel1.7.bias 0.00000000\nmodule.parallel1.9.weight module.parallel1.9.weight 0.00000000\nmodule.parallel1.10.weight module.parallel1.10.weight 0.00000000\nmodule.parallel1.10.bias module.parallel1.10.bias 0.00000000\nmodule.parallel1.12.weight module.parallel1.12.weight 0.00000000\nmodule.parallel1.13.weight module.parallel1.13.weight 0.00000000\nmodule.parallel1.13.bias module.parallel1.13.bias 0.00000000\nmodule.parallel1.15.weight module.parallel1.15.weight 0.00000000\nmodule.parallel1.16.weight module.parallel1.16.weight 0.00000000\nmodule.parallel1.16.bias module.parallel1.16.bias 0.00000000\nmodule.parallel1.18.weight module.parallel1.18.weight 0.00000000\nmodule.parallel1.19.weight module.parallel1.19.weight 0.00000000\nmodule.parallel1.19.bias module.parallel1.19.bias 0.00000000\nmodule.parallel2.0.weight module.parallel2.0.weight 0.00000000\nmodule.parallel2.1.weight module.parallel2.1.weight 0.00000000\nmodule.parallel2.1.bias module.parallel2.1.bias 0.00000000\nmodule.stage7.0.weight module.stage7.0.weight 0.00000000\nmodule.stage7.1.weight module.stage7.1.weight 0.00000000\nmodule.stage7.1.bias module.stage7.1.bias 0.00000000\nmodule.final.0.weight module.final.0.weight 0.00000000\nmodule.extra1.conv_l1.weight module.extra1.conv_l1.weight 1.50025647\nmodule.extra1.conv_l1.bias module.extra1.conv_l1.bias -0.37570643\nmodule.extra1.conv_l2.weight module.extra1.conv_l2.weight 27.04075950\nmodule.extra1.conv_l2.bias module.extra1.conv_l2.bias 0.08830106\nmodule.extra1.conv_r1.weight module.extra1.conv_r1.weight 29.77446331\nmodule.extra1.conv_r1.bias module.extra1.conv_r1.bias -0.20370359\nmodule.extra1.conv_r2.weight module.extra1.conv_r2.weight -12.84376761\nmodule.extra1.conv_r2.bias module.extra1.conv_r2.bias -0.37359143\nmodule.extra2.conv_l1.weight module.extra2.conv_l1.weight -10.23451808\nmodule.extra2.conv_l1.bias module.extra2.conv_l1.bias 0.01333411\nmodule.extra2.conv_l2.weight module.extra2.conv_l2.weight 11.42386136\nmodule.extra2.conv_l2.bias module.extra2.conv_l2.bias 0.10939794\nmodule.extra2.conv_r1.weight module.extra2.conv_r1.weight 1.97030002\nmodule.extra2.conv_r1.bias module.extra2.conv_r1.bias 0.05921281\nmodule.extra2.conv_r2.weight module.extra2.conv_r2.weight 13.66498772\nmodule.extra2.conv_r2.bias module.extra2.conv_r2.bias 0.14407422\nmodule.extra3.conv_l1.weight module.extra3.conv_l1.weight 1.52511122\nmodule.extra3.conv_l1.bias module.extra3.conv_l1.bias 0.18012815\nmodule.extra3.conv_l2.weight module.extra3.conv_l2.weight 20.19549281\nmodule.extra3.conv_l2.bias module.extra3.conv_l2.bias -0.76473302\nmodule.extra3.conv_r1.weight module.extra3.conv_r1.weight -2.59647552\nmodule.extra3.conv_r1.bias module.extra3.conv_r1.bias 0.14506025\nmodule.extra3.conv_r2.weight module.extra3.conv_r2.weight 11.67924830\nmodule.extra3.conv_r2.bias module.extra3.conv_r2.bias -0.00651512\nmodule.extra4.conv_l1.weight module.extra4.conv_l1.weight 14.54665439\nmodule.extra4.conv_l1.bias module.extra4.conv_l1.bias 0.08106837\nmodule.extra4.conv_l2.weight module.extra4.conv_l2.weight 16.46649296\nmodule.extra4.conv_l2.bias module.extra4.conv_l2.bias -0.26476345\nmodule.extra4.conv_r1.weight module.extra4.conv_r1.weight -12.99556065\nmodule.extra4.conv_r1.bias module.extra4.conv_r1.bias -0.05485360\nmodule.extra4.conv_r2.weight module.extra4.conv_r2.weight -2.79881258\nmodule.extra4.conv_r2.bias module.extra4.conv_r2.bias -1.07567936\nmodule.extra_1.bn.weight module.extra_1.bn.weight -152.28462493\nmodule.extra_1.bn.bias module.extra_1.bn.bias 0.31378557\nmodule.extra_1.conv1.weight module.extra_1.conv1.weight 2.76232860\nmodule.extra_1.conv1.bias module.extra_1.conv1.bias -0.00553248\nmodule.extra_1.conv2.weight module.extra_1.conv2.weight -100.39555516\nmodule.extra_1.conv2.bias module.extra_1.conv2.bias 0.00963779\nmodule.extra_2.bn.weight module.extra_2.bn.weight -144.90659545\nmodule.extra_2.bn.bias module.extra_2.bn.bias -0.43241561\nmodule.extra_2.conv1.weight module.extra_2.conv1.weight -18.49401752\nmodule.extra_2.conv1.bias module.extra_2.conv1.bias -0.03962684\nmodule.extra_2.conv2.weight module.extra_2.conv2.weight -98.76576164\nmodule.extra_2.conv2.bias module.extra_2.conv2.bias -0.07895776\nmodule.extra_3.bn.weight module.extra_3.bn.weight -137.74657961\nmodule.extra_3.bn.bias module.extra_3.bn.bias -1.83718258\nmodule.extra_3.conv1.weight module.extra_3.conv1.weight -6.63687622\nmodule.extra_3.conv1.bias module.extra_3.conv1.bias 0.16047683\nmodule.extra_3.conv2.weight module.extra_3.conv2.weight -64.03853174\nmodule.extra_3.conv2.bias module.extra_3.conv2.bias 0.37029462\nmodule.extra_4.bn.weight module.extra_4.bn.weight -150.30557569\nmodule.extra_4.bn.bias module.extra_4.bn.bias -0.88545457\nmodule.extra_4.conv1.weight module.extra_4.conv1.weight 8.52840125\nmodule.extra_4.conv1.bias module.extra_4.conv1.bias -0.16135700\nmodule.extra_4.conv2.weight module.extra_4.conv2.weight 39.86314841\nmodule.extra_4.conv2.bias module.extra_4.conv2.bias -0.30344061\nmodule.extra_5.bn.weight module.extra_5.bn.weight -153.87934927\nmodule.extra_5.bn.bias module.extra_5.bn.bias -0.57383157\nmodule.extra_5.conv1.weight module.extra_5.conv1.weight -1.10513980\nmodule.extra_5.conv1.bias module.extra_5.conv1.bias -0.10425282\nmodule.extra_5.conv2.weight module.extra_5.conv2.weight 36.12376689\nmodule.extra_5.conv2.bias module.extra_5.conv2.bias -0.45356037\nmodule.extra_6.bn.weight module.extra_6.bn.weight -118.99042057\nmodule.extra_6.bn.bias module.extra_6.bn.bias -1.05029858\nmodule.extra_6.conv1.weight module.extra_6.conv1.weight 47.75907117\nmodule.extra_6.conv1.bias module.extra_6.conv1.bias -0.30105668\nmodule.extra_6.conv2.weight module.extra_6.conv2.weight 82.42883147\nmodule.extra_6.conv2.bias module.extra_6.conv2.bias 0.24271000\nmodule.extra_7.bn.weight module.extra_7.bn.weight -112.90572042\nmodule.extra_7.bn.bias module.extra_7.bn.bias 2.30864563\nmodule.extra_7.conv1.weight module.extra_7.conv1.weight 14.77395574\nmodule.extra_7.conv1.bias module.extra_7.conv1.bias 0.08763358\nmodule.extra_7.conv2.weight module.extra_7.conv2.weight 7.20600131\nmodule.extra_7.conv2.bias module.extra_7.conv2.bias -0.28086568\nmodule.d.0.weight module.d.0.weight -0.09255437\nmodule.d.1.weight module.d.1.weight 8.70911378\nmodule.d.1.bias module.d.1.bias 0.00000000\nmodule.d.3.weight module.d.3.weight 0.05706205\n<\/code>\nAs I have expected, the difference between these two models\u2019 parameters for pre-trained section is 0 and it is good. I have got another checking for being definitely sure about frozen part. I have forwarded a random input image in these models and get the output of the frozen part and compare them. These section again proved the above claim that the frozen part\u2019s parameters has not changed. But when I\u2019ve changed the networks state to the evaluation mode using model1.train(False) and model0.train(False), the output were different. I don\u2019t know what is the problem and how can I figure it out?\nOne thing to note is that my models do not have any Dorpout layers. But it has lots of Batch Normalization layers after each Conv layer. I also compared the running variance and running mean of same bn layers in these two models using following code and I found that they were equal:\n<code class=\"lang-auto\">model0_running_variance = []\nmodel0_running_mean = []\nfor module in model0.modules():\n    if isinstance(module, nn.modules.BatchNorm2d):\n    \tmodel0_running_mean.append(module.running_mean)\n    \tmodel0_running_variance.append(module.running_var)\n\nmodel1_running_variance = []\nmodel1_running_mean = []\nfor module in model1.modules():\n    if isinstance(module, nn.modules.BatchNorm2d):\n    \tmodel1_running_mean.append(module.running_mean)\n    \tmodel1_running_variance.append(module.running_var)\n\nprint(\"running_mean difference\")\nfor m1, m2 in zip(model0_running_mean, model1_running_mean):\n\tprint(\"{0:.8f}\".format((m1-m2).sum()))\n\nprint(\"running_var difference\")\nfor m1, m2 in zip(mode0_running_variance, model1_running_variance):\n\tprint(\"{0:.8f}\".format((m1-m2).sum()))\n<\/code>\nand the output:\n<code class=\"lang-auto\">0.00000000\n0.00000000\n0.00000000\n0.00000000\n0.00000000\n0.00000000\n0.00000000\n0.00000000\n0.00000000\n0.00000000\n0.00000000\n0.00000000\n0.00000000\n0.00000000\n0.00000000\n0.00000000\n0.00000000\n0.00000000\n0.00000000\n0.00000000\n0.00000000\n0.00000000\n0.00000000\n0.00000000\n0.00000000\n0.00000000\n0.00000000\n0.00000000\n0.00000000\n0.00000000\nrunning_var difference\n0.00000000\n0.00000000\n0.00000000\n0.00000000\n0.00000000\n0.00000000\n0.00000000\n0.00000000\n0.00000000\n0.00000000\n0.00000000\n0.00000000\n0.00000000\n0.00000000\n0.00000000\n0.00000000\n0.00000000\n0.00000000\n0.00000000\n0.00000000\n0.00000000\n0.00000000\n0.00000000\n0.00000000\n0.00000000\n0.00000000\n0.00000000\n0.00000000\n0.00000000\n0.00000000\n<\/code>\nBest.","y":"The problem is that your BN layers differ.\nI used the following code to solve the problem (just override the train function of your model):\n<code class=\"lang-auto\">    def train(self, mode=True, freeze_bn=False, freeze_bn_affine=False):\n        \n        super(MyModel, self).train(mode)\n        if freeze_bn:\n            for m in self.modules():\n                if isinstance(m, nn.BatchNorm2d):\n                    '''Freezing Mean\/Var of BatchNorm2D'''\n                    m.eval()\n                    if freeze_bn_affine:\n                        '''Freezing Weight\/Bias of BatchNorm2D'''\n                        m.weight.requires_grad = False\n                        m.bias.requires_grad = False\n<\/code>","z":", sorry man! Do you have any idea about my problem?\nLet me understand the issue properly.\nYou have two models, each with the same pre-trained base model.\nComparing the pre-trained section yields exactly the same results.\nMaybe a stupid question, but when you pass the random input, is it the same input for both networks?\nWere the models in training mode when you passed the inputs?\nAfter this test (which gives the same result for the random input), you switched both models to evaluation mode and the results differ, right?\nDid you compare the running_mean and running_var after the test failed already?\nHey, Thanks for your response.\n\nyes. it is same for both networks\nyes. both are in the train mode.\nyes. yes the difference is in the evaluation mode.\nyes.\n\n:frowning. Not know what is going on. please help.\nHmm, something is odd.\nWhen you pushed the random input into your models in training mode, the BatchNorm layers should update its running_mean and running_var.\nSince you used the same input, the updates should be the same in both \u201cbase\u201d models, which you made sure with the last test.\nThe only chance I see is to post the model definitions and debug it.\nCould you provide a minimal working example (most likely just the base model will be sufficient)?\nActually, it is an ongoing project in which I don\u2019t have such a permission to share some part of it. If it is possible, please tell me what do you want to know (intermediate output or whatever else), I will share with you! \nYeah it should update them, even if they are excluded from what optimizer can see. But it\u2019s worth mentioning that DataParallel wrapper doesn\u2019t properly update running statistics. https:\/\/github.com\/pytorch\/pytorch\/issues\/1051\nCould you tell us in detail how you froze the pretrained part?\nyes sure. Using these following functions, I set the requires_grad to False or True based on my pre-trained section.\n<code class=\"lang-auto\">def release_weight(model):\n    for param in model.parameters():\n        param.requires_grad = True\n    return model\ndef freeze_weight(model):\n    for param in model.stage1.parameters():\n        param.requires_grad = False\n    for param in model.stage4.parameters():\n        param.requires_grad = False\n    for param in model.stage5.parameters():\n        param.requires_grad = False       \n    for param in model.stage5_1.parameters():\n        param.requires_grad = False\n    for param in model.parallel1.parameters():\n        param.requires_grad = False\n    for param in model.parallel2.parameters():\n        param.requires_grad = False\n    for param in model.stage7.parameters():\n        param.requires_grad = False\n    for param in model.final.parameters():\n        param.requires_grad = False\n    for param in model.d.parameters():\n        param.requires_grad = False\n    return model\n<\/code>\nActually, when I instantiate the model, I call above two functions:\n<code class=\"lang-auto\">model = Model(pretrained_path, 425)\nmodel = release_weight(model)\nmodel = freeze_weight(model)\n<\/code>\nThen I select the parameters which has requires_gard=True using following commands and give them to optimizer:\n<code class=\"lang-auto\">parameters = itertools.filterfalse(lambda p: not p.requires_grad, model.parameters())\noptimizer = optim.SGD(parameters, lr=baseLR, weight_decay=0.001, momentum=0.9)\n<\/code>\nI have the same problem. I\u2019m not sure if it is for using DataParalell or what\n That\u2019s good to know! Thanks for the link!\n Ok, I understand. Do you see the same error when you load both models from the checkpoints, set them to evaluation mode and pass a random sample through them?\nDo you use any \u201cspecial\u201d layers, e.g. with conditions etc.?\n Are you able to share a small code snippet reproducing this issue?\nWow! It\u2019s amazing!\nTest your saving procedure and load your parameter again.  I have some issue with the similar problem but it solved by checking save and load procedure.\nAlso, your definition of new layers would cause this problem.\nI hope it gets resolved soon\u2026\nLet me give you all of my possible permutation of networks. \nRule1: pre-trained net: is a network without any training, just loaded the the pre-trained weights and initialized the extra layers.\nRule2: checkpoint: is a network initialized by the pre-trained weight and learnt on my dataset (no parameter updating on my pre-trained section - frozen part).\n\npre-trained and pre-trained in training mode: the outputs are equal.\npre-trained and pre-trained in evaluation mode: the outputs are equal.\npre-trained and checkpoint0 in training mode: the outputs are equal.\npre-trained and checkpoint0 in evaluation mode: the outputs are not equal.\ncheckpoint0 and checkpoint1 in training mode: the outputs are equal.\ncheckpoint0 and checkpoint1 in evaluation mode: the outputs are equal.\n\nI did not have any special layer in my net. All of them are the regular layers such as Conv2d, Maxpool, BatchNorm and nothing else.\nHello , , , . I have prepared two scripts. one of them is used for training and the other for comparing the output. Here is my main file to overfit on a sample based on resnet18 pre-trained weight.\n<code class=\"lang-auto\">import torch\nimport torchvision.models as models\nimport torch.nn as nn\nfrom torch.autograd import Variable as V\nimport torch.optim as optim\n\n\nclass MyModel(nn.Module):\n\t\"\"\"docstring for MyModel\"\"\"\n\tdef __init__(self, resnet18):\n\t\tsuper(MyModel, self).__init__()\n\t\tself.features = nn.Sequential(*list(resnet18.children())[:-1])\n\t\tself.f1 = nn.Linear(2048, 4)\n\n\tdef forward(self, input):\n\t\tout = self.features(input).detach()\n\t\treturn self.f1(out.view(out.size(0),-1))\n\t\t\ndef release_weight(model):\n    for param in model.parameters():\n        param.requires_grad = True\n    return model\n\ndef freeze_weight(model):\n    for param in model.features.parameters():\n        param.requires_grad = False\n    return model    \n\nresnet18 = models.resnet18(pretrained = True)\nmodel = MyModel(resnet18)\nmodel = release_weight(model)\nmodel = freeze_weight(model)\n\nparameters = filter(lambda p: p.requires_grad, model.parameters())\noptimizer = optim.SGD(parameters, lr=1e-3, weight_decay=0.001, momentum=0.9)\n\ninp = V(torch.randn(1,3,256,256), requires_grad = False)\ntarget = V(torch.randn(1,4))\n\nfor i in range(2):\n\tfor j in range(100):\n\t\toptimizer.zero_grad()\n\t\tout = model(inp)\n\t\tloss = ((out - target) ** 2).mean()\n\t\tloss.backward()\n\t\toptimizer.step()\n\t\tprint(loss.data[0])\n\ttorch.save(model.state_dict(), '.\/simpleModel{}.pth'.format(i))\n<\/code>\nand then here is the comparing code:\n<code class=\"lang-auto\">import torch\nimport torchvision.models as models\nimport torch.nn as nn\nfrom torch.autograd import Variable as V\n\nclass MyModel(nn.Module):\n    \"\"\"docstring for MyModel\"\"\"\n    def __init__(self, resnet18):\n        super(MyModel, self).__init__()\n        self.features = nn.Sequential(*list(resnet18.children())[:-1])\n        self.f1 = nn.Linear(2048, 4)\n\n    def forward(self, input):\n        out1 = self.features(input)\n        return out1, self.f1(out1.view(out1.size(0),-1))\n\nckpt0 = 'simpleModel0.pth'\nckpt1 = 'simpleModel1.pth'\n\nresnet18 = models.resnet18(pretrained = True)\nmodel0 = MyModel(resnet18)\nmodel0.load_state_dict(torch.load(ckpt0), strict=True)\nmodel0.train(False)\n\n\nresnet18 = models.resnet18(pretrained = True)\nmodel1 = MyModel(resnet18)\n# model1.load_state_dict(torch.load(ckpt1))\nmodel1.train(False)\n\nmodel0_name = []\nfor name, param in model0.named_parameters():\n    model0_name.append(name)\n\nmodel1_name = []\nfor name, param in model1.named_parameters():\n\tmodel1_name.append(name)\n\ndiff = []\nfor p1, p2 in zip(model0.parameters(), model1.parameters()):\n\tdiff.append((p1.data - p2.data).sum())\n\nfor n in zip(model0_name, model1_name, diff):\n\tprint(n[0], n[1], \"{0:.8f}\".format(n[2]))\n\nmodel0_running_variance = []\nmodel0_running_mean = []\nfor module in model0.modules():\n    if isinstance(module, nn.modules.BatchNorm2d):\n    \tmodel0_running_mean.append(module.running_mean)\n    \tmodel0_running_variance.append(module.running_var)\n\nmodel1_running_variance = []\nmodel1_running_mean = []\nfor module in model1.modules():\n    if isinstance(module, nn.modules.BatchNorm2d):\n    \tmodel1_running_mean.append(module.running_mean)\n    \tmodel1_running_variance.append(module.running_var)\n\nprint(\"running_mean difference\")\nfor m1, m2 in zip(model0_running_mean, model1_running_mean):\n\tprint(\"{0:.8f}\".format((m1-m2).sum()))\n\nprint(\"running_var difference\")\nfor m1, m2 in zip(model0_running_variance, model1_running_variance):\n\tprint(\"{0:.8f}\".format((m1-m2).sum()))\n\ninp = V(torch.randn(1,3,256,256))\npretrained_head0, out0 = model0(inp)\npretrained_head1, out1 = model1(inp)\n\n\ndiffpretrained_head = (pretrained_head1 - pretrained_head0).data.abs().sum()\ndiffout = (out1 - out0).data.abs().sum()\n\nprint(diffpretrained_head, diffout)\n<\/code>\nAnd again, these scripts proved my claim. Could you please check them up on your system?\nHi, I had the same problem a couple of months ago, I thought there is some thing wrong in my code!\nI debugged your code a bit and it seems that the BatchNorm layers differ.\nYou can\u2019t see it, since you have a typo in saving the running_mean and running_var:\n<code class=\"lang-auto\">model0_running_variance = []\nmodel0_running_mean = []\nfor module in model0.modules():\n    if isinstance(module, nn.modules.BatchNorm2d):\n    \tmodel0_running_mean.append(module.running_mean)\n    \tmodel0_running_variance.append(module.running_var)\n\nmodel1_running_variance = []\nmodel1_running_mean = []\nfor module in model0.modules():\n    if isinstance(module, nn.modules.BatchNorm2d):\n    \tmodel1_running_mean.append(module.running_mean)\n    \tmodel1_running_variance.append(module.running_var)\n<\/code>\nIn both loops you iterate model0.\nI will check, why they differ.\nThe problem is that your BN layers differ.\nI used the following code to solve the problem (just override the train function of your model):\n<code class=\"lang-auto\">    def train(self, mode=True, freeze_bn=False, freeze_bn_affine=False):\n        \n        super(MyModel, self).train(mode)\n        if freeze_bn:\n            for m in self.modules():\n                if isinstance(m, nn.BatchNorm2d):\n                    '''Freezing Mean\/Var of BatchNorm2D'''\n                    m.eval()\n                    if freeze_bn_affine:\n                        '''Freezing Weight\/Bias of BatchNorm2D'''\n                        m.weight.requires_grad = False\n                        m.bias.requires_grad = False\n<\/code>\nThanks. I have edited my reply! Sorry.\nI\u2019ve checked the training part and the running_* stats are being updated.\nYou have to set the BatchNorm layers to evaluation mode.\nAdd this to your training:\n<code class=\"lang-auto\">def freeze_bn(m):\n    if isinstance(m, nn.BatchNorm2d):\n        m.eval()\nmodel.apply(freeze_bn)\n<\/code>\nEDIT: Too late,  was faster!  Good catch!\nWooow. Thanks . It was solved by your snippet."},{"x":"Hi,\nAssume below is my code:\nx = Variable(torch.ones(2, 2), requires_grad=True)\ny = x + 2\nz = y * y * 3\nout = z.mean()\nout.backward(retain_graph=True)\nx,y,z,out = 0,0,0,0\nWill the graph be freed up (deleted) at this point?\nThanks","y":"yes it will be freed.","z":"yes it will be freed."},{"x":"Hi, I\u2019m using GRU with 1 custom loss function. It seems that everything go right in forward direction, but when I call loss.backward(), my program has this error:\n<code class=\"lang-auto\">Traceback (most recent call last):\n  File \"\/home\/vietpd\/Python\/local\/lib\/python2.7\/site-packages\/grpc\/_server.py\", line 389, in _take_response_from_response_iterator\n    return next(response_iterator), True\n  File \"app.py\", line 344, in trainRNN\n    }, save_path=RNN_PATH)\n  File \"\/home\/vietpd\/Python\/CafebizPersonalize\/deeplearning\/pytorch\/SessionRnnLayer.py\", line 168, in fit\n    train_loss = _train(model, train_set, optimizer, epoch)  # train over train_set\n  File \"\/home\/vietpd\/Python\/CafebizPersonalize\/deeplearning\/pytorch\/SessionRnnLayer.py\", line 217, in _train\n    loss.backward()\n  File \"\/home\/vietpd\/Python\/local\/lib\/python2.7\/site-packages\/torch\/autograd\/variable.py\", line 121, in backward\n    torch.autograd.backward(self, gradient, retain_graph, create_graph)\n  File \"\/home\/vietpd\/Python\/local\/lib\/python2.7\/site-packages\/torch\/autograd\/__init__.py\", line 83, in backward\n    variables, grad_variables, retain_graph, create_graph)\nRuntimeError: The size of tensor a (4) must match the size of tensor b (204) at non-singleton dimension 1\n<\/code>\nThis is my model:\n<code class=\"lang-auto\">class SessionRnnLayer(nn.Module):\n    def __init__(self, embedding_size, n_items):\n        super(SessionRnnLayer, self).__init__()\n\n        self.embedding_size = embedding_size\n\n        self.features = nn.GRU(embedding_size, embedding_size, 1, batch_first=True, dropout=0.5)\n        self.embedding = nn.Embedding(n_items, embedding_size)\n\n    def forward(self, x, y, hidden=None):\n        lengths = [len(session) for session in x]\n\n        inputs = [Variable(torch.LongTensor(session)) for session in x]\n        inputs = [self.embedding(session) for session in inputs]\n        inputs = pad_sequence(inputs, batch_first=True)\n        inputs = pack_padded_sequence(inputs, lengths, batch_first=True)\n\n        # convert to cuda tensors if cuda flag is true\n        if USE_CUDA:\n            inputs = inputs.cuda()\n\n        features, hidden = self.features(inputs, hidden)\n        if isinstance(features, PackedSequence):\n            features, lengths = pad_packed_sequence(features, batch_first=True)\n        \n        loss = map(partial(self._get_loss, loss_func=bpr_max_loss), [feature[:seq_len] for feature, seq_len in zip(features, lengths)], y)\n\n        return torch.mean(torch.cat(loss, dim=0)), features, hidden\n\n    def _get_loss(self, feature, label, loss_func=None):\n        label = torch.LongTensor(label)\n        # convert to cuda tensors if cuda flag is true\n        if USE_CUDA:\n            label = label.cuda()\n        label = Variable(label)\n\n        target = self.embedding(label)\n        logits = torch.matmul(feature[:seq_len], target.t())\n\n        return loss_func(logits)\n\n    def init_embed(self, embed):\n        self.embedding.weight = nn.Parameter(torch.from_numpy(embed).float())\n        self.embedding.weight.requires_grad = False\n<\/code>\nAnd this is my loss function:\n<code class=\"lang-auto\">def bpr_max_loss(logits, alpha=0.5):\n    softmax_scores = softmax_neg(logits).t()\n    logits_T = logits.t()\n    diag = torch.diag(logits, 0)\n\n    return torch.mean(-torch.log(torch.sum(F.sigmoid(diag - logits_T) * softmax_scores, 0) + 1e-24)\n                      + alpha * torch.sum((logits_T ** 2) * softmax_scores, 0))\n\n\ndef softmax_neg(X):\n    hack_matrix = np.ones(X.size(), dtype=np.float32)\n    np.fill_diagonal(hack_matrix, 0)\n    hack_matrix = Variable(torch.from_numpy(hack_matrix).float())\n\n    X = X * hack_matrix\n    X_max = torch.max(X, 1)[0].unsqueeze(1).expand_as(X)\n\n    X = (X - X_max) * hack_matrix\n\n    return F.softmax(X, dim=1)\n<\/code>\n\n\nx: batch x list of input id\nEx: [ [1, 2, 3], [7, 8] ]\ny: batch x (list of target id + N negative samples id)\nEx (N=1): [ [2, 3, 4, 5], [8, 9, 10] ]\n\n\nI found out that when I set N=0 (mean that size(x)=size(y)), my code will run smoothly. When I set N=200, it show the above error message\nHow can I debug, and fix this? Thanks for reading.","y":"Thank you!\nThis doesn\u2019t crash for me on the master branch. What version of PyTorch are you using? If you\u2019re using something less than or equal to 0.2, you should upgrade to 0.3 and see if the problem goes away.","z":"Sounds like it could be a bug, especially if the forward pass runs successfully but the backwards pass fails. Could you provide a script (with dummy inputs, or something like that) that reproduces this?\nThanks for reply. This is mimimum script:\n<code class=\"lang-auto\">\nx = Variable(torch.rand(1, 10, 100), requires_grad=True)  # input\n\n\"\"\"\ny = Variable(torch.rand(1, 10, 100), requires_grad=True)  # target\n\nnegative_samples = Variable(torch.rand(1, 5, 100))  # samples = 5\nsamples = torch.cat([y, negative_samples], dim=1)  # (1, 15, 100)\n\n\"\"\"\n\nsamples = Variable(torch.rand(1, 15, 100), requires_grad=True)  # for short\n# this script will run if replace (1, 15, 100) with (1, 10, 100)\n\nrnn = nn.GRU(100, 100, 1, batch_first=True)\n\ny_hat, _ = rnn(x)  # (1, 10, 100)\n\nlogits = torch.bmm(y_hat, samples.permute(0, 2, 1)).squeeze(0)  # (10, 15)\n\nloss = -torch.mean(F.logsigmoid(logits.diag(0) - logits.t()))\nloss.backward()\n\n<\/code>\n<code class=\"lang-auto\">$ python test_pytorch.py \nTraceback (most recent call last):\n  File \"test_pytorch.py\", line 30, in <module>\n    loss.backward()\n  File \"\/Users\/chotoxautinh\/Workspaces\/Python\/lib\/python2.7\/site-packages\/torch\/autograd\/variable.py\", line 121, in backward\n    torch.autograd.backward(self, gradient, retain_graph, create_graph)\n  File \"\/Users\/chotoxautinh\/Workspaces\/Python\/lib\/python2.7\/site-packages\/torch\/autograd\/__init__.py\", line 83, in backward\n    variables, grad_variables, retain_graph, create_graph)\nRuntimeError: The size of tensor a (10) must match the size of tensor b (15) at non-singleton dimension 1\n\n<\/code>\nThank you!\nThis doesn\u2019t crash for me on the master branch. What version of PyTorch are you using? If you\u2019re using something less than or equal to 0.2, you should upgrade to 0.3 and see if the problem goes away.\n[UPDATE]\nThanks, it works after upgrading to latest master branch\n=======================\nI 'm using version 0.4.x. I just had installed last month by building from source, not too long. I will try to rebuild from master branch"},{"x":"Hi, I am trying to reproduce a NN in Python, using PyTorch. I set in my code np.random.seed(3)  and torch.manual_seed(3) (the numpy seed being the same as the one used for the NN without Pytorch, and the torch seed, being whatever value).\nI know the results are different with different initial seed because the optimizer will start in a different point, but the differences are huge. If I choose torch.manual_seed(1) the train and test accuracy are 65% and 34%, respectively. If I choose  torch.manual_seed(3), they are instead 97% and 68%. My datasets consist on a training set of only 209 images and 50 testing images. I am using SGD instead of gradient descent. Is there any reason to think that gradient descent would be less affected on the initial conditions than the SGD? (different seeds giving similar accuracies)\nI would expect strong differences if I would be working with a huge dataset as CIFAR-10 for example, but not with a small dataset as the one described. Am I wrong in the logic I follow?","y":"\n\n\n erm:\n\nIs there any reason to think that gradient descent would be less affected on the initial conditions than the SGD? (different seeds giving similar accuracies)\n\n\nThe loss curve of GD is usually less noisy and the loss should decrease in each step.\nUsing SGD on the other hand gives you more noise, but also the final accuracy is often better than in GD. One might claim that the noisy updates add some regularization, but I\u2019m not sure what the current theory is.\nIf random seeds give such different results, your overall training is unstable, which is a bad sign.\n\n\n\n erm:\n\nI would expect strong differences if I would be working with a huge dataset as CIFAR-10 for example, but not with a small dataset as the one described.\n\n\nI would expect it the other way around. From my experience, the more data you have, the less likely it is to get trapped in a local minima.\nE.g. if you are dealing with the XOR problem, you might easily stuck in a local minima and the seeds might decide if your model trains fine or not.\nIf you are training on ImageNet, I doubt that the seeds will make a huge difference. The training success would most likely be defined by the overall training, model architecture, augmentation etc.","z":"\n\n\n erm:\n\nIs there any reason to think that gradient descent would be less affected on the initial conditions than the SGD? (different seeds giving similar accuracies)\n\n\nThe loss curve of GD is usually less noisy and the loss should decrease in each step.\nUsing SGD on the other hand gives you more noise, but also the final accuracy is often better than in GD. One might claim that the noisy updates add some regularization, but I\u2019m not sure what the current theory is.\nIf random seeds give such different results, your overall training is unstable, which is a bad sign.\n\n\n\n erm:\n\nI would expect strong differences if I would be working with a huge dataset as CIFAR-10 for example, but not with a small dataset as the one described.\n\n\nI would expect it the other way around. From my experience, the more data you have, the less likely it is to get trapped in a local minima.\nE.g. if you are dealing with the XOR problem, you might easily stuck in a local minima and the seeds might decide if your model trains fine or not.\nIf you are training on ImageNet, I doubt that the seeds will make a huge difference. The training success would most likely be defined by the overall training, model architecture, augmentation etc.\nThanks for your answer. I need to develop intuition in this area yet. In Computational Chemistry we use also GD, SGD, and the sensitivity to initial conditions increases with the number of atoms, due to the presence of more degrees of freedom. So, my line of thinking was that the binary classification of cat vs non cat , would be less affected than a ten classes classification as CIFAR-10. But I see your point, and the role of the amount of data."},{"x":"I have a network (can be VGG, Resnet, Densenet) with its head\/final layer split into two sibling layers. Both the layers are of size equal to number of classes. One layer outputs logits (before softmax) while the other one outputs noise for each class. In simple terms, my loss function is a cross entropy over element-wise sum of both the layers.\nFor this, should I be extending autograd and implement both forward and backward separately for each sibling layer? or something else can be done?","y":"just write your loss in terms of autograd operations, and call backward. you dont need to do anything special like writing your own autograd.Function with a custom backward.","z":"There\u2019s no need to implement the backward. Just calculate the loss and do loss.backward().\nAre your sure?\nThe network produces two different outputs. I do an operation using the two outputs separately, not in the forward pass.\nHow will that work out with backward then?\nFor clarification, I am trying to do the following. \nAs you can see, here y and sigma are network outputs.\nRef: Image from https:\/\/alexgkendall.com\/media\/presentations\/oxford_seminar.pdf\njust write your loss in terms of autograd operations, and call backward. you dont need to do anything special like writing your own autograd.Function with a custom backward.\nThat worked out just fine. Just needed to dig more into PyTorch documentation. Thanks."},{"x":"The tutorial says( http:\/\/pytorch.org\/tutorials\/beginner\/former_torchies\/autograd_tutorial.html#gradients): so if you even want to do the backward on some part of the graph twice, you need to pass in retain_variables = True during the first pass\nThe example it gives is:\nimport torch\nfrom torch.autograd import Variable\nx = Variable(torch.ones(2, 2), requires_grad=True)\ny = x + 2\ny.backward(torch.ones(2, 2), retain_graph=True)\nprint(x.grad)\nz = y * y\nprint(z)\ngradient = torch.randn(2, 2)\ny.backward(gradient)\nprint(x.grad)\nBut when I try this code with retain_graph=True and  retain_graph=False, they both works with no error, and the gradients are corrects.\nAnything wrong with the example?\nThanks!","y":"In this specific case you do not need retain_graph=True, but in general you may need it. As you compute the forward pass, PyTorch saves variables that will be needed to compute the gradients in the backward pass. For example, z = y * y needs to save the value of y, because dz\/dy = 2*y (or y + y). However, y = x + 2 doesn\u2019t need to save anything because dy\/dx = 1 which doesn\u2019t depend on x.\nWhen you call backwards() with retain_graph=False (or without specifying it), the automatic differentiation engine frees the saved variables as it computes the gradients. If you call backwards() again, it will fail with an exception if it needs any freed saved variables. If it doesn\u2019t need any saved variables, like in your example, then it will succeed, but you shouldn\u2019t rely on this behavior.\nIf you change y = x + 2 to y = x * x, you will see an error:\n<code class=\"lang-auto\">Trying to backward through the graph a second time, but the buffers have already been freed. Specify retain_graph=True when calling backward the first time.\n<\/code>","z":"In this specific case you do not need retain_graph=True, but in general you may need it. As you compute the forward pass, PyTorch saves variables that will be needed to compute the gradients in the backward pass. For example, z = y * y needs to save the value of y, because dz\/dy = 2*y (or y + y). However, y = x + 2 doesn\u2019t need to save anything because dy\/dx = 1 which doesn\u2019t depend on x.\nWhen you call backwards() with retain_graph=False (or without specifying it), the automatic differentiation engine frees the saved variables as it computes the gradients. If you call backwards() again, it will fail with an exception if it needs any freed saved variables. If it doesn\u2019t need any saved variables, like in your example, then it will succeed, but you shouldn\u2019t rely on this behavior.\nIf you change y = x + 2 to y = x * x, you will see an error:\n<code class=\"lang-auto\">Trying to backward through the graph a second time, but the buffers have already been freed. Specify retain_graph=True when calling backward the first time.\n<\/code>\nThanks for the explanation!"},{"x":"Hello.\nI have a problem running below code\n<code class=\"lang-python\">import torch\nimport torch.nn as nn\nfrom torch.autograd import Variable\n\nweight_list = nn.Parameter(torch.randn(5, 3, 3), requires_grad=True)\nindices = torch.LongTensor([0, 2])\nweight_select = weight_list.index_select(0, indices)\n<\/code>\nTherefore, I want to select two 3x3 convolution kernels among the 5.\nHowever, when I execute this code, I get a runtime error:\n<code class=\"lang-bash\">Traceback (most recent call last):\n  File \"test.py\", line 10, in <module>\n    weight_select = weight_list.index_select(0, indices)\n  File \"\/home\/sanghyun\/anaconda3\/lib\/python3.6\/site-packages\/torch\/autograd\/variable.py\", line 681, in index_select\n    return IndexSelect.apply(self, dim, index)\nRuntimeError: save_for_backward can only save input or output tensors, but argument 0 doesn't satisfy this condition\n<\/code>\nThe strange thing is that if I change requires_grad=True to False, this does not happen.\nWhat is problem?\nThank you.","y":"Weird error message. Fortunately, this doesn\u2019t happen on master branch \nHowever, you should wrap your indices in Variable.","z":"Weird error message. Fortunately, this doesn\u2019t happen on master branch \nHowever, you should wrap your indices in Variable.\nYou are right.\nAfter wrapping indices with Variable, all works fine \nThank you!"},{"x":"Hello, I am trying to do the following forward calculation:\ny_ij = ||x_i||*cos(2<x_i,w_j>)\nwhere x_i and w_j are vectors from matrix X and W. y_ij is an element in resulting matrix Y.\nThere are two equivalent ways to realize it:\n\nxlen = x.pow(2).sum(1).pow(0.5).view(-1, 1)  # ||x||\nwlen = w.pow(2).sum(0).pow(0.5).view(1, -1)  # ||w||\ncos_theta = (x.mm(w) \/ xlen \/ wlen).clamp(-1, 1)\ntheta = cos_theta.acos()\ncos_2_theta = torch.cos(2*theta)\ny = cos_2_theta * xlen.view(-1, 1)\n\n\nAlternatively,\n\nxlen = x.pow(2).sum(1).pow(0.5).view(-1, 1)  # ||x||\nwlen = w.pow(2).sum(0).pow(0.5).view(1, -1)  # ||w||\ncos_theta = (x.mm(w) \/ xlen \/ wlen).clamp(-1, 1)\ncos_2_theta = 2 * cos_theta ** 2 - 1  # cos(2x) = 2cos(x)^2-1\ny = cos_2_theta * xlen\n\n\nHowever, the first one is very unstable, i.e. gradients turns to NaN after several iterations. While the second one is good. Can anyone explain this issue?\nThanks!","y":"When you do backprogation with the first, at some point you\u2019ll run into the derivative of acos(x), which is - 1 \/ sqrt( 1 - x^2 ). That can be nasty and lead to your NaNs if x is close to 1 or -1 at times.\nIn particular, consider the following two functions: f(x) = cos(acos(x)) and g(x) = x. They\u2019re almost equivalent (except for when x = 1, -1). When one needs to backprop against g(x), life is easy: for some operation z on the output y = g(x), the chain rule gives you dz\/dy * dy\/dx = dz\/dy.\nOn the other hand, with y = f(x), the backpropagation looks like:\ndz\/dy * dy\/dx = dz\/dy * (- sin (acos (x) ) (- 1\/ sqrt(1 - x^2))\nIf x is close to 1 or -1, this could be very bad.","z":"When you do backprogation with the first, at some point you\u2019ll run into the derivative of acos(x), which is - 1 \/ sqrt( 1 - x^2 ). That can be nasty and lead to your NaNs if x is close to 1 or -1 at times.\nIn particular, consider the following two functions: f(x) = cos(acos(x)) and g(x) = x. They\u2019re almost equivalent (except for when x = 1, -1). When one needs to backprop against g(x), life is easy: for some operation z on the output y = g(x), the chain rule gives you dz\/dy * dy\/dx = dz\/dy.\nOn the other hand, with y = f(x), the backpropagation looks like:\ndz\/dy * dy\/dx = dz\/dy * (- sin (acos (x) ) (- 1\/ sqrt(1 - x^2))\nIf x is close to 1 or -1, this could be very bad."},{"x":"I have a very simple piece of code which puzzles me (using Python 3.5.3 and PyTorch version 0.2.0_3, no CUDA)\nAs far as I understand, in order to run backward() on a variable\nagain (after already running it once), it is necessary to reset the\nleaf gradients to zero first. But even when I do this, PyTorch will still complain\nin the following example code:\n\u201cRuntimeError: Trying to backward through the graph a second time, but the buffers have already been freed. Specify retain_graph=True when calling backward the first time.\u201d\n<code class=\"lang-auto\">import torch\nfrom torch.autograd import Variable as V\nx = V(torch.ones(2,2), requires_grad=True)\ny = 3*x*x  # but with y=3*x it would work!!!\ny.backward(torch.ones(2,2))\nprint(\"x.grad=\",x.grad)\nx.grad.data.zero_()\ny.backward(torch.ones(2,2))\n<\/code>\nThis happens when I calculate y=3*x*x but it does NOT happen when I calculate y=3*x!\nHow can I reset my gradients so that I can run backward a second time in my case? Is there a different, better way to make this work?","y":"There is a key difference between what you did and the workflow you described. In your example, you are backproping through the same graph x->3x->3x*x twice. Yet in usual workflow, each iteration a new graph is built and you backprop  through it only once.\nIf retain_graph is not specified, then graph will be freed after something backproped through it.","z":"Pass retain_graph=True to the first backward.\nBest regards\nThomas\nThank you , I know that retain_graph=True should work, but\nwhat I do not understand is why my example does not work while it\ndoes work fine when the function is 3y? Also, I thought that zeroing\nthe gradient is a sufficient and safe way to do this - this is shown in\na number of tutorials I think.\nOr is there simply no way to reset the graph and I have to retain it\n(and zero it in addition in order to prevent accumulation of gradients)\nif I want to call backward again? Then what is the point of not retaining\nin the first place?\nThis is really confusing me.\nIf 3y works, it only works accidentally and it might change in future.\nZeroing gradient is different with retaining the graph. It makes sense to not retaining by default:\n\nafter each forward, it is common to use grad values to update the parameters. then your x should change, and calculating gradient through y is incorrect now because y is computed using old x.\nto save memory\n\nDo not think of x and y as symbolic variables. Think of them as tensors with values.\nWhat I don\u2019t understand is that why you don\u2019t like retain_graph + zero_grad. It is the perfectly reasonable thing to do.\nOK, here is maybe a better explanation of what I try to understand:\nWhen I use some pre-fabricated model for my network, the normal process of training it is\n\nforward the input through the net and get the output\ncalculate the loss\nzero the gradients (through optimizer.zero_grad() or mynetword.zero_grad())\nbackward the loss using something like loss.backward()\n\ntake an optimizer step\nrinse and repeat\n\nNow, nowhere have I ever seen that in the loss.backward() step we would specify retain_graph=True yet this always works!? In order for this to work, the loss.backward() function has to recursively call somevariable.backward() on all of the parameters of its network but will these calls use the retain_graph option? I assume not. So why does this work in general, but not in my case? Are there rules when it will work and when not?\nI mainly want to understand what is going on and how PyTorch works so I do not even know yet if I \u201clike\u201d retaining the graph \nBut from the fact that practically all examples do not retain the graph and still work, my feeling is that there must be an advantage to not retaining it \u2026 and I would like to understand that as well!\nThere is a key difference between what you did and the workflow you described. In your example, you are backproping through the same graph x->3x->3x*x twice. Yet in usual workflow, each iteration a new graph is built and you backprop  through it only once.\nIf retain_graph is not specified, then graph will be freed after something backproped through it.\nThank you , I totally  forgot that the forward step usually reconstructs the whole graph from scratch, and that this is what usually happens with the canned models!\nSo, the graph is re-built, but the variables the represent the model parameters are re-used, so the gradients for those parameters need to get set to zero in order to avoid accumulating them."},{"x":"when I run this code,I want to see the creator attribute of y,while gives me an error of:       AttributeError: \u2018Variable\u2019 object has no attribute 'creator\u2019\nhow to resolve it?\nimport torch\nfrom torch.autograd import Variable\nx = Variable(torch.ones(2, 2), requires_grad=True)\ny = x + 2\nprint(y)\nprint(y.creator)","y":"I think .creator was replaced with .grad_fn in May as can be seen here: https:\/\/github.com\/pytorch\/tutorials\/pull\/91\/files","z":"I think .creator was replaced with .grad_fn in May as can be seen here: https:\/\/github.com\/pytorch\/tutorials\/pull\/91\/files\nthanks for your answer:relaxed:"},{"x":"Hi all,\nCould you please let me know if there is anyway to calculate the Gauss-Hessian matrix ?\nGaussian Newton is a quasi-Newton method which is defined here with link \"https:\/\/en.wikipedia.org\/wiki\/Gauss%E2%80%93Newton_algorithm\". It does not calculate direct the Hessian but approximate the Hessian buy broastcast product of two gradients as following function.\nPytorch does calculate gradient, but it seems like a sum of gradients over all input vectors. Can I have a matrix of gradients wrt one variable that NOT SUM, so I can manipulate the gradients later ?","y":"My bad, zz needs to be a leaf node in the computation graph. Try the following:\n<code class=\"lang-auto\">zz = Variable(z.data.expand(5, 1), requires_grad=True)\nL=(x*w*zz)**2\nL.sum().backward()\nzz.grad\n<\/code>","z":"I\u2019m assuming L is some vector that is some function of a vector z.\nIt sounds like you\u2019re looking for the derivative matrix of L (something like this with link \"http:\/\/mathinsight.org\/derivative_matrix\").\nOne thing you could do is compute the gradient of each element of L with respect to z. This will give you N gradients, that you could then concatenate to form the derivative matrix.\nThanks much for your answer.\nThe derivative matrix is what I need.\nYes I can do it manually using some for loops. But I also want to write it in pytorch and then put it running on GPU.\nAs I understand, foor loops is not good for performance, also unconvenient to put on GPU.\nCould you please help to let me know does Pytorch has a better way to do that ?\nLet\u2019s say L is a transformation that takes a vector z as input. If your L can operate on multiple vectors at once, then you could do something like the following: (here, L squares all elements of the input):\n<code class=\"lang-auto\">z = torch.randn(3)\nx = Variable(z.expand(3, 3), requires_grad=True)\nout = (x ** 2).trace() # replace x ** 2 with L(x)\nout.backward()\nx.grad  # gives the derivatives matrix\n<\/code>\nThe idea is to have each row contribute to out independently. The first row would contribute the first element of L to out, the second row contributes the second element of L, etc.\nThanks much for your help.  But if L is not a scalar as following:\n\nz=Variable(torch.randn(1,1),requires_grad = True)\nw  = Variable(torch.randn(5,1),requires_grad = True)\nx = Variable(torch.randn(5,1),requires_grad = False)\nL = (xwz)**2\n\nCan it return a vector of derivatives of each\nL's elements wrt z ?\nCalling L.backward() returns error.\nSo I have to loop over each element of L to call backward. Is there a better way to do that ?\nI\u2019m assuming L has 5 elements, like in your example.\nWhat you want to do is duplicate z 5 times:\nzz = z.expand(5, 1)\nThen send it to L:\nL = (xwz) ** 2\nNow call L.sum().backward() and look at zz.grad.\nI guess you said that\n\nL=(xwzz)**2\n\nI did call L.sum().backward() but zz.grad shows nothing.\nI\u2019m sorry but did I do anything wrong ?\nMy bad, zz needs to be a leaf node in the computation graph. Try the following:\n<code class=\"lang-auto\">zz = Variable(z.data.expand(5, 1), requires_grad=True)\nL=(x*w*zz)**2\nL.sum().backward()\nzz.grad\n<\/code>\nIt works like a charm.\nI did a numerical derivative by hand which returned same result.\nCan\u2019t say thank you enough. I tried to do the same in tensorflow for weeks but couldn\u2019t. Just one day switched to pytorch and my problem solved.\nHi Richard,\nI\u2019m sorry to bother you again. Could you please help to take a look at this ?\n\nx = Variable(torch.randn(10,3),requires_grad = False)\nw1 = Variable(torch.randn(3,5),requires_grad = True)\nw2 = Variable(torch.randn(5,7),requires_grad = True)\nz=Variable(torch.randn(1,1),requires_grad = True)\nL = (sigmoid(x.mm with link \"http:\/\/x.mm\"(w1*z))).mm(w2)\n\nThen how can I calculate the gradient matrix of L wrt to z ?\nThe result matrix should have the same size as L, which is (10,7) . But expand the z following shape of L would not fit the multiplication with w1 ?\n<code class=\"lang-auto\">import torch \nfrom torch.autograd import Variable\nfrom torch.nn.functional import sigmoid\n\nx = Variable(torch.randn(10,3),requires_grad = False)\nw1 = Variable(torch.randn(3,5),requires_grad = True)\nw2 = Variable(torch.randn(5,7),requires_grad = True)\nz=Variable(torch.randn(1,1),requires_grad = True)\nL = (sigmoid(x.mm(w1*z))).mm(w2)\n<\/code>\nSo the key idea here is that if you were to do this in a for loop, you\u2019d be computing L 10 * 7 = 70 times.\nInstead of doing this in a for loop, we\u2019d want to batch the computation.\nWe can define a new Variable, zz, that will hold the derivative matrix. It will eventually be size (10, 7),\nbut for now we\u2019ll make it a flat size 70 so that we can perform batch multiply operations with it.\nCode as follows:\n<code class=\"lang-auto\">zz = Variable(z.data.expand(70, 1, 1), requires_grad=True)\nbatched_x = x.expand(70, 10, 3)\nbatched_w1 = w1.expand(70, 3, 5)\nbatched_w2 = w2.expand(70, 5, 7)\nbatched_L = (sigmoid(batched_x.bmm(batched_w1*zz))).bmm(batched_w2)\nout = batched_L.view(70, 70).trace()\nout.backward()\nzz.grad.view(10, 7)\n<\/code>\nThanks much.\nThat helps."},{"x":"I understand that calling backward will clear the computation graph and as a result, a second call of it will throw an exception. e.g.,\n<code class=\"lang-auto\">x = Variable(torch.Tensor(5, 3), requires_grad=True)\ny = Variable(torch.Tensor(5, 3), requires_grad=True)\nz = torch.mm(x, torch.transpose(y, 0, 1))\nz.backward(torch.ones(5, 5), retain_graph=False)\nz.backward(torch.ones(5, 5), retain_graph=False)) # >> Throws RuntimeError\n<\/code>\nHowever, this seems to work differently for some operators, e.g., elementwise addition, e.g.,\n<code class=\"lang-auto\">x = Variable(torch.Tensor(5, 3), requires_grad=True)\ny = Variable(torch.Tensor(5, 3), requires_grad=True)\nz = x + y\nz.backward(torch.ones(5, 3), retain_graph=False)\n# You can call it multiple times and the grad will just accumulate\nz.backward(torch.ones(5, 3), retain_graph=False) \n<\/code>\nI am using pytorch \u20180.2.0_4\u2019, can some one explain why the second case works differently? Thanks!","y":"See the answer here:\n\n\n\n\nMultiple call to backward still works when retain_graph=false, why? with link \"https:\/\/discuss.pytorch.org\/t\/multiple-call-to-backward-still-works-when-retain-graph-false-why\/9475\/2\" autograd with link \"\/c\/autograd\"\n\n\n    In this specific case you do not need retain_graph=True, but in general you may need it. As you compute the forward pass, PyTorch saves variables that will be needed to compute the gradients in the backward pass. For example, z = y * y needs to save the value of y, because dz\/dy = 2*y (or y + y). However, y = x + 2 doesn\u2019t need to save anything because dy\/dx = 1 which doesn\u2019t depend on x. \nWhen you call backwards() with retain_graph=False (or without specifying it), the automatic differentiation\u2026\n  \n\nIf retain_graph=False, intermediate outputs needed for the backwards computation are freed. If there are no saved intermediate outputs, (like in the case of addition) then subsequent calls may still work, but you should not rely on that behavior.","z":"See the answer here:\n\n\n\n\nMultiple call to backward still works when retain_graph=false, why? with link \"https:\/\/discuss.pytorch.org\/t\/multiple-call-to-backward-still-works-when-retain-graph-false-why\/9475\/2\" autograd with link \"\/c\/autograd\"\n\n\n    In this specific case you do not need retain_graph=True, but in general you may need it. As you compute the forward pass, PyTorch saves variables that will be needed to compute the gradients in the backward pass. For example, z = y * y needs to save the value of y, because dz\/dy = 2*y (or y + y). However, y = x + 2 doesn\u2019t need to save anything because dy\/dx = 1 which doesn\u2019t depend on x. \nWhen you call backwards() with retain_graph=False (or without specifying it), the automatic differentiation\u2026\n  \n\nIf retain_graph=False, intermediate outputs needed for the backwards computation are freed. If there are no saved intermediate outputs, (like in the case of addition) then subsequent calls may still work, but you should not rely on that behavior.\nCool. That makes sense. Thank you for the clarification."},{"x":"Here is a minimal reproducible example:\n<code class=\"lang-python\">\"\"\"\nUsing nn.Embedding as LookupTable\n\"\"\"\n\nfrom torch import nn\nfrom torch.autograd import Variable\nimport torch\n\nclass LinearMulti(nn.Module):\n    \"\"\"\n    Fetch the weight and bias from a lookup table based on agent\/model id\n    Params:\n        sz_in: input layer\n        sz_out: output layer\n        model_ids: agent\/model id\n    Returns:\n        Tensor [len(model_ids), sz_out]\n    \"\"\"\n    def __init__(self, nmodels, sz_in, sz_out):\n        super(LinearMulti, self).__init__()\n        self.nmodels = nmodels\n        self.sz_in = sz_in\n        self.sz_out = sz_out\n\n        if nmodels == 1:\n            self.linear = nn.Linear(sz_in, sz_out)\n        else:\n            # XXX: potential bug - updateGradInput is overidden,\n            # possible use of `register_backward_hook`\n            self.weight_lut = nn.Embedding(nmodels, sz_in * sz_out) # 1x3x200\n            self.bias_lut = nn.Embedding(nmodels, sz_out) # 1x3x20\n\n    def forward(self, input, model_ids):\n        \"\"\"\n        Params:\n            input: shape [len(model_ids), sz_in]\n        \"\"\"\n        if self.nmodels == 1:\n            return self.linear(input)\n        else:\n            weight = self.weight_lut(model_ids) # 1 x 3 x 200\n            weight_view = weight.view(-1, self.sz_in, self.sz_out) # 3 x 10 x 20\n            bias = self.bias_lut(model_ids) # 1 x 3 x 20\n            bias_view = bias.view(-1, self.sz_out) # 3x20\n\n            a, b = input.size()\n            input = input.view(a, 1, b) # 3x1x10\n\n            out = torch.matmul(input, weight_view) # 3x1x20\n\n            a, b, c = out.size()\n            out = out.view(a, c) #3x20\n            out = out.add(bias_view) # 3x20\n            return out\n\n\nif __name__ == \"__main__\":\n    x = Variable(torch.ones(3, 4))\n    model = LinearMulti(3, 4, 1)\n    y = model.forward(x, Variable(torch.LongTensor([[1,2,1]])))\n    target = Variable(torch.FloatTensor([\n        [3],\n        [10],\n        [3],\n        ]))\n    print target\n    print(y)\n\n    learning_rate = 1e-1\n    optimizer = torch.optim.Adagrad(model.parameters(), lr=learning_rate)\n    loss_fn = torch.nn.MSELoss(size_average=False)\n\n    for i in range(100):\n        y = model.forward(x, Variable(torch.LongTensor([[1,2,1]])))\n        loss = loss_fn(y, target)\n        loss.backward(retain_graph=True)\n        print loss\n\n    # # Note: in the original test, the weight of l1, l2 is copied to the\n    # # weight of linear_multi. Then test the matmul results are the same\n\n<\/code>","y":"That is a lot of code. Try printing out the input to logsoftmax.\nAlso, you might want to use better initialization on your weights and\/or tune down the lr.","z":"You are not updating the parameters\u2026 Call optimizer.zero_grad() before fwd and bwd, and call optimizer.step() after.\nSimon,\nThank you for you answer.\nI have modified the code for the toy example. If you could take a look at the full code and see if there\u2019s something immediately obvious, that would be great!\nI have inspected all the weights during each time step and none of them has nan in it. But the softmax or logsoftmax layer output is nan. This is very bizarre. Here is the link to the code: it should be runnable: Embeddings not getting updated with link \"https:\/\/discuss.pytorch.org\/t\/embeddings-not-getting-updated\/3796\/7?u=ricky_han\"\nThanks in advance.\nThat is a lot of code. Try printing out the input to logsoftmax.\nAlso, you might want to use better initialization on your weights and\/or tune down the lr.\nThank you so much. After using normal initialization on all the weights. The model started working fine(to my pleasant surprise)."},{"x":"Here I meet an error like this :\nExpected object of type Variable[CPUFloatType] but found type Variable[CUDAFloatType]\nWhen I use this code:\n<code class=\"lang-auto\">\n<\/code>","y":"I don\u2019t think we can see your code ","z":"I don\u2019t think we can see your code \nThanks for reply!\nI have solved the bug myself(when edit the question),and thanks a lot,again!\nI will close this question\nActually, I think the error warning has some problems.\nMy error warning is \u201cRequire CPUfloat tensor but meet CudaFloat tensor\u201d,but actually my error appears when I forget to .cuda()one input in the net. And this error warning misleads me for a while.\nWell, I also find that torch.sum()function 's prompt message has a little problem, because the input arguments don\u2019t has dim prompt,which contradicts to the document.\nThe warning is weird but it kinda makes sense. It is saying : to do operation with input Tensor (CPU), it needs to see a net weight tensor on CPU, but it sees one on GPU. That said, I agree that it is confusing.\nI don\u2019t quite understand your question about sum. Could you elaborate?\nWell, about the sum function. In document ,it says that torch.sum function can sum in different axis, such as\n<code class=\"lang-auto\">a=torch.randn(4,4)\nb=torch.sum(a,dim=1)\n<\/code>\nthen we get a tensor b with dimension 4. But in the __init__.py file of torch.sum, the definition is def sum(input): which losses some arguments that don\u2019t appear in the prompt message.\nIt doesn\u2019t affect the use at all and may be a very small problem.\nWhat prompt message are you talking about? I\u2019m still very confused. Also, I don\u2019t think sum is defined in an __init__.py.\nWell, maybe it is the problem of my IDE (Pycharm)?\nI know that torch.sum is written in C++,and the __init__.py only record the form function.\nI use the Go to declaration tools in the IDE, and it goes to an __init__.py  file, and the illustrate of torch.sum is like this:\n<code class=\"lang-auto\">\ndef sum(input): # real signature unknown; restored from __doc__\n    \"\"\"\n    .. function:: sum(input) -> float\n    \n    Returns the sum of all elements in the :attr:`input` Tensor.\n    \n    Args:\n        input (Tensor): the input `Tensor`\n    \n    Example::\n    \n        >>> a = torch.randn(1, 3)\n        >>> a\n    \n         0.6170  0.3546  0.0253\n        [torch.FloatTensor of size 1x3]\n    \n        >>> torch.sum(a)\n        0.9969287421554327\n    \n    \n    .. function:: sum(input, dim, keepdim=False, out=None) -> Tensor\n    \n    Returns the sum of each row of the :attr:`input` Tensor in the given\n    dimension :attr:`dim`.\n    \n    If :attr:`keepdim` is ``True``, the output Tensor is of the same size\n    as :attr:`input` except in the dimension :attr:`dim` where it is of size 1.\n    Otherwise, :attr:`dim` is squeezed (see :func:`torch.squeeze`), resulting in\n    the output Tensor having 1 fewer dimension than :attr:`input`.\n    \n    Args:\n        input (Tensor): the input `Tensor`\n        dim (int): the dimension to reduce\n        keepdim (bool): whether the output Tensor has :attr:`dim` retained or not\n        out (Tensor, optional): the result Tensor\n    \n    Example::\n    \n        >>> a = torch.randn(4, 4)\n        >>> a\n    \n        -0.4640  0.0609  0.1122  0.4784\n        -1.3063  1.6443  0.4714 -0.7396\n        -1.3561 -0.1959  1.0609 -1.9855\n         2.6833  0.5746 -0.5709 -0.4430\n        [torch.FloatTensor of size 4x4]\n    \n        >>> torch.sum(a, 1)\n    \n         0.1874\n         0.0698\n        -2.4767\n         2.2440\n        [torch.FloatTensor of size 4]\n    \"\"\"\n    return 0.0\n\n<\/code>\nWhat I want to say is the input part, I only see an arument input,  and my IDE\u2019s argument prompt message will only give the hint of input, and will drop an warning if I add the dim argument.\nDo I illustrate the little problem clearly this time? Thanks a lot!\nI see. It\u2019s the docstring. Actually, if you scroll down, the second definition in docstring shows .. function:: sum(input, dim, keepdim=False, out=None) -> Tensor"},{"x":"Traceback (most recent call last):\nFile \u201c\u201d, line 1, in \nRuntimeError: softplus(): argument \u2018input\u2019 (position 1) must be Variable, not torch.FloatTensor\nWhy softplus doesn\u2019t work without Variable(\u2026) wrapper?\nDoesn\u2019t torch.nn.functional.softplus(Variable(tensor)).data seem ugly?\nPytorch versions: 0.3.0.post4, 0.2.0.post4\u2019","y":"nn.funtional are supposed to work with torch Variable not the plain Tensor.  Ref here with link \"https:\/\/discuss.pytorch.org\/t\/why-do-some-of-the-function-in-nn-functional-returns-variable-and-some-do-not\/10163\/3\".","z":"nn.funtional are supposed to work with torch Variable not the plain Tensor.  Ref here with link \"https:\/\/discuss.pytorch.org\/t\/why-do-some-of-the-function-in-nn-functional-returns-variable-and-some-do-not\/10163\/3\".\nSo the only way out at the moment is to use F.softplus(Variable(x)).data until Variable and Tensor will be merged, because torch.log(torch.exp(x) + 1) is not as numerically stable as F.softplus?\nI suppose so. Or you can find the source code of softplus and see how it is implemented.\nVariable and Tensor will be merged into one class in near future."},{"x":"I have some problems with pytorch gradcheck.\nclass KL(Function):\n    # vectorized\n    # input is batch_size x dim\n    # Note that both forward and backward are \n    \n    def forward(ctx, logits, beta=1):\n        log_z = 94.358391688507595 # partition function for dim=300\n        ctx.save_for_backward(logits)\n        ctx.beta = beta\n        \n        p = logits.sigmoid()\n        # more computationally stable entropy\n        entropy = (-logits * (p - 1) + F.softplus(-logits).data)\n        entropy[entropy != entropy] = 0 # when probs are ones or zeros -> entropy is zero\n        a = p[:, :-1]\n        b = p[:, 1:]\n        binary_potentials = (a * (1 - b) + (1 - a) * b)\n        out = entropy.neg().sum(dim=1) + log_z\n        out = out + beta * binary_potentials.sum(dim=1)\n        out = out.sum(dim=0, keepdim=True)\n        \n        return out\n\n    \n    def backward(ctx, grad_output):\n        logits = ctx.saved_variables[0]\n        beta = ctx.beta\n        \n        logits_exp = torch.exp(logits)\n        term_1 = (logits_exp * logits) \/ (logits_exp + 1)**2\n        \n        p = logits.sigmoid()\n        prev = torch.zeros(*p.size())\n        prev[:, 1:] = p.data[:, :-1]\n        prev = Variable(prev)\n        nxt = torch.zeros(*p.size())\n        nxt[:, :-1] = p.data[:, 1:]\n        nxt = Variable(nxt)\n        if p.is_cuda:\n            prev = prev.cuda()\n            nxt = nxt.cuda()\n        mul_term = ( 2 - 2 * prev - 2 * nxt)\n        mul_term[:, 0] = mul_term[:, 0] - 1\n        mul_term[:, -1] = mul_term[:, -1] - 1\n        term_2 = beta * p * (1 - p) * mul_term\n\n        grad_input = term_1 + term_2\n        \n        grad_input[grad_input.data != grad_input.data] = 0\n        \n\n        return grad_output * grad_input, None\n\nI have custom function showed above, and gradcheck fails:\nlogits = Variable(torch.randn(5, 300), requires_grad=True)\ninpt = (logits, 1)\ngradcheck(KL.apply, inpt, eps=1e-6, atol=1e-4)\nTraceback (most recent call last):\n  File \"<stdin>\", line 1, in <module>\n  File \"\/usr\/local\/lib\/python3.6\/site-packages\/torch\/autograd\/gradcheck.py\", line 181, in gradcheck\n    return fail_test('for output no. %d,\\n numerical:%s\\nanalytical:%s\\n' % (j, numerical, analytical))\n  File \"\/usr\/local\/lib\/python3.6\/site-packages\/torch\/autograd\/gradcheck.py\", line 166, in fail_test\n    raise RuntimeError(msg)\nRuntimeError: for output no. 0,\n numerical:(\n    0\n    0\n    0\n  \u22ee\n    0\n    0\n    0\n[torch.FloatTensor of size 1500x1]\n,)\nanalytical:(\n-0.1566\n-0.0927\n 0.1289\n   \u22ee\n-0.0009\n 0.2219\n-0.2261\n[torch.FloatTensor of size 1500x1]\n,)\n\nBut when I check function and its gradients with scipy.optimize.check_grad all is ok and error is less than 1e-6. As I see, the problem is with finite differences, but I can\u2019t understand why. Forward pass seems legit.\nPytorch versions: 0.3.0.post4, 0.2.0.post4\u2019","y":"Hi,\nThe matrix that is shown is the Jacbian, so it\u2019s size is always number_of_inputs x number_of_outputs so it is the right size in your case.\nYou should be careful that and eps of 1e-6 with FloatTensors often lead to loss of precision. I think you want to try to either use DoubleTensor (best idea usually) or increase eps.","z":"Some problems with shape in application of numerical and analytical formulas, because grad shape must be 5x300, not 1500x1.\nHi,\nThe matrix that is shown is the Jacbian, so it\u2019s size is always number_of_inputs x number_of_outputs so it is the right size in your case.\nYou should be careful that and eps of 1e-6 with FloatTensors often lead to loss of precision. I think you want to try to either use DoubleTensor (best idea usually) or increase eps."},{"x":"Hi, I wrote a module based on this article: http:\/\/www.wildml.com\/2015\/12\/implementing-a-cnn-for-text-classification-in-tensorflow\/\nThe idea is pass the input into multiple streams then concat together and connect to a FC layer. I divided my source code into 3 custom modules: TextClassifyCnnNet >> FlatCnnLayer >> FilterLayer\n<code class=\"lang-auto\">class FilterLayer(nn.Module):\n    def __init__(self, filter_size, embedding_size, sequence_length, out_channels=128):\n        super(FilterLayer, self).__init__()\n\n        self.model = nn.Sequential(\n            nn.Conv2d(1, out_channels, (filter_size, embedding_size)),\n            nn.ReLU(inplace=True),\n            nn.MaxPool2d((sequence_length - filter_size + 1, 1), stride=1)\n        )\n\n        for m in self.modules():\n            if isinstance(m, nn.Conv2d):\n                n = m.kernel_size[0] * m.kernel_size[1] * m.out_channels\n                m.weight.data.normal_(0, math.sqrt(2. \/ n))\n\n    def forward(self, x):\n        return self.model(x)\n<\/code>\n<code class=\"lang-auto\">class FlatCnnLayer(nn.Module):\n    def __init__(self, embedding_size, sequence_length, filter_sizes=[3, 4, 5], out_channels=128):\n        super(FlatCnnLayer, self).__init__()\n\n        self.filter_layers = nn.ModuleList(\n            [FilterLayer(filter_size, embedding_size, sequence_length, out_channels=out_channels) for\n             filter_size in filter_sizes])\n\n    def forward(self, x):\n        pools = []\n        for filter_layer in self.filter_layers:\n            out_filter = filter_layer(x)\n            # reshape from (batch_size, out_channels, h, w) to (batch_size, h, w, out_channels)\n            pools.append(out_filter.view(out_filter.size()[0], 1, 1, -1))\n        x = torch.cat(pools, dim=3)\n\n        x = x.view(x.size()[0], -1)\n        x = F.dropout(x, p=dropout_prob, training=True)\n\n        return x\n<\/code>\n<code class=\"lang-auto\">class TextClassifyCnnNet(nn.Module):\n    def __init__(self, embedding_size, sequence_length, num_classes, filter_sizes=[3, 4, 5], out_channels=128):\n        super(TextClassifyCnnNet, self).__init__()\n\n        self.flat_layer = FlatCnnLayer(embedding_size, sequence_length, filter_sizes=filter_sizes,\n                                       out_channels=out_channels)\n\n        self.model = nn.Sequential(\n            self.flat_layer,\n            nn.Linear(out_channels * len(filter_sizes), num_classes)\n        )\n\n    def forward(self, x):\n        x = self.model(x)\n\n        return x\n\n\ndef fit(net, data, save_path):\n    if torch.cuda.is_available():\n        net = net.cuda()\n\n    for param in list(net.parameters()):\n        print(type(param.data), param.size())\n\n    optimizer = optim.Adam(net.parameters(), lr=0.01, weight_decay=0.1)\n\n    X_train, X_test = data['X_train'], data['X_test']\n    Y_train, Y_test = data['Y_train'], data['Y_test']\n\n    X_valid, Y_valid = data['X_valid'], data['Y_valid']\n\n    n_batch = len(X_train) \/\/ batch_size\n\n    for epoch in range(1, n_epochs + 1):  # loop over the dataset multiple times\n        net.train()\n        start = 0\n        end = batch_size\n\n        for batch_idx in range(1, n_batch + 1):\n            # get the inputs\n            x, y = X_train[start:end], Y_train[start:end]\n            start = end\n            end = start + batch_size\n\n            # zero the parameter gradients\n            optimizer.zero_grad()\n\n            # forward + backward + optimize\n            predicts = _get_predict(net, x)\n            loss = _get_loss(predicts, y)\n            loss.backward()\n            optimizer.step()\n\n            if batch_idx % display_step == 0:\n                print('Train Epoch: {} [{}\/{} ({:.0f}%)]\\tLoss: {:.6f}'.format(\n                    epoch, batch_idx * len(x), len(X_train), 100. * batch_idx \/ (n_batch + 1), loss.data[0]))\n\n        # print statistics\n        if epoch % display_step == 0 or epoch == 1:\n            net.eval()\n            valid_predicts = _get_predict(net, X_valid)\n            valid_loss = _get_loss(valid_predicts, Y_valid)\n            valid_accuracy = _get_accuracy(valid_predicts, Y_valid)\n            print('\\r[%d] loss: %.3f - accuracy: %.2f' % (epoch, valid_loss.data[0], valid_accuracy * 100))\n\n    print('\\rFinished Training\\n')\n\n    net.eval()\n\n    test_predicts = _get_predict(net, X_test)\n    test_loss = _get_loss(test_predicts, Y_test).data[0]\n    test_accuracy = _get_accuracy(test_predicts, Y_test)\n    print('Test loss: %.3f - Test accuracy: %.2f' % (test_loss, test_accuracy * 100))\n\n    torch.save(net.flat_layer.state_dict(), save_path)\n\n\ndef _get_accuracy(predicts, labels):\n    predicts = torch.max(predicts, 1)[1].data[0]\n    return np.mean(predicts == labels)\n\n\ndef _get_predict(net, x):\n    # wrap them in Variable\n    inputs = torch.from_numpy(x).float()\n    # convert to cuda tensors if cuda flag is true\n    if torch.cuda.is_available:\n        inputs = inputs.cuda()\n    inputs = Variable(inputs)\n    return net(inputs)\n\n\ndef _get_loss(predicts, labels):\n    labels = torch.from_numpy(labels).long()\n    # convert to cuda tensors if cuda flag is true\n    if torch.cuda.is_available:\n        labels = labels.cuda()\n    labels = Variable(labels)\n    return F.cross_entropy(predicts, labels)\n<\/code>\nIt seems parameters remain not changed at all, I have tried to print .grad of them but it return None.","y":"I realised that L2 Loss make loss value remain unchange. It works when I remove L2 Loss:\n<code class=\"lang-auto\"># optimizer = optim.Adam(net.parameters(), lr=0.01, weight_decay=0.1)\noptimizer = optim.Adam(model.parameters(), lr=0.001)\n<\/code>\nI think it\u2019s a bug of Pytorch, but this result is still acceptable for me, so I will close this topic.","z":"The first thing I would verify is that the loop that contains:\n\n            # forward + backward + optimize\n            predicts = self.get_predict(x)\n            loss = self.get_loss(predicts, y)\n            loss.backward()\n            optimizer.step() \n\n\nIs running as you expect. (is n_batch set to 0?) Is it looping?\nAssuming that is working correctly I would examine the actual output of the loss function:\nself.get_loss(...)\nAnd ensure it is outputting a loss signal that makes sense.\nAssuming all of the above checks out I would examine the output of self.model.parameters() \u2026 and ensure it is returning the modules \/ weights as you expect.\nfor param in list(self.model.parameters()):\n    print(type(param.data), param.size())\n\nn_batch = len(X_train) \/\/ batch_size\nprint \"n_batch: \", n_batch\n\nOutput:\n\n(<class \u2018torch.cuda.FloatTensor\u2019>, (128L, 1L, 3L, 128L))\n(<class \u2018torch.cuda.FloatTensor\u2019>, (128L,))\n(<class \u2018torch.cuda.FloatTensor\u2019>, (128L, 1L, 4L, 128L))\n(<class \u2018torch.cuda.FloatTensor\u2019>, (128L,))\n(<class \u2018torch.cuda.FloatTensor\u2019>, (128L, 1L, 5L, 128L))\n(<class \u2018torch.cuda.FloatTensor\u2019>, (128L,))\n(<class \u2018torch.cuda.FloatTensor\u2019>, (13L, 384L))\n(<class \u2018torch.cuda.FloatTensor\u2019>, (13L,))\nn_batch:  23\n\nit contains all parameters of conv layer, fc layer and bias\nDid you verify that all the contents within fit(\u2026) are being called correctly? Like, is n_epochs set > 0 ?\nPerhaps add to:\n\nfor _ in range(n_batch):\n# get the inputs\nx, y = X_train[start:end], Y_train[start:end]\nprint(_) # <-- add here, verify inner loop is being called\n\nof course, I added print in there to ensure everything correct\nI just try to replace cross_entropy with nll_loss and refactor forward function, but why it returns another loss value (but still wrong value)\n<code class=\"lang-auto\">    def forward(self, x):\n        x = self.model(x)\n\n        return F.log_softmax(x)\n<\/code>\nI have similar implementation in Tensorflow, so I know how loss value drop if doing correctly\nI realised that L2 Loss make loss value remain unchange. It works when I remove L2 Loss:\n<code class=\"lang-auto\"># optimizer = optim.Adam(net.parameters(), lr=0.01, weight_decay=0.1)\noptimizer = optim.Adam(model.parameters(), lr=0.001)\n<\/code>\nI think it\u2019s a bug of Pytorch, but this result is still acceptable for me, so I will close this topic."},{"x":"I want to normalize the output,for example,my batch size is 64 ,I will normalize for each line\ndef x_normalization(x):\n       num_pic,_=x.data.shape\n       for i in range(num_pic):\n            x[i,:]=x[i,:]\/torch.max(x[i,:])    #normalize for each line\n       return x\n\nI call this function in\n\nI get a error\none of the variables needed for gradient computation has been modified by an inplace operation","y":"Hi,\nThe problem is that you modify x in place while it\u2019s value is needed to compute gradients.\nYou can replace your function with:\n<code class=\"lang-auto\">def x_normalization(x):\n       x = x \/ x.max(0, keepdim=True)[0]\n       return x\n<\/code>","z":"Hi,\nThe problem is that you modify x in place while it\u2019s value is needed to compute gradients.\nYou can replace your function with:\n<code class=\"lang-auto\">def x_normalization(x):\n       x = x \/ x.max(0, keepdim=True)[0]\n       return x\n<\/code>\nexcuse me. I also encounter this problem when I run the code below in a forward() function(code is in the image). If I comment the guess_img_dist_arr[n]=(torch.norm(guess_img[y][x]-guess_img[y_co][x_co]))\nIt will not raise the error, but get a wrong result.\ncould you tell me how to fix this?\n\u5c4f\u5e55\u5feb\u7167 2017-12-21 \u4e0a\u534812.36.27.png1480\u00d7754 88.2 KB with link \"https:\/\/discuss.pytorch.org\/uploads\/default\/original\/2X\/4\/486e605d7a494a9655426242023f32c3b9f9fcab.png\"\nHi,\nYou should not modify a single Variable inplace multiple times.\nIn your case, one wait to solve this would be:\n<code class=\"lang-auto\">for i in range(k_clusters):\n  # some code\n\n  guess_img_dist_list = []\n  for n in range(length):\n    # some code\n    guess_img_dist_list.append(torch.norm(...))\n\n  guess_img_dist_arr = torch.cat(guess_img_dist_list, 0)\n<\/code>\nThanks a lot!\nYou saved my day!\nthanks, that is feasible"},{"x":"Hello everyone,\nI get broken pipeline error when num of workers > 0 in the dataloader. I did look at various solutions (which worked for multiple people) that suggested the code snippet as shown below:\n<code class=\"lang-python\">def run():\n     # actual code\n\nif __name__ = '__main__':  \n       run()\n<\/code>\nHowever, this does not seem to work for me. I have a PyTorch version of 1.0.0 , python version of 3.6.8 running on Windows 10 and my GPU details are shown below.\nimage887\u00d7503 18.2 KB with link \"https:\/\/discuss.pytorch.org\/uploads\/default\/original\/3X\/a\/9\/a9257697cdde783dc94b883de6913a3ed3b0bd53.png\"\ncuda version:\n\nCan you please help me with this?","y":"For Jupyter Notebook, you need to refactor your code a bit more. Please refer to this post for solution. https:\/\/medium.com\/\/speed-up-your-python-code-using-multiprocessing-on-windows-and-jupyter-or-ipython-2714b49d6fac","z":"Which terminal \/ IDE are you using? Could you please post a minimal reproducible example?\nI am using Jupyter notebook to simulate this. Here is the minimal reproducible example. Since my input and ouput are of the shape [1700, 402] and [1700,] I used the similar size here.\n<code class=\"lang-python\">import torch\nimport numpy\nfrom torch.utils.data import Dataset, DataLoader\nimport pdb\n\n# dataset class\nclass data(Dataset):\n    def __init__(self, x, y):\n        self.x = x\n        self.y = y\n        self.len = self.x.shape[0]\n\n    def __getitem__(self, idx):\n        return self.x[idx], self.y[idx]\n\n    def __len__(self):\n        return self.len\n\nif __name__ == \"__main__\": \n    device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\") \n    x = numpy.random.rand(1700,402)\n    y = numpy.random.rand(1700)\n    dataset = data(torch.Tensor(x),torch.Tensor(y))\n    print('the length of the dataset = ', len(dataset))\n    \n    print ('before loader')\n    data_loader = DataLoader(dataset, batch_size = 50, shuffle = True, pin_memory=True, num_workers = 1)\n    print ('after loader')\n    \n    for idxs, (xx, yy) in enumerate(data_loader):\n        xx, yy = xx.to(device), yy.to(device)\n        print ('idx: {} x: {} y: {}'.format(idxs, xx, yy))\n<\/code>\nThis time, the error is not shown however, the notebook gets stuck without displaying anything . It does print \u2018after loader\u2019 after which it gets stuck until I restart the kernel. It works perfectly fine if I set num_workers = 0.\nNote: the brokern pipe error is still consistent with my original case.\nFor Jupyter Notebook, you need to refactor your code a bit more. Please refer to this post for solution. https:\/\/medium.com\/\/speed-up-your-python-code-using-multiprocessing-on-windows-and-jupyter-or-ipython-2714b49d6fac\nThank you for your response. The workaround works for now but I am getting a new error at the line:\np.map(workers.run(used_device), [i for i in range(0,3)])\n<code class=\"lang-python\">    642             return self._value\n    643         else:\n--> 644             raise self._value\n    645 \n    646     def _set(self, i, obj):\n\nTypeError: 'NoneType' object is not callable\n<\/code>\nHere is the code snippet from the python file\n<code class=\"lang-python\">import torch\nimport numpy\nfrom torch.utils.data import Dataset, DataLoader\nimport pdb\n\nclass data(Dataset):\n    # Constructor\n    def __init__(self, x, y):\n        self.x = x\n        self.y = y\n        self.len = self.x.shape[0]\n\n    # Getter\n    def __getitem__(self, idx):\n        return self.x[idx], self.y[idx]\n\n    # Return the length\n    def __len__(self):\n        return self.len\n\ndef run(device):\n    x = numpy.random.rand(1700,402)\n    y = numpy.random.rand(1700)\n    dataset = data(torch.Tensor(x),torch.Tensor(y))\n    print('the length of the dataset = ', len(dataset))\n    data_loader = DataLoader(dataset, batch_size = 100, shuffle = True)\n    \n    for idxs, (xx, yy) in enumerate(data_loader):\n        xx, yy = xx.to(device), yy.to(device)\n        print ('idx: {} x: {} y: {}'.format(idxs, xx, yy))\n<\/code>\nthis is the code snippet from the Jupiter notebook:\n<code class=\"lang-python\">from multiprocessing import Pool\nimport workers\n    \nif __name__ == \"__main__\": \n    used_device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n    num_processors = 2\n    p=Pool(processes = num_processors)\n    p.map(workers.run(used_device), [i for i in range(0,3)]) \n<\/code>\nCan you tell me where the error comes from?\nNo no no, I don\u2019t think you understand the post. The correct thing is to do is to create a new source file worker.py with all your code in it and then in Jupyter Notebook you call worker.run(\"cuda\"). You don\u2019t need to use multiprocessing manually.\nThat\u2019s exactly what I did. I created a new file workers.py. The data class and run function are in a separate file workers.py which is loaded in the jupyter notebook as import workers.\nI did a mistake using multiprocessing pool here. Thanks for notifying me. Now it works smoothly. Cheers."},{"x":"<code class=\"lang-auto\">#model1\ndef getmodel():\n    model = EfficientNet.from_pretrained('efficientnet-b5')\n    model._fc = nn.Sequential(\n         nn.Linear(in_features=2048, out_features=1024, bias=True), #first layer \n         nn.ReLU(),\n         nn.BatchNorm1d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True),\n         nn.Dropout(p=0.5),\n         nn.Linear(in_features=1024, out_features=4, bias=True)) #last layer\n    model = model.to(device)\n    return model\n<\/code>\nI have lot of question from the above code block which I want to clarify.\n\nI have read that bias should be True (bias=True) at the last linear layer. And my model also performed well when turned on.\nMost people suggested that bias should be turned off (bias=False) before using batch norm ( Even bias in the Conv layers of EfficientNet are turned off before batch norm). But my model performed badly when I turned off the bias at the first layer. What should I follow?\nThis model (model2) just uses the single linear layer.\n\n<code class=\"lang-auto\">#model2\ndef getmodel():\n   model = EfficientNet.from_pretrained('efficientnet-b5')\n   model._fc = nn.Sequential(\n        nn.Linear(in_features=2048, out_features=4, bias=True)) #last layer\n   model = model.to(device)\n   return model\n<\/code>\nWhcih approch should I follow model1 or model2 ?","y":"Hi,\n\nThe thing is that in your case, you have a ReLU between the Linear and Batchnorm. So that statement may not be true for your model.\nI think that statement comes from the fact that the batchnorm will center the values. So a bias is useless in the previous layer as it will just be cancelled by the batchnorm.\nmodel2 has much less parameters. So it will perform differently. It will depend on your dataset though which one is better.\n","z":"Hi,\n\nThe thing is that in your case, you have a ReLU between the Linear and Batchnorm. So that statement may not be true for your model.\nI think that statement comes from the fact that the batchnorm will center the values. So a bias is useless in the previous layer as it will just be cancelled by the batchnorm.\nmodel2 has much less parameters. So it will perform differently. It will depend on your dataset though which one is better.\n\nSo just to clarify, If there is an activation layer(relu) in between then there is no need to turn off the bias in layer one. Right.\nI have this doubt that linear layer at the end (from 2048 > 4 output class) learns less compared with (2048 > 1024 > 4 output class). If yes in what situation this is helpful? Why most people choose model2 like architecture during fine-tuning?\nThis model2 is not as powerful, so it might be worst on the training set.\nBut because it has less parameters (and is linear) the model it is gonna learn is much simpler and so is naturally regularized (due to the structure of the function). This could be an advantage depending on your task and could lead to better validation accuracy in some cases."},{"x":"After performing Global Average Pooling, I have (N, C=3, 1, 1) (reshaped to (N, C=3, 1)) dimensional features, which I would like to pass to a linear layer. My desired output is of size (N, C, 1) or (N, C). However, I\u2019m unsure of what dimensions to use for out_features.\nDimensions-wise, nn.Linear(1, 1) returns the correct dims, but I\u2019m not sure it makes logical sense. If I\u2019m not mistaken, features of different classes will be sharing the same weight, and this will be treated as a binary problem instead of a 3-class one. How do I perform nn.Linear preserving both the correct output dimensions and the multi-class nature of the problem?\nThanks!","y":"With nn.Linear you\u2019re doing matrix multiplication: (N,n_in) @ (n_in,n_out) +bias = (N,n_out). Your n_out should be C(=3), unless you want to rescale+shift channels by same amounts. (n_in=1,n_out=3) makes almost no sense, this leaves two possibilities: 1)linear transformation 3x3, as done by nn.Linear(3,3) or nn.Conv1d(3,3,1) 2)independent channel transformations - nn.Linear is not suitable for this, but simple w*x+b expression does this; another fancy way to do this is is nn.Conv1d(3,3,1,groups=3) layer.\nAbout shapes, for nn.Linear you should reshape input to (N,C) (or permute NCL -> NLC for L>1). And transformatons with nn.ConvXd work without reshape.","z":"With nn.Linear you\u2019re doing matrix multiplication: (N,n_in) @ (n_in,n_out) +bias = (N,n_out). Your n_out should be C(=3), unless you want to rescale+shift channels by same amounts. (n_in=1,n_out=3) makes almost no sense, this leaves two possibilities: 1)linear transformation 3x3, as done by nn.Linear(3,3) or nn.Conv1d(3,3,1) 2)independent channel transformations - nn.Linear is not suitable for this, but simple w*x+b expression does this; another fancy way to do this is is nn.Conv1d(3,3,1,groups=3) layer.\nAbout shapes, for nn.Linear you should reshape input to (N,C) (or permute NCL -> NLC for L>1). And transformatons with nn.ConvXd work without reshape."},{"x":"Hi all!\nI have two datasets, dataset 1 has  \u00b1400 samples and dataset 2  \u00b11000 samples. I want to train my network such that it sees one sample from dataset 1 for every 5 samples of dataset 2 (roughly).\nWith a batch size of 32, each batch containing 5 samples from dataset 1 and 27 from dataset 2 would work. How would I go about setting up a DataLoader to achieve this? I want to definitely train on all 1000 samples from dataset 2 every epoch, and just randomly sample however much I need from dataset 2 to achieve that distribution.\nI looked at ConcatDataset but this just combines the two into one large dataset and samples from that. I saw DataLoader has a sampler parameter; could I achieve what I want with a WeightedRandomSampler with link \"https:\/\/pytorch.org\/docs\/stable\/_modules\/torch\/utils\/data\/sampler.html#WeightedRandomSampler\"?\nAny help is appreciated!","y":"One approach would be to create separate DataLoaders with batch_size=5 (small dataset) and =27 (large dataset), respectively.\nIn the outer loop you would iterate the large dataset to make sure you are sampling all data points.\nYou could create an iterator for the small dataset via small_iter = iter(small_loader) before entering the loop and sample the batch from the small dataset.\nOnce you have both batches, you could concatenate them via data = torch.cat((small, large), dim=0) to create your final batch.\nI think the WeightedRandomSampler wouldn\u2019t strictly work, as you need to sample all samples from the large dataset.","z":"One approach would be to create separate DataLoaders with batch_size=5 (small dataset) and =27 (large dataset), respectively.\nIn the outer loop you would iterate the large dataset to make sure you are sampling all data points.\nYou could create an iterator for the small dataset via small_iter = iter(small_loader) before entering the loop and sample the batch from the small dataset.\nOnce you have both batches, you could concatenate them via data = torch.cat((small, large), dim=0) to create your final batch.\nI think the WeightedRandomSampler wouldn\u2019t strictly work, as you need to sample all samples from the large dataset.\nThanks! smart solution"},{"x":"I am using Coco Dataset\u2019s pictures and mask image that I crreated with the below script to do sematic segmentation.\nWhy is my cross entropy loss function returning zero after a few dozen pictures?\n\n<code class=\"lang-auto\">\ndef make_palatte (classes):\n\tplt_dict = {0: [\"backgorund\", (0,0,0)]}\n\n\tall_colors_map = []\n\tpalette = []\n\n\tfor i in range(255):\n\t\tfor ii in range(255):\n\t\t\tfor iii in range(255):\n\t\t\t\tadding = [i, ii, iii]\n\t\t\t\tif adding ==[0,0,0] or adding == [255, 255, 255]:\n\t\t\t\t\tpass # [\"backgorund\", (0,0,0)] \n\t\t\t\tall_colors_map.append(adding)\n\t\t\t\n\n\tdistance = len(all_colors_map)\/(len(classes)+10)  # buffer\n\tdistance = math.floor(distance)\n\tfor one_class in classes:\n\t\t#print(one_class)\n\t\tid = one_class[\"id\"] #starts with 1 \n\t\tname = one_class[\"name\"] # word\n\t\tcolor = all_colors_map[int(id)*distance]\n\t\tpalette.extend(color)\n\t\tplt_dict[id] = [name, tuple(color)]\n\n\t\t# 1 already taken by background\n\n\tpalette.extend([255,255,255])\n\tplt_dict[len(plt_dict)+2] =[\"ambiguous\", (255,255,255)] \n\t\n\treturn plt_dict, palette\n\ndef mask_maker (palette_dict, img_id, height, width, palette, segmentation, export_dir):\n\t\n\tim = Image.new(\"P\", (height, width), color=(0,0,0)) # 0 0 0 >> background\n\tim.putpalette(palette)\n\td =  ImageDraw.Draw(im)\n\tif len(segmentation) == 0:\n\t\tim.save(export_dir)\n\t\treturn\n\n\tfor segment in segmentation: #a[\"segmentation\"] = xy coordinates\n\t\t#print(segment[\"segmentation\"])\n\t\txy_tup_list = []\n\t\tcategory_id = int(segment[\"category_id\"])\n\t\tif len(segment[\"segmentation\"]) == 0:\n\t\t\tim.save(export_dir)\n\t\t\treturn \n\t\tfor idx, point in enumerate(segment[\"segmentation\"][0]):\n\t\t\tif idx % 2 == 0: #STARTS WITH 0\n\t\t\t\tx = point\n\t\t\tif idx % 2 !=0:\n\t\t\t\ty = point\n\t\t\t\txy_tup_list.append((x, y))\n\t\t\t\tx = None\n\t\t\t\ty = None\t\t\n\t\td.polygon(xy_tup_list, fill=category_id)\n\t\t#d.polygon(xy, fill=category_id)\n\tim.save(export_dir)\n<\/code>\nI randomly picked 1000train pictures and 600 val from COCO 2014 dataset excluding ones that have iscrowd=1. I am trying to do semantic segmentation on those 90 coco classes+ background.\nI am using Pyramid Scenen Parsing Network which I pretty much copied from here except for the dataloader. https:\/\/github.com\/YutaroOgawa\/pytorch_advanced\/blob\/master\/3_semantic_segmentation\/3-7_PSPNet_training.ipynb.\nThis original model did fine tuning with VOC pascal dataset. but i am trying to use COCO datset instead.\nTo create the mask, I basically drew polygon with PIL\u2019s drawimage and assigned value by using \u201cP\u201d mode and original color pallet that has 90 colors and corresponding numbers.\nbelow is my train function\n<code class=\"lang-auto\">def train_model(net, dataloaders_dict, criterion, scheduler, optimizer, num_epochs):\n\n\n    device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n    print(\"available\uff1a\", device)\n    net.to(device)\n\n    torch.backends.cudnn.benchmark = True\n\n\n    num_train_imgs = len(dataloaders_dict[\"train\"].dataset)\n    num_val_imgs = len(dataloaders_dict[\"val\"].dataset)\n    batch_size = dataloaders_dict[\"train\"].batch_size\n\n\n    iteration = 1\n    logs = []\n\n    # multiple minibatch\n    batch_multiplier = 3\n\n    for epoch in range(num_epochs):\n\n        t_epoch_start = time.time()\n        t_iter_start = time.time()\n        epoch_train_loss = 0.0  \n        epoch_val_loss = 0.0  \n\n        print('-------------')\n        print('Epoch {}\/{}'.format(epoch+1, num_epochs))\n        print('-------------')\n\n        for phase in ['train', 'val']:\n            if phase == 'train':\n                net.train()  \n                scheduler.step()  \n                optimizer.zero_grad()\n                print('\uff08train\uff09')\n\n            else:\n                if((epoch+1) % 5 == 0):\n                    net.eval()   \n                    print('-------------')\n                    print('\uff08val\uff09')\n                else:\n                    continue\n\n            count = 0  # multiple minibatch\n            for imges, anno_class_imges in dataloaders_dict[phase]:\n                \n                if imges.size()[0] == 1:\n                    continue\n\n\n                imges = imges.to(device)\n                anno_class_imges = torch.squeeze(anno_class_imges)\n                anno_class_imges = anno_class_imges.to(device)\n\n                \n                # multiple minibatch\n                if (phase == 'train') and (count == 0):\n                    optimizer.step()\n                    optimizer.zero_grad()\n                    count = batch_multiplier\n\n          \n                with torch.set_grad_enabled(phase == 'train'):\n                    outputs = net(imges)\n                    loss = criterion(\n                        outputs, anno_class_imges.long()) \/ batch_multiplier\n                    print(\"loss:  \" loss)\n                    \n                    # \n                    if phase == 'train':\n                        loss.backward()  #\n                        count -= 1  # multiple minibatch\n\n                        if (iteration % 10 == 0):  \n                            t_iter_finish = time.time()\n                            duration = t_iter_finish - t_iter_start\n                            print('iteration {} || Loss: {:.4f} || 10iter: {:.4f} sec.'.format(\n                                iteration, loss.item()\/batch_size*batch_multiplier, duration))\n                            t_iter_start = time.time()\n\n                        epoch_train_loss += loss.item() * batch_multiplier\n                        iteration += 1\n\n\n                    else:\n                        epoch_val_loss += loss.item() * batch_multiplier\n\n       \n        t_epoch_finish = time.time()\n        print('-------------')\n        print('epoch {} || Epoch_TRAIN_Loss:{:.4f} ||Epoch_VAL_Loss:{:.4f}'.format(\n            epoch+1, epoch_train_loss\/num_train_imgs, epoch_val_loss\/num_val_imgs))\n        print('timer:  {:.4f} sec.'.format(t_epoch_finish - t_epoch_start))\n        t_epoch_start = time.time()\n\n\n        log_epoch = {'epoch': epoch+1, 'train_loss': epoch_train_loss \/\n                     num_train_imgs, 'val_loss': epoch_val_loss\/num_val_imgs}\n        logs.append(log_epoch)\n        df = pd.DataFrame(logs)\n        df.to_csv(\"log_output.csv\")\n\n\n    torch.save(net.state_dict(), 'weights\/pspnet50_' +\n               str(epoch+1) + '.pth')\nnum_epochs = 30\ntorch.cuda.empty_cache()\ntrain_model(net, dataloaders_dict, criterion, scheduler, optimizer, num_epochs=num_epochs)\n\n<\/code>","y":"I normalized the target and original image data and it somehow solved the problem which I don\u2019t quiet understand.  closing the issue.","z":"Could you explain your use case a bit, please?\nHow did you create the masks? Which values do they have and which shape?\nWhat kind of model are you using and how many data samples are you using?\nThank you so much. I added some more information.\nThis is one of my mask images. What is kind of curious is that I have to do squeeze to get rid of one dimension before feeding it to GPU, which the original model did not have to do. Could this be related?\nCOCO_train2014_000000581097480\u00d7640 2.48 KB with link \"https:\/\/discuss.pytorch.org\/uploads\/default\/original\/3X\/7\/5\/758be0cd75643566a1503f6f56baee367421b92d.png\"\n\n\n\n 7029279:\n\nWhat is kind of curious is that I have to do squeeze to get rid of one dimension before feeding it to GPU, which the original model did not have to do.\n\n\nThat sounds weird.\nWhich dimension are you squeezing and which shape does the target have before squeezing it?\nThe expected shape for a multi-class segmentation for the target is [batch_size, height, width] and the target tensor should contain the class indices in the range [0, nb_classes-1], if you are using nn.CrossEntropyLoss.\nThis is pre-squeeze size of my target data>> targets of size: : [4, 1, 300, 300] squeeze reduces this to [4,300,300]\nalso I noticed that the size of the target image does not match the roiginal image.\nAlso a side question. Is there any way to make masks out of xy coordinates? do people code tehir own script everytime?\n\nWhat min and max values does the target contain?\nThe shape looks OK after you squeeze it, if target is a LongTensor and contains the class indices in the range  [0, nb_classes-1].\n\n\n\n 7029279:\n\nAlso a side question. Is there any way to make masks out of xy coordinates? do people code tehir own script everytime?\n\n\nI would guess that you could find some good implementations in this discussion board or alternatively for numpy (which should be easily portable to PyTorch) \nI dont know how to interpret these.\nprint(\"max: \", torch.max(anno_class_imges), \"min: \", torch.min(anno_class_imges))\nmax:  tensor(0.3294) min:  tensor(0.)\nthis is my datatransoform\n<code class=\"lang-auto\">\n    def __init__(self, input_size):\n        self.data_transform = transforms.Compose([\n                    transforms.Resize((input_size, input_size)),\n                    transforms.ToTensor()\n                ])\n        \n<\/code>\nAre you transforming the target image with the provided transformation or just the data tensor?\nIn the former case, you will get a normalized target, which is wrong for a segmentation use case.\nYou would have to map the colors of your target to class indices.\nE.g. \u201cblue\u201d -> class0, \u201cred\u201d -> class1 etc.\nHere with link \"https:\/\/discuss.pytorch.org\/t\/training-semantic-segmentation\/49275\/4\" is an example on how to create a color mapping.\n\n\n\n ptrblck:\n\nclass1\n\n\nthe guy I copied from yes does normalize the data, but I did not, so I just commented out those lines\u2026 I just made PIL image into tensors. I did map 90 colors  + background in the range of 0, 0, 0 to 255, 255, 255 and I did PIL\u2019s putlpalette to each mask image I was creating.\n\n\n\n ptrblck:\n\nHere with link \"https:\/\/discuss.pytorch.org\/t\/training-semantic-segmentation\/49275\/4\" is an example on how to create a color mapping.\n\n\nShould my torch max and min values have integer values, instead of float? I thought I assigned numbers ranging from 0 to 91 in my color map. Also, you are using cmap in matplolib instead of PIL, is using PIL not reccomended?\n\n\n\n 7029279:\n\nShould my torch max and min values have integer values, instead of float?\n\n\nYes, the target should be a LongTensor. The min values should be 0, while the max value should be nb_classes-1.\n\n\n\n 7029279:\n\nAlso, you are using cmap in matplolib instead of PIL, is using PIL not reccomended?\n\n\nNo, use whatever works for you. I\u2019m probably more familiar with the matplotlib color map than with PIL, but don\u2019t have any recommendations.\nthank you so much.You have been very kind. I\u2019ll look into my code a little bit more.\nI normalized the target and original image data and it somehow solved the problem which I don\u2019t quiet understand.  closing the issue."},{"x":"My single fold model performed well(unseen test data) with a train validation slit of 80%,20% respectively.\nHere are more details about the single fold model.\nSeed  = 42, batch_size = 16, Epoch = 15, StepLR (step_size=5,factor=0.1), TTA = 6,\nImage_shape( 320x320), LR = 0.0005.\nNow I want to implement StratifiedKfold with n=5. I implemented with the same combination above to the StratifiedKfold and it performed worse than the single fold model(unseen test data).\nI know the problem is with Epoch, StepLR( step_size) and TTA.\nMy question is,\n\nWhether I should use TTA for each fold or not? ( because I heard that averaging 2 weaker models is better than 2 best model something like that)\nWhat should be my epoch range? or should I follow epoch=15?\nWhat should be the step_size for stepLR?\n","y":"\nI think averaging uncorrelated outputs could yield a performance gain. Generally you could take the average of a lot of weak models, if their performance is at least better than a random guess.\n\nFor 2. and 3. I would refer to \u2019s post on cross validation with link \"https:\/\/sebastianraschka.com\/faq\/docs\/evaluate-a-model.html\".","z":"\nI think averaging uncorrelated outputs could yield a performance gain. Generally you could take the average of a lot of weak models, if their performance is at least better than a random guess.\n\nFor 2. and 3. I would refer to \u2019s post on cross validation with link \"https:\/\/sebastianraschka.com\/faq\/docs\/evaluate-a-model.html\"."},{"x":"Hi,\nI have got a task (video analysis) where I have to divide the last fully connected layer of (1+2)D ResNet into multiple chunks and perform classification loss calculation on each individual chunk using CrossEntropyLoss. With more context, I\u2019ve reshaped my model\u2019s last layer (FC) to generate the output of shape (64, 4, 21) where 64 is the batch size, 4 is the number of chunks and 21 is the number of classes to perform classification. So basically my model is computing 4 class values for each video segment. I am computing loss in the following manner:\n<code class=\"lang-python\">def calculate_loss(output, target, criterion):\n    loss = 0\n    for i in range(output.size(1)):\n        loss += criterion(output[:, i, :], target[:,i])\n    return loss # \/ output.size(1)\n<\/code>\nWhere I am iterating over each chunk and calculate the classification loss. My model trains well for a few batches (150\/2500) but then loss and accuracy both stagnate. And when I check the output of each chunk it produces homogeneous classification values (almost same class), regardless of the target having different classes in each chunk.\nQuestions:\n\nIs this kind of loss calculation is permissible?\nHow does the backdrop happen in this scenario?\n","y":"Hello Yash!\n\n\n\n yash1994:\n\nI am computing loss in the following manner:\n<code class=\"lang-python\">def calculate_loss(output, target, criterion):\n    loss = 0\n    for i in range(output.size(1)):\n        loss += criterion(output[:, i, :], target[:,i])\n    return loss # \/ output.size(1)\n<\/code>\n\n\nAs an aside, you can use the \u201cK-dimensional case\u201d feature of\nCrossEntropyLoss (if you\u2019re using a recent enough version of\npytorch) to eliminate your for loop.\n<code class=\"lang-auto\">output.size (1) * torch.nn.CrossEntropyLoss() (output.transpose (1, 2), target)\n<\/code>\nshould yield the same result as your calculate_loss function.\n\n\nQuestions:\n\nIs this kind of loss calculation is permissible?\n\n\n\nYes, there is nothing in principle wrong with what you are doing.\n\n\n\nHow does the backdrop happen in this scenario?\n\n\n\nThe same as usual.  output is a differentiable function of your model\nparameters, the loss in your loop for each chunk is a differentiable\nfunction of output, and summing loss over the four chunks is\ndifferentiable, so the gradients backpropagate through your summed\nloss all the way back to your model parameters.\n\n\nMy model trains well for a few batches (150\/2500) but then loss and accuracy both stagnate. And when I check the output of each chunk it produces homogeneous classification values (almost same class), regardless of the target having different classes in each chunk.\n\n\nMany things could cause this, but it\u2019s not caused directly by your\n\u201cchunked-loss\u201d function.  There could be some bug in your model,\nor it could just be that the problem you\u2019re working on is hard (or\nimpossible), and the way your training data and model interact\nmakes training difficult.\nBest.\nK. Frank","z":"Hello Yash!\n\n\n\n yash1994:\n\nI am computing loss in the following manner:\n<code class=\"lang-python\">def calculate_loss(output, target, criterion):\n    loss = 0\n    for i in range(output.size(1)):\n        loss += criterion(output[:, i, :], target[:,i])\n    return loss # \/ output.size(1)\n<\/code>\n\n\nAs an aside, you can use the \u201cK-dimensional case\u201d feature of\nCrossEntropyLoss (if you\u2019re using a recent enough version of\npytorch) to eliminate your for loop.\n<code class=\"lang-auto\">output.size (1) * torch.nn.CrossEntropyLoss() (output.transpose (1, 2), target)\n<\/code>\nshould yield the same result as your calculate_loss function.\n\n\nQuestions:\n\nIs this kind of loss calculation is permissible?\n\n\n\nYes, there is nothing in principle wrong with what you are doing.\n\n\n\nHow does the backdrop happen in this scenario?\n\n\n\nThe same as usual.  output is a differentiable function of your model\nparameters, the loss in your loop for each chunk is a differentiable\nfunction of output, and summing loss over the four chunks is\ndifferentiable, so the gradients backpropagate through your summed\nloss all the way back to your model parameters.\n\n\nMy model trains well for a few batches (150\/2500) but then loss and accuracy both stagnate. And when I check the output of each chunk it produces homogeneous classification values (almost same class), regardless of the target having different classes in each chunk.\n\n\nMany things could cause this, but it\u2019s not caused directly by your\n\u201cchunked-loss\u201d function.  There could be some bug in your model,\nor it could just be that the problem you\u2019re working on is hard (or\nimpossible), and the way your training data and model interact\nmakes training difficult.\nBest.\nK. Frank\nThanks for reply . I looked at data more carefully and found some anomalies in data itself so that might be the reason for poor training. Thanks again for clearing my doubts."},{"x":"Hello, I\u2019m quite new to Pytorch. I was wondering how I could convert my tensor of size torch.Size([1, 3, 224, 224]) to display in an image format on a Jupyter notebook. A PIL format or a CV2 format should be fine.\nI tried using transforms.ToPILImage(x) but it resulted in a different format like this: ToPILImage(mode=ToPILImage(mode=tensor([[[[1.3034e-16, 1.3034e-16, 1.3034e-16,  ..., 1.4475e-16,. Maybe I\u2019m doing something wrong ","y":"ToPILImage() should work.\nI\u2019m not sure what you are passing as the mode argument, but this small code snippet works fine:\n<code class=\"lang-python\">x = torch.randn(1, 3, 224, 224)\ntrans = torchvision.transforms.ToPILImage()\nout = trans(x[0])\nout.show()\n<\/code>","z":"ToPILImage() should work.\nI\u2019m not sure what you are passing as the mode argument, but this small code snippet works fine:\n<code class=\"lang-python\">x = torch.randn(1, 3, 224, 224)\ntrans = torchvision.transforms.ToPILImage()\nout = trans(x[0])\nout.show()\n<\/code>\nThanks a lot "},{"x":"I am getting the previous error when I use the following line of code:\n<code class=\"lang-auto\">model=torchvision.models.segmentation.deeplabv3_resnet50(pretrained=True)\n<\/code>","y":"This model was added 8 days ago in this PR with link \"https:\/\/github.com\/pytorch\/vision\/commit\/bf843c664b8ba0ff49d2921237500c77d82f2d04\", so you could install the nightly binary or build from source to use it. ","z":"This model was added 8 days ago in this PR with link \"https:\/\/github.com\/pytorch\/vision\/commit\/bf843c664b8ba0ff49d2921237500c77d82f2d04\", so you could install the nightly binary or build from source to use it. \nHow can I finetuning this model??   This model return an ordered dict where key out has the output image but values are not 0 or 1 in binary segmentation!\nThe model wasn\u2019t trained on a binary segmentation use case, but for 20 classes as stated in the docs:\n\nThe pre-trained models have been trained on a subset of COCO train2017, on the 20 categories that are present in the Pascal VOC dataset. You can see more information on how the subset has been selected in  references\/segmentation\/coco_utils.py .\n\nMost likely you will get an activation containing the logits for these 20 classes (in dim1).\nIf you are dealing with a binary segmentation use case, you could set num_classes=2 (for a binary multi-class segmentation using nn.CrossEntropyLoss) or num_classes=1 (for a binary segmentation using nn.BCEWithLogitsLoss) in the instantiation of the model.\nUnderstood, I got this working this morning.\nHowever, I tried to make it work with resnet34 as backbone and it throws me Dilatation>1 not supported.\nThere is a pull request that solves this:\n\n\ngithub.com\/pytorch\/vision with link \"https:\/\/github.com\/pytorch\/vision\/pull\/2115#commits-pushed-e0f24f5\"\n\n\n\n\n\n\n\n\nResNet BasicBlock dilation with link \"https:\/\/github.com\/pytorch\/vision\/pull\/2115\"\n\n\npytorch:master \u2190 christian-rauch:resnet_dilation\n\n\n\n        opened 07:05PM - 17 Apr 20 UTC\n\n\nchristian-rauch with link \"https:\/\/github.com\/christian-rauch\"\n\n\n+2\n-4 with link \"https:\/\/github.com\/pytorch\/vision\/pull\/2115\/files\"\n\n\n\n\n\n\n\n\n\n\n\n\n ptrblck:\n\nThe pre-trained models have been trained on a subset of COCO train2017, on the 20 categories that are present in the Pascal VOC dataset. You can see more information on how the subset has been selected in references\/segmentation\/coco_utils.py .\n\n\nCould the backbone weights work with my problem? So just deeplab head weights need to get retrained!\nIs this right?\nThat should be possible, yes.\n\n\n\n ptrblck:\n\ncould install the nightly binary or build from source to use it.\n\n\n!pip install git+https:\/\/github.com\/pytorch\/vision.git\nIs not working! How can i install just turchvision nightly?\nPlease refer to the install instructions with link \"https:\/\/pytorch.org\/get-started\/locally\/\".\nIt looks like pretrained deeplabv3_resnet50 have been added to torchvision 0.6. However, when I try to use them it says that they is not implemented\nCould you post the installed torchvision version via print(torchvision.__version__)?\n\n\n\n ptrblck:\n\nprint(torchvision.version)\n\n\nThis is the version that gets printed: 0.6.0a0+82fd1c8\nThis is a pre-release version, so you would have to update to 0.6 or the nightly binary.\n\n\n\nInstalling torchaudio with pytorch 1.5.0 lead to downgrade with link \"https:\/\/discuss.pytorch.org\/t\/installing-torchaudio-with-pytorch-1-5-0-lead-to-downgrade\/77757\/8\"\n\n\nHi,\nWe updated all the linux binaries now. So all should work fine. Can you double check that it works on your side?\n\n\nIt was installed from pytorch channel on anaconda\nafter the solution. How can I install it properly?\nI installed from pip now and says: 0.6.0.\nHowever, I am getting same error: NotImplementedError: pretrained deeplabv3_resnet50_coco is not supported as of now\nYou are right. The model didn\u2019t make it into 0.6.0 and missed the code freeze apparently by a day, so you would still need to install the latest nightly."},{"x":"Hello,\nI have a dataset composed of 3D volumes, small enough to be loaded in memory.\nI need to process these volumes a batch of slices at a time (GPU memory restrictions)\nWhat would be a clean way to go about this?\nMy current solution is subclassing torch.utils.data.Dataset to return one volume at a time, then getting slices of the returned volume manually. But this just feels like a somewhat hacky way to create batches\u2026\nCode snipped for the Dataset:\n<code class=\"lang-auto\">class My_Dataset(Dataset):\n\n    def __init__(self, paths):\n        \"\"\"\n        Args:\n        paths: list of paths to the volumes\n        \"\"\"\n        self.paths = paths\n        self.load_everything_in_memory()\n\n    def __len__(self):\n        return len(self.images)\n\n    def __getitem__(self, idx):\n        image, mask = self.images[idx], self.masks[idx]\n        return image, mask\n\n    def load_everything_in_memory(self):\n        \"\"\"\n        self.images = list of volumes\n        self.masks = labels for those volumes\n        \"\"\"\n        images, masks = [], []\n        for path in self.paths:\n            # do some loading\n            # get image, mask\n            images.append(image)\n            masks.append(mask)\n            \n        self.images = images\n        self.masks = masks\n<\/code>\nIt feels like it should be some better way of doing this.\nThanks in advance","y":"This sounds like a valid approach.\nAlternatively, you could of course create a tensor or list with all slices and use the index to grab a slice and create the desired batch from it, but I guess load_everything_in_memory might then become a bit more complicated.\nYou could try to use torch.utils.checkpoint to trade compute for memory, but I\u2019m not sure how large your memory requirement is for a single batch.","z":"This sounds like a valid approach.\nAlternatively, you could of course create a tensor or list with all slices and use the index to grab a slice and create the desired batch from it, but I guess load_everything_in_memory might then become a bit more complicated.\nYou could try to use torch.utils.checkpoint to trade compute for memory, but I\u2019m not sure how large your memory requirement is for a single batch.\nThanks, for your reply! I suppose its good enough for now as it is, I can always look to optimise later if I really need to."},{"x":"Hello everyone!\nI am creating my own custom image dataset using torchs Dataset class.\nSo far, I iterate through all .jpg files in a given folder and store them as a list by appending.\nThis costs a lot of working memory + it takes ages to load the dataset.\nI was wondering what a smart way is to load the images? What is considered good practice when working with a lot of images?\nI was thinking about storing the paths where the images are (as .csv \/ .json) and only load the images in the def getitem(self, idx): method, when they are actually needed.\nWhat\u2019s the most efficient way?\nThanks for any suggestions!","y":"The second one is actually the way that -for example- ImageFolder dataset class load the images. So, the best practice is load the path directory where your images are ( for example with os.listdir() ) , and load the image in __getitem__ method","z":"The second one is actually the way that -for example- ImageFolder dataset class load the images. So, the best practice is load the path directory where your images are ( for example with os.listdir() ) , and load the image in __getitem__ method"},{"x":"Hi, I am trying to implement the lottery ticket hypothesis using torch.nn.utils.prune but I am having trouble trying to reset the model parameters. From what I understand, the pruning module adds a new parameter, weight_orig to each module by overriding weight. When I try to reinitialize the model parameters with model.apply(reinit_weights), the parameters corresponding to weight_orig in each module is not changing. Below is some code that demonstrates. Does someone have an explanation or potential workaround?\nThanks so much in advance.\n<code class=\"lang-auto\"># this allows the model to train with the new optimizer\n# but doesn't reset parameters that have the mask added so fails the test_model_change\ndef reinit_weights(m):\n    if isinstance(m, nn.Conv2d) or isinstance(m, nn.Linear):\n        m.reset_parameters()\n\ndef test_model_change(prev_iter_dict, model):\n    for name, param in model.named_parameters():\n        prev_param = prev_iter_dict[name]\n        assert not torch.allclose(prev_param,param), 'model not updating'\n\nmodel = LeNet5()\noptimizer = optim.Adam(model.parameters(), lr=lr)\nmodel.train().to(device)\n\npruner = L1Unstructured(0.0)\n# adds masks of all ones to each of the layers\nfor n, m in model.named_modules():\n    if isinstance(m, torch.nn.Conv2d):\n        pruner.apply(m, name='weight', amount=0.0)\n    if isinstance(m, torch.nn.Linear):\n        pruner.apply(m, name='weight', amount=0.0)\n\ntrain(model,1,train_loader,device,optimizer) # train for one epoch to test\nfull_cap = copy.deepcopy(model.state_dict())\nmodel.apply(reinit_weights) # this reinitalizes the weight parameter but doesn't change the new_weight parameter\ntest_model_change(full_cap, model) # passes for conv0.bias but fails for conv0.weight\n<\/code>","y":"Hi,\nI actually managed to hack a workaround with the tools in torch.nn.utils.prune. The trick is save the dictionary of masks and remove the pruning modules with remove and then add them back with CustomFromMask.\n<code class=\"lang-auto\">from torch.nn.utils.prune import l1_unstructured, remove, CustomFromMask,is_pruned\n\n\ndef add_masks(model,masks):\n    mask_pruner = CustomFromMask(None)\n    for module_name, module in model.named_modules():\n        key = f\"{module_name}.weight_mask\"\n        if key in masks:\n            if isinstance(module, torch.nn.Conv2d):\n                _mask = masks[key]\n                mask_pruner.apply(module, 'weight', _mask)\n            if isinstance(module, torch.nn.Linear):\n                _mask = masks[key]\n                mask_pruner.apply(module, 'weight', _mask)\n\ndef merge_masks(model):\n    for n, m in model.named_modules():\n        if is_pruned(m)==False:\n            continue\n        if isinstance(m, torch.nn.Conv2d):\n            remove(m, name='weight')\n        if isinstance(m, torch.nn.Linear):\n            remove(m, name='weight')\n<\/code>\nWith this fix, I can reinitialize my network with no issues. Hopefully this can help someone if they were dealing with my issue.","z":"Personally, I simply save the weights I\u2019d like to re-initialize to and then load the .pt file when its time to re-initialize.\nThank you for your response but this doesn\u2019t work for me. This passes my test_model_change function so the weights do get reinitialized but when I try to retrain my model, the loss does not decrease. Interestingly, when I reset the model to the original weight parameters at the first initialization I don\u2019t face this issue, only when I randomly try to set weights to some values.\n<code class=\"lang-auto\">def reset_weights_rand(model):\n    rand_state_dict = pickle.load(open('rand_dict.pkl', 'rb'))\n    for name, param in model.named_parameters():\n        param.data = rand_state_dict[name]\nrand_state_dict = copy.deepcopy(initial_state_dict)\n\nfor key,value in rand_state_dict.items():\n    if 'bias' in key:\n        init.normal_(value.data)\n    else:\n        init.xavier_normal_(value.data)        \n\nwith open(r\"rand_dict.pkl\", \"wb\") as output_file:\n    pickle.dump(rand_state_dict, output_file)\n\ntrain(model,1,train_loader,device,optimizer) # train for one epoch to test\n\nfull_cap = copy.deepcopy(model.state_dict())\n\nreset_weights_rand(model)\n\n# model.apply(reinit_weights) # this reinitalizes the weight parameter but doesn't change the new_weight parameter\ntest_model_change(full_cap, model) # passes now\n\n# train 90% capacity for 10 more epochs\noptimizer2 = torch.optim.Adam(model.parameters(), lr=lr) # problematic\n\ntrain(model,5, train_loader,device,optimizer2)\n<\/code>\nimage1530\u00d71480 342 KB with link \"https:\/\/discuss.pytorch.org\/uploads\/default\/original\/3X\/b\/1\/b164848d36b5edf7903adf4b84b68bea5ba12e02.png\"\nAs you can see the loss does not decrease even though no pruning as been done.\nHi,\nI actually managed to hack a workaround with the tools in torch.nn.utils.prune. The trick is save the dictionary of masks and remove the pruning modules with remove and then add them back with CustomFromMask.\n<code class=\"lang-auto\">from torch.nn.utils.prune import l1_unstructured, remove, CustomFromMask,is_pruned\n\n\ndef add_masks(model,masks):\n    mask_pruner = CustomFromMask(None)\n    for module_name, module in model.named_modules():\n        key = f\"{module_name}.weight_mask\"\n        if key in masks:\n            if isinstance(module, torch.nn.Conv2d):\n                _mask = masks[key]\n                mask_pruner.apply(module, 'weight', _mask)\n            if isinstance(module, torch.nn.Linear):\n                _mask = masks[key]\n                mask_pruner.apply(module, 'weight', _mask)\n\ndef merge_masks(model):\n    for n, m in model.named_modules():\n        if is_pruned(m)==False:\n            continue\n        if isinstance(m, torch.nn.Conv2d):\n            remove(m, name='weight')\n        if isinstance(m, torch.nn.Linear):\n            remove(m, name='weight')\n<\/code>\nWith this fix, I can reinitialize my network with no issues. Hopefully this can help someone if they were dealing with my issue."},{"x":"If a model is trained by image after normalization, is normalization needed for the prediction of one single image? My intuition tells me the result will not be affected without normalization, but I am not sure.","y":"Yeah I think that you should apply normalization to your test image if your model is trained on normalized inputs because then your model may be experiencing some strange input at test time and which may not be so good for the task that you are upto.","z":"Yeah I think that you should apply normalization to your test image if your model is trained on normalized inputs because then your model may be experiencing some strange input at test time and which may not be so good for the task that you are upto."},{"x":"this is a newby question I am asking here but for some reason, when I change the batch size at test time, the accuracy of my model changes. Decreasing the batch size reduces the accuracy until a batch size of 1 leads to 11% accuracy although the same model gives me 97% accuracy with a test batch size of 512 (I trained it with batch size 512). I am using a pretrained resnet 50 model and finetuning it on my own images and I am also using .train() and .eval() at train and test times properly. The best reason that I can come up with is that for some reason, the batch normalization layers in the model are still tracking the batch statistics at test time (which they are not supposed to do, instead they should be using the ones saved during the training) because a batch size of 1 should lead to mean(x) = x, and the output of bn layer will become 0, leading to zero prediction at the output and hence 11% accuracy because 11% data is from class 0. Also when I send in the validation and test loaders unshuffled, I get bad numbers but when I shuffle them, the same sets give me +96% accuracy. Can somebody help me please? Thank you so much!\nAnd one more thing, I have only changed the classifier at the end and didn\u2019t use bn there. The rest of the model is the same standard implementation that comes with pytorch models module","y":"Thank you so much everyone for your help. Steve_cruz with link \"https:\/\/discuss.pytorch.org\/t\/test-accuracy-with-different-batch-sizes\/23049\/4\" helped me solve my error. I retrained my model be removing the last softmax layer since cross entropy loss applies softmax itself. Also I decorated my evaluation function with torch.no_grad() and got the model running. Now it is giving me good accuracy with any batch size. But still those accuracies vary somewhere between 2-3% (93-95%) for different batch sizes for some reason. I\u2019ll try to find some fix for that. Thanks for your time everyone!","z":"\n\n\n Annus_Zulfiqar:\n\nAnd one more thing, I have only changed the classifier at the end and didn\u2019t use bn there. The rest of the model is the same standard implementation that comes with pytorch models module\n\n\nI can\u2019t give exact solution.\nBut here is my few suggestions\n1.Check each class level accuracy.Gives you for different test batches what are things the model not doing well.\nhttps:\/\/pytorch.org\/tutorials\/beginner\/blitz\/cifar10_tutorial.html\n<code class=\"lang-auto\">class_correct = list(0. for i in range(10))\nclass_total = list(0. for i in range(10))\nwith torch.no_grad():\n    for data in testloader:\n        images, labels = data\n        outputs = net(images)\n        _, predicted = torch.max(outputs, 1)\n        c = (predicted == labels).squeeze()\n        for i in range(4):\n            label = labels[i]\n            class_correct[label] += c[i].item()\n            class_total[label] += 1\n\n\nfor i in range(10):\n    print('Accuracy of %5s : %2d %%' % (\n        classes[i], 100 * class_correct[i] \/ class_total[i]))\n<\/code>\n2.If you dont shuffle you will get bad numbers.Because you see all the labels in your test and train sets.Sometimes the labels will be in order of the classes. which will lead your model to see same class until sometime(Eg:1-100) and then some other class(Eg: 100-200) which will lead to catastrophic forgetting.It\u2019s always better to send as shuffled batches\nThank you for your time and answer but I think I forgot to mention a few details. Firstly, I trained the model with shuffled training set, only the validation and test set were giving me different results with and without shuffling the validation and test loaders. Secondly, my model has more than 97% accuracy on each individual class; it\u2019s only when I change the batch size at test time and my test accuracy keeps  on changing and the model performs poorly with a batch size of 1 at test time.\nHave you set model to eval mode?  [model.eval()]\nIf so, Can you write the model, test code snippets here?\nyes. It is set to .eval(). Let me write the code here! thanks for your time\n<code class=\"lang-auto\">class ResNet(nn.Module):\n\"\"\"\n    Get a pretrained VGG network (on ImageNet) and try to finetune it on EuroSat images\n    Reported acc is > 98% on Resnet-50, let's see what can we get from a VGG network\n\"\"\"\n\ndef __init__(self, in_channels):\n    super(ResNet, self).__init__()\n    graph = models.resnet50(pretrained=True)\n    removed = list(graph.children())[:-2]\n    with_dropout = []\n    with_dropout.append(removed[0])\n    with_dropout.append(removed[1])\n    with_dropout.append(removed[2])\n    with_dropout.append(removed[3])\n    for part in removed[4:]:\n        with_dropout.append(part)\n        with_dropout.append(nn.Dropout2d(p=0.8))\n    # print(with_dropout)\n    self.feature_extracter = torch.nn.Sequential(*with_dropout)\n    self.kill = nn.Dropout(p=0.8)\n    self.classifier = nn.Sequential(\n        nn.Linear(in_features=2048*4, out_features=1024),\n        nn.ReLU(),\n        nn.Linear(in_features=1024, out_features=512),\n        nn.ReLU(),\n        nn.Dropout(p=0.8),\n        nn.Linear(in_features=512, out_features=256),\n        nn.ReLU(),\n        nn.Linear(in_features=256, out_features=128),\n        nn.ReLU(),\n        nn.Dropout(p=0.8),\n        nn.Linear(in_features=128, out_features=10),\n        nn.LogSoftmax(dim=0)\n    )\n\ndef forward(self, x):\n    x = self.feature_extracter(x)\n    x = self.kill(x)\n    x = self.classifier(x.view(x.size(0), -1))\n    return x, torch.argmax(input=x, dim=1)\n<\/code>\nHere is my training code\u2026\n<code class=\"lang-auto\">def train_net(model, base_folder, pre_model, save_dir, batch_size, lr, log_after, cuda, device):\n    if not pre_model:\n        print(model)\n    writer = SummaryWriter()\n    if cuda:\n        print('GPU')\n        model.cuda(device=device)\n        print('log: training started on device: {}'.format(device))\n    # define loss and optimizer\n    optimizer = Adam(model.parameters(), lr=lr)\n    criterion = nn.CrossEntropyLoss()\n    train_loader, val_dataloader, test_loader = get_dataloaders(base_folder=base_folder,\n                                                                batch_size=batch_size)\n    if not os.path.exists(save_dir):\n        os.mkdir(save_dir)\n\n    if True:\n        i = 1\n        m_loss, m_accuracy = [], []\n        if pre_model:\n            # self.load_state_dict(torch.load(pre_model)['model'])\n            model.load_state_dict(torch.load(pre_model))\n            print('log: resumed model {} successfully!'.format(pre_model))\n            print(model)\n\n            # starting point\n            # model_number = int(pre_model.split('\/')[1].split('-')[1].split('.')[0])\n            model_number = int(re.findall('\\d+', str(pre_model))[0])\n            i = i + model_number - 1\n        else:\n            print('log: starting anew using ImageNet weights...')\n\n        while True:\n            i += 1\n            net_loss = []\n            # new model path\n            save_path = os.path.join(save_dir, 'model-{}.pt'.format(i))\n            # remember to save only five previous models, so\n            del_this = os.path.join(save_dir, 'model-{}.pt'.format(i - 6))\n            if os.path.exists(del_this):\n                os.remove(del_this)\n                print('log: removed {}'.format(del_this))\n\n            if i > 1 and not os.path.exists(save_path):\n                torch.save(model.state_dict(), save_path)\n                print('log: saved {}'.format(save_path))\n\n            correct_count, total_count = 0, 0\n            for idx, data in enumerate(train_loader):\n                ##########################\n                model.train() # train mode at each epoch, just in case...\n                ##########################\n                test_x, label = data['input'], data['label']\n                if cuda:\n                    test_x = test_x.cuda(device=device)\n                    label = label.cuda(device=device)\n                # forward\n                out_x, pred = model.forward(test_x)\n                # out_x, pred = out_x.cpu(), pred.cpu()\n                loss = criterion(out_x, label)\n                net_loss.append(loss.item())\n\n                # get accuracy metric\n                batch_correct = (label.eq(pred.long())).double().sum().item()\n                correct_count += batch_correct\n                # print(batch_correct)\n                total_count += np.float(pred.size(0))\n                if idx % log_after == 0 and idx > 0:\n                    print('{}. ({}\/{}) image size = {}, loss = {}: accuracy = {}\/{}'.format(i,\n                                                                                            idx,\n                                                                                            len(train_loader),\n                                                                                            out_x.size(),\n                                                                                            loss.item(),\n                                                                                            batch_correct,\n                                                                                            pred.size(0)))\n                #################################\n                # three steps for backprop\n                model.zero_grad()\n                loss.backward()\n                # perform gradient clipping between loss backward and optimizer step\n                clip_grad_norm_(model.parameters(), 0.05)\n                optimizer.step()\n                #################################\n            mean_accuracy = correct_count \/ total_count * 100\n            mean_loss = np.asarray(net_loss).mean()\n            m_loss.append((i, mean_loss))\n            m_accuracy.append((i, mean_accuracy))\n\n            writer.add_scalar(tag='train loss', scalar_value=mean_loss, global_step=i)\n            writer.add_scalar(tag='train over_all accuracy', scalar_value=mean_accuracy, global_step=i)\n\n            print('####################################')\n            print('epoch {} -> total loss = {:.5f}, total accuracy = {:.5f}%'.format(i, mean_loss, mean_accuracy))\n            print('####################################')\n\n            # validate model after each epoch\n            eval_net(model=model, writer=writer, criterion=criterion,\n                     val_loader=val_dataloader, denominator=batch_size,\n                     cuda=cuda, device=device, global_step=i)\n    pass\n\n<\/code>\nand finally my test code\u2026\n<code class=\"lang-auto\">def eval_net(**kwargs):\n    model = kwargs['model']\n    cuda = kwargs['cuda']\n    device = kwargs['device']\n    if cuda:\n        model.cuda(device=device)\n    if 'criterion' in kwargs.keys():\n        writer = kwargs['writer']\n        val_loader = kwargs['val_loader']\n        criterion = kwargs['criterion']\n        global_step = kwargs['global_step']\n        correct_count, total_count = 0, 0\n        net_loss = []\n        model.eval()  # put in eval mode first ############################\n        for idx, data in enumerate(val_loader):\n            test_x, label = data['input'], data['label']\n            if cuda:\n                test_x = test_x.cuda(device=device)\n                label = label.cuda(device=device)\n            # forward\n            out_x, pred = model.forward(test_x)\n            loss = criterion(out_x, label)\n            net_loss.append(loss.item())\n\n            # get accuracy metric\n            batch_correct = (label.eq(pred.long())).double().sum().item()\n            correct_count += batch_correct\n            total_count += np.float(pred.size(0))\n        #################################\n        mean_accuracy = correct_count \/ total_count * 100\n        mean_loss = np.asarray(net_loss).mean()\n        # summarize mean accuracy\n        writer.add_scalar(tag='val. loss', scalar_value=mean_loss, global_step=global_step)\n        writer.add_scalar(tag='val. over_all accuracy', scalar_value=mean_accuracy, global_step=global_step)\n        print('$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$')\n        print('log: validation:: total loss = {:.5f}, total accuracy = {:.5f}%'.format(mean_loss, mean_accuracy))\n        print('$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$')\n\n    else:\n        # model, images, labels, pre_model, save_dir, sum_dir, batch_size, lr, log_after, cuda\n        pre_model = kwargs['pre_model']\n        base_folder = kwargs['base_folder']\n        batch_size = kwargs['batch_size']\n        log_after = kwargs['log_after']\n        criterion = nn.CrossEntropyLoss()\n        un_confusion_meter = tnt.meter.ConfusionMeter(10, normalized=False)\n        confusion_meter = tnt.meter.ConfusionMeter(10, normalized=True)\n        model.load_state_dict(torch.load(pre_model))\n        print('log: resumed model {} successfully!'.format(pre_model))\n        _, _, test_loader = get_dataloaders(base_folder=base_folder, batch_size=batch_size)\n        net_accuracy, net_loss = [], []\n        correct_count = 0\n        total_count = 0\n        for idx, data in enumerate(test_loader):\n            model.eval()  # put in eval mode first\n            test_x, label = data['input'], data['label']\n            # print(test_x)\n            # print(test_x.shape)\n            # this = test_x.numpy().squeeze(0).transpose(1,2,0)\n            # print(this.shape, np.min(this), np.max(this))\n            if cuda:\n                test_x = test_x.cuda(device=device)\n                label = label.cuda(device=device)\n            # forward\n            out_x, pred = model.forward(test_x)\n            loss = criterion(out_x, label)\n            un_confusion_meter.add(predicted=pred, target=label)\n            confusion_meter.add(predicted=pred, target=label)\n\n            ###############################\n            # pred = pred.view(-1)\n            # pred = pred.cpu().numpy()\n            # label = label.cpu().numpy()\n            # print(pred.shape, label.shape)\n\n            ###############################\n            # get accuracy metric\n            # correct_count += np.sum((pred == label))\n            # print(pred, label)\n            batch_correct = (label.eq(pred.long())).double().sum().item()\n            correct_count += batch_correct\n            # print(batch_correct)\n            total_count += np.float(batch_size)\n            net_loss.append(loss.item())\n            if idx % log_after == 0:\n                print('log: on {}'.format(idx))\n\n            #################################\n        mean_loss = np.asarray(net_loss).mean()\n        mean_accuracy = correct_count * 100 \/ total_count\n        print(correct_count, total_count)\n        print('$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$')\n        print('log: test:: total loss = {:.5f}, total accuracy = {:.5f}%'.format(mean_loss, mean_accuracy))\n        print('$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$')\n        with open('normalized.pkl', 'wb') as this:\n            pkl.dump(confusion_meter.value(), this, protocol=pkl.HIGHEST_PROTOCOL)\n\n        with open('un_normalized.pkl', 'wb') as this:\n            pkl.dump(un_confusion_meter.value(), this, protocol=pkl.HIGHEST_PROTOCOL)\n\n        pass\n    pass\n\n<\/code>\nas far as I can see,  the code looks correct to me.  Can you please print the intermediate output of some layer  (say, a linear layer) for the same input with different batch size (in eval mode)  to ensure that the model behaves differently with varying batch size?\nsure! just a minute\u2026\nThis is the ouput of the softmax at the final layer for a batch size of 512 (I am omitting some part of it so that the forum let\u2019s me post it\u2026)\n<code class=\"lang-auto\">-2.690155601501464844e+01 -8.082249450683593750e+01 -3.616087722778320312e+01 -3.803614139556884766e+00 -1.360958194732666016e+01 -4.190422439575195312e+01 -1.716705513000488281e+01 -1.744507598876953125e+01 -1.189443874359130859e+01 -4.641435623168945312e+01\n-1.381118583679199219e+01 -1.968629837036132812e+01 -1.586458396911621094e+01 -2.607204627990722656e+01 -3.955638885498046875e+01 -3.761519193649291992e+00 -2.691738891601562500e+01 -3.530610656738281250e+01 -1.903606224060058594e+01 -3.330869674682617188e+01\n-2.380311203002929688e+01 -2.999114608764648438e+01 -4.187136650085449219e+00 -2.493428802490234375e+01 -2.390682220458984375e+01 -1.868707847595214844e+01 -1.381414985656738281e+01 -2.050655364990234375e+01 -2.902218627929687500e+01 -5.156520462036132812e+01\n-4.055553817749023438e+01 -4.072654724121093750e+00 -3.130643081665039062e+01 -5.238175582885742188e+01 -6.296509552001953125e+01 -4.269557952880859375e+01 -5.359529876708984375e+01 -5.323943710327148438e+01 -4.236874389648437500e+01 -3.005449867248535156e+01\n-5.584639358520507812e+01 -8.653271484375000000e+01 -3.353703308105468750e+01 -2.608614921569824219e+01 -1.751148414611816406e+01 -6.213397598266601562e+01 -4.815479278564453125e+01 -4.242191314697265625e+00 -4.553392791748046875e+01 -9.855609893798828125e+01\n-1.203245925903320312e+01 -1.687045478820800781e+01 -1.372485160827636719e+01 -2.218313980102539062e+01 -3.322996139526367188e+01 -3.624969482421875000e+00 -2.281215286254882812e+01 -2.994762802124023438e+01 -1.638219070434570312e+01 -2.809778404235839844e+01\n-3.542594146728515625e+01 -1.036188278198242188e+02 -4.309928512573242188e+01 -1.152541255950927734e+01 -3.487170934677124023e+00 -5.836009597778320312e+01 -2.234853363037109375e+01 -1.220743560791015625e+01 -2.573122215270996094e+01 -7.186666870117187500e+01\n-2.466860389709472656e+01 -3.135066986083984375e+01 -4.166416645050048828e+00 -2.583732986450195312e+01 -2.465964889526367188e+01 -1.941736030578613281e+01 -1.405472564697265625e+01 -2.114776802062988281e+01 -3.017933845520019531e+01 -5.384698104858398438e+01\n-4.127064704895019531e+00 -1.877523803710937500e+01 -1.274282455444335938e+01 -9.385772705078125000e+00 -1.298989200592041016e+01 -1.141229343414306641e+01 -8.155844688415527344e+00 -1.573155784606933594e+01 -8.865839004516601562e+00 -1.400935554504394531e+01\n-4.084252834320068359e+00 -1.908169937133789062e+01 -1.252236747741699219e+01 -9.234300613403320312e+00 -1.258626079559326172e+01 -1.144107055664062500e+01 -7.732525348663330078e+00 -1.544504737854003906e+01 -9.017238616943359375e+00 -1.454797172546386719e+01\n-3.975222396850585938e+01 -4.072445869445800781e+00 -3.078602409362792969e+01 -5.138525772094726562e+01 -6.178840637207031250e+01 -4.187569046020507812e+01 -5.257308197021484375e+01 -5.231076049804687500e+01 -4.155389785766601562e+01 -2.947208976745605469e+01\n-3.123294830322265625e+01 -9.020487976074218750e+01 -3.787038803100585938e+01 -1.056235027313232422e+01 -3.583448171615600586e+00 -5.094428253173828125e+01 -1.996979141235351562e+01 -1.125793361663818359e+01 -2.285365486145019531e+01 -6.285837173461914062e+01\n-1.499895477294921875e+01 -5.100203704833984375e+01 -1.250618553161621094e+01 -1.890448379516601562e+01 -1.810605621337890625e+01 -2.396559906005859375e+01 -3.685824632644653320e+00 -2.261679649353027344e+01 -2.511251831054687500e+01 -5.419168472290039062e+01\n-1.734498023986816406e+01 -2.533816528320312500e+01 -2.026508903503417969e+01 -3.387181854248046875e+01 -5.228700256347656250e+01 -4.035984992980957031e+00 -3.520900726318359375e+01 -4.609833908081054688e+01 -2.428531837463378906e+01 -4.355126571655273438e+01\n-1.231824493408203125e+01 -1.733354187011718750e+01 -1.409014320373535156e+01 -2.281948089599609375e+01 -3.427051925659179688e+01 -3.647494316101074219e+00 -2.349081420898437500e+01 -3.083149337768554688e+01 -1.680347251892089844e+01 -2.891879463195800781e+01\n-4.168481349945068359e+00 -7.396075439453125000e+01 -4.082975769042968750e+01 -2.257537841796875000e+01 -3.564733886718750000e+01 -4.084517288208007812e+01 -1.726690673828125000e+01 -4.535559082031250000e+01 -2.421278953552246094e+01 -4.959044265747070312e+01\n-1.965868186950683594e+01 -5.605549621582031250e+01 -2.587337112426757812e+01 -3.987432479858398438e+00 -1.033438968658447266e+01 -2.947753906250000000e+01 -1.310202026367187500e+01 -1.330910682678222656e+01 -9.715766906738281250e+00 -3.316635131835937500e+01\n-3.479168701171875000e+01 -8.166891479492187500e+01 -5.464433288574218750e+01 -2.850371170043945312e+01 -4.872769927978515625e+01 -5.005518341064453125e+01 -4.506411743164062500e+01 -5.162623596191406250e+01 -3.928067922592163086e+00 -3.166503715515136719e+01\n-5.030834579467773438e+01 -4.067288398742675781e+00 -3.832155227661132812e+01 -6.484884643554687500e+01 -7.798222351074218750e+01 -5.316851043701171875e+01 -6.647455596923828125e+01 -6.537792968750000000e+01 -5.237901306152343750e+01 -3.686334609985351562e+01\n-3.729444503784179688e+01 -8.794846343994140625e+01 -5.875020599365234375e+01 -3.047664260864257812e+01 -5.233389282226562500e+01 -5.387952423095703125e+01 -4.839521026611328125e+01 -5.537379837036132812e+01 -3.903459072113037109e+00 -3.384850311279296875e+01\n-1.544962406158447266e+01 -2.230466079711914062e+01 -1.791401100158691406e+01 -2.969276428222656250e+01 -4.547164154052734375e+01 -3.889443159103393555e+00 -3.077057266235351562e+01 -4.032004547119140625e+01 -2.146554374694824219e+01 -3.804282760620117188e+01\n-4.058610439300537109e+00 -1.376746749877929688e+01 -9.614157676696777344e+00 -7.770594120025634766e+00 -1.003936576843261719e+01 -8.472715377807617188e+00 -6.603385925292968750e+00 -1.226200103759765625e+01 -7.491374969482421875e+00 -1.115189933776855469e+01\n-1.114142704010009766e+01 -2.219174957275390625e+01 -1.596575546264648438e+01 -1.016525650024414062e+01 -1.504627132415771484e+01 -1.393549728393554688e+01 -1.390664672851562500e+01 -1.656463050842285156e+01 -4.159469127655029297e+00 -1.087591171264648438e+01\n-2.040389251708984375e+01 -7.088414764404296875e+01 -1.523353958129882812e+01 -2.604238891601562500e+01 -2.423077392578125000e+01 -3.292998504638671875e+01 -3.408622503280639648e+00 -2.998207473754882812e+01 -3.507639312744140625e+01 -7.717485809326171875e+01\n-1.422254657745361328e+01 -4.484158706665039062e+01 -1.078249168395996094e+01 -1.766413879394531250e+01 -1.638128280639648438e+01 -2.127746963500976562e+01 -3.704056739807128906e+00 -2.021722412109375000e+01 -2.308171653747558594e+01 -4.890063858032226562e+01\n-6.136498641967773438e+01 -9.465768432617187500e+01 -3.610197448730468750e+01 -2.857654571533203125e+01 -1.911444854736328125e+01 -6.813326263427734375e+01 -5.272266387939453125e+01 -4.132856369018554688e+00 -5.017552566528320312e+01 -1.087619400024414062e+02\n-2.134333992004394531e+01 -6.163212203979492188e+01 -2.819044303894042969e+01 -3.948542833328247070e+00 -1.096827507019042969e+01 -3.231762313842773438e+01 -1.405270576477050781e+01 -1.413963890075683594e+01 -1.030910110473632812e+01 -3.626287460327148438e+01\n-1.611326408386230469e+01 -2.335536193847656250e+01 -1.871491813659667969e+01 -3.114327812194824219e+01 -4.783172607421875000e+01 -3.940395116806030273e+00 -3.230304718017578125e+01 -4.231876754760742188e+01 -2.245649147033691406e+01 -3.998496246337890625e+01\n-1.851421737670898438e+01 -2.720315933227539062e+01 -2.169389915466308594e+01 -3.643811416625976562e+01 -5.646389770507812500e+01 -4.125447750091552734e+00 -3.792764663696289062e+01 -4.963721847534179688e+01 -2.603151893615722656e+01 -4.697106552124023438e+01\n-5.876892471313476562e+01 -9.092440795898437500e+01 -3.499267959594726562e+01 -2.739908790588378906e+01 -1.834819793701171875e+01 -6.535050201416015625e+01 -5.062402343750000000e+01 -4.184595108032226562e+00 -4.797157287597656250e+01 -1.039532089233398438e+02\n-7.917993164062500000e+01 -1.219562988281250000e+02 -4.513483810424804688e+01 -3.630667114257812500e+01 -2.398303985595703125e+01 -8.795501708984375000e+01 -6.752887725830078125e+01 -3.772157192230224609e+00 -6.478369140625000000e+01 -1.415445556640625000e+02\n-6.323521041870117188e+01 -9.763261413574218750e+01 -3.710785675048828125e+01 -2.936536788940429688e+01 -1.960568237304687500e+01 -7.025308227539062500e+01 -5.427588272094726562e+01 -4.094451904296875000e+00 -5.168375015258789062e+01 -1.122126770019531250e+02\n-8.759860038757324219e+00 -9.706142425537109375e+00 -1.313257217407226562e+01 -1.258201980590820312e+01 -1.610619354248046875e+01 -1.201622867584228516e+01 -1.333721923828125000e+01 -1.675129127502441406e+01 -8.950416564941406250e+00 -4.275278568267822266e+00\n-4.126167297363281250e+00 -5.493610382080078125e+01 -3.090803718566894531e+01 -1.784050750732421875e+01 -2.744311523437500000e+01 -3.057565116882324219e+01 -1.382269096374511719e+01 -3.479645156860351562e+01 -1.891181373596191406e+01 -3.745549774169921875e+01\n-7.279552459716796875e+00 -7.940578460693359375e+00 -4.283311367034912109e+00 -8.163366317749023438e+00 -8.280757904052734375e+00 -5.495534896850585938e+00 -6.435780048370361328e+00 -8.410377502441406250e+00 -8.367765426635742188e+00 -1.181931114196777344e+01\n-2.451872444152832031e+01 -7.291885375976562500e+01 -3.288162612915039062e+01 -3.858707189559936523e+00 -1.271041107177734375e+01 -3.786814117431640625e+01 -1.579201126098632812e+01 -1.628011703491210938e+01 -1.104763412475585938e+01 -4.199660110473632812e+01\n-1.759007835388183594e+01 -1.984016227722167969e+01 -2.902506828308105469e+01 -2.685966110229492188e+01 -3.634279251098632812e+01 -2.857014656066894531e+01 -2.931857872009277344e+01 -3.622889328002929688e+01 -1.732245254516601562e+01 -4.110193252563476562e+00\n-4.752595138549804688e+01 -4.068867206573486328e+00 -3.630899810791015625e+01 -6.128831100463867188e+01 -7.368952941894531250e+01 -5.017343139648437500e+01 -6.279265213012695312e+01 -6.190445709228515625e+01 -4.952318191528320312e+01 -3.492961502075195312e+01\n-1.857810020446777344e+01 -2.122815704345703125e+01 -3.101051902770996094e+01 -2.849893188476562500e+01 -3.873007965087890625e+01 -3.054580879211425781e+01 -3.119842910766601562e+01 -3.857604598999023438e+01 -1.822972869873046875e+01 -4.083828926086425781e+00\n-2.072194671630859375e+01 -2.333090972900390625e+01 -3.461248397827148438e+01 -3.194519805908203125e+01 -4.354048538208007812e+01 -3.441202163696289062e+01 -3.499110031127929688e+01 -4.314307403564453125e+01 -2.031342315673828125e+01 -4.055376529693603516e+00\n-2.031427383422851562e+01 -3.011381912231445312e+01 -2.396684074401855469e+01 -4.042638397216796875e+01 -6.296613311767578125e+01 -4.263709068298339844e+00 -4.218260192871093750e+01 -5.515073013305664062e+01 -2.871058464050292969e+01 -5.219810485839843750e+01\n-3.040139770507812500e+01 -7.058265686035156250e+01 -4.746952056884765625e+01 -2.510282516479492188e+01 -4.246476745605468750e+01 -4.334822845458984375e+01 -3.930010223388671875e+01 -4.510353088378906250e+01 -3.971200942993164062e+00 -2.778851127624511719e+01\n-2.200777816772460938e+01 -6.440445709228515625e+01 -2.930238723754882812e+01 -3.922076463699340820e+00 -1.157084560394287109e+01 -3.355789947509765625e+01 -1.436968040466308594e+01 -1.484917449951171875e+01 -1.033179855346679688e+01 -3.752152633666992188e+01\n-4.112522602081298828e+00 -1.366378021240234375e+01 -9.944454193115234375e+00 -8.036000251770019531e+00 -1.062376594543457031e+01 -8.556116104125976562e+00 -7.098401069641113281e+00 -1.274724960327148438e+01 -7.442222595214843750e+00 -1.081157684326171875e+01\n-1.216378498077392578e+01 -2.036059188842773438e+01 -3.901335000991821289e+00 -1.344072532653808594e+01 -1.114278030395507812e+01 -1.044506072998046875e+01 -4.308685779571533203e+00 -1.221001052856445312e+01 -1.596279716491699219e+01 -2.897383308410644531e+01\n-2.955357742309570312e+01 -3.796686553955078125e+01 -4.134057998657226562e+00 -3.081291198730468750e+01 -2.924823760986328125e+01 -2.333085823059082031e+01 -1.617069244384765625e+01 -2.473273658752441406e+01 -3.632819366455078125e+01 -6.569927978515625000e+01\n-4.144152164459228516e+00 -3.874533081054687500e+01 -2.298302078247070312e+01 -1.427498626708984375e+01 -2.133903884887695312e+01 -2.217758369445800781e+01 -1.154710483551025391e+01 -2.658267784118652344e+01 -1.449073410034179688e+01 -2.695484924316406250e+01\n-1.967094230651855469e+01 -2.210584259033203125e+01 -3.271884918212890625e+01 -3.025439071655273438e+01 -4.114514923095703125e+01 -3.244316101074218750e+01 -3.309582138061523438e+01 -4.083662414550781250e+01 -1.932283210754394531e+01 -4.075018405914306641e+00\n-1.156970214843750000e+01 -3.723593139648437500e+01 -1.028598594665527344e+01 -1.435439205169677734e+01 -1.396442890167236328e+01 -1.779047203063964844e+01 -3.847182273864746094e+00 -1.752497673034667969e+01 -1.865164947509765625e+01 -3.903738021850585938e+01\n-5.022365951538085938e+01 -4.067265510559082031e+00 -3.827227020263671875e+01 -6.475291442871093750e+01 -7.787551879882812500e+01 -5.308722686767578125e+01 -6.637127685546875000e+01 -6.529892730712890625e+01 -5.229849624633789062e+01 -3.680891036987304688e+01\n-2.536238861083984375e+01 -7.591044616699218750e+01 -3.415812683105468750e+01 -3.834064722061157227e+00 -1.320643520355224609e+01 -3.935348892211914062e+01 -1.625482177734375000e+01 -1.688227462768554688e+01 -1.118497657775878906e+01 -4.341341018676757812e+01\n-2.682498359680175781e+01 -8.078113555908203125e+01 -3.618523406982421875e+01 -3.800046443939208984e+00 -1.376593875885009766e+01 -4.183815383911132812e+01 -1.708557319641113281e+01 -1.761177635192871094e+01 -1.169411087036132812e+01 -4.609952926635742188e+01\n-1.381227779388427734e+01 -3.501851654052734375e+01 -1.634001159667968750e+01 -6.638511180877685547e+00 -3.977850437164306641e+00 -2.028442955017089844e+01 -9.923542022705078125e+00 -7.489879608154296875e+00 -1.102954101562500000e+01 -2.554369735717773438e+01\n-3.592675399780273438e+01 -1.052188796997070312e+02 -4.371757507324218750e+01 -1.163378143310546875e+01 -3.475741386413574219e+00 -5.923502349853515625e+01 -2.262634086608886719e+01 -1.231928443908691406e+01 -2.606977272033691406e+01 -7.294050598144531250e+01\n-2.420422172546386719e+01 -3.063054275512695312e+01 -4.177126407623291016e+00 -2.535076522827148438e+01 -2.425317764282226562e+01 -1.903213691711425781e+01 -1.392302131652832031e+01 -2.079833793640136719e+01 -2.955807495117187500e+01 -5.262644958496093750e+01\n-5.555401611328125000e+01 -8.550027465820312500e+01 -3.286003494262695312e+01 -2.613315963745117188e+01 -1.761076545715332031e+01 -6.153792190551757812e+01 -4.779085540771484375e+01 -4.251764297485351562e+00 -4.553187179565429688e+01 -9.819395446777343750e+01\n-1.934419822692871094e+01 -2.855635833740234375e+01 -2.276466178894042969e+01 -3.828952407836914062e+01 -5.948717498779296875e+01 -4.189187049865722656e+00 -3.991167831420898438e+01 -5.220192718505859375e+01 -2.726786804199218750e+01 -4.937199020385742188e+01\n-2.511015510559082031e+01 -3.196306610107421875e+01 -4.162716865539550781e+00 -2.629226303100585938e+01 -2.507508087158203125e+01 -1.976881217956542969e+01 -1.423503684997558594e+01 -2.147984123229980469e+01 -3.074202537536621094e+01 -5.493445205688476562e+01\n-4.159975528717041016e+00 -4.161846160888671875e+01 -2.468890953063964844e+01 -1.506099700927734375e+01 -2.277891540527343750e+01 -2.383108901977539062e+01 -1.227544212341308594e+01 -2.836776733398437500e+01 -1.523793220520019531e+01 -2.859848403930664062e+01\n-2.294694709777832031e+01 -6.732299804687500000e+01 -3.055585289001464844e+01 -3.901713848114013672e+00 -1.190355777740478516e+01 -3.510186767578125000e+01 -1.492453193664550781e+01 -1.526358032226562500e+01 -1.062432289123535156e+01 -3.910974502563476562e+01\n-6.751013183593750000e+01 -1.042312164306640625e+02 -3.934717941284179688e+01 -3.121002960205078125e+01 -2.075756835937500000e+01 -7.503845977783203125e+01 -5.786325836181640625e+01 -4.007836341857910156e+00 -5.516710662841796875e+01 -1.200475540161132812e+02\n-1.165893077850341797e+01 -2.643288421630859375e+01 -1.295825958251953125e+01 -4.303713321685791016e+00 -4.586702346801757812e+00 -1.463102245330810547e+01 -8.459090232849121094e+00 -6.592896461486816406e+00 -7.630463123321533203e+00 -1.918774032592773438e+01\n-1.884151649475097656e+01 -2.773086547851562500e+01 -2.210214614868164062e+01 -3.716085433959960938e+01 -5.764160156250000000e+01 -4.150489330291748047e+00 -3.869596099853515625e+01 -5.063650512695312500e+01 -2.652139663696289062e+01 -4.792680740356445312e+01\n<\/code>\nand this is what I get for batch_size 1\n<code class=\"lang-auto\">0.000000000000000000e+00 0.000000000000000000e+00 0.000000000000000000e+00 0.000000000000000000e+00 0.000000000000000000e+00 0.000000000000000000e+00 0.000000000000000000e+00 0.000000000000000000e+00 0.000000000000000000e+00 0.000000000000000000e+00\n\n<\/code>\nsomething is definitely wrong somewhere that I can\u2019t figure out\u2026\nthis best I can guess is that it must be capturing the batch statistics at run time and using them to normalize the data which means instance normalization with batch size of 1 and hence zero output since mean(x) = x, but this makes no sense with .eval().\nCan you print  model.training during your evaluation to ensure, your model is really in eval-mode?\nof course. let me do it\nyes. It\u2019s printing false. meaning it is in eval mode\nbut still even if it wasn\u2019t, why do large batch sizes at test time give me better accuracy and 1 doesn\u2019t?\nIf your model would not be in eval mode, the normalization layers would not use the tracked statistics, but use per-batch stats. If you have only one sample, these stats might not be representative. But since your model is in eval mode, this should not matter.\nAre you using any transformations inside your dataloader?\nI could imagine, that you might use some normalizations only in your trainloader but not in your val_loader and therefore the images are not normalized properly inside your validation script.\nThanks for your time. I am using augmentations but they shouldn\u2019t matter. here is my dataloader code\n<code class=\"lang-auto\">\n\nfrom __future__ import print_function\nfrom __future__ import division\nimport os\nimport cv2\nimport gdal\nimport json\nimport torch\nimport random\nimport numpy as np\nrandom.seed(74)\nimport matplotlib.pyplot as pl\nfrom torch.utils.data import Dataset, DataLoader\nimport imgaug as ia\nfrom imgaug import augmenters as iaa\n\n\n# will implement all functionality (data augmentation) of doing\n# 1. random crops,\n# 2. random flips,\n# 3. random rotations,\n\nall_labels = {\n    'AnnualCrop'           : 0,\n    'Forest'               : 1,\n    'HerbaceousVegetation' : 2,\n    'Highway'              : 3,\n    'Industrial'           : 4,\n    'Pasture'              : 5,\n    'PermanentCrop'        : 6,\n    'Residential'          : 7,\n    'River'                : 8,\n    'SeaLake'              : 9\n}\n\ndef toTensor(image):\n    \"converts a single input image to tensor\"\n    # swap color axis because\n    # numpy image: H x W x C\n    # torch image: C X H X W\n    image = image.transpose((2, 0, 1))\n    return torch.from_numpy(image).float()\n\n######################################################################################################\n# Sometimes(0.5, ...) applies the given augmenter in 50% of all cases,\n# e.g. Sometimes(0.5, GaussianBlur(0.3)) would blur roughly every second\n# image.\nsometimes = lambda aug: iaa.Sometimes(0.5, aug)\n\n# Define our sequence of augmentation steps that will be applied to every image.\nseq = iaa.Sequential(\n    [\n        #\n        # Apply the following augmenters to most images.\n        #\n        iaa.Fliplr(0.5), # horizontally flip 50% of all images\n        iaa.Flipud(0.5), # vertically flip 50% of all images\n\n        # crop some of the images by 0-20% of their height\/width\n        sometimes(iaa.Crop(percent=(0, 0.2))),\n\n        # Apply affine transformations to some of the images\n        # - scale to 80-120% of image height\/width (each axis independently)\n        # - translate by -20 to +20 relative to height\/width (per axis)\n        # - rotate by -45 to +45 degrees\n        # - mode: use any available mode to fill newly created pixels\n        #         see API or scikit-image for which modes are available\n        sometimes(iaa.Affine(\n            scale={\"x\": (0.8, 1.2), \"y\": (0.8, 1.2)},\n            translate_percent={\"x\": (-0.2, 0.2), \"y\": (-0.2, 0.2)},\n            rotate=(-180, 175),\n            mode=ia.ALL\n        )),\n    ],\n    # do all of the above augmentations in random order\n    random_order=True\n)\n######################################################################################################\n\n\ndef get_dataloaders(base_folder, batch_size):\n    print('inside dataloading code...')\n\n    class dataset(Dataset):\n        def __init__(self, data_dictionary, bands, mode='train'):\n            super(dataset, self).__init__()\n            self.example_dictionary = data_dictionary\n            # with open(mode+'.txt', 'wb') as this:\n            #     this.write(json.dumps(self.example_dictionary))\n            self.bands = bands # bands are a list bands to use as data, pass them as a list []\n            self.mode = mode\n            pass\n\n        def __getitem__(self, k):\n            example_path, label_name = self.example_dictionary[k]\n            # print(example_path, label_name)\n            # example is a tiff image, need to use gdal\n            this_example = gdal.Open(example_path)\n            this_label = all_labels[label_name]\n            example_array = this_example.GetRasterBand(self.bands[0]).ReadAsArray()\n            for i in self.bands[1:]:\n                example_array = np.dstack((example_array,\n                                           this_example.GetRasterBand(i).ReadAsArray())).astype(np.int16)\n\n            # transforms\n            if self.mode == 'train':\n                example_array = np.squeeze(seq.augment_images(\n                    (np.expand_dims(example_array, axis=0))), axis=0)\n                pass\n\n            # convert \n            example_array = (example_array.astype(np.float ) * 1 \/4096)\n            example_array = toTensor(image=example_array)\n            return {'input': example_array, 'label': this_label}\n\n        def __len__(self):\n            return len(self.example_dictionary)\n\n    # create training set examples dictionary\n    all_examples = {}\n    for folder in sorted(os.listdir(base_folder)):\n        # each folder name is a label itself\n        # new folder, new dictionary!\n        class_examples = []\n        inner_path = os.path.join(base_folder, folder)\n        for image in [x for x in os.listdir(inner_path) if x.endswith('.tif')]:\n            image_path = os.path.join(inner_path, image)\n            # for each index as key, we want to have its path and label as its items\n            class_examples.append(image_path)\n        all_examples[folder] = class_examples\n\n    # split them into train and test\n    train_dictionary, val_dictionary, test_dictionary = {}, {}, {}\n    for class_name in all_examples.keys():\n        class_examples = all_examples[class_name]\n        # print(class_examples)\n        random.shuffle(class_examples)\n\n        total = len(class_examples)\n        train_count = int(total * 0.8); train_ = class_examples[:train_count]\n        test = class_examples[train_count:]\n\n        total = len(train_)\n        train_count = int(total * 0.9); train = train_[:train_count]\n        validation = train_[train_count:]\n\n        for example in train:\n            train_dictionary[len(train_dictionary)] = (example, class_name)\n        for example in test:\n            test_dictionary[len(test_dictionary)] = (example, class_name)\n        for example in validation:\n            val_dictionary[len(val_dictionary)] = (example, class_name)\n\n\n    # create dataset class instances\n    bands = [4, 3, 2]\n    train_data = dataset(data_dictionary=train_dictionary, bands=bands, mode='train')\n    val_data = dataset(data_dictionary=val_dictionary, bands=bands, mode='eval')\n    test_data = dataset(data_dictionary=test_dictionary, bands=bands, mode='test')\n    print('train examples =', len(train_dictionary), 'val examples =', len(val_dictionary),\n          'test examples =', len(test_dictionary))\n\n    train_dataloader = DataLoader(dataset=train_data, batch_size=batch_size,\n                                  shuffle=True, num_workers=4)\n    val_dataloader = DataLoader(dataset=val_data, batch_size=batch_size,\n                                shuffle=True, num_workers=4)\n    test_dataloader = DataLoader(dataset=test_data, batch_size=batch_size,\n                                 shuffle=True, num_workers=4)\n\n    return train_dataloader, val_dataloader, test_dataloader\n\n\ndef histogram_equalization(in_image):\n    for i in range(in_image.shape[2]): # each channel\n        image = in_image[: ,: ,i]\n        prev_shape = image.shape\n        # Flatten the image into 1 dimension: pixels\n        pixels = image.flatten()\n\n        # Generate a cumulative histogram\n        cdf, bins, patches = pl.hist(pixels, bins=256, range=(0 ,256), normed=True, cumulative=True)\n        new_pixels = np.interp(pixels, bins[:-1], cdf *255)\n        in_image[: ,: ,i] = new_pixels.reshape(prev_shape)\n    return in_image\n\n\ndef main():\n    train_dataloader, val_dataloader, test_dataloader = get_dataloaders(base_folder='\/home\/annus\/Desktop\/'\n                                                                                    'forest_cover_change\/'\n                                                                                    'eurosat\/images\/tif',\n                                                                        batch_size=1)\n    # #\n    # train_dataloader, val_dataloader, test_dataloader = get_dataloaders(base_folder='Eurosat\/tif\/',\n    #                                                                     batch_size=16)\n\n    count = 0\n    reversed_labels = {v :k for k, v in all_labels.iteritems()}\n    while True:\n        count += 1\n        for idx, data in enumerate(train_dataloader):\n            examples, labels = data['input'], data['label']\n            print('{} -> on batch {}\/{}, {}'.format(count, idx +1, len(train_dataloader), examples.size()))\n            if True:\n                this = np.max(examples[0].numpy())\n                print(this)\n                this = (examples[0].numpy( ) *255).transpose(1 ,2 ,0).astype(np.uint8)\n                # this = histogram_equalization(this)\n                pl.imshow(this)\n                pl.title('{}'.format(reversed_labels[int(labels.numpy())]))\n                pl.show()\n\n\nif __name__ == '__main__':\n    main()\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n<\/code>\nBTW, I divide by 4096 because my images are 12-bit mosaics of satellite imagery (sentinel-2 images). I just thought bringing them to 0-1 range might help in learning. The test function at the end I used to see if i could reconstruct the images and it was working perfectly.\nThank you so much everyone for your help. Steve_cruz with link \"https:\/\/discuss.pytorch.org\/t\/test-accuracy-with-different-batch-sizes\/23049\/4\" helped me solve my error. I retrained my model be removing the last softmax layer since cross entropy loss applies softmax itself. Also I decorated my evaluation function with torch.no_grad() and got the model running. Now it is giving me good accuracy with any batch size. But still those accuracies vary somewhere between 2-3% (93-95%) for different batch sizes for some reason. I\u2019ll try to find some fix for that. Thanks for your time everyone!\nIt seems I have similar problem with you. As the validation batch size gets smaller, the validation accuracy gets worse. Could you please explain your solution in more detail? Thank you.\nI think maybe you calculate the accuracy incorrectly.  avg_accuracy=total_correct_samples\/total_samples ; But not  avg_accuracy=(acc1+acc2+...+accN)\/N"},{"x":"I wanted to implement transfer learning on a data consisting of images which are divided into 52 classes. I wanted to do that using vgg16 and the concept of transfer learning. I have frozen the weights of feature block and from the ouptut of feature block, I am training my own classifier which takes input of 25088( 512 X 7 X 7 number of neurons ). Now, vgg16\u2019s classifier layer outputs 1000 dimension vector indicating which class the input image belonged to. I want to change this 1000 to 52. For that I have written something like:\nfor param in model_vgg.parameters():\n(Indented by one tab space) param.requires_grad = False   #freeze all the layers\nmodel_vgg.classifier[6].out_features = 52  #set number of output classes to 52\nfor param in model_vgg.classifier[6].parameters():\n(Indented by one tab space)param.requires_grad = True      #unfreeze only the last layer of classifier block\nBut this still doesn\u2019t work, as the output of my classifier block while training and validating is still a 1000 dimension vector for each input image. I tried print( model_vgg.classifier[6].out_features ) which prints 52, still the problem persists.\nWhat\u2019s the problem with my code? Any suggestions will be helpful.","y":"<code class=\"lang-auto\">class ClassifierModule(nn.Module):\n    def __init__(self):\n        super(ClassifierModule,self).__init__()\n        self.layer1 = nn.Linear(1000,52)\n        self.net = model_vgg.classifier\n        for p in self.net.parameters():\n            p.requires_grad=False\n\n    def forward(self,x):\n        x1 = self.net(x)\n        print 'Passed Thru VGG'\n        y = self.layer1(x1)\n        return y\n\nmodel = ClassifierModule()\n<\/code>\nThe above model will have 52 outputs. I think you can use similiar thing for your network.","z":"You can add a linear layer with output dimension to be 52 at the end of this network and with that you will be able to use the pre-trained network weights in the better manner.\nHi, thanks for your quick reply. I have just started using pytorch and thus new to it. Can you show me how do one add a new linear layer to an already established model?\nEdit: I have just figured out how to add linear layer. I wrote something like this:\nfor param in model_vgg.parameters():\n(Indented by one tab space) param.requires_grad = False #freeze all the layers\nmodel_vgg.classifier[6].out_features = 52 #set number of output classes to 52\nfor param in model_vgg.classifier[6].parameters():\n(Indented by one tab space)param.requires_grad = True #unfreeze only the last layer of classifier block\nmodel_vgg.classifier=nn.Linear(25088,52)  #I knew that number of inputs to my classifier block was 25088\nHowever, now I want to change only the last layer of my classifier that is model_vgg.classifier[6] and write something like:\nmodel_vgg.classifer[6]=nn.Linear(num_inputs,52)\nBut here I don\u2019t know the number of inputs to my model_vgg.classifer[6]. How to find that?\n<code class=\"lang-auto\">class ClassifierModule(nn.Module):\n    def __init__(self):\n        super(ClassifierModule,self).__init__()\n        self.layer1 = nn.Linear(1000,52)\n        self.net = model_vgg.classifier\n        for p in self.net.parameters():\n            p.requires_grad=False\n\n    def forward(self,x):\n        x1 = self.net(x)\n        print 'Passed Thru VGG'\n        y = self.layer1(x1)\n        return y\n\nmodel = ClassifierModule()\n<\/code>\nThe above model will have 52 outputs. I think you can use similiar thing for your network.\nprint the model and you should see the last linear classifier taking in 4096 features\nso, model_vgg.classifer[6]=nn.Linear(4096,52)"},{"x":"I am trying to generate potrait images, using image segmentation. I am using unet with the following architecture.\n<code class=\"lang-auto\">import torch\nimport torch.nn as nn\nclass Unet(nn.Module):\n    '''U-Net Architecture'''\n    def __init__(self,inp,out):\n        super(Unet,self).__init__()\n        self.c1=self.contracting_block(inp,64)\n        self.c2=self.contracting_block(64,128)\n        self.c3=self.contracting_block(128,256)\n        self.c4=self.contracting_block(256,512)\n        self.c5=self.contracting_block(512,1024)\n        self.maxpool=nn.MaxPool2d(2)\n        self.upsample=nn.Upsample(scale_factor=2,mode=\"bilinear\",align_corners=True)\n        self.c6=self.contracting_block(512+1024,512)\n        self.c7=self.contracting_block(512+256,256)\n        self.c8=self.contracting_block(256+128,128)\n        self.c9=self.contracting_block(128+64,64)\n        self.c10=nn.Conv2d(64,1,1)\n        \n\n    def contracting_block(self,inp,out,k=3):\n        block =nn.Sequential(\n            nn.Conv2d(inp, out, padding=1,kernel_size=3),\n            nn.BatchNorm2d(out),\n            nn.ReLU(inplace=True),\n            nn.Conv2d(out, out,padding=1,kernel_size=3),\n            nn.BatchNorm2d(out),\n            nn.ReLU(inplace=True)\n        )\n        return block\n\n\n    def forward(self,x):\n        conv1=self.c1(x) #256x256x64\n        conv1=self.maxpool(conv1) #128x128x64\n        conv2=self.c2(conv1) #128x128x128\n        conv2=self.maxpool(conv2) #64x64x128\n        conv3=self.c3(conv2) #64x64x256\n        conv3=self.maxpool(conv3) #32x32x256\n        conv4=self.c4(conv3) #32x32x512\n        conv4=self.maxpool(conv4) #16x16x512\n        conv5=self.c5(conv4) #8x8x1024\n        conv5=self.maxpool(conv5)\n        x=self.upsample(conv5) ##16x16x1024\n        #print(x.shape)\n        x=torch.cat([x,conv4],axis=1) #16x16x1536\n        x=self.c6(x) #16x16x512\n        x=self.upsample(x) #32x32x512\n        x=torch.cat([x,conv3],axis=1) \n        x=self.c7(x) #32x32x256\n        x=self.upsample(x) #64x64x256\n        x=torch.cat([x,conv2],axis=1)\n        x=self.c8(x) #64x64x128\n        x=self.upsample(x) #128x128x128\n        x=torch.cat([x,conv1],axis=1) \n        x=self.c9(x) #128x128x64\n        x=self.upsample(x)#256x256x64\n        x=self.c10(x)\n        return x\n\n\nif __name__==\"__main__\":\n    x=torch.ones(1,3,256,512)\n    net=Unet(3,1)\n    print(net(x).shape)\n\n<\/code>\nI am using a publicly available dataset with almost 1300 images.\nMy loss always diverges, I am using a learning rate of 1e-5, loss function-> BCEWithLogitsLoss\nMy training loop is as follows.\n<code class=\"lang-auto\">def training_loop(*args,**kwargs):\n    \"\"\"\n    Main training Loop\n    keyword parameters:\n    epochs:number of epochs\n    lr:learning_rate\n    \n    \"\"\"\n    global net,valid_loader,train_loader,device\n    epochs=kwargs[\"epochs\"]\n    lr=kwargs[\"lr\"]\n    if(os.path.isdir(\"checkpoints\")==False):\n        os.mkdir(\"checkpoints\")\n    criterion=None\n    if model==\"unet\":\n        criterion=nn.BCEWithLogitsLoss()\n    elif model==\"cnn\":\n        criterion=nn.CrossEntropyLoss()\n    opt=optim.Adam(net.parameters(),lr=lr,weight_decay=1e-8)\n    xx=[]\n    yy=[]\n    for epoch_num in range(1,epochs+1):\n        running_loss=0.0\n        for i,samples in enumerate(train_loader):\n\n            imgs,masks=samples[0],samples[1]\n            imgs,masks=imgs.to(device),masks.to(device)\n            opt.zero_grad()\n            outputs=net(imgs)\n            loss=criterion(outputs,masks)\n            loss.backward()\n            opt.step()\n            if(model==\"unet\"):\n                running_loss += torch.exp(loss).item()\n            elif(model==\"cnn\"):\n                running_loss+=loss.item()\n\n            if(i%20==19):\n                valid_loss=validation(valid_loader=valid_loader,criterion=criterion)\n                writer.add_scalars(\"first\",{'train_loss':torch.tensor(running_loss\/20),\n                                            'validation_loss':torch.tensor(valid_loss)},epoch_num*len(train_loader)+i)\n\n                writer.close()\n                print(\"Epoch [%3d] iteration [%4d] loss:[%.10f]\"%(epoch_num,i,running_loss\/20),end=\"\")\n                print(\" validation_loss:[%.10f]\"%(valid_loss))\n                running_loss=0.0\n        torch.save(net.state_dict(),\"checkpoints\/\"+str(epoch_num)+\".pth\")\n        \n<\/code>\nI am not able to find the problem. Please help.","y":"Perhaps there is a problem in the data.","z":"The model and training code look alright and your model is able to learn some random inputs:\n<code class=\"lang-python\">if __name__==\"__main__\":\n    x=torch.ones(1,3,256,512)\n    net=Unet(3,1)\n    print(net(x).shape)\n\n    net.cuda()\n    optimizer = torch.optim.Adam(net.parameters(), lr=1e-3)\n\n    data = torch.randn(10, 3, 256, 512, device='cuda')\n    target = torch.randint(0, 2, (10, 1, 256, 512)).float().cuda()\n    criterion = nn.BCEWithLogitsLoss()\n\n    for epoch in range(100):\n        optimizer.zero_grad()\n        output = net(data)\n        loss = criterion(output, target)\n        loss.backward()\n        optimizer.step()\n        print('epoch {}, loss {}'.format(epoch, loss.item()))\n<\/code>\nWhat issues are you seeing and could you check your target range in case you are seeing an increasing loss?\nSir, I normalised the images and masks to have mean 0.5 and std 0.5. I also rechecked and the min and max values of images and masks are:\n<code class=\"lang-auto\">image -->max and min  tensor(1.) tensor(-0.9294)\nmask--> max ad min tensor(1.) tensor(-1.)\n\n<\/code>\nWhat should I do ?\nTry to scale down your use case by overfitting a small subset of your data, e.g. just 10 samples.\nIf this doesn\u2019t work, play around with some hyperparameters.\nIn case your model is not able to overfit these samples, you would have to check the architecture again or there might be a bug in the complete training routine which is not shown here or which I have missed.\nAny reason why you do this?\n<code class=\"lang-auto\">running_loss += torch.exp(loss).item()\n<\/code>\nWhy aren\u2019t you simply summing the losses?\nSir, I am using BCEWithLogitsLoss as my loss function. To bring the loss in range [0,infinity] I take the exponent and I am printing the average loss after some steps.\nSir, I tried training on a small set of data. The loss becomes \u201cnan\u201d after some iterations. I also tried some other values of batch size and learning rate. But still no improvement.\nEntire training code is ->\n<code class=\"lang-auto\">import os\nimport cv2\nimport sys\nimport math\nimport torch\nimport argparse\nimport torchvision\nimport numpy as np\nsys.path.append('')\nimport torch.nn as nn\nimport torch.optim as optim\nfrom modelArch.unet import Unet\nfrom modelArch.cnn import Cnn\nfrom torchsummary import summary\nfrom dataLoader.dataLoader_unet import load\nfrom dataLoader.dataloader_cnn import load_cnn\nfrom torch.utils.data import DataLoader\nfrom torch.utils.tensorboard import SummaryWriter\n\nparser=argparse.ArgumentParser()\nparser.add_argument(\"--epochs\",default=50)\nparser.add_argument(\"--batch_size\",default=4)\nparser.add_argument(\"--lr\",default=0.0001)\nparser.add_argument(\"--model\",default=\"unet\",help=\"unet\/cnn\")\nargs=parser.parse_args()\n\nwriter=SummaryWriter('runs\/trial1')\n\nnet=None\nvalid_loader=None\ntrain_loader=None\ndevice=None\ndata=None\nmodel=None\n\ndef weights_init(m):\n    if isinstance(m,nn.Conv2d):\n        torch.nn.init.xavier_uniform_(m.weight)\n        torch.nn.init.zeros_(m.bias)\n\ndef init(*args,**kwargs):\n    \"\"\"\n    Initiates the training process\n    keyword parameters:\n    train_percent:[0,1]\n    resume:pass checkpoint number from where to resume training\n    batch_size\n    \"\"\"\n    #resume=kwargs[\"resume\"]\n    global net,valid_loader,train_loader,device,data\n    resume=None\n    train_percent=kwargs[\"train_percent\"]\n    batch_size=kwargs[\"batch_size\"]\n    width=kwargs[\"width\"]\n    height=kwargs[\"width\"]\n    #model=kwargs[\"model\"]\n    if(model==\"unet\"):\n        net=Unet(3,1)\n        data=load(width=width,height=height)\n\n    if(resume is not None):\n        net.load_state_dict(torch.load(\"checkpoints\/\"+str(name)+\".pth\"))\n        print(\"Resuming training from \"+str(name)+\" checkpoint\")\n    else:\n        net.apply(weights_init)\n        \n    device=torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n    print(\"using \",device)\n    #summary(net,input_size=(3,256,256))\n    \n    size=len(data)\n    train_size=math.floor(train_percent*size)\n    test_size=size-train_size\n    print(\"Data Loaded\")\n    train,validation=torch.utils.data.random_split(data,[train_size,test_size])\n    train_loader=DataLoader(train,batch_size=batch_size,shuffle=True,num_workers=4)\n    valid_loader=DataLoader(validation,batch_size=batch_size,shuffle=True,num_workers=4)\n    #x=iter(dataLoader)\n    #img,mask=x.next()\n    #grid=torchvision.utils.make_grid(img)\n    #writer.add_image('images',grid,0)\n    #writer.add_graph(net,img)\n    #writer.close()\n    net.to(device)\n\ndef validation(**kwargs):\n\n    \"\"\"\n    keyword args:\n    valid_loader: validation data loader\n    \"\"\"\n    global net,valid_loader,train_loader,device\n    valid_loader=kwargs[\"valid_loader\"]\n    criterion=kwargs[\"criterion\"]\n    #model=kwargs[\"model\"]\n    p=0\n    valid_loss=0.0\n    with torch.no_grad():\n        for no,data in enumerate(valid_loader):\n            imgs,masks=data[0].to(device),data[1].to(device)\n            outputs=net(imgs)\n            v_loss=criterion(outputs,masks)\n            if(model=='unet'):\n                valid_loss+=torch.exp(v_loss).item()       \n            p+=1\n    \n    return valid_loss\/p\n\n\ndef training_loop(*args,**kwargs):\n    \"\"\"\n    Main training Loop\n    keyword parameters:\n    epochs:number of epochs\n    lr:learning_rate\n    \n    \"\"\"\n    global net,valid_loader,train_loader,device\n    epochs=kwargs[\"epochs\"]\n    lr=kwargs[\"lr\"]\n    if(os.path.isdir(\"checkpoints\")==False):\n        os.mkdir(\"checkpoints\")\n    criterion=None\n    if model==\"unet\":\n        criterion=nn.BCEWithLogitsLoss()\n    opt=optim.Adam(net.parameters(),lr=lr,weight_decay=1e-8)\n    xx=[]\n    yy=[]\n    for epoch_num in range(1,epochs+1):\n        running_loss=0.0\n        for i,samples in enumerate(train_loader):\n\n            imgs,masks=samples[0],samples[1]\n            imgs,masks=imgs.to(device),masks.to(device)\n            opt.zero_grad()\n            outputs=net(imgs)\n            loss=criterion(outputs,masks)\n            loss.backward()\n            opt.step()\n            if(model==\"unet\"):\n                running_loss += torch.exp(loss).item()\n\n            if(i%20==19):\n                valid_loss=validation(valid_loader=valid_loader,criterion=criterion)\n                writer.add_scalars(\"first\",{'train_loss':torch.tensor(running_loss\/20),\n                                            'validation_loss':torch.tensor(valid_loss)},epoch_num*len(train_loader)+i)\n\n                writer.close()\n                print(\"Epoch [%3d] iteration [%4d] loss:[%.10f]\"%(epoch_num,i,running_loss\/20),end=\"\")\n                print(\" validation_loss:[%.10f]\"%(valid_loss))\n                running_loss=0.0\n        torch.save(net.state_dict(),\"checkpoints\/\"+str(epoch_num)+\".pth\")\n        \n    \nif __name__==\"__main__\":\n    #global model\n    model=args.model\n    init(batch_size=int(args.batch_size),train_percent=0.95,width=256,height=512)\n    training_loop(epochs=int(args.epochs),lr=1e-4)\n\n\n    \n<\/code>\nIs the loss still going up? Also, could you check your input for NaN values?\nSir, I am sorry for the mistake, the loss quickly becomes \u201cinf\u201d not \u201cnan\u201d. I check the input as well and I don\u2019t find any problem with the input.\n\nself.maxpool=nn.MaxPool2d(2)\n\nJust a suggestion. I think you could try nn.MaxPool2d(2) instead of using self.maxpool() repeatly. I don\u2019t know whether it will cause a ploblem because all gradients flow through the node self.maxpool() (only one node) in graph.\nI tried replacing self.maxpool with nn.MaxPool2d(2), but still no success, the loss decreases for a while and then quickly becomes infinity. My dataLoader looks likes this\n<code class=\"lang-auto\">import os\nimport cv2\nimport random\nimport numpy as np\nfrom tqdm import tqdm\nfrom PIL import Image\nfrom torch.utils.data import Dataset\nimport torchvision.transforms as transforms\n\n\n\n\nclass load(Dataset):\n    def __init__(self,**kwargs):\n        self.width=kwargs[\"width\"]\n        self.height=kwargs[\"height\"]\n        self.samples=[]\n        self.path1=\"\/home\/satinder\/Desktop\/deepWay\/DeepWay.v2\/dataSet\/Segmentation2\/img\/\"\n        self.path2=\"\/home\/satinder\/Desktop\/deepWay\/DeepWay.v2\/dataSet\/Segmentation2\/mask\/\"\n        img_folder=os.listdir(self.path1)\n        \n        for i in tqdm(img_folder):\n            num=i.split(\".\")[0]\n            self.samples.append((i,num+\".png\"))\n        self.color=transforms.ColorJitter(brightness = 1)\n        #self.translate=transforms.RandomAffine(translate=(0.1,0.1))\n        self.angle=transforms.RandomAffine(degrees=(60))\n        self.flip=transforms.RandomHorizontalFlip(p=0.5)\n        self.transforms_img=transforms.Compose([transforms.ToTensor(),\n                                                transforms.Normalize((0.5,0.5,0.5),(0.5,0.5,0.5))])\n\n        self.transforms_mask=transforms.Compose([transforms.Grayscale(num_output_channels=1),\n                                                transforms.ToTensor(),\n                                                transforms.Normalize((0.5,),(0.5,))])\n\n    def __len__(self):\n        return len(self.samples)\n\n    def __getitem__(self,idx):\n        i,j=self.samples[idx]\n        img=cv2.imread(self.path1+i,1)\n        img=cv2.cvtColor(img,cv2.COLOR_BGR2RGB)\n        #img=cv2.blur(img,(3,3))\n        mask=cv2.imread(self.path2+j,1)\n        mask=cv2.cvtColor(mask,cv2.COLOR_BGR2GRAY)\n        mask=cv2.Canny(mask,100,150)\n        mask=cv2.dilate(mask,None,iterations=5)\n        img=cv2.resize(img,(self.height,self.width))\n        mask=cv2.resize(mask,(self.height,self.width))\n        #print(mask.shape)\n        seed=np.random.randint(2147483647)\n        img=Image.fromarray(img)\n        mask=Image.fromarray(mask)\n        \n\n        random.seed(seed)\n        #img=self.color(img)\n        random.seed(seed)\n        #img=self.translate(img)\n        random.seed(seed)\n        #img=self.angle(img)\n        random.seed(seed)\n        #img=self.flip(img)\n        random.seed(seed)\n        img=self.transforms_img(img)\n        \n        random.seed(seed)\n        #mask=self.translate(mask)\n        random.seed(seed)\n        #mask=self.angle(mask)\n        random.seed(seed)\n        #mask=self.flip(mask)\n        random.seed(seed)\n        mask=self.transforms_mask(mask)\n        #print(img)\n        return (img,mask)\n    \n    def plot(self,img):\n        img=np.transpose(img.numpy(),(1,2,0))\n        img=img*0.5+0.5\n        img=cv2.cvtColor(img,cv2.COLOR_RGB2BGR)\n        cv2.imshow(\"ds\",img)\n        cv2.waitKey(0)\n\n\nif(__name__==\"__main__\"):\n    obj=load(width=256,height=256)\n    res=obj.__getitem__(7)\n    obj.plot(res[0])\n    obj.plot(res[1])\n    #cv2.imshow(\"img\",res[0].cpu().detach().numpy())\n    \n<\/code>\nAny suggestions?\nMaybe you are using your labels in the wrong way \u2026 why using canny filter?\nSir, it is because I was trying to only predict the boundary of the mask. I have tried it without applying canny as well and the results are same\nAny other suggestions?\n\n\n\n satinder147:\n\nmask\u2013> max ad min tensor(1.) tensor(-1.)\n\n\nTry to pass the mask with values in the range [0, 1].\nPerhaps there is a problem in the data.\nThere was a problem with the dataset, I changed it and now it works properly."},{"x":"Suppose I have a model that takes as input a multi-level mask like a segmentation map where each pixel can take one of N > 2 classes.\nMy question is should the input shape be (batch_size, num_classes, height, width) or can I instead use only one channel to encode the class index with an input shape of (batch_size, 1, height, width).\nWith output shape its pretty clear that it is (batch_size, num_classes, height, width). But what about the input shape?\nPlease advice.","y":"You can encode the input information as you want.\nE.g. you could certainly pass the input in a one-hot encoded way, as a single channel image, or even a color image, where each color represents a certain class.\nThe \u201cright\u201d approach also depends on your current model.\nI.e. are you creating a model from scratch? If so, try out different approaches and check their performance on the validation set.\nOn the other hand, if you are thinking about fine tuning a model, most pretrained models use 3 input channels, so that you could have to adapt your input shape to it if you don\u2019t want to replace\/manipulate the first convolution.","z":"You can encode the input information as you want.\nE.g. you could certainly pass the input in a one-hot encoded way, as a single channel image, or even a color image, where each color represents a certain class.\nThe \u201cright\u201d approach also depends on your current model.\nI.e. are you creating a model from scratch? If so, try out different approaches and check their performance on the validation set.\nOn the other hand, if you are thinking about fine tuning a model, most pretrained models use 3 input channels, so that you could have to adapt your input shape to it if you don\u2019t want to replace\/manipulate the first convolution.\nDear ,\nThanks for your great advice. I\u2019m trying to reimplement a model from a paper from scratch by literally following whatever is reported in the paper. However, some details seemed to be missing from the paper.\nThank you once again, and stay safe."},{"x":"Hey Community,\nlets say i want to change the dropout in densenet in the last layers, so i choose layer 10 and 11 and update the respective dropout like shown. As you can see, the dropout remains 0.2 even after model.eval() is called.\nWhy is that the case?\nIs there a more appealing way to change the dropout in the respective layers and turn in on\/off when in training\/evaluation mode?\nOr do I need to run the change_dropout function each time i change between train and evaluation mode?\nThe output is pretty long, but it shows that layer.drop_rate remains 0.2 in layers 10 and 11 after model.eval() was called.\nOutput:\n<code class=\"lang-auto\">###### BEFORE CHANGES #######\ntrain: 0.0\ntrain: 0.0\n... cut to the last to lines ...\ntrain: 0.0\ntrain: 0.0\n\neval: 0.0\neval: 0.0\n... cut to the last to lines ...\neval: 0.0\neval: 0.0\n\n###### AFTERCHANGES #######\ntrain: 0.0\ntrain: 0.0\n... cut to the last to lines ...\ntrain: 0.2\ntrain: 0.2\n\neval: 0.0\neval: 0.0\n... cut to the last to lines ...\neval: 0.2\neval: 0.2\n<\/code>\nCode: (should run on your machine)\n<code class=\"lang-auto\">from torchvision import models\ndef print_dropouts(model, phase):\n    for feature in model.features:\n        if type(feature) == models.densenet._DenseBlock:\n            for layer in feature.modules():\n                if type(layer) == models.densenet._DenseLayer:\n                    print(f\"{phase}: \", layer.drop_rate)\n\ndef change_dropout(model, layers_to_update, drop_rate):\n    for idx, feature in enumerate(model.features):\n        if (idx in layers_to_update) and (type(feature) == models.densenet._DenseBlock):\n            for layer in feature.modules():\n                if type(layer) == models.densenet._DenseLayer:\n                    layer.drop_rate = drop_rate\n# \nif __name__ == \"__main__\":\n    model = models.desnenet121(pretrained=True)\n    \n    print(\"###### BEFORE CHANGES #######\")\n    model.train()\n    print_dropouts(model, \"train\")\n    model.eval()\n    print_dropouts(model, \"eval\")\n\n    # change happens here\n    change_dropout(model, [10, 11], 0.2)\n\n    print(\"###### AFTER CHANGES #######\")\n    model.train()\n    print_dropouts(model, \"train\")\n    model.eval()\n    print_dropouts(model, \"eval\")\n<\/code>\nCheers","y":"self.drop_rate is an attribute of the _DenseLayer module as seen here with link \"https:\/\/github.com\/pytorch\/vision\/blob\/684f48db4e6f619389da3a6957b3edebf794ae79\/torchvision\/models\/densenet.py#L35\".\nThe value won\u2019t change by calling model.train() or model.eval().\nInstead the functional dropout call in this line of code with link \"https:\/\/github.com\/pytorch\/vision\/blob\/684f48db4e6f619389da3a6957b3edebf794ae79\/torchvision\/models\/densenet.py#L88-L89\" will use the self.training attribute to decide if dropout should be used or not.","z":"self.drop_rate is an attribute of the _DenseLayer module as seen here with link \"https:\/\/github.com\/pytorch\/vision\/blob\/684f48db4e6f619389da3a6957b3edebf794ae79\/torchvision\/models\/densenet.py#L35\".\nThe value won\u2019t change by calling model.train() or model.eval().\nInstead the functional dropout call in this line of code with link \"https:\/\/github.com\/pytorch\/vision\/blob\/684f48db4e6f619389da3a6957b3edebf794ae79\/torchvision\/models\/densenet.py#L88-L89\" will use the self.training attribute to decide if dropout should be used or not."},{"x":"From my understanding it\u2019s ok to reuse nn.Relu and nn.Maxpool in the forward because they don\u2019t have trainable parameters. I\u2019m not entirely sure of why this works, perhaps someone can clarify this for me. Let\u2019s say I have\n<code class=\"lang-auto\">class NN(nn.Module):\n    def __init__(self, input_size, num_classes):\n        super(NN, self).__init__()\n        self.relu = nn.ReLU()\n        self.fc1 = nn.Linear(input_size, 50)\n        self.fc2 = nn.Linear(50, 25)\n        self.fc3 = nn.Linear(25, num_classes)\n    \n    def forward(self, x):\n        x = self.relu(self.fc1(x))\n        x = self.relu(self.fc2(x))\n        x = self.fc3(x)\n        return x\n<\/code>\nHow does Pytorch create the graph? I\u2019m thinking if I reuse same nn.ReLU then there would be a problem for Pytorch to remember which gradients to set to 0\/1. From my understanding it\u2019s not a good idea to reuse dropout, why is this the case?","y":"You can reuse parameter-less modules, as only the computation will be tracked.\nA theoretical example would be reusing nn.Add(a, b) instead of a + b.","z":"Someone that could help me out with this?\nYou can reuse parameter-less modules, as only the computation will be tracked.\nA theoretical example would be reusing nn.Add(a, b) instead of a + b.\nThat makes sense, but if we have dropout with the same droprate can we reuse that as well?\nYes, that should also work.\nThat\u2019s good to know, do you know where can I read more about how Pytorch does this backprop or why this works in this way?\nThanks a lot, I appreciate your help!\nA general rule might be that all operations without parameters (as in nn.Parameter) are fine to reuse, since Autograd will only track the operation.\nOf course you can also reuse modules with parameters, but this could be seen as \u201cweight sharing\u201d."},{"x":"Hi,\nI use the Jaccard coefficient to validate my binary segmentation model. I\u2019ve found a definition:\n\ndef get_jaccard(y_true, y_pred):\nepsilon = 1e-15\nintersection = (y_pred * y_true).sum(dim=-2).sum(dim=-1).sum(dim = -1)\nunion = y_true.sum(dim=-2).sum(dim=-1).sum(dim=-1) + y_pred.sum(dim=-2).sum(dim=-1).sum(dim = -1)\nreturn (intersection \/ (union - intersection + epsilon)).mean()\n\nInput and output have the same shape: [N, 1, 256, 256] where N is the batch size following by the image size. So it basically performs intersection and union on each image in the batch then takes the mIoU over the batch.\nI\u2019ve made a lightened implementation:\n\ndef jaccard_coeff(input, target):\neps = 1e-15\ninput = input.view(-1)\ntarget = target.view(-1)\nintersection = (input * target).sum()\nunion = (input.sum() + target.sum()) - intersection\nreturn (intersection \/ (union + eps))\n\nI directly flatten the tensors and perform operation without computing the mean. It returns different results and I wanted to know which one is the true one ?\nThe results are not equal so I would like to be sure I\u2019m using the metric in the right way for validation\nThanks","y":"There are several repositories where it was already implemented (probably in the correct way):\n\nhttps:\/\/github.com\/pytorch\/vision\/blob\/master\/references\/segmentation\/utils.py#L95\nhttps:\/\/github.com\/pytorch\/ignite\/blob\/master\/ignite\/metrics\/confusion_matrix.py#L129\n\nIn your case, as far as I understand the difference is the following:\n\nfirst implementation computes the mean over N images of Jaccard Index per image\nsecond implementation computes Jaccard Index as all N images were concat.\n\nMaybe, the second is more standard.\nHTH","z":"There are several repositories where it was already implemented (probably in the correct way):\n\nhttps:\/\/github.com\/pytorch\/vision\/blob\/master\/references\/segmentation\/utils.py#L95\nhttps:\/\/github.com\/pytorch\/ignite\/blob\/master\/ignite\/metrics\/confusion_matrix.py#L129\n\nIn your case, as far as I understand the difference is the following:\n\nfirst implementation computes the mean over N images of Jaccard Index per image\nsecond implementation computes Jaccard Index as all N images were concat.\n\nMaybe, the second is more standard.\nHTH\nThanks for your answer \nIndeed there are similarities with the second option."},{"x":"I\u2019ve implemented a basic sequential model with a bilinear upsampling layer for semantic segmentation of the CAMVID dataset with link \"http:\/\/mi.eng.cam.ac.uk\/research\/projects\/VideoRec\/CamVid\/\".\nIt seems to train fine for about 15-25 epochs before throwing this error:\n<code class=\"lang-auto\">RuntimeError: non-empty 3D or 4D input tensor expected but got ndim: 4\n<\/code>\nWhy would the tensor shape be wrong after several steps of training as opposed to from step 1?\nMy code:\n<code class=\"lang-auto\">#Build model\nclass VGG16(nn.Module):\n    def __init__(self, \n                 num_channels = num_channels, \n                 num_classes=num_classes, \n                 init_weights=True,\n                 h=height,\n                 w=width):\n      \n        super(VGG16, self).__init__()\n        self.encoder = nn.Sequential(\n            nn.Conv2d(num_channels, 64, kernel_size=3, padding=1),\n            nn.ReLU(),\n\n            nn.Conv2d(64, 64, kernel_size=3, padding=1),\n            nn.ReLU(),\n            nn.MaxPool2d(kernel_size=2, stride=2),\n\n            nn.Conv2d(64, 128, kernel_size=3, padding=1),\n            nn.ReLU(),\n\n            nn.Conv2d(128, 128, kernel_size=3, padding=1),\n            nn.ReLU(),\n            nn.MaxPool2d(kernel_size=2, stride=2),\n\n            nn.Conv2d(128, 256, kernel_size=3, padding=1),\n            nn.ReLU(),\n            nn.Conv2d(256, 256, kernel_size=3, padding=1),\n            nn.ReLU(),\n            nn.Conv2d(256, 256, kernel_size=3, padding=1),\n            nn.ReLU(),\n            nn.MaxPool2d(kernel_size=2, stride=2),\n\n            nn.Conv2d(256, 512, kernel_size=3, padding=1),\n            nn.ReLU(),\n            nn.Conv2d(512, 512, kernel_size=3, padding=1),\n            nn.ReLU(),\n            nn.Conv2d(512, 512, kernel_size=3, padding=1),\n            nn.ReLU(),\n            nn.MaxPool2d(kernel_size=2, stride=2, padding=1),\n\n            nn.Conv2d(512, 512, kernel_size=3, padding=1),\n            nn.ReLU(),\n            nn.Conv2d(512, 512, kernel_size=3, padding=1),\n            nn.ReLU(),\n            nn.Conv2d(512, 512, kernel_size=3, padding=1),\n            nn.ReLU(),\n            nn.MaxPool2d(kernel_size=2, stride=2, padding=1),\n\n            nn.Conv2d(512, 4096, 1),\n            nn.ReLU(),\n\n            nn.Conv2d(4096, 4096, 1),\n            nn.ReLU(),\n\n            nn.Conv2d(4096, num_classes, 1),\n            nn.Upsample(scale_factor=30, mode='bilinear'),\n        )\n\n    def forward(self, x):\n        x = self.encoder(x)\n        return x\n      \ndef init_weights(m):\n    if type(m) == nn.Linear:\n        torch.nn.init.xavier_uniform_(m)\n        m.bias.data.fill_(0.01)\n\n#Prepare Network for Training\nnetwork = VGG16()\nnetwork.cuda()\nnetwork.apply(init_weights)\n\n# Optimizer\noptimizer = torch.optim.Adam(network.parameters(), lr=0.005)\n\n# Loss\ncriterion = nn.NLLLoss()\ncriterion2 = nn.CrossEntropyLoss(weight=weights(y_train))\n\n#Training Function\ndef shuffle(x_train, y_train):\n    perm = torch.randperm(len(x_train))\n    samples = x_train[perm]\n    labels = y_train[perm]\n    return(samples, labels)\n\n#Track Metrics\ntraining_scores = []\nvalidation_scores = []\nmean_iou_scores = []\ndef train(batch_size=20, epochs=100):\n    n_batches = int(n \/ batch_size)\n    \n    for i in range(epochs):\n        x_shuffled, y_shuffled = shuffle(x_train,y_train)\n        for j in range(n_batches):\n            # Local batches and labels\n            x_batch, y_batch = Tensor(x_shuffled[i*batch_size:(i+1)*batch_size,]), Tensor(y_shuffled[i*batch_size:(i+1)*batch_size,])\n            optimizer.zero_grad()\n            y_batch = torch.reshape(y_batch, (-1, height, width))\n            training_output = network(x_batch.detach().cuda())\n            loss = criterion2(training_output, y_batch.cuda().long())\n            loss.backward()\n            optimizer.step()\n\n        # print metrics\n        prediction = torch.argmax(training_output, dim=1).float()\n        ious = calc_iou(prediction, y_batch, 12)\n        mean_iou_scores.append(np.average(ious))\n        print('[%d] loss: %.3f' %(i + 1, loss))\n        print(ious)\n<\/code>\nErrorBlock2473\u00d7805 115 KB with link \"https:\/\/discuss.pytorch.org\/uploads\/default\/original\/3X\/b\/e\/be67a955efc29ccdaf6604b8bec06e377a4bff10.png\"","y":"I found my problem\u2026 As I was using the wrong index for selection of my minibatches, at loop 24 I would run out of samples to select. Switching the \u201ci\u201d for \u201cj\u201d in the inner training loop should fix it.\nFor anyone who encounters this error in the future, perhaps it\u2019s an indexing problem.","z":"I found my problem\u2026 As I was using the wrong index for selection of my minibatches, at loop 24 I would run out of samples to select. Switching the \u201ci\u201d for \u201cj\u201d in the inner training loop should fix it.\nFor anyone who encounters this error in the future, perhaps it\u2019s an indexing problem."},{"x":"Hi all,\nI try to train a model by using multi-GPUs on single machine. And I want to figure out how the multiple GPUs connect with each other, (I mean the shape of the connection, full connect? in parallel? or in series?\nThanks!","y":"If you want to check how the devices are connected inside your machine, you could run nvidia-smi topo -m in your terminal.\nI might have misunderstood the question, but nn.DataParallel replicated the model on each device as explained here with link \"https:\/\/pytorch.org\/tutorials\/beginner\/former_torchies\/parallelism_tutorial.html\".","z":"If you want to check how the devices are connected inside your machine, you could run nvidia-smi topo -m in your terminal.\nI might have misunderstood the question, but nn.DataParallel replicated the model on each device as explained here with link \"https:\/\/pytorch.org\/tutorials\/beginner\/former_torchies\/parallelism_tutorial.html\".\n thanks for your reply!  I used the DataParallel module, and after I checked the code, I found that the GPU0 will gather the results from other GPUs like GPU1,2 and then it processes some computations by itself and then scatter to others. So I want to check the network between them (the way of communication). And how the GPU communicate with the CPU in this DataParallel approach. I tried to dive into the C++ code, however, I am not clear where could I search for this.\nThanks\nYou can find the implementation in data_parallel.py with link \"https:\/\/github.com\/pytorch\/pytorch\/blob\/master\/torch\/nn\/parallel\/data_parallel.py\".\nIt helps. Thanks for your help"},{"x":"<h1>Simple custom activation function causes CUDA out of memory.<\/h1>\nTags: Custom Activation Function, Memory Usage, CUDA out of memory\nProblem\nI am trying to implement a very simple activation function that turns every value that is higher than 1 to 1. I did the following:\n<code class=\"lang-python\">def zolu(input):\n    input[input>1] = 1\n    return input\n\nclass ZOLU(nn.Module):\n\n    def __init__(self):\n        super().__init__() # init the base class\n\n    def forward(self, input):\n        return zolu(input)\n<\/code>\nAnd used the function as an activation in my also rather simple model:\n<code class=\"lang-python\">class SuperResNet(nn.Module):\n\n    def __init__(self):\n        self.size = MODEL_INPUT_SIZE\n        super(SuperResNet, self).__init__()\n        self.up = nn.Upsample(size=MODEL_OUTPUT_SIZE[1:], mode=\"bicubic\", align_corners=False)\n        self.conv1 = nn.Conv2d(in_channels=3, out_channels=10, kernel_size=5, padding=2)\n        self.conv2 = nn.Conv2d(in_channels=10, out_channels=50, kernel_size=5, padding=2)\n        self.conv3 = nn.Conv2d(in_channels=50, out_channels=10, kernel_size=5, padding=2)\n        self.conv4 = nn.Conv2d(in_channels=10, out_channels=3, kernel_size=5, padding=2)\n\n    def forward(self, x):\n        x = zolu(self.up(x))\n        x = zolu(self.conv1(x))\n        # x = TF.relu(x)\n        x = zolu(self.conv2(x))\n        # x = TF.relu(x)\n        x = zolu(self.conv3(x))\n        # x = TF.relu(x)\n        x = zolu(self.conv4(x))\n        # x = TF.relu(x)\n        return x\n<\/code>\nWhen i try to run training i get this output:\n<code class=\"lang-python\">Traceback (most recent call last):\n  File \"C:\/Users\/sokad\/PycharmProjects\/SuperResoStrekal\/scripts\/train.py\", line 59, in <module>\n    outputs = super_res(crappy_train)\n  File \"C:\\Python37\\lib\\site-packages\\torch\\nn\\modules\\module.py\", line 532, in __call__\n    result = self.forward(*input, **kwargs)\n  File \"C:\\Users\\sokad\\PycharmProjects\\SuperResoStrekal\\scripts\\model.py\", line 47, in forward\n    x = zolu(self.conv2(x))\n  File \"C:\\Users\\sokad\\PycharmProjects\\SuperResoStrekal\\scripts\\model.py\", line 19, in zolu\n    input[input>1] = 1\nRuntimeError: CUDA out of memory. Tried to allocate 5.62 GiB (GPU 0; 8.00 GiB total capacity; 1.16 GiB already allocated; 4.48 GiB free; 1.57 GiB reserved in total by PyTorch)\n<\/code>\n<h1>Further Infos<\/h1>\n\ninstead of input[input > 1] i also tried input[input.gt(1)]\nwhen i switch the activations to RELU, everything works fine\nTotal params: 26,573\nTrainable params: 26,573\nInput size (MB): 4.79\nForward\/backward pass size (MB): 546.48\nInput shape: torch.Size([4, 3, 560, 748])\nBatch-Size = 4\nUsing Sampler-Class (CUDA was OOM without a sampler)\n","y":"You could try to replace the indexing of input with input = torch.clamp(input, max=1), which might reduce the memory footprint.\nLet me know, if that helps.","z":"You could try to replace the indexing of input with input = torch.clamp(input, max=1), which might reduce the memory footprint.\nLet me know, if that helps.\nThank you very much! It works totally fine! I just didn\u2019t know the clamp()-function yet."},{"x":"Hello, I am a beginner in Deep Learning and PyTorch. If my question is not relevant or does not follow the community guidelines, please pardon me.\nI am studying the following Kaggle kernel and trying to replicate it:\nhttps:\/\/www.kaggle.com\/piantic\/plant-pathology-2020-pytorch-for-beginner\nIn the section titled, \u201cPreTrainedModels\u201d, the author uses a resnet34 model and writes the following loss function class:\n<code class=\"lang-auto\">class DenseCrossEntropy(nn.Module):\n\n    def __init__(self):\n        super(DenseCrossEntropy, self).__init__()\n        \n        \n    def forward(self, logits, labels):\n        logits = logits.float()\n        labels = labels.float()\n        \n        logprobs = F.log_softmax(logits, dim=-1)\n        \n        loss = -labels * logprobs\n        loss = loss.sum(-1)\n\n        return loss.mean()\n<\/code>\nI\u2019ve read about Cross Entropy Loss function and I think I get the basic gist of it. But I do not understand what\u2019s going on in this class. Can someone please explain what\u2019s going on?\nI can see its calculating the log softmax but then why the multiplication with the labels? And why loss.sum(-1)?\nThis is a Kaggle competition where given a plant image, we have to predict 4 classes (healthy or 3 types of diseases).\nThank you so much for your help!","y":"Hi  Subhankar!\n\n\n\n SubhankarHalder:\n\n<code class=\"lang-auto\">class DenseCrossEntropy(nn.Module):\n...\n    def forward(self, logits, labels):\n        logits = logits.float()\n        labels = labels.float()\n        \n        logprobs = F.log_softmax(logits, dim=-1)\n        \n        loss = -labels * logprobs\n        loss = loss.sum(-1)\n\n        return loss.mean()\n<\/code>\nBut I do not understand what\u2019s going on in this class.\n\u2026\nI can see its calculating the log softmax but then why the multiplication with the labels? And why loss.sum(-1)?\n\n\nPytorch\u2019s CrossEntropyLoss takes as its target (labels) a single\ninteger class label for each sample in the batch (i.e., a tensor of shape\n[nBatch]).  That is, target == 2 means that class \u201c2\u201d is the right answer\nwith 100% certainty.\nThe notion of cross-entropy is more general, however, in that it compares\ntwo probability distributions.\nSo you might wish to work with probabilistic targets. e.g. for nBatch = 1:\n<code class=\"lang-python\">target = torch.FloatTensor ([[0.1, 0.2, 0.7]])\n<\/code>\nThis means that this sample is in class \u201c0\u201d with probability 10%, in\nclass \u201c1\u201d with probability 20%, and in class \u201c2\u201d with probability 70%.\nThis more general version of cross-entropy (which Kaggle is calling\nDenseCrossEntropy) is not supported directly in pytorch, hence the\nneed to implement it explicitly.  (But it\u2019s not very hard, and you can\ndo it using pytorch tensor operations, so you get the full benefits of\nautograd and gpu support.)\n\n\n<code class=\"lang-python\">        loss = -labels * logprobs\n        loss = loss.sum(-1)\n<\/code>\n\n\nThe \u201cmultiplication with the labels\u201d is multiplying the vector of\nprobabilistic labels (element-wise) with the vector of logprobs\nobtained from the vector of logits (raw-score predictions).  The\nsum is just summing these terms in the formula for cross-entropy\nacross classes (within a single sample, hence .sum (dim = -1)).\nTo recap one key point:  for DenseCrossEntropy, labels has\nshape [nBatch, nClass], while for torch.nn.CrossEntropyLoss,\nlabels (target) has shape [nBatch].\nYou can see the formula for this general cross-entropy in Wikipedia\u2019s\nCross entropy with link \"https:\/\/en.wikipedia.org\/wiki\/Cross_entropy\" entry.\nCompare this to the formula for pytorch\u2019s CrossEntropyLoss with link \"https:\/\/pytorch.org\/docs\/stable\/nn.html#torch.nn.CrossEntropyLoss\" that uses\ncategorical class labels for its target.\n(As an aside, you can use one-hot-encoded class labels as a special\ncase of probabilistic labels with DenseCrossEntropy.  These are\njust labels that consist of all 0s (0% probability) for all of the classes\nexcept for one 1 (100% probability) for the class that is being labelled\nas \u201ccorrect.\u201d)\nBest.\nK. Frank","z":"Hi  Subhankar!\n\n\n\n SubhankarHalder:\n\n<code class=\"lang-auto\">class DenseCrossEntropy(nn.Module):\n...\n    def forward(self, logits, labels):\n        logits = logits.float()\n        labels = labels.float()\n        \n        logprobs = F.log_softmax(logits, dim=-1)\n        \n        loss = -labels * logprobs\n        loss = loss.sum(-1)\n\n        return loss.mean()\n<\/code>\nBut I do not understand what\u2019s going on in this class.\n\u2026\nI can see its calculating the log softmax but then why the multiplication with the labels? And why loss.sum(-1)?\n\n\nPytorch\u2019s CrossEntropyLoss takes as its target (labels) a single\ninteger class label for each sample in the batch (i.e., a tensor of shape\n[nBatch]).  That is, target == 2 means that class \u201c2\u201d is the right answer\nwith 100% certainty.\nThe notion of cross-entropy is more general, however, in that it compares\ntwo probability distributions.\nSo you might wish to work with probabilistic targets. e.g. for nBatch = 1:\n<code class=\"lang-python\">target = torch.FloatTensor ([[0.1, 0.2, 0.7]])\n<\/code>\nThis means that this sample is in class \u201c0\u201d with probability 10%, in\nclass \u201c1\u201d with probability 20%, and in class \u201c2\u201d with probability 70%.\nThis more general version of cross-entropy (which Kaggle is calling\nDenseCrossEntropy) is not supported directly in pytorch, hence the\nneed to implement it explicitly.  (But it\u2019s not very hard, and you can\ndo it using pytorch tensor operations, so you get the full benefits of\nautograd and gpu support.)\n\n\n<code class=\"lang-python\">        loss = -labels * logprobs\n        loss = loss.sum(-1)\n<\/code>\n\n\nThe \u201cmultiplication with the labels\u201d is multiplying the vector of\nprobabilistic labels (element-wise) with the vector of logprobs\nobtained from the vector of logits (raw-score predictions).  The\nsum is just summing these terms in the formula for cross-entropy\nacross classes (within a single sample, hence .sum (dim = -1)).\nTo recap one key point:  for DenseCrossEntropy, labels has\nshape [nBatch, nClass], while for torch.nn.CrossEntropyLoss,\nlabels (target) has shape [nBatch].\nYou can see the formula for this general cross-entropy in Wikipedia\u2019s\nCross entropy with link \"https:\/\/en.wikipedia.org\/wiki\/Cross_entropy\" entry.\nCompare this to the formula for pytorch\u2019s CrossEntropyLoss with link \"https:\/\/pytorch.org\/docs\/stable\/nn.html#torch.nn.CrossEntropyLoss\" that uses\ncategorical class labels for its target.\n(As an aside, you can use one-hot-encoded class labels as a special\ncase of probabilistic labels with DenseCrossEntropy.  These are\njust labels that consist of all 0s (0% probability) for all of the classes\nexcept for one 1 (100% probability) for the class that is being labelled\nas \u201ccorrect.\u201d)\nBest.\nK. Frank\nThank you so much!! I understood this now."},{"x":"Hello,\nI am building a model for multiclass prediction for targets [-1,0,1] and currently getting an error from loss function that\n<code class=\"lang-auto\">Target -1 is out of bounds.\n<\/code>\nWell, it is a good idea to encode classes with torch.nn.functional.one_hot , but I can\u2019t encode negative values, as I know. Is there any other alternatives or solutions of how to encode or directly feed my targets?","y":"Couldn\u2019t you have targets [0,1,2] if the -1 is causing the issue? Could you share some code snippet to how it looks like now","z":"Couldn\u2019t you have targets [0,1,2] if the -1 is causing the issue? Could you share some code snippet to how it looks like now"},{"x":"Hi,\nI have a rather unique situation I believe (I searched online for hours but couldn\u2019t find a solution).\nI have a network with 2 types of input:\nOne is an image (say, 254x254), and one is a pair of integers (coordinates in the image).\nSome background:\nI\u2019d like to train a binary classification network with the inputs being an image tensor and a coordinates tensor. The way the network is currently designed, the image goes through some convolutions and some other \u201cheavy-lifting\u201d layers, while the coordinates goes through some shallow network and finally everything goes through a few fully connected layers.\nThe problem:\nI would like to refrain from having my different image tensors uploaded to the GPU as a part of a batch and discarded afterwards, even though we only used them to train one (image, coordinates) pair. That just takes too long. I already tried sampling a subset of all of the possible 254*254 possible coordinates to reduce training time, but I still can\u2019t find a way to solve the problem mentioned above. I would like to have something like this:\nEach batch of N samples is of the form IMAGE_K, coordinate_i where IMAGE_K is a shared between all the coordinates of the batch, and coordinate_i is the i\u2019th coordinate where 0 <= i <= N. In other words, I only load one image from disk and only one image occupies the GPU, while many many different coordinates are used in the training batch.\nObviously I don\u2019t really care if it\u2019s a single image in a batch or 2 or 3, but my point is that I don\u2019t want every training sample to be some random image+coordinate, because it makes no sense. Another way of thinking about it, in pseudo-code, is:\nfor every Image:\nfor every Coordinate in sample:\ntrain on (Image, Coordinate)\nI also thought about using a hypernetwork to solve it - one network, F,  is fed with an image and produces weights for a network G, where G is fed with a coordinate and, using the F\u2019s generated weights, predict 0 or 1. The problem was that implementing it was a nightmare, especially handling batches. Couldn\u2019t get the thing working.\nI\u2019m open to any suggestion, I\u2019m pretty lost.\nThanks","y":"Would splitting the Datasetsinto anImageDatasetandCoordDataset` work?\nYou could then use the nested loop approach and keep the image constant in the inner loop.\nPseudo code for the idea:\n<code class=\"lang-python\">image_loader = DataLoader(image_dataset)\ncoord_loader = DataLoader(coord_dataset)\n\nfor image in image_loader:\n    # if the coordinates depend on the current batch, initialize the coord_loader here\n    image = image.to('cuda')\n    for coord in coord_loader:\n        optimizer.zero_grad()\n        coord = coord.to('cuda')\n        output = model(image, coord)\n        loss = ...\n<\/code>\nLet me know, if this approach would work or if I misunderstood the use case.","z":"Would splitting the Datasetsinto anImageDatasetandCoordDataset` work?\nYou could then use the nested loop approach and keep the image constant in the inner loop.\nPseudo code for the idea:\n<code class=\"lang-python\">image_loader = DataLoader(image_dataset)\ncoord_loader = DataLoader(coord_dataset)\n\nfor image in image_loader:\n    # if the coordinates depend on the current batch, initialize the coord_loader here\n    image = image.to('cuda')\n    for coord in coord_loader:\n        optimizer.zero_grad()\n        coord = coord.to('cuda')\n        output = model(image, coord)\n        loss = ...\n<\/code>\nLet me know, if this approach would work or if I misunderstood the use case.\nHi, sorry for the delayed response, we had a holiday here \nThat\u2019s a really elegant and simple solution to my problem I wish I have thought about, I\u2019m a bit embarrassed haha. Will give it a try and let you know how it went, thank you so much for helping me out!\nOkay, that seems to do that trick. Thanks again "},{"x":"I am trying to implement  \u2013 \u201cThe learning rate is decayed exponentially from 0.01 to 0.0001 for 30 epochs.\u201d\nI found lr_scheduler = optim.lr_scheduler.ExponentialLR(optimizer, gamma)\n<code class=\"lang-auto\">optimizer = optim.Adam()\nlr_scheduler = optim.lr_scheduler.ExponentialLR(optimizer, gamma)\n\nfor epoch in range(nb_epoch):\n        for data in train_loader:\n                ....\n                ....\n                optimizer.step()\nlr_scheduler.step()\n<\/code>\nSo how do I use the epoch and learning rate 0.01 to 0.0001 in this case?","y":"If I\u2019ve understood it correctly you want to use exponential lr rate scheduler and starting at 0.01 you want it to decrease to 0.0001 after 30 epochs. We can do some calculations where we should have:\n0.01 * gamma^30 = 0.0001, from this we can imply that gamma = 0.01^(1\/30) = 0.85769\u2026\nSo set gamma approximately to 0.86 and it should work as you wanted (approximately).\nI think your lr_scheduler.step() should be inside the epoch for-loop.","z":"If you want to decay the learning rate every 30 epochs you could use\ntorch.optim.lr_scheduler.StepLR (optimizer , 30 , gamma=0.01)\nThanks for the response, but the decay has to be exponentially. So can I do the same in ExponentialLR ?\nIf I\u2019ve understood it correctly you want to use exponential lr rate scheduler and starting at 0.01 you want it to decrease to 0.0001 after 30 epochs. We can do some calculations where we should have:\n0.01 * gamma^30 = 0.0001, from this we can imply that gamma = 0.01^(1\/30) = 0.85769\u2026\nSo set gamma approximately to 0.86 and it should work as you wanted (approximately).\nI think your lr_scheduler.step() should be inside the epoch for-loop."},{"x":"Hi,\nI have an multi-label classification problem. There are 4 targets (500 observations each, for the first three labels and 50 observations for the fourth label). The loss function is cross entropy.\nI use pretrained model (ResNet18) in Pytorch.\nCase 1:\nWhen I use train_test_split from sklearn (with stratify) and use it as usual (creating instance of Dataset class and then feeding to the DataLoader), my validation loss seems to reduce over epochs.\nCase 2:\nWhen I do train_test_split (with stratify) and use WeightedRandomSampler (to account for the imbalance), my validation loss reduces slowly (requiring more epochs) but never to the level seen in \u201cCase 1\u201d. There is lot of fluctuation in the loss too.\nI saw the code for WeightedRandomSampler in GitHub. Using the WeightedRandomSampler, my dataset now contains more observations of the minor class, which is what I want. My questions are:\n\nIs \u201cCase 2\u201d, the expected behavior, when using the WeightedRandomSampler?\nAny thoughts how I can reduce validation loss, when using the WeightedRandomSampler?\n","y":"In your first case, the loss might decrease fast, if your model simply overfits the majority class(es), thus ignoring the minority class.\nYou could calculate the confusion matrix for both use cases and calculate the per-class accuracies to rate both approaches.\nNote that a simple accuracy calculation might be misleading for an imbalanced use case as explained by the Accuracy Paradox with link \"https:\/\/en.wikipedia.org\/wiki\/Accuracy_paradox\".","z":"In your first case, the loss might decrease fast, if your model simply overfits the majority class(es), thus ignoring the minority class.\nYou could calculate the confusion matrix for both use cases and calculate the per-class accuracies to rate both approaches.\nNote that a simple accuracy calculation might be misleading for an imbalanced use case as explained by the Accuracy Paradox with link \"https:\/\/en.wikipedia.org\/wiki\/Accuracy_paradox\".\n\n\n\n ptrblck:\n\nconfusion matrix\n\n\nThank you, will explore."},{"x":"Hi there, I\u2019m testing with fp16 features of pytorch with a benchmark script provided here with link \"https:\/\/gist.github.com\/csarofeen\/f07e2d59e56052dbfa695bac0748d764\", getting these result(all with CUDA8 and cuDNN6):\n<code class=\"lang-auto\">\u279c  ~ python test_pytorch_vgg19_fp16.py\nTitan X Pascal(Dell T630, anaconda2, pytorch 0.3.0):\nFP32 Iterations per second:  1.7890313980917067\nFP16 Iterations per second:  1.8345766566297141\n\nTesla P100(DGX-1, anaconda3, pytorch 0.3.0):\n('FP32 Iterations per second: ', 2.001722807676548)\n('FP16 Iterations per second: ', 1.9109340821222125)\n\nTesla P100(DGX-1, pytorch docker image provided by NVIDIA, pytorch 0.2.0):\nFP32 Iterations per second:  1.9826932313239591\nFP16 Iterations per second:  1.8854441494961636\n\n<\/code>\nFor the detailed information of the docker image refer to here with link \"http:\/\/docs.nvidia.com\/deeplearning\/dgx\/pytorch-release-notes\/rel_17.12.html#rel_17.12\", it is theoretically optimized for DGX-1.\nIt can be seen that on P100 fp16 is even slower than fp32. Am I doing anything wrong? or PyTorch fp16 support is still under heavily developing? If so is there any doc for fp16 support status?\nI would like to do more testing if needed, thanks a lot!\nAnd another thanks for this great framework!","y":"on P100 we dont expect FP16 to be any faster, because we disabled FP16 math on P100 (it is numerically unstable). We use simulated FP16, where storage is FP16, but compute is in FP32 (so it upconverts to FP32 before doing operations).","z":"on P100 we dont expect FP16 to be any faster, because we disabled FP16 math on P100 (it is numerically unstable). We use simulated FP16, where storage is FP16, but compute is in FP32 (so it upconverts to FP32 before doing operations).\nHow about in 2080ti, I test a model trained in FP32 and using model.half() and input.half() in 2080TI with batch size fixed to 1, but do not see any speed up. But I noticed that the gpu memory usage is reduced by about 30%, why is that?Thanks.\nWhat kind of operations are you using and which cudnn version in particular?\nIf you are using cudnn 7.3 and later, convolutions should use TensorCores for FP16 inputs.\nGEMMs (e.g. used in linear layers) however have a size restriction of multiples of 8. For matrix A x matrix B, where A has size [I, J] and B has size [J, K], I, J, and K must be multiples of 8 to use TensorCores. This requirement exists for all cublas and cudnn versions.\nAlso, could you try to use torch.backends.cudnn.benchmark = True at the beginning of your script?\nOperation contains maily conv2d and conv_transpose2d. I have set cudnn benchmark to true, but some of my matrix is not multiples of 8, I will try to change it then test. CUDA Version 9.2.148, cudnn 7.6.3.\nThank you very much.\nwhich hardware do pytorch call fp16 cuda kernels during inference?\nI have tried inference with Xavier, but no luck.\nI guess it works on V100?"},{"x":"Thanks everyone.\nMy dataset contains 15 million images. I have convert them into lmdb format and concat them\nAt first I set shuffle = False\uff0cenvery iteration\u2019s IO take no extra cost.\nInorder to improve the performance , I set it into True and use num_workers.\n<code class=\"lang-auto\">train_data = ConcatDataset([train_data_1,train_data_2])\n    train_loader = DataLoader(dataset=train_data, batch_size=64,num_workers=32, shuffle=True,pin_memory=False)\nfor i in range(epochs):\nfor j,data in enumerate(train_loader):\n                print(i,\"ITER\",j,\"IO END\",datetime.now())\n                continue\n<\/code>\nBut IO takes too much time.\n\nIs there something I can do to makes IO faster?","y":"Hi,\nHave you evaluated the behaviour using both num_workers=0 in both cases?\nIs it possible your data is optimized for sequential reading (im not familiarized with lmdb  format)?\nSetting shuffle=True does nothing but replacing a sequential generator range(0,len) by a list of random indices.","z":"Hi,\nHave you evaluated the behaviour using both num_workers=0 in both cases?\nIs it possible your data is optimized for sequential reading (im not familiarized with lmdb  format)?\nSetting shuffle=True does nothing but replacing a sequential generator range(0,len) by a list of random indices.\nThanks very much.\nset num_workers = 0 doesnot work.\nIt seems that the cloud sever cause the problem. I put the dataset in the SSD of my local machine and solve the problem."},{"x":"this is my train.py  I use crosssentropy loss for my network , outputs  size is [4,2,224,224] where 4 means batchsize, 2 means channels, 224 means h and w .output_c1 size is [4,224,224] ,labels size is [4,224,224] too.\n<code class=\"lang-auto\">import torch\nimport torch.nn as nn\nimport torch.nn.functional as F\nfrom torch.autograd import Variable\nfrom model import U_net\nimport visdom\nfrom dataset import driveDateset\nfrom torch import optim\nfrom Dice_loss import DiceLoss\nfrom Dice_loss import MulticlassDiceLoss\nimport matplotlib.pylab as plt\nimport numpy as np\nimport time\n\nif __name__ == '__main__':\n    DATA_DIRECTORY = \"F:\\\\experiment_code\\\\U-net\\\\DRIVE\\\\training\"\n    DATA_LIST_PATH = \"F:\\\\experiment_code\\\\U-net\\DRIVE\\\\training\\\\images_id.txt\"\n    Batch_size = 4\n    epochs = 100\n    dst = driveDateset(DATA_DIRECTORY, DATA_LIST_PATH)\n\n    # Initialize model\n    device = torch.device(\"cuda\")\n    model = U_net()\n    model.to(device)\n    optimizer = torch.optim.SGD(model.parameters(), lr=0.001, momentum=0.9)\n    criteon = nn.CrossEntropyLoss() #reduce=False\n    best_acc, best_epoch =0, 0\n    global_step = 0\n    start_time = time.time()\n    viz = visdom.Visdom()\n   for epoch in range(epochs):\n        running_corrects = 0\n        since_epoch = time.time()\n        trainloader = torch.utils.data.DataLoader(dst, batch_size=Batch_size) #,shuffle =True\n        for step, data in enumerate(trainloader):\n            imgs, labels, _, _ = data\n            imgs, labels = imgs.to(device), labels.to(device)\n            labels = labels.long()\n            model.train()\n            outputs = model(imgs)   # output  B * C * H *W\n            output_c1 = outputs[:,0,:,:] # C are 2 channels ,I choose the second channel\n            Rounding_output_c1 = torch.round(output_c1)\n            Rounding_output_c11 = torch.stack([(Rounding_output_c1 == i).float() for i in range(256)]) #[4,224,224]->[256,4,224,224] 256 is the number of classes, means pixel from 0-255\n            Rounding_output_c11 = Rounding_output_c11.permute(1,0,2,3) #[256,4,224,224]->[4,256,224,224]\n            loss = criteon(Rounding_output_c11,labels)\n            loss.requires_grad = True\n            optimizer.zero_grad()\n            loss.backward()\n            optimizer.step()\n            viz.line([loss.item()],[global_step], win='loss', update='append',opts=dict(title='train_loss'))\n            labels_float = labels.float()\n            running_corrects = torch.sum(Rounding_output_c1 == labels_float).float()\n            labels_size = labels.size(1) * labels.size(2) * 4\n            training_acc = running_corrects \/ labels_size\n            time_elapsed_epoch = time.time() - since_epoch\n            print('epoch :', epoch, '\\t', 'loss:', loss.item(),'\\t','training_acc',training_acc,'\\t','{:.0f}m {:.0f}s'.format(time_elapsed_epoch \/\/ 60, time_elapsed_epoch % 60))\n            global_step += 1\n<\/code>\nI test Data incoming has no problem ,every training data has Traversed. training data and label like this\nHcLxo638\u00d7578 39.4 KB with link \"https:\/\/discuss.pytorch.org\/uploads\/default\/original\/3X\/1\/3\/13d3a37b4cd5741cc40a0fe7557569d3670233b1.png\"\nbut when I train my network ,output picture like this ,\n\nand some result like this :\n<code class=\"lang-auto\">epoch : 0 \t loss: 5.2415571212768555 \t training_acc tensor(0.3103, device='cuda:0') \t 0m 2s\nepoch : 0 \t loss: 5.228370666503906 \t training_acc tensor(0.3235, device='cuda:0') \t 0m 2s\nepoch : 0 \t loss: 5.224219799041748 \t training_acc tensor(0.3276, device='cuda:0') \t 0m 2s\nepoch : 0 \t loss: 5.222436428070068 \t training_acc tensor(0.3294, device='cuda:0') \t 0m 2s\nepoch : 0 \t loss: 5.2202887535095215 \t training_acc tensor(0.3316, device='cuda:0') \t 0m 2s\nepoch : 1 \t loss: 5.2415571212768555 \t training_acc tensor(0.3103, device='cuda:0') \t 0m 0s\nepoch : 1 \t loss: 5.22836971282959 \t training_acc tensor(0.3235, device='cuda:0') \t 0m 0s\nepoch : 1 \t loss: 5.224219799041748 \t training_acc tensor(0.3276, device='cuda:0') \t 0m 0s\nepoch : 1 \t loss: 5.222436428070068 \t training_acc tensor(0.3294, device='cuda:0') \t 0m 1s\nepoch : 1 \t loss: 5.2202887535095215 \t training_acc tensor(0.3316, device='cuda:0') \t 0m 1s\nepoch : 2 \t loss: 5.2415571212768555 \t training_acc tensor(0.3103, device='cuda:0') \t 0m 0s\nepoch : 2 \t loss: 5.22836971282959 \t training_acc tensor(0.3235, device='cuda:0') \t 0m 0s\nepoch : 2 \t loss: 5.224219799041748 \t training_acc tensor(0.3276, device='cuda:0') \t 0m 0s\nepoch : 2 \t loss: 5.222436428070068 \t training_acc tensor(0.3294, device='cuda:0') \t 0m 1s\nepoch : 2 \t loss: 5.2202887535095215 \t training_acc tensor(0.3316, device='cuda:0') \t 0m 1s\nepoch : 3 \t loss: 5.2415571212768555 \t training_acc tensor(0.3103, device='cuda:0') \t 0m 0s\nepoch : 3 \t loss: 5.22836971282959 \t training_acc tensor(0.3235, device='cuda:0') \t 0m 0s\nepoch : 3 \t loss: 5.224219799041748 \t training_acc tensor(0.3276, device='cuda:0') \t 0m 0s\nepoch : 3 \t loss: 5.222436428070068 \t training_acc tensor(0.3294, device='cuda:0') \t 0m 1s\nepoch : 3 \t loss: 5.2202887535095215 \t training_acc tensor(0.3316, device='cuda:0') \t 0m 1s\n\n<\/code>\nI don\u2019t konw how to solve this problem","y":"Hello fyy!\n\n\n\n fyy:\n\n,\nI try to use U-net model to perform medical image segmentation\n\n\nAt this point, the best I can offer you is some general advice.\nFirst:  Independent of u-net or pytorch or machine learning, you\nneed to understand the problem you are trying to solve.  You\nshould look at the data you are working with.  Print out some\nimages and try to segment them by hand (without a computer).\nDraw the segmentation in with a pencil, and see how well you\ncan do.\nWith all due respect, you replied:\n\n\nlabels[0,17,128]  tell us The first picture in a batch that Pixel values in row 17 and column 128\n\n\nto my question:\n\n\n\n KFrank:\n\nWhat do your labels mean? To be concrete, you say that your\nbatch-of-labels tensor has shape [4, 244, 244] . What does\nthe value of a specific element of that tensor mean. That is,\nlabels[0, 17, 128] is some number. Conceptually, what\nis that number telling us?\n\n\nYour data has meaning.  It\u2019s virtually impossible to do any kind\nof worthwhile data analysis (machine learning or not) without\nunderstanding your data.\nSecond:  You need to learn the basics of pytorch before tackling a\nmore substantive problem.  I suggest that you work though some of\nthe pytorch tutorials to learn the framework and tools.  In particular\nthe Training a Classifier with link \"https:\/\/pytorch.org\/tutorials\/beginner\/blitz\/cifar10_tutorial.html#sphx-glr-beginner-blitz-cifar10-tutorial-py\" tutorial should be useful because your\nsegmentation problem is a kind of classification problem.\nA hint:  I\u2019m convinced that you are working on a binary segmentation\n(classification) problem.  Even though you posted them in (false) color,\nwhen I look at your sample labels images, I see black-and-white (that\nis, binary, rather than grayscale) images.\nGood luck.\nK. Frank","z":"Hi fyy!\nI believe that your network isn\u2019t actually training.\n\n\n\n fyy:\n\n<code class=\"lang-auto\">        trainloader = torch.utils.data.DataLoader(dst, batch_size=Batch_size) #,shuffle =True\n<\/code>\n\n\nNote, that because you do not shuffle your dataset, you will run\non the exact same batches in each epoch.\n\n\n<code class=\"lang-auto\">            Rounding_output_c1 = torch.round(output_c1)\n<\/code>\n\n\nAlthough technically differentiable, the derivative of torch.round()\nis zero (almost) everywhere.  So the gradients that flow back to\nyour model parameters will be zero, and optimizer.step()\nwon\u2019t actually do anything.\n\n\n<code class=\"lang-auto\">            loss.requires_grad = True\n<\/code>\n\n\nThis is a red flag.  Did you just put this in for good luck?  Or did\nyou observe that loss.grad == False?  If the latter, something\nupstream of loss in your processing is breaking \/ detaching\nyour computation graph (and would prevent gradients, zero or\nnot, from flowing back to your model parameters).\n\n\nand some result like this :\n<code class=\"lang-auto\">epoch : 0 \t loss: 5.2415571212768555 \t training_acc tensor(0.3103, device='cuda:0') \t 0m 2s\nepoch : 0 \t loss: 5.228370666503906 \t training_acc tensor(0.3235, device='cuda:0') \t 0m 2s\nepoch : 0 \t loss: 5.224219799041748 \t training_acc tensor(0.3276, device='cuda:0') \t 0m 2s\nepoch : 0 \t loss: 5.222436428070068 \t training_acc tensor(0.3294, device='cuda:0') \t 0m 2s\nepoch : 0 \t loss: 5.2202887535095215 \t training_acc tensor(0.3316, device='cuda:0') \t 0m 2s\nepoch : 1 \t loss: 5.2415571212768555 \t training_acc tensor(0.3103, device='cuda:0') \t 0m 0s\nepoch : 1 \t loss: 5.22836971282959 \t training_acc tensor(0.3235, device='cuda:0') \t 0m 0s\nepoch : 1 \t loss: 5.224219799041748 \t training_acc tensor(0.3276, device='cuda:0') \t 0m 0s\nepoch : 1 \t loss: 5.222436428070068 \t training_acc tensor(0.3294, device='cuda:0') \t 0m 1s\nepoch : 1 \t loss: 5.2202887535095215 \t training_acc tensor(0.3316, device='cuda:0') \t 0m 1s\n...\n<\/code>\n\n\nI see that the loss ad accuracy repeats itself exactly* from epoch\nto epoch.  So it appears that your model is not training.\n*)  The second loss in epoch 0 is slightly different than the second\nloss in subsequent epochs.  I\u2019ll choose to attribute that to a slightly\ndifferent order of operations in the gpu and therefore differing\nround-off error.\nSo my working hypothesis is that your model isn\u2019t changing at all.\nYou get different results, of course, for different batches within an\nepoch, but when you analyze the same batch in a subsequent,\nyou get the same result.\nThe only thing that doesn\u2019t fit with this explanation is that your loss\nis going down systematically from batch to batch (within an epoch),\nand your accuracy is going up.  That makes it look like your model\nis training.  But I don\u2019t see anything in your code that would reset\nyour model from one epoch to the next.  It is hard for me to attribute\nthe decreasing loss to random luck, although it could be due to\nsome structure in the (unshuffled) order of samples in your training\nset.\nStep 1:  Get rid of torch.round() (and any other zero-derivative\nfunctions) leading up to your loss function.\nStep 2:  Why are you calling loss.requires_grad = True?\n(And why isn\u2019t it throwing an error?  What version of pytorch are\nyou using?)\nGood luck.\nK. Frank\n \uff0cThank you for your kind help\uff0c\n1.I use Rounding_output_c1 = torch.round(output_c1)\nbecause I want to make my network\u2019s output is integer,\nand use these code to caculate my training_acc\n labels_float = labels.float()  #labels is not Decimal, labels size is [4,224,224]\nrunning_corrects = torch.sum(Rounding_output_c1 == labels_float).float()\nlabels_size = labels.size(1) * labels.size(2) * 4\ntraining_acc = running_corrects \/ labels_size\nand if I don\u2019t use Rounding_output_c1 = torch.round(output_c1),  training_acc will be 0.\n2 I use loss.requires_grad = True   because when I debug my code I observed that  loss requires_grad is False if I don\u2019t use this code It will get an error .\nTraceback (most recent call last):\n File \"F:\/experiment_code\/U-net\/train_2.py\", line 85, in <module>\n loss.backward()\n File \"D:\\Anaconda3\\lib\\site-packages\\torch\\tensor.py\", line 107, in backward\ntorch.autograd.backward(self, gradient, retain_graph, create_graph)\nFile \"D:\\Anaconda3\\lib\\site-packages\\torch\\autograd\\__init__.py\", line 93, in backward\n allow_unreachable=True)  # allow_unreachable flag\nRuntimeError: element 0 of tensors does not require grad and does not have a grad_fn\nmy pytorch version is \nHi fyy!\nFirst, I don\u2019t think the overall structure of what you are doing is\ncorrect.  More on that, below.\nSome specific comments, in line:\n\n\n\n fyy:\n\n1.I use Rounding_output_c1 = torch.round(output_c1)\nbecause I want to make my network\u2019s output is integer,\nand use these code to caculate my training_acc\n\u2026\nand if I don\u2019t use Rounding_output_c1 = torch.round(output_c1),  training_acc will be 0.\n\n\nAccuracy and loss are two different things.  Accuracy is the count\nof how many right answers you get, and doesn\u2019t need to be\ndifferentiable.\nLoss is the quantity the optimizer attempts to minimize while\ntraining, and does need to be differentiable in order for pytorch\u2019s\ngradient-descent optimization algorithms to work.\nOf course, your loss function should have some relationship to\nyour accuracy for it to make sense, but they aren\u2019t the same.\nEven if it did make sense to use torch.round() to calculate\nyour accuracy (and I don\u2019t think it does in your case), you can\u2019t\nuse it for your loss function because it\u2019s not usefully differentiable.\n\n\n2 I use loss.requires_grad = True   because when I debug my code I observed that  loss requires_grad is False\n\n\nThis means that something in your code is breaking your autograd\ncomputation graph.  (Although torch.round() won\u2019t work for\nyour loss function, it is technically differentiable, and I don\u2019t think\nit breaks the computation graph.)\nI\u2019m not sure, but maybe this for loop breaks the graph?\n<code class=\"lang-auto\">Rounding_output_c11 = torch.stack([(Rounding_output_c1 == i).float() for i in range(256)]) #[4,224,224]->[256,4,224,224] 256 is the number of classes, means pixel from 0-255\n<\/code>\nPerhaps  might be able to spot where the graph is getting\nbroken.\nSome comments about your overall approach:\n\n\n\n fyy:\n\nI use crosssentropy loss for my network , outputs size is [4,2,224,224] where 4 means batchsize, 2 means channels, 224 means h and w .output_c1 size is [4,224,224] ,labels size is [4,224,224] too.\n\n\nFrom this I speculate that you are performing multi-class image\nsegmentation, that is, that you are classifying each pixel in an\nimage, assigning them nClass different class labels.\nThe question is what is nClass, that is, what are the possible\nvalues of each element of your [nBatch, 244, 244] labels\ntensor?\nYou do say # 256 is the number of classes, but that seems\nrather large.  It is the number of possible values of an 8-bit pixel, but\ndo all such values really occur in your labels tensor?\nAnyway, let\u2019s call it nClass, and assume that each value in labels\nis an integer class label in the range [0, nClass - 1], inclusive.\n(I am assuming that nClass != 2, that is, that this is not a binary\nsegmentation \/ classification problem.)\nFor this you want to use CrossEntropyLoss, and structure your\nnetwork output to have shape [nBatch, nClass, 244, 244].\n(These are your predictions and are the input to the loss function.)\nNote that your number of channels (2) does not show up in the\nshape your your output (nor in the shape of labels).\nYour labels, again, have shape [nBatch, 244, 244] and are\nthe target passed to CrossEntropyLoss.  (nClass enters into\nlabels not in its shape, but  in the fact the values in labels\nrange from 0 to nClass - 1.)\nIf the above makes sense for your use case, then to calculate your\naccuracy you would test input.argmax (dim = 1) for equality\nagainst labels (the target you pass to CrossEntropyLoss),\nand count the matches.\nGood luck.\nK. Frank\n\n\n\n KFrank:\n\nI\u2019m not sure, but maybe this for loop breaks the graph?\n\n\nYes, I think you nailed it down.\nHere is a small dummy example:\n<code class=\"lang-python\">x = torch.randn(10, requires_grad=True)\ny = torch.stack([(x == i).float() for i in range(256)])\nprint(y.requires_grad)\n>  False\n<\/code>\n explained the usage of nn.CrossEntropyLoss really well, so I don\u2019t have anything to add. \n\uff0cThank you for your answer\u3002 Rounding_output_c11 = torch.stack([(Rounding_output_c1 == i).float() for i in range(256)]) #[4,224,224]->[256,4,224,224] 256 is the number of classes, means pixel from 0-255]) is truthly break the graph, I just want to use this code to use one-hot encoding to change output_c1's shape , nClass is pixel value from 0 to 255.\nthe values of each element of my [nBatch, 244, 244] labels tensor is from 0 to 255.\nmy network\u2019s output is output_c1 which size is [4,2,224,224] ,It value from 0-255.\nI want to use\noutput_c1 = outputs[:,0,:,:]  # outputs [4,2,224,224] ->outputs_c1[4,224,2224]\nto choose the second channel to extract Foreground image\nIf I want to use CrossEntropyLoss ,I don\u2019t konw  how to change my outputs\u2019s shape and labels\u2019s shape to correctly calculate the loss . \n .thank you for your help ,from your small dummy example,  I find  Rounding_output_c11 = torch.stack([(Rounding_output_c1 == i).float() for i in range(256)]) breaks the graph. \nHi fyy!\n\n\n\n fyy:\n\nIf I want to use CrossEntropyLoss ,I don\u2019t konw  how to change my outputs\u2019s shape and labels\u2019s shape to correctly calculate the loss .\n\n\nI still don\u2019t think that you\u2019re going about this the right way.  And I\ndoubt that you really have a 256-class classification problem.\nI assume that the input to your model is some sort of image.\nWhat is the conceptual meaning of such an image, and what\nis its shape?\nIf you successfully train your model, what, at a high, conceptual\nlevel, is your model supposed to tell you about an input image?\nWhat do your labels mean?  To be concrete, you say that your\nbatch-of-labels tensor has shape [4, 244, 244].  What does\nthe value of a specific element of that tensor mean.  That is,\nlabels[0, 17, 128] is some number.  Conceptually, what\nis that number telling us?\nBest.\nK. Frank\n,\nHi, KFrank!\nMy dataset is Fundus image. I try to use U-net model to perform medical image segmentation\n        for step, data in enumerate(trainloader):\n            imgs, labels, _, _ = data\n            model.train()\n            outputs = model(imgs)   #   B * C * H *W  outputs [4,2,224,224]\n            output_c1 = outputs[:,1,:,:]  # output_c1 [4,224,224]\n            labels_show =labels.cpu().detach().numpy().astype(np.uint8)\n            img_show = imgs.cpu().detach().numpy().astype(np.uint8)\n            plt.figure()\n            plt.subplot(4, 2, 1)\n            plt.imshow(labels_show[0,:,:]),plt.axis('off')\n            plt.subplot(4, 2, 2)\n            plt.imshow(img_show[0,1,:,:]),plt.axis('off')\n            plt.subplot(4, 2, 3)\n            plt.imshow(labels_show[1,:,:]),plt.axis('off')\n            plt.subplot(4, 2, 4)\n            plt.imshow(img_show[1,1,:,:]),plt.axis('off')\n            plt.subplot(4, 2, 5)\n            plt.imshow(labels_show[2,:,:]),plt.axis('off')\n            plt.subplot(4, 2, 6)\n            plt.imshow(img_show[2,1,:,:]),plt.axis('off')\n            plt.subplot(4, 2, 7)\n            plt.imshow(labels_show[3,:,:]),plt.axis('off')\n            plt.subplot(4, 2, 8)\n            plt.imshow(img_show[3,1,:,:]),plt.axis('off')\n            plt.pause(0.5) \n\nimage639\u00d7570 53.8 KB with link \"https:\/\/discuss.pytorch.org\/uploads\/default\/original\/3X\/2\/2\/225e4092cfa62b3fad43fe891c238384e9e0d43c.png\"\nLeft column isFundus vessels pictures and also is  labels, right colum is  Fundus image and also is  imgs .\nlabels information is here\n\nlabels[0,17,128]  tell us The first picture in a batch that Pixel values in row 17 and column 128\nand imgs information is here\n\noutputs information is here\n\noutput_c1  information is here\n\nHello fyy!\n\n\n\n fyy:\n\n,\nI try to use U-net model to perform medical image segmentation\n\n\nAt this point, the best I can offer you is some general advice.\nFirst:  Independent of u-net or pytorch or machine learning, you\nneed to understand the problem you are trying to solve.  You\nshould look at the data you are working with.  Print out some\nimages and try to segment them by hand (without a computer).\nDraw the segmentation in with a pencil, and see how well you\ncan do.\nWith all due respect, you replied:\n\n\nlabels[0,17,128]  tell us The first picture in a batch that Pixel values in row 17 and column 128\n\n\nto my question:\n\n\n\n KFrank:\n\nWhat do your labels mean? To be concrete, you say that your\nbatch-of-labels tensor has shape [4, 244, 244] . What does\nthe value of a specific element of that tensor mean. That is,\nlabels[0, 17, 128] is some number. Conceptually, what\nis that number telling us?\n\n\nYour data has meaning.  It\u2019s virtually impossible to do any kind\nof worthwhile data analysis (machine learning or not) without\nunderstanding your data.\nSecond:  You need to learn the basics of pytorch before tackling a\nmore substantive problem.  I suggest that you work though some of\nthe pytorch tutorials to learn the framework and tools.  In particular\nthe Training a Classifier with link \"https:\/\/pytorch.org\/tutorials\/beginner\/blitz\/cifar10_tutorial.html#sphx-glr-beginner-blitz-cifar10-tutorial-py\" tutorial should be useful because your\nsegmentation problem is a kind of classification problem.\nA hint:  I\u2019m convinced that you are working on a binary segmentation\n(classification) problem.  Even though you posted them in (false) color,\nwhen I look at your sample labels images, I see black-and-white (that\nis, binary, rather than grayscale) images.\nGood luck.\nK. Frank\n  .thank you for your help."},{"x":"So after using multiple convolutional layers on an image, I end up with (b, c, x, y). I am now wanting each of my spatial units with their channels to go through their own separate linear layers.\nI believe I have accomplished this with a time-distributed layer via this code: (Please excuse my coding verbosity)\n<code class=\"lang-auto\">class TimeDistributed(nn.Module):\n    def __init__(self, module):\n        super(TimeDistributed, self).__init__()\n        self.module = module\n\n    def forward(self, x):\n        # input shape =  batch, channel, h, w\n        # transform into batch, timestep, channel\n        sh = x.size()\n        #isolate batch, channel, x and y sizes\n        b = sh[0]\n        c = sh[1]\n        x = sh[2]\n        y = sh[3]\n        #move around and reshape\n        x = x.permute(0,2,3,1).contiguous().view(b, x*y, c)\n        #put it through the neural module. \n        x = self.module(x)\n        # Reshape it back\n        x = x.permute(0, 2, 1).contiguous()\n        x = x.view((b, :, x, y)) \n        return x\n<\/code>\nWith the neural module being a series of linear layers. I believe this should work as linear layers can take 3d input. (correct me if I am wrong).\nNow that I have done this, however, I am wondering whether I could have accomplished the same thing via a conv layer with a (1,1) kernel. Is my thinking there correct? Are there any advantages\/disadvantages between the two techniques?","y":"Let future me answer past me: The answer is yes. A 1 by 1 convolution is essentially the same as doing a time distributed layer over the spatial dimensions. If you are in this situation I suggest the 1by1 conv because it is more efficient.","z":"Let future me answer past me: The answer is yes. A 1 by 1 convolution is essentially the same as doing a time distributed layer over the spatial dimensions. If you are in this situation I suggest the 1by1 conv because it is more efficient."},{"x":"I ran U-net (with softmax) on Camvid data to predict multi-class segmentation. Seems everything ran well but I am having trouble visualizing the predictions. They look flat as shown below.\nFull code can be accessed here with link \"https:\/\/gist.github.com\/gireeshkbogu\/567cfa31417d63e58a45e4c8bc5ced06\".\nThe output looks like this Screen Shot 2020-03-25 at 10.59.46 AM1678\u00d7418 203 KB with link \"https:\/\/discuss.pytorch.org\/uploads\/default\/original\/3X\/4\/0\/403504935577370f07cb64d678d319d145dc0974.png\".\nhelper function for data visualization\n<code class=\"lang-auto\">def visualize(**images):\n    \"\"\"PLot images in one row.\"\"\"\n    n = len(images)\n    plt.figure(figsize=(16, 5))\n    for i, (name, image) in enumerate(images.items()):\n        plt.subplot(1, n, i + 1)\n        plt.xticks([])\n        plt.yticks([])\n        plt.title(' '.join(name.split('_')).title())\n        plt.imshow(image)\n    plt.show()\n<\/code>\napplying visualize on predictions\n<code class=\"lang-auto\">image, gt_mask = test_dataset[n]\nx_tensor = torch.from_numpy(image).to(DEVICE).unsqueeze(0)\n\ngt_mask_0 = (gt_mask[...,0].squeeze())  \ngt_mask_1 = (gt_mask[...,1].squeeze())\n\npr_mask = model.predict(x_tensor)\npr_mask_0 = (pr_mask[...,0].squeeze().cpu().numpy().round())   \npr_mask_1 = (pr_mask[...,1].squeeze().cpu().numpy().round())   \n\nvisualize(\n    image=image_vis, \n    ground_truth_mask=gt_mask_0,\n    sky_mask = pr_mask_0\n)\n<\/code>\ngt_mask\n<code class=\"lang-auto\">array([[[1., 1., 1., ..., 1., 1., 1.],\n        [1., 1., 1., ..., 1., 1., 1.],\n        [1., 1., 1., ..., 1., 1., 1.],\n        ...,\n        [0., 0., 0., ..., 0., 0., 0.],\n        [0., 0., 0., ..., 0., 0., 0.],\n        [0., 0., 0., ..., 0., 0., 0.]],\n\n       [[0., 0., 0., ..., 0., 0., 0.],\n        [0., 0., 0., ..., 0., 0., 0.],\n        [0., 0., 0., ..., 0., 0., 0.],\n        ...,\n        [0., 0., 0., ..., 0., 0., 0.],\n        [0., 0., 0., ..., 0., 0., 0.],\n        [0., 0., 0., ..., 0., 0., 0.]],\n\n       [[0., 0., 0., ..., 0., 0., 0.],\n        [0., 0., 0., ..., 0., 0., 0.],\n        [0., 0., 0., ..., 0., 0., 0.],\n        ...,\n        [1., 1., 1., ..., 1., 1., 1.],\n        [1., 1., 1., ..., 1., 1., 1.],\n        [1., 1., 1., ..., 1., 1., 1.]],\n\n       ...,\n\n       [[0., 0., 0., ..., 0., 0., 0.],\n        [0., 0., 0., ..., 0., 0., 0.],\n        [0., 0., 0., ..., 0., 0., 0.],\n        ...,\n        [0., 0., 0., ..., 0., 0., 0.],\n        [0., 0., 0., ..., 0., 0., 0.],\n        [0., 0., 0., ..., 0., 0., 0.]],\n\n       [[0., 0., 0., ..., 0., 0., 0.],\n        [0., 0., 0., ..., 0., 0., 0.],\n        [0., 0., 0., ..., 0., 0., 0.],\n        ...,\n        [0., 0., 0., ..., 0., 0., 0.],\n        [0., 0., 0., ..., 0., 0., 0.],\n        [0., 0., 0., ..., 0., 0., 0.]],\n\n       [[0., 0., 0., ..., 0., 0., 0.],\n        [0., 0., 0., ..., 0., 0., 0.],\n        [0., 0., 0., ..., 0., 0., 0.],\n        ...,\n        [0., 0., 0., ..., 0., 0., 0.],\n        [0., 0., 0., ..., 0., 0., 0.],\n        [0., 0., 0., ..., 0., 0., 0.]]], dtype=float32)\n<\/code>\npr_mask\n<code class=\"lang-auto\">tensor([[[[0.0089, 0.0096, 0.0101,  ..., 0.0044, 0.0041, 0.0038],\n          [0.0094, 0.0098, 0.0099,  ..., 0.0057, 0.0056, 0.0053],\n          [0.0097, 0.0097, 0.0094,  ..., 0.0071, 0.0072, 0.0072],\n          ...,\n          [0.0126, 0.0129, 0.0128,  ..., 0.0063, 0.0070, 0.0076],\n          [0.0113, 0.0116, 0.0115,  ..., 0.0062, 0.0073, 0.0086],\n          [0.0101, 0.0103, 0.0102,  ..., 0.0060, 0.0077, 0.0097]],\n\n         [[0.0354, 0.0348, 0.0332,  ..., 0.0047, 0.0061, 0.0078],\n          [0.0471, 0.0452, 0.0421,  ..., 0.0052, 0.0068, 0.0086],\n          [0.0610, 0.0571, 0.0515,  ..., 0.0054, 0.0072, 0.0094],\n          ...,\n          [0.0111, 0.0161, 0.0225,  ..., 0.0364, 0.0368, 0.0369],\n          [0.0078, 0.0115, 0.0165,  ..., 0.0303, 0.0342, 0.0382],\n          [0.0054, 0.0081, 0.0120,  ..., 0.0251, 0.0316, 0.0394]],\n\n         [[0.0090, 0.0070, 0.0053,  ..., 0.0049, 0.0055, 0.0061],\n          [0.0097, 0.0084, 0.0070,  ..., 0.0065, 0.0074, 0.0082],\n          [0.0102, 0.0098, 0.0090,  ..., 0.0084, 0.0096, 0.0106],\n          ...,\n          [0.0245, 0.0227, 0.0202,  ..., 0.0142, 0.0136, 0.0129],\n          [0.0211, 0.0191, 0.0169,  ..., 0.0131, 0.0131, 0.0128],\n          [0.0180, 0.0160, 0.0140,  ..., 0.0120, 0.0124, 0.0127]],\n\n         ...,\n\n         [[0.0259, 0.0252, 0.0239,  ..., 0.0033, 0.0048, 0.0068],\n          [0.0253, 0.0244, 0.0228,  ..., 0.0043, 0.0060, 0.0082],\n          [0.0242, 0.0230, 0.0211,  ..., 0.0052, 0.0072, 0.0097],\n          ...,\n          [0.0636, 0.0476, 0.0343,  ..., 0.0413, 0.0394, 0.0371],\n          [0.0640, 0.0479, 0.0348,  ..., 0.0459, 0.0416, 0.0373],\n          [0.0639, 0.0479, 0.0352,  ..., 0.0508, 0.0437, 0.0372]],\n\n         [[0.7196, 0.7019, 0.6668,  ..., 0.6152, 0.5556, 0.4922],\n          [0.6529, 0.6269, 0.5832,  ..., 0.5199, 0.4687, 0.4144],\n          [0.5768, 0.5427, 0.4922,  ..., 0.4186, 0.3824, 0.3403],\n          ...,\n          [0.3214, 0.2712, 0.2201,  ..., 0.3356, 0.3038, 0.2719],\n          [0.3338, 0.2885, 0.2423,  ..., 0.3139, 0.2817, 0.2503],\n          [0.3442, 0.3053, 0.2652,  ..., 0.2915, 0.2599, 0.2290]],\n\n         [[0.0182, 0.0217, 0.0253,  ..., 0.0382, 0.0371, 0.0354],\n          [0.0259, 0.0309, 0.0358,  ..., 0.0667, 0.0543, 0.0434],\n          [0.0359, 0.0427, 0.0489,  ..., 0.1109, 0.0769, 0.0520],\n          ...,\n          [0.0222, 0.0215, 0.0199,  ..., 0.1235, 0.1231, 0.1213],\n          [0.0250, 0.0241, 0.0225,  ..., 0.1325, 0.1389, 0.1441],\n          [0.0278, 0.0268, 0.0253,  ..., 0.1412, 0.1559, 0.1701]]]],\n       device='cuda:0')\n<\/code>","y":"I figured out the solution. This worked for me though I had to display masks in different figures.\n<code class=\"lang-auto\">    visualize(\n        image=denormalize(image_vis.squeeze()),\n        gt_mask_car=gt_mask[0].squeeze(),\n        pr_mask_car=pr_mask[0].squeeze(),\n        gt_mask_pedestrian=gt_mask[1].squeeze(),\n        pr_mask_pedestrain=pr_mask[1].squeeze()\n    )\n<\/code>","z":"Could you post the shapes of the masks you are passing to visualize?\n<code class=\"lang-auto\"># check shapes \nimages, masks = next(iter(train_loader))\nprint(images.shape)\nprint(masks.shape)\n\n# %%\nprint(len(train_loader))\n\n# %%\nimages, masks = next(iter(test_loader))\nprint(images.shape)\nprint(masks.shape)\n\n# %%\nprint((len(test_loader)))\n<\/code>\n<code class=\"lang-auto\">#[batch_size, no_of_classes, size, size]\n\ntorch.Size([8, 3, 320, 320])\ntorch.Size([8, 12, 320, 320])\n46\ntorch.Size([8, 3, 384, 480])\ntorch.Size([8, 12, 384, 480])\n13\n<\/code>\nI figured out the solution. This worked for me though I had to display masks in different figures.\n<code class=\"lang-auto\">    visualize(\n        image=denormalize(image_vis.squeeze()),\n        gt_mask_car=gt_mask[0].squeeze(),\n        pr_mask_car=pr_mask[0].squeeze(),\n        gt_mask_pedestrian=gt_mask[1].squeeze(),\n        pr_mask_pedestrain=pr_mask[1].squeeze()\n    )\n<\/code>"},{"x":"Creating my first pytorch image classification project, stuck at this error, not really sure what I am doing wrong. Please help!!!..\nclass GarbageNet(nn.Module):\ndef init(self):\nsuper(GarbageNet,self).init()\nself.conv1 = nn.Conv2d(in_channels=3, out_channels=12, kernel_size=3,stride=1, padding=1)\nself.relu1 = nn.ReLU()\nself.maxpool1 = nn.MaxPool2d(kernel_size=2)\nself.conv2 = nn.Conv2d(in_channels=12, out_channels=24, kernel_size=3, stride=1, padding=1)\nself.relu2 = nn.ReLU()\nself.lf = nn.Linear(in_features=32 * 32 * 24, out_features=6)\n    #Defining a forward pass\n    \ndef forward(self, x):\n    output = self.conv1(input)\n    output = self.relu1(output)\n    output = self.maxpool1(output)\n    output = self.conv2(output)\n    output = self.relu2(output)\n    return x\n\ndef trainNet(net, n_epochs, learning_rate):\n#Backpropagation - Using SGD\noptimizer = optim.SGD(net.parameters(), lr = 0.001, momentum = 0.9)\n#Defining an optimizer - SGD\nloss = nn.CrossEntropyLoss()\nprint(net)\nfor epoch in range(n_epochs):\nrunning_loss = 0.0\nstart_time = time.time()\ntotal_train_loss = 0\n    for i,data in enumerate(train_loader):\n        images, labels = data\n        print((images.shape))\n        print((labels.shape))\n        optimizer.zero_grad()\n\n        #Forward pass, backward pass\n        outputs = net(images)\n        loss_size = loss(outputs, labels)\n        loss_size.backward()\n        optimizer.step()\n\n<h1>=======Hyperparameters=========\nepochs= 5\nlearning_rate= 0.001<\/h1>\nGarbageNet(\n(conv1): Conv2d(3, 12, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n(relu1): ReLU()\n(maxpool1): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n(conv2): Conv2d(12, 24, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n(relu2): ReLU()\n(lf): Linear(in_features=24576, out_features=6, bias=True)\n)\ntorch.Size([4, 3, 224, 224])\ntorch.Size([4])\n\nTypeError                                 Traceback (most recent call last)\n in \n----> 1 trainNet(cnn_model, n_epochs=5, learning_rate=0.001)\n in trainNet(net, n_epochs, learning_rate)\n43\n44             #Forward pass, backward pass\n\u2014> 45             outputs = net(images)\n46             loss_size = loss(outputs, labels)\n47             loss_size.backward()\n~\\AppData\\Local\\conda\\conda\\envs\\asdf\\lib\\site-packages\\torch\\nn\\modules\\module.py in call(self, *input, **kwargs)\n530             result = self._slow_forward(*input, **kwargs)\n531         else:\n\u2013> 532             result = self.forward(*input, **kwargs)\n533         for hook in self._forward_hooks.values():\n534             hook_result = hook(self, input, result)\n in forward(self, x)\n14\n15     def forward(self, x):\n\u2014> 16         output = self.conv1(input)\n17         output = self.relu1(output)\n18         output = self.maxpool1(output)\n~\\AppData\\Local\\conda\\conda\\envs\\asdf\\lib\\site-packages\\torch\\nn\\modules\\module.py in call(self, *input, **kwargs)\n530             result = self._slow_forward(*input, **kwargs)\n531         else:\n\u2013> 532             result = self.forward(*input, **kwargs)\n533         for hook in self._forward_hooks.values():\n534             hook_result = hook(self, input, result)\n~\\AppData\\Local\\conda\\conda\\envs\\asdf\\lib\\site-packages\\torch\\nn\\modules\\conv.py in forward(self, input)\n343\n344     def forward(self, input):\n\u2013> 345         return self.conv2d_forward(input, self.weight)\n346\n347 class Conv3d(_ConvNd):\n~\\AppData\\Local\\conda\\conda\\envs\\asdf\\lib\\site-packages\\torch\\nn\\modules\\conv.py in conv2d_forward(self, input, weight)\n339                             weight, self.bias, self.stride,\n340                             _pair(0), self.dilation, self.groups)\n\u2013> 341         return F.conv2d(input, weight, self.bias, self.stride,\n342                         self.padding, self.dilation, self.groups)\n343\nTypeError: conv2d(): argument \u2018input\u2019 (position 1) must be Tensor, not method","y":"Your forward method accepts an x input, while you are using input as the first argument to your conv layer.\nChange it to self.conv1(x) and it should work.","z":"Your forward method accepts an x input, while you are using input as the first argument to your conv layer.\nChange it to self.conv1(x) and it should work.\nCareless mistake, Thanks  for the solution. It works!!!"},{"x":"The indices return by torch.topk is strange.\nIn my option, if sorted=False, then the returned indices should be sorted, that is the elements in the indices are ascending.\nHowever, in the following example, t2 has something wrong:\n<code class=\"lang-auto\">2144,  20,  104,  118,  123,  136,  137,  144,  171\n<\/code>\n<code class=\"lang-auto\">>>> import torch\n>>> t0 = torch.load('.\/t0.pt')\n>>> t0.shape\ntorch.Size([2146])\n>>> t0\ntensor([4.4721, 4.5826, 4.3589,  ..., 4.5826, 3.7417, 4.0000], device='cuda:0')\n>>> t1, t2 = torch.topk(t0, 200, dim=0, largest=False, sorted=False)\n>>> t1\ntensor([3.1623, 3.7417, 3.6056, 3.7417, 3.6056, 3.7417, 3.7417, 3.6056, 3.7417,\n        3.7417, 3.6056, 3.6056, 3.3166, 3.7417, 3.6056, 3.7417, 3.7417, 3.6056,\n        3.7417, 3.7417, 3.7417, 3.7417, 3.7417, 3.7417, 3.7417, 3.4641, 3.7417,\n        3.7417, 3.7417, 3.6056, 3.6056, 3.6056, 3.7417, 3.4641, 3.7417, 3.6056,\n        3.7417, 3.7417, 3.6056, 3.1623, 3.7417, 3.4641, 3.7417, 3.7417, 3.7417,\n        3.7417, 3.7417, 3.7417, 3.6056, 3.7417, 3.7417, 3.7417, 3.7417, 3.4641,\n        3.4641, 3.7417, 3.1623, 3.4641, 3.3166, 3.4641, 3.7417, 3.3166, 3.4641,\n        3.6056, 3.3166, 3.3166, 3.3166, 3.7417, 3.3166, 3.7417, 3.3166, 3.7417,\n        3.4641, 3.7417, 3.3166, 3.7417, 3.7417, 3.6056, 3.7417, 3.7417, 3.4641,\n        3.7417, 3.3166, 3.7417, 3.4641, 3.7417, 3.7417, 3.7417, 3.7417, 3.7417,\n        3.4641, 3.7417, 3.1623, 3.4641, 3.7417, 3.3166, 3.7417, 3.7417, 3.3166,\n        3.4641, 3.4641, 3.3166, 3.4641, 3.7417, 3.7417, 3.4641, 3.7417, 3.7417,\n        3.3166, 3.3166, 3.7417, 3.3166, 3.3166, 3.7417, 3.7417, 3.4641, 3.6056,\n        3.7417, 3.7417, 3.4641, 3.7417, 3.7417, 3.4641, 3.3166, 3.7417, 3.7417,\n        3.7417, 3.4641, 3.7417, 3.3166, 3.7417, 3.3166, 3.4641, 3.7417, 3.3166,\n        3.3166, 3.4641, 3.3166, 3.3166, 3.7417, 3.7417, 3.1623, 3.3166, 3.4641,\n        3.7417, 3.7417, 3.3166, 3.7417, 3.4641, 3.3166, 3.4641, 3.6056, 3.7417,\n        3.7417, 3.6056, 3.7417, 3.0000, 3.6056, 3.7417, 3.4641, 3.7417, 3.3166,\n        3.7417, 3.7417, 3.3166, 3.3166, 3.3166, 3.3166, 2.8284, 3.4641, 3.3166,\n        3.7417, 3.7417, 3.6056, 3.6056, 3.6056, 3.7417, 3.7417, 3.6056, 3.3166,\n        3.7417, 3.7417, 3.7417, 3.4641, 3.4641, 2.8284, 3.4641, 3.4641, 3.7417,\n        3.1623, 3.7417, 3.7417, 3.8730, 3.8730, 3.8730, 3.8730, 3.8730, 3.8730,\n        3.8730, 3.8730], device='cuda:0')\n>>> t2\ntensor([   6,   18,   22,   30,  119,  127,  148,  202,  205,  211,  218,  221,\n         241,  249,  259,  281,  282,  288,  296,  312,  339,  366,  439,  503,\n         523,  526,  533,  534,  541,  665,  676,  677,  716,  720,  724,  734,\n         787,  797,  798,  823,  850,  879,  887,  913,  931,  933,  954,  955,\n         969,  978,  997, 1009, 1016, 1038, 1040, 1044, 1046, 1047, 1082, 1083,\n        1095, 1131, 1143, 1145, 1163, 1166, 1241, 1254, 1258, 1262, 1272, 1291,\n        1295, 1299, 1305, 1306, 1309, 1310, 1313, 1320, 1328, 1331, 1334, 1339,\n        1343, 1347, 1357, 1365, 1390, 1400, 1402, 1416, 1417, 1419, 1439, 1456,\n        1466, 1474, 1501, 1519, 1530, 1535, 1539, 1608, 1619, 1622, 1626, 1630,\n        1636, 1651, 1657, 1665, 1667, 1668, 1671, 1672, 1685, 1686, 1694, 1696,\n        1701, 1707, 1713, 1716, 1717, 1732, 1761, 1781, 1785, 1788, 1810, 1821,\n        1822, 1832, 1863, 1868, 1880, 1887, 1899, 1907, 1917, 1971, 1973, 1985,\n        1996, 1997, 1998, 2000, 2005, 2009, 2016, 2017, 2020, 2021, 2027, 2029,\n        2034, 2041, 2042, 2043, 2051, 2056, 2059, 2060, 2062, 2064, 2067, 2068,\n        2070, 2072, 2073, 2074, 2075, 2076, 2078, 2080, 2082, 2084, 2087, 2092,\n        2099, 2107, 2113, 2119, 2120, 2124, 2130, 2131, 2134, 2138, 2140, 2144,\n          20,  104,  118,  123,  136,  137,  144,  171], device='cuda:0')\n>>> \n<\/code>","y":"If you don\u2019t specify sorted=True, there is no guarantee of any order (sorted or original order).\n\n\n\n ShengweiAn:\n\nSuch as the non-determinism incurred by the parallelism.\nBut rerunning topk for servel times gave same results.\n\n\nYes, that\u2019s most likely the reason at least for GPU runs. I\u2019m not sure, how this method is implemented on the CPU.","z":"I could not upload the t0.py file, so I print it out:\n<code class=\"lang-auto\">>>> print(t0)\ntensor([4.4721, 4.5826, 4.3589, 4.7958, 4.5826, 4.2426, 3.1623, 4.8990, 4.5826,\n        4.8990, 4.7958, 4.8990, 4.5826, 4.8990, 4.7958, 4.5826, 4.4721, 4.8990,\n        3.7417, 4.7958, 3.8730, 4.6904, 3.6056, 4.8990, 4.7958, 4.6904, 4.2426,\n        4.3589, 4.8990, 4.5826, 3.7417, 4.7958, 4.5826, 4.7958, 4.7958, 4.8990,\n        4.5826, 4.5826, 4.5826, 4.8990, 4.2426, 4.5826, 4.4721, 4.1231, 4.8990,\n        4.5826, 4.8990, 4.7958, 4.5826, 4.8990, 4.1231, 4.7958, 4.4721, 4.8990,\n        4.5826, 4.8990, 4.8990, 4.5826, 4.5826, 4.8990, 4.0000, 4.1231, 4.8990,\n        4.5826, 4.8990, 4.3589, 4.2426, 4.8990, 4.2426, 4.8990, 4.2426, 4.8990,\n        4.2426, 4.1231, 4.5826, 4.2426, 4.1231, 4.5826, 4.5826, 4.5826, 4.5826,\n        4.4721, 4.5826, 4.7958, 4.5826, 4.1231, 4.4721, 4.8990, 4.2426, 4.5826,\n        4.5826, 4.1231, 4.3589, 4.5826, 4.4721, 4.1231, 4.4721, 4.4721, 4.5826,\n        4.8990, 4.7958, 4.5826, 4.7958, 4.4721, 3.8730, 4.4721, 4.4721, 4.8990,\n        5.2915, 4.8990, 4.5826, 4.7958, 4.4721, 4.4721, 4.8990, 4.7958, 4.5826,\n        4.8990, 3.8730, 3.6056, 4.7958, 4.5826, 4.5826, 3.8730, 4.0000, 4.6904,\n        4.0000, 3.7417, 4.4721, 4.1231, 4.8990, 4.1231, 4.4721, 4.4721, 4.7958,\n        4.5826, 3.8730, 3.8730, 4.2426, 4.1231, 4.2426, 4.2426, 4.8990, 4.2426,\n        3.8730, 4.1231, 4.4721, 4.4721, 3.7417, 4.3589, 4.3589, 4.1231, 4.1231,\n        4.0000, 4.4721, 4.4721, 4.1231, 4.4721, 4.5826, 4.5826, 4.5826, 4.3589,\n        4.4721, 4.3589, 4.2426, 4.5826, 4.1231, 4.2426, 4.7958, 4.2426, 4.7958,\n        3.8730, 4.7958, 4.8990, 4.2426, 4.4721, 4.0000, 4.8990, 4.6904, 4.6904,\n        4.8990, 4.1231, 4.5826, 4.8990, 4.5826, 4.4721, 4.8990, 4.1231, 4.8990,\n        4.8990, 4.2426, 4.4721, 4.4721, 4.3589, 4.5826, 5.0990, 4.5826, 4.7958,\n        4.2426, 4.2426, 4.1231, 4.1231, 3.6056, 4.5826, 4.1231, 3.7417, 4.4721,\n        4.8990, 4.4721, 4.8990, 4.8990, 3.7417, 4.1231, 4.4721, 4.5826, 4.4721,\n        4.0000, 4.7958, 3.6056, 4.4721, 4.8990, 3.6056, 4.7958, 4.4721, 3.8730,\n        4.8990, 4.7958, 4.2426, 4.4721, 4.8990, 4.1231, 4.7958, 4.8990, 4.8990,\n        4.2426, 4.8990, 4.2426, 4.5826, 4.2426, 4.8990, 4.7958, 3.3166, 4.7958,\n        4.8990, 4.1231, 3.8730, 4.2426, 4.2426, 4.4721, 3.7417, 4.8990, 4.8990,\n        4.2426, 4.4721, 4.8990, 4.8990, 4.4721, 4.5826, 4.8990, 3.6056, 4.5826,\n        4.7958, 4.4721, 4.6904, 4.5826, 4.2426, 4.5826, 4.8990, 4.8990, 4.8990,\n        4.2426, 4.5826, 4.8990, 4.5826, 4.2426, 4.4721, 4.7958, 4.7958, 4.8990,\n        4.1231, 4.2426, 3.7417, 3.7417, 4.3589, 4.8990, 4.7958, 4.2426, 4.6904,\n        3.6056, 4.0000, 4.4721, 4.4721, 4.4721, 4.4721, 4.8990, 4.8990, 3.7417,\n        4.1231, 4.5826, 4.5826, 4.7958, 4.1231, 4.4721, 4.5826, 4.4721, 4.2426,\n        4.4721, 4.0000, 4.5826, 4.5826, 4.8990, 4.4721, 3.7417, 4.5826, 4.0000,\n        4.0000, 4.5826, 4.8990, 4.2426, 4.2426, 4.4721, 4.4721, 4.5826, 4.4721,\n        4.7958, 4.8990, 4.8990, 4.5826, 3.8730, 4.5826, 4.1231, 3.8730, 4.5826,\n        4.5826, 4.1231, 4.8990, 4.1231, 4.5826, 3.8730, 3.7417, 4.8990, 4.2426,\n        4.5826, 4.4721, 4.5826, 4.8990, 4.5826, 4.7958, 4.5826, 4.4721, 4.1231,\n        4.8990, 5.0000, 4.2426, 4.8990, 4.7958, 4.8990, 4.5826, 4.5826, 4.2426,\n        4.8990, 4.6904, 4.7958, 4.4721, 4.4721, 4.4721, 3.7417, 4.8990, 4.5826,\n        4.5826, 3.8730, 4.1231, 4.8990, 4.8990, 4.1231, 4.8990, 4.1231, 4.5826,\n        4.7958, 4.4721, 4.8990, 4.8990, 4.8990, 4.8990, 4.2426, 4.5826, 4.8990,\n        4.2426, 4.8990, 4.5826, 4.1231, 4.7958, 4.2426, 4.2426, 4.5826, 4.7958,\n        4.8990, 4.2426, 4.8990, 4.5826, 4.7958, 4.1231, 4.8990, 4.1231, 4.8990,\n        4.7958, 4.4721, 4.7958, 4.1231, 4.1231, 4.1231, 4.7958, 4.5826, 4.5826,\n        4.5826, 4.7958, 4.2426, 4.4721, 4.8990, 4.7958, 4.7958, 4.4721, 4.5826,\n        4.3589, 3.8730, 4.8990, 4.7958, 4.1231, 4.7958, 4.1231, 4.5826, 4.1231,\n        4.4721, 4.4721, 4.2426, 4.2426, 4.5826, 4.8990, 4.4721, 3.7417, 4.1231,\n        4.8990, 4.5826, 4.2426, 4.5826, 4.8990, 4.5826, 4.2426, 4.8990, 4.8990,\n        4.5826, 4.4721, 4.8990, 4.8990, 3.8730, 4.5826, 4.8990, 4.8990, 4.2426,\n        4.2426, 4.5826, 4.2426, 4.3589, 4.7958, 4.4721, 4.3589, 4.8990, 4.7958,\n        4.5826, 4.4721, 4.1231, 4.1231, 4.5826, 4.8990, 4.5826, 4.1231, 3.8730,\n        4.5826, 4.5826, 4.8990, 4.1231, 4.1231, 4.4721, 4.8990, 4.5826, 4.5826,\n        4.5826, 4.4721, 4.2426, 4.4721, 4.7958, 4.8990, 4.1231, 4.4721, 3.8730,\n        4.5826, 4.7958, 4.2426, 3.8730, 4.4721, 4.4721, 4.2426, 4.1231, 3.7417,\n        4.8990, 4.7958, 4.4721, 4.2426, 4.5826, 4.4721, 4.5826, 4.4721, 4.8990,\n        4.1231, 4.7958, 4.2426, 4.4721, 4.8990, 4.5826, 4.2426, 4.5826, 4.4721,\n        4.4721, 3.7417, 4.1231, 4.4721, 3.4641, 4.4721, 4.8990, 4.7958, 4.0000,\n        4.8990, 4.1231, 3.7417, 3.7417, 3.8730, 4.4721, 4.8990, 4.8990, 4.5826,\n        4.5826, 3.7417, 4.5826, 4.1231, 4.0000, 4.2426, 4.5826, 4.8990, 3.8730,\n        4.1231, 4.1231, 4.2426, 4.4721, 4.3589, 4.7958, 4.1231, 4.1231, 4.2426,\n        4.4721, 4.2426, 4.1231, 4.0000, 4.1231, 4.5826, 4.1231, 4.1231, 4.5826,\n        4.2426, 4.8990, 4.5826, 4.2426, 4.1231, 4.3589, 4.1231, 4.4721, 4.4721,\n        4.0000, 4.7958, 3.8730, 4.7958, 3.8730, 4.1231, 4.5826, 4.3589, 4.5826,\n        4.3589, 4.5826, 4.8990, 4.5826, 4.2426, 4.5826, 4.5826, 4.2426, 4.0000,\n        4.7958, 4.3589, 4.5826, 4.7958, 4.2426, 4.3589, 4.8990, 4.7958, 4.7958,\n        4.8990, 4.1231, 4.4721, 4.8990, 4.8990, 4.2426, 4.8990, 4.5826, 4.7958,\n        4.2426, 4.5826, 4.7958, 4.2426, 4.7958, 4.5826, 4.5826, 4.4721, 4.4721,\n        4.1231, 4.2426, 4.3589, 4.5826, 4.4721, 4.6904, 4.6904, 4.3589, 4.5826,\n        4.4721, 4.8990, 4.8990, 4.4721, 4.5826, 4.1231, 4.5826, 4.8990, 4.4721,\n        4.5826, 4.3589, 4.3589, 4.3589, 3.8730, 4.1231, 4.6904, 4.7958, 4.2426,\n        4.4721, 4.5826, 4.8990, 4.7958, 4.3589, 4.5826, 4.8990, 4.1231, 4.3589,\n        4.7958, 4.2426, 4.3589, 4.4721, 4.4721, 4.7958, 4.4721, 3.8730, 3.6056,\n        4.5826, 4.8990, 4.4721, 4.7958, 4.7958, 4.5826, 4.2426, 4.7958, 4.5826,\n        4.7958, 3.6056, 3.6056, 4.1231, 4.5826, 4.4721, 4.5826, 4.7958, 4.1231,\n        4.6904, 4.3589, 4.8990, 4.3589, 4.2426, 4.7958, 4.0000, 4.7958, 4.3589,\n        4.4721, 4.7958, 4.7958, 3.8730, 4.8990, 4.7958, 4.5826, 3.8730, 4.5826,\n        4.7958, 4.7958, 4.8990, 4.1231, 3.8730, 4.5826, 4.1231, 4.2426, 4.1231,\n        4.4721, 4.5826, 4.5826, 4.5826, 4.5826, 3.7417, 3.8730, 4.8990, 4.2426,\n        3.4641, 4.8990, 4.8990, 4.5826, 3.7417, 4.8990, 4.5826, 4.7958, 4.8990,\n        4.0000, 4.2426, 4.2426, 4.8990, 4.6904, 3.6056, 4.3589, 4.7958, 4.2426,\n        4.1231, 4.3589, 4.5826, 4.5826, 4.7958, 4.4721, 4.8990, 4.8990, 4.5826,\n        4.5826, 4.8990, 4.7958, 4.5826, 4.5826, 4.4721, 4.5826, 4.7958, 4.8990,\n        4.8990, 4.8990, 4.8990, 4.5826, 3.8730, 4.7958, 4.4721, 4.8990, 4.8990,\n        4.7958, 4.3589, 4.8990, 4.4721, 4.8990, 4.4721, 4.3589, 4.4721, 4.4721,\n        4.8990, 4.7958, 4.8990, 4.4721, 4.7958, 4.5826, 4.4721, 4.7958, 4.5826,\n        4.4721, 4.5826, 4.1231, 4.7958, 3.7417, 4.2426, 4.1231, 4.2426, 4.2426,\n        3.8730, 4.2426, 4.1231, 4.2426, 4.2426, 3.7417, 3.6056, 4.4721, 4.2426,\n        4.2426, 4.5826, 4.2426, 4.1231, 4.1231, 3.8730, 4.5826, 4.0000, 4.5826,\n        4.7958, 4.2426, 4.5826, 4.8990, 4.6904, 4.8990, 4.4721, 4.8990, 4.5826,\n        4.1231, 4.4721, 4.2426, 4.8990, 3.1623, 4.5826, 4.8990, 4.4721, 4.5826,\n        4.4721, 4.4721, 4.0000, 4.8990, 4.7958, 4.7958, 4.8990, 4.7958, 4.4721,\n        4.2426, 4.2426, 4.2426, 4.4721, 4.1231, 4.5826, 4.2426, 4.4721, 4.4721,\n        4.2426, 4.7958, 4.1231, 4.7958, 3.7417, 4.2426, 4.8990, 4.7958, 4.8990,\n        4.5826, 4.4721, 4.5826, 4.1231, 4.7958, 4.4721, 4.7958, 4.2426, 3.8730,\n        4.8990, 4.0000, 4.1231, 4.2426, 4.1231, 4.1231, 4.2426, 4.5826, 4.1231,\n        4.5826, 4.1231, 3.8730, 4.2426, 4.4721, 4.0000, 3.4641, 4.5826, 4.1231,\n        4.5826, 3.8730, 4.8990, 4.3589, 4.8990, 3.7417, 4.5826, 4.8990, 4.4721,\n        4.8990, 4.1231, 4.7958, 4.5826, 4.0000, 4.8990, 4.4721, 4.5826, 4.5826,\n        4.1231, 4.4721, 4.5826, 4.7958, 4.1231, 4.7958, 4.0000, 4.8990, 4.4721,\n        4.5826, 4.4721, 4.2426, 4.5826, 3.7417, 5.0000, 4.7958, 4.8990, 4.4721,\n        4.5826, 4.1231, 4.4721, 4.8990, 4.5826, 4.8990, 4.1231, 4.2426, 4.8990,\n        4.8990, 4.0000, 4.5826, 4.2426, 3.7417, 4.2426, 3.7417, 4.1231, 4.2426,\n        4.2426, 4.4721, 4.1231, 4.8990, 4.7958, 4.0000, 4.2426, 4.1231, 4.5826,\n        4.5826, 4.8990, 4.4721, 4.5826, 4.8990, 4.5826, 4.3589, 4.7958, 4.4721,\n        3.7417, 3.7417, 4.4721, 4.8990, 4.1231, 4.4721, 4.8990, 4.2426, 4.5826,\n        4.8990, 4.4721, 4.4721, 4.5826, 4.5826, 4.4721, 3.6056, 4.3589, 4.3589,\n        4.5826, 4.2426, 4.1231, 4.5826, 4.3589, 4.4721, 3.7417, 4.5826, 4.7958,\n        3.8730, 4.8990, 4.8990, 4.7958, 4.7958, 4.2426, 4.8990, 4.4721, 4.8990,\n        4.4721, 4.3589, 4.5826, 4.5826, 4.7958, 4.5826, 4.1231, 3.7417, 4.4721,\n        4.1231, 4.5826, 4.8990, 4.5826, 4.1231, 3.8730, 4.5826, 4.0000, 4.1231,\n        4.2426, 3.7417, 5.1962, 4.1231, 4.2426, 4.7958, 4.6904, 4.7958, 3.7417,\n        4.1231, 5.0000, 3.8730, 4.2426, 4.5826, 4.8990, 3.8730, 4.7958, 3.8730,\n        3.8730, 3.8730, 4.0000, 4.4721, 4.4721, 4.1231, 4.2426, 4.7958, 4.7958,\n        4.5826, 4.5826, 3.8730, 3.4641, 4.5826, 3.4641, 4.5826, 4.3589, 4.8990,\n        3.7417, 4.2426, 3.1623, 3.4641, 4.1231, 4.8990, 5.0000, 4.4721, 4.2426,\n        4.4721, 4.2426, 4.4721, 4.4721, 4.8990, 4.8990, 4.8990, 4.5826, 4.2426,\n        4.7958, 4.5826, 4.4721, 3.8730, 3.8730, 4.1231, 4.7958, 4.7958, 4.4721,\n        3.8730, 4.0000, 4.1231, 4.5826, 4.1231, 4.2426, 4.7958, 4.8990, 4.1231,\n        4.2426, 4.4721, 3.3166, 3.4641, 4.5826, 4.2426, 4.5826, 4.8990, 4.2426,\n        4.4721, 4.5826, 4.7958, 4.8990, 4.4721, 4.6904, 3.7417, 4.7958, 4.8990,\n        4.2426, 3.8730, 4.8990, 4.8990, 4.5826, 4.7958, 4.1231, 4.4721, 4.4721,\n        4.7958, 4.1231, 4.2426, 4.2426, 4.2426, 4.4721, 4.1231, 4.4721, 3.8730,\n        4.2426, 4.8990, 4.4721, 5.1962, 4.2426, 4.2426, 4.2426, 4.4721, 4.5826,\n        4.5826, 4.0000, 4.2426, 4.1231, 4.2426, 4.8990, 3.3166, 4.4721, 4.7958,\n        4.8990, 4.5826, 4.0000, 4.8990, 4.2426, 4.2426, 4.1231, 4.7958, 4.3589,\n        3.4641, 4.5826, 3.6056, 4.0000, 4.2426, 4.5826, 4.8990, 4.8990, 4.2426,\n        3.8730, 4.1231, 4.5826, 4.7958, 4.5826, 3.8730, 3.8730, 4.1231, 4.1231,\n        4.1231, 5.1962, 3.3166, 4.2426, 4.5826, 3.3166, 4.2426, 4.2426, 3.8730,\n        4.5826, 4.5826, 4.5826, 4.2426, 4.7958, 4.5826, 5.1962, 4.2426, 4.4721,\n        4.8990, 4.8990, 4.5826, 4.2426, 4.1231, 4.4721, 4.5826, 4.1231, 4.5826,\n        4.1231, 4.4721, 4.7958, 4.4721, 4.8990, 4.8990, 4.7958, 4.0000, 4.1231,\n        4.7958, 4.5826, 4.5826, 4.2426, 4.6904, 4.1231, 4.3589, 4.5826, 4.5826,\n        4.4721, 4.5826, 3.8730, 4.2426, 4.5826, 4.1231, 3.8730, 4.2426, 4.6904,\n        4.7958, 3.8730, 5.2915, 4.7958, 4.4721, 4.7958, 4.4721, 4.5826, 4.8990,\n        4.5826, 5.0000, 4.1231, 4.2426, 4.7958, 4.4721, 4.2426, 4.1231, 4.8990,\n        3.8730, 4.1231, 4.8990, 4.4721, 4.3589, 4.5826, 4.3589, 4.3589, 3.3166,\n        4.2426, 4.8990, 4.5826, 4.1231, 4.1231, 4.0000, 4.1231, 4.2426, 3.8730,\n        3.8730, 4.7958, 4.1231, 3.7417, 4.5826, 4.1231, 4.4721, 3.3166, 4.5826,\n        4.0000, 4.7958, 3.7417, 5.1962, 4.4721, 4.8990, 4.5826, 4.5826, 4.4721,\n        4.2426, 3.8730, 4.1231, 3.3166, 5.0990, 4.2426, 4.6904, 4.4721, 4.4721,\n        4.2426, 4.5826, 4.2426, 4.5826, 3.8730, 4.8990, 3.8730, 4.5826, 4.2426,\n        4.8990, 4.5826, 4.1231, 4.6904, 3.7417, 4.2426, 4.7958, 4.8990, 3.4641,\n        4.4721, 4.4721, 4.2426, 3.7417, 4.4721, 4.1231, 4.8990, 4.8990, 4.4721,\n        3.3166, 3.7417, 4.5826, 4.8990, 3.7417, 3.6056, 4.3589, 4.2426, 3.7417,\n        4.1231, 4.4721, 4.2426, 4.7958, 4.4721, 4.4721, 3.7417, 3.8730, 4.1231,\n        4.5826, 4.1231, 4.8990, 4.2426, 4.4721, 3.4641, 3.8730, 4.3589, 3.7417,\n        4.1231, 3.8730, 3.3166, 4.4721, 4.4721, 4.2426, 4.7958, 3.7417, 4.7958,\n        3.8730, 4.4721, 3.4641, 4.2426, 4.1231, 4.5826, 3.7417, 4.4721, 4.4721,\n        4.3589, 4.8990, 4.4721, 4.2426, 4.1231, 4.3589, 4.2426, 3.7417, 4.8990,\n        4.1231, 4.5826, 3.8730, 4.1231, 4.1231, 4.1231, 3.7417, 4.1231, 4.5826,\n        4.4721, 4.4721, 4.4721, 3.8730, 4.1231, 4.8990, 4.5826, 4.5826, 4.4721,\n        3.8730, 4.5826, 4.1231, 4.0000, 4.1231, 3.8730, 4.1231, 5.0990, 4.4721,\n        4.1231, 4.7958, 4.6904, 4.7958, 3.7417, 4.1231, 4.1231, 5.0000, 3.8730,\n        4.2426, 4.5826, 4.8990, 4.6904, 3.8730, 3.7417, 4.0000, 3.4641, 4.2426,\n        4.2426, 4.8990, 4.7958, 4.5826, 4.5826, 3.8730, 3.8730, 4.8990, 3.8730,\n        4.5826, 4.3589, 4.8990, 3.7417, 3.1623, 3.8730, 3.4641, 4.1231, 4.8990,\n        5.0000, 4.4721, 4.2426, 4.4721, 4.2426, 4.1231, 4.5826, 5.0000, 4.8990,\n        4.8990, 4.5826, 4.5826, 4.7958, 4.5826, 4.4721, 4.2426, 4.5826, 3.7417,\n        4.7958, 4.6904, 3.8730, 4.0000, 4.1231, 4.1231, 4.7958, 4.5826, 4.1231,\n        4.2426, 4.4721, 4.6904, 4.8990, 4.1231, 4.5826, 4.4721, 3.3166, 4.2426,\n        4.5826, 4.8990, 4.8990, 4.2426, 4.4721, 4.5826, 4.8990, 4.6904, 3.7417,\n        4.7958, 4.8990, 4.2426, 3.8730, 4.8990, 4.5826, 4.7958, 3.7417, 4.1231,\n        4.4721, 4.7958, 4.1231, 4.2426, 4.2426, 4.5826, 4.1231, 4.2426, 4.1231,\n        4.2426, 4.2426, 4.5826, 4.4721, 5.1962, 4.2426, 4.2426, 4.2426, 4.2426,\n        4.4721, 4.5826, 4.1231, 4.2426, 4.7958, 4.1231, 4.2426, 3.3166, 4.4721,\n        4.4721, 4.7958, 4.8990, 5.0990, 4.5826, 4.1231, 4.8990, 4.8990, 4.5826,\n        4.2426, 4.8990, 4.5826, 4.4721, 4.4721, 3.8730, 4.5826, 3.4641, 4.7958,\n        4.1231, 4.2426, 4.5826, 4.8990, 4.8990, 4.2426, 4.1231, 4.5826, 4.5826,\n        3.4641, 3.8730, 4.1231, 4.1231, 5.1962, 3.3166, 4.1231, 4.2426, 4.5826,\n        3.4641, 4.5826, 3.8730, 3.8730, 4.5826, 4.4721, 4.5826, 4.2426, 4.7958,\n        4.5826, 5.1962, 4.1231, 4.4721, 4.8990, 4.5826, 4.2426, 4.1231, 4.1231,\n        4.4721, 4.4721, 4.8990, 4.1231, 4.3589, 4.2426, 4.7958, 4.4721, 4.7958,\n        4.1231, 4.4721, 4.7958, 4.4721, 4.5826, 4.5826, 4.4721, 4.0000, 4.2426,\n        4.3589, 4.5826, 4.2426, 4.4721, 4.5826, 3.8730, 3.8730, 4.5826, 4.8990,\n        4.1231, 3.8730, 4.5826, 4.6904, 3.8730, 5.2915, 4.7958, 4.1231, 4.7958,\n        4.4721, 4.2426, 4.8990, 5.0000, 4.1231, 4.2426, 4.4721, 3.8730, 4.8990,\n        4.0000, 4.4721, 4.4721, 4.6904, 4.3589, 4.3589, 3.7417, 4.2426, 4.2426,\n        4.1231, 4.0000, 4.1231, 4.2426, 3.8730, 3.8730, 4.7958, 4.1231, 3.7417,\n        4.1231, 4.1231, 3.4641, 4.5826, 4.3589, 4.2426, 3.7417, 5.1962, 4.4721,\n        4.5826, 3.7417, 4.2426, 4.4721, 4.2426, 4.2426, 4.1231, 3.3166, 5.1962,\n        4.2426, 4.6904, 4.4721, 4.2426, 4.5826, 4.2426, 4.5826, 3.8730, 4.8990,\n        3.8730, 4.2426, 4.8990, 4.2426, 3.3166, 4.6904, 4.1231, 4.1231, 4.7958,\n        5.1962, 3.7417, 4.1231, 4.0000, 4.4721, 3.8730, 4.2426, 4.1231, 4.1231,\n        3.3166, 4.8990, 3.3166, 3.7417, 4.5826, 4.8990, 3.7417, 3.4641, 4.2426,\n        4.1231, 4.4721, 4.1231, 4.1231, 4.3589, 4.2426, 4.4721, 4.7958, 4.2426,\n        4.4721, 4.4721, 3.6056, 3.7417, 3.8730, 4.1231, 4.5826, 4.1231, 4.8990,\n        4.2426, 4.4721, 3.7417, 4.4721, 3.4641, 4.2426, 4.5826, 3.8730, 4.4721,\n        3.7417, 4.1231, 4.4721, 4.4721, 4.2426, 4.7958, 3.7417, 4.4721, 3.8730,\n        4.4721, 4.1231, 4.4721, 3.4641, 4.8990, 4.6904, 3.3166, 3.7417, 4.7958,\n        4.4721, 4.8990, 4.2426, 4.5826, 4.5826, 4.3589, 4.2426, 4.1231, 4.0000,\n        4.2426, 4.8990, 4.1231, 4.5826, 3.7417, 4.0000, 4.1231, 4.2426, 4.1231,\n        4.1231, 4.5826, 4.4721, 4.4721, 4.4721, 4.1231, 4.8990, 4.5826, 4.8990,\n        4.0000, 4.5826, 4.4721, 4.5826, 4.1231, 4.0000, 4.1231, 4.2426, 4.1231,\n        5.0990, 4.1231, 4.2426, 4.7958, 4.6904, 4.3589, 3.7417, 4.1231, 5.0000,\n        3.8730, 4.2426, 4.5826, 4.8990, 4.7958, 3.8730, 3.8730, 3.8730, 4.2426,\n        4.1231, 4.2426, 5.0000, 4.7958, 4.5826, 3.8730, 3.8730, 4.8990, 3.4641,\n        4.5826, 4.3589, 4.8990, 3.7417, 4.4721, 3.8730, 3.3166, 4.0000, 4.2426,\n        4.1231, 4.8990, 5.0000, 4.4721, 4.2426, 4.1231, 4.2426, 4.1231, 4.4721,\n        5.0000, 4.8990, 4.8990, 4.5826, 4.5826, 4.7958, 4.4721, 4.4721, 3.8730,\n        4.5826, 3.7417, 4.7958, 4.6904, 4.2426, 4.0000, 4.5826, 4.1231, 4.2426,\n        4.7958, 4.8990, 4.1231, 3.3166, 3.4641, 4.5826, 4.2426, 4.5826, 4.8990,\n        4.2426, 4.4721, 4.2426, 4.8990, 4.6904, 3.7417, 4.4721, 4.4721, 4.8990,\n        4.2426, 3.8730, 4.8990, 4.7958, 4.5826, 4.7958, 4.1231, 4.4721, 4.4721,\n        4.6904, 4.1231, 4.2426, 4.2426, 4.5826, 4.1231, 4.4721, 4.2426, 4.2426,\n        4.8990, 4.4721, 5.1962, 4.2426, 3.8730, 4.2426, 4.4721, 4.5826, 4.5826,\n        3.3166, 4.0000, 4.2426, 4.1231, 4.2426, 3.3166, 4.7958, 4.6904, 5.0990,\n        4.5826, 4.7958, 4.8990, 4.5826, 4.2426, 4.1231, 4.4721, 4.3589, 3.4641,\n        4.8990, 4.1231, 4.2426, 4.5826, 4.8990, 4.8990, 3.3166, 4.8990, 4.2426,\n        4.2426, 4.5826, 4.5826, 4.2426, 3.8730, 4.1231, 4.1231, 4.6904, 5.1962,\n        3.3166, 4.2426, 4.5826, 3.8730, 4.2426, 3.8730, 3.8730, 4.5826, 3.7417,\n        4.5826, 4.5826, 4.2426, 4.5826, 5.1962, 4.1231, 4.4721, 4.8990, 4.5826,\n        3.7417, 4.1231, 4.4721, 4.5826, 4.5826, 4.1231, 4.3589, 4.7958, 4.4721,\n        4.8990, 4.7958, 4.4721, 4.4721, 4.7958, 4.7958, 4.5826, 4.5826, 4.2426,\n        4.8990, 4.1231, 4.3589, 4.5826, 4.5826, 4.4721, 4.5826, 3.8730, 4.2426,\n        4.5826, 4.2426, 4.1231, 3.8730, 4.5826, 4.6904, 5.2915, 4.7958, 4.1231,\n        4.7958, 4.4721, 4.5826, 4.8990, 5.0000, 4.1231, 3.8730, 4.4721, 4.8990,\n        4.2426, 4.1231, 4.8990, 4.1231, 4.5826, 4.4721, 4.4721, 3.8730, 4.3589,\n        3.1623, 4.3589, 3.3166, 4.5826, 4.1231, 4.1231, 4.1231, 4.2426, 3.8730,\n        3.8730, 4.7958, 4.1231, 4.1231, 4.5826, 3.4641, 4.5826, 4.0000, 3.8730,\n        4.7958, 4.4721, 4.1231, 5.1962, 4.4721, 4.5826, 4.5826, 3.7417, 3.7417,\n        3.3166, 4.0000, 3.7417, 4.2426, 3.8730, 4.2426, 3.8730, 3.4641, 4.2426,\n        4.4721, 3.8730, 3.3166, 4.2426, 4.0000, 4.5826, 4.4721, 4.8990, 4.2426,\n        3.4641, 3.6056, 4.0000, 4.5826, 3.7417, 3.7417, 4.1231, 3.8730, 4.5826,\n        4.1231, 4.4721, 3.6056, 4.8990, 3.7417, 4.2426, 4.1231, 4.2426, 4.7958,\n        3.0000, 4.2426, 4.1231, 4.1231, 4.0000, 3.8730, 4.4721, 3.6056, 3.7417,\n        3.4641, 3.8730, 4.2426, 4.5826, 4.3589, 4.4721, 4.2426, 4.4721, 3.7417,\n        4.1231, 4.2426, 3.8730, 4.2426, 3.3166, 4.1231, 4.0000, 3.7417, 3.7417,\n        4.4721, 3.3166, 4.2426, 3.3166, 4.5826, 4.1231, 3.3166, 3.3166, 4.5826,\n        2.8284, 4.4721, 3.4641, 3.3166, 3.7417, 3.7417, 3.6056, 4.4721, 3.6056,\n        4.1231, 3.6056, 4.5826, 3.7417, 3.8730, 3.7417, 4.1231, 4.5826, 3.6056,\n        4.1231, 4.0000, 4.4721, 4.1231, 3.3166, 4.0000, 4.2426, 4.1231, 4.0000,\n        4.7958, 4.8990, 3.7417, 4.2426, 4.2426, 4.4721, 4.2426, 4.2426, 3.8730,\n        3.8730, 3.7417, 4.1231, 4.4721, 4.5826, 4.2426, 4.8990, 3.7417, 4.1231,\n        4.8990, 4.4721, 3.8730, 4.5826, 3.4641, 3.4641, 4.2426, 4.0000, 3.8730,\n        2.8284, 3.8730, 4.0000, 4.2426, 4.1231, 4.2426, 3.4641, 3.4641, 4.1231,\n        4.2426, 3.7417, 4.2426, 4.2426, 4.5826, 3.1623, 4.4721, 3.7417, 4.1231,\n        4.0000, 4.5826, 3.7417, 4.0000], device='cuda:0')\n<\/code>\n\n\n\n ShengweiAn:\n\nIn my option, if sorted=False , then the returned indices should be sorted, that is the elements in the indices are ascending.\n\n\nI\u2019m not sure I can follow your assumption.\nWhy would you expect the output to be sorted, when you specify sorted=False?\nAre you seeing the expected result using sorted=True?\nThank you for your reply.\nI mean if sorted=False, then the returned elements should keep their original order in the tensor.\nTake the following codes as an example:\n<code class=\"lang-auto\">>>> import torch\n>>> a = torch.tensor([5., 4, 3, 2, 1, 0])\n>>> a\ntensor([5., 4., 3., 2., 1., 0.])\n>>> torch.topk(a, 3, largest=False, sorted=False)\ntorch.return_types.topk(\nvalues=tensor([0., 1., 2.]), \nindices=tensor([5, 4, 3]))  # since sorted=False, I think the indices should be [3, 4, 5]\n>>> torch.topk(a, 3, largest=False, sorted=True)\ntorch.return_types.topk(\nvalues=tensor([0., 1., 2.]),\nindices=tensor([5, 4, 3]))\n>>>\n<\/code>\nWhen I use sorted=True, the result is correct as expected.\nI think I didn\u2019t explain the problem clearly.\nWhen I use sorted=False, I would like to see the returned indices are sorted, not the returned value.\nI hope my previous reply is a bit more clearer.\nsorted defines, if the values should be returned in a sorted way or not.\nIt does not determine the order of the indices.\nIf you want to sort the indices, you could apply torch.sort on them after the topk call.\nThank you for your reply.\nI understood the definition of  sorted .\nWhat confuses me, is why the indices are not in the original order by default (when not sorting the values).\nAre there any design reasons? Such as the non-determinism incurred by the parallelism.\nBut rerunning  topk  for servel times gave same results.\nIf you don\u2019t specify sorted=True, there is no guarantee of any order (sorted or original order).\n\n\n\n ShengweiAn:\n\nSuch as the non-determinism incurred by the parallelism.\nBut rerunning topk for servel times gave same results.\n\n\nYes, that\u2019s most likely the reason at least for GPU runs. I\u2019m not sure, how this method is implemented on the CPU.\nI see. Thank you very much for your patience and help.\nHello Shengwei!\n\n\n\n ShengweiAn:\n\nWhat confuses me, is why the indices are not in the original order by default (when not sorting the values).\n\n\nI wouldn\u2019t consider this a bug, per se, because torch.topk()\ndoesn\u2019t make any promises about the order it returns (when\nsorted = False).\nI do think this could be a legitimate feature request.  There is\nvalue in having a known return order, and the original order\n(which corresponds to sorted indices) would seem to be the\nnatural choice.\nI don\u2019t know what the topk() code looks like, so I don\u2019t know\nhow hard it would be to do, but if it weren\u2019t too hard, and didn\u2019t\nsignificantly degrade the performance, I would think that having\ntopk() return the original (sorted-indices) order would be a\nworthwhile improvement.  (I also think that for sorted = True\nsets of indices for which the values are equal should have\nthe order of the indices within the set retained.)\n(On the other hand, there are undoubtedly more important\nbug fixes \/ feature requests out there in the queue.)\n(By analogy, one might compare merge sort and heapsort.\nMarge sort is a stable sort, that is, when values test equal, they\nare returned in their original order, while heapsort is not stable,\nbut can have performance advantages.  If there were no loss\nin performance, it would make sense to always use the stable\nsort.)\nBest.\nK. Frank"},{"x":"Hi,\nI am looking \u2018Bayesian optimization\u2019 for hyper-parameter tuning\nfor my multi-layered neural networks.\nWhat I would like to optimize are like, \u2018learning rate, batch size, number of layers and neurons\u2019\nCan you guide me any example or tutorial for bayesian optimization?\nAlso, Can you recommend any library for this?\nI\u2019ve been looking through \u2018hyper-opt\u2019 or \u2018bayesian-optimization\u2019 libraries and wondering if they works for pytorch.\nThanks in advance.","y":"From a quick browse of all above mentioned frameworks,\n\nBoTorch (Recommended) - Built on top of Pytorch with autograd features. All basic Bayesian optimization tools are included. This should be preferred if you are using Pytorch\nPros - Modular, Simple and Scalable.\nCons - Not extensive\nscikit-optimize - Integrated with scikit learn. Has extensive API and good example.\nPros - More extensive than BoTorch.\nCons - Not sure how easy it is to run in conjunction with Pytorch\nHyperopt - Provides parallelization capabilities with MongoDB and Apache Spark\nPros - Might be useful for if the database is huge and parallelization is required.\nCons- Might not be the right tool for prototyping.\nbayesian-optimization - Runs on top of scipy and scikitlearn.\nPros - good documentation, Clear examples, should be good for basic prototyping\nCons - Not sure how it would go along with pytorch.\n","z":"Check out Pyro(Provides api for probabilistic programming) -> https:\/\/pyro.ai\/\nHi, Thank you for your suggestion,\nDo you see any pros or cons with hyperopt, skopt, bayesian-optimization or botorch?\nFrom a quick browse of all above mentioned frameworks,\n\nBoTorch (Recommended) - Built on top of Pytorch with autograd features. All basic Bayesian optimization tools are included. This should be preferred if you are using Pytorch\nPros - Modular, Simple and Scalable.\nCons - Not extensive\nscikit-optimize - Integrated with scikit learn. Has extensive API and good example.\nPros - More extensive than BoTorch.\nCons - Not sure how easy it is to run in conjunction with Pytorch\nHyperopt - Provides parallelization capabilities with MongoDB and Apache Spark\nPros - Might be useful for if the database is huge and parallelization is required.\nCons- Might not be the right tool for prototyping.\nbayesian-optimization - Runs on top of scipy and scikitlearn.\nPros - good documentation, Clear examples, should be good for basic prototyping\nCons - Not sure how it would go along with pytorch.\n\nI really appreciate your help!\nThanks. The pleasure is mine."},{"x":"Hey I have a gray scale numpy ndarray of shape 224,224 [ Which I assume is in (H x W x C ) format. However I need to convert this into (C x H x W) format. When I printed the shape it showed only the H x W. Where is the channel, or is it not shown because the image is in gray scale?\n","y":"Use\n<code class=\"lang-auto\">numpy_array = ... # your numpy array image with shape (224,224)\ntensor = torch.from_numpy(numpy_array) #torch.Size([224, 224])\ntensor = tensor.unsqueeze(dim=0) # torch.Size([1, 224, 224])\n<\/code>","z":"Use\n<code class=\"lang-auto\">numpy_array = ... # your numpy array image with shape (224,224)\ntensor = torch.from_numpy(numpy_array) #torch.Size([224, 224])\ntensor = tensor.unsqueeze(dim=0) # torch.Size([1, 224, 224])\n<\/code>\nThank you so much  It worked.\nAlso you can use transforms.ToTensor() docs with link \"https:\/\/pytorch.org\/docs\/stable\/torchvision\/transforms.html#torchvision.transforms.ToTensor\", this  transforms a PIL or numpy image in range [0,255] to tensor in range [0,1]\n<code class=\"lang-auto\">from torchvision.transforms import ToTensor\nimport torch\nnumpy_array = ... # your numpy array image with shape (224,224)\ntensor = ToTensor()(numpy_array) #torch.Size([1, 224, 224])\n\n<\/code>\nHey is transforms.ToTensor() equivalent to numpy_array\/255 . Does this normalize the values in the range [0,1].\nP.S thanks for the docs link \nAccording to the docs:\n\nConverts a PIL Image or numpy.ndarray (H x W x C) in the range [0, 255] to a torch.FloatTensor of shape (C x H x W) in the range [0.0, 1.0] if the PIL Image belongs to one of the modes (L, LA, P, I, F, RGB, YCbCr, RGBA, CMYK, 1) or if the numpy.ndarray has dtype = np.uint8\n\nYou could see the difference doing:\n<code class=\"lang-auto\">numpy_array = np.ones((224,224))\nnumpy_array_uint8 = np.ones((224,224,1),dtype=np.uint8)\ntensor = ToTensor()(numpy_array)\ntensor_uint8 = ToTensor()(numpy_array_uint8)\nprint(tensor)\nprint(tensor_uint8)\n<\/code>\nSo only is equivalent if the numpy array has dtype = np.uint8"},{"x":"I\u2019m making a simple autoencoder on Mnist Digits\nSo the problem i\u2019m facing is that i\u2019m defining 3 different losses\nand when i combine them the loss doesn\u2019t decrease while using only one seems to work fine\n<code class=\"lang-auto\">epochs = 3\nmodel_2.train()\nfor e in range(epochs):\n    loss_r = [0,0,0]\n    r = 0\n    for train , test in zip(train_loader_2,train_loader) :\n        images = train[0].view(-1,14*14)\n        labels_1 = train[1]\n        labels_2 = test[0].view(-1,784)\n        optimizer.zero_grad()\n        x , x_en , x_dec = model_2(images)\n        loss_1 = criterion_1(x,labels_2)\n        loss_2 = criterion_2(x_en,labels_2)\n        loss_3 = criterion_3(x_dec,labels_1)\n        \n        torch.autograd.backward([loss_1,loss_2,loss_3])\n        \n        optimizer.step()\n        loss_r[0] += loss_1.item()\n        loss_r[1] += loss_2.item()\n        loss_r[2] += loss_3.item()\n        if r%10 == 9 :\n            print(\"epoch = {} batch = {} final_loss={} aux_loss = {} classification_loss={}\"\n                  .format(e+1,r+1,loss_r[0]\/10,loss_r[1]\/10,loss_r[2]\/10))\n            loss_r = [0,0,0]\n        \n        r+=1\n<\/code>\nwhat seems to be the problem here","y":"Both approaches should result in accumulated gradients.\nSince the gradients are accumulated, you might need to lower the learning rate or scale the losses.","z":"Hi,\nI think you need to specifically define the combination.\n\n\n\n mohamed_nabil:\n\ntorch.autograd.backward([loss_1,loss_2,loss_3])\n\n\nFor instance,\n<code class=\"lang-auto\">loss = loss_1 + loss_2 + loss_3\nloss.backward()\n<\/code>\nI have never seen the approach you haved used to combine multiple losses so maybe the function you calling is not doing what you expected.\nBests,\nNik\nBoth approaches should result in accumulated gradients.\nSince the gradients are accumulated, you might need to lower the learning rate or scale the losses.\nThanks for your help\ncan i use the same loss fuction or shoud i define different one for every loss\nYou can use the same, if you are fine with its setup (e.g. reduction, weight, if passed, etc.)."},{"x":"Hi,\nIt may seem dummy for some of the developers, but this is the first time that I am dealing with around 50gb training data. Until now, I was only implementing homeworks and can store the data in my local.\nNow, I want to debug the baseline code, visualize the data and training&amp; validation curves by using tensorboard (if you are using a different one, I am open to suggestions), but the code is running on the remote server on the GPU. The dataset is available on the remote server. Since I am using mac there is no chance to run in my local the code also store the dataset.\nI am wondering, how do you handle this kind of situations in bigger projects in terms of the scale?\n\nHow to visualize data\nHow implementation works (Are you implementing in your local but what about testing?)\n\nI am looking for general advice. Could you please help me?\nThanks a lot","y":"If you are working on a remote server (with docker), you could open a specific port (e.g. 6006) and use it to for the tensorboard visualization.\nThe workflow might differ, but you could of course directly work on the server, with a code editor in the terminal or you could use a remote session with e.g. VSCode and develop of your Laptop.\nI\u2019m not familiar with Mac, but Atom should also have this remote feature.","z":"If you are working on a remote server (with docker), you could open a specific port (e.g. 6006) and use it to for the tensorboard visualization.\nThe workflow might differ, but you could of course directly work on the server, with a code editor in the terminal or you could use a remote session with e.g. VSCode and develop of your Laptop.\nI\u2019m not familiar with Mac, but Atom should also have this remote feature."},{"x":"Does torch.cat work with backpropagation?","y":"And if I wanted to concatenate a list of tensors, how would I do it with cat?","z":"Hi,\nYes,\nAll operations that works with floating point numbers work with backprop in pytorch (or you will get an error when trying).\nThank you!\nSo if I have something like that\nclass NeuralNet2(nn.Module):\ndef init(self):\nsuper(NeuralNet2, self).init()\n    self.input = nn.Linear(2, 40)\n    self.hiddenLeft = nn.Linear(40, 2)\n    self.hiddenRight = nn.Linear(40, 2)\n    self.out = nn.Linear(4, 6)\n\ndef forward(self, x):\n\n    x = self.input(x)\n    xLeft, xRight = torch.sigmoid(self.hiddenLeft(x)), torch.sigmoid(self.hiddenRight(x))\n    x = torch.cat((xLeft, xRight), 2)\n    x = F.sigmoid(self.out(x))\n\n    return x\n\nIs it necessary to re-implement the backward? Or how would it work in this case?\nNo you don\u2019t need to reimplement the backward. Since we can just backprop through the cat operation, gradients will be computed for all your parameters.\nSo the concatenation causes something similar to what would be a sequential model?\nNot sequential because your modules don\u2019t operate on the result of the previous one.\nIt just concatenates two Tensors. That is it.\nThat is to say, independently of the concatenation, the neurons will be able to keep a history of who precedes them at the time of updating the weights\n\nthe neurons will be able to keep a history of who precedes them at the time of updating the weights\n\nNot sure what that means.\nBut if the question is \u201cWill it be able to compute gradients?\u201d, then the answer is yes.\nIf you would like to access specific positions in a tensor, how would you do it most efficiently?\ntensor([[[0.1743, 0.2439, 0.2543]],\n[[0.1325, 0.0778, 0.2292]],\n[[0.3474, 0.1554, 0.1000]],\n[[0.1599, 0.0283, 0.0305]],\n[[0.3842, 0.3997, 0.2972]],\n[[0.1876, 0.0815, 0.3047]],\n[[0.2280, 0.1096, 0.2919]],\n[[0.3843, 0.3552, 0.1672]]])\nAnd I want the tensor but only with the first and third columns???\nHi,\nThe simplest would be to use indexing: t[:, (0, 2)].\nThanks!\nIt would not be so t[:, :, (0, 2)]?\nHo yes, sorry, though it was a 2D Tensor.\nAnd if I wanted to concatenate a list of tensors, how would I do it with cat?\ncat takes any iterable as input. So you can just give the list to cat.\nIs it proper to do this torch.optim.Adam(net.parameters()) after this net(inputs.float())? I need it so I define the network in the forward for necessary reasons.\n    for i, [inputs, labels] in enumerate(trainloader):\n        inputs = inputs.to(device)\n        labels = labels.to(device)\n\n        # Forward + backward + optimize\n        outputs = net(inputs.float())\n\n        optimizer = torch.optim.Adam(net.parameters())\n        optimizer.zero_grad()\n\n        loss = criterion(outputs, labels.float())\n        loss.backward()\n        optimizer.step()\nI would advise against it because optimizers have states that you want to keep from one iteration to the next.\nYou can provide a dummy input to the network when you create it if you really need to know the input size to create all the Parameters.\nI get the following error if I put the sentence before the forward:\nValueError: optimizer got an empty parameter list.\nAnd it\u2019s happening to me because I define the net in the forward, because the parameters of the first layers are defined at that time, what do you recommend?\nIs it possible to put dummy values to create the layers? but in that case, this net.parameters() is not true at the time, how can this be updated later with the real parameters?\nI think you\u2019re not using Parameters properly.\nThey are supposed to contain a set of weights that you iteratively update so that they contain the right value. So you should not re-create them all the time as they are supposed to be updated by the optimizers iteratively.\nSo it is necessary to define the net, that is nn.Linear(n1, n2) before the forward? In my case I only have the values of n1 and n2 computed during the forward, what can I do?"},{"x":"If I have a tensor like this\ntensor([[0.3843, 0.3552]],\n[[0.1743, 0.2439]],\n[[0.3474, 0.1554]],\n[[0.1325, 0.0778]],\n[[0.2280, 0.1096]],\n[[0.3842, 0.3997]],\n[[0.1599, 0.0283]],\n[[0.1876, 0.0815]]])\nHow could I get a tensor from him like that:\ntensor([[0.3843]],\n[[0.1743]],\n[[0.3474]],\n[[0.1325]],\n[[0.2280]],\n[[0.3842]],\n[[0.1599]],\n[[0.1876]]])","y":"You can directly index tensors via tensor[] as e.g. in numpy:\n<code class=\"lang-python\">x = torch.tensor([[[0.3843, 0.3552]],\n                [[0.1743, 0.2439]],\n                [[0.3474, 0.1554]],\n                [[0.1325, 0.0778]],\n                [[0.2280, 0.1096]],\n                [[0.3842, 0.3997]],\n                [[0.1599, 0.0283]],\n                [[0.1876, 0.0815]]])\n\nprint(x[:, :, 0])\n<\/code>","z":"You can directly index tensors via tensor[] as e.g. in numpy:\n<code class=\"lang-python\">x = torch.tensor([[[0.3843, 0.3552]],\n                [[0.1743, 0.2439]],\n                [[0.3474, 0.1554]],\n                [[0.1325, 0.0778]],\n                [[0.2280, 0.1096]],\n                [[0.3842, 0.3997]],\n                [[0.1599, 0.0283]],\n                [[0.1876, 0.0815]]])\n\nprint(x[:, :, 0])\n<\/code>\nThis code is correct, assuming that inputs is all the instances of the training, i.e. there is only 1 batch in trainloader?\nfor epoch in range(n_epochs):\n\n    for i, [inputs, labels] in enumerate(trainloader):\n        inputs = inputs.to(device)\n        labels = labels.to(device)\n\n        # Forward + backward + optimize\n        outputs = net(inputs.float())\n\n        if epoch == 1:\n            optimizer = torch.optim.Adam(net.parameters())\n\n        optimizer.zero_grad()\n        \n        loss = criterion(outputs, labels.float())\n        loss.backward()\n        optimizer.step()\nI would create the optimizer before entering the training loop.\nBesides that it looks alright.\nI\u2019m not sure which criterion you are using, but I assume your criterion expects float targets.\nI can\u2019t put it in before because the net is built during the forward, and I get this error ValueError: optimizer got an empty parameter list.\nThis error is raised, if you didn\u2019t register the parameters properly.\nCould you post the model code, please?"},{"x":"I have searched for vgg-face pretrained model in pytorch, but couldn\u2019t find it. Is there a github repo for the pretrained model of vgg-face in pytorch?","y":"Hi! I hope it\u2019s not too late.\nI had found this link pertaining to details regarding vgg-face model along with its weights in the link below. Scroll down to the vgg-face section and download your requirements.\nhttp:\/\/www.robots.ox.ac.uk\/~albanie\/pytorch-models.html\nHope this helps.","z":"Hi! I hope it\u2019s not too late.\nI had found this link pertaining to details regarding vgg-face model along with its weights in the link below. Scroll down to the vgg-face section and download your requirements.\nhttp:\/\/www.robots.ox.ac.uk\/~albanie\/pytorch-models.html\nHope this helps.\nThank you .\nI hope this also helps.\n\n\n\nGitHub with link \"https:\/\/github.com\/cydonia999\/VGGFace2-pytorch\"\n\n\n\ncydonia999\/VGGFace2-pytorch with link \"https:\/\/github.com\/cydonia999\/VGGFace2-pytorch\"\nVGGFace2-pytorch - PyTorch Face Recognizer based on 'VGGFace2: A dataset for recognising faces across pose and age'\n\n\n\n\n\n\n\nHi! I hope it\u2019s not too late.\nI had found this link pertaining to details regarding vgg-face model along with its weights in the link below. Scroll down to the vgg-face section and download your requirements.\nhttp:\/\/www.robots.ox.ac.uk\/~albanie\/pytorch-models.html \nHope this helps.\n\nHi, is this loadable in the VGG16 torchvision model ?\nI dont think its available as torchvision model. You still have to load the pretrained weights manually.\nI managed to load them manually thanks for your response.\n<code class=\"lang-auto\">def compose_transforms(meta, resize=256, center_crop=True,\n                       override_meta_imsize=False):\n    \"\"\"\n    Compose preprocessing transforms for model\n    The imported models use a range of different preprocessing options,\n    depending on how they were originally trained. Models trained in MatConvNet\n    typically require input images that have been scaled to [0,255], rather\n    than the [0,1] range favoured by PyTorch.\n    Args:\n        meta (dict): model preprocessing requirements\n        resize (int) [256]: resize the input image to this size\n        center_crop (bool) [True]: whether to center crop the image\n        override_meta_imsize (bool) [False]: if true, use the value of `resize`\n           to select the image input size, rather than the properties contained\n           in meta (this option only applies when center cropping is not used.\n    Return:\n        (transforms.Compose): Composition of preprocessing transforms\n    \"\"\"\n    normalize = transforms.Normalize(mean=meta['mean'], std=meta['std'])\n    im_size = meta['imageSize']\n    assert im_size[0] == im_size[1], 'expected square image size'\n    if center_crop:\n        transform_list = [transforms.Resize(resize),\n                          transforms.CenterCrop(size=(im_size[0], im_size[1]))]\n    else:\n        if override_meta_imsize:\n            im_size = (resize, resize)\n        transform_list = [transforms.Resize(size=(im_size[0], im_size[1]))]\n    transform_list += [transforms.ToTensor()]\n    if meta['std'] == [1, 1, 1]:  # common amongst mcn models\n        transform_list += [lambda x: x * 255.0]\n    transform_list.append(normalize)\n    return transforms.Compose(transform_list)\n<\/code>\nThe model is using the above preprocessing function. What are the transforms actually applied here with link \"https:\/\/github.com\/albanie\/pytorch-benchmarks\/blob\/081e82eb43439ce2821622a6e3d627877caed67a\/utils\/benchmark_helpers.py#L11\" ?\nAlso why is the following transform used ?\n<code class=\"lang-auto\">if meta['std'] == [1, 1, 1]:  # common amongst mcn models\n    transform_list += [lambda x: x * 255.0]\n<\/code>\nHello,\nI\u2019ve been trying to use the ResNet50 model from the author site, and downloaded the model\/weights. My main use of this is to pass in a batch of images and extract a feature map. I was able to do this successfully with the original pytorch implementation, however, when I do the same setup, I\u2019m thrown an error because of the batch and because of the dimensions. I tried tracing back to see if I could adjust, but I\u2019m unable to find anywhere to work on this. Any ideas? I\u2019m initially using images of shape [B,C,H,W]=[8,3,256,256]. I know ResNet is supposed to be 224x224 but 256 was working, and needed for the size feature map I\u2019m requiring.\nIt seems like part of the issue might be handling batches itself?\nHere is the error:\n<code class=\"lang-auto\">Traceback (most recent call last):\n  File \"C:\/Users\/a-pollack\/Projects\/de_ident\/de_ident.py\", line 27, in <module>\n    gen_losses, dis_losses, gen, dis = train_net(num_epochs=5, batch_size=8, lr=1e-4, betas=(0.5, 0.99))\n  File \"C:\\Users\\a-pollack\\Projects\\de_ident\\train.py\", line 122, in train_net\n    z_raw, m, z_masked = gen(data[0])\n  File \"C:\\Users\\a-pollack\\Anaconda3\\lib\\site-packages\\torch\\nn\\modules\\module.py\", line 541, in __call__\n    result = self.forward(*input, **kwargs)\n  File \"C:\\Users\\a-pollack\\Projects\\de_ident\\generator.py\", line 75, in forward\n    face_descriptor = fd_conv2(x)\n  File \"C:\\Users\\a-pollack\\Anaconda3\\lib\\site-packages\\torch\\nn\\modules\\module.py\", line 541, in __call__\n    result = self.forward(*input, **kwargs)\n  File \"C:\\Users\\a-pollack\\Projects\\de_ident\\generator_utils.py\", line 54, in forward\n    x = self.features(x)\n  File \"C:\\Users\\a-pollack\\Anaconda3\\lib\\site-packages\\torch\\nn\\modules\\module.py\", line 541, in __call__\n    result = self.forward(*input, **kwargs)\n  File \"C:\\Users\\a-pollack\\Anaconda3\\lib\\site-packages\\torch\\nn\\modules\\container.py\", line 92, in forward\n    input = module(input)\n  File \"C:\\Users\\a-pollack\\Anaconda3\\lib\\site-packages\\torch\\nn\\modules\\module.py\", line 541, in __call__\n    result = self.forward(*input, **kwargs)\n  File \"C:\\Users\\a-pollack\\Anaconda3\\lib\\site-packages\\torch\\nn\\modules\\conv.py\", line 345, in forward\n    return self.conv2d_forward(input, self.weight)\n  File \"C:\\Users\\a-pollack\\Anaconda3\\lib\\site-packages\\torch\\nn\\modules\\conv.py\", line 342, in conv2d_forward\n    self.padding, self.dilation, self.groups)\nRuntimeError: Given groups=1, weight of size 256 64 1 1, expected input[8, 256, 64, 64] to have 64 channels, but got 256 channels instead\n<\/code>\nThe error points to a wrong number of input channels to this particular convolution.\nThis might be caused by a wrong reshape or by mixing up layers.\nDo we provide images in RGB or BGR format for VGGFace2 resnet50 model?\nIt doesn\u2019t say anything except that model is trained using the Caffe framework and Caffe uses a BGR color channel scheme for reading image files. It also says that mean image vector is [131.0912, 103.8827, 91.4953] but if see values we can conclude that this mean image vector is in RGB format (there is more red than blue in face images). Does anybody know fo sure what is the truth?\nYou are right. I use PIL to read images which is generally in RGB format. In case you use opencv to read images, please consider converting BGR to RGB."},{"x":"Which kernel is used in a Convolutional layer by default in pytorch?","y":"The kernels used by default in Pytorch use the He initialisation from this paper:\nDelving Deep into Rectifiers: Surpassing Human-Level Performance on ImageNet Classification with link \"https:\/\/arxiv.org\/abs\/1502.01852\"","z":"The kernels used by default in Pytorch use the He initialisation from this paper:\nDelving Deep into Rectifiers: Surpassing Human-Level Performance on ImageNet Classification with link \"https:\/\/arxiv.org\/abs\/1502.01852\"\nOkay, thank you for providing the link, appreciated.\nplease explain more, more specifid"},{"x":"Hi, I am working on semantic segmentation. Currently, my labels for training are of size [batch_size, channels, h, w] where channels = 1, each pixel is either 0 (pixel belongs to background) or 1 (pixel belongs to target_class). What I need to do is to convert my [batch_size, 1, h,w]-label into a [batch_size, 2, h, w]-label where each pixel with 0 is converted to [1,0] and each 1 to [0,1].\nBasicially one-hot encode each pixel.\nI don\u2019t want to iterate through every pixel by for loop since this would take way too long.\nCan someone help me out? I am sure there is a faster way to do this?\nCheers!","y":"Hi,\n<code class=\"lang-python\">one_hot_label = torch.cat((1 - current_label, current_label), dim=1)\n<\/code>\nexplanation:\n\n(1 - current_label, current_label) would be (1,0) when current label is 0; and it would be (0,1) when current label is 1.\ndim=1 means to operate on the 2nd dimension (which is channels)\n","z":"Hi,\n<code class=\"lang-python\">one_hot_label = torch.cat((1 - current_label, current_label), dim=1)\n<\/code>\nexplanation:\n\n(1 - current_label, current_label) would be (1,0) when current label is 0; and it would be (0,1) when current label is 1.\ndim=1 means to operate on the 2nd dimension (which is channels)\n"},{"x":"My code:\nclass LSTMModel(nn.Module):\ndef init(self):\nsuper(LSTMModel, self).init()\nself.LSTM = nn.LSTM(input_size=input_size, hidden_size=hidden_size,\nbatch_first=True)\ndef forward(self ,x, hidden, cell):\n    x = x.view(batch_size, sequence_length, input_size)\n    out, (hidden, cell)= self.LSTM(x, hidden, cell)\n    out= out.view(-1, num_classes)\n    return out,hidden,cell\ndef init_hidden(self):\n    return Variable(torch.zeros(num_layers, batch_size, hidden_size))\n\nmodel = LSTMModel()\ncriterion = nn.CrossEntropyLoss()\noptimizer = optim.Adam(model.parameters(), lr=0.1)\nfor epoch in range(100):\noptimizer.zero_grad()\nloss = 0\nhidden = model.init_hidden()\ncell = model.init_hidden()\nprint(\u201cPredicted String\u201d)\nfor ins, label in zip(inputs,labels):\noutput,hidden,cell = model(ins,hidden,cell)\nval, idx = output.max(1)\nprint(idx2char[idx.data[0]])\nloss+=criterion(output, label.unsqueeze(0))\nprint(\", epoch: %d, loss: %1.3f\" % (epoch+1, loss.data[0]))\n\nloss.backward()\noptimizer.step()\n\nPredicted String\nTraceback (most recent call last):\nFile \u201c\u201d, line 1, in \nrunfile(\u2018E:\/Python Deep Learnig Projects\/Code\/Section_3\/seqtest.py\u2019, wdir=\u2018E:\/Python Deep Learnig Projects\/Code\/Section_3\u2019)\nFile \u201cC:\\Users\\admin\\Miniconda3\\envs\\tensorflow\\lib\\site-packages\\spyder_kernels\\customize\\spydercustomize.py\u201d, line 827, in runfile\nexecfile(filename, namespace)\nFile \u201cC:\\Users\\admin\\Miniconda3\\envs\\tensorflow\\lib\\site-packages\\spyder_kernels\\customize\\spydercustomize.py\u201d, line 110, in execfile\nexec(compile(f.read(), filename, \u2018exec\u2019), namespace)\nFile \u201cE:\/Python Deep Learnig Projects\/Code\/Section_3\/seqtest.py\u201d, line 62, in \noutput,hidden,cell = model(ins,hidden,cell)\nFile \u201cC:\\Users\\admin\\Miniconda3\\envs\\tensorflow\\lib\\site-packages\\torch\\nn\\modules\\module.py\u201d, line 532, in call\nresult = self.forward(*input, **kwargs)\nFile \u201cE:\/Python Deep Learnig Projects\/Code\/Section_3\/seqtest.py\u201d, line 45, in forward\nout, (hidden, cell)= self.LSTM(x, hidden, cell)\nFile \u201cC:\\Users\\admin\\Miniconda3\\envs\\tensorflow\\lib\\site-packages\\torch\\nn\\modules\\module.py\u201d, line 532, in call\nresult = self.forward(*input, **kwargs)\nTypeError: forward() takes from 2 to 3 positional arguments but 4 were given","y":"\n\n\n AntuAnant:\n\nout, (hidden, cell)= self.LSTM(x, hidden, cell)\n\n\nModify this line to\n<code class=\"lang-python\">out, (hidden, cell)= self.LSTM(x, (hidden, cell))\n<\/code>","z":"\n\n\n AntuAnant:\n\nout, (hidden, cell)= self.LSTM(x, hidden, cell)\n\n\nModify this line to\n<code class=\"lang-python\">out, (hidden, cell)= self.LSTM(x, (hidden, cell))\n<\/code>"},{"x":"For Faster\/Mask RCNN, before training the model, min and max size of the input images is fixed, which is determined to a great extent by the CUDA memory capacity. This is OK.\nWhat I found at inference time, is that accuracy of the model strongly depends on the size of the input image. By default, min_size=800, max_size=1333, but as I varied both hyperparameters, I got either better of worse results. I think the best results are obtained when the min_size and max_size are close to the input image\u2019s size, but I\u2019m not sure.\nSo this is my question: is there a confident way to find optimal input image size?","y":"I would assume the model should work the best during validation using images with shapes and other properties as close as possible to the training data.\nThe shape limits are most likely added due to the architecture (but I haven\u2019t looked into the source code to verify it).","z":"I would assume the model should work the best during validation using images with shapes and other properties as close as possible to the training data.\nThe shape limits are most likely added due to the architecture (but I haven\u2019t looked into the source code to verify it)."},{"x":"I\u2019m applying transfer learning to the resnet50 from torchvision models, i.e. replaced the last fc layer with 2 neurons for binary classification problem. All other layers are frozen. I started training the network with learning rate 1e-2 and reducing it by a factor of 0.1 after 5 epochs if validation loss is not decreased. However, when the learning rate decreases from 1e-2 to 1e-3 for the first time, there is a suddent and very big increase in training accuracy (therefore decrease in the loss). Training accuracy jumps from around 54% to 96%. I attach the loss and accuracy plots. I use cross entropy loss. What might be the reason of this sudden big increase in the training accuracy?\n ","y":"Your initial learning rate might be too high. Lowering the learning rate might \u201cstart\u201d the training.\nIf your dataset is quite easy (based on the step size it seems you are using very few samples), the model might instantly overfit the training data.","z":"Your initial learning rate might be too high. Lowering the learning rate might \u201cstart\u201d the training.\nIf your dataset is quite easy (based on the step size it seems you are using very few samples), the model might instantly overfit the training data.\nActually you\u2019re right. My dataset size is not much. I suspected learning rate to be high, therefore started training with learning rate 1e-3. In that case the model doesn\u2019t fully overfit (training accuracy doesn\u2019t reach around 100%), meaning I didn\u2019t observe such dramatic increases. However, validation accuracy also doesn\u2019t increase accordingly. This is also overfit, I guess, because the model can convey what it seems to learn from training set to validation set.\nWhat made me surprised is that after jumping around global minima with high learning rate, one epoch would be enough to reach almost 100%. It didn\u2019t make sense to me. Thanks for your response."},{"x":"Inception v3 is taking a lot of time to load compared to other torchvision models. Is there any bug?\nusing torchvision 0.4.2","y":"Yes, it\u2019s a known bug with link \"https:\/\/discuss.pytorch.org\/t\/torchvisions-inception-v3-takes-much-longer-to-load-than-other-models\/68756\/6\" and here is the corresponding bug with link \"https:\/\/github.com\/pytorch\/vision\/issues\/1797\".\nYou should be able to pass init_weights=False in the nightly binaries to bypass the scipy issue.","z":"Yes, it\u2019s a known bug with link \"https:\/\/discuss.pytorch.org\/t\/torchvisions-inception-v3-takes-much-longer-to-load-than-other-models\/68756\/6\" and here is the corresponding bug with link \"https:\/\/github.com\/pytorch\/vision\/issues\/1797\".\nYou should be able to pass init_weights=False in the nightly binaries to bypass the scipy issue."},{"x":"Say I have a 3x3 kernel for a conv. A normal conv2d would multiply the same 3x3 numbers to all the channels and add them up. What if, say i have 3 channels as input and instead of using the same 3x3 numbers for all the channels, i have different numbers effectively 3(channel) x 3 x 3 (kernel size). How would I do this? Thank you.","y":"The second use case is actually a vanilla convolution and you can check the kernel shape via:\n<code class=\"lang-python\">print(nn.Conv2d(3, 1, 3).weight.shape)\n> torch.Size([1, 3, 3, 3]) # out_channels, in_channels, height, width\n<\/code>\nHave a look at CS231n - Convolution layers with link \"http:\/\/cs231n.github.io\/convolutional-networks\/#conv\", which describes the underlying method quite well.","z":"The second use case is actually a vanilla convolution and you can check the kernel shape via:\n<code class=\"lang-python\">print(nn.Conv2d(3, 1, 3).weight.shape)\n> torch.Size([1, 3, 3, 3]) # out_channels, in_channels, height, width\n<\/code>\nHave a look at CS231n - Convolution layers with link \"http:\/\/cs231n.github.io\/convolutional-networks\/#conv\", which describes the underlying method quite well.\n\n\n\n ptrblck:\n\nprint(nn.Conv2d(3, 1, 3).weight.shape)\n\n\nOhhhh. okay thanks. I didn\u2019t know sorry."},{"x":"my below code snippet:\nmodel = \u2026 #somemodel\nmodel = model.cuda()\nmodel, optimizer = amp.initialize(model, optimizer, opt_level=\u201cO2\u201d)\nmodel = nn.DataParallel(model)\n<h1>in dataloader batch<\/h1>\nimages = images.cuda()\noutput = model(images)\nlast line is giving me this error:\nExpected tensor for argument #1 \u2018input\u2019 to have the same device as                                                                                                                                                              tensor for argument #2 \u2018weight\u2019; but device 1 does not equal 0 (while checking                                                                                                                                                              arguments for cudnn_convolution)\ni am running this code on gcp 2xT4 server. What portion of the above mentioned code is wrong or needs to be corrected to use multi gpu training?\n[pytorch version 1.1.0]","y":"Hi,\nI think this is a known limitation of amp. You can see this issue on their repo for more info: https:\/\/github.com\/NVIDIA\/apex\/issues\/503","z":"Hi,\nI think this is a known limitation of amp. You can see this issue on their repo for more info: https:\/\/github.com\/NVIDIA\/apex\/issues\/503"},{"x":"Hello guys, i\u00b4m trying to solve the Poisson\u00b4s equation by using the relaxation method to know the potential and electric field matrixs but i have an error that say  \u201cindex 101 is out of bounds for axis 0 with size 101\u201d and i dont understand what is the problem. \u00bfCan anyone helpme please?\nimport numpy as np\nimport math\n<h1>SYMBOLS USED IN THIS CODE<\/h1>\n<h1>E = Total electric field matrix using Poisson\u2019s equation<\/h1>\n#% V = Potential matrix\n#% Nx = Number of grid points in X- direction\n#% Ny =Number of grid points in Y-Direction\n#Enter the dimensions\nNx = 101 # Number of X-grids\nNy = 101 # Number of Y-grids\nmpx = math.ceil(Nx\/2) # Mid-point of x\nmpy = math.ceil(Ny\/2) #Mid point of y\nNi = 750 #Number of iterations for the Poisson solver\nV = np.zeros((Nx,Ny))\nT = 0 # Top-wall potential\nB = 0 # Bottom-wall potential\nL = 0 # Left-wall potential\nR = 0 # Right-wall potential\n#-------------------------------------------------------------------------\n<h1>Initializing edges potentials<\/h1>\n#-------------------------------------------------------------------------\nV[1,:] = L\nR= V[Nx,:]\nV[:,1] = B\nV[:,Ny] = T\n#-------------------------------------------------------------------------\n<h1>Initializing Corner potentials<\/h1>\n#-------------------------------------------------------------------------\nV[1,1] = 0.5*(V[1,2]+V[2,1])\nV[Nx,1] = 0.5*(V[Nx-1,1]+V[Nx,2])\nV[1,Ny] = 0.5*(V[1,Ny-1]+V[2,Ny])\nV[Nx,Ny] = 0.5*(V[Nx,Ny-1]+V[Nx-1,Ny])\nlength_plate = 51 # Length of plate in terms of number of grids\nlp = math.floor(length_plate\/2)\nposition_plate = 15 # Position of plate on x axis\npp1 = mpx+position_plate\npp2 = mpx-position_plate\nfor z in range(1,Ni): #Number of iterations\nfor i in range(2,Nx-1):\nfor j in range(2,Ny-1):\n#The next two lines are meant to force the matrix to hold the\n<h1>potential values for all iterations<\/h1>\nV[pp1,mpy-lp:mpy+lp] = 100\nV[pp2,mpy-lp:mpy+lp] = -100\nV[i,j]=0.25*(V[i+1,j]+V[i-1,j]+V[i,j+1]+V[i,j-1])\n<h1>Take transpose for proper x-y orientation<\/h1>\nV = np.transpose(V)\n[Ex,Ey]=np.gradient(V)\nEx = -Ex\nEy = -Ey\n<h1>Electric field Magnitude<\/h1>\nE= math.sqrt( ExEx + EyEy )","y":"Hi,\nYou should have a stack trace of where the error happened right? The error means that the index you use is out of bound of the Tensor you try to read from (python is 0 indexed, so for a Tensor of size 101, the valid indices are 0 to 100 ","z":"Hi,\nYou should have a stack trace of where the error happened right? The error means that the index you use is out of bound of the Tensor you try to read from (python is 0 indexed, so for a Tensor of size 101, the valid indices are 0 to 100 "},{"x":"I am using a pretrained mobilenet as follows :\n<code class=\"lang-auto\">model = torchvision.models.mobilenet_v2(pretrained=True)\n<\/code>\nmodel.children() gives all the layers, including the last classification head.\nHowever , model.features gives all the layers excluding the classification head.\nWhy is this so? Are there any cases where both give the same result?\nI would also be thankful if anyone pointed me to the PyTorch documentation for .features (I couldn\u2019t seem to find it).","y":"model.features is defined inside the model\u2019s __init__ method and thus not a general attribute of nn.Modules.\nDepending how you are designing your model you might want to create certain \u201cblocks\u201d e.g. features, classifier etc. In autoencoder-like models you would most likely find a model.encoder and model.decoder attribute.","z":"model.features is defined inside the model\u2019s __init__ method and thus not a general attribute of nn.Modules.\nDepending how you are designing your model you might want to create certain \u201cblocks\u201d e.g. features, classifier etc. In autoencoder-like models you would most likely find a model.encoder and model.decoder attribute."},{"x":"I am trying to use PyTorch to get the outputs from intermediate layers of AlexNet\/VGG:\n<code class=\"lang-auto\">alexnet_model = models.alexnet(pretrained=True)\nmodules = list((alexnet_model).children())[:-1*int(depth)]\nalexnet_model = nn.Sequential(*modules)\n<\/code>\nWhat is odd is that I get the same output values (i.e. the same exact model) when depth=1 and depth=2, and then the same output values for depth=3 all the way to depth=10. I observe this same phenomenon for VGG too. However, I don\u2019t observe this for ResNet, which gives me different output values (i.e. different models) for all depths [1, 10].\nAny ideas about what might be going on?","y":"You can access a module inside an nn.Sequential block by indexing it:\n<code class=\"lang-python\">model = nn.Sequential(\n    nn.Conv2d(3, 6, 3, 1, 1),\n    nn.ReLU(),\n    nn.Conv2d(6, 1, 3, 1, 1)\n)\n# get second conv layer\nc = model[2]\n<\/code>","z":"list((alexnet_model).children()) will return a list of length 3 containing the first nn.Sequential block for the feature extraction, the nn.AdaptiveAvgPool2d layer, and the last nn.Sequential block used as the classifier.\nIf you use depth>=3, modules will be empty and you will just get back your input tensor.\nThanks for your response! How do I get outputs of the layers within the sequential blocks? And how is this working for the ResNet architecture?\nYou could use forward hook as described in this example with link \"https:\/\/discuss.pytorch.org\/t\/how-can-l-load-my-best-model-as-a-feature-extractor-evaluator\/17254\/6\".\nThanks for the reference! Apparently, the layers within the sequential blocks (of AlexNet, VGG, etc.) don\u2019t have names associated with them (e.g. \u2018self.fc2\u2019); how could I extract outputs from certain layers within the last sequential block using your function?\nYou can access a module inside an nn.Sequential block by indexing it:\n<code class=\"lang-python\">model = nn.Sequential(\n    nn.Conv2d(3, 6, 3, 1, 1),\n    nn.ReLU(),\n    nn.Conv2d(6, 1, 3, 1, 1)\n)\n# get second conv layer\nc = model[2]\n<\/code>"},{"x":"Hi ya\u2019ll!\nI\u2019m trying to train a binary classification model and I\u2019ve observed my validation loss is way lower than the training loss. How is that possible?\nHere\u2019s the training loop.\n<code class=\"lang-auto\">model.train()\nfor e in tqdm(range(1, EPOCHS+1)):\n    train_epoch_loss = 0\n    \n    for x_batch_train, y_batch_train in train_loader:\n        x_batch_train, y_batch_train = x_batch_train.to(device), y_batch_train.to(device)\n        \n        optimizer.zero_grad()\n    \n        y_pred_probs_train = model(x_batch_train).squeeze()\n        \n        train_loss = criterion(y_pred_probs_train, y_batch_train)\n                \n        train_loss.backward()\n        \n        optimizer.step()\n        \n        train_epoch_loss += train_loss.item()\n        \n    \n    with torch.no_grad():\n        model.eval()\n        for x_batch_val, y_batch_val in validation_loader:\n            val_epoch_loss = 0\n\n            x_batch_val, y_batch_val = x_batch_val.to(device), y_batch_val.to(device)\n\n            y_pred_probs_val = model(x_batch_val).squeeze()\n\n            val_loss = criterion(y_pred_probs_val, y_batch_val)\n\n            val_epoch_loss += val_loss.item()\n\n\n    print(f'Epoch {e+0:02}: | Train Loss: {train_epoch_loss\/len(train_loader):.5f} | Val Loss: {val_epoch_loss\/len(validation_loader):.5f}')\n<\/code>\nAnd following is the output of the loop.\n<code class=\"lang-auto\">########## OUTPUT ################\n\nEpoch 01: | Train Loss: 16.43517 | Val Loss: 0.45582\nEpoch 02: | Train Loss: 5.16361 | Val Loss: 0.05326\nEpoch 03: | Train Loss: 0.69327 | Val Loss: 0.05327\nEpoch 04: | Train Loss: 0.69327 | Val Loss: 0.05327\nEpoch 05: | Train Loss: 0.69319 | Val Loss: 0.05329\nEpoch 06: | Train Loss: 0.69314 | Val Loss: 0.05330\nEpoch 07: | Train Loss: 0.69310 | Val Loss: 0.05334\nEpoch 08: | Train Loss: 0.69302 | Val Loss: 0.05336\nEpoch 09: | Train Loss: 0.69291 | Val Loss: 0.05335\nEpoch 10: | Train Loss: 0.69270 | Val Loss: 0.05330\nEpoch 11: | Train Loss: 0.69020 | Val Loss: 0.05365\nEpoch 12: | Train Loss: 0.68584 | Val Loss: 0.05278\nEpoch 13: | Train Loss: 0.68309 | Val Loss: 0.05325\nEpoch 14: | Train Loss: 0.68111 | Val Loss: 0.05341\nEpoch 15: | Train Loss: 0.67870 | Val Loss: 0.05416\nEpoch 16: | Train Loss: 0.67404 | Val Loss: 0.05502\nEpoch 17: | Train Loss: 0.67135 | Val Loss: 0.05591\nEpoch 18: | Train Loss: 0.66845 | Val Loss: 0.05643\nEpoch 19: | Train Loss: 0.66542 | Val Loss: 0.05629\nEpoch 20: | Train Loss: 0.66551 | Val Loss: 0.05840\n<\/code>\nThe class distribution in my train and validation set are pretty balanced -\nTrain - {'class_1': 199, 'class_0': 201}\nValidation - {'class_1': 50, 'class_0': 48}\nI used random_split() to create validation+train from my original dataset. \u2019\nThe test-performance is even worse -\n<code class=\"lang-auto\"># Classification Report\n\n              precision    recall  f1-score   support\n\n           0       0.50      1.00      0.67       250\n           1       0.00      0.00      0.00       250\n\n    accuracy                           0.50       500\n   macro avg       0.25      0.50      0.33       500\nweighted avg       0.25      0.50      0.33       500\n\n\n# Confusion Matrix\n[[250   0]\n [250   0]]\n<\/code>\nHere\u2019s my model -\n<code class=\"lang-auto\">HotDogClassifier(\n  (block1): Sequential(\n    (0): Conv2d(3, 256, kernel_size=(5, 5), stride=(1, 1), padding=(2, 2))\n    (1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n    (2): ReLU()\n    (3): Dropout2d(p=0.1, inplace=False)\n  )\n  (block2): Sequential(\n    (0): Conv2d(256, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n    (1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n    (2): ReLU()\n    (3): Dropout2d(p=0.1, inplace=False)\n  )\n  (block3): Sequential(\n    (0): Conv2d(128, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n    (1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n    (2): ReLU()\n    (3): Dropout2d(p=0.1, inplace=False)\n  )\n  (lastcnn): Conv2d(64, 2, kernel_size=(56, 56), stride=(1, 1))\n  (maxpool): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n)\n<\/code>\nWhat am I doing something wrong?","y":"<code class=\"lang-auto\">val_epoch_loss = 0\n<\/code>\nis inside the validation for loop , it should be before the for loop","z":"<code class=\"lang-auto\">val_epoch_loss = 0\n<\/code>\nis inside the validation for loop , it should be before the for loop"},{"x":"Can batch norm act like regularization?","y":"You might find useful reading carefully the article with link \"https:\/\/arxiv.org\/pdf\/1502.03167.pdf\".\nBatch norm enables training with larger learning rates, and this other article with link \"https:\/\/arxiv.org\/pdf\/1806.02375.pdf\" argues \u201c\u2026that the larger learning rate increases the implicit regularization of SGD, which improves generalization\u2026\u201d.\nI believe you could start reading these two articles and their references. As far as I know, there is still a lot of mystery on why exactly batch norm works so well.","z":"You might find useful reading carefully the article with link \"https:\/\/arxiv.org\/pdf\/1502.03167.pdf\".\nBatch norm enables training with larger learning rates, and this other article with link \"https:\/\/arxiv.org\/pdf\/1806.02375.pdf\" argues \u201c\u2026that the larger learning rate increases the implicit regularization of SGD, which improves generalization\u2026\u201d.\nI believe you could start reading these two articles and their references. As far as I know, there is still a lot of mystery on why exactly batch norm works so well."},{"x":"As the question suggests, I\u2019m trying to convert images to tensor.\nX, y = train_sequence[idx]  \n        \nimages = Variable(torch.from_numpy(X)).to(device) # [batch, channel, H, W]\nmasks = Variable(torch.from_numpy(y)).to(device) \nprint(type(images)) ## Output: <class 'torch.Tensor'>\n\n\nimages = transforms.Normalize((0.5, 0.5, 0.5, 0.5, 0.5), (0.5, 0.5, 0.5,0.5, 0.5))(images)\nmasks =  transforms.Normalize((0.5), (0.5))(masks)\n\nBut I get the error at\n---> 19         images = transforms.Normalize((0.5, 0.5, 0.5, 0.5, 0.5), (0.5, 0.5, 0.5,0.5, 0.5))(images)\nTypeError: tensor is not a torch image.","y":"I think I solved it. I was missing the first parameter.\n images = transforms.Normalize(images,(0.5, 0.5, 0.5, 0.5, 0.5), (0.5, 0.5, 0.5,0.5, 0.5))\n masks =  transforms.Normalize(masks, (0.5), (0.5))","z":"I think I solved it. I was missing the first parameter.\n images = transforms.Normalize(images,(0.5, 0.5, 0.5, 0.5, 0.5), (0.5, 0.5, 0.5,0.5, 0.5))\n masks =  transforms.Normalize(masks, (0.5), (0.5))"},{"x":"Hello everyone\nI am trying to initialize the layers of my network using kaiming_normal_ initializer. This method has an argument called mode with fan_in and fan_out options.\nI read the docs and found out it depends on the number of filters or channels. For example, I have a Conv2d layer with size of  [64, 3, 4, 4]. When I calculate the values, I get fan_in=48 and fan_out=1024 which is a huge difference and result in std = gain\/math.sqrt(fan) is like this:\nfan_in enabled: std=0.2\nfan_out enabled: std=0.04\nAnd std is passed to tensor.normal_ method.\nThe question is I did not find any specific explanation about which mode should be chose in the paper I am trying to implement or other resources.\nBest regards","y":"There are two parts:\n\nAs Avinash points out, the default mode 'fan_in' is probably a good choice.\nFor some intuition of why this is: Each output is a weighted sum of fan_in inputs. For linear, this is one row of the matrix multiplication, for convolutions it is number of in-channels  * kernel size.\n\nWhen  and I implemented StyleGAN with link \"https:\/\/github.com\/lernapparat\/lernapparat\/blob\/master\/style_gan\/pytorch_style_gan.ipynb\" for PyTorch (but I don\u2019t think the StyleGAN authors necessarily invented it), I\u2019ve come across the idea of not having the multiplier used during init, but applying them to the weight before using it. This has the effect of using the scaling for both initial values and gradient updates. (Even if it is not that efficient without a hand-made convolution kernel.)\nBest regards\nThomas","z":"As far as I know, kaiming_normal_ or he_normal is generally initialized using fan_in.\nThere are two parts:\n\nAs Avinash points out, the default mode 'fan_in' is probably a good choice.\nFor some intuition of why this is: Each output is a weighted sum of fan_in inputs. For linear, this is one row of the matrix multiplication, for convolutions it is number of in-channels  * kernel size.\n\nWhen  and I implemented StyleGAN with link \"https:\/\/github.com\/lernapparat\/lernapparat\/blob\/master\/style_gan\/pytorch_style_gan.ipynb\" for PyTorch (but I don\u2019t think the StyleGAN authors necessarily invented it), I\u2019ve come across the idea of not having the multiplier used during init, but applying them to the weight before using it. This has the effect of using the scaling for both initial values and gradient updates. (Even if it is not that efficient without a hand-made convolution kernel.)\nBest regards\nThomas\nWhy dose resnet in torchvision apply kaiming_normal_ with mode \u2018fan_out\u2019? Is there a specific reason to do so?\nhttps:\/\/github.com\/pytorch\/vision\/blob\/master\/torchvision\/models\/resnet.py#L156"},{"x":"I have a 96x96 input image. My first layer is nn.Conv2d(1, 16, 3, padding = 1).\nThe equation for the output shape in the docs is: height + 2*padding - dilation * (kernel_size - 1) - 1 + 1 assuming a stride of 1.\nSo my output should be 96 + 2 - 1 * (3 - 1) - 1 + 1 = 96, the same dimension as the input.\nBut when I print(x.shape) after applying the layer, it\u2019s torch.Size([1, 16, 48, 48]).\nWhat\u2019s going on, I thought a padding of 1 would make it the same dimensions, but somehow it split in two? It\u2019s 48 instead of 96. Why is this?","y":"Nevermind, I forgot I was pooling (2, 2) in the same line I was convulsing.","z":"Nevermind, I forgot I was pooling (2, 2) in the same line I was convulsing."},{"x":"torchvision.transforms.functional.to_grayscale() can only applied to PIL Image.\nthen how can I convert torch.Tensor RGB to gray?","y":"Well, like  says, you can try to \u201cCompose Transforms\u201d. An example would be here with link \"https:\/\/github.com\/psyec1\/Lipreading-PyTorch\/blob\/ebfca05ea004fa3cd63980470268a043922432cb\/data\/preprocess.py#L56\". And then use image to PIL transform with link \"https:\/\/pytorch.org\/docs\/master\/torchvision\/transforms.html#torchvision.transforms.ToPILImage\" to solve your issue. This though will likely add a ton of time overhead to your data loading pipeline!\nIf you\u2019re not using many transforms from pytorch on your data during \u201cComposing your transform\u201d, why not try using normal tensor operations to go from RGB to Gray? Try the function OpenCV uses like here with link \"https:\/\/docs.opencv.org\/3.1.0\/de\/d25\/imgproc_color_conversions.html\". You can find the reasons for such an averaging here with link \"https:\/\/www.tutorialspoint.com\/dip\/grayscale_to_rgb_conversion.htm\"","z":"You can convert the Tensor to a PIL image, apply that transform, then convert it back to a Tensor.\nWell, like  says, you can try to \u201cCompose Transforms\u201d. An example would be here with link \"https:\/\/github.com\/psyec1\/Lipreading-PyTorch\/blob\/ebfca05ea004fa3cd63980470268a043922432cb\/data\/preprocess.py#L56\". And then use image to PIL transform with link \"https:\/\/pytorch.org\/docs\/master\/torchvision\/transforms.html#torchvision.transforms.ToPILImage\" to solve your issue. This though will likely add a ton of time overhead to your data loading pipeline!\nIf you\u2019re not using many transforms from pytorch on your data during \u201cComposing your transform\u201d, why not try using normal tensor operations to go from RGB to Gray? Try the function OpenCV uses like here with link \"https:\/\/docs.opencv.org\/3.1.0\/de\/d25\/imgproc_color_conversions.html\". You can find the reasons for such an averaging here with link \"https:\/\/www.tutorialspoint.com\/dip\/grayscale_to_rgb_conversion.htm\"\n \nThanks a lot. actually, I want to convert MNIST-M channel 3 to 1.\nEasiest way to make this is using transforms.Compose() like this.\n<code class=\"lang-auto\">    pre_process = transforms.Compose(\n        [transforms.Grayscale(num_output_channels=1), transforms.ToTensor(),\n         transforms.Normalize(mean=[0.5], std=[0.5])])\n<\/code>\n\nBut I have to do this in dataloader for loop statement, so I write like this.\nand  there are some differences between two methods.\n<code class=\"lang-auto\">    pre_process = transforms.Compose(\n        [transforms.ToTensor(),\n         transforms.Normalize(mean=[0.5, 0.5, 0.5], std=[0.5, 0.5, 0.5])])\n\n    for i, (images, labels) in enumerate(loader_mnistm):\n        images, labels = images.to(device), labels.to(device)\n\n        images = images.cpu()\n        images = [torchvision.transforms.ToPILImage()(x) for x in images]\n        images = [torchvision.transforms.Grayscale()(x) for x in images]\n        images = [torchvision.transforms.ToTensor()(x) for x in images]\n        images = torch.stack(images).to(device)\n<\/code>\n\nWhat`s wrong with my code?\nSorry, I am not sure I fully get what you\u2019re trying to do. If you simply want to apply those transforms to all images you can just pass the transforms into the Dataset\u2019s (or any of the child classes) constructor parameter.\nIn my case I pass it like this:\n<code class=\"lang-auto\">dataset_in = datasets.ImageFolder('.\/rooms', transform = transform_in )\n<\/code>\nWhere transform_in is the transform created via Compose.\nFinally I Solve it.\nThanks a lot !\nHi, what was the issue and how did you solve it?"},{"x":"Hi everyone \nI need to create a layer that samples data from a tensor that keeps the computation graph in order to backpropagate properly.\nMore precisely, I have objects represented as point-cloud from the ModelNet dataset and I have to draw some points from the cloud.\nThe layer I came up with is:\n<code class=\"lang-auto\">class Sampler(nn.Module):\n    def __init__(self, N: int):\n        super(Sampler, self).__init__()\n        # Number of points to extract\n        self.N = N\n    \n    def forward(self, x):\n        r''' source: https:\/\/discuss.pytorch.org\/t\/take-random-sample-of-long-tensor-create-new-subset\/36244\/3\n        author: \n        '''\n        rand_columns = torch.randperm(x.shape[1])[:self.N]\n        out = x.clone().detach()[:, rand_columns, :].requires_grad_(True)\n        return out\n<\/code>\nWhere x is a tensor of shape [batch_size, n_points, n_features] and N < n_points\nSo the out tensor is of shape [batch_size, N, n_features]\nEverything seems fine but I keep getting the UserWarning alert about copying tensors:\nUserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor)\neven tho I tried to use the clone-detach on the x tensor. What am I missing here?\nThe source for the forward function is taken from a reply by \n\n\n\n\nTake random sample of long tensor, create new subset with link \"https:\/\/discuss.pytorch.org\/t\/take-random-sample-of-long-tensor-create-new-subset\/36244\/3\"\n\n\n[outputs_hw1] with link \"https:\/\/discuss.pytorch.org\/uploads\/default\/original\/2X\/9\/9095d6af7e012afda56825501733506a50f49131.png\"\nouputs are from final layer and are of shape 1000x10. outputs 2 are pulled 2nd from last layer and are of shape 1000x1024. I want to randomly sample out of my outputs2 to get 1000x10. \nI tried this, and it looks like tensors wrapped in a tensor\u2026 \n [output_test4] with link \"https:\/\/discuss.pytorch.org\/uploads\/default\/original\/2X\/c\/ca690354a937b28ad8e961f9626ac970d3fed3c4.png\"\n\n\n","y":"Also not sure why you\u2019d get the warning. What you are doing seems to be right. Maybe it\u2019s just a  false positive warning, I dunno. What you could try is\n<code class=\"lang-python\">out = torch.tensor(x.clone().detach()[:, rand_columns, :], requires_grad=True)\n<\/code>\ninstead of\n<code class=\"lang-python\">out = x.clone().detach()[:, rand_columns, :].requires_grad_(True)\n<\/code>","z":"Also not sure why you\u2019d get the warning. What you are doing seems to be right. Maybe it\u2019s just a  false positive warning, I dunno. What you could try is\n<code class=\"lang-python\">out = torch.tensor(x.clone().detach()[:, rand_columns, :], requires_grad=True)\n<\/code>\ninstead of\n<code class=\"lang-python\">out = x.clone().detach()[:, rand_columns, :].requires_grad_(True)\n<\/code>"},{"x":"<code class=\"lang-auto\">model = models.vgg16(pretrained=True).to(device)\noptimizer = optim.Adam(model.parameters(), lr=1e-3)\ncriterion = nn.NLLLoss().to(device) # using NLL since our last output is log\nBATCH_SIZE = 50\nEPOCHS = 1\n\ndef load_data():\n    mean=[0.485, 0.456, 0.406]\n    std=[0.229, 0.224, 0.225]\n\n    data_transforms = {\n        'train': transforms.Compose([\n            transforms.RandomResizedCrop(224),\n            transforms.RandomHorizontalFlip(),\n            transforms.ToTensor(),\n            transforms.Normalize(mean, std)\n        ]),\n        'val': transforms.Compose([\n            transforms.Resize(256),\n            transforms.CenterCrop(224),\n            transforms.ToTensor(),\n            transforms.Normalize(mean, std)\n        ])\n    }\n\n    path = os.path.dirname(__file__)\n    data = {\n        'train': datasets.ImageFolder(root, data_transforms['train']),\n        'val': datasets.ImageFolder(root', data_transforms['val'])\n    }\n    dataloaders = {\n        'train': DataLoader(data['train'], batch_size=BATCH_SIZE, shuffle=True),\n        'val': DataLoader(data['val'], batch_size=BATCH_SIZE, shuffle=True)\n    }\n    return dataloaders\n<\/code>\n<code class=\"lang-auto\">    dataloaders = load_data()\n    inputs, classes = next(iter(dataloaders['train']))\n    train(inputs, classes, dataloaders['train'])\n<\/code>\n<code class=\"lang-auto\">def train(inputs, classes, dataloader):\n    model.train()\n\n    # freeze models weights and unfreeze few layers we want\n    for layer in model.parameters():\n        layer.requires_grad = False\n\n\n    # custom classifier on 6th sequence\n    model.classifier[6] = nn.Sequential(\n        nn.Linear(4096, 256), # in features from prevous out of model.classifier\n        nn.ReLU(),\n        nn.Dropout(0.4),\n        nn.Linear(256, 2),\n        nn.LogSoftmax(dim=1)\n    )\n    # print(model.classifier)\n\n    for epoch in range(EPOCHS):\n        for inputs, classes in tqdm(dataloader):\n            inputs = inputs.to(device)\n            classes = classes.to(device)\n            optimizer.zero_grad()\n            outputs = model(inputs)\n            loss = criterion(outputs, classes)\n            loss.backward()\n            optimizer.step()\n<\/code>\n<code class=\"lang-auto\">Traceback (most recent call last):\n  File \"ml\\train&amp;test.py\", line 114, in <module>\n    train(inputs, classes, dataloaders['train'])\n  File \"ml\\train&amp;test.py\", line 88, in train\n    outputs = model(inputs)\n  File \"C:\\Users\\Vijay\\Anaconda3\\envs\\desert\\lib\\site-packages\\torch\\nn\\modules\\module.py\", line 532, in __call__\n    result = self.forward(*input, **kwargs)\n  File \"C:\\Users\\Vijay\\Anaconda3\\envs\\desert\\lib\\site-packages\\torchvision\\models\\vgg.py\", line 46, in forward\n    x = self.classifier(x)\n  File \"C:\\Users\\Vijay\\Anaconda3\\envs\\desert\\lib\\site-packages\\torch\\nn\\modules\\module.py\", line 532, in __call__\n    result = self.forward(*input, **kwargs)\n  File \"C:\\Users\\Vijay\\Anaconda3\\envs\\desert\\lib\\site-packages\\torch\\nn\\modules\\container.py\", line 100, in forward\n    input = module(input)\n  File \"C:\\Users\\Vijay\\Anaconda3\\envs\\desert\\lib\\site-packages\\torch\\nn\\modules\\module.py\", line 532, in __call__\n    result = self.forward(*input, **kwargs)\n  File \"C:\\Users\\Vijay\\Anaconda3\\envs\\desert\\lib\\site-packages\\torch\\nn\\modules\\container.py\", line 100, in forward\n    input = module(input)\n  File \"C:\\Users\\Vijay\\Anaconda3\\envs\\desert\\lib\\site-packages\\torch\\nn\\modules\\module.py\", line 532, in __call__\n    result = self.forward(*input, **kwargs)\n  File \"C:\\Users\\Vijay\\Anaconda3\\envs\\desert\\lib\\site-packages\\torch\\nn\\modules\\linear.py\", line 87, in forward\n    return F.linear(input, self.weight, self.bias)\n  File \"C:\\Users\\Vijay\\Anaconda3\\envs\\desert\\lib\\site-packages\\torch\\nn\\functional.py\", line 1370, in linear\n    ret = torch.addmm(bias, input, weight.t())\nRuntimeError: Expected object of device type cuda but got device type cpu for argument #1 'self' in call to _th_addmm\n<\/code>\nI don\u2019t know why in the train function with the loop over inputs and classes are not going to gpu.","y":"I think the error is raised, since model.classifier[0] might not have been pushed to the device.\nWhile you are using model = models.vgg().to(device) correctly, you are manipulating the classifier later without calling .to() again on this part of the model, so it should stay on the CPU.","z":"I think the error is raised, since model.classifier[0] might not have been pushed to the device.\nWhile you are using model = models.vgg().to(device) correctly, you are manipulating the classifier later without calling .to() again on this part of the model, so it should stay on the CPU.\n<code class=\"lang-auto\">    model.classifier[6] = nn.Sequential(\n        nn.Linear(4096, 256), # in features from prevous out of model.classifier\n        nn.ReLU(),\n        nn.Dropout(0.4),\n        nn.Linear(256, 2),\n        nn.LogSoftmax(dim=1)\n    ).to(device)\n<\/code>\nMissed that .to(device) on the end. Thank you very much, ptrblck."},{"x":"Hello guys. I am very interested in learning PyTorch.so I am trying to understand some basic code in PyTorch.\nhere are some of the doubts I got while going through the code\n\nI found a piece of code like this in training function\n\n<code class=\"lang-auto\">for batch_idx, (data, target) in enumerate(pbar):\n    # get samples\n    data, target = data.to(device), target.to(device)\n<\/code>\nHere my doubt is what does \"data.to(device), target.to(device ) \"do in PyTorch.\n2)I also found line of code\noptimizer.zero_grad()\nwhat does this piece of code do in PyTorch? One of the comments says that it will make gradients zero.\nso what does the advantage if we make gradients to zero.??","y":"data.to(device) moves the data to cpu or GPU based on what device is. This is required for faster computations.\nIn PyTorch, the gradients are accumulated using loss.backward() and then the gradients are applied using optimizer.step(). The stale gradients from the previous back propagation need to be cleared before running the optimizer.step() again. This is achived through optimzer.zero_grad().","z":"data.to(device) moves the data to cpu or GPU based on what device is. This is required for faster computations.\nIn PyTorch, the gradients are accumulated using loss.backward() and then the gradients are applied using optimizer.step(). The stale gradients from the previous back propagation need to be cleared before running the optimizer.step() again. This is achived through optimzer.zero_grad().\n.to(x) as states in Docs with link \"https:\/\/pytorch.org\/docs\/stable\/tensors.html#torch.Tensor.to\" moves the tensor from current device to the device x, i.e. cpu (\"cpu\") or gpu (\"cuda:0\", \"cuda:1\" \u2026). It\u2019s useful, because you can specify x in the beginning of code and from there do not care whether it is cpu or gpu, and just move all your tensors, models etc. to it. The alternative is to call .cpu() or cuda(), but it lacks the flexibility of .to(). There is awesome explanation of this in the official PyTorch Docs here with link \"https:\/\/pytorch.org\/docs\/stable\/notes\/cuda.html\".\nabout .zero_grad() you can find extensive discussion here with link \"https:\/\/discuss.pytorch.org\/t\/why-do-we-need-to-set-the-gradients-manually-to-zero-in-pytorch\/4903\"."},{"x":"I am using the Weighted random sampler function of PyTorch to sample my classes equally, But while checking the samples of each class in a batch, it seems to sample randomly.\nhere is a snippet of my code.\ndef cal_sample_weight(files):\nprint(\"file length \",len(files))\nlabels = [int(f[-5])-1 for f in files]\nclass_count = [labels.count\u00a9 for c in np.unique(labels)]\nprint(\"class count is \",class_count)\n# weights_class = [(1.0\/len(np.unique(labels)))\/(cc) for cc in class_count]\nweights_class = [1.0\/(cc) for cc in class_count]\nprint(\"Weight class is \", weights_class)\nweight_array = np.array([weights_class[l] for l in labels])\nprint(weight_array.shape)\nreturn torch.from_numpy(weight_array)\nnp.random.seed(0)\nrandom.shuffle(Train_files)\nrandom.shuffle(Val_files)\nprint([(int(image_path[-5])-1) for image_path in Train_files[0:32]])\nTrain_labels = [(int(image_path[-5])-1) for image_path in Train_files]\nVal_labels = [(int(image_path[-5])-1) for image_path in Val_files]\n<h1><\/h1>\nTrain_sample_weight = cal_sample_weight(Train_files)\nVal_sample_weight = cal_sample_weight(Val_files)\ntraining_set = Dataset(list_IDs=Train_files,labels=Train_labels, sample_weight=Train_sample_weight, shuffle=True, augment=True)\nprint(\"length of train is \", len(training_set))\nsampler_train = torch.utils.data.sampler.WeightedRandomSampler(weights = Train_sample_weight, num_samples=len(training_set))\ntrainloader = DataLoader(torch.utils.data.Subset(training_set,Train_labels), batch_size=batch_size, sampler=sampler_train,shuffle=False, num_workers=24, pin_memory=True)","y":"Hey, I figured this out. Seems that even though I am zipping my weights with the samples while shuffling at the epoch end, the weights with the sampler don\u2019t get shuffled. The weighted random sampler just allocated the probabilities to all the samples once and has no effect of my custom shuffling.\nMy accuracies are still 1-2% off as compared to my Keras code and I think the reason is \u2018no-shuffle\u2019 after each epoch. If I set shuffle=True in the data loader, it throws the error that sampler and shuffle are mutually exclusive options.\nIs there a way to shuffle properly with a weighted random sampler?","z":"I can\u2019t find any obvious errors in the code.\nCould you post the current class distribution you are seeing in each batch?\nI assume Train_labels contains the labels for the complete training_set?\nIf so you don\u2019t need to wrap it into a Subset.\nPS: You can post code snippets by wrapping them into three backticks ```, which would make debugging easier. \n\nThis is the distribution most of the times. Distribution is pretty random. If I remove the subset part and the labels part, then none of the class has samples as 0 in the batch, but the sampling contains majority from class 2,3 and minority from 1,4 which should not be the case.\nIts sampling is random. When I run the code, sometimes it samples really badly, sometimes its better. Why is this so?\nYes train labels contains labels for complete set.\n<code class=\"lang-auto\">class Dataset(data.Dataset):\n\tdef __init__(self, list_IDs, labels, sample_weight, augment=True, shuffle=True):\n\t\t#'Initialization'\n\t\tself.list_IDs = list_IDs\n\t\tself.labels=labels\n\t\tself.augment = augment\n\t\tself.shuffle = shuffle\n\t\tself.sample_weight = sample_weight\n\t\t# self.on_epoch_end()\n# \n\tdef __getitem__(self, index):\n\t\tnp.random.seed(index)\n\t\t# image_path = np.random.choice(a=self.list_IDs)\n\t\timage_path = self.list_IDs[index]\n\t\tmy_path=image_path.replace(\"\/home\/ken.chang.io\/mnt\",\"\/data\" )\n\t\timage = np.load(my_path)\n\t\tif self.augment:\n\t\t\timage = self.__preprocess_input(image)\n\t\tlabel = (int(image_path[-5])-1)\n\t\timage = TF.to_tensor(image).float()\n\t\tlabel = torch.from_numpy(np.array(label))\n\t\treturn image, label\n\n\n\tdef __len__(self):\n\t\treturn int(len(self.list_IDs))\n\n\tdef on_epoch_end(self):\n\t\tprint(\"hello_epoch\")\n\t\tif self.shuffle == True:\n\t\t\tc = list(zip(self.list_IDs,self.sample_weight, self.labels))\n\t\t\trandom.shuffle(c)\n\t\t\tself.list_IDs, self.sample_weight, self.labels = zip(*c)\n\n\tdef __preprocess_input(self, image):\n\t\tif np.random.rand(1)[0]>.5:\n\t\t\timage = np.fliplr(image)\n\t\tif np.random.rand(1)[0]>.5:\n\t\t\timage = np.flipud(image)\n\t\timage = rotate(image, angle=np.random.rand(1)[0]*45, mode='nearest', reshape=False)\n\t\treturn image\n\n#draw from each sample with equal class probability\ndef cal_sample_weight(files):\n\tprint(\"file length \",len(files))\n\tlabels = [int(f[-5])-1 for f in files]\n\tclass_count = [labels.count(c) for c in np.unique(labels)]\n\tprint(\"class count is \",class_count)\n\tweights_class = [((1.0\/len(np.unique(labels)))\/(cc)) for cc in class_count]\n\t# weights_class = [1.0\/(cc) for cc in class_count]\n\tprint(\"Weight class is \", weights_class)\n\tweight_array = np.array([weights_class[l] for l in labels])\n\treturn torch.from_numpy(weight_array)\n\t# return weight_array\n\n\nos.chdir('\/data\/2015P002510\/MammoDensity\/scripts_preprocessing\/splits')\nwith open(\"Train_images_DMIST2.txt\", \"rb\") as fp:\n\tTrain_files1 = pickle.load(fp)\nwith open(\"Train_images_DMIST3.txt\", \"rb\") as fp:\n\tTrain_files2 = pickle.load(fp)\nwith open(\"Train_images_DMIST4.txt\", \"rb\") as fp:\n\tTrain_files3 = pickle.load(fp)\nwith open(\"Train_images_MGH.txt\", \"rb\") as fp:\n\tTrain_files4 = pickle.load(fp)\nTrain_files = Train_files1+Train_files2+Train_files3+Train_files4\n#\nwith open(\"Val_images_DMIST2.txt\", \"rb\") as fp:\n\tVal_files1 = pickle.load(fp)\nwith open(\"Val_images_DMIST3.txt\", \"rb\") as fp:\n\tVal_files2 = pickle.load(fp)\nwith open(\"Val_images_DMIST4.txt\", \"rb\") as fp:\n\tVal_files3 = pickle.load(fp)\nwith open(\"Val_images_MGH.txt\", \"rb\") as fp:\n\tVal_files4 = pickle.load(fp)\nVal_files = Val_files1+Val_files2+Val_files3+Val_files4\n\nprint([(int(image_path[-5])-1) for image_path in Train_files[0:32]])\n\nrandom.shuffle(Train_files)\nrandom.shuffle(Val_files)\n\n\nprint([(int(image_path[-5])-1) for image_path in Train_files[0:32]])\nTrain_labels = [(int(image_path[-5])-1) for image_path in Train_files]\nVal_labels = [(int(image_path[-5])-1) for image_path in Val_files]\n\n#\nTrain_sample_weight = cal_sample_weight(Train_files)\nprint(Train_sample_weight[0:32])\nVal_sample_weight = cal_sample_weight(Val_files)\n\n\n# LOADING DATASET \ntraining_set = Dataset(list_IDs=Train_files,labels=Train_labels, sample_weight=Train_sample_weight, shuffle=True, augment=True)\nprint(\"length of train is \", len(training_set))\n\nsampler_train = torch.utils.data.sampler.WeightedRandomSampler(weights = Train_sample_weight, num_samples=len(training_set))\ntrainloader = DataLoader(torch.utils.data.Subset(training_set,Train_labels), batch_size=batch_size, sampler=sampler_train,num_workers=24, pin_memory=True)\n# trainloader = DataLoader(dataset=training_set, batch_size=batch_size, sampler=sampler_train,shuffle=False, num_workers=24, pin_memory=True)\n\n\ntest_set = Dataset(Val_files,labels=Val_labels, sample_weight=Val_sample_weight, augment=False)\nprint(\"lenght of val is \", len(test_set))\nsampler_test = torch.utils.data.sampler.WeightedRandomSampler(weights = Val_sample_weight, num_samples=len(test_set))\ntestloader = DataLoader(dataset=test_set, batch_size=batch_size, sampler=sampler_test, num_workers=24, pin_memory=True)<\/code>\nCould you provide some information about your current data distribution, i.e. how many instances do you have from each class?\nHey, I figured this out. Seems that even though I am zipping my weights with the samples while shuffling at the epoch end, the weights with the sampler don\u2019t get shuffled. The weighted random sampler just allocated the probabilities to all the samples once and has no effect of my custom shuffling.\nMy accuracies are still 1-2% off as compared to my Keras code and I think the reason is \u2018no-shuffle\u2019 after each epoch. If I set shuffle=True in the data loader, it throws the error that sampler and shuffle are mutually exclusive options.\nIs there a way to shuffle properly with a weighted random sampler?\nThe weights are used to sample the sample, which already samples \u201crandomly\u201d using the weights as probabilities.\nYou cannot shuffle the dataset again.\nSorry, what is the meaning of \u201cRandomly\u201d here. Also, does the random seed being used in the WeightedRandomSampler to choose the batch getting changed after each epoch? since otherwise it will be choosing same batch again and again\nRandom sampling is done using some kind of probability distribution.\nIf you are shuffling the data, it could be seen as sampling from a uniform distribution without replacement.\nSince you are providing the weights to sample each data sample, how should shuffling work in this case?\nThanks for your response.\nI meant that since each sample has a weight, what if we can zipping the sample and the weight, shuffle them randomly, further unzip them and now use the sampler to draw the samples\nThat wouldn\u2019t change anything, as the weight is still assigned to the same sample.\nInside the WeightedRandomSampler this line of code with link \"https:\/\/github.com\/pytorch\/pytorch\/blob\/0bde610c14b92d351b968a0228df29e92442b1cc\/torch\/utils\/data\/sampler.py#L160\" will return the samples.\nIf the order of the weights and samples is shuffled it won\u2019t make a difference, as torch.multinomial will still use the same weight for the corresponding sample.\nOkay. Thanks a lot for your response."},{"x":"hello everyone.\ni just keep getting this error whenever i try to train my model\n\nRuntimeError: Input type (torch.cuda.DoubleTensor) and weight type (torch.cuda.FloatTensor) should be the same\n\nyou can find my code here\n\n\ngithub.com with link \"https:\/\/github.com\/HemaZ\/MNIST\/blob\/master\/pytorch_mnist.ipynb\"\n\n\nHemaZ\/MNIST\/blob\/master\/pytorch_mnist.ipynb with link \"https:\/\/github.com\/HemaZ\/MNIST\/blob\/master\/pytorch_mnist.ipynb\"\n<code class=\"lang-ipynb\">{\n \"cells\": [\n  {\n   \"cell_type\": \"code\",\n   \"execution_count\": 1,\n   \"metadata\": {},\n   \"outputs\": [],\n   \"source\": [\n    \"from __future__ import print_function\\n\",\n    \"import torch\\n\",\n    \"import torch.optim as optim\\n\",\n    \"import torch.nn as nn\\n\",\n    \"import torch.nn.functional as F\\n\",\n    \"import argparse\\n\",\n    \"from torchvision import datasets,transforms\\n\",\n    \"from torch.utils.data import Dataset, DataLoader\\n\",\n    \"import pandas as pd\\n\",\n    \"import matplotlib.pyplot as plt\"\n   ]\n  },\n<\/code>\n\n  This file has been truncated. show original with link \"https:\/\/github.com\/HemaZ\/MNIST\/blob\/master\/pytorch_mnist.ipynb\"\n\n\n\n\n\nthank you","y":"cast your input to float type by doing\ninput=input.float() before calling the forward pass\n(I\u2019m not sure if u should call this method after or before allocating tensor to cuda, before seems better.)","z":"Try moving the data to torch.float, i.e data.to(device=device, dtype=torch.float).\ncast your input to float type by doing\ninput=input.float() before calling the forward pass\n(I\u2019m not sure if u should call this method after or before allocating tensor to cuda, before seems better.)\nafter i tried this i got another error\n\nRuntimeError: 1only batches of spatial targets supported (non-empty 3D tensors) but got targets of size: : [64]\n\nit seems you have to input 3d tensors, like x[i,j,k] but you are inputing a tensor of dim 64 ??\nyes that was a mistake I swapped (output, target) in the loss function.\nthank you\ni meet the same error \uff0cif some somewhere not use cuda"},{"x":"I have built a network and it seems that my first fully connected layer has not been set the correct input size, my problem is I don\u2019t know how to properly set it. I keep getting the error:\n\n in inner(_it, _timer)\n\/usr\/local\/lib\/python3.6\/dist-packages\/torch\/nn\/functional.py in linear(input, weight, bias)\n1368     if input.dim() == 2 and bias is not None:\n1369         # fused op is marginally faster\n-> 1370         ret = torch.addmm(bias, input, weight.t())\n1371     else:\n1372         output = input.matmul(weight.t())\nRuntimeError: size mismatch, m1: [5 x 196608], m2: [26912 x 512] at \/pytorch\/aten\/src\/TH\/generic\/THTensorMath.cpp:136\n\nThis is the code for my network I would appreciate any help I can get\n<code class=\"lang-auto\"># Convolutional neural network\nclass ConvNet(nn.Module):\n    \n    def __init__(self, num_classes=10):\n        super(ConvNet, self).__init__()\n\n  \n        # Add network layers here\n        # Layer 1\n        self.conv1 = nn.Conv2d(3,16, (3,3))\n        self.pool = nn.MaxPool2d(2,2)\n        self.relu = nn.ReLU()\n        self.dropout = nn.Dropout(p=0.3)\n\n\n            # Layer 2\n        self.conv2 = nn.Conv2d(16,24, (4,4))\n        self.pool = nn.MaxPool2d(2,2)\n        self.relu = nn.ReLU()\n        self.dropout = nn.Dropout(p=0.3)\n\n        conv = self.conv2\n        print(conv)\n\n            # Layer 3\n        self.conv3 = nn.Conv2d(24,32, (4,4))\n        self.pool = nn.MaxPool2d(2,2)\n        self.relu = nn.ReLU()\n        self.dropout = nn.Dropout(p=0.3)\n\n            # Layer 4 - Fully connected\n        self.fc1 = nn.Linear(32 * 29 * 29, 512)\n        \n\n        self.fc2 = nn.Linear(512, num_classes)\n        self.final = nn.Softmax(dim=1)\n        \n    def forward(self, x):\n       out = x.reshape(x.size(0), -1) # TODO what does this do? Why do we need it?\n       out = self.fc1(out)\n\n        # Complete the graph\n\n       return out\n       \n\nnet = ConvNet()\n<\/code>\n\nloss_function = nn.CrossEntropyLoss()\noptimizer = optim.SGD(net.parameters(), lr=0.0001,momentum=0.9)\nprint(len(test_loader))\ndef train_model_epochs(num_epochs):\nfor epoch in range(10):\nrunning_loss = 0.0\n\nfor i, data in enumerate(test_loader, 0):\nimages, labels = data\noptimizer.zero_grad()\n\noutputs = net(images)\n\nloss = loss_function(outputs, labels)\n\nloss.backward()\n\noptimizer.step()\n\nrunning_loss =+ loss.item()\nif i%1000 == 999:\n  print('Epoch \/ Batch [%d \/ %d] - Loss: %.3f' %\n      (epoch + 1, i + 1, running_loss \/ 1000))\n  running_loss = 0.0    \n\nprint(\u201cTraining finished\u201d)\ncpu_train_time = timeit.timeit(\n\u201ctrain_model_epochs(num_epochs)\u201d,\nsetup=\u201cnum_epochs=10\u201d,\nnumber=1,\nglobals=globals(),\n)\nprint(\"Training time: \" + cpu_train_time)\n","y":"Your forward method uses x as the variable name, while you are passing out to the first layer.","z":"In your forward method you are directly flattening your input x and passing it to the linear layer skipping all conv layers.\nI assume you would like to pass the input first to self.conv1, self.pool etc.\nYou are also redefining the modules inside the __init__ method, so use different names for the layers.\nThis tutorial with link \"https:\/\/pytorch.org\/tutorials\/beginner\/examples_nn\/two_layer_net_module.html\" might be helpful.\nAhh yes that makes sense!\n\nI assume you would like to pass the input first to  self.conv1 ,  self.pool  etc.\n\nFor the above, I am unsure if my method is accurate in terms of syntax:\n\ndef forward(self, x):\nout = x.reshape(x.size(0), -1)\nout = self.conv1(out)\nout = self.pool1(out)\nout = self.relu1(out)\n   out = self.conv2(out)\n   out = self.pool2(out)\n   out = self.relu2(out)\n\n   out = self.conv3(out)\n   out = self.pool3(out)\n   out = self.relu3(out)\n\n   out = self.fc1(out)\n\n\nWith this new forward method I get the error:\n\nExpected 4-dimensional input for 4-dimensional weight 16 3 3 3, but got 2-dimensional input of size [5, 196608] instead\n\n\ndef forward(self, x):\n   out = self.conv1(out)\n   out = self.conv2(out)\n   out = self.conv3(out)\n   \n   out = x.reshape(x.size(0), -1)\n\n   out = self.fc1(out)\n\n\nI changed the model to use sequential layers however i am getting the error:\n\nlocal variable \u2018out\u2019 referenced before assignment\n\nSorry to bother you I am at a loss\nYour forward method uses x as the variable name, while you are passing out to the first layer."},{"x":"Hello, I\u2019m relatively new to PyTorch, I want to to apply Instance Normalization to my images. I am wondering if I can use Normalize as a stand alone function, without needing a tranforms.Compose() or any of the sort. Here is how I am currently trying to implement it:\n<code class=\"lang-auto\">img = someTensor # dimensions [1x224x224] I am using grayscale images\nmean, std = img.mean(), img.std()\nnormal  = Normalize(mean,std)\nnormal(tensor = img)\n<\/code>\nI am getting the following error:\nIndexError: too many indices for tensor of dimension 0\nFor now, I will apply element-wise normalisation (i.e. x-mu\/sigma) but I would like to make it work with the torch functions too!\nI would really appreciate if anyone could help me clarify how all this works!\nThanks in advance!","y":"You should unsqueeze the statistics, as you are currently passing scalar values:\n<code class=\"lang-python\">normal = transforms.Normalize(mean.unsqueeze(0), std.unsqueeze(0))\n<\/code>\nThis should avoid the error, which is caused by mean[:, None, None] inside Normalize.","z":"You should unsqueeze the statistics, as you are currently passing scalar values:\n<code class=\"lang-python\">normal = transforms.Normalize(mean.unsqueeze(0), std.unsqueeze(0))\n<\/code>\nThis should avoid the error, which is caused by mean[:, None, None] inside Normalize.\nThat makes sense, thank you so much for the speedy reply!"},{"x":"If I run\n<code class=\"lang-python\">model = models.googlenet(pretrained=False)\nparams_count = sum(p.numel() for p in model.parameters() if p.requires_grad)\nprint(\"GoogLeNet number of trainable parameters: {}\".format(params_count))\n<\/code>\nI get\n\nGoogLeNet number of trainable parameters: 13004888\n\nBut, from the original paper, the number of parameters should be 6.7977 millions.\nWhat I\u2019m missing? is TorchVision implementing a different version of GoogLeNet respect to the original paper?\nThanks,\nMario","y":"When you skip the aux layers, you should get the ~6 million parameters.\nIt seems Table 1 with link \"https:\/\/arxiv.org\/pdf\/1409.4842.pdf\" of the original paper does the same.","z":"When you skip the aux layers, you should get the ~6 million parameters.\nIt seems Table 1 with link \"https:\/\/arxiv.org\/pdf\/1409.4842.pdf\" of the original paper does the same.\nThanks for the explanation ptrblck !"},{"x":"Hi guys,\nI am studying about semantic segmentation these days.\nAnd I need to transform a grey image into a matrix with the value of trainId, as\n\nSo, I want to use PyTorch to realize the function of looking up table in a most efficient way.\nAnd your suggestion and idea will be appreciated!","y":"Hi,\nI don\u2019t think any pytorch-specific feature is needed here. You can use regular python dictionaries to do this ","z":"Hi,\nI don\u2019t think any pytorch-specific feature is needed here. You can use regular python dictionaries to do this \nThanks for your kind response.\nAnd I just wanted to make sure whether there is a PyTorch function can help me realize a looking up table function."},{"x":"Hi,\nSo I have been working on fruits classification problem whose dataset is available on kaggle. It contains 120 classes of different fruits type and has image size of 100x100. I am doing transfer learning by fine-tuning with link \"https:\/\/pytorch.org\/tutorials\/beginner\/finetuning_torchvision_models_tutorial.html\" the ResNet-50 model. I split the dataset into train, validation and test sets and after 10 epochs, it has 99.7% validation accuracy and is doing an amazing job of classifying test images as you can see here:Screen Shot 2020-02-24 at 7.47.49 AM2578\u00d7634 374 KB with link \"https:\/\/discuss.pytorch.org\/uploads\/default\/original\/3X\/c\/6\/c6d78caa66538761da01c567b76cf0e09c252cf2.jpeg\"\nBut when I try testing it on a few images from google, it is just predicting one class. Even when I try to use a random image from test set, it just outputs the same class. My code is here:\n<code class=\"lang-auto\">loader = transforms.Compose([transforms.Resize(224),\n                            transforms.ToTensor(),\n                            transforms.Normalize((0.5, 0.5, 0.5), \n                            (0.5, 0.5, 0.5))])\n# Since the model inputs images of size 100x100\n# need to resize them before feeding them to `loader`\nh = 100\nw = 100\nimg_path = test_dir + '\/Dates\/80_100.jpg'\n# img_path = 'avocado.jpg'\nimg = Image.open(img_path)\n\nimg = img.resize((h, w))\nimg = loader(img)\nimg = torch.unsqueeze(img, 0)\nprint(img.shape)\nmodel.eval()\nresult = model(img)\nprint(result.shape)\n _, preds_tensor = torch.max(result, 1)\npreds = np.squeeze(preds_tensor.numpy()) \nprint(class_names[preds])\n<\/code>\nI even tried the solution discussed here with link \"https:\/\/discuss.pytorch.org\/t\/how-to-read-just-one-pic\/17434\" but still getting the same result as above. Any help will be appreciated. Thank you in advance.\nLakshya","y":"Nevermind, I fixed it. It was missing model.eval(). It is correctly recognizing standalone test images and training images. But still making errors on the images downloaded from google. Can this be possible since the images from google belong to different distribution?","z":"Nevermind, I fixed it. It was missing model.eval(). It is correctly recognizing standalone test images and training images. But still making errors on the images downloaded from google. Can this be possible since the images from google belong to different distribution?\nHi,\nYes, you will need the model.eval() if you have batchnorm-like models.\nThis is very possible that the image from google are very different?"},{"x":"Is it possible to visualize the activations of each layer? I am using the ImageNet dataset and would like to see the features.","y":"You could use forward hooks as described here with link \"https:\/\/discuss.pytorch.org\/t\/visualize-feature-map\/29597\/2\".","z":"You could use forward hooks as described here with link \"https:\/\/discuss.pytorch.org\/t\/visualize-feature-map\/29597\/2\"."},{"x":"I used random_split() to divide my data into train and test and I observed that if random split is done after the dataloader is created, batch size is missing when getting a batch of data from the dataloader.\n<code class=\"lang-auto\">import torch\nfrom torchvision import transforms, datasets\nfrom torch.utils.data import random_split\n\n# Normalize the data\ntransform_image = transforms.Compose([transforms.Resize((240, 320)),\n                                    transforms.ToTensor(),\n                                transforms.Normalize((0.5, 0.5, 0.5), (0.5, 0.5, 0.5))])\n\ndata = '\/data\/imgs\/train'\n\ndef load_dataset():\n    data_path = data\n    main_dataset = datasets.ImageFolder(\n        root = data_path,\n        transform = transform_image\n    )\n\n    loader = torch.utils.data.DataLoader(\n        dataset = main_dataset,\n        batch_size= 64,\n        num_workers = 0,\n        shuffle= True\n    )\n\n    # Dataset has 22424 data points\n    trainloader, testloader = random_split(loader.dataset, [21000, 1424])\n\n    return trainloader, testloader\n\ntrainloader, testloader = load_dataset()\n<\/code>\nNow to get a single batch of images from the train and test loaders:\n<code class=\"lang-auto\">images, labels = next(iter(trainloader))\nimages.shape\n# %%\nlen(trainloader)\n\n# %%\nimages_test, labels_test = next(iter(testloader))\nimages_test.shape\n\n# %%\nlen(testloader)\n<\/code>\nThe output that I get is does not have the batch size for train or test batches. Teh output dims should be [batch x channel x H x W] but I get [channel x H x W].\nOutput:\nCapture961\u00d7888 14.4 KB with link \"https:\/\/discuss.pytorch.org\/uploads\/default\/original\/3X\/b\/6\/b621bdf7e831a35997ea6fc2b96f2c694fd90750.png\"\nBut if I create the split from the dataset and then make two data loaders using the splits, I get the batchsize in the output.\n<code class=\"lang-auto\">def load_dataset():\n    data_path = data\n    main_dataset = datasets.ImageFolder(\n        root = data_path,\n        transform = transform_image\n    )\n    # Dataset has 22424 data points\n    train_data, test_data = random_split(main_dataset, [21000, 1424])\n\n    trainloader = torch.utils.data.DataLoader(\n        dataset = train_data,\n        batch_size= 64,\n        num_workers = 0,\n        shuffle= True\n    )\n\n    testloader = torch.utils.data.DataLoader(\n        dataset = test_data,\n        batch_size= 64,\n        num_workers= 0,\n        shuffle= True\n    )\n\n    return trainloader, testloader\n\n\ntrainloader, testloader = load_dataset()\n<\/code>\nOn running the same 4 commands to get a single train and test batch:\n<code class=\"lang-auto\">images, labels = next(iter(trainloader))\nimages.shape\n# %%\nlen(trainloader)\n\n# %%\nimages_test, labels_test = next(iter(testloader))\nimages_test.shape\n\n# %%\nlen(testloader)\n<\/code>\nOutput:\n<code class=\"lang-auto\">>>> images, labels = next(iter(trainloader))\n>>> images.shape\ntorch.Size([64, 3, 240, 320])\n\n>>> len(trainloader)\n329\n\n>>> images_test, labels_test = next(iter(testloader))\n>>> images_test.shape\ntorch.Size([64, 3, 240, 320])\n\nlen(testloader)\n>> 23\n\n\n<\/code>\nIs the first approach wrong? Although the length shows that the data has been split. So why do I not see the batch size?","y":"Yes, because indexing a Dataset will return the sample without a batch dimension. The DataLoader creates the batch. Since random_split returns a Dataset, you won\u2019t have a batch dimension using this approach.","z":"random_split splits Datasets and returns Datasets, so trainloader and testloader should be of type Dataset not DataLoader.\nThe recommended way is your second approach: split the Datasets first, then wrap them in a DataLoader.\n: Thanks for the explanation,  I amusing the second approach. But do you have any idea why the batch size would be missing in the first approach ?\nYes, because indexing a Dataset will return the sample without a batch dimension. The DataLoader creates the batch. Since random_split returns a Dataset, you won\u2019t have a batch dimension using this approach.\nHi Ptrblck,\nI used thei command but dataloader works in correct, I think I should do sth before passing the index from split data. TrainData1 and  ValidationData1 are index.\n<code class=\"lang-auto\">    [TrainData1, ValidationData1]= train_test_split( np.arange(len(wholetargetArray)),train_size=0.7,test_size=0.3,stratify=wholetargetArray)\n    trainloader=torch.utils.data.DataLoader(TrainData1, batch_size=64,shuffle=True,drop_last=True, num_workers=0)\n    validationloader=torch.utils.data.DataLoader(ValidationData1, batch_size=6, drop_last=True,num_workers=0)```\n\nwhat is your suggestion?\n\nI appreciate your help<\/code>\nTrainData1 and ValidationData1 should contain the split indices, so you could pass these indices to a torch.utils.data.Subset."},{"x":"brainMRI1611\u00d7668 242 KB with link \"https:\/\/discuss.pytorch.org\/uploads\/default\/original\/3X\/3\/f\/3fa498b304c008cd3f14f3c4b7dbb5a5c0e64a00.png\"\nAs you can see in this picture above there are lots of image files in that directory.\nbrainMRI21245\u00d7557 235 KB with link \"https:\/\/discuss.pytorch.org\/uploads\/default\/original\/3X\/2\/c\/2cf70bae46bda84b98314b2839d00ab43dab0c42.png\"\nI don\u2019t know why the number of datapoints is 2\ndoesn\u2019t it mean the imagefolder reads 2 imagefiles?\nI have no idea why ImageFolder doesn\u2019t reads the whole imagefiles I have in that directory.\n\nedited\njust found another weird thing.\nbrainmri3865\u00d7498 168 KB with link \"https:\/\/discuss.pytorch.org\/uploads\/default\/original\/3X\/6\/d\/6da3f2d3a13b3f7622196a60bf434cac9c9e0863.png\"\nwhy in the mask directory gets 4 image data???\nall the files have same name but mask folder with mask on the back.\njust like this\n\nMRI folder \u2014>  TCGS_CS_4941.tif\nMask folder \u2014> TCGS_CS_4941_mask.tif\n","y":"As per the documentation documentation with link \"https:\/\/pytorch.org\/docs\/stable\/torchvision\/datasets.html#torchvision.datasets.ImageFolder\", ImageFolder expects images to be organized as class\/instance.tif with class subfolders.\nBest regards\nThomas","z":"As per the documentation documentation with link \"https:\/\/pytorch.org\/docs\/stable\/torchvision\/datasets.html#torchvision.datasets.ImageFolder\", ImageFolder expects images to be organized as class\/instance.tif with class subfolders.\nBest regards\nThomas\n\nhmm but I still don\u2019t get it.\nEven if I do what I did,\nI think It needs to get the whole image files with 1 class which is \u2018BrainMRI_train\u2019 in my case.\nNo, it\u2019s not in a subfolder? You would need to create a dummy subfolder of MRI and move the tifs to that.\nBest regards\nThomas\n\nThank you for the replies.\nI created subfolders but now I\u2019m getting only 1 datapoint each\u2026\nbrainmri4916\u00d7498 173 KB with link \"https:\/\/discuss.pytorch.org\/uploads\/default\/original\/3X\/3\/f\/3fb432c34efca8a9989dfe8174cde679e0184b16.png\"\n\nEdited\noh nevermind I think I figured it out\nI didn\u2019t need to change the root directory all the way to subdirectory.\nthank you very much!!\nBest regards\nYoonho"},{"x":"Hi everyone,\nthis is my first post in the forum and I believe you can help me clarify one thing: is it acceptable to have validation log loss lowe (or much lower) than training log loss? And if so, how much this gap should be at most?\nI am using Inception V3 to classify 120 classes of images with 30K training examples which I split into 21K images for training and 9k for testing using Stratified Random Sampling (30% of each class chosen at random for the validation set).\nMy model is pretty much the same as the pre-trained one and the only major change the number of classes which I set to 120.\nHere\u2019s my most recent callback:\n<code class=\"lang-auto\">Epoch 1\/1499\n\u26a0 train Loss: 6.3281 Acc: 0.0770\n\u26a0 val Loss: 4.2735 Acc: 0.0965\n(...)\nEpoch 25\/1499\n\u26a0 train Loss: 4.2652 Acc: 0.3317\n\u26a0 val Loss: 3.4767 Acc: 0.2571\n<\/code>\nI\u2019m aware that if the gap between the two is too big and that training error is much smaller than validation error then I am over-fitting.\nAnd then there\u2019s also this thread: https:\/\/twitter.com\/aureliengeron\/status\/1110839223878184960\nWhere Aur\u00e9lien Geron,author of the book Hands-On MachineLearning with ScikitLearn, Keras and TensorFlow explains that this might be normal\u2026 but I\u2019m not sure how credible this is.\nThis happens regardless of data augmentation technique (even if train\/val has the same transforms using just cropsize and ToTensor()), learning rate or optimiser. More recently I\u2019ve been exploring cyclical learning rate as well\u2026 Changed batch sizes, used different techniques for sampling\u2026 nothing seems to change this behaviour.\nI also based myself mostly on: https:\/\/pytorch.org\/tutorials\/beginner\/finetuning_torchvision_models_tutorial.html\nto build the scaffolding for my code. Difference is the dataset and some data augmentation changes.\nSo\u2026Is this normal or is it likely that my code is badly implemented?\nThanks a lot!","y":"Aurelien\u2019s explanations seem reasonable and it\u2019s what I observe usually.\nIf you want to make sure dropout and the running estimates might be the cause for the gap, you could set your model to model.eval() and use the training dataset after the epoch (same as where you are calculating the validation loss) and compare these two values.","z":"Aurelien\u2019s explanations seem reasonable and it\u2019s what I observe usually.\nIf you want to make sure dropout and the running estimates might be the cause for the gap, you could set your model to model.eval() and use the training dataset after the epoch (same as where you are calculating the validation loss) and compare these two values.\nThanks a lot  - Did as suggested and, indeed, the values have changed. I am now experiment a few other pre-trained models. Cheers!"},{"x":"Hello, I am new in pytorch and currently I trained mnist data in resnet 34 model.  After I extracting my feature the size is [1, 512, 10, 10] which I was expecting [1, 512, 1, 1].  Can someone explain to me what does 10 10 stands for?","y":"Thanks for the code.\nSince you are rewrapping some modules and remove the last linear layer, the output shape will be the last activation after the average pooling layer.\nFor your input shape, it\u2019ll be [batch_size, 512, 10, 10].\nIf you want to get a single pixel in the spatial size, you would have to pass the \u201cstandard\u201d input shape as [batch_size, channels, 224, 224].","z":"These values will most likely correspond to the spatial size of the activation.\nCould you some information where these activation come from, if you are concerned about the shape?\nI am implementing classification model based on extracted features.\n<code class=\"lang-auto\">ResNet(\n  (conv1): Conv2d(1, 64, kernel_size=(7, 7), stride=(2, 2), padding=(3, 3), bias=False)\n  (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n  (relu): ReLU(inplace=True)\n  (maxpool): MaxPool2d(kernel_size=3, stride=2, padding=1, dilation=1, ceil_mode=False)\n  (layer1): Sequential(\n    (0): BasicBlock(\n      (conv1): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n      (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n      (relu): ReLU(inplace=True)\n      (conv2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n      (bn2): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n    )\n    (1): BasicBlock(\n      (conv1): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n      (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n      (relu): ReLU(inplace=True)\n      (conv2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n      (bn2): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n    )\n    (2): BasicBlock(\n      (conv1): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n      (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n      (relu): ReLU(inplace=True)\n      (conv2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n      (bn2): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n    )\n  )\n  (layer2): Sequential(\n    (0): BasicBlock(\n      (conv1): Conv2d(64, 128, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\n      (bn1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n      (relu): ReLU(inplace=True)\n      (conv2): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n      (bn2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n      (downsample): Sequential(\n        (0): Conv2d(64, 128, kernel_size=(1, 1), stride=(2, 2), bias=False)\n        (1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n      )\n    )\n    (1): BasicBlock(\n      (conv1): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n      (bn1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n      (relu): ReLU(inplace=True)\n      (conv2): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n      (bn2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n    )\n    (2): BasicBlock(\n      (conv1): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n      (bn1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n      (relu): ReLU(inplace=True)\n      (conv2): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n      (bn2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n    )\n    (3): BasicBlock(\n      (conv1): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n      (bn1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n      (relu): ReLU(inplace=True)\n      (conv2): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n      (bn2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n    )\n  )\n  (layer3): Sequential(\n    (0): BasicBlock(\n      (conv1): Conv2d(128, 256, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\n      (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n      (relu): ReLU(inplace=True)\n      (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n      (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n      (downsample): Sequential(\n        (0): Conv2d(128, 256, kernel_size=(1, 1), stride=(2, 2), bias=False)\n        (1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n      )\n    )\n    (1): BasicBlock(\n      (conv1): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n      (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n      (relu): ReLU(inplace=True)\n      (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n      (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n    )\n    (2): BasicBlock(\n      (conv1): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n      (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n      (relu): ReLU(inplace=True)\n      (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n      (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n    )\n    (3): BasicBlock(\n      (conv1): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n      (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n      (relu): ReLU(inplace=True)\n      (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n      (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n    )\n    (4): BasicBlock(\n      (conv1): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n      (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n      (relu): ReLU(inplace=True)\n      (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n      (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n    )\n    (5): BasicBlock(\n      (conv1): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n      (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n      (relu): ReLU(inplace=True)\n      (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n      (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n    )\n  )\n  (layer4): Sequential(\n    (0): BasicBlock(\n      (conv1): Conv2d(256, 512, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\n      (bn1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n      (relu): ReLU(inplace=True)\n      (conv2): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n      (bn2): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n      (downsample): Sequential(\n        (0): Conv2d(256, 512, kernel_size=(1, 1), stride=(2, 2), bias=False)\n        (1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n      )\n    )\n    (1): BasicBlock(\n      (conv1): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n      (bn1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n      (relu): ReLU(inplace=True)\n      (conv2): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n      (bn2): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n    )\n    (2): BasicBlock(\n      (conv1): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n      (bn1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n      (relu): ReLU(inplace=True)\n      (conv2): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n      (bn2): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n    )\n  )\n  (avgpool): AvgPool2d(kernel_size=7, stride=1, padding=0)\n  (fc): Linear(in_features=512, out_features=10, bias=True)\n)\n<\/code>\nthis is my resnet model which i am using to extract my features. As my understanding [1, 512, 1, 1] here 512 values representing for the images. Therefore when i got [1, 512, 10, 10] this result i am confused .\nAlso, is resnet not suitable for MNIST data? I got error saying Given input size: (512x1x1). Calculated output size: (512x-5x-5). Output size is too small\nThank you so much\nBased on the model definition, your output should come from the linear layer with an output of 10 units.\nSo apparently you are using a hook or skip some layers in your forward method?\nThe images from the MNIST dataset are too small for a resnet, as the standard spatial shape is 224x224.\nYou could use torchvision.transforms.Resize to increase the spatial size of your samples.\nThank you again for your reply.\n<code class=\"lang-auto\">def forward(self, x):\n        x = self.conv1(x)\n        x = self.bn1(x)\n        x = self.relu(x)\n        x = self.maxpool(x)\n        \n        x = self.layer1(x)\n        x = self.layer2(x)\n        x = self.layer3(x)\n        x = self.layer4(x)\n        \n        x = torch.flatten(x, 1)\n        x = self.fc(x)\n        \n        return x\n<\/code>\nthis is my forward method.\nThanks for the code!\nDoes the output have a shape of [1, 512, 10, 10] for this forward method?\nIf so, could you please post the complete model?\nThank you again for the reply. I still got the same size\ntorch.Size([1, 512, 10, 10])\nHere is my model.\n<code class=\"lang-auto\">def conv3x3(in_planes, out_planes, stride = 1):\n    \"\"\"3x3 convolution with padding\"\"\"\n    return nn.Conv2d(in_planes, out_planes, kernel_size = 3, stride = stride, padding = 1, bias = False )\n\nclass BasicBlock(nn.Module):\n    expansion = 1\n    \n    def __init__(self, inplanes, planes, stride = 1, downsample = None):\n        super(BasicBlock, self).__init__()\n        self.conv1 = conv3x3(inplanes, planes, stride)\n        self.bn1 = nn.BatchNorm2d(planes)\n        self.relu = nn.ReLU(inplace = True)\n        self.conv2 = conv3x3(planes, planes)\n        self.bn2 = nn.BatchNorm2d(planes)\n        self.downsample = downsample\n        self.stride = stride\n        \n    def forward(self, x):\n        residual = x\n        \n        out = self.conv1(x)\n        out = self.bn1(out)\n        out = self.relu(out)\n        \n        out = self.conv2(out)\n        out = self.bn2(out)\n        \n        if self.downsample is not None:\n            residual = self.downsample(x)\n            \n        out += residual\n        out = self.relu(out)\n        return out\n\nclass ResNet(nn.Module):\n    def __init__(self, block, layers, num_classes, grayscale):\n        self.inplanes = 64\n        super (ResNet, self).__init__()\n        self.conv1 = nn.Conv2d(1, 64, kernel_size = 7, stride = 2, padding = 3, bias = False)\n        self.bn1 = nn.BatchNorm2d(64)\n        self.relu = nn.ReLU(inplace = True)\n        self.maxpool =nn.MaxPool2d(kernel_size = 3, stride = 2, padding = 1)\n        self.layer1 = self._make_layer(block, 64,  layers[0])\n        self.layer2 = self._make_layer(block, 128, layers[1], stride = 2)\n        self.layer3 = self._make_layer(block, 256, layers[2], stride = 2)\n        self.layer4 = self._make_layer(block, 512, layers[3], stride = 2)\n        self.avgpool = nn.AvgPool2d(7, stride = 1)\n        self.fc = nn.Linear(512 * block.expansion, num_classes)\n        \n        for m in self.modules():\n            if isinstance(m, nn.Conv2d):\n                n = m.kernel_size[0] * m.kernel_size[1] * m.out_channels\n                m.weight.data.normal_(0, (2. \/ n) **.5)\n            elif isinstance(m, nn.BatchNorm2d):\n                m.weight.data.fill_(1)\n                m.bias.data.zero_()\n\n    def _make_layer(self, block, planes, blocks, stride = 1):\n        downsample = None\n        if stride != 1 or self.inplanes != planes * block.expansion:\n            downsample = nn.Sequential(\n                nn.Conv2d(self.inplanes, planes * block.expansion,\n                         kernel_size = 1, stride = stride, bias = False),\n                nn.BatchNorm2d(planes * block.expansion),\n            )\n        layers = []\n        layers.append(block(self.inplanes, planes, stride, downsample))\n        self.inplanes = planes * block.expansion\n        for i in range(1, blocks):\n            layers.append(block(self.inplanes, planes))\n            \n        return nn.Sequential(*layers)\n    \n    def forward(self, x):\n        x = self.conv1(x)\n        x = self.bn1(x)\n        x = self.relu(x)\n        x = self.maxpool(x)\n        \n        x = self.layer1(x)\n        x = self.layer2(x)\n        x = self.layer3(x)\n        x = self.layer4(x)\n    \n       # x = self.avgpool(x)\n        x = x.view(x.size(0), -1)\n        x = self.fc(x)\n        \n        return x\n    \ndef resnet34(num_classes):\n    model = ResNet(block = BasicBlock, layers = [3, 4, 6, 3],\n                     num_classes = classes, grayscale = 1 )\n    return model\n<\/code>\nAnd here is following\n<code class=\"lang-auto\">#instantiating model\nclasses=10\nmodel = resnet34(classes)\n\n#setting loss function\ncriterion = nn.CrossEntropyLoss()\n\n#setting optimizer\noptimizer = optim.SGD(model.parameters(), lr = 0.001, momentum = 0.9)\n<\/code>\nHere is feature extraction part.\n<code class=\"lang-auto\">model.eval()\n\nprint(model)\n\nnum_ftrs = model.fc.in_features\n\n#extracting features v1 stripping layer before average pool\nfeature_extractor = torch.nn.Sequential(*list(model.children())[:-1])\n\n#testing the feature extractor output\nftr1 = feature_extractor(torch.randn(1,1,512,512))\n\nprint(ftr1.size())\n\n\n<\/code>\nAnd i got the same\n<code class=\"lang-auto\">torch.Size([1, 512, 10, 10])\n<\/code>\nI am very thankful for making time for me.\nThanks for the code.\nSince you are rewrapping some modules and remove the last linear layer, the output shape will be the last activation after the average pooling layer.\nFor your input shape, it\u2019ll be [batch_size, 512, 10, 10].\nIf you want to get a single pixel in the spatial size, you would have to pass the \u201cstandard\u201d input shape as [batch_size, channels, 224, 224].\nThank you for your reply.\nThe reason I got [1, 512, 1, 1] was  i gave my input as\n\nx = torch.randn([1,3,224,224])\noutput1 = feature_extractor(x)\n\nBut my real feature shape is  [4, 512, 1, 1].\nSo my batch size is 4? And one input\u2019s feature is [4:512] ?\nThank you.\nThe batch size should be in dim0, so if you are passing a batch with 4 samples, your output shape would be correct.\nEach feature of this batch has 512 values, yes.\nLet me know, if I misunderstood your question.\nOkay thank you very much."},{"x":"I\u2019m using nn.Unfold to implementing a custom pooling layer, however, I\u2019d like to keep the last value, like what ceil_mode=True for MaxPool would do. It seems that Unfold does not support this, is there a workaround?\nThanks.","y":"You could pad the input to create the desired output shape.","z":"You could pad the input to create the desired output shape."},{"x":"https:\/\/pytorch.org\/tutorials\/beginner\/transfer_learning_tutorial.html\n<code class=\"lang-auto\">def imshow(inp, title=None):\n    \"\"\"Imshow for Tensor.\"\"\"\n    inp = inp.numpy().transpose((1, 2, 0))\n    mean = np.array([0.485, 0.456, 0.406])\n    std = np.array([0.229, 0.224, 0.225])\n    inp = std * inp + mean\n    *inp = np.clip(inp, 0, 1)*\n    plt.imshow(inp)\n    if title is not None:\n        plt.title(title)\n    plt.pause(0.001)  # pause a bit so that plots are updated\n\n\n# Get a batch of training data\ninputs, classes = next(iter(dataloaders['train']))\n\n# Make a grid from batch\nout = torchvision.utils.make_grid(inputs)\n\nimshow(out, title=[class_names[x] for x in classes])\n<\/code>\nWhy the bolded line(inp = np.clip(inp,0,1)) is needed??\nI hae visualized by commenting out that line and it still worked the same way. Then why do i need to clip??","y":"\n\n\n SrJ:\n\nThis function is for visualize only. So, i guess i need not normalize the input .\n\n\nRight.\n\n\n\n SrJ:\n\nBut Why plt.imshow() i s showing a normalized image correctly??\n\n\nplt.imshow supports both 0-1 and 0-255 arrays","z":"np.clip is to make sure the tensor values lie in the range [0, 1]. You need the input images normalized and be in a specific range(either [0, 1] or [0, 255]) which is what np.clip ensures.\nVisualizing such unbounded tensors may not be a problem though, but they need to be in a certain bound in order to train a network.\nThis function is for visualize only. So, i guess i need not normalize the input .\nBut Why plt.imshow() i s showing a normalized image correctly??\n\n\n\n SrJ:\n\nThis function is for visualize only. So, i guess i need not normalize the input .\n\n\nRight.\n\n\n\n SrJ:\n\nBut Why plt.imshow() i s showing a normalized image correctly??\n\n\nplt.imshow supports both 0-1 and 0-255 arrays"},{"x":"I predict a binary segmentation mask using an array with values 1 and 0, multiply this by 255, and try to save the resulting array as an image. However, I\u2019m getting the following error:\n\nTraceback (most recent call last):\nFile \u201ctest.py\u201d, line 71, in \ntorchvision.utils.save_image(predicted, path_ + idx[0])\nFile \u201cC:\\Users\\CCL\\Anaconda3\\lib\\site-packages\\torchvision\\utils.py\u201d, line 107, in save_image\nndarr = grid.mul(255).add_(0.5).clamp_(0, 255).permute(1, 2, 0).to(\u2018cpu\u2019, torch.uint8).numpy()      RuntimeError: result type Float can\u2019t be cast to the desired output type Int\n\nHere is my code:\n<code class=\"lang-auto\">mask_pred = net(img)\npredicted = torch.sigmoid(mask_pred) > 0.5\npredicted = torch.tensor(predicted, dtype=torch.int32)*255\ntorchvision.utils.save_image(predicted, path_ + idx[0])\n<\/code>\nI\u2019m using torch version 1.4.0 and torchvision version 0.5.0. The above code works on torch v1.1.0 and torchvision v0.3.0.","y":"Internally the output array will be created via:\n<code class=\"lang-python\">ndarr = grid.mul(255).add_(0.5).clamp_(0, 255).permute(1, 2, 0).to('cpu', torch.uint8).numpy()\n<\/code>\nso you don\u2019t need to multiply your predictions with 255 and can pass it as a float tensor.\nThe error is thrown at the .add_(0.5) step.\nThe documentation doesn\u2019t mention any required ranges, so you might open a GitHub issue and link to this topic.","z":"Internally the output array will be created via:\n<code class=\"lang-python\">ndarr = grid.mul(255).add_(0.5).clamp_(0, 255).permute(1, 2, 0).to('cpu', torch.uint8).numpy()\n<\/code>\nso you don\u2019t need to multiply your predictions with 255 and can pass it as a float tensor.\nThe error is thrown at the .add_(0.5) step.\nThe documentation doesn\u2019t mention any required ranges, so you might open a GitHub issue and link to this topic.\nThank you! Directly passing in mask_pred worked  Will open issue.\nGitHub issue for reference: https:\/\/github.com\/pytorch\/vision\/issues\/1847"},{"x":"Hi everyone,\ncompletely new to DL\/Pytorch here so feel free to treat me like a nube. I have been training an image dataset of 3 category animals (Cats, Dogs and Pandas) on a very simple CNN architecture. The structure goes like below:\n<code class=\"lang-auto\">from torch import nn\n\nclass ShallowNetTorch(nn.Module):\n    def __init__(self, width, height, depth, classes):\n        super(ShallowNetTorch, self).__init__()\n\n        self.width = width\n        self.height = height\n\n        # first and only conv\n        self.conv1 = nn.Conv2d(in_channels=depth, out_channels=32,\n                               kernel_size=3, stride=1, padding=1)\n        # Relu Activation\n        self.activation = nn.ReLU()\n\n        # linear layer (32*32*32 -> classes)\n        self.fc1 = nn.Linear(self.width * self.height * 32, classes)\n\n    def forward(self, x):\n        # add sequence of convolutions\n        x = self.activation(self.conv1(x))\n\n        # flatten the activations\n        x = x.view(-1, self.width * self.height * 32)\n\n        # pass thru last activations into 3 classes\n        x = self.fc1(x)\n\n        return x\n<\/code>\nAfter training the model for 100 epochs, I am plotting the training\/validation losses and accuracies. While my training loss decreases (training accuracy + validation accuracy increases) over the epochs as expected, the validation loss keeps on having an increasing trend over the 100 epochs. Please see the image below:\nimage808\u00d7447 64.2 KB with link \"https:\/\/discuss.pytorch.org\/uploads\/default\/original\/3X\/4\/5\/4544ab673f4937224834dd011e85db425b72be90.png\"\nAs can be seen from the training and validation curves, its quite obvious I am overfitting. The validation loss however, keeps on increasing and I am not sure why is this happening. My expectation was, since the validation accuracy is effectively settling at a value, the validation loss should also be doing that at higher epochs. Am I calculating the validation loss correctly? I am attaching a link of the jupyter notebook here where I trained the ShallowNet model and calculated all the losses and accuracies that have been later plotted in the image above. [LINK with link \"https:\/\/github.com\/nombreinvicto\/DeepLearningCV\/blob\/master\/torch_dir\/Project1_FullyConnectedNN_FashionMNIST\/shallownet_animals_torch.ipynb\"]","y":"This might be the case or the model is classifying the wrong classes with a higher confidence.","z":"The calculation looks alright, even though you could use torch.argmax(logits, 1), since the predicted class will be the same as with the probabilities using softmax. \nOne possible reason for the increasing validation loss would be that the model increases its logits for the wrong classes.\n Thanks for the answer! so to put it simply, can it be said that due to the intense overfitting, the model is now classifying the correctly classified images less better than before (e.g assigning them a lesser softmax probability than before). This while keeping the accuracy relatively constant, increases the loss. Will that be the correct interpretation of what you are saying?\nThis might be the case or the model is classifying the wrong classes with a higher confidence."},{"x":"I learnt of this functionality. For example, we have a VGG16 model:\n<code class=\"lang-auto\">import torchvision.models as models\nmodel=models.vgg16()\nmodel._modules['classifier'][6] = 1\n<\/code>\n<code class=\"lang-auto\">Sequential(\n  (0): Linear(in_features=25088, out_features=4096, bias=True)\n  (1): ReLU(inplace)\n  (2): Dropout(p=0.5)\n  (3): Linear(in_features=4096, out_features=4096, bias=True)\n  (4): ReLU(inplace)\n  (5): Dropout(p=0.5)\n  (6): Linear(in_features=4096, out_features=1, bias=True)\n)\n<\/code>\nIt turns out you can manipulate the number of input and output features, e.g.\nvgg._modules[\u2018classifier\u2019][6].in_features=5\nvgg._modules[\u2018classifier\u2019][6].out_features=1\n<code class=\"lang-auto\">  (classifier): Sequential(\n    (0): Linear(in_features=25088, out_features=4096, bias=True)\n    (1): ReLU(inplace)\n    (2): Dropout(p=0.5)\n    (3): Linear(in_features=4096, out_features=4096, bias=True)\n    (4): ReLU(inplace)\n    (5): Dropout(p=0.5)\n    (6): Linear(in_features=5, out_features=1, bias=True)\n  )\n<\/code>\nI don\u2019t quite understand how this thing works. If I reduce 1000 neurons to 1, which one is that? And what happens to all the weights? I realize with in_features=5 this wouldn\u2019t work, but with out_features=1 this would. But how?","y":"You could reassign a new matix with your desired shape.\nChanging the in_* and out_features will not change the underlying weight parameter.","z":"Your code shouldn\u2019t work, as you are trying to assign an int as a child module.\nAnyway, changing the out_features won\u2019t have any effect after the layer was initialized:\n<code class=\"lang-python\">model = models.resnet50().eval()\nx = torch.randn(1, 3, 224, 224)\n\nout1 = model(x)\n\nmodel.fc.out_features = 1\n\nout2 = model(x)\n\nprint(out1.shape, out2.shape)\n> torch.Size([1, 1000]) torch.Size([1, 1000])\nprint((out1 == out2).all())\n> tensor(True)\n<\/code>\nIt\u2019s a type, I meant of course\nmodel._modules['classifier'][6].out_features = 1\nSo can I change both the number of out_features or in_features and the resize the weights matrix accordingly?\nYou could reassign a new matix with your desired shape.\nChanging the in_* and out_features will not change the underlying weight parameter."},{"x":"Here, I\u2019ve declared a custom multi-task loss in Pytorch with BCEWithLogitsLoss for the (binary) mask segmentation loss, and BCELoss for the classification loss (I use fully-connected layers and then sigmoid). I weight BCEWithLogitsLoss at 0.9 and BCELoss at 0.1, sum them, and back propagate this summed loss. I\u2019m using the Adam optimiser for minimisation. Please note that the y-axis of the plot is wrong (should be multi-task loss instead of just BCEWithLogits).\n\nHowever, as seen from the graph, this loss cannot be minimised well. Why do you think this is? How do I solve this problem? Thanks!","y":"Are you using the sigmoid for both layers?\nNote that nn.BCEWithLogitsLoss does not expect probabilities, but raw logits.\nCould you post the training code, so that we can have a look?","z":"Are you using the sigmoid for both layers?\nNote that nn.BCEWithLogitsLoss does not expect probabilities, but raw logits.\nCould you post the training code, so that we can have a look?\nAh yes you are completely right! I\u2019ll fix that!"},{"x":"I am trying to train a UNet for road segmentation. When I normalize the images using transforms.Normalize((0.5,0.5,0.5),(0.5,0.5,0.5))  the loss decreases but if I normalize the images by dividing them by 255.0 then the loss stops decreasing after a certain point. What is the difference between the two?","y":"It\u2019s not generally true, I think, and might depend on your application.\nYou can find literature about how standardizing whitens the input data and this creates a \u201cround\u201d loss surface, but I\u2019m not sure if these simple abstractions are applicable for deep neural networks.\nHave a look at this post with link \"https:\/\/sebastianraschka.com\/Articles\/2014_about_feature_scaling.html#about-standardization\" written by  for some more information.","z":"transforms.Normalize subtracts the provided mean and divides by the stddev to create standardized tensors (Wikipedia - Standard Score with link \"https:\/\/en.wikipedia.org\/wiki\/Standard_score\"), while the latter approach normalizes the values to the range [0, 1].\nbut why the first one works efficiently, but the loss for the latter stops decreasing after some time ?\nIt\u2019s not generally true, I think, and might depend on your application.\nYou can find literature about how standardizing whitens the input data and this creates a \u201cround\u201d loss surface, but I\u2019m not sure if these simple abstractions are applicable for deep neural networks.\nHave a look at this post with link \"https:\/\/sebastianraschka.com\/Articles\/2014_about_feature_scaling.html#about-standardization\" written by  for some more information.\nIn practice, I often find that dividing by 255 works similarly well as z-score standardization (the Normalize approach with means and standard deviation) for images. However, many gradient-based optimization algorithms benefit from the symmetry of having the data centered at 0 with positive and negative values"},{"x":"Can someone explain how to use spatial_scale in the torchvision.ops.roi_pool( *input* ,  *boxes* ,  *output_size* ,  *spatial_scale=1.0* )) using some example. Let\u2019s say my original image is 800*800 and subsampling ratio is 16, so feature map size is 50*50. Now in the input to torchvision.ops.roi_pool we give feature map and boxes are in image coordinate scale.\nSo  spatial_scale should be equal to subsampling ratio?","y":"spatial_scale = 1\/subsampling_ratio","z":"Hi, did you found any explanation about this parameter ?\nspatial_scale = 1\/subsampling_ratio"},{"x":"Hi,\nI have come across a problem where despite setting random seeds, I obtain different outputs from a simple network depending on whether I use CPU or GPU. I also receive different CPU results using different computers but receive the same GPU results.\nThe model weights are equal as is the randomly generated input \u201ca\u201d, however differences occur after passing \u201ca\u201d through the network. I also get the same results using model.eval() or model.train().\nSo my 2 questions are:\n\nWhy are the results different when using CPU vs GPU even though random seeds have been set?\nProbably linked to the first but why are the CPU results across computers different?\n\nMy results using:\nSystem details: intel i5-7500, GTX 1060 6GB, 32GB RAM, Samsung Evo 850 500GB\nnvidia-smi: 435.21\n<code class=\"lang-auto\">output[0, :]: [[-0.18105512857437134], [-0.9114810228347778], [-0.5673332810401917], [-1.0145820379257202]]\noutput[0, :]: [[-0.18105511367321014], [-0.9114810228347778], [-0.5673332214355469], [-1.0145819187164307]]\n<\/code>\nWhen I run this code on another computer using:\nIntel\u00ae Xeon\u00ae Gold 6134 CPU @ 3.20GHz, GTX 1080 Ti\nnvidia-smi: 430.50\n<code class=\"lang-auto\">output[0, :]: [[-0.18105512857437134], [-0.9114810228347778], [-0.5673332810401917], [-1.0145820379257202]]\noutput[0, :]: [[-0.18105512857437134], [-0.9114810228347778], [-0.5673332214355469], [-1.0145819187164307]]\n\n<\/code>\n<code class=\"lang-auto\">import torch\nimport torch.nn as nn\nimport numpy\nimport random\nimport math\n\n\nclass SimpleModel1(nn.Module):\n    def __init__(self):\n        super(SimpleModel1, self).__init__()\n        self.hidden = nn.Linear(in_features=6,\n                                out_features=1)\n        self.init_weights()\n\n    def init_weights(self):\n        self.init_layer(self.hidden)\n\n    def init_layer(self, layer):\n        (n_out, n) = layer.weight.size()\n        std = math.sqrt(2. \/ n)\n        scale = std * math.sqrt(3.)\n        layer.weight.data.uniform_(-scale, scale)\n\n        if layer.bias is not None:\n            layer.bias.data.fill_(0.)\n\n    def forward(self, x):\n        x = self.hidden(x)\n        return x\n\n\nif __name__ == '__main__':\n    model_weights = []\n    pre_a = []\n    post_a = []\n    for i in range(2):\n        chosen_seed = 0\n        torch.manual_seed(chosen_seed)\n        torch.backends.cudnn.deterministic = True\n        torch.backends.cudnn.benchmark = False\n        torch.cuda.manual_seed_all(chosen_seed)\n        numpy.random.seed(chosen_seed)\n        random.seed(chosen_seed)\n       \n        # (batch, time_frame, feature)\n        a = torch.rand(2, 4, 6)\n        model = SimpleModel1()\n\n        if i == 0:\n            model.cuda()\n            a = a.cuda()\n        test_model1 = []\n        test_a_1 = a\n        for params in model.parameters():\n            test_model1.append(params.data)\n\n        model.train()\n\n        output = model(a)\n        test_model2 = []\n        for params in model.parameters():\n            test_model2.append(params.data)\n        print(f\"output[0, :]: {output[0, :, :].tolist()}\")\n        model_weights.append([test_model1[0], test_model2[0]])\n        pre_a.append(test_a_1)\n        post_a.append(output)\n\n    for i in range(2):\n        if torch.all(torch.eq(model_weights[0][i].cpu(), model_weights[1][i])):\n            print('Model Weights OK')\n        else:\n            print('Error in Weights')\n    if torch.all(torch.eq(pre_a[0].cpu(), pre_a[1])):\n        print('Pre-model inputs OK')\n    else:\n        print('Error in pre_model inputs')\n    if torch.all(torch.eq(post_a[0].cpu(), post_a[1])):\n        print('Post-model inputs OK')\n    else:\n        print('Error in post_model inputs')\n\n<\/code>\nMy environment details:\n<code class=\"lang-auto\">name: repro\nchannels:\n  - pytorch\n  - anaconda\n  - defaults\ndependencies:\n  - _libgcc_mutex=0.1=main\n  - blas=1.0=mkl\n  - ca-certificates=2019.11.27=0\n  - certifi=2019.11.28=py37_0\n  - cudatoolkit=10.1.243=h6bb024c_0\n  - freetype=2.9.1=h8a8886c_1\n  - intel-openmp=2019.5=281\n  - jpeg=9b=h024ee3a_2\n  - libedit=3.1.20181209=hc058e9b_0\n  - libffi=3.2.1=hd88cf55_4\n  - libgcc-ng=9.1.0=hdf63c60_0\n  - libgfortran-ng=7.3.0=hdf63c60_0\n  - libpng=1.6.37=hbc83047_0\n  - libstdcxx-ng=9.1.0=hdf63c60_0\n  - libtiff=4.1.0=h2733197_0\n  - mkl=2019.5=281\n  - mkl-service=2.3.0=py37he904b0f_0\n  - mkl_fft=1.0.15=py37ha843d7b_0\n  - mkl_random=1.1.0=py37hd6b4f25_0\n  - ncurses=6.1=he6710b0_1\n  - ninja=1.9.0=py37hfd86e86_0\n  - numpy=1.18.1=py37h4f9e942_0\n  - numpy-base=1.18.1=py37hde5b4d6_1\n  - olefile=0.46=py37_0\n  - openssl=1.1.1=h7b6447c_0\n  - pillow=7.0.0=py37hb39fc2d_0\n  - pip=19.3.1=py37_0\n  - python=3.7.3=h0371630_0\n  - pytorch=1.4.0=py3.7_cuda10.1.243_cudnn7.6.3_0\n  - readline=7.0=h7b6447c_5\n  - setuptools=44.0.0=py37_0\n  - six=1.13.0=py37_0\n  - sqlite=3.30.1=h7b6447c_0\n  - tk=8.6.8=hbc83047_0\n  - torchvision=0.5.0=py37_cu101\n  - wheel=0.33.6=py37_0\n  - xz=5.2.4=h14c3975_4\n  - zlib=1.2.11=h7b6447c_3\n  - zstd=1.3.7=h0b5b093_0\n\n<\/code>","y":"The differences you are seeing are approx. 1e-6 which comes most likely due to the limited floating point precision of float32.\nThe order of operations might yield different results as seen here:\n<code class=\"lang-python\">x = torch.randn(10, 10, 10)\ns1 = x.sum()\ns2 = x.sum(0).sum(0).sum(0)\nprint((s1 - s2).abs().max())\n> tensor(3.8147e-06)\n<\/code>\nI assume the difference in CPU architecture between both posted CPUs might explain the difference, but that\u2019s just by best guess.\nAlso, the implementation of the pseudo-random number generator might be different for different hardware devices (e.g. CPU vs. GPU).","z":"The differences you are seeing are approx. 1e-6 which comes most likely due to the limited floating point precision of float32.\nThe order of operations might yield different results as seen here:\n<code class=\"lang-python\">x = torch.randn(10, 10, 10)\ns1 = x.sum()\ns2 = x.sum(0).sum(0).sum(0)\nprint((s1 - s2).abs().max())\n> tensor(3.8147e-06)\n<\/code>\nI assume the difference in CPU architecture between both posted CPUs might explain the difference, but that\u2019s just by best guess.\nAlso, the implementation of the pseudo-random number generator might be different for different hardware devices (e.g. CPU vs. GPU).\nThanks for your answer, I tried increasing model complexity and printing out the maximum difference between GPU and CPU results and I think you are correct, the highest difference I received was 2.3543834686279297e-06.\nArchitecture difference is also a good guess."},{"x":"Hi everyone,\nI\u2019m working on a custom dataset using the tutorial   use custom dataset with link \"https:\/\/colab.research.google.com\/drive\/16jcaJoc6bCFAQ96jDe2HwtXj7BMD_-m5\".\nI\u2019ve have tested (print out) my image annotations and they are all there but I just see the image without annotation overlays using the instruction in the tutorial. Below is the output of an image I\u2019m trying to visualize the annotations. I showed only the first 10 items of the segmentation key.\n\nnb of labels : 3\nlabel 0\nbbox : [38, 61, 54, 80]\nbbox_mode : 0\ncategory_id : 0\niscrowd : 0\nsegmentation: [63.5, 62.5, 64.5, 61.5, 66.5, 61.5, 67.5, 62.5, 68.5, 61.5]\nlabel 1\nbbox : [64, 85, 62, 61]\nbbox_mode : 0\ncategory_id : 1\niscrowd : 0\nsegmentation: [103.5, 86.5, 104.5, 85.5, 105.5, 85.5, 106.5, 86.5, 107.5, 86.5]\nlabel 2\nbbox : [68, 90, 53, 51]\nbbox_mode : 0\ncategory_id : 2\niscrowd : 0\nsegmentation: [92.5, 91.5, 93.5, 90.5, 94.5, 90.5, 95.5, 91.5, 98.5, 91.5]\n\nSome images don\u2019t have my target objects so I set category_id to  num_categories (here 3) to indicate that this image is only background (is my use of it correct ??).\nAs I work in google colab, I\u2019ve tried to modify the source that prompts when accessing the method definition in order to add some print in the overlay_instances. But nothing appears.\nPlease how can debug this matter.\nThank you so much.","y":"Hi, sorry I figured out my dumb tiny mistake: forgot to put \u201cs\u201d at the end the \u201cannotations\u201d key in the data dict.","z":"Concerning the local repo modification, I\u2019ve found that the instruction below adapted from the custom dataset tutorial with link \"https:\/\/colab.research.google.com\/drive\/16jcaJoc6bCFAQ96jDe2HwtXj7BMD_-m5\":\n<code class=\"lang-auto\">!pip install -e \"\/content\/gdrive\/My Drive\/Colab Notebooks\/project_dir\"\/detectron2_repo\n<\/code>\ndoesn\u2019t  make the repo editable at all. No changes in files in the local repo take effect.\nSo if someone could point me a way to make the repo editable, it\u2019ll help debug my issue.\nI\u2019ve searched around and found some other CLI but nothing works:\n<code class=\"lang-auto\">!cd \"\/content\/gdrive\/My Drive\/Colab Notebooks\/acdc\"\/detectron2_repo ; python setup.py develop\n<\/code>\nPS: I\u2019m working on google colab! is that related to my inability to modify the repo ?\nHi, sorry I figured out my dumb tiny mistake: forgot to put \u201cs\u201d at the end the \u201cannotations\u201d key in the data dict."},{"x":"I have an input image with shape (1, 3, 392, 196) but after calling transforms.ToTensor() it converts it to (1, 3, 196, 392), by swapping the width and height. I want to know whether my model is seeing a rotated version of the input image because this is crucial for my object detection task.","y":"Thank you . I figured it out. I had a misconception regarding how the image will look after swapping H and W. Later I realized that it is correct, nothing changes about how the model sees the image. It is a thing about how arrays are represented. An image with height H and width W, when represented as an array will consist of H sub-arrays of width W: each sub-array is a row. So when Torch reports the dimension of the array it will count H sub-arrays of width W  and that is why we have the dimension as [H,W].\nThank you once again for offering an assistance.","z":"Could you post a reproducible code snippet, please?\nThank you . I figured it out. I had a misconception regarding how the image will look after swapping H and W. Later I realized that it is correct, nothing changes about how the model sees the image. It is a thing about how arrays are represented. An image with height H and width W, when represented as an array will consist of H sub-arrays of width W: each sub-array is a row. So when Torch reports the dimension of the array it will count H sub-arrays of width W  and that is why we have the dimension as [H,W].\nThank you once again for offering an assistance."},{"x":"Hi, I\u2019m trying to implement the model in this paper: https:\/\/arxiv.org\/pdf\/1912.08967.pdf\nMy inputs to the model are a triplet of outfit images (3 images), positive image (1 image), negative images (3 images).\nLet\u2019s ignore the labels of the images because they\u2019re not important\nI created a custom dataset that can return that triplet like this:\n<code class=\"lang-auto\">return (outfit_imgs, positive_img, negative_imgs)\n<\/code>\nThe shape of a single training example is: ((3, 3, 244, 224), (1, 3, 224, 224), (3, 3, 224, 224))\nEverything went fine with a single training example but when I try to use the dataloader and set batchsize=4 the training example\u2019s shape becomes ((4, 3, 3, 224, 224), (4, 1, 3, 224, 224), (4, 3, 3, 224, 224)) that my model can\u2019t understand.\nHave anyone countered this situation before?\nDo I need to reshape the training example or are there any other ways to work around?\nThank you very much.","y":" I see the problem. You could pad your inputs to have always the same number of images. Meaning extending the number of images with zero filled images to (max_img, 1, max_neg_img). This could be done with a custom transform with link \"https:\/\/pytorch.org\/tutorials\/beginner\/data_loading_tutorial.html#transforms\" to pad for max image count in the whole dataset or the collate_fn with link \"https:\/\/pytorch.org\/docs\/stable\/data.html?highlight=collate#dataloader-collate-fn\" for batch level padding.\nWith padding you need to make sure to mask your values to ignore the zeros in further processing.\nGood luck and have fun with your implementation. ","z":"Hey ,\nIf you set the batch size to n the dataloader will return n-samples resulting in your additional dimension. It seems like if you use batch size = 1 the batch dimension is omitted. I never experienced such a behaviour. In my examples the dataloader would return ((1, 3, 3, 224, 224), (1, 1, 3, 224, 224), (1, 3, 3, 224, 224)) for a batch size = 1 (not omitting the batch dimension). Can you share your code?\nHowever, you need to adjust your model to be able to load different batches. Probably flatten the batch and triplet dimension and make sure the model uses the correct inputs.\n<code class=\"lang-auto\"># reshape\/view for one input where m_images = #input images (= 3 for triplet)\ninput = input.contiguous().view(batch_size * m_images, 3, 224, 244)\n<\/code>\nThe flattened tensor would have the shape: ((12, 3, 224, 224), (4, 3, 224, 224), (12, 3, 224, 224))\n\n\n\n christopherkuemmel:\n\nHowever, you need to adjust your model to be able to load different batches. Probably flatten the batch and triplet dimension and make sure the model uses the correct inputs.\n\n\nThank you \nI understand your solution, I\u2019m at home so I can\u2019t post all the codes. Here are some of them, maybe you can help me review it\ndataset class:\n<code class=\"lang-auto\">from __future__ import print_function, division\nimport json\nimport os\nimport torch\nimport numpy as np\nimport matplotlib.pyplot as plt\n\nfrom config import IDX2CLASS, NUM_CLASSES\nfrom PIL import Image\nfrom torch.utils.data import Dataset\n# Ignore warnings\nimport warnings\n\nwarnings.filterwarnings(\"ignore\")\n\n\ndef read_img(impath):\n    image = plt.imread(impath)\n    if len(image.shape) == 2:\n        image = np.repeat(image[:, :, None], repeats=3, axis=2)\n    elif image.shape[2] == 1:\n        image = np.repeat(image, repeats=3, axis=2)\n    else:\n        image = image[:, :, 0:3]\n    return image\n\n\nclass OutfitDataset(Dataset):\n    \"\"\"\n    This dataset is used to hold training triplet: outfit imgs, positive img, negative imgs as per the paper:\n    https:\/\/arxiv.org\/pdf\/1912.08967.pdf\n    \"\"\"\n    def __init__(self, index_file, transform=None, root_dir=\"\/home\/hanhvn\/Pictures\/shopping 100k\/Images\/Female\", device=None):\n        \"\"\"\n        Init the dataset\n        :param index_file: (string) file that contains img paths and categories\n        :param transform: (torchvision.transform) transform will be used to transform input imgs to approriate data\n        :param root_dir: (string) full or relative path where we store images\n        :param device: (int) id of cuda device\n        \"\"\"\n        self.index_file = index_file\n        self.transform = transform\n        self.root_dir = root_dir\n        self.device = device\n        # load data\n        with open(index_file, \"r\") as f:\n            self.data = json.load(f)\n\n    def __len__(self):\n        return len(self.data)\n\n    def __getitem__(self, idx):\n        idx = str(idx)\n        # get training triplet img's names\n        outfit_img_names = self.data[idx]['outfit']\n        negative_img_names = self.data[idx]['negatives']\n        positive_img_name = self.data[idx]['positive']\n        # get labels\n        labels = self.data[idx]['categories']\n        # separate labels according to their triplet:\n        # first labels group belongs to outfit, second group belongs to positive img\n        # last labels group belongs to negative imgs\n        outfit_labels = labels[0: len(outfit_img_names)]\n        positive_label = labels[len(outfit_img_names)]\n        negative_labels = labels[len(outfit_img_names) + 1:]\n\n        # read imgs and transform them\n        outfit_imgs = []\n        negative_imgs = []\n        positive_img = self.transform(read_img(os.path.join(self.root_dir, IDX2CLASS[positive_label], positive_img_name)))\n        for i in range(len(outfit_img_names)):\n            image = read_img(os.path.join(self.root_dir, IDX2CLASS[outfit_labels[i]], outfit_img_names[i]))\n            # print(os.path.join(self.root_dir, IDX2CLASS[outfit_labels[i]], outfit_img_names[i]))\n            outfit_imgs.append(self.transform(image))\n        for i in range(len(negative_img_names)):\n            image = read_img(os.path.join(self.root_dir, IDX2CLASS[negative_labels[i]], negative_img_names[i]))\n            negative_imgs.append(self.transform(image))\n\n        # convert imgs to tensor using torch.stack()\n        outfit_imgs = torch.stack(outfit_imgs)\n        negative_imgs = torch.stack(negative_imgs)\n        positive_img = torch.tensor(positive_img)\n\n        # convert label to one-hot encoding instead of scalar\n        outfit_labels = torch.nn.functional.one_hot(torch.tensor(outfit_labels), num_classes=NUM_CLASSES).float()\n        positive_label = torch.nn.functional.one_hot(torch.tensor(positive_label), num_classes=NUM_CLASSES).float()\n        negative_labels = torch.nn.functional.one_hot(torch.tensor(negative_labels), num_classes=NUM_CLASSES).float()\n\n        # move all variables to cuda device if available\n        # TODO: check if we really need to move labels to cuda device\n        if self.device is not None:\n            outfit_imgs = outfit_imgs.to(self.device)\n            positive_img = positive_img.to(self.device)\n            negative_imgs = negative_imgs.to(self.device)\n            outfit_labels = outfit_labels.to(self.device)\n            positive_label = positive_label.to(self.device)\n            negative_labels = negative_labels.to(self.device)\n\n        return (outfit_imgs, positive_img, negative_imgs), \\\n               (outfit_labels, positive_label, negative_labels)\n\n<\/code>\nmodel definition:\n<code class=\"lang-auto\">import torch.nn as nn\nimport torchvision.models as models\nimport torch\n\n\nclass CSANet(nn.Module):\n    \"\"\"\n    Category-based subspace attention network (CSA-Net)\n    reference: https:\/\/arxiv.org\/pdf\/1912.08967.pdf\n    \"\"\"\n    def __init__(self, num_subspaces=5, embedding_size=64):\n        \"\"\"\n        :param num_subspaces: (int) number of subspaces that an image can be in\n        :param embedding_size: (int) dimension of embedding feature\n        \"\"\"\n        # TODO: cache extracted features using resnes before feeding to the network\n        super(CSANet, self).__init__()\n        self.num_subspaces = num_subspaces\n        self.embedding_size = embedding_size\n        # we use reset18 as per the paper\n        self.resnet18 = models.resnet18(pretrained=True)\n        # get the second-to-last layer to extract the features\n        self.resnet18 = nn.Sequential(*list(self.resnet18.children())[:-1])\n        # disable gradient computation\n        for param in self.resnet18.parameters():\n            param.requires_grad = False\n\n        # embedding layer that embed image's feature from resnet to embedding size(64)\n        self.embedding_layer = nn.Linear(512, self.embedding_size)\n        # learnable masks that have the same dimensionality as the image feature vector (64)\n        self.masks = nn.Parameter(data=torch.Tensor(self.num_subspaces, self.embedding_size), requires_grad=True)\n        # init weights, without it we will encounter nan when calculate loss\n        torch.nn.init.xavier_uniform(self.embedding_layer.weight)\n        torch.nn.init.xavier_uniform(self.masks)\n\n    def forward(self, image):\n        # extract img's features\n        feature = self.resnet18(image)\n        # calculate pre-embedding\n        # it's pre-embedding because we haven't multiply it with masks and attention weights yet\n        feature = self.embedding_layer(feature.squeeze())\n        # repeat these features (num_subspaces times) to perform multiply with masks\n        # TODO: check if there is a better way to do the multiplication without repeat\n        feature = feature.repeat(1, self.num_subspaces)\n        feature = feature.reshape(-1, self.num_subspaces, self.embedding_size)\n        feature = self.masks * feature\n\n        return feature\n\n\nclass AttentionLayer(nn.Module):\n    \"\"\"\n    The attention layer will calculate attention weights and\n    combine those weights with features from CSA-Net to output final embedding result\n    \"\"\"\n    def __init__(self, num_subspaces=5):\n        super(AttentionLayer, self).__init__()\n        self.num_subspaces = num_subspaces\n        # two fc layers as per the paper\n        # TODO: removes hardcoded dimension in the first fc layer\n        self.fc1 = nn.Linear(20, 10)\n        self.fc2 = nn.Linear(10, self.num_subspaces)\n        # init them\n        torch.nn.init.xavier_uniform(self.fc1.weight)\n        torch.nn.init.xavier_uniform(self.fc2.weight)\n\n    def forward(self, feature, item_category, target_category):\n        \"\"\"\n        :param feature: (tensor) image features extracted from CSA-Net (dim=64)\n        :param item_category: (one-hot tensors) categories of source item\n        :param target_category: (one-hot tensors) categories of item that we want to predict compatibility\n        :return: (tensor) embedding of item in the subspace of source and target category\n        \"\"\"\n        # we usually in a situation when there is only one item category vs multiple target categories and vice versa\n        # so we have to stack the one that have smaller shape to make them equal in term of shape\n        # TODO: find a better way to deal with this situation\n        if len(item_category.shape) > len(target_category.shape):\n            target_category = target_category.repeat(item_category.shape[0], 1)\n        elif len(item_category.shape) < len(target_category.shape):\n            item_category = item_category.repeat(target_category.shape[0], 1)\n\n        # same thing happens with feature\n        if feature.shape[0] < item_category.shape[0]:\n            feature = feature.repeat(item_category.shape[0], 1, 1)\n\n        # combied_category = torch.cat((item_category, target_category), 1)\n        attention_weights = self.fc1(torch.cat((item_category, target_category), 1))\n        attention_weights = self.fc2(attention_weights)\n        attention_weights = nn.functional.softmax(attention_weights)\n        attention_weights = attention_weights.unsqueeze(-1)\n        feature = feature * attention_weights\n        embedding = torch.sum(feature, dim=1)\n\n        return embedding\n<\/code>\ntraining code:\n<code class=\"lang-auto\">import torch\nimport numpy as np\nimport torch.optim as optim\nimport matplotlib.pyplot as plt\n\nfrom config import NUM_EPOCH, LEARNING_RATE, IDX2CLASS\nfrom torchvision import transforms\nfrom model import CSANet, AttentionLayer\nfrom ranking_loss import RankingLoss\nfrom dataset import OutfitDataset\nfrom torch.utils.data import DataLoader\n\n\ndef _init_fn(worker_id):\n    \"\"\"\n    Function to make the pytorch dataloader deterministic\n    :param worker_id: id of the parallel worker\n    :return:\n    \"\"\"\n    np.random.seed(0 + worker_id)\n\n\nif __name__ == '__main__':\n    # check if cuda is available\n    device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n    print(device)\n    # init models and loss function\n    model = CSANet().to(device)\n    attention_layer = AttentionLayer().to(device)\n    criteria = RankingLoss().to(device)\n\n    optimizer = optim.Adam(list(model.parameters()) + list(attention_layer.parameters()), lr=LEARNING_RATE)\n\n    # init datasets and dataloaders, transforms\n    # TODO: inserts validation and test datasets, dataloaders, transforms\n    transform = transforms.Compose([transforms.ToPILImage(),\n                                    transforms.Resize((224, 224)),\n                                    transforms.ToTensor(),\n                                    transforms.Normalize(mean=(0.485, 0.456, 0.406), std=(0.229, 0.224, 0.225))])\n    traindataset = OutfitDataset(\"data\/dataset.json\", transform=transform, device=device)\n    # TODO: we cant train with batchsize > 1, fix it\n    # TODO: fix RuntimeError: Cannot re-initialize CUDA in forked subprocess when running with dataloader\n    dataloader = DataLoader(traindataset, batch_size=1,\n                            shuffle=True, num_workers=4)\n    for epoch in range(NUM_EPOCH):  # loop over the dataset multiple times\n        running_loss = 0.0\n        for t, data in enumerate(traindataset):\n            # get the inputs; data is a list of [inputs, labels]\n            inputs, labels = data\n            # input images is a tuple consists of (outfit imgs, positive img, negative imgs\n            outfit, positive_example, negative_examples = inputs\n            # input labels go the same way as input images\n            outfit_categories, positive_example_category, negative_examples_categories = labels\n\n            # extract features from input imgs\n            outfit_features = model(outfit)\n            positive_features = model(positive_example.unsqueeze(0))\n            negative_features = model(negative_examples)\n\n            # calculate embedding of these features and correspond categories\n            outfit_embeds = attention_layer(outfit_features, outfit_categories, positive_example_category)\n            positive_embeds = attention_layer(positive_features, positive_example_category, outfit_categories)\n            # negative_embeds = attention_layer(negative_features[1], negative_examples_categories[1], outfit_categories)\n            negative_embeds = []\n            for i in range(len(negative_features)):\n                negative_embeds.append(attention_layer(negative_features[i], negative_examples_categories[i], outfit_categories))\n\n            # run loss function and update gradients\n            loss = criteria(outfit_embeds, positive_embeds, negative_embeds)\n            loss.backward()\n            optimizer.step()\n            optimizer.zero_grad()\n\n            # print statistics\n            print(\"i: {}, loss: {}\".format(t, loss))\n    print('Finished Training')\n\n<\/code>\nHi, unfortunately I can\u2019t came up with a reason why your DataLoader omits the batch size dimension. I thought about the tuple unpacking part in your training script.\n\n\n\n hanhvn:\n\noutfit, positive_example, negative_examples = inputs # input labels go the same way as input images outfit_categories, positive_example_category, negative_examples_categories = labels\n\n\nBut if I try to reproduce this it returns a 1 dimension for the batch size\u2026\n((1, 3, 3, 224, 224), (1, 1, 3, 224, 224), (1, 3, 3, 224, 224))\nrather than your case\n((3, 3, 224, 224), (1, 3, 224, 224), (3, 3, 224, 224))\n\nFor the batch size > 1 part you could try these lines of code.\n<code class=\"lang-auto\"># input images is a tuple consists of (outfit imgs, positive img, negative imgs\noutfit, positive_example, negative_examples = inputs\n\n# flatten batch dimension\noutfit = outfit.contiguous().flatten(end_dim=1)\npositive_example = positive_example.contiguous().flatten(end_dim=1)\nnegative_examples = negative_examples.contiguous().flatten(end_dim=1)\n\n# input labels go the same way as input images\noutfit_categories, positive_example_category, negative_examples_categories = labels\n\n# flatten batch dimension\noutfit_categories = outfit_categories.contiguous().flatten(end_dim=1)\npositive_example_category = positive_example_category.contiguous().flatten(end_dim=1)\nnegative_examples_categories = negative_examples_categories.contiguous().flatten(end_dim=1)\n<\/code>\nThe CSANet call should work as before. However, I\u2019m not sure if your AttentionLayer works correctly with the flattened batches. You need to make sure that the attention only consider the corresponding images of the batch.\nHey , I really appreciate your help. I\u2019ll try your solution tomorrow and let you know the result \n I tried your method and it worked but turned out the number of input images is not fixed in each training example.\nFor example, the first training triplet could have (3 imgs, 1 positive imgs, 2 negative imgs) and the second would have (4 imgs, 1 positive imgs, 4 negative imgs). This raise an RuntimeError: RuntimeError: invalid argument 0: Sizes of tensors must match except in dimension 0. Got 3 and 4 in dimension 1 at \/pytorch\/aten\/src\/TH\/generic\/THTensor.cpp:689 and they say the only solution  is to have batchsize=1.\nSo I think I would stick with batchsize=1 for now. Thank for your help anyway\n I see the problem. You could pad your inputs to have always the same number of images. Meaning extending the number of images with zero filled images to (max_img, 1, max_neg_img). This could be done with a custom transform with link \"https:\/\/pytorch.org\/tutorials\/beginner\/data_loading_tutorial.html#transforms\" to pad for max image count in the whole dataset or the collate_fn with link \"https:\/\/pytorch.org\/docs\/stable\/data.html?highlight=collate#dataloader-collate-fn\" for batch level padding.\nWith padding you need to make sure to mask your values to ignore the zeros in further processing.\nGood luck and have fun with your implementation. "},{"x":"Torchivison\u2019s model uses ResNet51+FPN as a feature extractor.\nI usually transform images by first converting them to a tensor, and then multiplying again by 255\n<code class=\"lang-auto\">    t_ = transforms.Compose([\n                    transforms.ToPILImage(),\n                    transforms.Resize(img_size),\n                    transforms.ToTensor(),\n                    transforms.Normalize(mean=[0.407, 0.457, 0.485],\n                    std=[1,1,1])])\n   \n   img = 255*t_(img)\n<\/code>\nWhen I did that with an image input to Faster R-CNN, the result was None, but when I removed the multiplication, it seems to be working fine. It that a ResNet thing?","y":"No, it\u2019s common to normalize the inputs for a lot of machine learning models, as this might accelerate and stabilize the training.\nSome methods e.g. RandomForest classifiers are not sensitive to the input range, while e.g. neural networks are.\nYou would have to check the dataset creation (or just get a single sample) and check the range of the inputs the model was trained on.","z":"Could you post a code snippet, which reproduces this issue?\nThis:\n<code class=\"lang-auto\">    threshold = 0.75\n    im = PILImage.open(im)\n    img = np.array(im)\n    img = transforms.ToTensor()(img)\n    print(img.size())\n    out = frcnn_model([img])\n    print(out)\n    scores = out[0]['scores']\n    bboxes = out[0]['boxes']\n    classes = out[0]['labels']\n    best_idx = np.where(scores>threshold)\n    best_bboxes = bboxes[best_idx]\n    best_classes = bboxes[best_idx]\n    if len(best_idx)>0:\n       plt.imshow(im)\n       ax=plt.gca()\n       for b in best_bboxes:\n           rect = Rectangle((b[0],b[1]), b[2]-b[0], b[3]-b[1], linewidth=2, edgecolor='r', facecolor='none')\n           ax.add_patch(rect)\n       plt.show()\n<\/code>\nAlso, it seems that Faster R-CNN requires an RGB input not BGR, at least that seems from the normalization step:\n<code class=\"lang-auto\">\n        if image_mean is None:\n            image_mean = [0.485, 0.456, 0.406]\n        if image_std is None:\n            image_std = [0.229, 0.224, 0.225]\n        transform = GeneralizedRCNNTransform(min_size, max_size, image_mean, image_std)\n<\/code>\nYour code snippet neither seem to use the defined transformation nor the multiplication after the transformation.\nThe pretrained torchvision models should use RGB inputs by default.\nI get empty output if I use the multiplication by 255 in the transform method.\nIn the tutorials I\u2019ve found the input image pixels are between 0 and 1, as returned by the ToTensor() transform. So what should they be?\n\n\n\n sigma_x:\n\nI get empty output if I use the multiplication by 255 in the transform method.\n\n\nWe would still need a code snippet to further debug the issue.\n\n\n\n sigma_x:\n\nIn the tutorials I\u2019ve found the input image pixels are between 0 and 1, as returned by the ToTensor() transform. So what should they be?\n\n\nI would recommend to stick to the tutorial with link \"https:\/\/pytorch.org\/tutorials\/intermediate\/torchvision_tutorial.html\" and use the train transformation. Note that the normalization will be done in side the model as seen here with link \"https:\/\/discuss.pytorch.org\/t\/preprocessing-for-faster-rcnn\/65266\/2\".\nI was referring to the same lines of code as you.\nObviously the model takes inputs between 0 and 1.\nThat\u2019s in contrast to VGG16 model, that takes inputs between 0 and 255\nvgg16 takes in a normalized input as all classification models:\n\nAll pre-trained models expect input images normalized in the same way, i.e. mini-batches of 3-channel RGB images of shape (3 x H x W), where H and W are expected to be at least 224. The images have to be loaded in to a range of [0, 1] and then normalized using  mean = [0.485, 0.456, 0.406]  and  std = [0.229, 0.224, 0.225] . You can use the following transform to normalize:\n\nOK it\u2019s just for backtransform then\nThanks!\nOK I\u2019m at loss. I tried FCN8s, without multiplying by 255 I get all 0s, with 255 I get a good solution\n<code class=\"lang-auto\">fcn8s = fcn_models.FCN8s(n_class = len(pascal_object_categories))\n\nfcn8s.load_state_dict(\"fcn8s_from_caffe.pth\")\nprint(fcn8s)\n\n#evaluate the pretrained FCN8s model on one image\ndef deploy_fcn_model(im):\n    im = PILImage.open(im)\n    img = np.array(im)\n    # these mean values are for RGB\n    t_ = transforms.Compose([\n                    transforms.ToPILImage(),\n                    transforms.ToTensor(),\n                    transforms.Normalize(mean=[0.485, 0.457, 0.407],\n                                        std=[1,1,1])])\n    \n    #multiply by 255 for the network input\n    img = 255*t_(img) \n    img.unsqueeze_(0)\n    if device == torch.device(\"cuda\"):\n        img = img.to(device)\n    # get the output from the model\n    output = fcn8s(img)    \n    #remove from cuda, convert to numpy, squeeze\n    out = output.argmax(1).squeeze_(0).detach().clone().cpu().numpy()    \n    plt.imshow(out)\n    plt.show()\n    #load the image\n    bgr_img = cv2.imread(\"dogcat1.jpg\")\n    # convert FCN8s pixelwise predictions to color array\n    color_array = np.zeros([out.shape[0], out.shape[1],3], dtype=np.uint8)\n    for id in np.unique(out):\n        print(id)\n        if id == 8:\n            color_array[out==id] = [255,0,0]\n        elif id == 12:\n            color_array[out==id] = [0,255,0]            \n\n    #overlay images\n    added_image = cv2.addWeighted(bgr_img, 0.5, color_array,0.6, 0)\n    #plot\n    plt.imshow(added_image)\n    plt.show()\n    \n    \ndeploy_fcn_model(\"dogcat1.jpg\")\n<\/code>\nWhy is this happening?\nCould you post the model definition or the repository?\nMaybe this model was trained with unnormalized values?\nFor FCN I used the weights from https:\/\/github.com\/wkentaro\/pytorch-fcn\nIs squeezing inputs between 0 and 1 specific to pytorch?\nNo, it\u2019s common to normalize the inputs for a lot of machine learning models, as this might accelerate and stabilize the training.\nSome methods e.g. RandomForest classifiers are not sensitive to the input range, while e.g. neural networks are.\nYou would have to check the dataset creation (or just get a single sample) and check the range of the inputs the model was trained on."},{"x":"Why can the dimensions of input and target be different when using this class of torch.nn.CrossEntropyLoss? I remember that one_hot processing was needed when using keras. This\u2019s why?","y":"nn.CrossEntropyLoss expects the target as a LongTensor containing the class indices rather than a one-hot encoded target.\nSince you are indexing the logits for the target of the corresponding sample, you can use the class indices directly.\nHave a look at the docs with link \"https:\/\/pytorch.org\/docs\/stable\/nn.html?highlight=crossentropyloss#torch.nn.CrossEntropyLoss\" for more information.","z":"nn.CrossEntropyLoss expects the target as a LongTensor containing the class indices rather than a one-hot encoded target.\nSince you are indexing the logits for the target of the corresponding sample, you can use the class indices directly.\nHave a look at the docs with link \"https:\/\/pytorch.org\/docs\/stable\/nn.html?highlight=crossentropyloss#torch.nn.CrossEntropyLoss\" for more information.\nThanks for your help."},{"x":"Hi,\nI am trying to load images from a folder with no subfolders and having the error below. When I try to read an image from the folder I can load .jpg file with no error. My file format meets the pytorch image format criteria. Also, transforms.Compose just works fine (this must mean nothing wrong with my pytorch installation) Is there any other way to load image data or anything I can do to solve this error.\nThanks in advance!\nAyshine\nimage.png1037\u00d7512 30.3 KB with link \"https:\/\/discuss.pytorch.org\/uploads\/default\/original\/2X\/4\/41f8b5479c899d2c5981524aa9329be4a45813dd.png\"\nimage.jpg721\u00d7487 125 KB with link \"https:\/\/discuss.pytorch.org\/uploads\/default\/original\/2X\/e\/efccb6c51365593d1b47580477eb0319e1e33eb3.jpeg\"","y":"I decided to come up with a model have classes 0, 1 or 2 (2 for 2 or more people) and moved my images to data folder using this code.It took seconds\n<code class=\"lang-auto\">i = 0\nimport shutil\nfrom pathlib import Path\n\nfor i in range(len(filenames)):\n    if train_test[i] != '0':\n        my_file = Path('images\/'+ filenames[i])\n        if my_file.is_file():\n            if len(sps[i]) == 0:\n                shutil.copyfile('images\/'+ filenames[i], 'data\/0\/'+filenames[i]) \n            elif len(sps[i]) == 1:\n                shutil.copyfile('images\/'+ filenames[i], 'data\/1\/'+filenames[i]) \n            else :\n                shutil.copyfile('images\/'+ filenames[i], 'data\/2\/'+filenames[i]) \n<\/code>\nremoved couple of things so my data in .csv file looks like this\n<code class=\"lang-auto\">015601864.jpg;1;sports;curling;12\n015599452.jpg;1;sports;curling;3\n<\/code>\nWriting this if it would help someone else too and marking this reply as solution.","z":"torchvision.datasets.ImageFolder expects subfolders representing the classes containing images of the corresponding class.\nIf you just would like to load a single image, you could load it with e.g. PIL.Image.open and pass it to your transform.\nHowever, if you don\u2019t want to change your code, just move your image to a subfolder and ImageFolder should work.\nThanks ,\nUnfortunately I need to load 25000 image dataset. I had a couple of issues about labels, too. May be I can write a script to arrange them in folders.\nHow are the images labeled at the moment? Are the labels somehow written into the file names or do you have a separate list with the image paths and the corresponding label?\nThere are couple of label files that are all in .mat file. I couldn\u2019t find the way to open them so started converting two relevant files to .csv files. One of those files labels images if there is one person or not and the other one classifies them in terms of main activity and the sub activity.\nYou can load .mat files with scipy.io.loadmat with link \"https:\/\/docs.scipy.org\/doc\/scipy\/reference\/tutorial\/io.html\".\nIf you could create one .csv file containing the image path and both labels, we could write a Dataset and load all corresponding images and labels using this file without moving your data.\nThat would be great! Let me try to merge them all in one .csv then come back here.\nSure! If you\u2019ve created the .csv, could you post a few lines of the file so that we can write a dummy Dataset using your format?\nAlso, let me know, if you need some help creating the .csv from your MATLAB files.\nThanks for the offer helping with MATLAB files. Now, my problem became more about reading MATLAB files. I am working on this dataset: http:\/\/human-pose.mpi-inf.mpg.de\/#download\nIf you are also familiar with the paper I would really appreciate your recommendations about how to feed this dataset to a deep learning model built using pytorch.I could f\u0131nd the f\u0131rst \u0131mage file name as follows.I\u2019ll try to find a way to retrieve the rest.\n\nI had a look at the labels of the dataset, and it seems loadmat wraps the data in a ton of unnecessary nested arrays. I\u2019ll try to load it with Octave and see how far I come.\nOh, Thanks! I managed to gather all the important \u0131nfo to one CSV file. Here is a couple of lines.\n\nimage_path,train_test,category,activity,single_person\n029122914.jpg,1,\u2018occupation\u2019, \u2018truck driving, loading and unloading truck, tying down load, standing, walking and carrying heavy lo\u2019,1\n061185289.jpg,1,\u2018occupation\u2019, \u2018truck driving, loading and unloading truck, tying down load, standing, walking and carrying heavy lo\u2019,1\n013949386.jpg,1,\u2018occupation\u2019, \u2018truck driving, loading and unloading truck, tying down load, standing, walking and carrying heavy lo\u2019,1\n029214465.jpg,1,\u2018occupation\u2019, \u2018truck driving, loading and unloading truck, tying down load, standing, walking and carrying heavy lo\u2019,1\n\nAwesome!\nDid you manage to create the Dataset?\nIf not, could you explain the format the the labels a bit?\nThe first column obviously contains the image names. Which column refers to the labels and what kind of ino do you need for your training?\nSorry, I should\u2019ve been more clear\nimage_path: the name of the images\ntrain_test: if 1 the image is training data 0 the image is\ncategory: the main category of the action\nactivity: detailed activity of the person or people in the image\nsingle_person: contains rectangle id  ridx  of  sufficiently separated  individuals (this explanation is from the database page I haven\u2019t figured out how to use this information)\nIt looks like test data does not have any category, activity or single_person information. After filtering train data I no longer need this column. For my initial model, I only take category column as the label but single_person information is also crucial for my use case. I might also drop the activity column  In Sum, for my ultimate goal is a binary decision between the states below:\n\nThere is one person on the image doing a regular activity\nThere are multiple people doing something else or The figure in the image does not belong to one person doing a regular activity.\n\nI might be heading nowhere by using this dataset, but I\u2019d like to share my use case to explain more clearly why I chose the dataset in the first place.\nOK, I see.\nCould you save the .csv with ; as a separator, as this will make the reading a bit easier, since activity uses , inside the text?\nLet\u2019s make sure I understand your use case.\nThe \u201cregular activity\u201d is defined by category? If so, I assume you would like to group the activities into regular and not regular. Which categories would belong to which class?\nCould you post an example for each case?\nIf single_person is set to 0, we know, that there are more than one person in the image.\nHow do you know the person in the image is not doing the regular activity?\nNow, I saved the file with ; separator. The three most important information I have image_path, category and single_person. After a little further reading single person has a list having ids of the coordinates of the head rectangle. If 0 then no one in the image, else if 1 there is one person, else more there are more people. I can use this column as my label and look for if there is one person in the image or not then use category as further info. In this case, I have 3 distinct labels.\n<code class=\"lang-auto\">image_path,train_test,category,single_person\n030424224.jpg;1;miscellaneous;[1,2]\n052475643.jpg;1;miscellaneous;[1,2]\n<\/code>\nI decided to come up with a model have classes 0, 1 or 2 (2 for 2 or more people) and moved my images to data folder using this code.It took seconds\n<code class=\"lang-auto\">i = 0\nimport shutil\nfrom pathlib import Path\n\nfor i in range(len(filenames)):\n    if train_test[i] != '0':\n        my_file = Path('images\/'+ filenames[i])\n        if my_file.is_file():\n            if len(sps[i]) == 0:\n                shutil.copyfile('images\/'+ filenames[i], 'data\/0\/'+filenames[i]) \n            elif len(sps[i]) == 1:\n                shutil.copyfile('images\/'+ filenames[i], 'data\/1\/'+filenames[i]) \n            else :\n                shutil.copyfile('images\/'+ filenames[i], 'data\/2\/'+filenames[i]) \n<\/code>\nremoved couple of things so my data in .csv file looks like this\n<code class=\"lang-auto\">015601864.jpg;1;sports;curling;12\n015599452.jpg;1;sports;curling;3\n<\/code>\nWriting this if it would help someone else too and marking this reply as solution.\nConsidering that testing images are in most cases not categorized into classes as train images are, does pytorch have a way of loading such images or do I have to write a custom loop to load all of them.\nA custom Dataset would probably be \u201ccleaner\u201d, but you could also load all images from a single folder and just discard the target."},{"x":"Hi, can anyone tell me, what is the purpose of this Layer. I\u2019m reading mobilenetV3 and GhostNet btw.\n<code class=\"lang-auto\">class SELayer(nn.Module):\n    def __init__(self, channel, reduction=4):\n        super(SELayer, self).__init__()\n        self.avg_pool = nn.AdaptiveAvgPool2d(1)\n        self.fc = nn.Sequential(\n                nn.Linear(channel, channel \/\/ reduction),\n                nn.ReLU(inplace=True),\n                nn.Linear(channel \/\/ reduction, channel),\n            )\n\n    def forward(self, x):\n        b, c, _, _ = x.size()\n        y = self.avg_pool(x).view(b, c)\n        print(y.shape)\n        y = self.fc(y).view(b, c, 1, 1)\n        print(y.shape)\n        y = torch.clamp(y, 0, 1)\n        print(y.shape)\n        return x * y\n<\/code>\nIs it some kind of special operation for weighting feature?\nThank you,","y":"It looks like the Squeeze-and-Excitation block with link \"https:\/\/arxiv.org\/pdf\/1709.01507.pdf\".","z":"It looks like the Squeeze-and-Excitation block with link \"https:\/\/arxiv.org\/pdf\/1709.01507.pdf\"."},{"x":"Hi, I have trained a neural network for Purpose 1 and I mistakenly use torch.save(model, PATH), which saves the entire model rather than only weights.\nAs the performance is very good, I would like to plug this model into another model to assist Purpose 2\u2026\nI have created the exact model object\u2026\nThen, I try to load the parameters by\n<code class=\"lang-auto\">checkpoint = torch.load(\".\/models\/xxxnet_weights.pk\", map_location=torch.device('cpu'))\nmodel.load_state_dict(checkpoint['model_state_dict'])\n<\/code>\nBut, the torch.load gives an error like this\n<code class=\"lang-auto\">AttributeError: Can't get attribute 'xxxNet' on <module 'models' from '\/home\/xxx\/Projects\/xxxx\/models\/__init__.py'>\n<\/code>\n(As stated in the official tutorial https:\/\/pytorch.org\/tutorials\/beginner\/saving_loading_models.html#what-is-a-state-dict) I suspect this is due to the torch.save saves the directory information, which is different now\u2026\nIs there a way that I can still load these parameters?\nMany thanks","y":"I think I can provide a solution for myself.\nLoad the model in the original project folder, save using torch.save(model.state_dict(), PATH) with parameters only again to another checkpoint. Then, load this new one in the new project.","z":"I think I can provide a solution for myself.\nLoad the model in the original project folder, save using torch.save(model.state_dict(), PATH) with parameters only again to another checkpoint. Then, load this new one in the new project."},{"x":"I am trying to run a deep learning model for the segmentation task.\nI have 2 gpus.\nMy doubt is whether can I parallelise my model or not?\nMy batch size is 1 and can\u2019t change it (maybe change it but I have to figure that out) because of some image transformations in the code.","y":"You could most likey use nn.DistributedDataParallel to use both GPUs each using a batch size of 1. I assume your model fits on a single GPU and you don\u2019t need to split it onto two devices.\nWould that work for you?","z":"You could most likey use nn.DistributedDataParallel to use both GPUs each using a batch size of 1. I assume your model fits on a single GPU and you don\u2019t need to split it onto two devices.\nWould that work for you?\nThanks.\nYes, that would work for me.I\u2019ll try and will post here it again if it doesn\u2019t work.\nokay, I tried wrapping my head around nn.DistributedDataParallel, following this link -\nhttps:\/\/pytorch.org\/docs\/stable\/nn.html#torch.nn.parallel.DistributedDataParallel\nbut I still not figure out how to do it as tutorial just tells to include following lines for single process multi-gpu.\n<code class=\"lang-auto\">torch.distributed.init_process_group(backend=\"nccl\")\nmodel = DistributedDataParallel(model)\n<\/code>"},{"x":"IncompatibleKeys(missing_keys=[], unexpected_keys=[])\nWhen I tried to use IBN-net as an feature extractor, I met an error.\n\n\n\nGitHub with link \"https:\/\/github.com\/XingangPan\/IBN-Net\"\n\n\n\nXingangPan\/IBN-Net with link \"https:\/\/github.com\/XingangPan\/IBN-Net\"\nInstance-Batch Normalization Networks (ECCV2018). Contribute to XingangPan\/IBN-Net development by creating an account on GitHub.\n\n\n\n\n\n\n<code class=\"lang-python\">class LargeModel(nn.Module):\n    def __init__(self, pretrain=True):\n        super(LargeModel, self).__init__()\n\n        # define ibn_model and initilize it with pretrained model.\n        IBN_model_name = 'resnet50_ibn_a.pth.tar'\n        IBN_model = resnet50_ibn_a(pretrained=False)\n        pretrained_model_weight = torch.load(IBN_model_name)['state_dict']\n        new_state_dict = OrderedDict()\n        for k, v in pretrained_model_weight.items():\n            name = k[7:] # remove `module.`\n            new_state_dict[name] = v\n     #   for k, v in new_state_dict.items():\n     #       print(k)\n     #   print('----------')\n     #   for k in IBN_model.state_dict().keys():\n     #       print(k)\n\n        self.ibn_res50 = IBN_model.load_state_dict(new_state_dict)\n        print(self.ibn_res50)\n         # Define Extra layers and initilize them with uniform distribution\n<\/code>\nWhen I try to print self.ibn_res50, it gives such error.\nIn fact, when I try to print new_state_dict and IBN_model.state_dict().keys(), both of them return the same keys.","y":"This is not an error, since no incompatible keys were found.\nSince this message was quite misleading, it was removed already and you shouldn\u2019t see it in the current stable release (1.3.1).","z":"This is not an error, since no incompatible keys were found.\nSince this message was quite misleading, it was removed already and you shouldn\u2019t see it in the current stable release (1.3.1)."},{"x":"I noticed that there is a weird slow down after using an if statement in my code. I load an image onto CUDA device, then my neural network (fixed parameters) detects if there is an object or not in the given image. If there is an object, pixel values take values different from zero in the corresponding region, otherwise  0. I must send a signal if there is a non-zero value in the predicted output. I\u2019m doing it by using an if statement. Execution time slows down drastically when using this if statement. Below is my code:\n<code class=\"lang-auto\">torch.cuda.synchronize()\ni=0\nwhile i < 10: \n    with torch.no_grad():\n        a = time.perf_counter()\n        image_i = torch.from_numpy(dataset).float().cuda()\/255.0\n        pred = torch.argmax(network(image_i)[\"seg\"][0] , dim=0)\n        if pred.byte().any()>0:\n            b = time.perf_counter()\n        print(b-a)\n        torch.cuda.synchronize()\n        \n        i=i+1\n<\/code>\n2.8723719230001734 seconds\n2.821866113000169 seconds\n2.8291808970000147 seconds\n2.806728226000132 seconds\n2.804821959000037 seconds\n2.8151050120000036 seconds\n2.808966260000034 seconds\n2.847957038000004 seconds\n2.812290454000049 seconds\n2.826942403999965 seconds\nIf I remove the if statement, the code looks like as below:\n<code class=\"lang-auto\">torch.cuda.synchronize()\ni=0\nwhile i < 10: \n    with torch.no_grad():\n        a = time.perf_counter()\n        image_i = torch.from_numpy(dataset).float().cuda()\/255.0\n        pred = torch.argmax(network(image_i)[\"seg\"][0] , dim=0)\n        pred.byte().any()>0\n        b = time.perf_counter()\n        print(b-a)\n        torch.cuda.synchronize()\n        \n        i=i+1\n<\/code>\n0.011929868999914106 seconds\n0.00671789400007583 seconds\n0.009328374000006079 seconds\n0.006993827000087549 seconds\n0.008924279999973805 seconds\n0.008238326999844503 seconds\n0.010348931999942579 seconds\n0.00666478800008008 seconds\n0.008329585999945266 seconds\n0.0066920950000621815 seconds\nAs you can see the problem is really in the if branch.\nCould you give an explanation to this if there is one?","y":"You are most likely just measuring the kernel launch times in your second code snippet.\nTo properly time a segment, you would have to synchronize before starting and stopping the timer.\nE.g. this code should show a high duration in the actual forward pass:\n<code class=\"lang-python\">while i < 10: \n    with torch.no_grad():\n        image_i = torch.from_numpy(dataset).float().cuda()\/255.0\n        torch.cuda.synchronize()\n        a = time.perf_counter()\n        pred = torch.argmax(network(image_i)[\"seg\"][0] , dim=0)\n        torch.cuda.synchronize()\n        b = time.perf_counter()\n        pred.byte().any()>0\n        print(b-a)\n        \n        i=i+1\n<\/code>","z":"It looks rather like a synchronization issue.\nIn your second example, pred.byte().any() > 0 would be computed on the GPU, thus it can be just enqueued and your second timer b will be immediately called without waiting.\nThe if condition should be executed on the CPU by the Python runtime, this it would create a synchronization point.\nThank you very much for your fast reply. Yet, I\u2019m still confused. Am I measuring the time in an incorrect way or is sending signal conditioned on if, really taking 3 seconds while my network predicts in less than 2 miliseconds? If so, is there a way to execute this if statement on GPU?\nYou are most likely just measuring the kernel launch times in your second code snippet.\nTo properly time a segment, you would have to synchronize before starting and stopping the timer.\nE.g. this code should show a high duration in the actual forward pass:\n<code class=\"lang-python\">while i < 10: \n    with torch.no_grad():\n        image_i = torch.from_numpy(dataset).float().cuda()\/255.0\n        torch.cuda.synchronize()\n        a = time.perf_counter()\n        pred = torch.argmax(network(image_i)[\"seg\"][0] , dim=0)\n        torch.cuda.synchronize()\n        b = time.perf_counter()\n        pred.byte().any()>0\n        print(b-a)\n        \n        i=i+1\n<\/code>\nThis was exactly the case. Thank you very much for your guidance!"},{"x":"Hey,\nI\u2019m trying to implement the optimization algorithems by myself and to compare them . However, not sure why, but the backwards flow doesnt update the gradients and they left as None values.\nMy optimizer :\n<code class=\"lang-auto\">class MyOptimizer:\n\n  def __init__(self, parameters,lr=0.001, momentum=0.9):\n    self.momentum = momentum\n    self.layers_data_list=[]\n    for layer_params in list(parameters):\n      layer_dict = dict()\n      layer_dict['params']=layer_params\n      layer_dict['momentum']=momentum\n      layer_dict['velocity']=None\n      layer_dict['lr']=lr\n      self.layers_data_list.append(layer_dict)\n\n      \n  def step_sgd(self):\n    for layer_data in self.layers_data_list:\n      for p in layer_data['params']:\n        if p.data.grad is None: #Update : tried if p.grad is None as suggested in the comments\n          print(\"grad is None\")\n          continue\n        lr=layer_data['lr']\n        d_p=p.grad.data\n        p.data.add_(-lr,d_p)\n\n  def zero_grad(self):\n    for layer_data in self.layers_data_list:\n      for p in layer_data['params']:\n        if p.grad is not None:\n          p.grad.zero_()\n<\/code>\n<code class=\"lang-auto\">the training part isnt so unusual, and it worked on different notebooks that I used :\n\ndef train_and_eval(optimizer,net,optimizer_step,GPU=False):\n  loss_function = nn.CrossEntropyLoss()\n  epochs=100\n  train_loss_per_epoch=[]\n  test_loss_per_epoch=[]\n  for epoch in range(epochs) :\n    print(\"[train]-----epoch \"+str(epoch+1)+\" -----\")\n    train_loss = 0.0\n    test_loss = 0.0\n    running_loss = 0.0\n    for i, data in enumerate(trainloader, 0):\n        # get inputs from data \n        inputs, labels = data\n\n        if GPU:\n            inputs = inputs.cuda()  # -- For GPU\n            labels = labels.cuda()  # -- For GPU\n\n        optimizer.zero_grad()\n        outputs = net(inputs)\n        loss = loss_function(outputs, labels)\n        loss.backward()\n        if optimizer_step == \"sgd\":\n          optimizer.step_sgd()\n\n        train_loss += loss.item()\n\n        # print statistics\n        running_loss += loss.item()\n        if (i + 1) % 200 == 0:\n            print('[%d, %5d] loss: %.3f' % (epoch + 1, i + 1, running_loss \/ 200))\n            running_loss = 0.0\n\n    train_loss_per_epoch.append(train_loss \/ len(trainloader))\n    print('[%d] train loss: %.3f' %\n          (epoch + 1, train_loss \/ len(trainloader)))\n\n    # test\n    print(\"[test]-----epoch \"+str(epoch+1)+\" -----\")\n    for i, data in enumerate(testloader, 0):\n\n        # get inputs from data \n        inputs, labels = data\n\n        if GPU:\n            inputs = inputs.cuda()  # -- For GPU\n            labels = labels.cuda()  # -- For GPU\n\n        outputs = net(inputs)\n        loss = loss_function(outputs, labels)\n        test_loss += loss.item()\n    test_loss_per_epoch.append(test_loss \/ len(testloader))\n    print('[%d] test loss: %.3f' %\n          (epoch + 1, test_loss \/ len(testloader)))\n  \n  return train_loss_per_epoch,test_loss_per_epoch\n\ndef runOptimizerTest(optimizer_step,GPU):\n  if GPU :\n    net=CNN().cuda()\n  else :\n    net=CNN()\n  optimizer=MyOptimizer(net.parameters())\n  return train_and_eval(optimizer,net,optimizer_step,GPU)\n \n \noptimizer_step=\"sgd\"\ntrain_loss_per_epoch_sgd,test_loss_per_epoch_sgd=runOptimizerTest(optimizer_step,GPU=False)\n<\/code>\nthanks","y":"So the problem is that layer_dict['params'] is not a list, but just a Tensor.\nSo when you do for p in layer_data['params']:, you actually slice the Tensor along the 0th dimension. Since this returns new Tensors, the .grad field is not set for them.\nYou can either do layer_dict['params']=[layer_params,] or replace for p in layer_data['params']: by p =  layer_data['params'].","z":"Don\u2019t use the .data attribute, as the manipulation of the underlying data might yield some side effects.\nIf you want to check the gradient of a parameter, you could access it via print(param.grad).\nAlso, you are mixing p.data.grad and p.grad.data. \nTo add to the comment above, you want to do the sgd step within with torch.no_grad(): block instead of using .data.\nFirst of all thank u for the fast response.\nSo I changed my if and I\u2019m checking  if p.grad is None: but it seems that the grad still is None. I added also a print inside the if, and the result is the same.\nCan you give a bit more code?\nIn particular, do you send the net to cuda after giving the parameters to the optimizer for example?\nDo you have a complete example that we can run to check this?\nI added the whole code I used.\nSo the problem is that layer_dict['params'] is not a list, but just a Tensor.\nSo when you do for p in layer_data['params']:, you actually slice the Tensor along the 0th dimension. Since this returns new Tensors, the .grad field is not set for them.\nYou can either do layer_dict['params']=[layer_params,] or replace for p in layer_data['params']: by p =  layer_data['params'].\ninstead of using for p in layer_data['params'] I worked on the tensor level -> p=layer_data['params'] as u suggested.\n<code class=\"lang-auto\">  def zero_grad(self):\n    for layer_data in self.layers_data_list:\n      if layer_data['params'].grad is not None:\n        layer_data['params'].grad.zero_()\n\n  def step_sgd(self):\n    '''Stochastic gradient descent'''\n    for layer_data in self.layers_data_list:\n      p=layer_data['params']\n      lr=layer_data['lr']\n      d_p=p.grad.data\n      p.data.add_(-lr,d_p)\n<\/code>\nit seems that now I\u2019m in overfitting state. Still, I\u2019l be happy if u can explain why I cant iterate over the tensor and run p.grad per weight and instead we do this action on the tensor level :tensor.grad\nIt is because the .grad field is associated with the Tensor, not an entry in the Tensor.\nSo if you index the Tensor, you get some entries into it, but these are a new Tensor, and so with a different .grad field.\nBut the entries inside the tensor got .grad func. I thought that its better to work on the tensor level in order to benefit from GPU performance (matrix multiplication) but it seems that I\u2019m wrong. Why then each of the entires in the tensor has .grad func ?\nI did the changes that u suggested, now indeed I see that the grads are set and not None. However , it seems that now I\u2019m getting overfitting very fast and my test loss increases \n[1] train loss: 2.105\n[1] test loss: 2.390\n[2] train loss: 1.235\n[2] test loss: 5.240\n[3] train loss: 0.511\n[3] test loss: 7.921\n[4] train loss: 0.167\n[4] test loss: 10.697\n[5] train loss: 0.033\n[5] test loss: 11.992\nand so on\u2026\n.grad_fn is very different from .grad. You have a .grad_fn because this new Tensor was obtained in a differentiable way (using the slicing operation on the original one).\nYou indeed want to work with the full Tensor, not the individual elements !\nFor the overfitting, you can try to increase the training set size, l2 regularization, momentum, etc (the usual suspects :D)\nThank you for all the help !"},{"x":"I have not had much experience with RNNs and have been  looking at some examples in the  pytorch repository and I have a question about the example provided here: https:\/\/github.com\/pytorch\/examples\/blob\/master\/word_language_model\/model.py\nIn this example, the RNNModel forward function looks as follows:\n<code class=\"lang-auto\">def forward(self, input, hidden):\n        emb = self.drop(self.encoder(input))\n        output, hidden = self.rnn(emb, hidden)\n        output = self.drop(output)\n        decoded = self.decoder(output)\n        return decoded, hidden\n<\/code>\nI have 2 questions:\n1: Why is the output being used in the decoder step? I thought the decoder should take the hidden state as input? Should this not be self.decoder(hidden)?\n2: Assuming there is an explanation for (1), why is usually the dropout used on the output? I am guessing the output here is a tensor. So, how is dropout applied on a tensor? I thought dropout is a property of the weights?\nI hope my questions make some sense.","y":"Well, I had a look at the code. While I\u2019m not familiar with this setup for a Language Model (LM) either, a look at the training data made a bit clearer to me. Still, everything that follows are not much more than educated guesses.\nMost fundamentally, an LM aims to predicts the next word given an input sequence of words. Most LM architectures reflect this \u201cdirectly\u201d in the training date. For example, given a long document like \u201cA B C D E F G H I J K L M N O P \u2026\u201d, the training data (input => output) is generates as (assuming an input sequence of 5)\nA B C D E => F\nB C D E F => G\nC D E F G => H\nD E F G H => I\n...\n\nSo yeah, in this case you would simple pump input through the RNN and use the hidden state as input for the final linear layers, e.g.:\noutput, hidden = self.rnn(emb, hidden)\n# optional dropout or what not\ndecoded = self.decoder(hidden)\n\nThat essentially simply an RNN-based classifier where the classes are all the word in the vocabulary.\nThe architecture here works differently. If I understand in example\/main.py currectly, the training data is generated differently:\nA B C D E => B C D E F\n...\n\n(the structure of subsequent training examples) depend on the batch size; check the methods batchify() and get_batch(). Anyway, the output sequence is the input sequence shifted by one word to the left.\nNo they treat the whole training as a sequence labeling task, which is kind of a prediction for\/after each time time step and not just the last. As  said, output[i] is the hidden state of step i. I always use the figure below to check if I use the right return values of an LSTM\/GRU:\nlstm-output-vs-hidden640\u00d7548 20.9 KB with link \"https:\/\/discuss.pytorch.org\/uploads\/default\/original\/2X\/e\/e7496a33d835f085d800ee17c0ade05895a89551.png\"\nAnyway, that\u2019s how understand the proposed architecture. I assume it yields better results compared to more traditional architectures. I hope that helps\u2026well, at least gives some food for thoughts.","z":"\n\noutput will contain the output features of the last layer in your rnn for all time steps, while hidden will contain the features for the last step.\nThis small code example shows the equality:\n\n<code class=\"lang-python\">rnn = nn.RNN(10, 20)\ninput = torch.randn(5, 3, 10)\nh0 = torch.randn(1, 3, 20)\noutput, hn = rnn(input, h0)\n\nprint((output[-1] == hn).all())\n> tensor(true)\n<\/code>\n\nDropout will drop output activations randomly as shown here:\n\n<code class=\"lang-python\">lin = nn.Linear(10, 10)\nx = torch.randn(1, 10)\nout = lin(x)\nprint(out)\nprint(F.dropout(out, 0.5))\n<\/code>\nThank you for your answer! This really clarifies a few things for me. Many thanks.\nLooking at the code again last night, there is also one more thing that stuck out. Usually these auto encoder like things have a RNN for encoding and another one for decoding (recon). However, in this code, the decoder is just a linear layer. I wonder why that is?\n<code class=\"lang-auto\">self.decoder = nn.Linear(nhid, ntoken)\n....\ndecoded = self.decoder(output)\n<\/code>\nWhy should the reconstruction be a linear combination of the output? I am trying to wonder what this pytorch example is trying to achieve  with this? I also ask because I have seen similar things in RNN based anomaly detection but am confused about the role of this linear decoder layer.\nI\u2019m not sure about the architecture decision for this model and I would guess  might know more about this particular decoding strategy. \nHowever, if you are looking for an encoder and decoder model with RNNs in both parts, have a look at the Seq2Seq tutoorial with link \"https:\/\/pytorch.org\/tutorials\/intermediate\/seq2seq_translation_tutorial.html\" \nWell, I had a look at the code. While I\u2019m not familiar with this setup for a Language Model (LM) either, a look at the training data made a bit clearer to me. Still, everything that follows are not much more than educated guesses.\nMost fundamentally, an LM aims to predicts the next word given an input sequence of words. Most LM architectures reflect this \u201cdirectly\u201d in the training date. For example, given a long document like \u201cA B C D E F G H I J K L M N O P \u2026\u201d, the training data (input => output) is generates as (assuming an input sequence of 5)\nA B C D E => F\nB C D E F => G\nC D E F G => H\nD E F G H => I\n...\n\nSo yeah, in this case you would simple pump input through the RNN and use the hidden state as input for the final linear layers, e.g.:\noutput, hidden = self.rnn(emb, hidden)\n# optional dropout or what not\ndecoded = self.decoder(hidden)\n\nThat essentially simply an RNN-based classifier where the classes are all the word in the vocabulary.\nThe architecture here works differently. If I understand in example\/main.py currectly, the training data is generated differently:\nA B C D E => B C D E F\n...\n\n(the structure of subsequent training examples) depend on the batch size; check the methods batchify() and get_batch(). Anyway, the output sequence is the input sequence shifted by one word to the left.\nNo they treat the whole training as a sequence labeling task, which is kind of a prediction for\/after each time time step and not just the last. As  said, output[i] is the hidden state of step i. I always use the figure below to check if I use the right return values of an LSTM\/GRU:\nlstm-output-vs-hidden640\u00d7548 20.9 KB with link \"https:\/\/discuss.pytorch.org\/uploads\/default\/original\/2X\/e\/e7496a33d835f085d800ee17c0ade05895a89551.png\"\nAnyway, that\u2019s how understand the proposed architecture. I assume it yields better results compared to more traditional architectures. I hope that helps\u2026well, at least gives some food for thoughts.\nThank you for such a detailed reply. This really clears a lot of doubts!"},{"x":"While training semantic segmentation  i had 23 classes including Ignore label, during my prediction i get only 22 classes. I have used cross entropy loss and provided ignore index as 255.\nWhat looks from the predicted image is one of the class (class_name:vegetation, id:21)  merge with ignore index as seen in the below image.\nBelow image shows the Unique values for Ground truth and Prediction along with Original, GT  &amp; Predicted image.\nPlease help with what may have caused this!!!\nScreenshot from 2020-01-10 18-08-211154\u00d7336 115 KB with link \"https:\/\/discuss.pytorch.org\/uploads\/default\/original\/3X\/7\/a\/7a8b44a08f9f1b78e4e78213f4665cce28737118.png\"","y":"This would explain, why your model predicts random classes for these pixel locations.\nBasically, your model will not get any information for these locations, as they are ignored, and can just output any random class. In your case it seems the vegetation class was picked.\nYou could transform all 255 class values to an \u201cunknown\u201d or background class and let the model learn this additional class instead.","z":"What is the shape of your output activation?\nIn a segmentation use case, your output activation will most likely have the shape [batch_size, nb_classes, height, width].\nIf nb_classes was defined as 22, you will only get these labels.\nThanks for replying  , I am using deeplab v3+ architecture and my segmentation output shape is in [batch_size, nb_classes, height, width] format. I just printed out the unique values to check whether ignore index is their or not.\nDuring training output tensor is of shape [batch, 23, 512, 512] after that i calculated the labels using _, pred = torch.max(scores, dim=1) .List of unique values of pred contain 22 (which is ignore index),\nBut at inference time when i use the trained weights, output is of shape [batch, 23, 512, 512] and the the unique values contains max label upto 21.\nIt seems that the model output contains the background class + all other 22 classes.\nIf I understand your work flow correctly, you have passed the background class index as ignore_index, so that your model never learned how to predict the background?\nIf that\u2019s the case, I would expect the model never to output the background class during inference either.\nFYI i am using appoloscapes dataset, below are the labels available and in the trainID column we don\u2019t have any classes for background. So i am using ignore index + 22 other classes.\nFor the architecture, loss i am passing below argument:\n<code class=\"lang-auto\">model = DeepLab (num_classes=23,backbone=resnet101,output_stride=16)\ncriterion = nn.CrossEntropyLoss(ignore_index=255)\n\n<\/code>\n<code class=\"lang-auto\">labels = [\n    #     name                    clsId    id   trainId   category  catId  hasInstanceignoreInEval   color\n    Label('others'              ,    0 ,    0,   255   , '\u5176\u4ed6'    ,   0  ,False , True  , 0x000000 ),\n    Label('rover'               , 0x01 ,    1,   255   , '\u5176\u4ed6'    ,   0  ,False , True  , 0X000000 ),\n    Label('sky'                 , 0x11 ,   17,    0    , '\u5929\u7a7a'    ,   1  ,False , False , 0x4682B4 ),\n    Label('car'                 , 0x21 ,   33,    1    , '\u79fb\u52a8\u7269\u4f53',   2  ,True  , False , 0x00008E ),\n    Label('car_groups'          , 0xA1 ,  161,    1    , '\u79fb\u52a8\u7269\u4f53',   2  ,True  , False , 0x00008E ),  \n    Label('motorbicycle'        , 0x22 ,   34,    2    , '\u79fb\u52a8\u7269\u4f53',   2  ,True  , False , 0x0000E6 ),\n    Label('motorbicycle_group'  , 0xA2 ,  162,    2    , '\u79fb\u52a8\u7269\u4f53',   2  ,True  , False , 0x0000E6 ),\n    Label('bicycle'             , 0x23 ,   35,    3    , '\u79fb\u52a8\u7269\u4f53',   2  ,True  , False , 0x770B20 ),\n    Label('bicycle_group'       , 0xA3 ,  163,    3    , '\u79fb\u52a8\u7269\u4f53',   2  ,True  , False , 0x770B20 ),\n    Label('person'              , 0x24 ,   36,    4    , '\u79fb\u52a8\u7269\u4f53',   2  ,True  , False , 0x0080c0 ),\n    Label('person_group'        , 0xA4 ,  164,    4    , '\u79fb\u52a8\u7269\u4f53',   2  ,True  , False , 0x0080c0 ),\n    Label('rider'               , 0x25 ,   37,    5    , '\u79fb\u52a8\u7269\u4f53',   2  ,True  , False , 0x804080 ),\n    Label('rider_group'         , 0xA5 ,  165,    5    , '\u79fb\u52a8\u7269\u4f53',   2  ,True  , False , 0x804080 ),\n    Label('truck'               , 0x26 ,   38,    6    , '\u79fb\u52a8\u7269\u4f53',   2  ,True  , False , 0x8000c0 ),\n    Label('truck_group'         , 0xA6 ,  166,    6    , '\u79fb\u52a8\u7269\u4f53',   2  ,True  , False , 0x8000c0 ), \n    Label('bus'                 , 0x27 ,   39,    7    , '\u79fb\u52a8\u7269\u4f53',   2  ,True  , False , 0xc00040 ),\n    Label('bus_group'           , 0xA7 ,  167,    7    , '\u79fb\u52a8\u7269\u4f53',   2  ,True  , False , 0xc00040 ),\n    Label('tricycle'            , 0x28 ,   40,    8    , '\u79fb\u52a8\u7269\u4f53',   2  ,True  , False , 0x8080c0 ),\n    Label('tricycle_group'      , 0xA8 ,  168,    8    , '\u79fb\u52a8\u7269\u4f53',   2  ,True  , False , 0x8080c0 ),\n    Label('road'                , 0x31 ,   49,    9    , '\u5e73\u9762'    ,   3  ,False , False , 0xc080c0 ),\n    Label('siderwalk'           , 0x32 ,   50,    10   , '\u5e73\u9762'    ,   3  ,False , False , 0xc08040 ),\n    Label('traffic_cone'        , 0x41 ,   65,    11   , '\u8def\u95f4\u969c\u788d',   4  ,False , False , 0x000040 ),\n    Label('road_pile'           , 0x42 ,   66,    12   , '\u8def\u95f4\u969c\u788d',   4  ,False , False , 0x0000c0 ),\n    Label('fence'               , 0x43 ,   67,    13   , '\u8def\u95f4\u969c\u788d',   4  ,False , False , 0x404080 ),\n    Label('traffic_light'       , 0x51 ,   81,    14   , '\u8def\u8fb9\u7269\u4f53',   5  ,False , False , 0xc04080 ),\n    Label('pole'                , 0x52 ,   82,    15   , '\u8def\u8fb9\u7269\u4f53',   5  ,False , False , 0xc08080 ),\n    Label('traffic_sign'        , 0x53 ,   83,    16   , '\u8def\u8fb9\u7269\u4f53',   5  ,False , False , 0x004040 ),\n    Label('wall'                , 0x54 ,   84,    17   , '\u8def\u8fb9\u7269\u4f53',   5  ,False , False , 0xc0c080 ),\n    Label('dustbin'             , 0x55 ,   85,    18   , '\u8def\u8fb9\u7269\u4f53',   5  ,False , False , 0x4000c0 ),\n    Label('billboard'           , 0x56 ,   86,    19   , '\u8def\u8fb9\u7269\u4f53',   5  ,False , False , 0xc000c0 ),\n    Label('building'            , 0x61 ,   97,    20   , '\u5efa\u7b51'    ,   6  ,False , False , 0xc00080 ),\n    Label('bridge'              , 0x62 ,   98,    255  , '\u5efa\u7b51'    ,   6  ,False , True  , 0x808000 ),\n    Label('tunnel'              , 0x63 ,   99,    255  , '\u5efa\u7b51'    ,   6  ,False , True  , 0x800000 ),\n    Label('overpass'            , 0x64 ,  100,    255  , '\u5efa\u7b51'    ,   6  ,False , True  , 0x408040 ),\n    Label('vegatation'          , 0x71 ,  113,    21   , '\u81ea\u7136'    ,   7  ,False , False , 0x808040 ),\n    Label('unlabeled'           , 0xFF ,  255,    255  , '\u672a\u6807\u6ce8'  ,   8  ,False , True  , 0xFFFFFF ),\n]\n<\/code>\nThanks for the information.\nCould you check, what the white color represents in your ground truth image?\nI would assume it\u2019s class0 (sky), but that wouldn\u2019t make sense, as part of the buildings and car is also marked as this color.\nwhat you told is correct ,actually all the white color in the image is ignore index. Sky, building and host vehicle all should have corresponding classes but the dataset provided by them labels some parts as 255.\nIs their any workaround?\nThis would explain, why your model predicts random classes for these pixel locations.\nBasically, your model will not get any information for these locations, as they are ignored, and can just output any random class. In your case it seems the vegetation class was picked.\nYou could transform all 255 class values to an \u201cunknown\u201d or background class and let the model learn this additional class instead.\nIt worked, i trained my model for 10 epoch by adding all the ignore labels to extra class(class id:22). I have attached the results i got after 10 epochs.\nThanks  for your support, i have closed the issue with your last answer.\nScreenshot from 2020-01-13 18-32-21951\u00d7330 112 KB with link \"https:\/\/discuss.pytorch.org\/uploads\/default\/original\/3X\/2\/3\/23f7077cf7e173bd698a8e825d8c95da90c996ac.png\""},{"x":"  I referred to your discussion to map my multiclass segmentation problem. The output shape of my network matches that of my input i.e [8, 4, 572, 572] 8 is my batch size and 4 is  the number of classes in my image including the background. I tried this code snippet to visualize my network output\noutput = model(images)\nprint(output.size()) #*[8,4,572,572]\nprob = F.softmax(output, 1)\nprint(prob.size())# [8,4,572,572] outputs match\nprob_imgs = torchvision.utils.make_grid(output.permute(1, 0, 2, 3))\nprint(prob_imgs.size()) #RuntimeError: The size of tensor a (3) must match the size of tensor b (8) at non-singleton dimension 0\nI dont know where I am wrong. Could someone help me out.","y":"update: output = model(images)\nprint(output.size())\npred = torch.argmax(output, dim=0).detach().cpu()\nplt.imshow(pred.permute(1,2,0))\nplt.show()\nThis code snippet solved this issue","z":"update: output = model(images)\nprint(output.size())\npred = torch.argmax(output, dim=0).detach().cpu()\nplt.imshow(pred.permute(1,2,0))\nplt.show()\nThis code snippet solved this issue"},{"x":"I have two tensors A and B. A is a float tensor with shape (batch size, hidden dim). B is a Long tensor with shape (batch size, data len). What I want is somewhat like A[:, B], a float tensor still with shape (batch size, data len), the elements are certain indices from A which depends on B.\nAn example would be A=[[5, 2, 6], [7, 3, 4]] and B=[[0, 2, 1, 1], [2, 2, 1, 0]]. Then what I want is a tensor [[5, 6, 2, 2], [4, 4, 3, 7]]. Is there any way to achieve this?\nI have tried A[:, B], but what I achieve is a tensor with shape (batch size, batch size, data len), which is a large tensor. And I only want the \u201cdiagonal value\u201d from this tensor.","y":"<code class=\"lang-python\">import torch\nimport torch.nn as nn\n\nA=torch.tensor([[5, 2, 6], [7, 3, 4]]).float()  \nB=torch.tensor([[0, 2, 1, 1], [2, 2, 1, 0]]).long()\nresult = torch.gather(A, 1, B)\nprint(result)\n<\/code>","z":"<code class=\"lang-python\">import torch\nimport torch.nn as nn\n\nA=torch.tensor([[5, 2, 6], [7, 3, 4]]).float()  \nB=torch.tensor([[0, 2, 1, 1], [2, 2, 1, 0]]).long()\nresult = torch.gather(A, 1, B)\nprint(result)\n<\/code>\nThanks for the reply! How about when A has another dimension, now the shape of A is (batch size, hidden dim, data dim). And B is still the same, a long tensor with shape (batch size, data len). I actually want to have the resulting tensor with shape (batch size, data len, data dim).\nFor example, A=[[[5], [2], [6]], [[7], [3], [4]]] and B=[[0, 2, 1, 1], [2, 2, 1, 0]]. Then what I want is a tensor [[[5], [6], [2], [2]], [[4], [4], [3], [7]]]. Basically now data dim is 1 as an example.\nIt seems that torch.gather(A, 1, B) requires A and B to have the same shapes.\nIn this case, just expand B as the same shape of A in order to solve the problem."},{"x":"I want to make a Discriminator (GAN) and below is my code,\n<code class=\"lang-auto\">  def __init__(self):\n        super(_D, self).__init__()\n        self.conv1 = nn.Conv2d(3, 128, kernel_size=3, stride=2, padding=1)\n        self.batchNorm1 = nn.BatchNorm2d(128)\n        self.leakyReLU1 = nn.LeakyReLU(negative_slope=0.2, inplace=True)\n       \n        self.conv2 = nn.Conv2d(128, 256, kernel_size=3, stride=2, padding=1)\n        self.batchNorm2 = nn.BatchNorm2d(256)\n        self.leakyReLU2 = nn.LeakyReLU(negative_slope=0.2, inplace=True)\n\n        self.conv3 = nn.Conv2d(256, 512, kernel_size=3, stride=2, padding=1)\n        self.batchNorm3 = nn.BatchNorm2d(512)\n        self.leakyReLU3 = nn.LeakyReLU(negative_slope=0.2, inplace=True)\n\n        self.conv4 = nn.Conv2d(512, 256, kernel_size=3, stride=2, padding=1)\n        self.batchNorm4 = nn.BatchNorm2d(256)\n        self.leakyReLU4 = nn.LeakyReLU(negative_slope=0.2, inplace=True)\n\n        self.conv5 = nn.Conv2d(256, 128, kernel_size=3, stride=2, padding=1)\n        self.batchNorm5 = nn.BatchNorm2d(128)\n        self.leakyReLU5 = nn.LeakyReLU(negative_slope=0.2, inplace=True)\n\n        self.conv6 = nn.Conv2d(128, 3, kernel_size=3, stride=1)\n\n    def forward(self, input):\n        print()\n        x = self.leakyReLU1(self.batchNorm1(self.conv1(input)))\n        print(x.size())\n        x = self.leakyReLU2(self.batchNorm2(self.conv2(x)))\n        print(x.size())\n        x = self.leakyReLU3(self.batchNorm3(self.conv3(x)))\n        print(x.size())\n        x = self.leakyReLU4(self.batchNorm4(self.conv4(x)))\n        print(x.size())\n        x = self.leakyReLU5(self.batchNorm5(self.conv5(x)))\n        print(x.size())\n        x = self.conv6(x)\n<\/code>\nthere is an error  before   x = self.conv6(x). and it\u2019s said that\n\nExpected input batch_size(18) to match target batch_size (32)\n\nI am not really sure how to determine the batch size of my convolution, but the last thing I try is to use batch_size=2  and the num_workers=2, and here are the tensors\u2019 size that I print after every convolution block\ntorch.Size([2, 3, 152, 152])\ntorch.Size([2, 128, 76, 76])\ntorch.Size([2, 256, 38, 38])\ntorch.Size([2, 512, 19, 19])\ntorch.Size([2, 256, 10, 10])\ntorch.Size([2, 128, 5, 5])","y":"The error message seems to point to the loss calculation.\nCould you check the shape of your model output and target and make sure the batch size is equal?","z":"The error message seems to point to the loss calculation.\nCould you check the shape of your model output and target and make sure the batch size is equal?\nThank you for your reply, my target is torch.Size([32]), and now I\u2019m making it equal by using 32, here are the shapes for the tensors\u2019 size after every convolution block\ntorch.Size([32, 3, 152, 152])\ntorch.Size([32, 128, 76, 76])\ntorch.Size([32, 256, 38, 38])\ntorch.Size([32, 512, 19, 19])\ntorch.Size([32, 256, 10, 10])\ntorch.Size([32, 128, 5, 5])\nnow I use 32 as the batch_size and it gives me error\n\nExpected input batch_size (288) to match target batch_size (32)\n\nHow to counter this? Thank you very much once again\nI think I get the problem, you are right, because the channel of my target output and the model output is different, I need to make it the same. Thank you very much for your answer."},{"x":"HI guys,\nIn the context of semantic segmentation (19 classes) my model output a tensor of size :\n(b, 19, h, w).\nI need to merge some classes to get a tensor of size (b, 10, h, w) with respect to a mapping dictionary:\ndic = {9:[0,1], 8:[3,4], 7:[9,10,11], 6:[6,7,8], 5:[2,5], 4:[18,17,16], 3:[4,5,6], 2:[12,13], 1:[15], 0:[14]}\nWhat is the most efficient way to do that ?\nThanks !","y":"I\u2019m not sure, if you could use an alternative to a simple for loop \n<code class=\"lang-python\">b, h, w = 2, 4, 4\nx = torch.randn(b, 19, h, w)\nz = torch.zeros(b, 10, h, w)\n\ndic = {9:[0,1], 8:[3,4], 7:[9,10,11], 6:[6,7,8], 5:[2,5], 4:[18,17,16], 3:[4,5,6], 2:[12,13], 1:[15], 0:[14]}\n\nfor key in dic:\n    d = dic[key]\n    z[:, key] = x[:, d].sum()\n<\/code>","z":"I\u2019m not sure, if you could use an alternative to a simple for loop \n<code class=\"lang-python\">b, h, w = 2, 4, 4\nx = torch.randn(b, 19, h, w)\nz = torch.zeros(b, 10, h, w)\n\ndic = {9:[0,1], 8:[3,4], 7:[9,10,11], 6:[6,7,8], 5:[2,5], 4:[18,17,16], 3:[4,5,6], 2:[12,13], 1:[15], 0:[14]}\n\nfor key in dic:\n    d = dic[key]\n    z[:, key] = x[:, d].sum()\n<\/code>\nHi !\nThank you for your answer, yes I don\u2019t see any other alternative I will go for the for loop then.\nThanks"},{"x":"As far as I know, CrossEntropy() equals to\ntorch.mean(torch.sum(-target * nn.LogSoftmax(input), dim=1))\nHow can I express nn.LogSoftmax in equation?\nnn.LogSoftmax  equals to torch.mean(torch.sum(F.softmax(input)*torch.log(F.softmax(input)),1) ?","y":"<code class=\"lang-python\">import torch\nimport torch.nn as nn\nimport torch.nn.functional as F\n\n\nx = torch.rand(4, 5) * 100\ny_1 = nn.LogSoftmax(dim=-1)(x)\ny_2 = torch.log(F.softmax(x, dim=-1))\n\nprint((y_1 - y_2).abs().max())  # 3e-8\n<\/code>\nSee here with link \"https:\/\/pytorch.org\/docs\/stable\/nn.html?highlight=logsoftmax#torch.nn.LogSoftmax\".","z":"<code class=\"lang-python\">import torch\nimport torch.nn as nn\nimport torch.nn.functional as F\n\n\nx = torch.rand(4, 5) * 100\ny_1 = nn.LogSoftmax(dim=-1)(x)\ny_2 = torch.log(F.softmax(x, dim=-1))\n\nprint((y_1 - y_2).abs().max())  # 3e-8\n<\/code>\nSee here with link \"https:\/\/pytorch.org\/docs\/stable\/nn.html?highlight=logsoftmax#torch.nn.LogSoftmax\"."},{"x":"I am using the following code to load the MNIST dataset:\n<code class=\"lang-auto\">batch_size = 64\ntrain_loader = torch.utils.data.DataLoader(\n    datasets.MNIST('..\/data', train=True, download=True,\n                   transform=transforms.Compose([\n                       transforms.ToTensor()\n                   ])),\n    batch_size=batch_size)\n<\/code>\nIf I try to load one batch:\n<code class=\"lang-auto\">for data, target in train_loader:\n    print(data.shape)\n    break\n<\/code>\nthis prints:\n<code class=\"lang-auto\">torch.Size([64, 1, 28, 28])\n<\/code>\nWhat I don\u2019t understand is why the size is 64 x 1 x 28 x 28 instead of just 64 x 28 x 28? What is this extra dimension of length 1?","y":"dim1 represents the channel dimension. Since MNIST uses grayscale images, the channel dim has a value of 1.\nnn.Conv2d and other 2-dimensional layers expect an input of [batch_size, channels, height, width].","z":"dim1 represents the channel dimension. Since MNIST uses grayscale images, the channel dim has a value of 1.\nnn.Conv2d and other 2-dimensional layers expect an input of [batch_size, channels, height, width]."},{"x":"I want to use VGG16 network for transfer learning. Following the transfer learning tutorial with link \"https:\/\/pytorch.org\/tutorials\/beginner\/transfer_learning_tutorial.html\", which is based on the Resnet network, I want to replace the lines:\n<code class=\"lang-auto\">model_ft = models.resnet18(pretrained=True)\nnum_ftrs = model_ft.fc.in_features\nmodel_ft.fc = nn.Linear(num_ftrs, 2)\noptimizer_ft = optim.SGD(model_ft.parameters(), lr=0.001, momentum=0.9)\n<\/code>\nwith their equivalent for VGG16.\nMy attempt is:\n<code class=\"lang-auto\">model_ft = models.vgg16(pretrained=True)\nnum_ftrs = model_ft.fc.in_features\nmodel_ft.fc = nn.Linear(num_ftrs, 2)\noptimizer_ft = optim.SGD(model_ft.parameters(), lr=0.001, momentum=0.9)\n<\/code>\nwhere, as far as I understand, the two lines in the middle are required in order to replace the classification process (from 10 classes, to 2).  The problem is that the VGG16 class does not contain a \u201c.fc\u201d attribute, so running these lines results in an error.\nWhat is the best way by which I can replace the corresponding lines in the Resnet transfer learning?\nSince I am new in Pytorch (and Machine learning in general), any further (relevant) details regarding the structure of the VGG16 class (even details that are not necessarily required for the specific implementation I requested) will be gratefully appreciated.\nThanks!","y":"For VGG16 you would have to use model_ft.classifier. You can find the corresponding code here with link \"https:\/\/github.com\/pytorch\/vision\/blob\/master\/torchvision\/models\/vgg.py#L29\".\nHere is a small example how to reset the last layer. Of course you could also replace the whole classifier, if that\u2019s what you wish.\n<code class=\"lang-python\">model = models.vgg16(pretrained=False)\nmodel.classifier[-1] = nn.Linear(in_features=4096, out_features=num_classes)\n<\/code>","z":"For VGG16 you would have to use model_ft.classifier. You can find the corresponding code here with link \"https:\/\/github.com\/pytorch\/vision\/blob\/master\/torchvision\/models\/vgg.py#L29\".\nHere is a small example how to reset the last layer. Of course you could also replace the whole classifier, if that\u2019s what you wish.\n<code class=\"lang-python\">model = models.vgg16(pretrained=False)\nmodel.classifier[-1] = nn.Linear(in_features=4096, out_features=num_classes)\n<\/code>\nMany thanks ptrblck! For future reference, I also found this really helpful tutorial:\nhttps:\/\/www.kaggle.com\/carloalbertobarbano\/vgg16-transfer-learning-pytorch\nI have a similar question, but for the fcn resnet 101 segmentation model. In my case I am following this tutorial with link \"https:\/\/pytorch.org\/tutorials\/beginner\/finetuning_torchvision_models_tutorial.html#model-training-and-validation-code\" and I am trying to adapt this part of the code to fcn resnet 101.\nThe code from the tutorial is:\n<code class=\"lang-auto\">    if model_name == \"resnet\":\n        \"\"\" Resnet18\n        \"\"\"\n        model_ft = models.resnet18(pretrained=use_pretrained)\n        set_parameter_requires_grad(model_ft, feature_extract)\n        num_ftrs = model_ft.fc.in_features\n        model_ft.fc = nn.Linear(num_ftrs, num_classes)\n        input_size = 224\n<\/code>\nand so far I have changed it to:\n<code class=\"lang-auto\">if model_name == \"resnet\":\n\n        \"\"\" FCN_resnet101\n        \"\"\"\n        model_ft = models.segmentation.fcn_resnet101(pretrained=use_pretrained)\n        set_parameter_requires_grad(model_ft, feature_extract)\n        num_ftrs = model_ft.fc.in_features\n        model_ft.fc = nn.Linear(num_ftrs, num_classes)\n        input_size = 768, 1024\n<\/code>\nWhen I do this I get this error: \u2018FCN\u2019 object has no attribute \u2018fc\u2019\nSo I was wondering how I can change the two lines below to work with the fcn segmentation model\n<code class=\"lang-auto\">        num_ftrs = model_ft.fc.in_features\n        model_ft.fc = nn.Linear(num_ftrs, num_classes)\n<\/code>\nSince this is a segmentation model, the output layer would be a conv layer instead of a linear one.\nIf you want to train your model from scratch, you could just use the num_classes argument:\n<code class=\"lang-python\">modelA = models.segmentation.fcn_resnet101(\n    pretrained=False, num_classes=2)\n<\/code>\nOn the other hand, if you just want to use the pretrained model and create a new classification layer, you could use:\n<code class=\"lang-python\">modelB = models.segmentation.fcn_resnet101(pretrained=True)\nmodelB.classifier[4] = nn.Conv2d(512, num_classes, 1, 1)\n<\/code>\nThanks!\nI am fine tuning a pretrained model with my own data, so the second method would work for me.\nOne follow up question:\nWouldn\u2019t I have to fetch the number of in_channels of the existing pre-trained model, similarly to how  its done in the example with \u2018num_ftrs\u2019?\nSo in the tutorial there is this line before creating a new layer:\nnum_ftrs = model_ft.fc.in_features\nWould the equivalent for segmentation be the line below?:\nin_chnls =  modelB.classifier[4].in_channels\nAnd then:\nmodelB.classifier[4] = nn.Conv2d(in_chnls, num_classes, 1, 1)\nYes, that would be the corresponding code.\nYou could also get the kernel_size and stride which are set as 1 in my code example.\nThanks! I am getting this part to work now!"},{"x":"I\u2019m using torch of version 1.3.1+cpu and torchvision version of 0.4.2+cpu.\nI\u2019m using the VAE example in master ( 0c1654d with link \"https:\/\/github.com\/pytorch\/examples\/commit\/0c1654d6913f77f09c0505fb284d977d89c17c1a\") but the interpreter results in this warning:\n<code class=\"lang-auto\">\/home\/gon1332\/Development\/Training\/ML\/learning-data-augmentation\/model.py:147: UserWarning: Using a target size (torch.Size([128, 784])) that is different to the input size (torch.Size([128, 20])) is deprecated. Please ensure they have the same size.\n  BCE = F.binary_cross_entropy(recon_x, x.view(-1, 784), reduction='sum')\nTraceback (most recent call last):\n  File \"main.py\", line 38, in <module>\n    main()\n  File \"main.py\", line 34, in main\n    ...\n    BCE = F.binary_cross_entropy(recon_x, x.view(-1, 784), reduction='sum')\n  File \"-\/.local\/lib\/python3.6\/site-packages\/torch\/nn\/functional.py\", line 2058, in binary_cross_entropy\n    \"!= input nelement ({})\".format(target.numel(), input.numel()))\nValueError: Target and input must have the same number of elements. target nelement (100352) != input nelement (2560)\n<\/code>\nx and x_recon have the below sizes:\n<code class=\"lang-auto\">x_recon.size() = torch.Size([128, 20])\nx.size() = torch.Size([128, 1, 28, 28]) and with x.view(-1, 128) torch.Size([128, 784])\n<\/code>","y":"The printed shapes correspond to the shape mismatch mentioned in the error message.\nHow would you like to compute the binary cross entropy with a different number of samples?\nDid you change anything in the example code?\nI just retried it with the latest nightly build and it seems to work without any errors.","z":"The printed shapes correspond to the shape mismatch mentioned in the error message.\nHow would you like to compute the binary cross entropy with a different number of samples?\nDid you change anything in the example code?\nI just retried it with the latest nightly build and it seems to work without any errors."},{"x":"Hello, I am new to Pytorch, I need to train two different networks, and their inputs are different. I need to multiply their outputs and train them simultaneously, how to do it ?","y":"Your forward method seems to be missing the return statement, so you should add:\n<code class=\"lang-python\">return x\n<\/code>\nat the end of it.","z":"inputs of net1 works, but inputs of net2 failed. The  input data of net2 is a np.array generated by a function I def, how to send it to net2\nYou could transform the numpy array to a tensor via:\n<code class=\"lang-python\">data = torch.from_numpy(array)\n<\/code>\nDepending on the dtype of the numpy array you might need to transform the tensor to a FloatTensor using:\n<code class=\"lang-python\">data = data.float()\n<\/code>\ndata = torch.from_numpy(array)\ndata = data.float()\ndata = data.cuda()\nThese I have done, but when y = net2(data)\ngot y is NoneType\nCould you post the definition of net2?\nwell a strange net\nclass Matrix(nn.Module):\ndef init(self):\nsuper(Matrix,self).init()\nself.conv1 = nn.Conv2d(1024,1024,3)\nself.conv2 = nn.Conv2d(1024,512,5)\nself.conv3 = nn.Conv2d(512,256,5)\nself.conv4 = nn.Conv2d(256,128,3)\nself.conv5 = nn.Conv2d(128,64,5)\nself.pool = nn.AvgPool2d(2,2)\nself.fc1 = nn.Linear(64,32)\nself.fc2 = nn.Linear(32,10)\ndef forward(self,x):\n    x = self.pool(F.relu(self.conv1(x)))\n    x = F.relu(self.conv2(x))\n    x = F.relu(self.conv3(x))\n    x = F.relu(self.conv4(x))\n    x = F.relu(self.conv5(x))\n    x = x.view(-1,64)\n    x = F.relu(self.fc1(x))\n    x = self.fc2(x)\n\nnet2 = Matrix()\nand input data:\nh = hadamard(1024)\nh = h.reshape([1,1024,32,32])\nh = torch.from_numpy(h)\nh = h.float()\nh = h.cuda()\ny = net2(h)\nAnd I find I have not def a optimizer of net2, is this the reason?\nYour forward method seems to be missing the return statement, so you should add:\n<code class=\"lang-python\">return x\n<\/code>\nat the end of it.\nOMG!! Thx so much\nso ridiculous i am, i need to update my glasses"},{"x":"It seems no different between these .\n<code class=\"lang-auto\">>>> import torch\n>>> a=torch.randn(4,4)\n>>> a\ntensor([[ 2.0155, -1.1138, -1.7522,  0.7299],\n        [ 1.0620,  0.1840,  0.2790,  1.1942],\n        [ 1.7519,  1.8871,  1.4988, -0.1911],\n        [-1.6222, -0.2044,  1.6316, -1.0949]])\n>>> a.max(1,keepdim=True)[1]\ntensor([[0],\n        [3],\n        [1],\n        [2]])\n>>> a.max(1,keepdim=True)[1].data\ntensor([[0],\n        [3],\n        [1],\n        [2]])\n<\/code>","y":"The usage of .data is not recommended, as you\u2019ll get access to the \u201creal\u201d underlying data and Autograd cannot track the changes applied on it, which might yield wrong results and thus silent errors.\nIf you don\u2019t want to track some changes on your tensors, you should wrap these operations in a with torch.no_grad() block.","z":"The usage of .data is not recommended, as you\u2019ll get access to the \u201creal\u201d underlying data and Autograd cannot track the changes applied on it, which might yield wrong results and thus silent errors.\nIf you don\u2019t want to track some changes on your tensors, you should wrap these operations in a with torch.no_grad() block."},{"x":"Hello I\u2019m new to Pytorch and I\u2019ve been trying to work through this tutorial.\n[https:\/\/github.com\/pytorch\/tutorials\/blob\/master\/intermediate_source\/torchvision_tutorial.rst]\nI\u2019m using Visual Studio Code, conda installed the Pytorch. Initially the issue was that it was missing the module engine. But this problem was solved here:\n\n\n\n\nModuleNotFoundError: No module named 'engine' with link \"https:\/\/discuss.pytorch.org\/t\/modulenotfounderror-no-module-named-engine\/59564\/3\"\n\n\n    So how do we get the references\/detection\/ folders? What should we download and install? I have installed the pytorch, torchvision in my environment, but I could not find those files. Thanks\n  \n\n\nWith a step I misread.\nSo I downloaded and copied the pycocotools into the project directory and the vision\/detection\/ .py files in to it too.\nThat being done it fixed that issue but the error it is now giving is.\n<code class=\"lang-auto\">(base) C:\\Users\\Sean\\Desktop\\Project\\Test\\Tutorial>D:\/Anaconda\/python.exe c:\/Users\/Sean\/Desktop\/Project\/Test\/Tutorial\/tv-training-code.py\nTraceback (most recent call last):\n  File \"c:\/Users\/Sean\/Desktop\/Project\/Test\/Tutorial\/tv-training-code.py\", line 13, in <module>\n    from engine import train_one_epoch, evaluate\n  File \"c:\\Users\\Sean\\Desktop\\Project\\Test\\Tutorial\\engine.py\", line 8, in <module>\n    from coco_utils import get_coco_api_from_dataset\n  File \"c:\\Users\\Sean\\Desktop\\Project\\Test\\Tutorial\\coco_utils.py\", line 9, in <module>\n    from pycocotools import mask as coco_mask\n  File \"c:\\Users\\Sean\\Desktop\\Project\\Test\\Tutorial\\pycocotools\\mask.py\", line 3, in <module>\n    import pycocotools._mask as _mask\nModuleNotFoundError: No module named 'pycocotools._mask'\n<\/code>\nI\u2019m not quite sure what the issue here is or how to fix it. Given that the _mask.pyx is present and is what I think is being imported. As I said I\u2019m not to sure what the issue is but I would appreciate the help.\nPython Version : 3.7.4\nPytorch: 1.2.0 (Cuda 10)","y":"Probably pycocotools is missing, which can be installed via:\n<code class=\"lang-python\"># install pycocotools\ncd $INSTALL_DIR\ngit clone https:\/\/github.com\/cocodataset\/cocoapi.git\ncd cocoapi\/PythonAPI\npython setup.py build_ext install\n<\/code>","z":"Probably pycocotools is missing, which can be installed via:\n<code class=\"lang-python\"># install pycocotools\ncd $INSTALL_DIR\ngit clone https:\/\/github.com\/cocodataset\/cocoapi.git\ncd cocoapi\/PythonAPI\npython setup.py build_ext install\n<\/code>"},{"x":"Hello!\nI recently found something interesting when trying to train a Mask RCNN. For those unfamiliar with this type of model, there are 2 inputs:\n\nA list of images (variable size)\nA list of dictionaries (where each dictionary contains 3 tensors: bounding boxes, segmentation, and class label)\n\nMore information regarding input data format can be found here in the documentation with link \"https:\/\/pytorch.org\/docs\/stable\/torchvision\/models.html#mask-r-cnn\"\nThe output is a dictionary of losses (classification loss, segmentation loss, etc). There\u2019s a total of 5 keys\/losses. I\u2019m currently summing the losses into one tensor and then calling backward on the summed total loss. (The sum operation will be captured in the tensor graph).\nAnyway, here is my problem that I ran into:\nBelow is some code for a simple training epoch:\n<code class=\"lang-auto\">for data in enumerate(someDataLoader):\n    # get the inputs; data is a list of [inputs, labels]\n    images, targets = data\n        \n    scenes = images = list(image.to(device) for image in images)\n    for t in targets:\n        t['boxes'] = t['boxes'].to(device)\n        t['labels'] = t['labels'].to(device)\n        t['masks'] = t['masks'].to(device)\n\n    # forward + backward + optimize\n    loss_dict = maskrcnn_model(scenes, targets)\n    \n    tot_loss = sum(loss for loss in loss_dict.values())\n    \n    # zero the parameter gradients\n    maskrcnn_opt.zero_grad()\n    tot_loss.backward()\n    maskrcnn_opt.step()\n<\/code>\nWhen running this training loop, I notice that the GPU memory is increasing as more mini-batches are fed. (For example, at the start of training there would be a 3 GB memory usage, after 250 mini-batches, the GPU is at 12 GB memory usage, steadily increasing by 300 MB every 2 seconds). Batch size is fairly small.\nTo fix this, I looked at some online examples of training a Mask RCNN model. Everything was the same except this portion of the training loop:\n<code class=\"lang-auto\">for t in targets:\n    t['boxes'] = t['boxes'].to(device)\n    t['labels'] = t['labels'].to(device)\n    t['masks'] = t['masks'].to(device)\n<\/code>\nInstead of doing it the way I did above, they used a list comprehension and basically made a new dictionary for each target, and saved into a separate list. An example of that would be either of the following code snippets below (I find the top code snippet to be more read-able, but they both do the same thing).\n<code class=\"lang-auto\">newTargs = []\nfor t in targets:\n    x = {}\n    x['boxes'] = t['boxes'].to(device)\n    x['labels'] = t['labels'].to(device)\n    x['masks'] = t['masks'].to(device)\n    newTargs.append(x)\n<\/code>\n<code class=\"lang-auto\">newTargs = [{k: v.to(device) for k, v in t.items()} for t in targets]\n<\/code>\nAfter making a new list of dictionaries, newTargs is then used in the forward pass of the model, rather than targets.\nE.g. loss_dict = maskrcnn_model(images, newTargs).\nThis small change in code maintains GPU memory of 3 GB throughout the training epoch.\nAny thoughts on why?\nThanks,\nEpoching","y":"Hi,\nBecause labels = labels.cuda() this does not change the original labels inplace. But your code does change the original dictionary inplace ! So if this dictionary is the same as the one stored in your Dataset, then you change the one stored in your dataset.","z":"In your first case, you actually modify targets inplace. Which is what the dataloader returned. It is very possible that you are incrementally changing your whole Dataset by moving each target to the GPU one by one.\nAh interesting, are you thinking that doing this in-place keeps each minibatch on the GPU (because the DataLoader is passing original references from the source Dataset)?\nIf this is the case, why doesn\u2019t this happen for a simple image classification dataloader (made from torchvision.datasets, for example):\n<code class=\"lang-auto\">images = images.cuda()\nlabels = labels.cuda()\n<\/code>\nI also forgot to mention that we have to use a custom collate_fn, to get the DataLoader to output batches of a list of image tensors, and a list of dictionaries.\nMaybe the default collate_fn argument makes copies of the Dataset examples via torch.stack so it doesn\u2019t alter the device status of the original data in the Dataset.\nWill definitely look into this suggestion tonight! Thanks for the tip \nHi,\nBecause labels = labels.cuda() this does not change the original labels inplace. But your code does change the original dictionary inplace ! So if this dictionary is the same as the one stored in your Dataset, then you change the one stored in your dataset.\nThis is correct! Below is an example of the collate_fn I was using to batch together examples from the Dataset:\n<code class=\"lang-auto\">def myCollater(examples):\n    input_imgs = []\n    target_dicts = []\n    for ex in examples:\n        input_imgs.append(ex[0])\n        target_dicts.append(ex[1])\n        \n    return torch.stack(input_imgs), target_dicts\n<\/code>\nI ended up appending the dictionaries to a list that came directly from the Dataset. The input image tensors didn\u2019t stay on the GPU when training because I thought that torch.stack() basically combines multiple image tensors into one single batched tensor (which is a copy of all of the original tensors, so putting it on the GPU isn\u2019t necessarily putting the original image tensors on the GPU).\nAlso, if we wanted to train a Mask_RCNN with variable image sizes, then the collate function should return a list of image tensors (rather than a single stacked\/batched image tensor, since you can\u2019t stack multiple tensors of variable size). This meant I had to switch my return function of myCollater to:\nreturn input_imgs, target_dicts\nThis also means that I have to now put the images on the GPU one at a time (since input_imgs is a list of image tensors), so in the training loop we now have:\n<code class=\"lang-auto\">for j in range(len(images)):\n    images[j] = images[j].to(device)\n<\/code>\ninstead of:\n<code class=\"lang-auto\">images = images.to(device)\n<\/code>\nI thought that this would now cause the images to stay on the GPU, since they are supplied directly from the Dataset class, and appended in a list through the myCollater function. Turns out that these images don\u2019t stay on the GPU, and everything runs fine.\nI ran a small experiment to find out that when you add a tensor to a list, a copy of the tensor is made every time it\u2019s added to a list. (So each list has it\u2019s own unique reference to a tensor). For dictionary values, it\u2019s the same thing!\nimage1323\u00d7752 125 KB with link \"https:\/\/discuss.pytorch.org\/uploads\/default\/original\/3X\/9\/6\/96609dc7180f6a0f1293ff4f7d88a4552003e8b5.png\"\nSo my GPU memory problem was caused from appending original dictionaries of tensors. Appending dictionaries (or lists) doesn\u2019t necessarily copy the objects for you automatically, which is why we have to do it ourselves explicitly in the training loop (or in the collate_fn). Oops!\nThanks for the info again "},{"x":"Hey!\nI\u2019ve noticed that for sufficiently high resolution of images (e.g. full HD) I cannot get a good enough \u201cidentity grid\u201d using torch.float32 precision, code below.\nPosting here:\n\nto give a heads up to others\nto double check - am I right this just a numerical precision issue?\n\nI saw  mention in the issue on github with link \"https:\/\/github.com\/pytorch\/pytorch\/issues\/24870\"\n\nAllow option for sampling using only the residual displacement\/flow. Eliminates the need to constantly add an identity grid to the flow\/displacement field, which is imprecise, slow, and very prone to user error.\n\nIf what I\u2019m seeing is what I think I\u2019m seeing it would be a pretty good motivation to add a function which just takes residual displacement as input.\nCode:\n<code class=\"lang-auto\">import torch.nn.functional as F\n\nW = 1920\nH = 1080\nshape = [1, 1, H, W]\n\ndtype = torch.float\n\nx_prev = torch.zeros(shape, dtype=dtype)\nx_prev[..., :, 0, 0] = 1.\n\nx_next_expected = x_prev.clone()\n\nbase_grid = torch.stack((torch.linspace(-1, 1, W, dtype=dtype).unsqueeze(0).repeat(H, 1),\n                         torch.linspace(-1, 1, H, dtype=dtype).unsqueeze(-1).repeat(1, W),), dim=-1)\nbase_grid = base_grid.unsqueeze(0)\n\nx_next = F.grid_sample(x_prev, base_grid, align_corners=True)\n\nprint((x_next_expected - x_next).abs().sum())\nprint(x_next[..., :2, :2])\nprint(x_next_expected[..., :2, :2])\nprint(x_next[..., :2, :2] - x_next_expected[..., :2, :2])\n\nassert torch.allclose(x_next_expected, x_next)\n<\/code>","y":"Yeah well, then you have to code up the function that computes without the identity grid.","z":"To me it looks like the expectation is implicitly like align_corners=True but you don\u2019t pass that to grid_sample.\nAt least I get an error of 0.75 with your code and 2.134e-5 when adding align_corners=True.\nBest regards\nThomas\nI meant with align_corners=True, I\u2019ll update the code.\nAlso, isn\u2019t align_corners=True the default as of v1.3? I know it\u2019s going to default to align_corners=False in v1.4.\nThanks,\nAdam\nYeah well, then you have to code up the function that computes without the identity grid."},{"x":"I am new to PyTorch, so I am trying to learn from many different examples and sources, so please excuse any glaring inconsistencies. Anyways, I am trying to implement an AutoEncoder for the Fashion-MNIST dataset, and then take the trained output of the bottleneck layer to send to an MLP classifier. The AE was not too troubling, but adding the classifier is where I am running into issues.\nMy MLP output layer has 10 nodes (corresponding to 10 classes), and the MNIST labels are integers by default rather than one-hot codes. Based on what I have read so far, it seems that this should be no problem for the nll_loss function or cross_entropy loss (which to my understanding calls nll_loss). However, in my case this is not so, as I get the following error when attempting to calculate the cross_entropy loss between my 10-dim predictions and the integer target labels:\n<code class=\"lang-auto\">Traceback (most recent call last):\n  File \"sae_fmnist.py\", line 194, in <module>\n    test_SAE(model, test_set, test_bs)\t# see initial perf with random weights\n  File \"sae_fmnist.py\", line 142, in test_SAE\n    test_loss_c += F.cross_entropy(pred, target)\n  File \"\/home\/ryan\/.local\/lib\/python3.5\/site-packages\/torch\/nn\/functional.py\", line 2009, in cross_entropy\n    return nll_loss(log_softmax(input, 1), target, weight, None, ignore_index, None, reduction)\n  File \"\/home\/ryan\/.local\/lib\/python3.5\/site-packages\/torch\/nn\/functional.py\", line 1848, in nll_loss\n    out_size, target.size()))\nValueError: Expected target size (1, 10), got torch.Size([1])\n<\/code>\nMy model is defined as follows:\n<code class=\"lang-auto\">class Autoencoder(nn.Module):\n\tdef __init__(self):\n\t\tsuper(Autoencoder,self).__init__()\n\t\t\n\t\tself.encoder = nn.Sequential(\n\t\t\tnn.Linear(in_features=28*28, out_features=500),\n\t\t\tnn.ReLU(inplace=True),\n\t\t\tnn.Linear(in_features=500, out_features=200),\n\t\t\tnn.ReLU(inplace=True),\n\t\t\tnn.Linear(in_features=200, out_features=20),\n\t\t\tnn.ReLU(inplace=True)\n\t\t)\n\t\n\t\tself.decoder = nn.Sequential(\n\t\t\tnn.Linear(in_features=20, out_features=200),\n\t\t\tnn.ReLU(inplace=True),\n\t\t\tnn.Linear(in_features=200, out_features=500),\n\t\t\tnn.ReLU(inplace=True),\n\t\t\tnn.Linear(in_features=500, out_features=28*28)\n\t\t)\n\t\t\n\t\tself.classifier = nn.Sequential(\n\t\t\tnn.Linear(in_features=20, out_features=10)#,\n\t\t\t#nn.LogSoftmax(dim=1)\n\t\t)\n\n\t# define forward function\n\tdef forward(self, x):\n\t\tencoded = self.encoder(x)\n\t\tdecoded = self.decoder(encoded)\n\t\tpred = self.classifier(encoded)\n\t\treturn decoded, pred\n\n<\/code>\nAnd the erroneous function is here:\n<code class=\"lang-auto\">def test_SAE(model, dataset, batch_size):\n\tmodel.eval()\t# puts model in evaluation mode\n\ttest_loss_d = 0\n\ttest_loss_c = 0\n\tnum_correct = 0\n\tnum_digits = 10\n\ttest_loader = torch.utils.data.DataLoader(dataset)\n\twith torch.no_grad():\n\t\tfor data, target in test_loader:\n\t\t\tdata = data.flatten(start_dim=2)\n\t\t\toutput, pred = model(data)\n\t\t\tcriterion_d = torch.nn.MSELoss(size_average=False)\n\t\t\t#criterion_c = torch.nn.CrossEntropyLoss(size_average=False)\n\t\t\ttest_loss_d += criterion_d(output, data).item()\n\t\t\t#test_loss_c += criterion_c(pred, target).item()\n\t\t\ttest_loss_c += F.cross_entropy(pred, target)\n\t\t\t#p_class = pred.data.max(1, keepdim=True)[1]\n\t\t\t#num_correct += p_class.eq(target.data.view_as(p_class)).sum()\n\t\t\tnum_correct += pred.argmax(dim=1).eq(target).sum().item()\n\t\t\t\n\ttest_loss_d \/= len(test_loader.dataset)\n\ttest_loss_c \/= len(test_loader.dataset)\n\tprint('\\nTest set: Avg. loss: {:.4f}, {:.4f}, Accuracy: {}\/{} ({:.0f}%)\\n'.format(\n\t\ttest_loss_d, test_loss_c, num_correct, len(test_loader.dataset),\n\t\t100. * num_correct \/ len(test_loader.dataset)))\n<\/code>\nSome things of note:\n-I have left a few comments of other attempted methods which too produced similar errors\n-The dual output of the network is based on this AE: Autoencoder and Classification inside the same model with link \"https:\/\/discuss.pytorch.org\/t\/autoencoder-and-classification-inside-the-same-model\/36248\"\n-I call test once before any training, which is why I get errors in my test function. I expect the same errors in my similar training function, but they should correspond to whatever the problem is here, I assume.\nAny suggestions would be appreciated. Thanks!","y":"Try changing data = data.flatten(start_dim=2) to data = data.view(data.size(0), -1).","z":"Try changing data = data.flatten(start_dim=2) to data = data.view(data.size(0), -1).\nI use start_dim=2 based on the dimensions of the MNIST data.\nThe data is brought in as follows:\n<code class=\"lang-auto\"># Fashion-MNIST dataset\ntrain_set = torchvision.datasets.FashionMNIST(\n\troot = '.\/data\/FashionMNIST',\n\ttrain = True,\n\tdownload = True,\n\ttransform = transforms.Compose([\n\t\ttransforms.ToTensor()                                 \n\t])\n)\n\ntest_set = torchvision.datasets.FashionMNIST(\n\troot = '.\/data\/FashionMNIST',\n\ttrain = False,\n\tdownload = True,\n\ttransform = transforms.Compose([\n\t\ttransforms.ToTensor()                                 \n\t])\n)\n<\/code>\nThen performing this:\n<code class=\"lang-auto\">test_loader = torch.utils.data.DataLoader(dataset)\nfor data, target in test_loader:\n\t\t\tprint(\"Init shape: {}, nreshape: {}, length: {}, target shape: {}\".format(\n\t\t\t\tdata.shape, data.flatten(start_dim=2).shape, len(test_loader.dataset), target.shape))\n<\/code>\nYields this:\n<code class=\"lang-auto\">Init shape: torch.Size([1, 1, 28, 28]), nreshape: torch.Size([1, 1, 784]), length: 10000, target shape: torch.Size([1])\n<\/code>\nDo you still think the start_dim=1 would be helpful?\nThanks\nTry the solution in my latest updated reply.\nI think the issue is that the extra channel dimension in your tensor is retained in the output so your model output has shape (n_examples, 1, 10), which makes your loss function expect a label of shape (1, 10). If you reshape your output to (n_examples, 10), then the loss will take scalar labels.\nOk, I see. Wouldn\u2019t I need something closer to data = data.flatten(start_dim=2).view(data.size(0), -1) to ensure I keep all 784 dimensions?\nNo just try the fix I mentioned.\nThe extra dummy channel dimension makes the loss function think you are giving it multi-dimensional outputs, which you really aren\u2019t. See the doc for more details.\nLooks like that did the trick. I will look into the .view() documentation at some point, right now I have training to do\u2026 Thank you so much!"},{"x":"I think this has come up a few times but never been directly addressed in a topic, but is there a way to get the maximum values of a tensor along one axis using tuple indexing for the axes?\n(If this has come up as a full topic before, please let me know and I\u2019ll edit the topic accordingly)\ne.g. using numpy one can do the following to get the maximum values in each 3x3 slice of a datacube called \u201ca\u201d\n<code class=\"lang-auto\">import numpy as np\n\na = np.random.uniform(0,1,(5,1,3,3))\nmax_vals = a.max(axis=(1,2,3))\n<\/code>\nHowever, in PyTorch this will break with an exit error line:\n<code class=\"lang-auto\">import torch \n\nat = torch.tensor(a)\nmax_vals = a.max(axis=(1,2,3))\n<\/code>\nHave I missed a very simple workaround for this problem or does PyTorch not support one-line outputting of maximum values in this way?\nMany thanks in advanced!","y":"Not sure if this is the best way to solve the problem but one can do it recursively:\n<code class=\"lang-auto\">max_vals = a.max(1)[0].max(1)[0].max(1)[0]\n<\/code>","z":"Not sure if this is the best way to solve the problem but one can do it recursively:\n<code class=\"lang-auto\">max_vals = a.max(1)[0].max(1)[0].max(1)[0]\n<\/code>\nI dont think pytorch.max function supports the tuple functionality. It only works along a single dimension as per the documentation with link \"https:\/\/pytorch.org\/docs\/stable\/torch.html#torch.max\". Might be a good idea to request for this feature.\nReally silly question but is there a channel through which I could make a request for a feature like this? \nSure! Feel free to open a feature request on the Github issues with link \"https:\/\/github.com\/pytorch\/pytorch\/issues\" and explain the use case with your sample code a bit. "},{"x":"Hello all,\nI have a network that generates kernel weights for a 2D convolution operation. It takes a single input and generates a weight vector which then reshaped into KxK kernel where K is the kernel size. Finally, I apply this kernel to an image. I\u2019m trying to make this work with batches. So when the network generates a [N, KxK] weight vector, I would like to have a tensor with size [N, K, K] that is applied to [N, 1, H, W] images. Here, N is the batch size, H and W are the image sizes. I can do this with for loop over the batches but I\u2019m wondering if there is a more efficient way. Below is my network:\n<code class=\"lang-auto\">class FilterNet(nn.Module):\n\n    def __init__(self, obs_size, hidden_size, kernel_size, activation_func='tanh'):\n        super(FilterNet, self).__init__()\n        self.kernel_size = kernel_size\n\n        self.fc_obs1 = nn.Linear(obs_size, hidden_size)\n        self.fc_obs2 = nn.Linear(hidden_size, hidden_size)\n        self.fc_obs3 = nn.Linear(hidden_size, hidden_size)\n        self.fc_obs4 = nn.Linear(hidden_size, kernel_size * kernel_size)\n\n        self.act_func = getattr(torch, activation_func)\n\n    def forward(self, images, tau):\n        hidden = self.act_func(self.fc_obs1(tau))\n        hidden = self.act_func(self.fc_obs2(hidden))\n        hidden = self.act_func(self.fc_obs3(hidden))\n        kernel_weigths = self.fc_obs4(hidden)\n\n        heatmaps = []\n        for weights_i, image_i in zip(kernel_weigths.chunk(kernel_weigths.size(0), dim=0), images.chunk(images.size(0), dim=0)):\n            kernel = weights_i.view(1, 1, self.kernel_size, self.kernel_size)\n            heatmap = F.conv2d(image_i, kernel, padding=2)\n            heatmap = heatmap.squeeze()\n            heatmaps += [heatmap]\n        heatmaps = torch.stack(heatmaps, 0)\n\n        # kernel_weigths = kernel_weigths.view(kernel_weigths.size(0), 1, self.kernel_size, self.kernel_size)\n        # heatmaps = F.conv2d(images, kernel_weigths, padding=2)\n        \n        return heatmaps\n<\/code>\nNote the commented section of the code does not work.\nThanks!","y":"Since you are using single channel images, you could swap the batch and the channel dimension and use a grouped convolution:\n<code class=\"lang-python\">class FilterNet(nn.Module):\n\n    def __init__(self, obs_size, hidden_size, kernel_size, activation_func='tanh'):\n        super(FilterNet, self).__init__()\n        self.kernel_size = kernel_size\n\n        self.fc_obs1 = nn.Linear(obs_size, hidden_size)\n        self.fc_obs2 = nn.Linear(hidden_size, hidden_size)\n        self.fc_obs3 = nn.Linear(hidden_size, hidden_size)\n        self.fc_obs4 = nn.Linear(hidden_size, kernel_size * kernel_size)\n\n        self.act_func = getattr(torch, activation_func)\n\n    def forward(self, images, tau, use_loop=False):\n        hidden = self.act_func(self.fc_obs1(tau))\n        hidden = self.act_func(self.fc_obs2(hidden))\n        hidden = self.act_func(self.fc_obs3(hidden))\n        kernel_weigths = self.fc_obs4(hidden)\n\n        if use_loop:\n            heatmaps = []\n            for weights_i, image_i in zip(kernel_weigths.chunk(kernel_weigths.size(0), dim=0), images.chunk(images.size(0), dim=0)):\n                kernel = weights_i.view(1, 1, self.kernel_size, self.kernel_size)\n                heatmap = F.conv2d(image_i, kernel, padding=2)\n                heatmap = heatmap.squeeze()\n                heatmaps += [heatmap]\n            heatmaps = torch.stack(heatmaps, 0)\n        else:\n            kernel_weigths = kernel_weigths.view(kernel_weigths.size(0), 1, self.kernel_size, self.kernel_size)\n            images = images.permute(1, 0, 2, 3)\n            heatmaps = F.conv2d(images, kernel_weigths, padding=2, groups=images.size(1))\n            \n        return heatmaps\n\n\nmodel = FilterNet(10, 10, 3)\nimages = torch.randn(16, 1, 24, 24)\ntau = torch.randn(16, 10)\n\noutput_loop = model(images, tau, use_loop=True)\noutput_grouped = model(images, tau, use_loop=False)\n\nprint((output_loop - output_grouped).max())\n> tensor(2.3842e-07, grad_fn=<MaxBackward1>)\n<\/code>\nThis will apply each kernel in a single input channel, so that in fact we are applying each kernel on a single sample from the batch.","z":"Since you are using single channel images, you could swap the batch and the channel dimension and use a grouped convolution:\n<code class=\"lang-python\">class FilterNet(nn.Module):\n\n    def __init__(self, obs_size, hidden_size, kernel_size, activation_func='tanh'):\n        super(FilterNet, self).__init__()\n        self.kernel_size = kernel_size\n\n        self.fc_obs1 = nn.Linear(obs_size, hidden_size)\n        self.fc_obs2 = nn.Linear(hidden_size, hidden_size)\n        self.fc_obs3 = nn.Linear(hidden_size, hidden_size)\n        self.fc_obs4 = nn.Linear(hidden_size, kernel_size * kernel_size)\n\n        self.act_func = getattr(torch, activation_func)\n\n    def forward(self, images, tau, use_loop=False):\n        hidden = self.act_func(self.fc_obs1(tau))\n        hidden = self.act_func(self.fc_obs2(hidden))\n        hidden = self.act_func(self.fc_obs3(hidden))\n        kernel_weigths = self.fc_obs4(hidden)\n\n        if use_loop:\n            heatmaps = []\n            for weights_i, image_i in zip(kernel_weigths.chunk(kernel_weigths.size(0), dim=0), images.chunk(images.size(0), dim=0)):\n                kernel = weights_i.view(1, 1, self.kernel_size, self.kernel_size)\n                heatmap = F.conv2d(image_i, kernel, padding=2)\n                heatmap = heatmap.squeeze()\n                heatmaps += [heatmap]\n            heatmaps = torch.stack(heatmaps, 0)\n        else:\n            kernel_weigths = kernel_weigths.view(kernel_weigths.size(0), 1, self.kernel_size, self.kernel_size)\n            images = images.permute(1, 0, 2, 3)\n            heatmaps = F.conv2d(images, kernel_weigths, padding=2, groups=images.size(1))\n            \n        return heatmaps\n\n\nmodel = FilterNet(10, 10, 3)\nimages = torch.randn(16, 1, 24, 24)\ntau = torch.randn(16, 10)\n\noutput_loop = model(images, tau, use_loop=True)\noutput_grouped = model(images, tau, use_loop=False)\n\nprint((output_loop - output_grouped).max())\n> tensor(2.3842e-07, grad_fn=<MaxBackward1>)\n<\/code>\nThis will apply each kernel in a single input channel, so that in fact we are applying each kernel on a single sample from the batch.\nThanks a lot, . This is exactly what I wanted.\nJust did a quick comparison and it seems that the grouped convolution is a lot faster than for loop.\nA single forward pass used to take 0.01 seconds now it\u2019s 0.0002 seconds.\n I would like to do the same thing with multi-channel inputs and kernels now.\nSo I have a tensor with shape [N, C, W, H] as the input and a tensor [N, K, kW, kH] where N is the batch size, C is the image channels, and K is the number of kernels and K=C. I want first K filters from the kernel batch to be applied to first image with C channels from the image batch and so on. Each k filter should be only applied one channel c. In the output I should have a feature map with size [N, C, W, H].\nCan I do this without a for loop too?"},{"x":"please find code snippet below:-\ndef load_torch_model(self):\n    if self.pre_trained_model == 'VGG16':\n        self.torch_model = models.vgg16(pretrained=True)\n        print(self.torch_model)\n        self.torch_model.classifier[6] = nn.Sequential(nn.Linear(in_features=self.in_features[0],\n                                                                 out_features=self.out_features[0], bias=True),\n                                                       nn.ReLU(), nn.Dropout(p=self.dropout_ratio[0]),\n                                                       nn.Linear(in_features=self.in_features[1],\n                                                                 out_features=self.out_features[1], bias=True),\n                                                       nn.LogSoftmax(dim=1))\n        print(self.torch_model)\n\n    self.torch_model.load_state_dict(torch.load(<Path of model with .pt extension>))\n\nI got traceback:-\nTraceback (most recent call last):\nFile \u201cC:\/AI_Projects\/Pytorch_to_TensorFlow_API\/test.py\u201d, line 20, in \nconvert.load_torch_model()\nFile \u201cC:\\AI_Projects\\Pytorch_to_TensorFlow_API\\converter.py\u201d, line 43, in load_torch_model\nself.torch_model.load_state_dict(torch.load(self.torch_model_path))\nFile \u201cC:\\Python37\\lib\\site-packages\\torch\\nn\\modules\\module.py\u201d, line 818, in load_state_dict\nstate_dict = state_dict.copy()\nFile \u201cC:\\Python37\\lib\\site-packages\\torch\\nn\\modules\\module.py\u201d, line 591, in getattr\ntype(self).name, name))\nAttributeError: \u2018VGG\u2019 object has no attribute \u2018copy\u2019","y":"How did you save the state_dict and what keys are inside it?\nSaving and loading the state_dict using your model, works fine:\n<code class=\"lang-python\">torch_model = models.vgg16(pretrained=False)\n\ntorch_model.classifier[6] = nn.Sequential(\n    nn.Linear(in_features=4096,\n        out_features=1, bias=True),\n    nn.ReLU(),\n    nn.Dropout(p=0.5),\n    nn.Linear(in_features=1, out_features=2, bias=True),\n    nn.LogSoftmax(dim=1)\n)\n\ntorch_model(torch.randn(1, 3, 224, 224))\nstate_dict = torch_model.state_dict()\ntorch.save(state_dict, 'tmp.pt')\n\ntorch_model.load_state_dict(torch.load('tmp.pt'))\n<\/code>","z":"How did you save the state_dict and what keys are inside it?\nSaving and loading the state_dict using your model, works fine:\n<code class=\"lang-python\">torch_model = models.vgg16(pretrained=False)\n\ntorch_model.classifier[6] = nn.Sequential(\n    nn.Linear(in_features=4096,\n        out_features=1, bias=True),\n    nn.ReLU(),\n    nn.Dropout(p=0.5),\n    nn.Linear(in_features=1, out_features=2, bias=True),\n    nn.LogSoftmax(dim=1)\n)\n\ntorch_model(torch.randn(1, 3, 224, 224))\nstate_dict = torch_model.state_dict()\ntorch.save(state_dict, 'tmp.pt')\n\ntorch_model.load_state_dict(torch.load('tmp.pt'))\n<\/code>"},{"x":"Given a trained object detection network the goal of my program is to prune the network, deleting the least contributing filters and their dependent feature maps and biases. After a network is pruned the network is retrained to regain the accuracy it has lost. Retraining this network a second time after pruning and retraining it a first time results in the following error:\n<code class=\"lang-auto\">Traceback (most recent call last):\n  File \"Pruning.py\", line 212, in <module>\n    prune()\n  File \"Pruning.py\", line 52, in __call__\n    self.pruneLoop(prune)\n  File \"Pruning.py\", line 80, in pruneLoop\n    traineng()\n  File \"\/home\/bullseye\/.local\/lib\/python3.6\/site-packages\/lightnet\/engine\/_engine.py\", line 124, in __call__\n    self.process_batch(data)\n  File \"\/home\/bullseye\/Documents\/git\/Master-Thesis-Pruning-for-Object-Detection\/docker-omgeving\/Code\/trainengine.py\", line 34, in process_batch\n    loss.backward()\n  File \"\/home\/bullseye\/.local\/lib\/python3.6\/site-packages\/torch\/tensor.py\", line 166, in backward\n    torch.autograd.backward(self, gradient, retain_graph, create_graph)\n  File \"\/home\/bullseye\/.local\/lib\/python3.6\/site-packages\/torch\/autograd\/__init__.py\", line 99, in backward\n    allow_unreachable=True)  # allow_unreachable flag\nRuntimeError: The size of tensor a (1010) must match the size of tensor b (798) at non-singleton dimension 1\n<\/code>\nSince the error is occurring in the backwards function it hints that the problem is created when retraining a first time. My first thought was that the grad created by training a first time was the issue but the problem remained even after deleting the gradients after pruning.\nWhat is causing this error?","y":"I am unsure of how to provide a minimal code snippet of my program that is able to reproduce the error since it is several hundreds of lines. However I have found the solution to my problem.\nAs I first suspected it was the grad attributes of the tensors that were the problem. The size of these tensors were different to the size of the weights itself after pruning them. Setting them to None solved the issue.\nThe first time I attempted this solution I forgot that the running mean and running var tensors of batch normalization layers also contain gradients.\n Thank you for your help, your dummy code helped me understand and find the problem better. I am however still unsure of why your dummy code doesn\u2019t face the same issues.","z":"I\u2019m not completely sure, how your pruning works, but could it be that you are removing some parameters after the computation graph was already created (e.g. after a new forward pass)?\n Yes I delete some filters, their biases and the dependent feature maps (for example in model.weight.data) during pruning. Although the pruning is surrounded by a\n<code class=\"lang-auto\">with torch.no_grad\n<\/code>\nI suspect that the training would generate this graph in it\u2019s forward pass. Should I delete this graph? And if so, how do I access it?\nCould you post a minimal code snippet to reproduce this issue?\nThis dummy approach seems to work fine:\n<code class=\"lang-python\">class MyModel(nn.Module):\n    def __init__(self):\n        super(MyModel, self).__init__()\n        self.conv = nn.Conv2d(3, 10, 3, 1, 1, bias=False)\n        \n    def forward(self, x):\n        x = self.conv(x)\n        return x\n\nmodel = MyModel()\nx = torch.randn(1, 3, 3, 3)\noutput = model(x)\nprint(output.shape)\noutput.mean().backward()\nprint(model.conv.weight.grad)\nmodel.zero_grad()\n\n# prune\nwith torch.no_grad():\n    model.conv.weight = nn.Parameter(model.conv.weight[:5])\n\noutput = model(x)\nprint(output.shape)\noutput.mean().backward()\nprint(model.conv.weight.grad)\nmodel.zero_grad()\n<\/code>\nI am unsure of how to provide a minimal code snippet of my program that is able to reproduce the error since it is several hundreds of lines. However I have found the solution to my problem.\nAs I first suspected it was the grad attributes of the tensors that were the problem. The size of these tensors were different to the size of the weights itself after pruning them. Setting them to None solved the issue.\nThe first time I attempted this solution I forgot that the running mean and running var tensors of batch normalization layers also contain gradients.\n Thank you for your help, your dummy code helped me understand and find the problem better. I am however still unsure of why your dummy code doesn\u2019t face the same issues."},{"x":"Hi,\nI am trying to solve an object detection problem using FasterRCNN, where I have N detection scenarios. I am getting good results with transfer learning just the last layer (box predictor), but I end up having N models, one for each scenario. Consequently, it takes up a lot of storage space and RAM if I want all the models to be loaded simultaneously. Now, only the box predictor (last layer) is different for each network, so I want to pass my inputs in a batch, get the output from forward pass till only the last layer, and then pass the individual outputs to N different box predictors. What\u2019s the best strategy to implement this? I can only think of swapping the last layer before running on individual inputs, which is too slow.\nThanks","y":"Hi ,\nI think the best way to achieve that would be to create a Module inheriting from FasterRCNN (minus the box_predictor) with a list of your fine-tuned box_predictors, define a custom forward with an extra \u201cdetection scenario\u201d argument where you would switch on the right box_predictor, depending on that extra argument.","z":"Hi ,\nI think the best way to achieve that would be to create a Module inheriting from FasterRCNN (minus the box_predictor) with a list of your fine-tuned box_predictors, define a custom forward with an extra \u201cdetection scenario\u201d argument where you would switch on the right box_predictor, depending on that extra argument.\nThank you . I will try this out with clues from Add multiple FC layers in parallel with link \"https:\/\/discuss.pytorch.org\/t\/add-multiple-fc-layers-in-parallel\/19008\/8\" and post an update soon."},{"x":"I\u2019m trying to rewrite the answer to this stack thread with link \"https:\/\/stackoverflow.com\/questions\/58836274\/set-2d-array-elements-based-on-2d-index-array\/\" in native PyTorch language (as I don\u2019t want to be flicking back and forth between numpy and PyTorch objects.\nHowever I\u2019m struggling to make it work in tensor form, so I was wondering if anyone could give me a hand in seeing where I\u2019m going wrong?\n\nThe new form of the question in the thread goes as follows: Say you have the arrays as described in the thread as tensors\u2026\n<code class=\"lang-auto\">z = torch.tensor([[[0., 0., 0., 0.],\n        [0., 0., 0., 0.],\n        [0., 0., 0., 0.],\n        [0., 0., 0., 0.]],\n\n       [[0., 0., 0., 0.],\n        [0., 0., 0., 0.],\n        [0., 0., 0., 0.],\n        [0., 0., 0., 0.]],\n\n       [[0., 0., 0., 0.],\n        [0., 0., 0., 0.],\n        [0., 0., 0., 0.],\n        [0., 0., 0., 0.]]])\n\n\ni = torch.tensor([[2, 1, 2, 1],\n       [1, 1, 2, 1],\n       [1, 1, 1, 1],\n       [1, 0, 0, 1]])\n\n\nv = torch.tensor([[5., 5., 0., 4.],\n       [4., 6., 8., 3.],\n       [4., 0., 4., 8.],\n       [7., 6., 5., 7.]])\n<\/code>\nAnd you wish to perform the answer operation:\n<code class=\"lang-auto\">z = np.array([*map(i.__eq__,np.unique(i))])*np.array([v]*z.shape[0])\n<\/code>\n\u2026but in a PyTorch way, how does one alter this?\nI have tried:\n<code class=\"lang-auto\">z = torch.tensor([*map(it.__eq__,torch.unique(it))])*torch.stack([vt]*zt.shape[0])\n<\/code>\n\u2026but I still get an error relating to the use of a list inside torch.tensor() procedure I believe. You can see I\u2019ve altered the second multiplication term to use torch.stack() instead of a list multiplication but I can\u2019t see how to fix the list error.\nError:\n\nValueError: only one element tensors can be converted to Python scalars\n\nAny help on this matter would be greatly appreciated, thanks in advance!","y":"Hi,\nI think you just want to use scatter for this \n<code class=\"lang-auto\">i.unsqueeze_(0)\nv.unsqueeze_(0)\nz.scatter_(0, i, v)\n<\/code>","z":"Solved with:\n<code class=\"lang-auto\">z = torch.stack([*map(i.__eq__,torch.unique(i))]).type(torch.float)*(torch.stack([v]*z.shape[0])).type(torch.float)\n<\/code>\nHi,\nI think you just want to use scatter for this \n<code class=\"lang-auto\">i.unsqueeze_(0)\nv.unsqueeze_(0)\nz.scatter_(0, i, v)\n<\/code>"},{"x":"image1164\u00d7603 18.1 KB with link \"https:\/\/discuss.pytorch.org\/uploads\/default\/original\/3X\/2\/a\/2ae2548cd5beea95c48be8a158318e7d33f05676.png\"\nUsing pretrain model(ImageNet), I want to edit model to fit cifar dataset resnet architecture.\nI can edit layer, but don\u2019t know how to delete what I don\u2019t want to use.\nPlease let me know.\nThank you.","y":"See this topic.\n\n\n\n\nHow to remove the adaptive average pool layer from vgg19? with link \"https:\/\/discuss.pytorch.org\/t\/how-to-remove-the-adaptive-average-pool-layer-from-vgg19\/63633\" vision with link \"\/c\/vision\"\n\n\n    I have loaded the pretrained model of vgg19. How to remove the adaptive average pool layer which is present before the classifier?\n  \n\n","z":"See this topic.\n\n\n\n\nHow to remove the adaptive average pool layer from vgg19? with link \"https:\/\/discuss.pytorch.org\/t\/how-to-remove-the-adaptive-average-pool-layer-from-vgg19\/63633\" vision with link \"\/c\/vision\"\n\n\n    I have loaded the pretrained model of vgg19. How to remove the adaptive average pool layer which is present before the classifier?\n  \n\n\nOh, Thank you so much! ^^\nThen can I ask you one more Q?\nDo you know how to get hidden activation output?\nIn pretrained resnet model, I want to get each blocks output !\nThank you.\nSearch register_hook or register_forward_hook in PyTorch Forums.\nOne example is Understanding-Pytorch-Hooks with link \"https:\/\/www.kaggle.com\/sironghuang\/understanding-pytorch-hooks\".\nOr you can also search PyTorch Hook in Google to learn more."},{"x":"I am working on the object detection tutorial with link \"https:\/\/pytorch.org\/tutorials\/intermediate\/torchvision_tutorial.html#putting-everything-together\" on PyTorch. The original tutorial works fine with the few epochs given. I expanded it to large epochs and encounter out of memory error.\nI tried to debug it and find something interesting. This is the tool I am using:\n<code class=\"lang-auto\">def debug_gpu():\n    # Debug out of memory bugs.\n    tensor_list = []\n    for obj in gc.get_objects():\n        try:\n            if torch.is_tensor(obj) or (hasattr(obj, 'data') and torch.is_tensor(obj.data)):\n                tensor_list.append(obj)\n        except:\n            pass\n    print(f'Count of tensors = {len(tensor_list)}.')\n<\/code>\nAnd I used it to monitor the memory of training one epoch:\n<code class=\"lang-auto\">def train_one_epoch(model, optimizer, data_loader, device, epoch, print_freq):\n    ...\n    for images, targets in metric_logger.log_every(data_loader, print_freq, header):\n        # inference + backward + optimization\n        debug_gpu()\n<\/code>\nThe output is something like this:\n<code class=\"lang-auto\">Count of tensors = 414.\nCount of tensors = 419.\nCount of tensors = 424.\nCount of tensors = 429.\nCount of tensors = 434.\nCount of tensors = 439.\nCount of tensors = 439.\nCount of tensors = 444.\nCount of tensors = 449.\nCount of tensors = 449.\nCount of tensors = 454.\n<\/code>\nAs you can see, the count of tensors tracked by garbage collector increases constantly.\nRelevant files to execute can be found here with link \"https:\/\/github.com\/pytorch\/vision\/tree\/master\/references\/detection\".\nI have two questions:\n\nWhat is holding up the garbage collector to release these tensors?\nWhat should I do with the out of memory error?\n","y":"Hi,\nThank you for the kind reply.\n\nI used one gpu. But even if I used CPU, the situation happens;\nI did try to remove metric_logger, but it still happens. I doubt if the model itself caused it.\n\nThe original tutorial has such few epochs to trigger the OOM, but I believe you can easily repeat my situation with my code.","z":"Hi,\nI think we might need some more information before we can understand where the problem is.\n\nare you using multiple GPUs to train the model?\ncan you remove the metric_logger logging, and just iterate over the dataloader\n\nThose are the two things that came to my mind. I haven\u2019t experienced this OOM before, so I\u2019m not sure what else it could be for now\nHi,\nThank you for the kind reply.\n\nI used one gpu. But even if I used CPU, the situation happens;\nI did try to remove metric_logger, but it still happens. I doubt if the model itself caused it.\n\nThe original tutorial has such few epochs to trigger the OOM, but I believe you can easily repeat my situation with my code.\nHi , I think I found the reason: rpn.anchor_generator._cache keeps the pair with the model and the number of tensors it holds expands as proposal grows. I provide a sample with link \"https:\/\/github.com\/tengerye\/display_memory_leak\" for an easy repeat.\nBTW, I send a pull request with link \"https:\/\/github.com\/pytorch\/vision\/pull\/1657\" with the fix along with other data type error solution but codecov failed me. Would you please have a look?\nPR was merged. Thanks for the contribution  "},{"x":"Hi,\nI\u2019m trying to concatenate more than 1 dataset and after the concatenation, looks like the index of the dataset would be not in order.\nI\u2019m having 6 images and labels related datasets, and want to pass them in a similar sequence.\nThis is the length of each dataset:\n666\n80\n59\n39\n22\n72\nand this is how i concatenate them:\ntrain_all = torch.utils.data.ConcatDataset({train_good,train_blow_hole,train_break,train_crack,train_fray,train_uneven})\ntrain_all_label = torch.utils.data.ConcatDataset({train_label_good,train_label_blow_hole,train_label_break,train_label_crack,train_label_fray,train_label_uneven})\nAfter the concatenation:\nprint(train_all.cumulative_sizes)\n[72, 738, 797, 877, 899, 938]\nprint(train_all_label.cumulative_sizes)\n[72, 111, 777, 836, 858, 938]\nThe index is not the same anymore and this would causing issue in the training stage later. Is there an efficient way to concatenate the dataset while preserving their index in the concatenated dataset?\nThanks","y":"Hello,\nI am not super familiar with ConcatDataset, but after looking at the docs with link \"https:\/\/pytorch.org\/docs\/1.3.0\/data.html#torch.utils.data.ConcatDataset\", here is how I would proceed in your situation. First, I would create a CustomDataset class that inherits PyTorch\u2019s torch.utils.data.Dataset class so that CustomDataset has both the data and the target of each individual dataset. Second, once you have your 6 CustomDataset objects, you can use torch.utils.data.ConcatDataset to form your new dataset. I believe your mistake is in separating the data from the labels, which should be in the same class. Here is a small example:\n<code class=\"lang-auto\">class CustomDataset(torch.utils.data.Dataset):\n    def __init__(self, data, target):\n        self.data = data\n        self.target = target\n\n    def __getitem__(self, index):\n        return self.data[index], self.target[index]\n\n    def __len__(self):\n        return len(self.data)\n\nx = torch.rand(size=(666, 10), dtype=torch.float32)\nx_labels = torch.zeros(size=(666,), dtype=torch.int32)\ny = torch.rand(size=(80, 10), dtype=torch.float32)\ny_labels = torch.ones(size=(80,), dtype=torch.int32)\nz = torch.rand(size=(59, 10), dtype=torch.float32)\nz_labels = torch.zeros(size=(59,), dtype=torch.int32)\n\nx_dataset = CustomDataset(x, x_labels)\ny_dataset = CustomDataset(y, y_labels)\nz_dataset = CustomDataset(z, z_labels)\n\ntrain_dataset = torch.utils.data.ConcatDataset([x_dataset, y_dataset, z_dataset])\nprint(train_dataset.cumulative_sizes)\n<\/code>\nHope it helps!","z":"is there anyone have any idea to solve the issue or possibly to provide some reference?\nHello,\nI am not super familiar with ConcatDataset, but after looking at the docs with link \"https:\/\/pytorch.org\/docs\/1.3.0\/data.html#torch.utils.data.ConcatDataset\", here is how I would proceed in your situation. First, I would create a CustomDataset class that inherits PyTorch\u2019s torch.utils.data.Dataset class so that CustomDataset has both the data and the target of each individual dataset. Second, once you have your 6 CustomDataset objects, you can use torch.utils.data.ConcatDataset to form your new dataset. I believe your mistake is in separating the data from the labels, which should be in the same class. Here is a small example:\n<code class=\"lang-auto\">class CustomDataset(torch.utils.data.Dataset):\n    def __init__(self, data, target):\n        self.data = data\n        self.target = target\n\n    def __getitem__(self, index):\n        return self.data[index], self.target[index]\n\n    def __len__(self):\n        return len(self.data)\n\nx = torch.rand(size=(666, 10), dtype=torch.float32)\nx_labels = torch.zeros(size=(666,), dtype=torch.int32)\ny = torch.rand(size=(80, 10), dtype=torch.float32)\ny_labels = torch.ones(size=(80,), dtype=torch.int32)\nz = torch.rand(size=(59, 10), dtype=torch.float32)\nz_labels = torch.zeros(size=(59,), dtype=torch.int32)\n\nx_dataset = CustomDataset(x, x_labels)\ny_dataset = CustomDataset(y, y_labels)\nz_dataset = CustomDataset(z, z_labels)\n\ntrain_dataset = torch.utils.data.ConcatDataset([x_dataset, y_dataset, z_dataset])\nprint(train_dataset.cumulative_sizes)\n<\/code>\nHope it helps!\nHi beaupreda,\nThanks and appreciate the information.\nI didn\u2019t aware of the inherit feature until you mention it, this sure looks useful.\nActually what i\u2019m trying to do is perform segmentation classification with unet, so in each epoch, i would need to feed in a mask and an image. Do you aware of a better way to do this?\nAnyway, i will proceed with your suggestion and see how it would work, thanks again"},{"x":"I have some data of the following form,\nshape = (batch_size X max_seq_len X embedding_dim)\n<code class=\"lang-auto\">np.ndarray([torch.tensor([torch.tensor(), torch.tensor(), ....]), ...])\n<\/code>\nIs there a convenient way to convert it to,\n<code class=\"lang-auto\">torch.tensor([torch.tensor([torch.tensor(), torch.tensor(), ....]), ...])\n<\/code>\nRight now, I do\n<code class=\"lang-auto\">torch.from_numpy(devX)\n<\/code>\nand get the error\n<code class=\"lang-auto\">TypeError: can't convert np.ndarray of type numpy.object_. The only supported types are: float64, float32, float16, int64, int32, int16, int8, uint8, and bool.\n<\/code>","y":"Well, so ideally you would have shown more about how your data looks like (I must admit I don\u2019t understand that exactly), but say it is similar to\n<code class=\"lang-python\">a,b,c = torch.randn(3, 2, 10) # three 2x10 tensors\narr = numpy.array([a,b,c], dtype=object) # array of size 3 of 2x10 tensors\n<\/code>\nthen you can do\n<code class=\"lang-python\">t = torch.stack(list(arr), dim=0) # 3x2x10 tensor\n<\/code>\nBest regards\nThomas","z":"Tensors fundamentally cannot hold arbitrary objects, but assuming you have a 1d array of 2d tensors, you can use stack to get a 3d Tensor.\nBest regards\nThomas\nThanks for the reply! Could you elaborate a little more? I\u2019m sort of new to the framework. Thanks.\nWell, so ideally you would have shown more about how your data looks like (I must admit I don\u2019t understand that exactly), but say it is similar to\n<code class=\"lang-python\">a,b,c = torch.randn(3, 2, 10) # three 2x10 tensors\narr = numpy.array([a,b,c], dtype=object) # array of size 3 of 2x10 tensors\n<\/code>\nthen you can do\n<code class=\"lang-python\">t = torch.stack(list(arr), dim=0) # 3x2x10 tensor\n<\/code>\nBest regards\nThomas"},{"x":"\n\n\n\nHello, I tried to use L-BFGS optimizer as\nself.optimizer = optim.LBFGS(self.net.parameters(),max_iter=1)\nAs I know, it only saves data history_size times. When I run\n<code class=\"lang-auto\">for epoch in range(30000):\n    self.optimizer.step(closure)\n    self.train_loss.append(closure().cpu())\nif (epoch%(n_print\/\/self.iter)==0) &amp; (epoch>0):\n    print('[Epoch: %s], test_error: [%.4f, %.4f], %.3f seconds went by' %(self.iter*epoch, mean_loss, var_loss, time.time()-st_time))\n<\/code>\nit gives training records\n<code class=\"lang-auto\">[Epoch: 1000], test_error: [0.0022, 0.0294], 172.912 seconds went by\n[Epoch: 2000], test_error: [0.0009, 0.0135], 290.782 seconds went by\n[Epoch: 3000], test_error: [0.0006, 0.0099], 407.872 seconds went by\n[Epoch: 4000], test_error: [0.0006, 0.0082], 524.397 seconds went by\n[Epoch: 5000], test_error: [0.0007, 0.0063], 640.674 seconds went by\n[Epoch: 6000], test_error: [0.0007, 0.0056], 756.475 seconds went by\n[Epoch: 7000], test_error: [0.0008, 0.0050], 840.142 seconds went by\n[Epoch: 8000], test_error: [0.0008, 0.0044], 958.420 seconds went by\n[Epoch: 9000], test_error: [0.0008, 0.0038], 1074.280 seconds went by\n[Epoch: 10000], test_error: [0.0008, 0.0032], 1191.190 seconds went by\n[Epoch: 11000], test_error: [0.0009, 0.0026], 1309.013 seconds went by\n[Epoch: 12000], test_error: [0.0009, 0.0025], 1427.236 seconds went by\n[Epoch: 13000], test_error: [0.0009, 0.0021], 1544.839 seconds went by\n[Epoch: 14000], test_error: [0.0009, 0.0020], 1661.872 seconds went by\n[Epoch: 15000], test_error: [0.0009, 0.0017], 1690.628 seconds went by\n[Epoch: 16000], test_error: [0.0009, 0.0015], 1808.329 seconds went by\n[Epoch: 17000], test_error: [0.0009, 0.0013], 1874.316 seconds went by\n[Epoch: 18000], test_error: [0.0010, 0.0014], 1991.727 seconds went by\n[Epoch: 19000], test_error: [0.0010, 0.0011], 2109.931 seconds went by\n[Epoch: 20000], test_error: [0.0010, 0.0012], 2192.570 seconds went by\n---------------------------------------------------------------------------\nRuntimeError                              Traceback (most recent call last)\n<ipython-input-4-ad2c47ee49d3> in <module>\n----> 1 model.train_model(lr=1e0, max_iter = 30000, n_print=1000)\n\n<ipython-input-2-e72f5af10348> in train_model(self, lr, max_iter, n_print)\n    150 \n    151         for epoch in range(max_iter\/\/self.iter):\n--> 152             self.optimizer.step(closure)\n    153 \n    154             mean_ResNet = []\n\n\/usr\/local\/lib\/python3.6\/dist-packages\/torch\/optim\/lbfgs.py in step(self, closure)\n    430                     # the reason we do this: in a stochastic setting,\n    431                     # no use to re-evaluate that function here\n--> 432                     loss = float(closure())\n    433                     flat_grad = self._gather_flat_grad()\n    434                     opt_cond = flat_grad.abs().max() <= tolerance_grad\n\n<ipython-input-2-e72f5af10348> in closure()\n    133             #pde_loss\n    134             u_x = grad(u.sum(), X, create_graph=True)[0]\n--> 135             tau_x = grad(tau.sum(), X, create_graph=True)[0]\n    136 \n    137             loss_constitutive = ((k * u_x + tau) ** 2).mean()\n\n\/usr\/local\/lib\/python3.6\/dist-packages\/torch\/autograd\/__init__.py in grad(outputs, inputs, grad_outputs, retain_graph, create_graph, only_inputs, allow_unused)\n    155     return Variable._execution_engine.run_backward(\n    156         outputs, grad_outputs, retain_graph, create_graph,\n--> 157         inputs, allow_unused)\n    158 \n    159 \n\nRuntimeError: CUDA out of memory. Tried to allocate 24.00 MiB (GPU 0; 15.90 GiB total capacity; 15.23 GiB already allocated; 23.62 MiB free; 92.33 MiB cached)\n<\/code>\nI think memory error should happened before epoch=100 or it never happen, am I miss something?\n\n\n\n\nOn the other hand, I tried\n<code class=\"lang-auto\">self.optimizer = optim.LBFGS(self.net.parameters(),max_iter=500)\nfor epoch in range(600):\n    self.optimizer.step(closure)\n    self.train_loss.append(closure().cpu())\n    print('[Epoch: %s], test_error: [%.4f, %.4f], %.3f seconds went by' %(self.iter*epoch,\n<\/code>\nmean_loss, var_loss, time.time()-st_time))\nIn other words, I changed (max_iter, max_epoch) from (1,30000) to (500,600) to compare.\nAll the time I set\n<code class=\"lang-auto\">np.random.seed(0)\ntorch.manual_seed(0)\n<\/code>\nto fix random seed, bit results give different test errors.  Why this happends? Thank you for any helps!\n------------------------------------------------------------------------added--------------------------------------------------\nFor the question 1, I found that train_loss.append part arise such an error, so\n<code class=\"lang-auto\">for epoch in range(30000):\n    self.optimizer.step(closure)\n   #self.train_loss.append(closure().cpu())\n<\/code>\nwill not cause memory error.\nHow can I record my training loss without memory occupied?","y":"Hi,\nThe memory error in your first question is because when you do self.train_loss.append(closure().cpu()) you save both the value of the loss and all the metadata that allows to compute gradients. You can use detach to fix this if you want as well self.train_loss.append(closure().cpu().detach()).\nFor the seed, you can check the notes on reproducibility but the short answer is that it is very hard to get reproducible results because of how float arithmetic works. And in your case, since you do operations in different orders, it is impossible to get the exact same result.","z":"<code class=\"lang-auto\">loss = self.optimizer.step(closure).data.cpu()\nself.train_loss.append(loss)\n<\/code>\nresolved the problem. I\u2019m waiting for the answer of the second question.\nHi,\nThe memory error in your first question is because when you do self.train_loss.append(closure().cpu()) you save both the value of the loss and all the metadata that allows to compute gradients. You can use detach to fix this if you want as well self.train_loss.append(closure().cpu().detach()).\nFor the seed, you can check the notes on reproducibility but the short answer is that it is very hard to get reproducible results because of how float arithmetic works. And in your case, since you do operations in different orders, it is impossible to get the exact same result.\nSo, different results caused by the randomness? I got it, thank you so much!\nNot the randomness but the fact that for float numbers (a + b) + c != a + (b + c)."},{"x":"\nhow to check the model on which specific cuda device, not just is_cuda, is there a solution?\nif the input on cuda, but the model hasn\u2019t been explicitly declared on cuda, in this time, i execute mode(input),what happens? if model and input are not on the same device ,it\u2019ll be wrong?\nabout the details of DataParallel, what is the difference between the host cuda device data and other devices data, the model and input allocate and sth , what is the inner mechanics of DataParallel, it seems host device uses more memory.\n\ntotally,i feel a little confused on the details of data allocate\u2026someone can help me ? thx!!","y":"Hi,\n\n\nYou can use tensor.device. It returns you the current device of the tensor, and the index of the GPU if the device is cuda.\n\n\nIf model and input are not in the same device, it will crash. You cannot do operations between tensors in cpu and tensors in cuda.\n\n\nExemple:\n<code class=\"lang-auto\">In [12]: torch.rand(10, 10).cuda() + torch.rand(10, 10)                                                                                                                                                            \n---------------------------------------------------------------------------\nRuntimeError                              Traceback (most recent call last)\n<ipython-input-12-4280bdc898ed> in <module>\n----> 1 torch.rand(10, 10).cuda() + torch.rand(10, 10)\nRuntimeError: expected device cuda:0 and dtype Float but got device cpu and dtype Float\n<\/code>\n\nIn DataParallel, the batch will be split among the gpus. At the end, the gradients will be sent back to the host and summed, and the step happens here.\nSo every device is hosting the whole model, plus a portion of the batch.\n","z":"Hi,\n\n\nYou can use tensor.device. It returns you the current device of the tensor, and the index of the GPU if the device is cuda.\n\n\nIf model and input are not in the same device, it will crash. You cannot do operations between tensors in cpu and tensors in cuda.\n\n\nExemple:\n<code class=\"lang-auto\">In [12]: torch.rand(10, 10).cuda() + torch.rand(10, 10)                                                                                                                                                            \n---------------------------------------------------------------------------\nRuntimeError                              Traceback (most recent call last)\n<ipython-input-12-4280bdc898ed> in <module>\n----> 1 torch.rand(10, 10).cuda() + torch.rand(10, 10)\nRuntimeError: expected device cuda:0 and dtype Float but got device cpu and dtype Float\n<\/code>\n\nIn DataParallel, the batch will be split among the gpus. At the end, the gradients will be sent back to the host and summed, and the step happens here.\nSo every device is hosting the whole model, plus a portion of the batch.\n\nthanks , i understand it now,"},{"x":"I get the data from torchvision.datasets.MNIST.\nThen, I try to save the path and labels in a txt named mnist_test.txt,but here is an error\nque1.png958\u00d7881 32.6 KB with link \"https:\/\/discuss.pytorch.org\/uploads\/default\/original\/3X\/f\/1\/f1cb69c843f24ba6e7bfe866504b22aec6807a47.png\"\ni try to change the type of img,but it doesn\u2019t work.\nThanks for your help !","y":"i found delete the sentence  \u201cif arr.dtype == bool:\nwarn(\u2019%s is a boolean image: setting True to 1 and False to 0\u2019 % fname)\u201d   in _io.py.\nit works!\nqu2.png732\u00d7327 29.7 KB with link \"https:\/\/discuss.pytorch.org\/uploads\/default\/original\/3X\/3\/5\/3536dbd5a0eeef63678fc3f6f3a500f739c23b3d.png\"","z":"i found delete the sentence  \u201cif arr.dtype == bool:\nwarn(\u2019%s is a boolean image: setting True to 1 and False to 0\u2019 % fname)\u201d   in _io.py.\nit works!\nqu2.png732\u00d7327 29.7 KB with link \"https:\/\/discuss.pytorch.org\/uploads\/default\/original\/3X\/3\/5\/3536dbd5a0eeef63678fc3f6f3a500f739c23b3d.png\""},{"x":"I am a new user of Pytorch. I am adapting the Unet segmentation model, but I have an error in the evaluation of the Cross Entropy Loss  function during training.\nI used  torch.utils.data.Dataset to build a specific dataset\ntrain_data = DataLoaderSegmentation(train_path, mode=\u2018train\u2019)\ntrain_loader = DataLoader(train_data, batch_size = 4, shuffle=True, num_workers=4)\nwhich works fine and load both the image and its mask in the format:\n[batch,channel,W,H] , like  [4,1,220,220], for both images and masks. I put channel to 1 since for background and foreground segmentation.\nParameters for train:\nmodel = Unet(n_filters=32, n_class=1, input_channels=1)\ndevice = \u2018cuda\u2019\nLR = 0.001\ncriterion = nn.CrossEntropyLoss()\noptimizer = torch.optim.Adam(model.parameters(), lr=LR)\nmodel = model.to(device)\nDuring training I have an error on the marked line:\nfor data, target in train_loader:\ndata = data.to(device)\ntarget = target.to(device)\noptimizer.zero_grad()\noutput = model(data)\nloss = criterion(output, target) ############## ERROR\nfor data, target in train_loader:\ndata = data.to(device)\ntarget = target.to(device)\noptimizer.zero_grad()\noutput = model(data)\nloss = criterion(output, target) ######## ERROR\n\nFile \u201cC:\\Anaconda3\\envs\\envTorch\\lib\\site-packages\\torch\\nn\\functional.py\u201d, line 1840, in nll_loss\n**    ret = torch._C._nn.nll_loss2d(input, target, weight, _Reduction.get_enum(reduction), ignore_index)**\nRuntimeError: Expected object of scalar type Long but got scalar type Float for argument #2 \u2018target\u2019 in call to _thnn_nll_loss2d_forward\n\nI see that  target and output have the same dim [4,1,220,220] but  the criterion(output, target) trow error.\nSorry, but after read some answer, I cannot resolve it yet.","y":"\n\n\n KFrank:\n\nAssuming batchsize = 4 , nClasses = 5 , H = 224 , and\nW = 224 , CrossEntropyLoss will be expecting the input\n(prediction) you give it to be a FloatTensor of shape\n(4, 5, 244, 244) , and the target (ground truth) to be a\nLongTensor of shape (4, 244, 244).\n\n\nDear  you hit the nail, thank you. Thank you.\nThe target is a single image HxW, each pixel labeled as belonging to [0\u2026nClasses-1]\nloss = CrossEntropyLoss(input, target)  in the specified  size computes correctly (I was too wrong).\nCan you suggest me some place to learn the keys of DL, I am starting on it. Thank you again.","z":"Do output.float() and  target.float() and see if it works.\n,  It gives the same error.\nRuntimeError: Expected object of scalar type Long but got scalar type Float for argument #2 \u2018target\u2019 in call to _thnn_nll_loss2d_forward\nWould the error come from the model?\n\nloss = criterion(output.float(), target.float())\nTraceback (most recent call last):\nFile \u201c\u201d, line 1, in \nloss = criterion(output.float(), target.float())\nFile \u201cC:\\Anaconda3\\envs\\envTorch\\lib\\site-packages\\torch\\nn\\modules\\module.py\u201d, line 541, in call\nresult = self.forward(*input, **kwargs)\nFile \u201cC:\\Anaconda3\\envs\\envTorch\\lib\\site-packages\\torch\\nn\\modules\\loss.py\u201d, line 916, in forward\nignore_index=self.ignore_index, reduction=self.reduction)\nFile \u201cC:\\Anaconda3\\envs\\envTorch\\lib\\site-packages\\torch\\nn\\functional.py\u201d, line 2009, in cross_entropy\nreturn nll_loss(log_softmax(input, 1), target, weight, None, ignore_index, None, reduction)\nFile \u201cC:\\Anaconda3\\envs\\envTorch\\lib\\site-packages\\torch\\nn\\functional.py\u201d, line 1840, in nll_loss\nret = torch._C._nn.nll_loss2d(input, target, weight, _Reduction.get_enum(reduction), ignore_index)\nRuntimeError: Expected object of scalar type Long but got scalar type Float for argument #2 \u2018target\u2019 in call to _thnn_nll_loss2d_forward\n\nlog_softmax,\nthe last activation function in the model is a nn.Sigmoid(), no a nn.Softmax(), it could be?\nSorry, I meant try criterion(ouput.double(),target.double())\nLet me know if this works\nsorry, it doesn\u2019t work.\nI only changed the loss function to\ncriterion = torch.nn.BCEWithLogitsLoss()\nand the error disappeared. However during training the accuracy showed is always low.\nIf you are dealing with a binary classification use case, a single output channel and nn.BCEWithLogitsLoss should be working.\nMake sure to pass raw logits to the criterion (no sigmoid at the end).\nnn.CrossEntropyLoss is usually applied for multi class classification\/segmentation use cases, where you are dealing with more than two classes.\nIn this case, your target should be a LongTensor, should not have the channel dimension, and should contain the class indices in [0, nb_classes-1].\nSince your accuracy is low, could you try to overfit a small data sample and see, if your model and training routine can successfully overfit this sample?\nThank you for your comments. I can\u2019t make it work (I no tested more nn.BCEWithLogitsLoss, I focused in N=5 multi class segmentation). Images are [1,220,220] and  their mask [5,220,220]. Each channel is a binary image with values 0 and 1, 1s for the object of interest on the respective channel, and 0s for the background. I don\u2019t know if this mask form is correct.\nThe error occurs in the line\n\nloss = criterion(output, target) ,\n\nduring the first pass. The error seems related to data type, but If I am not mistaken, internal tensors work with float numbers (all images and masks are floats).\nHere is my code:\nmodel = UnetModel(n_filters=32, n_class=5, input_channels=1)\ndevice = \u2018cuda\u2019\nmodel = model.to(device)\ncriterion = torch.nn.CrossEntropyLoss()\noptimizer = torch.optim.Adam(model.parameters(), lr=0.001)\nmodel.train()\nepochs = 20\nitr = 1\np_itr = 50\ntotal_loss = 0\ntrain_loss = []\ntrain_acc = []\nfor epoch in range(epochs):\nfor data, target in train_loader:\n    data = data.to(device)\n    target = target.to(device)\n    \n    optimizer.zero_grad()\n    output = model(data) \n    loss = criterion(output, target) ######## **Error**\n    loss.backward()  \n    optimizer.step() \n            \n    total_loss += loss.item()        \n    if itr%p_itr == 0:\n        pred = torch.argmax(output, dim=1)\n        correct = pred.eq(target)\n        acc = torch.mean(correct.float())\n        print('[Epoch {}\/{}] Iteration {} -> Train Loss: {:.4f}, Accuracy: {:.3f}'.format(epoch+1, epochs, itr, total_loss\/p_itr, acc))\n        train_loss.append(total_loss\/p_itr)\n        train_acc.append(acc)\n        total_loss = 0            \n    itr += 1\n\nThe error is\nFile \u201cC:\\Anaconda3\\envs\\envTorch\\lib\\site-packages\\torch\\nn\\functional.py\u201d, line 1840, in nll_loss\nret = torch._C._nn.nll_loss2d(input, target, weight, _Reduction.get_enum(reduction), ignore_index)\nRuntimeError: Expected object of scalar type Long but got scalar type Float for argument #2 \u2018target\u2019 in call to _thnn_nll_loss2d_forward\nHi Joe!\n\n\n\n joekid:\n\nThank you for your comments. I can\u2019t make it work\n\u2026\nThe error occurs in the line\n\nloss = criterion(output, target) ,\n\nHere is my code:\n\u2026\ncriterion = torch.nn.CrossEntropyLoss()\n\u2026\nThe error is\n\u2026\nRuntimeError: Expected object of scalar type Long but got scalar type Float for argument #2 \u2018target\u2019 in call to _thnn_nll_loss2d_forward\n\n\nThe error (\u201cscalar type Long\u201d) and the documentation for\nyour loss function, CrossEntropyLoss with link \"https:\/\/pytorch.org\/docs\/stable\/nn.html#torch.nn.CrossEntropyLoss\", should serve to sort\nthings out.\nAs  mentioned in one of his posts, above,\nCrossEntropyLoss takes a LongTensor for its targets,\nnot a FloatTensor, hence the error.\nCrossEntropyLoss (perhaps better called more explicitly\n\u201ccategorical cross-entropy loss\u201d) requires integer class labels\nin [0, nClass - 1] for its targets.  Provided the target in\nyour code is, indeed, class labels (as compared to probabilities,\nor some such), it should suffice to convert it to a LongTensor,\ne.g., loss = criterion (output, target.long()).\nBCELossWithLogits does take a FloatTensor for its targets.\n(It takes probabilities, not just 0,1 class labels.)  So that\u2019s why\nyou didn\u2019t get the error when you tried BCELossWithLogits.\nGood luck.\nK. Frank\nI am getting a new error.\nDear .\n\nI remake the target as [batch, nClasses, H, W], being nClasses = 5. I split  the objects labels along layers as follows (but I don\u2019t know if it is correct):\n\nLayer 0: image with all  pixels value to zero, label 0 = background\nLayer 1:  pixels value to 1 for object 1 (e.g. dog), and 0s for the background\nLayer 2:  pixels value to 2 for object 2 (e.g. cat), and 0s for the background\nand son on.\nis it a reasonable model for the layers in semantic segmentation?\n\nHowever a get a new error:\n\nRuntimeError: 1only batches of spatial targets supported (non-empty 3D tensors) but got targets of size: : [4, 5, 224, 224] ,\nI use batchsize=5, nClasses=1.  The input data is [4,1,224,244] aiming to obtain [4,5,224,244],\nHello Joe!\n\n\n\n joekid:\n\n\nI remake the target as [batch, nClasses, H, W], being nClasses = 5.\n\n\n\nNo, this is not right.  Your target (\u201cground truth\u201d) should be (a\nLongTensor) of shape (batch, H, W).  For any given sample\nwithin the batch, the values of the  \u201cpixels\u201d in your \u201cH x W\u201d \u201cimage\u201d\nshould be the integer class labels in [0, 5).\nYou don\u2019t use multiple \u201cLayers\u201d to specify which class a given\npixel is in \u2013 you just use an integer class label as the value of\nthat pixel.\nNote, the input to CrossEntropyLoss is not of the same shape\nas the target; the input is of shape\n(batch, nClasses, H, W).  For any given sample in the batch,\neach pixel in the H x W input \u201cimage\u201d has nClasses values.  These\nare the \u201clogits\u201d (\u201cscores\u201d in -infinity to +infinity) that tell you how likely\nthe model thinks a given pixel is to be in each of the nClasses\nclasses.\n\n\n\nHowever a get a new error:\n\nRuntimeError: 1only batches of spatial targets supported (non-empty 3D tensors) but got targets of size: : [4, 5, 224, 224] ,\nI use batchsize=5, nClasses=1.  The input data is [4,1,224,244] aiming to obtain [4,5,224,244],\n\n\nThis error message looks reasonable in light of what I said above\n(except that it looks like you are actually using batchsize = 4,\nnot batchsize = 5).\nAssuming batchsize = 4, nClasses = 5, H = 224, and\nW = 224, CrossEntropyLoss will be expecting the input\n(prediction) you give it to be a FloatTensor of shape\n(4, 5, 244, 244), and the target (ground truth) to be a\nLongTensor of shape (4, 244, 244).\nGood luck.\nK. Frank\n\n\n\n KFrank:\n\nAssuming batchsize = 4 , nClasses = 5 , H = 224 , and\nW = 224 , CrossEntropyLoss will be expecting the input\n(prediction) you give it to be a FloatTensor of shape\n(4, 5, 244, 244) , and the target (ground truth) to be a\nLongTensor of shape (4, 244, 244).\n\n\nDear  you hit the nail, thank you. Thank you.\nThe target is a single image HxW, each pixel labeled as belonging to [0\u2026nClasses-1]\nloss = CrossEntropyLoss(input, target)  in the specified  size computes correctly (I was too wrong).\nCan you suggest me some place to learn the keys of DL, I am starting on it. Thank you again."},{"x":"Hi, everybody, I met a strange problem when trying to define a weighted cross entropy myself.\nMy loss func is simple:\n<code class=\"lang-auto\">def weighted_BCE(sigmoid_x, targets):                  \n#this is a weighted Binary Cross Entropy\n    assert sigmoid_x.size() == targets.size()           \n#make sure input and label have same shape\n    count_p = (targets == 1.0).sum() + 1                 \n    count_n = (targets == 0.0).sum() + 1                \n#count 1(positive) and 0(negative), add 1 to avoid 0\n    loss = -((targets * sigmoid_x.log()) * (1 \/ count_p)) - (((1 - targets) * (1 - sigmoid_x).log()) * (1 \/ count_n))\n#divide the positive part and negative part with their count respectively\n    return loss.mean()\n<\/code>\nthen try it with a random input and all 0 label\n<code class=\"lang-auto\">if __name__ == '__main__':\n    a = np.random.uniform(low = 0.0, high = 1.0, size = (3, 3))\n    a = torch.from_numpy(a).float().to('cuda:0')\n    a.requires_grad = True\n    b = np.zeros((3, 3))\n    b = torch.from_numpy(b).float().to('cuda:0')\n    b.requires_grad = False\n    loss = weighted_BCE(a, b)\n    loss2 = torch.mean(-((b * a.log()) * (1 \/ 1)) - (((1 - b) * (1 - a).log()) * (1 \/ 10)))\n# calculate the loss in the same way without definition\n    print(loss.item())\n    print(loss2.item())\n    print((loss - loss2).item())\n<\/code>\n<code class=\"lang-auto\"># here is the result\n0.0\n0.15303052961826324\n-0.15303052961826324 #this is the supposed to be zero\n<\/code>\nIt may be a silly question, but I can\u2019t figure out the reason why the loss func returns 0.0 in a definition and different result when calculate it in the main func.\nMy environment is\n<code class=\"lang-auto\">Os : Ubuntu 18.04.2 LTS\nGPU: GTX 1080ti\npytorch: 1.2.0\n<\/code>","y":"Hi,\nThe logic you using is correct, actually the data types are causing this differences.\nFor instance, if you use your own code in functions:\n<code class=\"lang-auto\">count_p = (b == 1.0).sum() + 1\ncount_n = (b == 0.0).sum() + 1                \nloss2 = torch.mean(-((b * a.log()) * (1 \/ count_p)) - (((1 - b) * (1 - a).log()) * (1 \/ count_n)))\n<\/code>\ncount_p instead of 1 and count_n instead of 10, you will get same result. Note if you add 1. or 10., you will get different result.\nSo the most secure way to use constant numbers is to use torch.tensor() method.\n<code class=\"lang-auto\">loss2 = torch.mean(-((b * a.log()) * (1 \/ torch.tensor([1]).cuda())) - (((1 - b) * (1 - a).log()) * (1 \/ torch.tensor([10]).cuda())))\n<\/code>\ntorch.tensor() retains the datatype while moves tensor to the given device.\nBests\nNik","z":"Hi,\nThe logic you using is correct, actually the data types are causing this differences.\nFor instance, if you use your own code in functions:\n<code class=\"lang-auto\">count_p = (b == 1.0).sum() + 1\ncount_n = (b == 0.0).sum() + 1                \nloss2 = torch.mean(-((b * a.log()) * (1 \/ count_p)) - (((1 - b) * (1 - a).log()) * (1 \/ count_n)))\n<\/code>\ncount_p instead of 1 and count_n instead of 10, you will get same result. Note if you add 1. or 10., you will get different result.\nSo the most secure way to use constant numbers is to use torch.tensor() method.\n<code class=\"lang-auto\">loss2 = torch.mean(-((b * a.log()) * (1 \/ torch.tensor([1]).cuda())) - (((1 - b) * (1 - a).log()) * (1 \/ torch.tensor([10]).cuda())))\n<\/code>\ntorch.tensor() retains the datatype while moves tensor to the given device.\nBests\nNik\nThank you and sorry for my late response. In fact what I want is a little different.\nBased on your advice, I change the\n<code class=\"lang-auto\">loss = -((targets * sigmoid_x.log()) * (1 \/ count_p)) - (((1 - targets) * (1 - sigmoid_x).log()) * (1 \/ count_n))\n<\/code>\nin my loss def to\n<code class=\"lang-auto\">loss = -((targets * sigmoid_x.log()) * (1 \/ count_p.float())) - (((1 - targets) * (1 - sigmoid_x).log()) * (1 \/ count_n.float()))\n<\/code>\nand it gives me the right result.\nBut I am still unsure about the int and float type here.\nAs count_p is a int type and sigmoid_x.log() is a float type, float multiply a int type should result in a float type, isn\u2019t it? Or is there anything different with tensor?\nActually, You are right about the casting to float. But something I am not sure about is the system dependent type sizes or even other parameters. I tried using 1. to convert to python default float type and it did not work. I do not know much about C++ (backend of PyTorch) and Python variable system and type checking, so that\u2019s why I suggest using torch.tensor() because it automatically handle type conversion of retaining current type of numpy ndarray object as argument.\nI did not check numpy float type number to check the answer because in the docs it says PyTorch tensors and numpy ndarrays can be converted to each other seamlessly. (Using sole constant number is python pure not numpy.)\nFor instance, numpy has system dependent int and independent version too. There all can effect and an interface is needed to handle this situations which I think torch.tensor is doing it.\nSomething else I thing need to be checked is that GPU is considering different sizes rather than normal code or not.\nMy knowledge is very basic about these topics.\nI think you can create new question and demonstrating different answers regarding using python pure constant number or numpy array or even the tensor. PyTorch dev team is really good and almost all the time answer the question.\nBests\nThank you for your kindness and patience. I will take your advice.\nedit:\njust found something alike on github.\n\n\ngithub.com\/pytorch\/pytorch with link \"https:\/\/github.com\/pytorch\/pytorch\/issues\/5411\"\n\n\n\n\n\n\n\n\nInteger division behavior is different from Python and NumPy with link \"https:\/\/github.com\/pytorch\/pytorch\/issues\/5411\"\n\n\n\n        opened 04:57PM - 26 Feb 18 UTC\n\n\nmdraw with link \"https:\/\/github.com\/mdraw\"\n\n\n\n\n\nPyTorch version: current master (10fd27)\nHow you installed PyTorch (conda, pip, source): from source\nPython version: 3.6\nWhen dividing two ints in pure Python,...\n\n\nmedium priority (this tag is deprecated)\nmodule: cpu\nmodule: numpy\ntopic: numerical-reproducibility\ntriaged\n\n\n\n\n\n\n"},{"x":"Hi, thanks first for the amazing tools. I am currently trying to use grid_sample() to sample colors from a texture image to be the color of vertices from a 3D .obj model (part of a rendering process), and then the color is used to compute loss and gradients are expected to back-propagate to the texture image. However, in the experiment gradients on the texture image are all zero.\nTo find out the reason, I replace the grid with an \u201cordered\u201d one, like those generated by affine_grid(), then the gradients are non-zero. Thus, I am wondering if this is because the original grid is defined by the UV coordinates corresponding to each vertice in the 3D model, and thus there is no regularity in it. I would appreciate if anyone could tell me whether there are certain constraints on defining the grid, or if there may be other reasons that make the grad zero.\nMany thanks!!","y":"It turns out the gradients are not totally zeros. It\u2019s just updating in a very very low speed. This is a stupid question and I will delete it soon. Thanks and sorry for your time!","z":"Hi,\nWould you have a small code sample that reproduces this behavior please?\nIt turns out the gradients are not totally zeros. It\u2019s just updating in a very very low speed. This is a stupid question and I will delete it soon. Thanks and sorry for your time!"},{"x":"I want to re-arrange the MNIST into to 2 classes - odd numbers in one class and even numbers in another class, then use this rearranged dataset for train and test. How to do it?","y":" The following works for me. Thanks \n<code class=\"lang-auto\">for i in range(10):\n  idx = (dataset.targets==i)\n  if (i == 0) or ((i % 2) == 0): dataset.targets[idx] = 0\n\n  else: dataset.targets[idx] = 1\n<\/code>","z":"You could try to adapt this code with link \"https:\/\/discuss.pytorch.org\/t\/how-to-use-one-class-of-number-in-mnist\/26276\/2\" for your use case.\nI will try it and get back.\n The following works for me. Thanks \n<code class=\"lang-auto\">for i in range(10):\n  idx = (dataset.targets==i)\n  if (i == 0) or ((i % 2) == 0): dataset.targets[idx] = 0\n\n  else: dataset.targets[idx] = 1\n<\/code>"},{"x":"I am training my own code, and I want to implement one function:\nAfter training n(=100) iterations, I want to see feature maps.\nNow I have done with register_forward_hook to see the feature maps, But I want to see them after n iterations. Any idea about how to implement this?\nThanks.","y":"That\u2019s an interesting approach!\nBased on this, you could also create a \u201chook class\u201d holding this internal parameter, which could be simpler than creating new modules:\n<code class=\"lang-python\">class ActivationHook():\n    def __init__(self, name, every_steps=1):\n        self.name = name\n        self.every_steps = every_steps\n        self.steps = 0\n        self.activation = {}\n    \n    def hook(self, model, input, output):\n        if (self.steps % self.every_steps) == 0:\n            self.activation[self.name] = output.detach()\n        self.steps += 1\n    \n\nmodel = MyModel()\nact_hook = ActivationHook('fc2', every_steps=2)\nmodel.fc2.register_forward_hook(act_hook.hook)\nx = torch.randn(1, 25)\noutput = model(x)\nprint(act_hook.activation['fc2'])\n<\/code>","z":"I have implement it already\u2026but a bit slow\nYou can keep track of the step or iteration by using a member variable inside the Network module or the module containing the hook function.\nFor example, inside your Network module,\n<code class=\"lang-auto\"># network module\nclass Network(nn.Module):\n    def __init__(self):\n        self.step = -1\n# while training\nmodel = Network()\nfor i in range(steps):\n    # increment step var before forward pass\n    model.step += 1\n    output = model(input) # check self.step inside register_forward_hook\n<\/code>\nThat\u2019s an interesting approach!\nBased on this, you could also create a \u201chook class\u201d holding this internal parameter, which could be simpler than creating new modules:\n<code class=\"lang-python\">class ActivationHook():\n    def __init__(self, name, every_steps=1):\n        self.name = name\n        self.every_steps = every_steps\n        self.steps = 0\n        self.activation = {}\n    \n    def hook(self, model, input, output):\n        if (self.steps % self.every_steps) == 0:\n            self.activation[self.name] = output.detach()\n        self.steps += 1\n    \n\nmodel = MyModel()\nact_hook = ActivationHook('fc2', every_steps=2)\nmodel.fc2.register_forward_hook(act_hook.hook)\nx = torch.randn(1, 25)\noutput = model(x)\nprint(act_hook.activation['fc2'])\n<\/code>"},{"x":"Hi,\nI am testing my understanding of torch.nn.BatchNorm2d but there is a slight error in numbers that I have failed to resolve.\nI thought what the Batch Normalization in training mode without any update shall do\n<code class=\"lang-python\">(x - E[x]) \/ sqrt(var(X) + 1e-5)\n<\/code>\nwhich in python code\n<code class=\"lang-python\">(x - x.mean([0,2,3], keepdim=True)) \/ (x.var([0,2,3], keepdim=True) + 1e-5).sqrt()\n<\/code>\nBut when I actually compare it to torch.nn.BatchNorm2d with the following code,\n<code class=\"lang-python\">import torch\nimport torch.nn as nn\n\nbn = nn.BatchNorm2d(5)\nx = torch.randn(500,5,2,2)\n\nbn_x = (x - x.mean([0,2,3], keepdim=True)) \/ (x.var([0,2,3], keepdim=True) + 1e-5).sqrt()\nprint((bn(x) - bn_x).abs().max().item())\n<\/code>\nI constantly get a nonzero difference in range of ~1e-3.\nThe error decreases as the B, H, W numbers get higher.\nI tried Bessel\u2019s correction with different numbers but it did not work.\nI tried removing 1e-5 (eps parameter in nn.BatchNorm2d) but it did not work either.\nIt might be because of the floating point precision, but the error seems quite high.\nCould you point me to the source of this error?\nI would like an answer to the following questions:\n\nIs my understanding of nn.BatchNorm2d correct?\nWhat should I change in my formula \/ code to remove the difference?\n","y":"I guess the difference domes from the var calculation, which should apparently be unbiased.\nI\u2019ve written a manual implementation some time ago here with link \"https:\/\/github.com\/ptrblck\/pytorch_misc\/blob\/master\/batch_norm_manual.py\" so feel free to compare your code to this one.","z":"I guess the difference domes from the var calculation, which should apparently be unbiased.\nI\u2019ve written a manual implementation some time ago here with link \"https:\/\/github.com\/ptrblck\/pytorch_misc\/blob\/master\/batch_norm_manual.py\" so feel free to compare your code to this one.\nThank you so much for your answer. It was more than perfect."},{"x":"Hi, I\u2019m trying to add a ReLU layer to PART of my model\u2019s output, here is my code:\n<code class=\"lang-auto\">import torch.nn as nn\nimport torch\n\ninput = torch.ones(2).cuda()\nlinear = nn.Linear(2, 2).cuda()\noutput = linear(input)\noutput[0] = nn.ReLU(inplace=False)(output[0])\nloss = torch.sum(output)\nloss.backward()\n\n<\/code>\nbut I got the error:\n<code class=\"lang-auto\">RuntimeError: one of the variables needed for gradient computation has been modified by an inplace operation: [torch.cuda.FloatTensor []], which is output 0 of SelectBackward, is at version 3; expected version 2 instead. Hint: enable anomaly detection to find the operation that failed to compute its gradient, with torch.autograd.set_detect_anomaly(True).\n<\/code>\nI\u2019m sure it was caused by this row:\n<code class=\"lang-auto\">output[0] = nn.ReLU(inplace=False)(output[0])\n\n<\/code>\nbut why is this an inplace operation? How should I modify part of the output in a differentiable way? Thanks in advance.","y":"The error is thrown, if the output of the computation is needed for the backward pass, as explained here with link \"https:\/\/discuss.pytorch.org\/t\/when-would-the-error-one-of-the-variables-needed-for-gradient-computation-has-been-modified-occur\/11976\/4\" (the linked topic doesn\u2019t exactly use your model, but might give you an idea).","z":"Since you are assigning the result to the same tensor, it\u2019s considered an inplace operation.\nYou could instead create a new result tensor, using torch.cat and slicing:\n<code class=\"lang-python\">input = torch.ones(2)\nlinear = nn.Linear(2, 2)\noutput = linear(input)\noutput1 = nn.ReLU(inplace=False)(output[0])\noutput = torch.cat((output1.unsqueeze(0), output[1:]))\nloss = torch.sum(output)\nloss.backward()\n<\/code>\nThanks for your reply, and your solution just works.\nBut there\u2019s another question, if I modify my code:\n<code class=\"lang-auto\">import torch.nn as nn\nimport torch\n\ninput = torch.ones(2).cuda()\nlinear = nn.Linear(2, 2).cuda()\noutput = linear(input)\n# output[0] = nn.ReLU(inplace=False)(output[0])\noutput[0] = output[0] * 2 + 1\noutput[0] = output[0] + output[1]\nloss = torch.sum(output)\nloss.backward()\n<\/code>\nSimilarly,  I assigned the result to the same tensor in line 8 and line 9, but this code works well. What\u2019s the different between the two versions?\nThe error is thrown, if the output of the computation is needed for the backward pass, as explained here with link \"https:\/\/discuss.pytorch.org\/t\/when-would-the-error-one-of-the-variables-needed-for-gradient-computation-has-been-modified-occur\/11976\/4\" (the linked topic doesn\u2019t exactly use your model, but might give you an idea).\nThanks. I\u2019ve got it."},{"x":"my code looks like this, i write a forward hook,and try to move \u2018QPs\u2019 to the \u2018output\u2019 tensor device then do some operations, but my \u2018assert\u2019 code failed , because two tensor are not on the same device, i am confused\u2026BTY,i use dataparallel but i think it doesn\u2019t matter\u2026 so what happens?\n<code class=\"lang-auto\">def layer1_hook_fn_forward(module, input, output):\n    # output shape 2, 64, 10, 56, 56\n    global QPs\n    QPs = output.new(QPs)\n    print('**************')\n    print(output.device)\n    print(QPs.device)\n    print('**************')\n    assert QPs.device == output.device, \"tensor device not match\"\n    return output.mul(QPs)\n\n<\/code>\nbesides , i tried QPs.to(output.device), it still wrong\u2026","y":"You need to do QPs = QPs.to(output.device) for it to work as to does not change the Tensor inplace.\nAlso you can try QPs = output.new(QPs, device=output.device).","z":"You need to do QPs = QPs.to(output.device) for it to work as to does not change the Tensor inplace.\nAlso you can try QPs = output.new(QPs, device=output.device).\nall right,i understand it now and fix it,but i got a error like this post, RuntimeError: OrderedDict mutated during iteration (while using hook) with link \"https:\/\/discuss.pytorch.org\/t\/runtimeerror-ordereddict-mutated-during-iteration-while-using-hook\/23929\",\nimage.png1451\u00d7165 18 KB with link \"https:\/\/discuss.pytorch.org\/uploads\/default\/original\/3X\/4\/6\/46ac584a24824c3f0fbd81ae6e0503a081ec84e7.png\" and no answer under this post, i find this line code in pytorch \u2018nn\/module.py\u2019 like this\nimage.png875\u00d7137 18.3 KB with link \"https:\/\/discuss.pytorch.org\/uploads\/default\/original\/3X\/b\/e\/bedac479e548a557a85b5ab5eb6ef43a055f54f1.png\" ,so is there a bug or my way using hook are not right?\nNot sure. Would you have a small code sample to reproduce this crash please?\nok, i think ,there exists a little bug in nn\/module\/module.py, in \u2018call\u2019 function, i change the line\n<code class=\"lang-auto\">        for hook in self._forward_hooks.values():\n\n<\/code>\nto\n<code class=\"lang-auto\">        for hook in list(self._forward_hooks.values()):\n<\/code>\nand the code will be fine.\nThe following code is the changed module.py -\u2018call()\u2019 func:\n<code class=\"lang-auto\">def __call__(self, *input, **kwargs):\n        for hook in list(self._forward_pre_hooks.values()):\n            result = hook(self, input)\n            if result is not None:\n                if not isinstance(result, tuple):\n                    result = (result,)\n                input = result\n        if torch._C._get_tracing_state():\n            result = self._slow_forward(*input, **kwargs)\n        else:\n            result = self.forward(*input, **kwargs)\n        for hook in list(self._forward_hooks.values()):\n            hook_result = hook(self, input, result)\n            if hook_result is not None:\n                result = hook_result\n        if len(self._backward_hooks) > 0:\n            var = result\n            while not isinstance(var, torch.Tensor):\n                if isinstance(var, dict):\n                    var = next((v for v in var.values() if isinstance(v, torch.Tensor)))\n                else:\n                    var = var[0]\n            grad_fn = var.grad_fn\n            if grad_fn is not None:\n                for hook in list(self._backward_hooks.values()):\n                    wrapper = functools.partial(hook, self)\n                    functools.update_wrapper(wrapper, hook)\n                    grad_fn.register_hook(wrapper)\n        return result\n<\/code>\nI guess this works because creating the list makes a copy of the hooks data stucture and hides the error.\nBut the error is still there that you change the hooks during while you run them. Is that what you\u2019re doing?\nright\u2026i have a dynamic weight  matrix, and in every time forward process i register the forward hook,change the hook output, then remove it, in model \u2018forward\u2019 func. i wonder if i register the hook in model \u2018init\u2019 func, the variable QP doesn\u2019t get right value, because the QP is changed by specific input,the forward hook will change the output correctly? if that case, where should i register the forward hook and where should i  remove it?  i am little confused by the hook mechanic, thanks !\n<code class=\"lang-auto\">\ndef layer1_hook_fn_forward(module, input, output):\n    # output shape 2, 64, 10, 56, 56\n    global QPs #it's a variable\n    a = QPs.to(output.device)\n    assert a.device == output.device, \"tensor device not match\"\n    return output.mul(a)\n\n<\/code>\n<code class=\"lang-auto\">    def forward(self, inputs):\n        # ( (img1,mv1,qp1), (img2,mv2,qp2))\n        outputs = []\n        for features in inputs:\n            mix_features = []\n            global QPs\n            QPs = self.deal_qp_data(features[2]) #change by input\n            # print(next(self.qp_model.parameters()).is_cuda)\n            for i in range(len(features)):\n                if i == 0:\n                    # for rgb\n                    features[i] = self.data_bn_channel_3(features[i])\n                    handle1 = self.base_model_channel_3.layer1.register_forward_hook(layer1_hook_fn_forward)\n                    x = self.base_model_channel_3(features[i])\n                    handle1.remove()\n                if i == 1:\n                    # for mv and residual need batch_normalization\n                    features[i] = self.data_bn_channel_2(features[i])\n                    handle2 = self.base_model_channel_2.layer1.register_forward_hook(layer1_hook_fn_forward)\n                    x = self.base_model_channel_2(features[i])\n                    handle2.remove()\n                if i == 2:\n                    continue\n                x = self.dropout(x)\n                x = self.key_feature_layer(x)\n                # x = (batch, features)\n                mix_features.append(x)\n            mix_features = torch.cat([mix_features[0], mix_features[1]], dim=1)\n            # print(mix_features.shape)\n            outputs.append(mix_features)\n        x = self.fc_layer_1(torch.abs(outputs[0] - outputs[1]))\n        x = F.relu(x)\n        x = self.fc_layer_2(x)\n        x = torch.sigmoid(x)\n        x = self.clf_layer(x)\n        return outputs, x\n<\/code>\nCould you create a small code sample (30-40 lines) that reproduces this problem please? The code you showed above looks ok to me.\ni write the following code to test ,it\u2019 right, but the last code segment is still wrong, so the error means i change the hook during running?   besides my model is a DataParallel model\u2026 so where is the problem\u2026i am thinking.\n<code class=\"lang-auto\">import torch\nfrom torch import nn\nimport numpy as np\nQP = []\nclass SimpleConv(nn.Module):\n    def __init__(self):\n        super(SimpleConv, self).__init__()\n        self.layer1 = nn.Linear(1,32)\n        self.layer2 = nn.Linear(32,64)\n\n    def forward(self, x):\n        global QP\n        QP = np.repeat(x,32)\n        handle = self.layer1.register_forward_hook(layer1_hook_fn_forward)\n        x = self.layer1(x)\n        handle.remove()\n        x = self.layer2(x)\n        return x\n\ndef layer1_hook_fn_forward(module, input, output):\n    # this hook will change the output\n    global QPs\n    a = QP.to(output.device)\n    assert a.device == output.device, \"tensor device not match\"\n    return output.mul(a)\n\nfor i in range(50):\n    model = SimpleConv()\n    a= torch.tensor([1.0])\n    model(a)\n<\/code>\nHi,\nI can run this sample without errors locally. Is it not working for you?\nyeah,i can run this sample without errors too, but my real project code is still wrong\u2026if i change the source code like pre post and make the project can run, from the view of hook function that change the layer output, can i do that?\nSorry I\u2019m not sure to understand your question here, could you reformulate it?\nI mean, now my example can run through, but my project will still report an error. In my previous post, I can modify the code in the \u2018module.py\u2019 so that the project does not report an error, but you said that the error still exists, but I want to know if this still existing error will affect my purpose of using hooks, that is, modifying the layer output results.\nI am not sure exactly what will happen here.\nMost likely, since the hook list was copied before, all the original hooks will run irrelevant of how you modify them during the for loop.\nThe bigger problem is that it is going to be complex for you to keep changing the module.py file to hack around the problem in your code. In the mid\/long term, it will be more time efficient I think to fix your code.\nright, i am tring to find the problems in my code\u2026thx all right\nthe hook problem ,it seems a runtime error, it can run 250 iteration and suddenly error\u2026i think it\u2019s not my code error.\nimage1697\u00d7786 118 KB with link \"https:\/\/discuss.pytorch.org\/uploads\/default\/original\/3X\/b\/e\/beb0896cbfcb1e74ef7276a665e027677f323d36.png\"\nimage1421\u00d7932 109 KB with link \"https:\/\/discuss.pytorch.org\/uploads\/default\/original\/3X\/2\/7\/2783446ed4c95b64db36e0f40e83e3cce18d58ed.png\"\nIf you give me a code sample that reproduces the issue, I can take a closer look."},{"x":"Getting THCudaCheck FAIL file=..\\aten\\src\\THC\\THCCachingHostAllocator.cpp line=278 error=719 : unspecified launch failure error during the execution of epochs of my LSTM model after a few epochs. The error stacktrace points the line out, hidden = self.rnn(x, hidden) in the forward function as the reason for error.\nHere is my network model:\n<code class=\"lang-auto\">import torch\nfrom torch.utils.data import TensorDataset, DataLoader\nimport torch.nn as nn\n\nimport numpy as np\nimport pandas as pd\nfrom time import time\n\n\nclass SignalNet(nn.Module):\n    def __init__(self, input_size, output_size, hidden_dim, num_layers):\n        super(SignalNet, self).__init__()\n\n        self.hidden_dim = hidden_dim\n        self.num_layers = num_layers\n\n        self.rnn = nn.LSTM(input_size, hidden_dim, num_layers, batch_first=True)\n        self.fc = nn.Linear(hidden_dim, output_size)\n\n    def forward(self, x):\n        batch_size = x.size(0)\n\n        hidden = self.init_hidden(batch_size)\n\n        out, hidden = self.rnn(x, hidden)\n\n        out = out.contiguous().view(-1, self.hidden_dim)\n        out = self.fc(out)\n\n        return out, hidden\n\n    def init_hidden(self, batch_size):\n        return (torch.zeros(self.num_layers, batch_size, self.hidden_dim).to(device),\n                torch.zeros(self.num_layers, batch_size, self.hidden_dim).to(device))\n\n\ndef main():\n    global device\n    is_cuda = torch.cuda.is_available()\n\n    if is_cuda:\n        device = torch.device(\"cuda\")\n    else:\n        device = torch.device(\"cpu\")\n\n    input_dim = 8\n    batch_size = 1024  # was 32\n    output_dim = 1\n    num_layers = 5\n    hidden_dim = 10\n    learning_rate = 0.1\n    num_epochs = 5\n\n    model = SignalNet(input_size=input_dim, output_size=output_dim, hidden_dim=hidden_dim, num_layers=num_layers)\n    model.to(device)\n\n    loss_fn = torch.nn.MSELoss(reduction='sum')\n    optimizer = torch.optim.Adam(model.parameters(), lr=learning_rate)\n\n    df = pd.read_csv('data\\\\emg2.csv', sep=',')\n\n    split_frac = 0.8  # 80% train, 20% test\n    split_id = int(split_frac * len(df))\n\n    train_data, train_labels = df.iloc[:split_id, :-1], df.iloc[:split_id, -1]\n    test_data, test_labels = df.iloc[split_id:len(df) * 9 \/\/ 10, :-1], df.iloc[split_id:len(df) * 9 \/\/ 10, -1]\n    val_data, val_labels = df.iloc[len(df) * 9 \/\/ 10:, :-1], df.iloc[len(df) * 9 \/\/ 10:, -1]\n\n    # LSTM starts HERE\n    train_dataset = TensorDataset(torch.from_numpy(train_data.values).float(),\n                                  torch.from_numpy(train_labels.values).float())\n    val_dataset = TensorDataset(torch.from_numpy(val_data.values).float(),\n                                torch.from_numpy(val_labels.values).float())\n    test_dataset = TensorDataset(torch.from_numpy(test_data.values).float(),\n                                 torch.from_numpy(test_labels.values).float())\n\n    train_loader = DataLoader(train_dataset, shuffle=True, batch_size=batch_size, num_workers=6)\n    val_loader = DataLoader(val_dataset, shuffle=True, batch_size=batch_size, num_workers=6)\n    test_loader = DataLoader(test_dataset, shuffle=True, batch_size=batch_size, num_workers=6)\n\n    # Training Run\n    model.train()\n    for epoch in range(1, num_epochs + 1):\n        epoch_loss = 0\n        index = 0\n        for batch_idx, (X_train, y_train) in enumerate(train_loader):\n            optimizer.zero_grad()  # Clears existing gradients from previous epoch\n            X_train, y_train = X_train.to(device), y_train.to(device)\n            # model.init_hidden(batch_size)\n            output, hidden = model(X_train[np.newaxis, ...])\n            # loss = loss_fn(output, y_train.view(-1).long())\n            loss = loss_fn(output.view(-1), y_train.view(-1))\n            epoch_loss += loss.item()\n            loss.backward()  # Does backpropagation and calculates gradients\n            optimizer.step()  # Updates the weights accordingly\n\n            if batch_idx % 50 == 0:\n                print(f'Epoch: {epoch}\/{num_epochs} Batch #{batch_idx + 1}\/{len(train_loader)}.............', end=' ')\n                print(\"Loss: {:.4f}\".format(loss.item()))\n            index = batch_idx\n\n        print(f'Epoch #{epoch}: Avg. loss: {epoch_loss \/ index + 1}')\n\n    # TEST\n    test_losses = []\n    num_correct = 0\n    h = model.init_hidden(batch_size)\n\n    model.eval()\n    for inputs, labels in test_loader:\n        h = tuple([each.data for each in h])\n        inputs, labels = inputs.to(device), labels.to(device)\n        output, h = model(inputs[np.newaxis, ...])\n        test_loss = loss_fn(output.squeeze(), labels.float())\n        test_losses.append(test_loss.item())\n        pred = torch.round(output.squeeze())  # Rounds the output to 0\/1\n        correct_tensor = pred.eq(labels.float().view_as(pred))\n        correct = np.squeeze(correct_tensor.cpu().numpy())\n        num_correct += np.sum(correct)\n\n    print(\"Test loss: {:.3f}\".format(np.mean(test_losses)))\n    test_acc = num_correct \/ len(test_loader.dataset)\n    print(\"Test accuracy: {:.3f}%\".format(test_acc * 100))\n\n\nif __name__ == '__main__':\n    main()\n<\/code>","y":"Could you run your code with CUDA_LAUNCH_BLOCKING=1 python script.py args and post the stack trace here again, please?","z":"Could you run your code with CUDA_LAUNCH_BLOCKING=1 python script.py args and post the stack trace here again, please?\nThank you very much, now, I do not get any errors.  Could you please briefly explain what was the issue and the effect of this parameter as I\u2019d like to learn the insight to get more knowledge about PyTorch?\nOh, that\u2019s not the solution for the issue.\nThe command blocks each CUDA call, which is originally called asynchronously, so it\u2019s just useful for debugging.\nCould you give me the shapes of train_data and train_labels so that I could run the code on my machine?\nAlso, which setup are you using (PyTorch version, CUDA, cudnn, how did you install PyTorch)?\nSorry for being late,  but I have been extremely busy. The shapes of train_data and train_labels are (240000, 8), and (240000,), respectively. I installed PyTorch through pip, and my CUDA and cudnn versions are both 10.1. And, PyTorch version is 1.3.1. Feel free to ask any further information. Thanks."},{"x":"what is the mean of (most likely classes )?\ntop_p, top_class = ps.topk(1, dim=1)\n<h1>Look at the most likely classes for the first 10 examples<\/h1>\nprint(top_class[:10,:])","y":"I assume ps gives you the class probabilities (or logits), so topk will give you the highest class probabilities (logits) with the class indices for the current samples.\nSince you are using .topk(k=1, dim=1), you\u2019ll only get the single most likely prediction.","z":"I assume ps gives you the class probabilities (or logits), so topk will give you the highest class probabilities (logits) with the class indices for the current samples.\nSince you are using .topk(k=1, dim=1), you\u2019ll only get the single most likely prediction."},{"x":"I\u2019ve been reading a piece of code found on github. I specifically don\u2019t understand why we keep on verifying if the model\u2019s layers have got weights and bias (is not None) when we\u2019ve just created it. And why would we do (nn.init.uniform_(module.weight)) if there are weights. What\u2019s the logic here? Thanks\n\ndef get_head(nf: int, n_classes):\n\n    model = nn.Sequential(\n    nn.ReLU(),\n    nn.AdaptiveAvgPool2d(1),\n    Flatten(),\n    nn.BatchNorm1d(nf),\n    nn.Dropout(p=0.25),\n    nn.Linear(nf, n_classes)\n    )\n    for i, module in enumerate(model):\n        if isinstance(module, (nn.BatchNorm1d, nn.BatchNorm2d)):\n             if module.weight is not None:\n                  nn.init.uniform_(module.weight)\n             if module.bias is not None:\n                  nn.init.constant_(module.bias, 0)\n        if isinstance(module, nn.Linear):\n             if getattr(module, \"weight_v\", None) is not None:\n                  print(\"Initing linear with weight normalization\")\n                  assert model[i].weight_g is not None\n             else:\n                 nn.init.kaiming_normal_(module.weight)\n                 print(\"Initing linear\")\n             if module.bias is not None:\n                 nn.init.constant_(module.bias, 0)\n\n\nreturn model\n","y":"Hi,\nThis is because the batchnorm can be created without the affine transformation (see the code here with link \"https:\/\/github.com\/pytorch\/pytorch\/blob\/f7b12a985812f278b225fd58549ac95c7864a22c\/torch\/nn\/modules\/batchnorm.py#L26-L31\"). In that case, the weight and bias will be None.\nFor the linear layer, the .bias can be None for the same reason.\nFor the weight_v attribute. I guess this is something that the user added himself as it does not exist in the original Linear layer.","z":"Hi,\nThis is because the batchnorm can be created without the affine transformation (see the code here with link \"https:\/\/github.com\/pytorch\/pytorch\/blob\/f7b12a985812f278b225fd58549ac95c7864a22c\/torch\/nn\/modules\/batchnorm.py#L26-L31\"). In that case, the weight and bias will be None.\nFor the linear layer, the .bias can be None for the same reason.\nFor the weight_v attribute. I guess this is something that the user added himself as it does not exist in the original Linear layer.\ntorch.Tensor(num_features) returns uninitialized memory. So you definitely want to put some sensible values in there before using it "},{"x":"I\u2019m transforming a TensorFlow model to Pytorch. And I\u2019d like to initialize the mean and variance of BatchNorm2d using TensorFlow model.\nI\u2019m doing it in this way:\n<code class=\"lang-auto\">bn.running_mean = torch.nn.Parameter(torch.Tensor(TF_param))\n<\/code>\nAnd I get this error:\n<code class=\"lang-auto\">RuntimeError: the derivative for 'running_mean' is not implemented\n<\/code>\nBut is works for bn.weight and bn.bias. Is there any way to initialize the mean and variance using my pre-trained Tensorflow model? Is there anything like  moving_mean_initializer  and  moving_variance_initializer  in Pytorch?\nThanks!","y":"Could you try to assign a torch.tensor instead of an nn.Parameter, since the running estimates do not require gradients?","z":"Could you try to assign a torch.tensor instead of an nn.Parameter, since the running estimates do not require gradients?\nWoW! It worked! Thank you! By the way, if I freeze bn parameters (stop bn from updating), the running_mean and running_var will not change. The running_mean and running_var will be saved in model directly. Am I right? Sorry for bad English expression.\nWoW! It worked! Thank you! By the way, if I freeze bn parameters (stop bn from updating), the  running_mean  and  running_var  will not change. The  running_mean  and  running_var  will be saved in model directly. Am I right? Sorry for bad English expression.\nIt depends on what you mean by \u201cfreezing\u201d.\nTo use the running estimates without updating, you could simply call .eval() on the batchnorm layer.\nIf you would like to freeze the affine parameters (weight and bias), you would need to set their requires_grad attribute to False.\nThanks. I meant requires_grad."},{"x":"Here is my conv model, and I\u2019m trying to visualize feature maps during training (in the train() function) based on the answers for another question with link \"https:\/\/discuss.pytorch.org\/t\/visualize-feature-map\/29597\". Getting the error TypeError: Invalid shape (7,) for image data. Could you please guide me through what I am missing?\n<code class=\"lang-auto\">class CKNet(nn.Module):\n    def __init__(self):\n        super(CKNet, self).__init__()\n\n        kernel_size = 3\n\n        self.activation_fn = nn.ReLU(inplace=True)\n\n        self.conv1 = nn.Sequential(\n            nn.Conv2d(1, 16, kernel_size=kernel_size),\n            self.activation_fn,\n            nn.BatchNorm2d(16),\n            # nn.MaxPool2d(2, 2),\n            nn.Dropout2d()\n        )\n\n        self.conv2 = nn.Sequential(\n            nn.Conv2d(16, 16, kernel_size=kernel_size),\n            self.activation_fn,\n            nn.BatchNorm2d(16),\n            # nn.MaxPool2d(2, 2),\n            nn.Dropout2d()\n        )\n\n        self.conv3 = nn.Sequential(\n            nn.Conv2d(16, 32, kernel_size=kernel_size),\n            self.activation_fn,\n            nn.BatchNorm2d(32),\n            # nn.MaxPool2d(2, 2),\n            nn.Dropout2d()\n        )\n\n        self.conv4 = nn.Sequential(\n            nn.Conv2d(32, 32, kernel_size=kernel_size),\n            self.activation_fn,\n            nn.BatchNorm2d(32),\n            nn.MaxPool2d(2, 2),\n            nn.Dropout2d()\n        )\n\n        self.conv5 = nn.Sequential(\n            nn.Conv2d(32, 64, kernel_size=kernel_size),\n            self.activation_fn,\n            nn.BatchNorm2d(64),\n            # nn.MaxPool2d(2, 2),\n            nn.Dropout2d()\n        )\n\n        self.conv6 = nn.Sequential(\n            nn.Conv2d(64, 64, kernel_size=kernel_size),\n            self.activation_fn,\n            nn.BatchNorm2d(64),\n            nn.MaxPool2d(2, 2),\n            nn.Dropout2d()\n        )\n\n        self.conv7 = nn.Sequential(\n            nn.Conv2d(64, 128, kernel_size=kernel_size),\n            self.activation_fn,\n            nn.BatchNorm2d(128),\n            # nn.MaxPool2d(2, 2),\n            nn.Dropout2d()\n        )\n\n        self.conv8 = nn.Sequential(\n            nn.Conv2d(128, 128, kernel_size=kernel_size),\n            self.activation_fn,\n            nn.BatchNorm2d(128),\n            nn.MaxPool2d(2, 2),\n            nn.Dropout2d()\n        )\n\n        last_width_height = 4\n        conv_out_channels = self.conv8._modules['0'].out_channels\n        input_features = last_width_height * last_width_height * conv_out_channels\n\n        self.fc1 = nn.Sequential(\n            nn.Linear(input_features, 768),\n            self.activation_fn,\n            nn.Dropout2d())\n        self.fc2 = nn.Sequential(\n            nn.Linear(768, 128),\n            self.activation_fn,\n            nn.Dropout2d())\n        self.fc3 = nn.Sequential(\n            nn.Linear(128, len(folders)))\n\n    def forward(self, x):\n        x = x.to(device)\n        return x\n\n\ndef train(epoch):\n    epoch_start = time()\n    model.train()\n    train_total_loss = 0\n    index = 0\n    for batch_idx, (images, labels) in enumerate(train_loader):\n        images, labels = images.to(device), labels.to(device)\n\n        # zero the parameter gradients\n        optimizer.zero_grad()\n\n        # forward + backward + optimize\n        output = model(images)  # forward\n        output = output.to(device)\n\n        loss = criterion(output, labels)\n        # backward\n        loss.backward()\n        # update weights\n        optimizer.step()\n\n        train_total_loss += loss.item()\n        train_counter.append((batch_idx * train_batch_size) + (epoch * len(train_loader.dataset)))\n\n        index += 1\n\n        # print statistics\n        if batch_idx % log_interval == 0:\n            # print(f'batch_idx: {batch_idx}')\n            print('Train Epoch: {} [{}\/{} ({:.0f}%)]\\tLoss: {:.6f}'.format(\n                epoch + 1, (batch_idx + 1) * len(images), len(train_loader.dataset),\n                100. * batch_idx \/ len(train_loader), loss.item()))\n\n            train_losses.append(train_total_loss \/ index)\n            print(f'Epoch #{epoch + 1} duration: {time() - epoch_start} seconds')\n\n            # Visualize feature maps\n            activation = {}\n\n            def get_activation(name):\n                def hook(model, input, output):\n                    activation[name] = output.detach()\n\n                return hook\n\n            activation['conv1'] = output.detach()\n            model.conv1[0].register_forward_hook(get_activation('conv1'))\n            act = activation['conv1'].squeeze().cpu()\n            num_plot = 4\n            fig, axarr = plt.subplots(min(act.size(0), num_plot))\n            for idx in range(min(act.size(0), num_plot)):\n                axarr[idx].imshow(act[idx])\n            plt.xticks([])\n            plt.yticks([])\n            plt.show()\n\ndef main():\n    global train_loader, validation_loader\n\n    global face_train_dataset\n    face_train_dataset = datasets.ImageFolder(root=DEST_PATH_TRAIN, transform=data_transforms)\n\n    train_loader = DataLoader(face_train_dataset,\n                              batch_size=train_batch_size, shuffle=True,\n                              num_workers=4)\n\n    global prediction_counter\n    prediction_counter = Counter()\n\n    face_validation_dataset = datasets.ImageFolder(root=DEST_PATH_VALIDATION,\n                                                   transform=data_transforms)\n    validation_loader = DataLoader(face_validation_dataset,\n                                   batch_size=test_batch_size, shuffle=False,\n                                   num_workers=4)\n\n    print(f'# of training images: {len(train_loader.dataset)}')\n    print(f'# of validation images: {len(validation_loader.dataset)}')\n\n    global model\n    model = CKNet().to(device)\n    print(f'Model Overview:\\n{model}')\n\n    global criterion\n    criterion = nn.CrossEntropyLoss()\n    criterion = criterion.to(device)\n    global optimizer\n    optimizer = optim.SGD(model.parameters(), lr=learning_rate, momentum=momentum, weight_decay=weight_decay)\n\n    global train_losses, train_counter, validation_losses, validation_counter\n    train_losses = []\n    train_counter = []\n    validation_losses = []\n    validation_counter = [i * len(train_loader.dataset) for i in range(n_epochs + 1)]\n\n    global accuracy\n    accuracy = 0  # initial value\n    best_accuracy = 0\n    start_time = time()\n    # test()\n    global last_epoch\n    last_epoch = 0\n    for epoch in range(n_epochs):\n        train(epoch)\n        validation_loss, accuracy = test()\n        last_epoch = epoch\n\n<\/code>","y":"The shape (16, 62, 62) I suppose should be (channels, height, width). Iterate over all those 16 feature maps(each of size (62, 62)) and plot them individually.\nAs I mentioned above, imshow expects (H, W, C) and here C=62 which is why it is throwing error.","z":"Very interested on this!\n Firstly a forward hook should be attached to a module before its forward call. In the above code, you first do output = model(images)  # forward and then model.conv1[0].register_forward_hook(get_activation('conv1')). Even though this doesn\u2019t call hook() during the first forward call to conv1, subsequent forward calls will invoke hook. Also you register a hook for every batch_idx % log_interval which is not required.\nComing to visualization of activation maps, can you let know the shape of the conv1 output?\nProblem might occur because the conv outputs are in the order C, H, W whereas the image plot expects H, W, C(Note: if C=1, then it shouldn\u2019t cause issues since you do squeeze anyway)\nIf I call it before the forward call, what would be the output? \nAnd if I hook the visualization registration just before the forward call (as I posted below), getting TypeError: Invalid shape (16, 62, 62) for image data. \n<code class=\"lang-auto\"># Visualize feature maps\nactivation = {}\ndef get_activation(name):\n    def hook(model, input, output):\n        activation[name] = output.detach()\n\n    return hook\nmodel.conv1[0].register_forward_hook(get_activation('conv1'))\n# forward + backward + optimize\noutput = model(images)  # forward\noutput = output.to(device)\n\n# activation['conv1'] = output.detach()\n\nact = activation['conv1'].squeeze().cpu()\nnum_plot = 4\nfig, axarr = plt.subplots(min(act.size(0), num_plot))\nfor idx in range(min(act.size(0), num_plot)):\n    axarr[idx].imshow(act[idx])\nplt.xticks([])\nplt.yticks([])\nplt.show()\n<\/code>\nThe shape (16, 62, 62) I suppose should be (channels, height, width). Iterate over all those 16 feature maps(each of size (62, 62)) and plot them individually.\nAs I mentioned above, imshow expects (H, W, C) and here C=62 which is why it is throwing error."},{"x":"Hi,\ngiven the first 5 layers of a VGG network:\nSequential(\n(0): Conv2d(3, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n(1): ReLU(inplace=True)\n(2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n(3): ReLU(inplace=True)\n(4): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n(5): Conv2d(64, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n(6): ReLU(inplace=True)\n(7): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n(8): ReLU(inplace=True)\n(9): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n(10): Conv2d(128, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n(11): ReLU(inplace=True)\n(12): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n(13): ReLU(inplace=True)\n(14): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n(15): ReLU(inplace=True)\n(16): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n(17): Conv2d(256, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n(18): ReLU(inplace=True)\n(19): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n(20): ReLU(inplace=True)\n(21): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n(22): ReLU(inplace=True)\n(23): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n(24): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n(25): ReLU(inplace=True)\n(26): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n(27): ReLU(inplace=True)\n(28): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n(29): ReLU(inplace=True)\n(30): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n)\nHow to add a layer to classify each activation into n_classes? I am trying with a 1x1 convolution from 512 (number of filters in last layer) to n_classes, something like this:\nnn.Sequential(\nnn.Conv2d(in_channels=512, out_channels=n_classes, kernel_size=1),\nnn.Softmax(dim=1)\n)\nThis network gives (batch_size, n_classes H \/\/ 32, W \/\/ 32) outputs. However, all outputs for each class are equal for a given image, I mean that, for a given batch, B and  class C, all values are the same.\nI have tried training with CrossEntropyLoss and even converting to logits and applying BCELoss, but without success. I also have tried other networks in my train loop and this do not happens.\nAny clue?\nThanks","y":"Hi,\nthanks for the tip, I have removed Softmax from classification layer. Also, I found my mistake: it was a problem of weight initialization of last 1x1 convolution layer. Now the network is learning properly.\nmany thanks","z":"You can use nn.AdaptiveAvgPool2d to reduce the spatial dimensions to 1x1(HxW) and then use 1x1 convolution to bring num_filters = num_classes. Use this after the last conv output from vgg.\n<code class=\"lang-auto\">self.classifier = nn.Sequential(\n          nn.AdaptiveAvgPool2d((1, 1)), \n          nn.Conv2d(in_channels=512, out_channels=n_classes, kernel_size=1),\n          nn.ReLU(inplace=True)\n         )\n<\/code>\nGiven what you have done above, I am not sure how you were able to calculate loss since the output will be num_classes, H\/\/32, W\/\/32. However one important thing to note is, do not use nn.Softmax at the end of the network when using nn.CrossEntropy(it combines softmax and nll loss). That can cause problems.\nHi,\nthanks for the tip, I have removed Softmax from classification layer. Also, I found my mistake: it was a problem of weight initialization of last 1x1 convolution layer. Now the network is learning properly.\nmany thanks"},{"x":"I\u2019m trying to initialize weight of a conv layer like below,\nclass some_model(nn.Module):\ndef __init__(self):\n\n    super(some_model, self).__init__()\n\n    self.blur=nn.Conv2d(1,1,kernel_size=3,padding=1,bias=False)\n\n    self.blur.weight[0][0][0][0]=0.0751\n    self.blur.weight[0][0][0][1]=0.1238\n    ...\ndef forward(self, x):\n    xblur=self.blur(x)\n    return xblur\n\nBut I got the following error message:\nValueError: can\u2019t optimize a non-leaf Tensor\nWhat is the correct way to initialize weight mannually?","y":"I should instead do:\nself.blur.weight.data[0][0][0][0]=0.0751\nweird flex but okay\u2026","z":"I should instead do:\nself.blur.weight.data[0][0][0][0]=0.0751\nweird flex but okay\u2026\nHi,\nUsing .data i discouraged, you should do:\n<code class=\"lang-auto\">with torch.no_grad():\n    self.blur.weight[0][0][0][0]=0.0751\n    self.blur.weight[0][0][0][1]=0.1238\n<\/code>\nwhat does torch.no_grad() do here?\nI know normally we would put evaluation calls inside it, since gradients are not needed. But why do we need it when initializing weights?\nSince everything that happens inside this block is not recorded. The inplace operation that change the value of your weights is not recorded either. And so it is a good way to initialize the value.\nI guess my question then is what does \u2018being recorded\u2019 or \u2018not being recorded\u2019 do?\nThat is not such an easy question.\nIn some sense, ignore these operations when computing gradients."},{"x":"Hi I am quite new to pytorch. I was trying to implement transfer learning with CIFAR10 and resnet18 model built in. For that what I am intending to do is first download original dataset and apply some transformations onto it and the take 500 samples from each class among the 10 classes and create a new dataset with 5000 total training samples(instead of 50,000 samples in original CIFAR10).\nIn that case, I am just extracting and arranging the indices of the samples of the set as per their class and then randomly picking 500 of them for each class. But with that method I am not able to achieve any performance from the network even if I am doing data augmentations on the original set.\nSO  is there a way to truly get the 500\/per class data(using some custom dataloader) and then apply data augmentation on that to achieve the performance?\nThanks in advance!!","y":"Hey, I wrote this dataset for you that gets a subset of CIFAR10 . Just set the n_images_per_class to 500 and it should be ready to use!\n<code class=\"lang-auto\">from torchvision import datasets\nfrom collections import defaultdict, deque\nimport itertools\n\n\nclass Cifar5000(datasets.CIFAR10):\n    def __init__(self, path, transforms, train=True):\n        super().__init__(path, train, download=True)\n        self.transforms = transforms\n        self.n_images_per_class = 5\n        self.n_classes = 10\n        self.new2old_indices = self.create_idx_mapping()\n\n    def create_idx_mapping(self):\n        label2idx = defaultdict(lambda: deque(maxlen=self.n_images_per_class))\n        for original_idx in range(super().__len__()):\n            _, label = super().__getitem__(original_idx)\n            label2idx[label].append(original_idx)\n\n        old_idxs = set(itertools.chain(*label2idx.values()))\n        new2old_indices = {}\n        for new_idx, old_idx in enumerate(old_idxs):\n            new2old_indices[new_idx] = old_idx\n\n        return new2old_indices\n\n    def __len__(self):\n        return len(self.new2old_indices)\n\n    def __getitem__(self, index):\n        index = self.new2old_indices[index]\n        im, label = super().__getitem__(index)\n        return self.transforms(im), label\n<\/code>","z":"Hey, I wrote this dataset for you that gets a subset of CIFAR10 . Just set the n_images_per_class to 500 and it should be ready to use!\n<code class=\"lang-auto\">from torchvision import datasets\nfrom collections import defaultdict, deque\nimport itertools\n\n\nclass Cifar5000(datasets.CIFAR10):\n    def __init__(self, path, transforms, train=True):\n        super().__init__(path, train, download=True)\n        self.transforms = transforms\n        self.n_images_per_class = 5\n        self.n_classes = 10\n        self.new2old_indices = self.create_idx_mapping()\n\n    def create_idx_mapping(self):\n        label2idx = defaultdict(lambda: deque(maxlen=self.n_images_per_class))\n        for original_idx in range(super().__len__()):\n            _, label = super().__getitem__(original_idx)\n            label2idx[label].append(original_idx)\n\n        old_idxs = set(itertools.chain(*label2idx.values()))\n        new2old_indices = {}\n        for new_idx, old_idx in enumerate(old_idxs):\n            new2old_indices[new_idx] = old_idx\n\n        return new2old_indices\n\n    def __len__(self):\n        return len(self.new2old_indices)\n\n    def __getitem__(self, index):\n        index = self.new2old_indices[index]\n        im, label = super().__getitem__(index)\n        return self.transforms(im), label\n<\/code>\nThanks  will definitely check on this one!!\n With your given method I created the dataset. But when I checked\n<code class=\"lang-python\">len(trainset.data) \n<\/code>\nit\u2019s still 50,000. On the other hand when I checked the trainloader details it showed number of data points to be 5,000? Can you please explain what\u2019s happening here?\nThanks!!\nWhat is trainset? If you do len(Cifar5000_dataset) it should give 5000. That\u2019s why we implemented the\nmethod\n<code class=\"lang-auto\">def __len__(self):\n        return len(self.new2old_indices)\n<\/code>\nSo after creating the class I created the object called trainset as\n<code class=\"lang-auto\">trainset = Cifar500(path = \".\/data\",transforms = transform)\n<\/code>\nThen I am checking the number of samples as\n<code class=\"lang-auto\">len(trainset.data)\n<\/code>\nwhich returns 50,000(not 5,000).\nI created the trainloader with the trainset as\n<code class=\"lang-auto\">trainloader = torch.utils.data.DataLoader(trainset, batch_size =32 ,shuffle=True, num_workers=2)\n<\/code>\nAnd\n<code class=\"lang-auto\">trainloader.dataset\n<\/code>\nprints\n<code class=\"lang-bash\">Dataset Cifar500\n    Number of datapoints: 5000\n    Root location: .\/data\n    Split: Train\n    Compose(\n    ToTensor()\n    Normalize(mean=(0.4914, 0.4822, 0.4465), std=(0.2023, 0.1994, 0.201))\n)\n<\/code>\n(5000 datapoints)\nI am not able to understand which actually is being fed to the network 5000 datapoints or 50,000 samples.\n\n\n\n Ajinkya_Ambatwar:\n\nlen(trainset.data)\n\n\nIf you change this to len(trainset) it should work \nThat\u2019s the one which is being used\n  I checked. Yes it is 5,000. Thanks for your precious help:pray:\nBy the way, with this implementation does the trainloader picks random 500 samples during each epoch or it is sampled only once at start?\n You\u2019re welcome \nIt samples it only once at start, but it can be different each time you run your python file. If you want it to be the same every time you run your python script, you can use this seed function at the very beginning of your program\n<code class=\"lang-auto\">def seed_program(seed=0):\n  ''' Seed for reproducability '''\n  random.seed(seed)\n  np.random.seed(seed)\n  torch.manual_seed(seed)\n  torch.cuda.manual_seed_all(seed)\n  # torch.backends.cudnn.deterministic = True # You can add this\n<\/code>\n Yes I am aware of that. Thanks for all the clarification!!\nAlso I had one another unrelated doubt. How can we add dropout to inbuilt pytorch models like say resnet18?I know that if we are custom building the network from scratch then we can add dropout layers using nn.dropout. But with built in models how can we do that?\nI don\u2019t know a way other than building a custom network. But you don\u2019t have to build it from scratch, the resnet18 code is on github so you can copy that and change the source code \nOh thanks\u2026That should help!!"},{"x":"I\u2019m trying to write some code like below:\n<code class=\"lang-auto\">x = Variable(torch.Tensor([[1.0,2.0,3.0]]))\ny = Variable(torch.LongTensor([1]))\nw = torch.Tensor([1.0,1.0,1.0])\nF.cross_entropy(x,y,w)\nw = torch.Tensor([1.0,10.0,1.0])\nF.cross_entropy(x,y,w)\n<\/code>\nHowever, the output of cross entropy loss is always 1.4076 whatever w is. What is behind the weight parameter for F.cross_entropy()? How to use it correctly?\nI\u2019m using pytorch 0.3:drooling_face:","y":"You are using it correctly!\nHowever, I think there is an explanation missing on how size_average works regarding the weight in the docs with link \"https:\/\/pytorch.org\/docs\/master\/nn.html#crossentropyloss\".\nHave a look at the docs of NLLLoss with link \"https:\/\/pytorch.org\/docs\/master\/nn.html#torch.nn.NLLLoss\". It states, that each loss will be divided by the sum of all corresponding class weights, if reduce=True and size_average=True.\nIn your case, since you just have one example, the loss will by divided by 10, which yields exactly the same result as the unweighted loss:\n<code class=\"lang-auto\">x = torch.Tensor([[1.0,2.0,3.0]])\ny = torch.LongTensor([1])\nw = torch.Tensor([1.0,1.0,1.0])\nF.cross_entropy(x,y,w, size_average=False)\nw = torch.Tensor([1.0,10.0,1.0])\nF.cross_entropy(x,y,w, size_average=False)\nF.cross_entropy(x,y,w, size_average=True)\n\nloss = 10.0 * (-x[0, y] + torch.log(torch.sum(torch.exp(x))))\naveraged_loss = loss \/ w[y]\n<\/code>\nIf you create another sample, the loss will differ:\n<code class=\"lang-auto\">x = torch.Tensor([[1.0,2.0,3.0], [2.0, 1.0, 3.0]])\ny = torch.LongTensor([1, 1])\nw = torch.Tensor([1.0,1.0,1.0])\nF.cross_entropy(x,y,w, size_average=False)\n> tensor(3.8152)\nw = torch.Tensor([1.0,10.0,1.0])\nF.cross_entropy(x,y,w, size_average=False)\n> tensor(38.1521)\nF.cross_entropy(x,y,w, size_average=True)\n> tensor(1.9076)\n<\/code>\nIn this example 38.1521 will be divided by the sum of the corresponding weights (w[1] for each sample), thus 38.1521\/20. = 1.9076.","z":"You are using it correctly!\nHowever, I think there is an explanation missing on how size_average works regarding the weight in the docs with link \"https:\/\/pytorch.org\/docs\/master\/nn.html#crossentropyloss\".\nHave a look at the docs of NLLLoss with link \"https:\/\/pytorch.org\/docs\/master\/nn.html#torch.nn.NLLLoss\". It states, that each loss will be divided by the sum of all corresponding class weights, if reduce=True and size_average=True.\nIn your case, since you just have one example, the loss will by divided by 10, which yields exactly the same result as the unweighted loss:\n<code class=\"lang-auto\">x = torch.Tensor([[1.0,2.0,3.0]])\ny = torch.LongTensor([1])\nw = torch.Tensor([1.0,1.0,1.0])\nF.cross_entropy(x,y,w, size_average=False)\nw = torch.Tensor([1.0,10.0,1.0])\nF.cross_entropy(x,y,w, size_average=False)\nF.cross_entropy(x,y,w, size_average=True)\n\nloss = 10.0 * (-x[0, y] + torch.log(torch.sum(torch.exp(x))))\naveraged_loss = loss \/ w[y]\n<\/code>\nIf you create another sample, the loss will differ:\n<code class=\"lang-auto\">x = torch.Tensor([[1.0,2.0,3.0], [2.0, 1.0, 3.0]])\ny = torch.LongTensor([1, 1])\nw = torch.Tensor([1.0,1.0,1.0])\nF.cross_entropy(x,y,w, size_average=False)\n> tensor(3.8152)\nw = torch.Tensor([1.0,10.0,1.0])\nF.cross_entropy(x,y,w, size_average=False)\n> tensor(38.1521)\nF.cross_entropy(x,y,w, size_average=True)\n> tensor(1.9076)\n<\/code>\nIn this example 38.1521 will be divided by the sum of the corresponding weights (w[1] for each sample), thus 38.1521\/20. = 1.9076.\nThanks a lot, it totally cleared up my confusion!\nThanks for the help.\nHi,\nargument \u2018size_average\u2019 is deprecated,\ninstead of that now you use\nprint(F.cross_entropy(x,y,w, reduction=\u2018sum\u2019)) instead of \u2018size_average\u2019 = False\nprint(F.cross_entropy(x,y,w, reduction=\u2018mean\u2019) instead of \u2018size_average\u2019 = True"},{"x":"I tried coding a custom data set.  When I try to load the class into a data loader, it states that:\n\n\nNameError                                 Traceback (most recent call last)\n in \n16 # Generators\n17 training_set = roof_dataset(partition[\u2018train\u2019], labels, transform = train_transforms)\n\u2014> 18 training_generator = data.DataLoader(training_set, **params)\n19\n20 test_set = roof_dataset(partition[\u2018test\u2019], labels, transform = train_transforms)\nNameError: name \u2018data\u2019 is not defined\n\nHere is the code pulling the data:\n\n<h1>CUDA for PyTorch<\/h1>\nuse_cuda = torch.cuda.is_available()\ndevice = torch.device(\u201ccuda:0\u201d if use_cuda else \u201ccpu\u201d)\n<h1>Parameters<\/h1>\nparams = {\u2018batch_size\u2019: 1000,\n\u2018shuffle\u2019: True,\n\u2018num_workers\u2019: 4}\nmax_epochs = 100\n<h1>Generators<\/h1>\ntraining_set = roof_dataset(partition[\u2018train\u2019], labels, transform = train_transforms)\ntraining_generator = data.DataLoader(training_set, **params)\n\nHere is the custom class.  I\u2019m not sure where it\u2019s going wrong;\n\nroot_dir = \u2018D:\\CIS inspection images 0318\\train\\roof\\\u2019\nclass roof_dataset(Dataset):\n'Characterizes a dataset for PyTorch'\n\ndef __init__(self, list_IDs, labels, transform):\n    'Initialization'\n    self.labels = labels\n    self.list_IDs = list_IDs\n    self.transform = transform\n\ndef __len__(self):\n        \n    'Denotes the total number of samples'\n    return len(self.list_IDs)\n\ndef __getitem__(self, index):\n\n        'Generates one sample of data'\n        # Select sample\n        ID = self.list_IDs[index]\n\n        # Load data and get label\n        X = torch.load(Path(root_dir) + ID)\n        y = self.labels[ID]\n        \n        if self.transform:\n            X = self.transform(X)\n\n        return X, y\n","y":"From the error that you linked, it seems that you\u2019re missing a from torch.utils import data to have data.DataLoader to work properly.","z":"From the error that you linked, it seems that you\u2019re missing a from torch.utils import data to have data.DataLoader to work properly."},{"x":"For a given n x n tensor and window size k, is there a quick way to set every k x k window to the average value of the window in the original tensor? This would basically be average pooling, but without resizing, this would effectively just blur the image.\nThanks","y":"Another solution would be to average pool, and then use nn.UpsamplingNearest2d()","z":"Hi,\nIf you use average pooling with link \"https:\/\/pytorch.org\/docs\/stable\/nn.functional.html#avg-pool2d\" with a kernel of k (must be odd) and a padding of floor(k\/2). Then you will maintain the image size.\nI wasn\u2019t clear enough in my first post. What I mean is the following:\nGiven a 4 x 4 image [[0, 1, 2, 3], [4, 5, 6, 7], [8, 9, 10, 11], [12, 13, 14, 15]] and a window size of 2, what I\u2019d like to get is the following:\n[[2.5, 2.5, 4.5, 4.5], [2.5, 2.5, 4.5, 4.5], [10.5, 10.5, 12.5, 12.5], [10.5, 10.5, 12.5, 12.5]]\nThanks\nHo,\nI\u2019m not sure there is a very clean way to do this but the following will work:\n<code class=\"lang-python\">import torch\n\nkern_size = 2\n\n# Add 0th dimension because avg_pool expect batch dimension\ninp = torch.arange(16).float().view(1, 4, 4)\npooled = torch.nn.functional.avg_pool2d(inp, kern_size)\n# remove 0th dimension\npooled.squeeze_(0)\nrep_col = []\nfor c in range(pooled.size(1)):\n  col = pooled.select(1, c)\n  rep_col += [col] * kern_size\nrepeated = torch.stack(rep_col, 1)\nrep_row = []\nfor r in range(repeated.size(0)):\n  row = repeated.select(0, r)\n  rep_row += [row] * kern_size\noutput = torch.stack(rep_row, 0)\n\nprint(output)\n<\/code>\nThanks a lot for this\nAnother solution would be to average pool, and then use nn.UpsamplingNearest2d()\nThis upsampling is a Kronecker product with link \"https:\/\/en.wikipedia.org\/wiki\/Kronecker_product\" with a matrix of ones. Unfortunately, torch does not have an implementation of it. If you are not dependent on the GPU it might be faster to do it with numpy with link \"https:\/\/docs.scipy.org\/doc\/numpy\/reference\/generated\/numpy.kron.html\":\n<code class=\"lang-python\">import numpy as np\nimport torch\nfrom torch.nn.functional import avg_pool2d\n\nimage = torch.arange(16, dtype=torch.float).view(4, 4)\n\nkernel_size = 2\nstride = 2\n\nimage_pooled = image.view(1, 1, *image.size())\nimage_pooled = avg_pool2d(image_pooled, kernel_size=kernel_size, stride=stride)\nimage_pooled = image_pooled.squeeze()\n\nimage_blured = image_pooled.numpy()\nimage_blured = np.kron(image_blured, np.ones(image_blured.shape))\nimage_blured = torch.from_numpy(image_blured)\n\nprint(image_blured)\n<\/code>\nAlso you cannot use this if you need gradients !\nYou are right, I totally forgot about that."},{"x":"I\u2019m getting  a weird error trying to feed data into my first pytorch cov-net.  Here is the error traceback.\n\n\nUnpicklingError                           Traceback (most recent call last)\n in \n6\n7 for epoch in range(max_epochs):\n----> 8     for i in (partition[\u2018train\u2019], labels) in enumerate(training_generator):\n9         # Run the forward pass\n10         outputs = model(images)\nC:\\ProgramData\\Anaconda3\\lib\\site-packages\\torch\\utils\\data\\dataloader.py in next(self)\n344     def next(self):\n345         index = self._next_index()  # may raise StopIteration\n\u2013> 346         data = self.dataset_fetcher.fetch(index)  # may raise StopIteration\n347         if self.pin_memory:\n348             data = _utils.pin_memory.pin_memory(data)\nC:\\ProgramData\\Anaconda3\\lib\\site-packages\\torch\\utils\\data_utils\\fetch.py in fetch(self, possibly_batched_index)\n42     def fetch(self, possibly_batched_index):\n43         if self.auto_collation:\n\u2014> 44             data = [self.dataset[idx] for idx in possibly_batched_index]\n45         else:\n46             data = self.dataset[possibly_batched_index]\nC:\\ProgramData\\Anaconda3\\lib\\site-packages\\torch\\utils\\data_utils\\fetch.py in (.0)\n42     def fetch(self, possibly_batched_index):\n43         if self.auto_collation:\n\u2014> 44             data = [self.dataset[idx] for idx in possibly_batched_index]\n45         else:\n46             data = self.dataset[possibly_batched_index]\n in getitem(self, index)\n24\n25             # Load data and get label\n\u2014> 26             X = torch.load(root_dir + ID)\n27             y = self.labels[ID]\n28\nC:\\ProgramData\\Anaconda3\\lib\\site-packages\\torch\\serialization.py in load(f, map_location, pickle_module, **pickle_load_args)\n384         f = f.open(\u2018rb\u2019)\n385     try:\n\u2013> 386         return _load(f, map_location, pickle_module, **pickle_load_args)\n387     finally:\n388         if new_fd:\nC:\\ProgramData\\Anaconda3\\lib\\site-packages\\torch\\serialization.py in _load(f, map_location, pickle_module, **pickle_load_args)\n561             f.seek(0)\n562\n\u2013> 563     magic_number = pickle_module.load(f, **pickle_load_args)\n564     if magic_number != MAGIC_NUMBER:\n565         raise RuntimeError(\u201cInvalid magic number; corrupt file?\u201d)\nUnpicklingError: invalid load key, \u2018\\xff\u2019.\n\nMy image file names and data resides in two dictionaries.  Dictionary 1 called partition has a train and test key with a list of image filenames.\nHere is the data class, transform and data loader:\n\nroot_dir = \u2018D:\\CIS inspection images 0318\\train\\roof\\\u2019\nclass roof_dataset(Dataset):\n'Characterizes a dataset for PyTorch'\n\ndef __init__(self, list_IDs, labels, transform):\n    'Initialization'\n    self.labels = labels\n    self.list_IDs = list_IDs\n    self.transform = transform\n\ndef __len__(self):\n        \n    'Denotes the total number of samples'\n    return len(self.list_IDs)\n\ndef __getitem__(self, index):\n\n        'Generates one sample of data'\n        # Select sample\n        ID = self.list_IDs[index]\n\n        # Load data and get label\n        X = torch.load(root_dir + ID)\n        y = self.labels[ID]\n        \n        if self.transform:\n            X = self.transform(X)\n\n        return X, y\n\n<h1>CUDA for PyTorch<\/h1>\nuse_cuda = torch.cuda.is_available()\ndevice = torch.device(\u201ccuda:0\u201d if use_cuda else \u201ccpu\u201d)\n<h1>Parameters<\/h1>\nparams = {\u2018batch_size\u2019: 1000,\n\u2018shuffle\u2019: True,\n\u2018num_workers\u2019: 0}\nmax_epochs = 100\n<h1>Generators<\/h1>\ntraining_set = roof_dataset(partition[\u2018train\u2019], labels, transform = train_transforms)\ntraining_generator = data.DataLoader(training_set, **params)\ntest_set = roof_dataset(partition[\u2018test\u2019], labels, transform = train_transforms)\ntest_generator = data.DataLoader(test_set, **params)\n\nFinally, here is where I get the error:\n\ntotal_step = len(training_generator)\nloss_list = []\nacc_list = []\nfor epoch in range(max_epochs):\nfor i in (partition[\u2018train\u2019], labels) in enumerate(training_generator):\n# Run the forward pass\noutputs = model(images)\nloss = criterion(outputs, labels)\nloss_list.append(loss.item())\n    # Backprop and perform Adam optimisation\n    optimizer.zero_grad()\n    loss.backward()\n    optimizer.step()\n\n    # Track the accuracy\n    total = labels.size(0)\n    _, predicted = torch.max(outputs.data, 1)\n    correct = (predicted == labels).sum().item()\n    acc_list.append(correct \/ total)\n\n    if (i + 1) % 100 == 0:\n        print('Epoch [{}\/{}], Step [{}\/{}], Loss: {:.4f}, Accuracy: {:.2f}%'\n              .format(epoch + 1, num_epochs, i + 1, total_step, loss.item(),\n                      (correct \/ total) * 100))\n\n\nI think it\u2019s in the data class but I\u2019m not sure.  Does anyone see what I\u2019m doing wrong?","y":"This is some kind of data loading issue. Pickle with link \"https:\/\/docs.python.org\/3\/library\/pickle.html\" is a Python de\/serialization module used in Dataloader. \/xff might refer to hexademical FF. So most probably you are going out of range, perhaps trying to access a resource that does not exist?\nX = torch.load(root_dir + ID)\nI suggest to debug or print out root_dir + ID and see if the file really exists (or perhaps it does, but can  not be unpickled, e.g., corrupted, wrong format etc).","z":"This is some kind of data loading issue. Pickle with link \"https:\/\/docs.python.org\/3\/library\/pickle.html\" is a Python de\/serialization module used in Dataloader. \/xff might refer to hexademical FF. So most probably you are going out of range, perhaps trying to access a resource that does not exist?\nX = torch.load(root_dir + ID)\nI suggest to debug or print out root_dir + ID and see if the file really exists (or perhaps it does, but can  not be unpickled, e.g., corrupted, wrong format etc)."},{"x":"I was f\u2019tuning VGG network for style transfer (Gatys et al, 2015) and kept getting results that made no sense, so after some debugging it turned out I uploaded the weights incorrectly. Can someone point out my error, because I tried it on other models, and it seemed to work?\n<code class=\"lang-auto\">import vgg16\npretrained_weights = torch.load(os.path.join('pretrained_weights', 'vgg16-00b39a1b.pth'))\nfeature_extractor = vgg16.vgg16(pretrained=False)\n\nfor _n,par in feature_extractor.named_parameters():  \n    par.requires_grad = False\n    par = copy.deepcopy(pretrained_weights[_n]) \n    par = par.to(device)\n<\/code>\nvgg16 is the same vgg16.py from torchvision.models.\nI believe copy.deepcopy didn\u2019t change the parameters for some reason","y":"Hi,\nThis does not work because par = foo is assigning to the object foo the new name of par. What was in par before is deleted.\nIf you want to write in the par Tensor, you need to use an inplace operation like par.copy_(foo) to copy into the Tensor that is in par.\nAlso the .to() operation is always out of place, so you cannot do it like that. You will have to use feature_extractor.to(device) to move your weifghts to the right device.","z":"Did you try the recommended way of loading weights in the tutorials with link \"https:\/\/pytorch.org\/tutorials\/beginner\/saving_loading_models.html\" ? Does it not work in your case ?\nI did later, and it fixed the issue. My question is, why this doesn\u2019t work.\nHi,\nThis does not work because par = foo is assigning to the object foo the new name of par. What was in par before is deleted.\nIf you want to write in the par Tensor, you need to use an inplace operation like par.copy_(foo) to copy into the Tensor that is in par.\nAlso the .to() operation is always out of place, so you cannot do it like that. You will have to use feature_extractor.to(device) to move your weifghts to the right device.\nThanks, I\u2019m sorry why is that operation \u2018out of place\u2019? This won\u2019t put the tensor on CUDA?\nThe code below should be clear.\n<code class=\"lang-auto\">t = some_cpu_tensor\nt.cuda()\n# here t is not on the the gpu\n\nt = some_cpu_tensor\ntc = t.cuda()\n# here tc is on the the gpu\n\nt = some_cpu_tensor\nt = t.cuda()\n# here t is on the the gpu\n<\/code>\n Just curious, why aren\u2019t you using\n feature_extractor = vgg16.vgg16(pretrained=True)?\nbut in my code\npar=par.to(device)\nare you saying that par tensor will not be placed on device?\nThe thing is that the Tensor itself and the python name associated to it are two different things.\nif you have par being some tensor. When you do par.copy_(), you write inplace into the tensor named par. If you do par = par + 1 then you create a new tensor that will contain the value of par plus 1, then associate that new tensor the name par. See that the Tensor that corresponds to par at the end is not the same as the original one.\nIn your case, if you do par = par.cuda(), after that line, par will point to a cuda tensor. But the original Tensor that is contained in the network won\u2019t be changed.\nIn your code snippet\n<code class=\"lang-auto\">t = some_cpu_tensor\nt = t.cuda()\n<\/code>\nisn\u2019t this what I did to put par on CUDA with par=par.to(device). I don\u2019t see the difference between putting the whole model model=model.to(device) and par tensor. Are you saying that a single tensor can only be put on cuda with t=t.cuda() command?\nHi,\nThe difference is that model.to(device) is inplace while tensor.to(device) is not:\nDoes the sample below helps?\n<code class=\"lang-python\">import torch\nimport copy\n\n# I will use `.double()` instead of `.cuda()` because my local\n# machine does not have cuda, but they behave the same way\n\ntensor = torch.zeros(10)\nmodel = torch.nn.Linear(10, 10)\n\nnew_val = torch.ones(10)\ndef your_copy(par):\n  par = copy.deepcopy(new_val)\n  par = par.double()\n\nprint(\"Your changes are out of place and so not reflected outside\")\nprint(\"Before your copy on tensor\")\nprint(tensor)\nprint(tensor.type())\n\nyour_copy(tensor)\n\nprint(\"After your copy on tensor\")\nprint(tensor)\nprint(tensor.type())\n\nprint(\"Models are changed inplace:\")\nprint(\"Before on module\")\nprint(model.weight.type())\n\nmodel.double()\n\nprint(\"After on module\")\nprint(model.weight.type())\n\n\ndef my_suggestion(par):\n  par.copy_(new_val)\n  # No way to change type here inplace\n\nprint(\"With inplace change, you can change value and have it reflected outside\")\nprint(\"Before my copy\")\nprint(tensor)\n\nmy_suggestion(tensor)\n\nprint(\"After my copy\")\nprint(tensor)\n<\/code>\n<code class=\"lang-auto\"><\/code>\n: some layers can be modified so the number of weights does not match. It\u2019s better then to load weights manually."},{"x":"Hi.\nI am running a NAS (neural network search). The capacity of these models are too large even with batch_size =1 accounting for 18GB memory for a single GPU.\nI have 4GPUs with with 12GB memory (48 GB totaly), how to run these code?","y":"Depending on the model architecture, you could try to apply model sharding as shown in this exmaple with link \"https:\/\/discuss.pytorch.org\/t\/split-single-model-in-multiple-gpus\/13239\/2\".\nThis approach would store submodules on different devices and transfer the output to the corresponding device in the forward method.\nWould this be possible or is a single layer already creating the OOM issue?","z":"Depending on the model architecture, you could try to apply model sharding as shown in this exmaple with link \"https:\/\/discuss.pytorch.org\/t\/split-single-model-in-multiple-gpus\/13239\/2\".\nThis approach would store submodules on different devices and transfer the output to the corresponding device in the forward method.\nWould this be possible or is a single layer already creating the OOM issue?\nThank you for your reply.  In network perimeter training , the gpu0 were fully used, but gpu1,2,3 half but it works.  when network architecture searching, it also create oom. In another words:\nepoch < 5 :Data (4,3,224,224) GPU0:12GB GPU1~3 5GB\nepoch >=5:  Data(4,3,224,224) and Architect[(20,8),(12,4,3)]\nHow to do that or relax the gpu0 burden into others?\nHow did you split the model for the first runs?\nDid you manually \u201cchopped\u201d the model in different parts and pushed each to a different device?\nIf so, try to move more layers\/submodules to other devices.\nI\u2019m not sure, how the Architect[(20, 8), (12, 4, 3)] class works exactly, but I assume it\u2019s creating different submodules?\nactually, the code is a implementation of autodeeplab. I follow your instruction to modify my code:\n<code class=\"lang-auto\">class Architect_search(nn.Module):\n    def __init__(self,archi_search,data_model):\n        super(Architect_search, self).__init__()\n        self.archi_model = nn.DataParallel(archi_search,device_ids=[0,1]).to('cuda:0')\n        self.data_model = nn.DataParallel(data_model,device_ids=[2,3]).to('cuda:2')\n    def forward(self, data_image,search_image ):\n        search_result = self.archi_search(data_image)\n        train_result = self.data_model(search_image)\n        return search_result,train_result\n<\/code>\nbug got errors:\nRuntimeError: module must have its parameters and buffers on device cuda:0 (device_ids[0]) but found one of them on device: cuda:2\nspecifically, the architect_search governs part of parameters of data_model.\n<code class=\"lang-auto\">class Architect_search (nn.Module) :\n    def __init__(self, data_model, args):\n        super(Architect_search, self).__init__()\n        self.network_momentum = args.momentum\n        self.network_weight_decay = args.weight_decay\n        self.optimizer = torch.optim.Adam(model.arch_parameters(),\n            lr=args.arch_lr, betas=(0.5, 0.999), weight_decay=args.arch_weight_decay)\n        self.data_model = data_model\n\n\n    def forward(self, input_valid):\n\n            logits = self.model(input_valid)\n\n            return logits\n<\/code>\ndata_model is a common nn.Module class. arch_parameters will return [(20,8),(12,4,3)]\n20 potential connection x 8 opperation types every block.  12 layers x4blockx3concatnationWX20190716-111419.png2108\u00d7486 116 KB with link \"https:\/\/discuss.pytorch.org\/uploads\/default\/original\/2X\/9\/98b5286b778653db25e41c31b0ef318a052e784c.png\"\nIf you use different modules on different GPUs, you would also have to transfer the tensors in the forward to the device as given in the example.\nIn your case search_result should be transferred to 'cuda:2'.\nThank you . It works."},{"x":"I\u2019m building a training data set.  While it\u2019s running it shows this:\nimage.png725\u00d7618 5.01 KB with link \"https:\/\/discuss.pytorch.org\/uploads\/default\/original\/3X\/8\/1\/8134566d3914834ea5e345a2fd86ba108e907462.png\"\nWhat not hold it to one line and show the progress as it goes?  Did I code something wrong?  Below is my training class:\n\nclass roof_dataset():\nclaims = \u2018D:\\CIS inspection images 0318\\train\\roof\\claims\u2019\nno_claims = \u2018D:\\CIS inspection images 0318\\train\\roof\\no_claims\u2019\nLABELS  = {claims: 1, no_claims: 0}\ntraining_data = []\nclaim_count = 0\nno_claim_count = 0\n\n\ndef make_training_data(self):\n    for label in self.LABELS:\n        print(label)\n        for f in tqdm(os.listdir(label)):\n            if \"JPG\" in f:\n                try:\n                    path = os.path.join(label, f)\n                    img = Image.open(path)\n                    pic = train_transform(img)\n                    self.training_data.append([pic, np.eye(2)[self.LABELS[label]]])\n                    \n                    if label == self.claims:\n                        self.claim_count += 1\n                    elif label == self.no_claims:\n                        self.no_claim_count += 1\n\n                except Exception as e:\n                    pass\n                    #print(label, f, str(e))\n                    \n    np.random.shuffle(self.training_data)\n    np.save(\"D:\\\\CIS inspection images 0318\\\\self_build\\\\training_data.npy\", self.training_data)\n    print('claims:',self.claim_count)\n    print('no_claims:',self.no_claim_count)\n","y":"Instead of\n\nfrom tqmd import tqmd\n\n, I put:\n\nfrom tqdm import tqdm_notebook as tqdm\nnow it works!\n","z":"that\u2019s about tqdm not pytorch and iot\u2019s probably because you are printing label\nI took the print(label) out and it still is doing it.  if it\u2019s a tqdm issue, I\u2019ll close it out here.  Thanks for pointing that out.\nI also use and sometime i find that issue but couldn\u2019t figure out why.\nInstead of\n\nfrom tqmd import tqmd\n\n, I put:\n\nfrom tqdm import tqdm_notebook as tqdm\nnow it works!\n\nwhat\u2019s the difference between both?\nI have no idea.  I didn\u2019t have time to look into it.\ntqdm_notebook was written for Jupyter notebooks, if I\u2019m not mistaken, since the vanilla tqdm might have some kind of weird behavior (like the one mentioned in the thread)."},{"x":"I\u2019m training a 3D U-Net-like architecture (with a patch size of 128^3), on a Tesla V100 16GB, which runs out of memory in the loss.backward() step. The forward pass goes through, but the next line which is loss.backward() throws the following CUDA OOM error :\n<code class=\"lang-auto\">Traceback (most recent call last):\n  File \"trainer.py\", line 200, in <module>\n    loss.cpu().backward()\n  File \"\/cbica\/external\/python\/anaconda\/3\/envs\/pytorch\/1.0\/lib\/python3.6\/site-packages\/torch\/tensor.py\", line 102, in backward\n    torch.autograd.backward(self, gradient, retain_graph, create_graph)\n  File \"\/cbica\/external\/python\/anaconda\/3\/envs\/pytorch\/1.0\/lib\/python3.6\/site-packages\/torch\/autograd\/__init__.py\", line 90, in backward\n    allow_unreachable=True)  # allow_unreachable flag\nRuntimeError: CUDA out of memory. Tried to allocate 512.00 MiB (GPU 0; 15.78 GiB total capacity; 14.70 GiB already allocated; 74.62 MiB free; 54.63 MiB cached)\n<\/code>\nThe error shows loss.cpu().backward() since I try pushing the loss to CPU for backprop but I still get the error of CUDA OOM.\nThe crux of my training script is given below:\n<code class=\"lang-auto\">for ep in range(num_epochs):\n    start = time.time()\n    model.train\n    for batch_idx, (subject) in enumerate(train_loader):\n        # Load the subject and its ground truth\n\n        image = subject['image']\n        mask = subject['gt']\n        # Loading images into the GPU and ignoring the affine\n        image, mask = image.float().cuda(), mask.float().cuda()\n        #Variable class is deprecated - parameteters to be given are the tensor, whether it requires grad and the function that created it\n\n        image, mask = Variable(image, requires_grad = True), Variable(mask, requires_grad = True)\n\n        # Making sure that the optimizer has been reset\n        optimizer.zero_grad()\n        # Forward Propagation to get the output from the models\n        output = model(image.float())\n        # Computing the loss\n        loss = loss_fn(output.cpu().double(), mask.cpu().double(), n_classes)\n        # Back Propagation for model to learn\n        print(loss)\n        loss = loss.cpu()\n        loss.cpu().backward()\n        #Updating the weight values\n        optimizer.step()\n        #Pushing the dice to the cpu and only taking its value\n        curr_loss = MCD_loss(output.double(), mask.double(), n_classes).cpu().data.item()\n        #train_loss_list.append(loss.cpu().data.item())\n        total_loss+=curr_loss\n        # Computing the average loss\n        average_loss = total_loss\/(batch_idx + 1)\n        #Computing the dice score \n        curr_dice = 1 - curr_loss\n        #Computing the total dice\n        total_dice+= curr_dice\n        #Computing the average dice\n        average_dice = total_dice\/(batch_idx + 1)\n        scheduler.step()\n<\/code>\nAny information would be of great help. Thanks in advance.","y":"Hi,\nThe backward operation is always performed on the same device where the forward was performed. So moving the loss to the cpu does not force the backward to be computed on the cpu.\nThere is no way to do this at the moment.\nThe usual way to deal with this is to reduce the batch size. Potentially doing 2 forward\/backward before doing an optimizer.step to double the effective batch size.","z":"Hi,\nThe backward operation is always performed on the same device where the forward was performed. So moving the loss to the cpu does not force the backward to be computed on the cpu.\nThere is no way to do this at the moment.\nThe usual way to deal with this is to reduce the batch size. Potentially doing 2 forward\/backward before doing an optimizer.step to double the effective batch size.\nThank you for the information!\nAlso, torch.utils.checkpoint might be useful to trade compute for memory."},{"x":"HTTPError Traceback (most recent call last)\n with link \"https:\/\/nptszgc59b-b5278c2458b95a70-0-colab.googleusercontent.com\/outputframe.html?vrz=colab-20191017-080046-RC00_275259420#\" in () ----> 1 DA=torchvision.datasets.ImageNet(\"\/\",download=True,transform=transforms.ToTensor())\n\/usr\/lib\/python3.6\/urllib\/request.py with link \"https:\/\/nptszgc59b-b5278c2458b95a70-0-colab.googleusercontent.com\/outputframe.html?vrz=colab-20191017-080046-RC00_275259420#\" in http_error_default(self, req, fp, code, msg, hdrs) 648 class HTTPDefaultErrorHandler(BaseHandler): 649 def http_error_default(self, req, fp, code, msg, hdrs): --> 650 raise HTTPError(req.full_url, code, msg, hdrs, fp) 651 652 class HTTPRedirectHandler(BaseHandler):\nHTTPError: HTTP Error 404: Not Found","y":"Hi,\nYou might want to upgrade your torchvision.\nBut as you can see in the new code here with link \"https:\/\/github.com\/pytorch\/vision\/blob\/d409c1173511becdc4607cf4628d73bd0fa9ba5b\/torchvision\/datasets\/imagenet.py#L41-L45\", the dataset is not available publicly unfortunately. You will need to to download the dataset by yourself.","z":"Hi,\nYou might want to upgrade your torchvision.\nBut as you can see in the new code here with link \"https:\/\/github.com\/pytorch\/vision\/blob\/d409c1173511becdc4607cf4628d73bd0fa9ba5b\/torchvision\/datasets\/imagenet.py#L41-L45\", the dataset is not available publicly unfortunately. You will need to to download the dataset by yourself."},{"x":"I really need help with cross-correlation. From pytorch docs i saw that conv2d layer can be used for cross-correlation, but when i tried to do it i keep on getting errors and cant figure out how to use conv2d layers for cross-correlation to find a template object in a search region. Could someone help me with the code.","y":"You are using an image and filter the same shape (100x100), which will create a single pixel output.\nThis is expected in a cross-correlation as well as convolution.\nIf you want a bigger output shape, use a smaller kernel or a larger image input.","z":"Could you post the code snippet you\u2019ve already written so that we can assist in debugging?\nF.conv2d should work with a custom weight parameter as your kernel.\nhere is the code:\nimport torch\nimport torch.nn as nn\nimport torch.nn.functional as F\nimport cv2\nimport numpy as np\nimport matplotlib.pyplot as plt\nIMG_SIZE = 100\nimage = cv2.imread(\u201cC:\\Users\\Dutchfoundation\\Desktop\\AI\\Torch\\image_01.jpg\u201d,cv2.IMREAD_GRAYSCALE)\ntemplate = cv2.imread(\u201cC:\\Users\\Dutchfoundation\\Desktop\\AI\\Torch\\template_01.jpg\u201d,cv2.IMREAD_GRAYSCALE)\nimage = cv2.resize(image, (IMG_SIZE, IMG_SIZE))\ntemplate = cv2.resize(template, (IMG_SIZE, IMG_SIZE))\nprint(\u201cImage:\u201d,image.shape)\nprint(\u201cTemplate:\u201d,template.shape)\nimage_tensor = torch.Tensor(image).view(-1,100,100)\ntemplate_tensor = torch.Tensor(template).view(-1,100,100)\nprint(\u201cTensor Image:\u201d,image_tensor.shape)\nprint(\u201cTensor Template:\u201d,template_tensor.shape)\nimage_final = image_tensor.view(-1,1,100,100)\ntemplate_final = template_tensor.view(-1,1,100,100)\nprint(\u201cFinal Image:\u201d,image_final.shape)\nprint(\u201cFinal Template:\u201d,template_final.shape)\ncorr_map = F.conv2d(image_final, template_final)\nprint(corr_map.shape)\n#plt.imshow(corr_map)\nThe output shape of the corr_map is [1,1,1,1] which does\u2019nt make sense\nYou are using an image and filter the same shape (100x100), which will create a single pixel output.\nThis is expected in a cross-correlation as well as convolution.\nIf you want a bigger output shape, use a smaller kernel or a larger image input.\nThanks a lot for the help"},{"x":"I am shifting to using PyTorch from Keras and TensorFlow. For both of those, the setup on Anaconda is fairly simple.\n<code class=\"lang-auto\">conda install keras-gpu\n<\/code>\nOne command does quick work of installing all libraries including cudatoolkit and keras recognizes my GPU.\nI installed pytorch and tried running Chatbot example by pytorch on my GPU (GTX 1050 ti) but it doesn\u2019t seem to recognize my device.\nCan anyone illustrate the process I need to follow for this?\nDo i need to install CudaToolKit separately?\nThank you.","y":"You just need to install the GPU driver on your machine. The binaries ship with CUDA, cudnn and other libraries. Have a look at the website with link \"https:\/\/pytorch.org\/get-started\/locally\/\" for install instructions.","z":"You just need to install the GPU driver on your machine. The binaries ship with CUDA, cudnn and other libraries. Have a look at the website with link \"https:\/\/pytorch.org\/get-started\/locally\/\" for install instructions.\nSolution worked for me, I was installing only PyTorch via Anaconda Navigator. Using command line and the command specified on the page fixed it for me. Thanks.\n<code class=\"lang-auto\">conda install pytorch torchvision cudatoolkit=10.0 -c pytorch\n<\/code>\nThis one."},{"x":"image.png641\u00d7581 30.9 KB with link \"https:\/\/discuss.pytorch.org\/uploads\/default\/original\/2X\/3\/3194ad545a6f045dc746fe34f4a04a79a4fa0420.png\"\nWhen I show image tensor\nI use\n<code class=\"lang-auto\">img = images.numpy().transpose(1, 2, 0)\nplt.imshow(img)\nplt.show()\n<\/code>\nBut the color is wrong. Can anybody help me??\nThanks ahead.","y":"\n\n\n DonghunP:\n\nimg = images.numpy().transpose(1, 2, 0)\n\n\nHi,\nFor imshow, if the image is 3 channel \/ RGB,  the pixel values should be [0-1] float or [0-255] int. I feel that the problem is coming from data type of pixel values.\nIf it does not fix your issue, please print some values in img.\nThanks","z":"\n\n\n DonghunP:\n\nimg = images.numpy().transpose(1, 2, 0)\n\n\nHi,\nFor imshow, if the image is 3 channel \/ RGB,  the pixel values should be [0-1] float or [0-255] int. I feel that the problem is coming from data type of pixel values.\nIf it does not fix your issue, please print some values in img.\nThanks"},{"x":"I meet this error as following, is there some actions is not differentiable or in-place operation cause this error. I want to know how to debug this error.\nTraceback (most recent call last):\nFile \u201ctraining\/train_super_pixel_net.py\u201d, line 370, in \nmain(args)\nFile \u201ctraining\/train_super_pixel_net.py\u201d, line 103, in main\navg_loss = train(args, model, optimizer, epoch, trainloader, scheduler, best_iou, loss_fn)\nFile \u201ctraining\/train_super_pixel_net.py\u201d, line 151, in train\nloss.backward()\nFile \u201c\/home\/lxt\/anaconda3\/lib\/python3.6\/site-packages\/torch\/tensor.py\u201d, line 93, in backward\ntorch.autograd.backward(self, gradient, retain_graph, create_graph)\nFile \u201c\/home\/lxt\/anaconda3\/lib\/python3.6\/site-packages\/torch\/autograd\/init.py\u201d, line 90, in backward\nallow_unreachable=True)  # allow_unreachable flag\nFile \u201c\/home\/lxt\/anaconda3\/lib\/python3.6\/site-packages\/torch\/autograd\/function.py\u201d, line 76, in apply\nreturn self._forward_cls.backward(self, *args)\nFile \u201c\/home\/lxt\/anaconda3\/lib\/python3.6\/site-packages\/torch\/nn\/parallel\/_functions.py\u201d, line 30, in backward\nreturn (None,) + ReduceAddCoalesced.apply(ctx.input_device, ctx.num_inputs, *grad_outputs)\nFile \u201c\/home\/lxt\/anaconda3\/lib\/python3.6\/site-packages\/torch\/nn\/parallel\/_functions.py\u201d, line 41, in forward\nreturn comm.reduce_add_coalesced(grads, destination)\nFile \u201c\/home\/lxt\/anaconda3\/lib\/python3.6\/site-packages\/torch\/cuda\/comm.py\u201d, line 119, in reduce_add_coalesced\nflat_tensors = [_flatten_dense_tensors(chunk) for chunk in chunks]\nFile \u201c\/home\/lxt\/anaconda3\/lib\/python3.6\/site-packages\/torch\/cuda\/comm.py\u201d, line 119, in \nflat_tensors = [_flatten_dense_tensors(chunk) for chunk in chunks]\nFile \u201c\/home\/lxt\/anaconda3\/lib\/python3.6\/site-packages\/torch\/_utils.py\u201d, line 144, in _flatten_dense_tensors\nflat = torch.cat([t.contiguous().view(-1) for t in tensors], dim=0)\nRuntimeError: cuda runtime error (77) : an illegal memory access was encountered at \/pytorch\/aten\/src\/THC\/THCCachingHostAllocator.cpp:271","y":"I would probably first try to establish where the exact location is. Due to the asynchronous nature of GPU computing, the error is usually reported later. A while ago, I wrote a quick howto for this. with link \"https:\/\/lernapparat.de\/debug-device-assert\/\" There some operations that may cause errors like this when used with invalid data, e.g. indexing functions when you demand an index that is outside the array.\nBest regards\nThomas","z":"This looks like a bug in cuda code: https:\/\/github.com\/NVIDIA\/vid2vid\/issues\/19\nI bet if you run in cpu mode this will work fine.\nI would probably first try to establish where the exact location is. Due to the asynchronous nature of GPU computing, the error is usually reported later. A while ago, I wrote a quick howto for this. with link \"https:\/\/lernapparat.de\/debug-device-assert\/\" There some operations that may cause errors like this when used with invalid data, e.g. indexing functions when you demand an index that is outside the array.\nBest regards\nThomas\nHi ! Thanks for you all. It is cuda code problem, it takes me a long time to find the location of the code\nI met a similar issue.\nCould tell me the reason you found about this issue?\nThanks a lot.\nHi! I have add \"os.environ[\u2018CUDA_LAUNCH_BLOCKING\u2019] = \u201c1\"\u201d in my code, but it seems that the  traceback is still random and useless. Do you have any advice to debug such kind of error?\nThanks for any suggestions\uff01\nDid you add it at the very top, though?\nIt needs to be there when PyTorch is first imported.\nBest regards\nThomas\nYes, I add it at the very top.\nSo this worked for me, basically I downgraded from python 3.7 to 3.6.9 and installed pytorch 1.0.0 with link \"https:\/\/download.pytorch.org\/whl\/cu100\/torch-1.0.0-cp36-cp36m-linux_x86_64.whl\" using pip.\nHere\u2019s my config,\n\nUbuntu 18.04 LTS\nCUDA 10.2\nCuDNN 7.6.1\nNvidia TITAN RTX x4\n"},{"x":"Hello,\nin order to evaluate the performance of different models (not only pre-trained models),  I want to use datasets from ImageNet.\nThe problem is, that I want to go sure, that the images I present, are \u2018new\u2019 to the model in order to control the bias but it could be very likely, that popular datasets were used to train the models.\nThe papers related to the models only mention the datasets used to benchmark the models.\nIs there a way to get informations about how these networks have been trained?\nThank you very much in advance.","y":"The classification models were pretrained on ImageNet, while the segmentation and detection models use e.g. COCO. The pretrained with link \"https:\/\/pytorch.org\/docs\/stable\/torchvision\/models.html\" argument gives you information about the used dataset.","z":"The classification models were pretrained on ImageNet, while the segmentation and detection models use e.g. COCO. The pretrained with link \"https:\/\/pytorch.org\/docs\/stable\/torchvision\/models.html\" argument gives you information about the used dataset."},{"x":"Anybody can clarify why I keep getting cuda error when using torch.nn.parallel.DistributedDataParallel?\nMWE:\n<code class=\"lang-auto\">if torch.cuda.device_count() > 1:\n    torch.distributed.init_process_group(backend='nccl')\n    model = torch.nn.parallel.DistributedDataParallel(model).to(device)\n<\/code>\nexecuting script:\n<code class=\"lang-auto\">python -m torch.distributed.launch main.py\n<\/code>\nAm I using it incorrectly or am I missing something else?\nA MWE:\n<code class=\"lang-auto\">import os\nimport torch\nos.environ['MASTER_ADDR'] = 'localhost'\nos.environ['MASTER_PORT'] = '12355'\ntorch.distributed.init_process_group(backend='nccl', rank=1, world_size=2)\n<\/code>\nThen the universe hangs completely:\nimage.png867\u00d775 28.5 KB with link \"https:\/\/discuss.pytorch.org\/uploads\/default\/original\/2X\/0\/021a116dc95983c8899b9e67aa557c88ed6ee94c.png\"\nTerminal is not released back to the user. Any ideas why?","y":"You have to use DistributedDataParallel with as many processes as the value of world_size. If you specify rank=1 on a single process, it will hang around waiting for the process with rank=0 to start.\nCheck out the launch utility with link \"https:\/\/pytorch.org\/docs\/stable\/distributed.html#launch-utility\" for easy launching of multiple processes. You can use it like this:\n<code class=\"lang-auto\">python -m torch.distributed.launch --nproc_per_node=2 .\/my_script.py\n<\/code>","z":"You have to use DistributedDataParallel with as many processes as the value of world_size. If you specify rank=1 on a single process, it will hang around waiting for the process with rank=0 to start.\nCheck out the launch utility with link \"https:\/\/pytorch.org\/docs\/stable\/distributed.html#launch-utility\" for easy launching of multiple processes. You can use it like this:\n<code class=\"lang-auto\">python -m torch.distributed.launch --nproc_per_node=2 .\/my_script.py\n<\/code>\nAwesome, thanks so much for clarifying that!\nIf you don\u2019t mind me asking for some additional clarifications?\n\n\n\n pietern:\n\nCheck out the launch utility with link \"https:\/\/pytorch.org\/docs\/stable\/distributed.html#launch-utility\" for easy launching of multiple processes. You can use it like this\n\n\nThe lauch utility has to be used in combination with DistributedDataParallel right, or can it be used on its own?\nUsing DistributedDataParallel does it mean that it has to be used in combination with torch.utils.data.distributed.DistributedSampler?\nThanks again!\nThe correct call for multi-gpu single node is:\nworld_size = 1  <-- number of nodes\nrank=0  <-- which node is running which process"},{"x":"Hi,\nCan torch.clamp() which I assume is some form of ReLU change the output of my NN classification?\nI am running a celebrity face image on a pretrained VGG Face model. I set the model in eval mode and make a forward pass.\n<code class=\"lang-auto\">        image, label = image.to(device), label.to(device)  \n        preds = F.softmax(vgg_model_instance(image.view(1, 3, 224, 224)), dim=1)\n        values, indices = preds.max(-1)\n<\/code>\nI get a results. 87% accuracy.\nThen i apply clamp on the input image.\n<code class=\"lang-auto\">        image, label = image.to(device), label.to(device) \n        **image = torch.clamp(image, 0, 1)**\n        preds = F.softmax(vgg_model_instance(image.view(1, 3, 224, 224)), dim=1)\n        values, indices = preds.max(-1)\n<\/code>\nThe accuracy crashes like anything. Any idea what is going on.\nWith all other setting constant I have tried ranges (-1,1) and (0,1) for clamp. I also tried running the model through softmax, max and logsoftmax.\nThe reason I am trying this experiment for testing a FGSM attack. While I am not sure if clamping the tensor is necessary for the attack, all examples I follow seem to do it. Would really appreciate if one could explain why this is done too.","y":"If you\u2019ve normalized the input data, it\u2019ll have a zero mean and unit variance.\nClamping it to [0, 1] might delete a lot of important features, which could explain the bad accuracy.","z":"If you\u2019ve normalized the input data, it\u2019ll have a zero mean and unit variance.\nClamping it to [0, 1] might delete a lot of important features, which could explain the bad accuracy."},{"x":"I have been using my custom transformation namely FaceDetection which I added below. Now I am trying to apply it to another dataset which contains face images within the same format of the former (.png). What am I missing? When I do not use this transformation, I do not get any errors.\nYour help is appreciated a lot.\nHere is my custom transformation class, FaceDetection, based on OpenCV to detect the location of faces in the images:\n<code class=\"lang-auto\">class FaceDetection(object):\n\n    def __init__(self):\n        pass\n\n    def __call__(self, frame):\n        faceCascade = cv2.CascadeClassifier(cascPath)\n\n        if frame is not None:\n            gray = cv2.cvtColor(np.array(frame), cv2.COLOR_BGR2GRAY)\n\n            faces = faceCascade.detectMultiScale(\n                gray,\n                scaleFactor=1.1,\n                minNeighbors=5,\n                minSize=(1, 1)\n            )\n\n            for (x, y, w, h) in faces:\n                image_as_arr = np.array(frame)[y:y + w, x:x + h]\n                return Image.fromarray(image_as_arr.astype('uint8'), 'RGB')\n<\/code>\nAnd here is the composition of my transformations:\n<code class=\"lang-auto\">data_transforms = transforms.Compose([\n    FaceDetection(),\n    transforms.Resize(48),\n    transforms.Grayscale(num_output_channels=1),\n    transforms.ToTensor(),\n    transforms.Normalize(mean=[0.5],\n                         std=[0.5])\n])\n<\/code>\nFinally, I apply it when I load the training dataset:\nface_train_dataset = datasets.ImageFolder(root=DEST_PATH_TRAIN, transform=data_transforms)","y":"I think the face cascade doesn\u2019t find any faces and this loop:\n<code class=\"lang-python\">for (x, y, w, h) in faces:\n    image_as_arr = np.array(frame)[y:y + w, x:x + h]\n    return Image.fromarray(image_as_arr.astype('uint8'), 'RGB')\n<\/code>\nwill be empty.\nSince you don\u2019t have any other return statement, your transformation will return None by default.\nIf you add an assert before the loop and check faces for a valid entry, you should see an exception.","z":"It seems your custom transformation didn\u2019t find any faces and thus didn\u2019t return a valid PIL.Image.\nI see but it works when I skip my custom transformation. So, the destination (DEST_PATH_TRAIN) should be a right path as the transformation does nothing related to the path of the images.\nI think the face cascade doesn\u2019t find any faces and this loop:\n<code class=\"lang-python\">for (x, y, w, h) in faces:\n    image_as_arr = np.array(frame)[y:y + w, x:x + h]\n    return Image.fromarray(image_as_arr.astype('uint8'), 'RGB')\n<\/code>\nwill be empty.\nSince you don\u2019t have any other return statement, your transformation will return None by default.\nIf you add an assert before the loop and check faces for a valid entry, you should see an exception.\nJust added a control in the case of not founding any faces in the image. Now works as expected. Thanks a lot for your guidance."},{"x":"I am testing using Pytorch with multiple GPUs. I am not using DataParallel, but I want to use Model Parallelism.\nMy Model design is two input branches (each in separate GPUs). I have done an example with MNIST for reproducibility.\nBut at training, I get the exception mentioned below. Any help would be appreciated.\n<code class=\"lang-auto\">import torch\nfrom torch.autograd import Variable\nimport torch.nn as nn\nimport torch.nn.functional as F\nimport torch.optim as optim\nfrom torchvision import datasets, transforms\nfrom torch.utils.data import DataLoader\n\n# Hyperparameters\nnum_epochs = 5\nnum_classes = 10\nbatch_size = 100\nlearning_rate = 0.001\n\nDATA_PATH = '\/data\/'\nMODEL_STORE_PATH = '\/models\/'\n\n# transforms to apply to the data\ntrans = transforms.Compose([transforms.ToTensor(), transforms.Normalize((0.1307,), (0.3081,))])\n\n# MNIST dataset\ntrain_dataset = datasets.MNIST(root=DATA_PATH, train=True, transform=trans, download=True)\ntest_dataset = datasets.MNIST(root=DATA_PATH, train=False, transform=trans)\n\ntrain_loader = DataLoader(dataset=train_dataset, batch_size=batch_size, shuffle=True)\ntest_loader = DataLoader(dataset=test_dataset, batch_size=batch_size, shuffle=False)\n\ngpu1 = torch.device(\"cuda:0\")\ngpu2 = torch.device(\"cuda:1\")\n\nclass DistConvNet(nn.Module):\n    def __init__(self):\n        super(DistConvNet, self).__init__()\n        \n        # gpu1\n        self.layer1 = nn.Sequential(\n            nn.Conv2d(1, 32, kernel_size=5, stride=1, padding=2),\n            nn.ReLU(),\n            nn.MaxPool2d(kernel_size=2, stride=2))\n        self.layer2 = nn.Sequential(\n            nn.Conv2d(32, 64, kernel_size=5, stride=1, padding=2),\n            nn.ReLU(),\n            nn.MaxPool2d(kernel_size=2, stride=2))\n        self.layer1.to(gpu1)\n        self.layer2.to(gpu1)\n        \n        # gpu2\n        self.layer3 = nn.Sequential(\n            nn.Conv2d(1, 32, kernel_size=5, stride=1, padding=2),\n            nn.ReLU(),\n            nn.MaxPool2d(kernel_size=2, stride=2))\n        self.layer4 = nn.Sequential(\n            nn.Conv2d(32, 64, kernel_size=5, stride=1, padding=2),\n            nn.ReLU(),\n            nn.MaxPool2d(kernel_size=2, stride=2))\n        self.layer3.to(gpu2)\n        self.layer4.to(gpu2)\n        \n        self.drop_out = nn.Dropout()\n        self.fc1 = nn.Linear(7 * 7 * 64 * 2, 1000)\n        self.fc2 = nn.Linear(1000, 10)\n        \n        self.drop_out.to(gpu1)\n        self.fc1.to(gpu1)\n        self.fc2.to(gpu1)\n        \n    def forward(self, x1, x2):\n        out1 = self.layer1(x1)\n        out1 = self.layer2(out1)\n        \n        out2 = self.layer3(x2)\n        out2 = self.layer4(out2)\n        \n        out1 = out1.reshape(out1.size(0), -1)\n        out2 = out2.reshape(out2.size(0), -1)\n        out2.to(gpu1)\n        \n        \n        out = torch.cat((out1, out2), 1)\n        out = self.drop_out(out)\n        out = self.fc1(out)\n        out = self.fc2(out)\n        return out\n\nmodel_dist = DistConvNet()\ncriterion = nn.CrossEntropyLoss()\noptimizer = torch.optim.Adam(model_dist.parameters(), lr=learning_rate)\n\n\ntotal_step = len(train_loader)\nloss_list = []\nacc_list = []\nfor epoch in range(num_epochs):\n    for i, (images, labels) in enumerate(train_loader):\n        # Run the forward pass\n        images_gpu1, images_gpu2, labels = images.to(gpu1), images.to(gpu2), labels.to(gpu1)\n        \n        outputs = model_dist(images_gpu1, images_gpu2)\n        loss = criterion(outputs, labels)\n        loss_list.append(loss.item())\n\n        # Backprop and perform Adam optimisation\n        optimizer.zero_grad()\n        loss.backward()\n        optimizer.step()\n\n        # Track the accuracy\n        total = labels.size(0)\n        _, predicted = torch.max(outputs.data, 1)\n        correct = (predicted == labels).sum().item()\n        acc_list.append(correct \/ total)\n\n        if (i + 1) % 100 == 0:\n            print('Epoch [{}\/{}], Step [{}\/{}], Loss: {:.4f}, Accuracy: {:.2f}%'\n                  .format(epoch + 1, num_epochs, i + 1, total_step, loss.item(),\n                          (correct \/ total) * 100))\n\n<\/code>\nException:\n<code class=\"lang-auto\">---------------------------------------------------------------------------\nRuntimeError                              Traceback (most recent call last)\n<ipython-input-7-27984b0eb824> in <module>\n     13         # Backprop and perform Adam optimisation\n     14         optimizer.zero_grad()\n---> 15         loss.backward()\n     16         optimizer.step()\n     17 \n\n\/opt\/conda\/lib\/python3.6\/site-packages\/torch\/tensor.py in backward(self, gradient, retain_graph, create_graph)\n    100                 products. Defaults to ``False``.\n    101         \"\"\"\n--> 102         torch.autograd.backward(self, gradient, retain_graph, create_graph)\n    103 \n    104     def register_hook(self, hook):\n\n\/opt\/conda\/lib\/python3.6\/site-packages\/torch\/autograd\/__init__.py in backward(tensors, grad_tensors, retain_graph, create_graph, grad_variables)\n     88     Variable._execution_engine.run_backward(\n     89         tensors, grad_tensors, retain_graph, create_graph,\n---> 90         allow_unreachable=True)  # allow_unreachable flag\n     91 \n     92 \n\nRuntimeError: Function CatBackward returned an invalid gradient at index 1 - expected device 1 but got 0\n<\/code>\nThanks.","y":"Your code looks generally alright besides this line of code:\n<code class=\"lang-python\">out2.to(gpu1)\n<\/code>\nWhile nn.Modules are transferred inplace, you have have to assign tensors back:\n<code class=\"lang-python\">out2 = out2.to(gpu1)\n<\/code>\nCould you fix this line and see it it\u2019s working?","z":"Your code looks generally alright besides this line of code:\n<code class=\"lang-python\">out2.to(gpu1)\n<\/code>\nWhile nn.Modules are transferred inplace, you have have to assign tensors back:\n<code class=\"lang-python\">out2 = out2.to(gpu1)\n<\/code>\nCould you fix this line and see it it\u2019s working?\nThanks. It is fixed with this.\nHi,\nI am getting the same error while using Model parallelism . Below is my code.\n<code class=\"lang-auto\">import torch\nfrom torch import nn\nimport torch.nn.functional as F\n\n\nclass UNet(nn.Module):\n    def __init__(\n        self,\n        in_channels=1,\n        n_classes=1,\n        depth=5,\n        wf=6,\n        padding=False,\n        batch_norm=False,\n        up_mode='upconv',\n    ):\n        \"\"\"\n        Implementation of\n        U-Net: Convolutional Networks for Biomedical Image Segmentation\n        (Ronneberger et al., 2015)\n        https:\/\/arxiv.org\/abs\/1505.04597\n\n        Using the default arguments will yield the exact version used\n        in the original paper\n\n        Args:\n            in_channels (int): number of input channels\n            n_classes (int): number of output channels\n            depth (int): depth of the network\n            wf (int): number of filters in the first layer is 2**wf\n            padding (bool): if True, apply padding such that the input shape\n                            is the same as the output.\n                            This may introduce artifacts\n            batch_norm (bool): Use BatchNorm after layers with an\n                               activation function\n            up_mode (str): one of 'upconv' or 'upsample'.\n                           'upconv' will use transposed convolutions for\n                           learned upsampling.\n                           'upsample' will use bilinear upsampling.\n        \"\"\"\n        super(UNet, self).__init__()\n        assert up_mode in ('upconv', 'upsample')\n        self.padding = padding\n        self.depth = depth\n        prev_channels = in_channels\n        self.down_path = nn.ModuleList().cuda(0)\n        for i in range(depth):\n            self.down_path.append(\n                UNetConvBlock(prev_channels, 2 ** (wf + i), padding, batch_norm)\n            )\n            prev_channels = 2 ** (wf + i)\n\n        self.up_path = nn.ModuleList().cuda(1)\n        for i in reversed(range(depth - 1)):\n            self.up_path.append(\n                UNetUpBlock(prev_channels, 2 ** (wf + i), up_mode, padding, batch_norm)\n            )\n            prev_channels = 2 ** (wf + i)\n\n        self.last = nn.Conv2d(prev_channels, n_classes, kernel_size=1).cuda(2)\n\n    def forward(self, x):\n        blocks = []\n        x=x.cuda(0)\n        for i, down in enumerate(self.down_path):\n            x = down(x)\n            if i != len(self.down_path) - 1:\n                blocks.append(x)\n                x = F.max_pool2d(x, 2)\n        x=x.cuda(1)\n        for i, up in enumerate(self.up_path):\n            x = up(x, blocks[-i - 1])\n        x=x.cuda(2)\n        return self.last(x)\n\n\nclass UNetConvBlock(nn.Module):\n    def __init__(self, in_size, out_size, padding, batch_norm):\n        super(UNetConvBlock, self).__init__()\n        block = []\n\n        block.append(nn.Conv2d(in_size, out_size, kernel_size=3, padding=int(padding)))\n        block.append(nn.ReLU())\n        if batch_norm:\n            block.append(nn.BatchNorm2d(out_size))\n\n        block.append(nn.Conv2d(out_size, out_size, kernel_size=3, padding=int(padding)))\n        block.append(nn.ReLU())\n        if batch_norm:\n            block.append(nn.BatchNorm2d(out_size))\n\n        self.block = nn.Sequential(*block).cuda(0)\n\n    def forward(self, x):\n        x=x.cuda(0)\n        out = self.block(x)\n        return out\n\n\nclass UNetUpBlock(nn.Module):\n    def __init__(self, in_size, out_size, up_mode, padding, batch_norm):\n        super(UNetUpBlock, self).__init__()\n        if up_mode == 'upconv':\n            self.up = nn.ConvTranspose2d(in_size, out_size, kernel_size=2, stride=2).cuda(1)\n        elif up_mode == 'upsample':\n            self.up = nn.Sequential(\n                nn.Upsample(mode='bilinear', scale_factor=2),\n                nn.Conv2d(in_size, out_size, kernel_size=1),\n            ).cuda(1)\n\n        self.conv_block = UNetConvBlock(in_size, out_size, padding, batch_norm)\n\n    def center_crop(self, layer, target_size):\n        _, _, layer_height, layer_width = layer.size()\n        diff_y = (layer_height - target_size[0]) \/\/ 2\n        diff_x = (layer_width - target_size[1]) \/\/ 2\n        return layer[\n            :, :, diff_y : (diff_y + target_size[0]), diff_x : (diff_x + target_size[1])\n        ]\n\n    def forward(self, x, bridge):\n        x=x.cuda(1)\n        up = self.up(x)\n        crop1 = self.center_crop(bridge, up.shape[2:])\n        out = torch.cat([up, crop1], 1)\n        out = self.conv_block(out)\n\n        return out\noptimizer.zero_grad()\n x, y = batch\n y_pred = model(x.reshape(bs,1,image_size,image_size).float())\n label=y.reshape(bs,1,image_size,image_size).cuda(2).float()\n loss_fn = nn.BCEWithLogitsLoss()\n dice = f_score(y_pred, label)\n iou1=iou(y_pred, label)\n loss_fn(y_pred, label).backward()\n optimizer.step()\n<\/code>\nError:\n<code class=\"lang-auto\"><ipython-input-5-fc64eb051240> in process_function(engine, batch)\n      8     dice = f_score(y_pred, label)\n      9     iou1=iou(y_pred, label)\n---> 10     loss_fn(y_pred, label).backward()\n     11     optimizer.step()\n     12     #print(\"train\", loss.item(), dice.item(), iou1.item())\n\n~\/anaconda3\/envs\/fm\/lib\/python3.7\/site-packages\/torch\/tensor.py in backward(self, gradient, retain_graph, create_graph)\n    105                 products. Defaults to ``False``.\n    106         \"\"\"\n--> 107         torch.autograd.backward(self, gradient, retain_graph, create_graph)\n    108 \n    109     def register_hook(self, hook):\n\n~\/anaconda3\/envs\/fm\/lib\/python3.7\/site-packages\/torch\/autograd\/__init__.py in backward(tensors, grad_tensors, retain_graph, create_graph, grad_variables)\n     91     Variable._execution_engine.run_backward(\n     92         tensors, grad_tensors, retain_graph, create_graph,\n---> 93         allow_unreachable=True)  # allow_unreachable flag\n     94 \n     95 \n\nRuntimeError: Function CatBackward returned an invalid gradient at index 1 - expected device cuda:0 but got cuda:1```<\/code>"},{"x":"Hi I am trying to load my custom dataset using dataloader and custom collate_fn for training the neural network, but an error occurred saying that input should be tensor not list:\n<code class=\"lang-auto\">def my_collate(batch):\n    data = [item[0] for item in batch]\n    target = [item[1] for item in batch]\n    return [data, target]\n<\/code>\n<code class=\"lang-auto\">class bsds_dataset(Dataset):\n    def __init__(self, ds_main, ds_energy):\n        self.dataset1 = ds_main\n        self.dataset2 = ds_energy\n    \n    def __getitem__(self, index):\n        x1 = self.dataset1[index]\n        x2 = self.dataset2[index]\n        \n        return x1, x2\n    \n    def __len__(self):\n        return len(self.dataset1)\n\n<\/code>\n<code class=\"lang-auto\">generic_transform = transforms.Compose([\n    transforms.ToTensor(),\n    transforms.ToPILImage(),\n    #transforms.CenterCrop(size=128),\n    #transforms.Lambda(lambda x: myimresize(x, (128, 128))),\n    transforms.ToTensor(),\n    #transforms.Normalize((0., 0., 0.), (6, 6, 6))\n])\n\noriginal_imagefolder = '.\/images\/whole'\ntarget_imagefolder = '.\/results\/whole'\n\noriginal_ds = ImageFolder(original_imagefolder, transform=generic_transform)\nenergy_ds = ImageFolder(target_imagefolder, transform=generic_transform)\n\ndataset = bsds_dataset(original_ds, energy_ds)\nloader = DataLoader(dataset, batch_size=16, collate_fn=my_collate)\n\nepochs = 2\nmodel = UNet(1, depth=5, merge_mode='concat')\nmodel.cuda()\nloss = torch.nn.MSELoss()\ncriterion_pixelwise = torch.nn.L1Loss()\n\nloss.cuda()\ncriterion_pixelwise.cuda()\n\noptimizer = optim.SGD(model.parameters(), lr=0.001)\n\nTensor = torch.cuda.FloatTensor\n\nfor epoch in range(epochs):\n    for i, batch in enumerate(loader):\n        original, target = batch\n        out = model(original)\n<\/code>\nI have made the dataset and collate_fn but I don\u2019t know how to access each instance of batch.\nerror:\n<code class=\"lang-auto\">---------------------------------------------------------------------------\nTypeError                                 Traceback (most recent call last)\n<ipython-input-147-d1dea1bc00f8> in <module>\n     15     for i, batch in enumerate(loader):\n     16         original, target = batch\n---> 17         out = model(original)\n\nC:\\Anaconda3\\envs\\torchgpu\\lib\\site-packages\\torch\\nn\\modules\\module.py in __call__(self, *input, **kwargs)\n    491             result = self._slow_forward(*input, **kwargs)\n    492         else:\n--> 493             result = self.forward(*input, **kwargs)\n    494         for hook in self._forward_hooks.values():\n    495             hook_result = hook(self, input, result)\n\n<ipython-input-7-5f743c3455c4> in forward(self, x)\n     89         # encoder pathway, save outputs for merging\n     90         for i, module in enumerate(self.down_convs):\n---> 91             x, before_pool = module(x)\n     92             encoder_outs.append(before_pool)\n     93 \n\nC:\\Anaconda3\\envs\\torchgpu\\lib\\site-packages\\torch\\nn\\modules\\module.py in __call__(self, *input, **kwargs)\n    491             result = self._slow_forward(*input, **kwargs)\n    492         else:\n--> 493             result = self.forward(*input, **kwargs)\n    494         for hook in self._forward_hooks.values():\n    495             hook_result = hook(self, input, result)\n\n<ipython-input-5-26a0f7e21ea6> in forward(self, x)\n     14 \n     15     def forward(self, x):\n---> 16         x = F.relu(self.conv1(x))\n     17         x = F.relu(self.conv2(x))\n     18         before_pool = x\n\nC:\\Anaconda3\\envs\\torchgpu\\lib\\site-packages\\torch\\nn\\modules\\module.py in __call__(self, *input, **kwargs)\n    491             result = self._slow_forward(*input, **kwargs)\n    492         else:\n--> 493             result = self.forward(*input, **kwargs)\n    494         for hook in self._forward_hooks.values():\n    495             hook_result = hook(self, input, result)\n\nC:\\Anaconda3\\envs\\torchgpu\\lib\\site-packages\\torch\\nn\\modules\\conv.py in forward(self, input)\n    336                             _pair(0), self.dilation, self.groups)\n    337         return F.conv2d(input, self.weight, self.bias, self.stride,\n--> 338                         self.padding, self.dilation, self.groups)\n    339 \n    340 \n\nTypeError: conv2d(): argument 'input' (position 1) must be Tensor, not list\n<\/code>\nplease help. Thanks a lot","y":"Also answered here with link \"https:\/\/discuss.pytorch.org\/t\/making-custom-image-to-image-dataset-using-collate-fn-and-dataloader\/55951\/4\".","z":"It would help if you post your custom collate function my_collate \nWith that said, at the end of your my_collate, try to do a torch.stack(images).\nIt\u2019s done, sorry, would you please check it again?\nAlso answered here with link \"https:\/\/discuss.pytorch.org\/t\/making-custom-image-to-image-dataset-using-collate-fn-and-dataloader\/55951\/4\"."},{"x":"I\u2019m trying to visualize the feature maps of my network through guided backpropagation. I\u2019m running this through model.eval().\nMy first error, upon the final stages, is RuntimeError: invalid gradient at index 0 - got [1, 6] but expected shape compatible with [1, 10647, 6]. It seems this stems from:\n<code class=\"lang-auto\">onehot_out = torch.FloatTensor(1, model_out.size()[-1]).zero_()\n<\/code>\nso I fix it with:\n<code class=\"lang-auto\">onehot_out = torch.FloatTensor(1, model_out.size()[1], model_out.size()[-1]).zero_()\n<\/code>\nThis then seems to work, however when I then do the backward pass:\n<code class=\"lang-auto\">onehot_out[0][target_class] = 1\nmodel_out.backward(gradient = onehot_out)\n<\/code>\nI get the error present in the title:\n<code class=\"lang-auto\">RuntimeError: one of the variables needed for gradient computation has been modified by an inplace operation: [torch.FloatTensor [1, 3, 52, 52, 2]], which is output 0 of SigmoidBackward, is a\nt version 6; expected version 5 instead. Hint: enable anomaly detection to find the operation that failed to compute its gradient, with torch.autograd.set_detect_anomaly(True)\n<\/code>\nI\u2019m unsure what this means or where to start looking. Would appreciate any help.","y":"Hi,\nThe best way to look at these issues is to enable the anomaly detection with link \"https:\/\/pytorch.org\/docs\/stable\/autograd.html#anomaly-detection\" mode. This will give you the method in the forward that caused this issue.","z":"Hi,\nThe best way to look at these issues is to enable the anomaly detection with link \"https:\/\/pytorch.org\/docs\/stable\/autograd.html#anomaly-detection\" mode. This will give you the method in the forward that caused this issue.\nHi,\nThank you yes. I enabled it shortly after posting this and solved my problem.\nShall I close or delete this thread?\nYou can simply accept the answer of \u201cenable anomaly mode\u201d and leave it like that.\nNice that you found the problem !"},{"x":"Hi, I use unpickle to read image info of cifar10 and put the data together as an <np.ndarray> with shape = (50000, 3). Each column represents data, label and category respectively.\nI build my own cifar10_data_loader as the roadmap:\nndarray -> tensor -> Dataset -> DataLoader\nHere is my code for preprocessing image data:\n<code class=\"lang-auto\">def ndarray_to_tensor(ndarray):\n    \"\"\"\n    It is a function to transform <np.ndarray> to <tensor>\n    :param ndarray: np.ndarray\n    :return: tensor object\n    \"\"\"\n    out = torch.Tensor(list(ndarray))\n    return out\n\n\ndef form_dataset(X_tensor, y_tensor):\n    \"\"\"\n    It is a function to form dataset.\n    :param X_tensor: tensor x\n    :param y_tensor: tensor y\n    :return: dataset\n    \"\"\"\n    dataset = TensorDataset(X_tensor, y_tensor)\n    return dataset\n\n\ndef form_dataloader(tensor_dataset, batch_size, num_workers):\n    \"\"\"\n    It is a function to form data loader from tensor dataset.\n    :param tensor_dataset: tensor dataset\n    :return: data loader\n    \"\"\"\n    data_loader = DataLoader(tensor_dataset, batch_size=batch_size, num_workers=num_workers, shuffle=True)\n    return data_loader\n\n\ndef train_target_model(ndarray_path, trainset_size, net, num_epochs, criterion, optimizer, device, dataset_path):\n    \"\"\"\n    It is a function to train the target model\n    :param net: neural network\n    :param train_loader: train data loader\n    :param num_epochs: number of epochs\n    :param criterion: loss function\n    :param optimizer: optimizer\n    :param device: device (cuda or cpu)\n    \"\"\"\n    # 1. Load norm_all_batch_data array\n    norm_all_data_array = (np.load(ndarray_path, allow_pickle=True))    # (5, 10000, 3)\n    concatenate_norm_array = np.concatenate((norm_all_data_array))      # (50000, 3)\n\n    # 2. Extract train set array\n    trainset_array = concatenate_norm_array[:trainset_size, :]      # (trainset_size, 3)\n    # print(trainset_array)\n\n    X = trainset_array[:, 0]            # Data, shape = (trainset_size, )\n    y = trainset_array[:, 1]            # Label, shape = (trainset_size, )\n\n    # 3. ndarray -> tensor\n    X_tensor = ndarray_to_tensor(ndarray=X)\n    y_tensor = ndarray_to_tensor(ndarray=y)\n\n    # 4. tensor -> dataset\n    target_train_dataset = form_dataset(X_tensor=X_tensor, y_tensor=y_tensor)\n\n    # 5. dataset -> data loader\n    target_train_loader = form_dataloader(tensor_dataset=target_train_dataset,\n                                          batch_size=batch_size,\n                                          num_workers=num_workers)\n    # 6. train\n    train(net=net,\n          train_loader=target_train_loader,\n          num_epochs=num_epochs,\n          criterion=criterion,\n          optimizer=optimizer,\n          device=device)\n\n\ndef main():\n\n    # 1. Define target neural network\n    target_net = CNN_CIFAR10()\n\n    # 2. Define loss function\n    criterion = nn.CrossEntropyLoss()\n\n    # 3. Define optimizer\n    optimizer = optim.Adam(target_net.parameters(), lr=learning_rate)\n\n    # 4. Train target model\n    train_target_model(ndarray_path=\"cifar10\/norm_all_batch_data.npy\",\n                       trainset_size=2500,\n                       net=target_net,\n                       num_epochs=num_epochs,\n                       criterion=criterion,\n                       optimizer=optimizer,\n                       device=device,\n                       dataset_path=dataset_path)\n\n<\/code>\nHere is my CNN model, like LeNet-5:\n<code class=\"lang-auto\">class CNN_CIFAR10(nn.Module):\n    \"\"\"\n    Local Target Model: A standard convolutional neural network.\n    - 2 Conv layer\n    - 2 Max pooling layer\n    - 1 FC layer\n    - 1 Softmax layer\n    - Tanh as the activation function\n    \"\"\"\n    def __init__(self):\n        super(CNN_CIFAR10, self).__init__()\n\n        self.convnet = nn.Sequential(\n            # Conv1\n            nn.Conv2d(in_channels=3, out_channels=6, kernel_size=5),\n            nn.Tanh(),\n\n            # MaxPool1\n            nn.MaxPool2d(kernel_size=2, stride=2),\n\n            # Conv2\n            nn.Conv2d(in_channels=6, out_channels=16, kernel_size=5),\n            nn.Tanh(),\n\n            # MaxPool2\n            nn.MaxPool2d(kernel_size=2, stride=2))\n\n        self.fc = nn.Sequential(\n            # FC1\n            nn.Linear(in_features=16 * 5 * 5, out_features=128),\n            nn.Tanh(),\n\n            # FC2\n            nn.Linear(in_features=128, out_features=10),\n            nn.Softmax())\n\n    def forward(self, x):\n        output = self.convnet(x)\n        output = output.view(output.size[0], -1)\n        output = self.fc(output)\n\n        return output\n<\/code>\nHowever, when I run the main() function above, I encountered this error RuntimeError: Given groups=1, weight of size 6 3 5 5, expected input[100, 32, 32, 3] to have 3 channels, but got 32 channels instead\nCould somebody help me fix this issue? Thanks in advance!","y":"I think permute could be used here,\nX_tensor = X_tensor.permute(0, 3, 1, 2)","z":"I think input should be of size [100, 3, 32, 32], it is batch_size, channels, height, width.\nnn.Conv2d, in_channels=3, expects it in this order.\nThanks for your help. I\u2019m new to Pytorch, could you please tell me where to modify my code?\nMy array X = trainset_array[:, 0] with the shape of (2500, ) and it saved with the 32x32x3 image info.\nWhen I use torch.Tensor(list(ndarray)), X_tensor.size() = torch.Size([2500, 32, 32, 3]), I wonder how can I modify the in_channels=32 to in_channels=3 ?\nIf I don\u2019t wanna modify the model code, how can I modify the array? Thanks in advance!\nI think permute could be used here,\nX_tensor = X_tensor.permute(0, 3, 1, 2)\nThank you so much. Finally solved by reshape tensor"},{"x":"Is it possible to modify the activation maps when our model is in evaluation mode? e.g. set value of certain activation map to 0 but others keep unchanged. I only know we can indirectly turn off an activation by setting the corresponding weight matrix to zero, but if we want to make detailed modification on the activation map, it is impossible in this way.","y":"Hi \nYou should be able to do it with forward pre hooks with link \"https:\/\/pytorch.org\/docs\/stable\/nn.html#torch.nn.Module.register_forward_pre_hook\".\nLet\u2019s say you have the following model:\n<code class=\"lang-auto\">net = nn.Sequential(nn.Linear(16,4),\n                    nn.Linear(4, 16))\n<\/code>\nIf you want to modify the activation maps that goes to the second Linear, and only in eval mode, your hook will look like this:\n<code class=\"lang-auto\">def hook_fn(module, input)\n    if not module.training:\n        for i in input:\n            i[mask] = 0\n<\/code>\nand to register it:\n<code class=\"lang-auto\">hook = net[1].register_forward_pre_hook(hook_fn)\n<\/code>","z":"Hi \nYou should be able to do it with forward pre hooks with link \"https:\/\/pytorch.org\/docs\/stable\/nn.html#torch.nn.Module.register_forward_pre_hook\".\nLet\u2019s say you have the following model:\n<code class=\"lang-auto\">net = nn.Sequential(nn.Linear(16,4),\n                    nn.Linear(4, 16))\n<\/code>\nIf you want to modify the activation maps that goes to the second Linear, and only in eval mode, your hook will look like this:\n<code class=\"lang-auto\">def hook_fn(module, input)\n    if not module.training:\n        for i in input:\n            i[mask] = 0\n<\/code>\nand to register it:\n<code class=\"lang-auto\">hook = net[1].register_forward_pre_hook(hook_fn)\n<\/code>\nThanks for your help. It is so useful to get familiar with hook in pytorch. I only treated it as a intermediate output reader in the past.\nI consider the complete version will be:\n<code class=\"lang-auto\">def modify_input(idx, mask):\n    def hook(module, input):\n        if not module.training:\n            input[idx] = mask\n    return hook\nhook = net[1].register_forward_pre_hook(modify_input(idx, mask))\n<\/code>"},{"x":"I have two tensors, B and a predefined kernel.\n<code class=\"lang-auto\">B.size()\n>> torch.Size([6, 6, 1])\n\nkernel.size()\n>> torch.Size([5])\n<\/code>\nIn scipy it\u2019s possible to convolve the tensor with the kernel along a single axis like:\nconvolve1d(B.numpy(),kernel.numpy(), axis=0,mode=\"constant\")\nmode=\"constant\" refers to zero-padding.\ntorch.nn.functional.conv1d, however, doesn\u2019t have a parameter to convolve along a single axis. Is it possible to mimic that behaviour of scipy?\nThank you in advance!","y":"So you work in mono-channel.\nIn this case, try using B.permute(1, 2, 0) to obtain the desired shape:\n[batch_size, in_channels, sequence_length] (see docs with link \"https:\/\/pytorch.org\/docs\/stable\/nn.functional.html#conv1d\").\nThen your kernel of size [5] needs to be [out_channel, in_channels, kernel_size], so it needs a kernel.view(1, 1, -1).\nAnd to keep the same size, you need a padding of ((kernel_size - 1) \/ 2) pixels.\nSo for example, torch.nn.functional.conv1d(B.permute(1, 2, 0), kernel.view(1, 1, -1), padding=2)","z":"nn.Conv1d will use dim2 as the time axis and convolve (or rather cross-correlate) in this dimension.\nTherefore, you would need to permute the dimensions in your input.\nHere is a small example:\n<code class=\"lang-python\">conv = nn.Conv1d(6, 1, kernel_size=5, padding=2)\ninput = torch.randn(6, 6, 1)\ninput = input.permute(2, 1, 0)\nprint(input.shape) # [batch_size, channels, seq]\n> torch.Size([1, 6, 6])\noutput = conv(input)\n<\/code>\nBut the weight tensor then actually has the shape torch.Size([1, 6, 5]), while my kernel is just torch.Size([5]). Do you suggest to assign a repeated sequence of my kernel to the weights, such as\nconv.weight = torch.nn.Parameter(kernel.repeat(6).view(1,6,5))\nBut also I expect an output of the same shape as the input, but in your example the output is of shape torch.Size([1, 1, 6]), which is different from the input shape torch.Size([1, 6, 6]). Above scipy function doesn\u2019t change the shape.\nThe scipy function doesn\u2019t change the shape because it uses padding, by default.\nThere is an optional padding option for torch.nn.Conv1d.\nThe shape of the weight tensor depends of the number of in_channels and out_channels.\nIn your example, can you clarify what does the size [6, 6, 1] of your tensor B correspond to ?\nIt\u2019s actually [sequence_length, batch_size, value], and I\u2019m trying to convolve the value along the sequence.\nSo you work in mono-channel.\nIn this case, try using B.permute(1, 2, 0) to obtain the desired shape:\n[batch_size, in_channels, sequence_length] (see docs with link \"https:\/\/pytorch.org\/docs\/stable\/nn.functional.html#conv1d\").\nThen your kernel of size [5] needs to be [out_channel, in_channels, kernel_size], so it needs a kernel.view(1, 1, -1).\nAnd to keep the same size, you need a padding of ((kernel_size - 1) \/ 2) pixels.\nSo for example, torch.nn.functional.conv1d(B.permute(1, 2, 0), kernel.view(1, 1, -1), padding=2)\nThank you!  That did work (almost).\nThe kernel needed to be mirrored to work exactly like expected and like the scipy function."},{"x":"I am working on a model which looks like below fig:\n\nInput image (I) is fed to the system which goes through some convolution layers and produces feature vector X. The feature vector X is then connected to two fully connected layers U and V. U and V are then connected to a series of deconvolution layers that generate some image. I have different losses and ground truths at the end of U and V.\nThe problem is that I am not able to understand how to perform the backpropagation in such a model. How would I be able to combine the two gradients when they backpropagate from U->X and V->X? The idea is to simply add the two gradients, but I am not able to understand how to achieve that.\nThis is my first question in PyTorch discussion. Let me know if I am unclear and if you need more details.","y":"Hi ,\nIn PyTorch, gradients are accumulating by default.\nSo if you simply have this before optimizer.step:\n<code class=\"lang-auto\">Loss1.backward()\nLoss2.backward()\n<\/code>\nwith no optimizer.zero_grad()  in between,\nyou will end up with x.grad containing dLoss1\/dx + dLoss2\/dx.\nAlso when you create the optimzer you have to give it the params of the 3 models:\n<code class=\"lang-auto\">optimizer = torch.optim.Adam(list(model1.parameters()) + list(model2.parameters()) + list(model3.parameters()))\n<\/code>","z":"Hi ,\nIn PyTorch, gradients are accumulating by default.\nSo if you simply have this before optimizer.step:\n<code class=\"lang-auto\">Loss1.backward()\nLoss2.backward()\n<\/code>\nwith no optimizer.zero_grad()  in between,\nyou will end up with x.grad containing dLoss1\/dx + dLoss2\/dx.\nAlso when you create the optimzer you have to give it the params of the 3 models:\n<code class=\"lang-auto\">optimizer = torch.optim.Adam(list(model1.parameters()) + list(model2.parameters()) + list(model3.parameters()))\n<\/code>\nThis is working.\nJust that the Loss1 had to be written like\nLoss1.backward(retain_graph = True)"},{"x":"I am trying to run my model on gpu, but I am getting\nRuntimeError: CUDA error: device-side assert triggered\nMy model runs fine on CPU.\nI have set os.environ['CUDA_LAUNCH_BLOCKING'] = \"1\" and it is showing error when I am loading the model on cuda on the following line -\n<code class=\"lang-auto\">if train_on_gpu:\n    unet = unet.to('cuda')\n<\/code>","y":"The thing is that this environment variable is only affecting the initialization of the cuda driver. This means that if you set it in your notebook after the driver has been initialized, it won\u2019t have any effect.\nIt either needs to be set when you launch the notebook itself. Or the first thing a clean python kernel runs before calling any cuda related stuff.","z":"Hi,\nDid you make sure to change the env variable before loading pytorch (or at least any cuda related stuff)?\nIs anything else than the assert getting printed?\nHi,\nNo, I didn\u2019t change any env variables, could you please tell how to do it?\nThe whole error traceback is getting printed, I am posting it here-\n<code class=\"lang-auto\">RuntimeError                              Traceback (most recent call last)\n<ipython-input-80-b3e46229c2c3> in <module>\n----> 1 unet = unet.to('cuda')\n      2 print(unet.device)\n\n~\\.conda\\envs\\opt\\lib\\site-packages\\torch\\nn\\modules\\module.py in to(self, *args, **kwargs)\n    430             return t.to(device, dtype if t.is_floating_point() else None, non_blocking)\n    431 \n--> 432         return self._apply(convert)\n    433 \n    434     def register_backward_hook(self, hook):\n\n~\\.conda\\envs\\opt\\lib\\site-packages\\torch\\nn\\modules\\module.py in _apply(self, fn)\n    206     def _apply(self, fn):\n    207         for module in self.children():\n--> 208             module._apply(fn)\n    209 \n    210         def compute_should_use_set_data(tensor, tensor_applied):\n\n~\\.conda\\envs\\opt\\lib\\site-packages\\torch\\nn\\modules\\module.py in _apply(self, fn)\n    206     def _apply(self, fn):\n    207         for module in self.children():\n--> 208             module._apply(fn)\n    209 \n    210         def compute_should_use_set_data(tensor, tensor_applied):\n\n~\\.conda\\envs\\opt\\lib\\site-packages\\torch\\nn\\modules\\module.py in _apply(self, fn)\n    228                 # `with torch.no_grad():`\n    229                 with torch.no_grad():\n--> 230                     param_applied = fn(param)\n    231                 should_use_set_data = compute_should_use_set_data(param, param_applied)\n    232                 if should_use_set_data:\n\n~\\.conda\\envs\\opt\\lib\\site-packages\\torch\\nn\\modules\\module.py in convert(t)\n    428 \n    429         def convert(t):\n--> 430             return t.to(device, dtype if t.is_floating_point() else None, non_blocking)\n    431 \n    432         return self._apply(convert)\n\nRuntimeError: CUDA error: device-side assert triggered\n<\/code>\nChanging os.environ is actually changing an environment variable .\nA safer way to do it is to launch you program with CUDA_LAUNCH_BLOCKING=1 python your_script.py.\nIs there anything printed before the traceback that is done your own printing?\nI have included in os.environ['CUDA_LAUNCH_BLOCKING'] = \"1\" in my jupyter notebook.\nNo, I am not printing before the traceback\nThe thing is that this environment variable is only affecting the initialization of the cuda driver. This means that if you set it in your notebook after the driver has been initialized, it won\u2019t have any effect.\nIt either needs to be set when you launch the notebook itself. Or the first thing a clean python kernel runs before calling any cuda related stuff.\n\n\n\n Rishav_Sapahia:\n\nRuntimeError: CUDA error: device-side assert triggered\n\n\nThanks a lot. It worked. If I am running a script then how to do it?\nYou can run your command as: CUDA_LAUNCH_BLOCKING=1 python your_script.py. This will define the environment variable only for your command. You can also enable it in a given shell by running export CUDA_LAUNCH_BLOCKING=1 and all subsequent commands in this shell will have it."},{"x":"I am using pytorch dataloaders for loading my data. I have 2 folders , one for real and other for fake (1000 images each) in the folder.\nHere is my current code.\n<code class=\"lang-auto\">import PIL.ImageOps\nfrom torch.utils.data import Dataset\nimport torchvision.datasets as dset\nfrom torchvision import transforms\nimport numpy as np\nimport cv2\n\nclass CustomData(Dataset):\n    \"\"\"\n    CustomData dataset\n    \"\"\"\n\n    def __init__(self, name, base_path, transform=None, should_invert=False):\n        super(Dataset, self).__init__()\n        self.base_path = base_path\n        self.inputFolderDataset = dset.ImageFolder(root=self.base_path + '\/')\n        self.transform = transform\n        self.should_invert = should_invert\n        self.to_tensor = transforms.ToTensor()\n\n    def __getitem__(self, index):\n        # Training input images\n        input_images = self.inputFolderDataset.imgs\n        # Assign label to class\n        # 0 for original, 1 for fake\n        input_images = [(t[0], 0) if \"orig\" in t[0] else (t[0], 1) for t in input_images]\n        input_img = cv2.imread(input_images[index][0])\n        input_img = np.array(input_img, dtype='uint8')\n        input_img = cv2.cvtColor(input_img, cv2.COLOR_BGR2RGB)\n\n        if self.should_invert:\n            input_img = PIL.ImageOps.invert(input_img)\n\n        if self.transform is not None:\n            input_img_as_tensor = self.transform(input_img)\n        else:\n            input_img_as_tensor = self.to_tensor(input_img)\n\n        return input_img_as_tensor, input_images[index][1]\n\n    def __len__(self):\n        return len(self.inputFolderDataset.imgs)\n<\/code>\nI have 2 doubts:\n\nI want to do something like few-shot learning (1-shot, 2-shot, 3-shot) and compare the results. Is it possible to load only a subset of data during training (without adding\/deleting the images in my folder).\nAlso lets say if I am doing 1-shot learning and I want to average the results over 10 runs, so for these 10 runs every time I want to randomly select the image that I am using for training. Is it possible to do so ?\n","y":"Hi,\nI do not know about x-shot learning but if you want to get a subset of your dataset, you just need to change the __len__() function to return maximum size you want.\nThis function return the length of your dataset, so if you put a constant number or a function the current number, you get as much as you wanted.\n<code class=\"lang-python\"> def __len__(self):\n        return len(self.inputFolderDataset.imgs)\/10 # any number will work\n<\/code>\n\nAbout your second question, DataLoader class can solve your problem. It \u2018shuffle\u2019 argument which can create random batches of your dataset.\nHere is a snippet you can use.\n<code class=\"lang-python\">from torch.utils.data import DataLoader\n# your CustomData is defined here\ncustom_data = CustomData(*args)\ntrain_loader = DataLoader(dataset=custom_data,\n                          batch_size= 16 # batch size,\n                          shuffle=True,  # this line do the random thing\n                          num_workers=0)\n<\/code>\nBased on PyTorch, every epoch, it will generate different random numbers so you\u2019ll get different images.\nGood luck","z":"Hi,\nI do not know about x-shot learning but if you want to get a subset of your dataset, you just need to change the __len__() function to return maximum size you want.\nThis function return the length of your dataset, so if you put a constant number or a function the current number, you get as much as you wanted.\n<code class=\"lang-python\"> def __len__(self):\n        return len(self.inputFolderDataset.imgs)\/10 # any number will work\n<\/code>\n\nAbout your second question, DataLoader class can solve your problem. It \u2018shuffle\u2019 argument which can create random batches of your dataset.\nHere is a snippet you can use.\n<code class=\"lang-python\">from torch.utils.data import DataLoader\n# your CustomData is defined here\ncustom_data = CustomData(*args)\ntrain_loader = DataLoader(dataset=custom_data,\n                          batch_size= 16 # batch size,\n                          shuffle=True,  # this line do the random thing\n                          num_workers=0)\n<\/code>\nBased on PyTorch, every epoch, it will generate different random numbers so you\u2019ll get different images.\nGood luck"},{"x":"Hi all,\nI have a question, I found a solution to brutforce the problem but that\u2019s not very pretty coding. I was wondering if you had a better a solution for my following problem.\nI have made a VAE that\u2019s in the following form:\n<code class=\"lang-auto\">def forward(self, input):\n    latent_vector = encoder(input)\n    output = decoder(latent_vector)\n    return output \n<\/code>\nI actually have loss in the following form:\n<code class=\"lang-auto\">predicted = self.model(inputs).float()\nloss = self.criterion(predicted, expected)\nloss.backward()\nself.optimizer.step()\n<\/code>\nI would like to force the latent vector to have a particular form, so I would like to add a term in the loss like:\n<code class=\"lang-auto\">loss = self.criterion(predicted, expected) + some_function(latent_vector)\n<\/code>\nIs there a proper way to get my latent vector to perform that ?\nBest,\nDidier","y":"So based on your explanation, you need all latent vectors of the dataset to be able to calculate the for loop. Actually, it may be tricky, because of memory issues because you need to pass all the dataset containing a class such as cat.\nThere is point here. the encoder generates latent_vector, so if you want the latent vector of whole dataset to calculate loss for only a batch, you have to prepare latent vectors one the go. I mean when you passed a batch through the network to calculate below code:\n\n\n\n DidierDeschamps:\n\nLoss = self.criterion(current_reconstructed_cat_i, cat_i)\n\n\nyou need all latent vectors, then you will optimize your model a step. So for the next batch, the latent vectors you already have are no longer valid and you have to calculate them again. This will be a slow process.But let say there is no such a problem. To calculate latent vector without updating weights, you can do following in your training procedure:\n<code class=\"lang-python\">latent_vectors = []\nwith torch.no_grad():\n        for data in data_loader:\n            latent_vector = model.encoder(data)\n            latent_vectors.append(latent_vector)\n return latent_vectors\n<\/code>\nThe only point is to use torch.no_grad to preserve the models state. But still I am concerned about the performance.","z":"Hi,\nIf I have understood your question properly, You can just return output and latent_vector and do the other things as you have mentioned. But if you do not want to return the output not a tuple, I think it is good to add a attribute to your Model class and update it every time through forward function and you can obtain its value just by calling model.self.latent_vector.\n<code class=\"lang-python\">class Model:\n  def __init__(self):\n    self.latent_vector = None\n\n  def forward(self, x):\n    self.latent_vector = x\n    output = x*2\n    return output\n\nmodel(2)  # prints 4\nmodel.latent_vector  # prints 2\n<\/code>\nGood luck\nNik\nOkay thank you so much for your answer in fact I just found a problem with this method according to my problem\nIn fact I don\u2019t want to look at the latent vector of the current input.\nI have different classes let say for examples animals : cats, dogs and birds.\nIf my current input is a cat: cat_i and I have n cats in my dataset: {cat_1, cat_2, \u2026, cat_n}.\nI want the latent vector to learn the distribution among the cats, the dogs and the birds separately.\nSo I would like to code the Loss this way:\n<code class=\"lang-auto\">Loss = self.criterion(current_reconstructed_cat_i, cat_i) \n+ sum{for j=1 to n, j!=i} some_function(current_latent_vector_of_cat_j, current_latent_vector_of_cat_i)\n<\/code>\ncurrent_latent_vector_of_cat_j is the current pass forward stopped to the latent_vector.\nBest,\nDidier\nSo based on your explanation, you need all latent vectors of the dataset to be able to calculate the for loop. Actually, it may be tricky, because of memory issues because you need to pass all the dataset containing a class such as cat.\nThere is point here. the encoder generates latent_vector, so if you want the latent vector of whole dataset to calculate loss for only a batch, you have to prepare latent vectors one the go. I mean when you passed a batch through the network to calculate below code:\n\n\n\n DidierDeschamps:\n\nLoss = self.criterion(current_reconstructed_cat_i, cat_i)\n\n\nyou need all latent vectors, then you will optimize your model a step. So for the next batch, the latent vectors you already have are no longer valid and you have to calculate them again. This will be a slow process.But let say there is no such a problem. To calculate latent vector without updating weights, you can do following in your training procedure:\n<code class=\"lang-python\">latent_vectors = []\nwith torch.no_grad():\n        for data in data_loader:\n            latent_vector = model.encoder(data)\n            latent_vectors.append(latent_vector)\n return latent_vectors\n<\/code>\nThe only point is to use torch.no_grad to preserve the models state. But still I am concerned about the performance."},{"x":"Hi i have been trying to build a simple CNN for the MNIST dataset, but about after 4 epochs, I have a memory overflow issues. Any suggestions on how I can fix this? Thank you!\nnotebook here : https:\/\/colab.research.google.com\/drive\/16RFi_fERIyHMpiNXlGJb01faUsJWrkot","y":"Hi Nishanth,\nI had a look at the training code. The training loop is correct. The validation had this line:\n<code class=\"lang-auto\">loss += criterion(outputs, classes)\n<\/code>\nThis was causing pytorch to retain the memory of all the previous graphs executed for validation.\nChanging it to the below line will solve your problem.\n<code class=\"lang-auto\">loss += criterion(outputs, classes).item()\n<\/code>","z":"Hi Nishanth,\nThe notebook doesn\u2019t have the code for the training loop. Could you upload that as well? Since, the training loop is the one causing the error.\nsorry, I accidentally removed it. It should be there now thought!\nthanks for any help\nHi Nishanth,\nI had a look at the training code. The training loop is correct. The validation had this line:\n<code class=\"lang-auto\">loss += criterion(outputs, classes)\n<\/code>\nThis was causing pytorch to retain the memory of all the previous graphs executed for validation.\nChanging it to the below line will solve your problem.\n<code class=\"lang-auto\">loss += criterion(outputs, classes).item()\n<\/code>\nYep! that solved my problem. Thank you!"},{"x":"As the title clearly describes, the  accuracy  of my  CNN  stays the same when the  criterion  is selected as  CrossEntropyLoss. I especially selected  CrossEntropyLoss  since only it achieves the  test loss  close to the  training loss. No issues at all for the other loss functions.\nHere is the overview of the constructed  CNN  model:\n<code class=\"lang-auto\">MyNet(\n  (activation_fn): ReLU(inplace)\n  (conv1): Sequential(\n    (0): Conv2d(3, 16, kernel_size=(5, 5), stride=(1, 1))\n    (1): ReLU(inplace)\n    (2): Dropout2d(p=0.5)\n  )\n  (conv2): Sequential(\n    (0): Conv2d(16, 32, kernel_size=(5, 5), stride=(1, 1))\n    (1): ReLU(inplace)\n    (2): Dropout2d(p=0.5)\n    (3): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n  )\n  (conv3): Sequential(\n    (0): Conv2d(32, 64, kernel_size=(5, 5), stride=(1, 1))\n    (1): ReLU(inplace)\n    (2): Dropout2d(p=0.5)\n  )\n  (conv4): Sequential(\n    (0): Conv2d(64, 128, kernel_size=(5, 5), stride=(1, 1))\n    (1): ReLU(inplace)\n    (2): Dropout2d(p=0.5)\n  )\n  (conv5): Sequential(\n    (0): Conv2d(128, 256, kernel_size=(5, 5), stride=(1, 1))\n    (1): ReLU(inplace)\n    (2): Dropout2d(p=0.5)\n    (3): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n  )\n  (fc1): Sequential(\n    (0): Linear(in_features=4096, out_features=1600, bias=True)\n    (1): Dropout2d(p=0.5)\n  )\n  (fc2): Sequential(\n    (0): Linear(in_features=1600, out_features=400, bias=True)\n    (1): Dropout2d(p=0.5)\n  )\n  (fc3): Sequential(\n    (0): Linear(in_features=400, out_features=100, bias=True)\n    (1): Dropout2d(p=0.5)\n  )\n  (fc4): Sequential(\n    (0): Linear(in_features=100, out_features=8, bias=True)\n  )\n)\n<\/code>\nHere is my  test  function:\n<code class=\"lang-auto\">def test():\n    model.eval()\n    test_loss = 0\n    correct = 0\n    with torch.no_grad():\n        for data, target in test_loader:\n            data, target = data.to(device), target.to(device)\n\n            output = model(data)\n            output = output.to(device)\n            test_loss += criterion(output, target).item()\n            _, predicted = torch.max(output.data, 1)\n            correct += (predicted == target).sum().item()\n\n    test_loss \/= math.ceil((len(test_loader.dataset) \/ test_batch_size))\n    test_losses.append(test_loss)\n\n    acc = 100. * correct \/ len(test_loader.dataset)\n    print('\\nTest set: Average loss: {:.4f}, Accuracy: {}\/{} ({:.2f}%)\\n'.format(\n        test_loss, correct, len(test_loader.dataset), acc))\n<\/code>","y":"Balancing the dataset should trace the accuracy of the majority class for an increase in the accuracy of the minority classes.\nI\u2019ve created a tutorial a while ago here with link \"https:\/\/github.com\/ptrblck\/tutorials\/blob\/imbalanced_tutorial\/intermediate_source\/imbalanced_data_tutorial.py\", which is quite old by now, but might give you some insight.","z":"Which other loss functions did you use?\nWas the accuracy better or worse?\nIs your current loss changing, while the accuracy stays the same from the beginning?\nThanks   for your care. Well, I used CrossEntropyLoss as I have stated in the OP. The accuracy stays at 50.22% which is worse than NLLLoss but the test loss is a lot better than it. When it comes to current loss, yes, it is changing.\nAre you applying F.log_softmax inside your forward method? nn.CrossEntropyLoss expects logits, as the log softmax will be applied internally. If you pass raw logits directly to nn,NLLLoss, I would assume your model performs worse.\nHave you played around with some hyperparameters, e.g. the learning rate?\nIs your dataset balanced, i.e. is the class distribution approx. equal?\nYes, I am applying F.log_softmax inside the forward function. When I change learning rate from 0.001 to 0.01, now the accuracy changes a little bit but still it is way lower than nn.NLLLoss. So when using nn.CrossEntropyLoss, should I directly return the output inside the forward function?\nRegarding my dataset, it is a publicly available facial expression dataset hosted on GitHub with link \"https:\/\/github.com\/muxspace\/facial_expressions\", it is not well balanced due to the nature of the context.\nThanks for the information!\nSince you are applying F.log_softmax, you should use nn.NLLLoss as the criterion.\nNot at all, thanks for the feedback. Fair enough, but actually I am testing various loss functions in order to reveal if I might improve the accuracy. So, when using  nn.CrossEntropyLoss, should I directly return the output of the FC layers as the last command of the forward function?\nnn.CrossEntropyLoss expects logits, so you should return the output of the last layer without any non-linearity.\nHowever, internally nn.CrossEntropyLoss will apply F.log_softmax and nn.NLLLoss as seen here with link \"https:\/\/github.com\/pytorch\/pytorch\/blob\/d3f6d5885d68d47d38a475d4fc9cf1611c91920c\/torch\/nn\/functional.py#L1994\", so you would get the same results.\nLast question, what could be the reason for achieving a high loss despite a good accuracy when using nn.NLLLoss? For the same dataset, the average loss is 46.84 despite the accuracy is 84.19%. And the result does not improve through the epochs - even decreases. I have played around the hyper-parameters and tried different optimizers through the recommendations available on the Internet, but nothing was significantly changed.\nYour model might be overfitting to the majority class and thus not learning anything useful.\nCould you check your class distribution and see if the majority class occurs approx. In 84% of all cases?\nAlso check the prediction for unique values.\nIf you see that the class imbalance might be the reason for this issue, you could use a WeightedRandomSampler or pass class weights to your loss function to counter this effect.\nI see, just integrated the proposed method on another topic here with link \"https:\/\/discuss.pytorch.org\/t\/balanced-sampling-between-classes-with-torchvision-dataloader\/2703\/3\" to use WeightedRandomSampler and will post the result here when the training and test phases are completed. Between, could you recommend any utility function to check the class distribution and the prediction for unique values?\nI would store the targets in a tensor and call .unique(return_counts=True) on it. The same would work for the predictions (use torch.argmax(output, 1) to get the class predictions).\nWhen I use WeightedRandomSampler or the one published on GitHub named imbalanced-dataset-sampler with link \"https:\/\/github.com\/ufoym\/imbalanced-dataset-sampler\", the average loss was decreased a lot (now it is ~2), but the accuracy was decreased dramatically as well, which was ~50%, and now is ~2.50%. So, is the only solution simply skipping the used dataset since it is imbalanced, and samplers do not work well?\nBalancing the dataset should trace the accuracy of the majority class for an increase in the accuracy of the minority classes.\nI\u2019ve created a tutorial a while ago here with link \"https:\/\/github.com\/ptrblck\/tutorials\/blob\/imbalanced_tutorial\/intermediate_source\/imbalanced_data_tutorial.py\", which is quite old by now, but might give you some insight."},{"x":"I have batch size = 5\nmy network output is given by the following code Output = F.upsample(per_frame_logits, t, mode='linear')\nShape of output is = torch.Size([5, 2, 64])\nShape of target is = torch.Size([5]) (i.e. ex [1.0, 0.0, 0.0, 1.0, 1.0])\nThen i pass it to following loss function loss = F.binary_cross_entropy_with_logits(output, target)\nI get the following value error\n<code class=\"lang-auto\">raise ValueError(\"Target size ({}) must be the same as input size ({})\".format(target.size(), input.size()))\nValueError: Target size (torch.Size([5])) must be the same as input size (torch.Size([5, 2, 64]))\n\n<\/code>","y":"If you would like to classify each video sequence (64 frames) to a single class (binary classification), your output and target should both have the shape [batch_size, 1].\nTo achieve this you would need to reduce the model\u2019s output, e.g. using an nn.Linear layer as the final classifier.","z":"Could you explain your current use case a bit?\nIt seems your model outputs a batch of 5 samples, each containing the logits (probabilities) for 2 classes for a tensor of length 64.\nAlso, it seems you are dealing with a multi-label classification, i.e. each sample might contain none, one, or both classes. Is this correct?\nIf so, what are the targets correspond to? It seems you are just passing a binary target for each sample in the batch, which would point to a vanilla binary classification.\nI\u2019m a beginner to pytorch and implementing i3d network for binary classification. I have RGB video (64 frames simultaneously) input to the network and each video have a single label which is 0 (failure) or 1 (success).\nI kept my batch size to 5 just to check if my network or code is working or not. (I would call it a debug run)\n\n\n\n ptrblck:\n\nAlso, it seems you are dealing with a multi-label classification, i.e. each sample might contain none, one, or both classes. Is this correct?\n\n\nEach class have a single label. So I guess I should change network output from 2 to 1.\nSo the output shape would correspond to [batch_size, nb_classes, frames]?\nI\u2019m not that familiar with the i3d model, but I assume the temporal (and spatial) dimensions were reduced somehow?\nThe current output format would correspond to a frame-wise multi-label classification.\n\n\n\n Pritesh_Gohil:\n\nEach class have a single label. So I guess I should change network output from 2 to 1.\n\n\nIn that case, you could use nn.BCEWithLogitsLoss (or nn.BCELoss + sigmoid) with a since output channel. Alternatively you could keep the two output channels and use nn.CrossEntropyLoss.\nBelow is the link to the author\u2019s i3d network. In their case they frame-wise multi-label classification.\n[https:\/\/github.com\/piergiaj\/pytorch-i3d](http:\/\/i3d Network for charades dataset)\nI\u2019m using Visual-Tactile dataset and\n\n\n\n ptrblck:\n\nSo the output shape would correspond to [batch_size, nb_classes, frames] ?\n\n\nI3D is designed on kinetics dataset and I didn\u2019t change default architecture from the above link having file \u201cpytorch_i3d.py\u201d.\n\n\n\n ptrblck:\n\nI\u2019m not that familiar with the i3d model, but I assume the temporal (and spatial) dimensions were reduced somehow?\n\n\nI\u2019m also new to this. But according to the author input frames to the network is 64. So each video i have converted to 64 frames. because I do not have to do multilabel classification.\n\n\n\n ptrblck:\n\nThe current output format would correspond to a frame-wise multi-label classification.\n\n\nProbably I need to change the final layer since i don\u2019t want multi-label classification.\n\n\n\n ptrblck:\n\nIn that case, you could use nn.BCEWithLogitsLoss (or nn.BCELoss + sigmoid) with a since output channel. Alternatively you could keep the two output channels and use nn.CrossEntropyLoss .\n\n\nI still have the issue with dimension because it is clear that target and output are not of the same shape or as expected input to the loss function.\nIf you would like to classify each video sequence (64 frames) to a single class (binary classification), your output and target should both have the shape [batch_size, 1].\nTo achieve this you would need to reduce the model\u2019s output, e.g. using an nn.Linear layer as the final classifier.\nThank you \nNow it\u2019s working. Yes, that\u2019s a possible solution I tried and it worked. But instead disturbing the i3d architecture I converted the output of network into[batch_size, 1] by max-pooling with the dimension of 1 and then squeezed the output which makes my target and output shape the same."},{"x":"Hi, I am a newcomer to PyTorch.\nAs cuda() give a better performance since it\u2019s on GPU, I wonder if I can call other module during CUDA calculation.\nI have known that currently we can not use numpy directly in GPU mode, and if I want to do that I have to use .cpu().numpy() to migrate the data to CPU first.\nSo I am curious about other module. For example, just like using  queue module as following:\n<code class=\"lang-auto\">import queue\nx=torch.cuda.FloatTensor([1.0])\nq=queue.Queue()\nq.put(x)\n<\/code>\nAbove code didn\u2019t get any errors. So does it run on GPU exactly? Which means I have created a Queue object on the GPU? And is it advisable to do that? Does it mean that some modules are supported by torch cuda but others are not ?\nWhat\u2019s more, if I hard code a list, dict or something else (eg. x=[1,2,3,4,5]) in my model code, will it (list[1,2,3,4,5]) be migrated to GPU memory automatically when use model.cuda() ?\nThank you very much.","y":"What are you trying to achieve using the queue?\nHave a look at the Multiprocessing Best Practices with link \"https:\/\/pytorch.org\/docs\/stable\/notes\/multiprocessing.html#cuda-in-multiprocessing\", as it might get tricky.\n\n\n\n Wayne_Mai:\n\nWhat\u2019s more, if I hard code a list, dict or something else (eg. x=[1,2,3,4,5] ) in my model code, will it ( list[1,2,3,4,5] ) be migrated to GPU memory automatically when use model.cuda() ?\n\n\nNo, plain Python types won\u2019t be pushed to the GPU. You would have to create a tensor and register it as an nn.Parameter (if trainable) or buffer (using self.register_buffer, if not trainable).","z":"What are you trying to achieve using the queue?\nHave a look at the Multiprocessing Best Practices with link \"https:\/\/pytorch.org\/docs\/stable\/notes\/multiprocessing.html#cuda-in-multiprocessing\", as it might get tricky.\n\n\n\n Wayne_Mai:\n\nWhat\u2019s more, if I hard code a list, dict or something else (eg. x=[1,2,3,4,5] ) in my model code, will it ( list[1,2,3,4,5] ) be migrated to GPU memory automatically when use model.cuda() ?\n\n\nNo, plain Python types won\u2019t be pushed to the GPU. You would have to create a tensor and register it as an nn.Parameter (if trainable) or buffer (using self.register_buffer, if not trainable).\nThank you very much! Actually I\u2019m trying to implement a simple breadth first search algorithm, I\u2019ll try if I can speed it using torch.multiprocessing."},{"x":"Hi All,\nI have a network that takes three images in the input layer. Now the three images must be frames of the same video (this can be known only from the filename of the image). I understood how to change the dataloader\u2019s __getitem__ to send multiple inputs from here with link \"https:\/\/discuss.pytorch.org\/t\/upload-a-customize-data-set-for-multi-regression-task\/43413\/2\". But how do I make sure three images from the same video are chosen and that none of them are repeated in the same epoch ?","y":"Here is a small dummy example using multiple video folders.\nNote that I\u2019ve used tensors directly, so you should add your frame loading logic into the Dataset.\n<code class=\"lang-python\">class MyDataset(Dataset):\n    def __init__(self, videos, transform=None, nb_frames=3):\n        self.nb_frames = nb_frames\n        self.transform = transform\n        \n        # Crop data to multiple of nb_frames\n        self.data = [v[:-(len(v)%self.nb_frames)] if len(v)%self.nb_frames!=0 else v \n                     for v in videos]\n        \n        # calculate lengths\n        self.lens = [len(d)\/\/self.nb_frames for d in self.data]\n        # calculate offsets\n        self.offsets = np.concatenate(([0], np.cumsum(self.lens[:-1])))\n        \n    \n    def __getitem__(self, index):\n        # subtract offset\n        print('index: {}'.format(index))\n        # get corresponding video file\n        found = False\n        for i, offset in enumerate(self.offsets):\n            if index < offset:\n                print('subtracting {} from index'.format(self.offsets[i-1]))\n                index -= self.offsets[i-1]\n                index *= self.nb_frames\n                found = True\n                break\n        # handle last video separately\n        if not found:\n            index -= self.offsets[-1]\n            index *= self.nb_frames\n            i += 1\n\n        # select correspondind data\n        print('selecing video {}'.format(i-1))\n        data = self.data[i-1]\n        # get frames\n        print('reading frames {}'.format([idx for idx in range(index, index+self.nb_frames)]))\n        x = []\n        for idx in range(index, index+self.nb_frames):\n            tmp = data[idx]\n            if self.transform:\n                tmp = self.transform(tmp)\n            x.append(tmp)\n        x = torch.cat(x)\n        \n        return x\n        \n    def __len__(self):\n        return np.sum(self.lens)\n\n\nvideos = [torch.ones(torch.randint(3, 12, (1,)), 1)*i for i in range(5)]\n\ndataset = MyDataset(videos)\nfor data in dataset:\n    print(data)\n\nloader = DataLoader(\n    dataset,\n    batch_size=2,\n    shuffle=True)\nfor data in loader:\n    print(data)\n<\/code>\nThe code currently uses nb_frames consecutive frames for each video folder and removes the trailing frames.\nShuffling using a DataLoader will work.\nI\u2019ve also tried to add some debug print statement for better understanding, but let me know, if your need more information about this code.","z":"It depends a bit on the current way of storing these files.\nDid you store each image from a video in a corresponding folder?\nIf so, how would you like to sample the files, if their amount is not divisible without a remainder by 3? Would you like to drop the last files or fill it up with repetition?\nShould the frames be contiguous in a sample or would you like to shuffle the frames from a single video?\nAlso, would you like to shuffle the video folders or should the videos be loaded in a consecutive way?\nThanks for replying, \nCurrently all images are together in test, train and val folders. If amount not divisible by 3, dropping the last frame is no problem . The frames from a single video need not be contiguous. It\u2019s fine as long as they are from the same video.\nThe video folders (I can change my dataset to that format) can be shuffled freely. There\u2019s no order among the videos, and the order of the frames in a single video can also be ignored.\nHere is a small dummy example using multiple video folders.\nNote that I\u2019ve used tensors directly, so you should add your frame loading logic into the Dataset.\n<code class=\"lang-python\">class MyDataset(Dataset):\n    def __init__(self, videos, transform=None, nb_frames=3):\n        self.nb_frames = nb_frames\n        self.transform = transform\n        \n        # Crop data to multiple of nb_frames\n        self.data = [v[:-(len(v)%self.nb_frames)] if len(v)%self.nb_frames!=0 else v \n                     for v in videos]\n        \n        # calculate lengths\n        self.lens = [len(d)\/\/self.nb_frames for d in self.data]\n        # calculate offsets\n        self.offsets = np.concatenate(([0], np.cumsum(self.lens[:-1])))\n        \n    \n    def __getitem__(self, index):\n        # subtract offset\n        print('index: {}'.format(index))\n        # get corresponding video file\n        found = False\n        for i, offset in enumerate(self.offsets):\n            if index < offset:\n                print('subtracting {} from index'.format(self.offsets[i-1]))\n                index -= self.offsets[i-1]\n                index *= self.nb_frames\n                found = True\n                break\n        # handle last video separately\n        if not found:\n            index -= self.offsets[-1]\n            index *= self.nb_frames\n            i += 1\n\n        # select correspondind data\n        print('selecing video {}'.format(i-1))\n        data = self.data[i-1]\n        # get frames\n        print('reading frames {}'.format([idx for idx in range(index, index+self.nb_frames)]))\n        x = []\n        for idx in range(index, index+self.nb_frames):\n            tmp = data[idx]\n            if self.transform:\n                tmp = self.transform(tmp)\n            x.append(tmp)\n        x = torch.cat(x)\n        \n        return x\n        \n    def __len__(self):\n        return np.sum(self.lens)\n\n\nvideos = [torch.ones(torch.randint(3, 12, (1,)), 1)*i for i in range(5)]\n\ndataset = MyDataset(videos)\nfor data in dataset:\n    print(data)\n\nloader = DataLoader(\n    dataset,\n    batch_size=2,\n    shuffle=True)\nfor data in loader:\n    print(data)\n<\/code>\nThe code currently uses nb_frames consecutive frames for each video folder and removes the trailing frames.\nShuffling using a DataLoader will work.\nI\u2019ve also tried to add some debug print statement for better understanding, but let me know, if your need more information about this code.\nHi , I was wondering how the concept of ordinary dataloaders fit with this particular Dataset. That is, how the ordinary structure of -\n<code class=\"lang-auto\">train\/  \n   classA\/\n   classB\/\nval\/\n   classA\/\n   classB\/ \ntest\/ \n   classA\/ \n   classB\/ \n<\/code>\nfit with this type of loading. Inside a particular train, test or val folder, do we have to further pass the paths of the subfolders corresponding to each of the classes ? (this was automatically detected in the default approach)\nI would create separate Datasets for the train, val, and test folders, so that you could stick yo your current Dataset implementation and have a clean cut between the data splits to avoid data leakage.\nThanks, that makes much more sense. Should I worry about the labels or will it  be handled automatically ?\nIn my code snippet I\u2019m not handling the labels currently, so you would need to add them for your use case.\nHi  ,I noticed in the source here with link \"https:\/\/pytorch.org\/docs\/stable\/_modules\/torchvision\/datasets\/folder.html#ImageFolder\" that the __getitem__ method of the image folder class returns a sample, target pair where sample is the image and target is the target label. In case of my dataset, how do I handle the case of three images having the same label ?\nThe forward method of my final model looks like this -\n<code class=\"lang-auto\">def forward(self,image1,image2,image3):\n        x1 = self.model1(image1)\n        x2 = self.model2(image2)\n        x3 = self.model3(image3)\n        x4 = torch.cat((x1, x2, x3), dim=1)\n        x5 = self.classifier1(F.relu(x4))\n        x6 = self.classifier2(F.relu(x5))\n        x7 = self.classifier3(F.relu(x6))\n        x8 = self.logSoftmax(x7)\n        #print(x6.shape)\n        return x8\n<\/code>\nIn the train function, the images are fetched like -\n<code class=\"lang-auto\">for inputs, labels in dataloaders[phase]:\n                inputs = inputs.to(device)\n                labels = labels.to(device)\n                .....\n<\/code>\nIf I concatenate the three images in one tensor and pass along a single label, does the model understand it\u2019s actually three images and treat it as such ? I feel a bit lost here.\nI would rather write a custom Dataset and return these three images in your __getitem__ method:\n<code class=\"lang-python\">def __getitem__(self, index):\n    # load the images and your labels according to your code logic\n    ...\n    reutrn image1, image2, image3, target\n    # or concatenate the images and return them as a single one\n    return images, target\n<\/code>\n\n\n\n styx97:\n\ndoes the model understand it\u2019s actually three images and treat it as such ?\n\n\nI\u2019m not sure to understand this question properly.\nYour current code snippet will work, but I\u2019m not sure what your use case is.\nMy use case is that I\u2019m cropping certain portions of frames from a video and storing them in folders (hence the images have to be from the same folder) . Then three images (which are frames of the same video) are passed through three models before the models join at the fc layers.\n\n\n\n ptrblck:\n\nYour current code snippet will work, but I\u2019m not sure what your use case is.\n\n\nI was not sure if it would work, thank you.\n\n\n\n styx97:\n\nThen three images (which are frames of the same video) are passed through three models before the models join at the fc layers.\n\n\nThat should work with your code snippet, however you could alternatively also use a single base model and pass these 3 frames through the same model and concatenate the output afterwards.\nHi ,\nI tried both return x, target  and return x[0],x[1],x[2],target and the error I got was -\n<code class=\"lang-auto\">TypeError                                 Traceback (most recent call last)\n<ipython-input-83-e03efae61dea> in <module>\n----> 1 model, history_model = train_model(model, criterion, optimizer, exp_lr_scheduler, num_epochs=10)\n\n<ipython-input-47-f9a9be299e22> in train_model(model, criterion, optimizer, scheduler, num_epochs)\n     34                 # track history if only in train\n     35                 with torch.set_grad_enabled(phase == 'train'):\n---> 36                     outputs = model(inputs)\n     37                     #print(outputs.shape)\n     38                     _, preds = torch.max(outputs, 1)\n\n\/opt\/conda\/lib\/python3.6\/site-packages\/torch\/nn\/modules\/module.py in __call__(self, *input, **kwargs)\n    491             result = self._slow_forward(*input, **kwargs)\n    492         else:\n--> 493             result = self.forward(*input, **kwargs)\n    494         for hook in self._forward_hooks.values():\n    495             hook_result = hook(self, input, result)\n\nTypeError: forward() missing 2 required positional arguments: 'image2' and 'image3'\n<\/code>\nin both the cases.\nSince your forward definition is defined as:\n<code class=\"lang-python\">def forward(self,image1,image2,image3)\n<\/code>\nyou should pass inputs separately or unwrap it:\n<code class=\"lang-python\">outputs = model(*inputs)\n# or\noutputs = model(inputs[0], inputs[1], inputs[3]) # index in the dimemsion you've concatenated the inputs\n<\/code>\nThanks a lot.\nLet me try that and get back to you.\nFor some reason, the size of inputs in model(*inputs) is getting dependent on the batch size.\nFor a batch size of 16, doing model(*inputs) gives me Expected 3 inputs, got 16. I must have  made a mistake.\nMy code snippet was probably wrong.\nCould you check the shape of inputs and split it in the right dimension?\nE.g. if your inputs have the shape [batch_size, 3, ...] split it in dim1.\nYes the inputs have the shape - torch.Size([16, 3, 224, 224])\nWhat do I put as the split size or sections in torch.split( inputs , split_size_or_sections, dim=1 ) ?\nYou could use the following (depending on the expected shape inside forward):\n<code class=\"lang-python\">outputs = model(*x.split(1, dim=1))  # each input will have shape [16, 1, 224, 224]\noutputs = model(x[:, 0], x[:, 1], x[:, 2])  # each input will have shape [16, 224, 224]\n<\/code>\nUsing the first option, I get the error\n\n<code class=\"lang-auto\">RuntimeError                              Traceback (most recent call last)\n<ipython-input-118-e03efae61dea> in <module>\n----> 1 model, history_model = train_model(model, criterion, optimizer, exp_lr_scheduler, num_epochs=10)\n\n<ipython-input-117-f1d1a61a074d> in train_model(model, criterion, optimizer, scheduler, num_epochs)\n     36                 with torch.set_grad_enabled(phase == 'train'):\n     37                     #outputs = model(inputs[:, 0], inputs[:, 1], inputs[:, 2])\n---> 38                     outputs = model(*inputs.split(1, dim=1))\n     39                     #outputs = model(inputs)\n     40                     #print(outputs.shape)\n\n\/opt\/conda\/lib\/python3.6\/site-packages\/torch\/nn\/modules\/module.py in __call__(self, *input, **kwargs)\n    491             result = self._slow_forward(*input, **kwargs)\n    492         else:\n--> 493             result = self.forward(*input, **kwargs)\n    494         for hook in self._forward_hooks.values():\n    495             hook_result = hook(self, input, result)\n\n<ipython-input-11-f590b21f7c95> in forward(self, image1, image2, image3)\n     11 \n     12     def forward(self,image1,image2,image3):\n---> 13         x1 = self.model1(image1)\n     14         x2 = self.model2(image2)\n     15         x3 = self.model3(image3)\n\n\/opt\/conda\/lib\/python3.6\/site-packages\/torch\/nn\/modules\/module.py in __call__(self, *input, **kwargs)\n    491             result = self._slow_forward(*input, **kwargs)\n    492         else:\n--> 493             result = self.forward(*input, **kwargs)\n    494         for hook in self._forward_hooks.values():\n    495             hook_result = hook(self, input, result)\n\n\/opt\/conda\/lib\/python3.6\/site-packages\/torchvision\/models\/vgg.py in forward(self, x)\n     40 \n     41     def forward(self, x):\n---> 42         x = self.features(x)\n     43         x = self.avgpool(x)\n     44         x = x.view(x.size(0), -1)\n\n\/opt\/conda\/lib\/python3.6\/site-packages\/torch\/nn\/modules\/module.py in __call__(self, *input, **kwargs)\n    491             result = self._slow_forward(*input, **kwargs)\n    492         else:\n--> 493             result = self.forward(*input, **kwargs)\n    494         for hook in self._forward_hooks.values():\n    495             hook_result = hook(self, input, result)\n\n\/opt\/conda\/lib\/python3.6\/site-packages\/torch\/nn\/modules\/container.py in forward(self, input)\n     90     def forward(self, input):\n     91         for module in self._modules.values():\n---> 92             input = module(input)\n     93         return input\n     94 \n\n\/opt\/conda\/lib\/python3.6\/site-packages\/torch\/nn\/modules\/module.py in __call__(self, *input, **kwargs)\n    491             result = self._slow_forward(*input, **kwargs)\n    492         else:\n--> 493             result = self.forward(*input, **kwargs)\n    494         for hook in self._forward_hooks.values():\n    495             hook_result = hook(self, input, result)\n\n\/opt\/conda\/lib\/python3.6\/site-packages\/torch\/nn\/modules\/conv.py in forward(self, input)\n    336                             _pair(0), self.dilation, self.groups)\n    337         return F.conv2d(input, self.weight, self.bias, self.stride,\n--> 338                         self.padding, self.dilation, self.groups)\n    339 \n    340 \n\nRuntimeError: Given groups=1, weight of size 64 3 3 3, expected input[16, 1, 224, 224] to have 3 channels, but got 1 channels instead\n<\/code>\nThis is strange, the torch tensor originally had a size of [16,3,224,224]. Is the problem in the architecture of the model ?\nUsing the second option I get a similar error where it says the expected tensor is 4d and [16, 224, 224] is 3d.\n\u200b"},{"x":"I\u2019ve two models (based on faster_rcnn from torchvision). I want to replace the weights of roi_head in model 2 with that of model 1\u2019s.\nMy dict keys from model 1 has these keys:\n<code class=\"lang-auto\"> 'roi_heads.box_head.fc6.weight'\n 'roi_heads.box_head.fc6.bias'\n 'roi_heads.box_head.fc7.weight'\n 'roi_heads.box_head.fc7.bias'\n 'roi_heads.box_predictor.cls_score.weight'\n 'roi_heads.box_predictor.cls_score.bias'\n 'roi_heads.box_predictor.bbox_pred.weight' \n 'roi_heads.box_predictor.bbox_pred.bias'\n<\/code>\nHere\u2019s my model 1\n<code class=\"lang-auto\">ckpt = torch.load('saved_models\/model1.pth')\nmodel = get_model(15)\nmodel.load_state_dict(ckpt['model'])\n<\/code>\nModel 2 is identical but roi_heads is different.How can I load weights of roi_heads (from model 1) to my model 2\u2019s roi_head ?","y":"In that case something like this should work:\n<code class=\"lang-python\">modelA.roi_head.load_state_dict(modelB.roi_head.state_dict())\n<\/code>\nAlternatively, remove the roi_heads strings from the checkpoint keys and try:\n<code class=\"lang-python\">checkpoint = chpt['model']\n# Remove 'roi_heads'\ncheckpoint_cleaned = ...\nmodel.roi_head.load_state_dict(checkpoint_cleaned)\n<\/code>","z":" Can you please provide some snippet?\nHow would you like to replace the roi_head parameters, if both roi_heads are different?\nAre they just using different names or also different shapes of the parameters?\nBoth roi_head have same key names, it\u2019s just the weights that are different. Even the shapes of in between layers is same. I just want to load the weights of roi_head with that of another model.\nI want to just replace the weights of these :\n'roi_heads.box_head.fc6.weight' 'roi_heads.box_head.fc6.bias' with that of another identical roi_head which will be used for taking predictions.\nIn that case something like this should work:\n<code class=\"lang-python\">modelA.roi_head.load_state_dict(modelB.roi_head.state_dict())\n<\/code>\nAlternatively, remove the roi_heads strings from the checkpoint keys and try:\n<code class=\"lang-python\">checkpoint = chpt['model']\n# Remove 'roi_heads'\ncheckpoint_cleaned = ...\nmodel.roi_head.load_state_dict(checkpoint_cleaned)\n<\/code>"},{"x":"Hi Everyone, I am working with the ImageNet dataset. As a part of the experiments, I want to shift the input images right by 10 pixels (among other affine transforms).\nI can\u2019t use torchvision.transforms.RandomAffine because I want deterministic transformations. So I\u2019m left with torchvision.transforms.functional.affine.\nAny idea how I could plug it in as a part of a pipeline made with transforms.Compose?\nFrom what I see in the codebase, it seems easy enough by wrapping affine inside a class and call it inside __call__. Is this the right way to go about this or are there better ways to do this?","y":"Your suggestion should work. Alternatively you could use transforms.Lambda to warp your functional transformation.","z":"Your suggestion should work. Alternatively you could use transforms.Lambda to warp your functional transformation."},{"x":"I want to extract the feature maps at different layers of a VGG model.\nThe VGG model is implemented as a nested sequence of blocks. Each elementary block is a sequence of layers. For example:\n<code class=\"lang-auto\">block3 = Sequence(\n                  Conv2d(128, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1)),\n                 ReLU(inplace)\n                 Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n                 ReLU(inplace)\n                 Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n                 ReLU(inplace)\n<\/code>\nTo extract the activations in block3 for instance, i used the approach detailed here with link \"https:\/\/discuss.pytorch.org\/t\/how-can-l-load-my-best-model-as-a-feature-extractor-evaluator\/17254\/6\" with get_activation(name) function.\n<code class=\"lang-auto\">for name, module in model._modules.items():\n    if name == 'encoder':\n        for nname, submodule in module._modules.items():\n            if nname == 'block3':\n                for layer_name, layer in submodule._modules.items():\n                    print(nname, layer)\n                    model.encoder.block3.register_forward_hook(get_activation(layer_name))\n\noutput = model(x_input)\nprint(activation.keys())   # dict_keys(['1', '3', '4', '0', '2', '5'])\n<\/code>\nSo far, so good. I can visualize the activations. However, they look exactly the same: when I compute the difference between the output activation of block3.0 and of block3.4, I get 0:\n<code class=\"lang-auto\">block3_0 = activation['0'].numpy()[0]\nblock3_4 = activation[4'].numpy()[0]\nprint(np.sum(block3_0 - block3_4))  # -> 0\n<\/code>\nDid I setup the forward hook properly so it \u201cattached\u201d to different layers?\nThank you.","y":"You are hooking the output of the whole block3 for each activation, I think this is what you want to do:\n<code class=\"lang-auto\">for layer_name, layer in module._modules.items():\n     print(layer_name, layer)\n     model.encoder.block3[int(layer_name)].register_forward_hook(get_activation(layer_name))\n<\/code>","z":"Could you print layer_name inside the loop and check, if the same name is passed to get_activation, thus overwriting the old activations?\nThanks for the reply.\nHere is the result of print(layer_name, layer) inside the last loop:\n<code class=\"lang-auto\">0 Conv2d(256, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n1 ReLU(inplace)\n2 Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n3 ReLU(inplace)\n4 Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n5 ReLU(inplace)\n<\/code>\nThe 5 layers are listed (also shown in the list of keys of activations).\nYou are hooking the output of the whole block3 for each activation, I think this is what you want to do:\n<code class=\"lang-auto\">for layer_name, layer in module._modules.items():\n     print(layer_name, layer)\n     model.encoder.block3[int(layer_name)].register_forward_hook(get_activation(layer_name))\n<\/code>\nGood catch! That should be the reason for the same activations.\n , I replaced the nested loop by the loop that you provided. It prints only 3 layers of block3:\n<code class=\"lang-auto\">0 Conv2d(256, 128, kernel_size=(2, 2), stride=(1, 1))\n1 ReLU(inplace)\n2 Conv2d(128, 21, kernel_size=(1, 1), stride=(1, 1))\n<\/code>\nBut the architecture of the block is:\n<code class=\"lang-auto\">self.block3 = nn.Sequential(nn.Conv2d(128, 256, kernel_size=3, stride=1, padding=1),\n                                    nn.ReLU(inplace=True),\n                                    nn.Conv2d(256, 256, kernel_size=3, stride=1, padding=1),\n                                    nn.ReLU(inplace=True),\n                                    nn.Conv2d(256, 256, kernel_size=3, stride=1, padding=1),\n                                    nn.ReLU(inplace=True))\n<\/code>\nOn the good side, if I compare block3_0 and block3_2, the sum of the difference is > 0.\nHmm that\u2019s weird, I just tried it with the block you provided and that\u2019s what I get:\n<code class=\"lang-auto\">>>> x = torch.rand(1, 128, 64, 64)\n>>> block3 = nn.Sequential(nn.Conv2d(128, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1)),\n...                  nn.ReLU(),\n...                  nn.Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1)),\n...                  nn.ReLU(),\n...                  nn.Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1)),\n...                  nn.ReLU())\n>>> activation = {}\n>>> for layer_name, layer in block3._modules.items():\n...     print(layer_name, layer)\n...     block3[int(layer_name)].register_forward_hook(get_activation(layer_name))\n...\n0 Conv2d(128, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n<torch.utils.hooks.RemovableHandle object at 0x101aa9ba8>\n1 ReLU()\n<torch.utils.hooks.RemovableHandle object at 0x10a1452e8>\n2 Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n<torch.utils.hooks.RemovableHandle object at 0x101aa9ba8>\n3 ReLU()\n<torch.utils.hooks.RemovableHandle object at 0x10a1452e8>\n4 Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n<torch.utils.hooks.RemovableHandle object at 0x101aa9ba8>\n5 ReLU()\n<torch.utils.hooks.RemovableHandle object at 0x10a1452e8>\n>>> block3(x).shape\ntorch.Size([1, 256, 64, 64])\n>>> activation.keys()\ndict_keys(['0', '1', '2', '3', '4', '5'])\n\n<\/code>\nCan you check if this is the code you have now:\n<code class=\"lang-auto\">for name, module in model._modules.items():\n    if name == 'encoder':\n        for nname, submodule in module._modules.items():\n            if nname == 'block3':\n                for layer_name, layer in submodule._modules.items():\n                    print(nname, layer) \n                    model.encoder.block3[int(layer_name)].register_forward_hook(get_activation(layer_name))\n<\/code>\nGreat! It works now. All layers of block3 are printed out (I had to restart the notebook kernel).\nThanks a lot!\n<code class=\"lang-auto\">this_block = model.encoder.block3\nfor layer_name, layer in this_block._modules.items():\n    print(layer_name, layer)\n    model.encoder.block3[int(layer_name)].register_forward_hook(get_activation(layer_name))\n<\/code>"},{"x":"As the title clearly describes, the images in the dataset I use do have 3 color channels despite they are grayscale. So, I used transforms.Grayscale(num_output_channels=1) transformation in order to reduce the number of color channels to 1, but still the loaded images do have 3 channels.\nHere is my implementation:\n<code class=\"lang-auto\">data_transforms = transforms.Compose([\n    transforms.Grayscale(num_output_channels=1),\n    transforms.ToTensor(),\n    transforms.Normalize(mean=[0.485, 0.456, 0.406],\n                         std=[0.229, 0.224, 0.225])\n])\nface_train_dataset = datasets.ImageFolder(root=DEST_PATH_TRAIN, transform=data_transforms)\n\ntrain_loader = DataLoader(face_train_dataset,\n                          batch_size=train_batch_size, shuffle=True, num_workers=4)\n<\/code>","y":"If I run your code, I get the valid error:\n<code class=\"lang-python\">RuntimeError: output with shape [1, 334, 690] doesn't match the broadcast shape [3, 334, 690]\n<\/code>\nwhich is thrown, since Grayscale returns a single-channel image, while Normalize uses three values.\nFixing the mean and std to a single value, yields valid output tensors of shape [batch_size, 1, 334, 690].","z":"If I run your code, I get the valid error:\n<code class=\"lang-python\">RuntimeError: output with shape [1, 334, 690] doesn't match the broadcast shape [3, 334, 690]\n<\/code>\nwhich is thrown, since Grayscale returns a single-channel image, while Normalize uses three values.\nFixing the mean and std to a single value, yields valid output tensors of shape [batch_size, 1, 334, 690].\nNot that I was aware of, thanks . Thought it could still work as it was expected.\nBetween, after decreasing the number of channels of the images, which were already grayscale (8 bits depth images), the accuracy of the network has decreased dramatically. The only change was that. Are there any workarounds or point that I miss? \nAre you sure the depth images are not stored in 16bit?\nAnyway, are you training your model from scratch or finetuning a pretrained model?\nWas the accuracy better when you loaded the depth images as 3-channel images?\nYes, here is the detail of an image (which is a sample image from the FER2013 dataset with link \"https:\/\/www.kaggle.com\/c\/challenges-in-representation-learning-facial-expression-recognition-challenge\/data\"):\n\nI have constructed my model from scratch, now trying to improve its accuracy. Yes, the accuracy was way better when I loaded the images as 3-channels as the accuracy has dropped to about 25% from 50%."},{"x":"i have a mask file saved as a .tiff file ranging from 0\u2026255.\nI would like to split it into X classes so my tensor look something like this [batch, classes, width, height].\nMy network currently does Binary classification and outputs [9, 2, 256, 256].\nSo i would like to match this with the segmentation mask?, and understand how i should do if i wannt to increase the number of classes?","y":"The segmentation mask in a multi-class classification use case should have the shape [batch_size, height, width] and contain the class indices in the range [0, nb_classes-1].\nDoes your target image contain 256 unique classes? If not, you should map each color value to a class index.\nThe output of your model should have the shape [batch_size, nb_classes, height, width].","z":"The segmentation mask in a multi-class classification use case should have the shape [batch_size, height, width] and contain the class indices in the range [0, nb_classes-1].\nDoes your target image contain 256 unique classes? If not, you should map each color value to a class index.\nThe output of your model should have the shape [batch_size, nb_classes, height, width]."},{"x":"I want to backpropagate through input to generate adversarial data. How do I train the model to do this task such that during training time, the image is not altered?","y":"Instead of doing somthing like:\noptimizer = torch.optim.Adam(model.parameters(), lr=learning_rate)\ntry\noptimizer = torch.optim.Adam(inputTensor, lr=learning_rate)\nAnd maybe, as you said, you should make sure requires_grad=True for the input tensor.","z":"I guess you mean you have a first model that outputs what is going to be the input of a second model?\nIf you want to propagate an error from, let\u2019s say, the output of the second model all the way to the first model, I think PyTorch should be doing that for you automatically,along. Of course that is if you are using PyTorch tensors all along and unless you have at some point in between some sort of detachment, or you have something like  with torch.no_grad()\nI\u2019m not sure if I understood your problem, I hope it helps.\nI am trying to train a classifier on the CIFAR Dataset first and then use this trained model to generate adversarial examples.\nAfter training, I want to freeze the weights and pass a new image to the model with a target label and then optimise the input such that the probability of the target class become maximum.\nI am trying to train a classifier on the CIFAR Dataset first and then use this trained model to generate adversarial examples.\nAfter training, I want to freeze the weights and pass a new image to the model with a target label and then optimise the input such that the probability of the target class become maximum.\nWhen you do the forward pass, all the gradients are computed. Not only the gradients for the model weights (which you will not need if you freeze de model), but all the gradients of the tensors that have been involved in computing the loss function (including the input image). So what you have to do is to declare an optimizer (Such as adam, SDG,\u2026) , passing it the input image instead of the model\u2019s weights as parameters to optimize.\nThen when you do \u2018optimizer.step()\u2019, you are optimizing the input image to a version that minimizes the loss function you defined.\nHowever, I still didn\u2019t understand what you meant by \u201cHow do I train the model to do this task such that during training time, the image is not altered?\u201d. The way I explained it, the input image is going to be altered. Isn\u2019t that the whole point of this kind of training?\nSorry for the delay in the answer.\nThank you for the answer. That was exactly what I am trying to do.\nJust one question, how do I pass the image only to the optimizer? Should I make it a parameter or can I just pass the image tensor with requires_grad = True to the optimizer?\nInstead of doing somthing like:\noptimizer = torch.optim.Adam(model.parameters(), lr=learning_rate)\ntry\noptimizer = torch.optim.Adam(inputTensor, lr=learning_rate)\nAnd maybe, as you said, you should make sure requires_grad=True for the input tensor.\nThanks a lot for the help."},{"x":"I have read the 2019 CVPR paper:RePr: Improved Training of Convolutional Filters and want to implement its method. First, we need to train a CNN several iterations. Then we can use a metric algorithm to rank all filters and drop the 30% least important filters to continue to train a few iterations just like pruning. After that, re-initialize the filters  that are dropped before and continue to train the networks. We modify the training process by cyclically removing redundant \ufb01lters, retraining the network, re-initializing the removed \ufb01lters, and repeating.\nIn my opinion, drop the filter  is not really removing it. It just make the weight of that filters don\u2019t work and don\u2019t update. I want to achieve this goal by resetting the weight of dropped filters to 0. For example, I want drop the 23rd filter in first layer,\nparams = list(model.named_parameters()) params[0][1][22] = 0\nBut it has an error when loss.backward():RuntimeError:leaf variable has been moved into graph interior.\nDoes anyone know an official method or some other methods to drop specific filters in some layers? Thanks very much!","y":"One option can be something like this:\n<code class=\"lang-auto\">for key, value in dict(model.named_parameters()).items():\n    if 'the_name_of_the_conv_you_want_to_zero_out' in key:\n        value.data[0, 1, 22] = 0\n<\/code>\nor, another option:\nmodel.the_name_of_the_conv_you_want_to_zero_out.weight.data[0, 1, 22] = 0","z":"One option can be something like this:\n<code class=\"lang-auto\">for key, value in dict(model.named_parameters()).items():\n    if 'the_name_of_the_conv_you_want_to_zero_out' in key:\n        value.data[0, 1, 22] = 0\n<\/code>\nor, another option:\nmodel.the_name_of_the_conv_you_want_to_zero_out.weight.data[0, 1, 22] = 0\nHowever, if you want to prune that network, you have to choose another way.\nHere is the post about how to replace network in running time: https:\/\/find1dream.github.io\/en\/How_to_change_CNN_structure_in_running_time_with_pytorch"},{"x":"I use resnet50 and change fc layer to classifying, like this:\nresnet50.fc = nn.Sequential(\nnn.Linear(fc_inputs, 256),\nnn.ReLU(),\nnn.Dropout(0.4),\nnn.Linear(256, num_classes),\nnn.LogSoftmax(dim=1)\n)\nI want to get both outputs and the features of nn.Linear(fc_inputs, 256) when forwarding, it looks like:\nfeatures, outputs = resnet50(data)\nThanks for all your supporting. (Sorry for my bad English)","y":"I think it would be easier to not use the Sequential API in this case, but rather define each layer in the .__init__() method of your ResNet network, and call them in order in the .forward() method, this way you can have intermediate variables for features and outputs.","z":"I think it would be easier to not use the Sequential API in this case, but rather define each layer in the .__init__() method of your ResNet network, and call them in order in the .forward() method, this way you can have intermediate variables for features and outputs.\nThanks for your suggestion, it work for me."},{"x":"I tired to create a simple dataset (SVHN) using the code below, but it always generates the error :\n\nBrokenPipeError: [Errno 32] Broken pipe\n\nI have no idea why its failing! I used both 1 and 2 workers (num_works) and it doesn\u2019t make any difference.\nHere is the actual code snippet that results in error :\n<code class=\"lang-auto\">class SVHN_Dataset(torch.utils.data.Dataset):\n    def __init__(self, img_size, split='train', label_mask_size = 1000):\n        super().__init__()\n        self.split = split.strip().lower()\n        self.img_size = img_size\n        self.mask_size = label_mask_size\n        trans = transforms.Compose([transforms.Resize(img_size),\n                                    transforms.ToTensor(),\n                                    transforms.Normalize(mean=(0.5,0.5,0.5), std=(0.5,0.5,0.5))])\n        self.dataset = datasets.SVHN(root='.\/SVHN',split=self.split, transform = trans, download=True)\n        self._create_label_mask()\n    \n    def _is_train_split(self):\n        return True if self.split == 'train' else False\n    \n    def _create_label_mask(self):\n        if not self._is_train_split():\n            self.mask = None\n        mask = np.zeros(shape=(len(self.dataset)))    \n        mask[0 : self.mask_size] = 1\n        np.random.shuffle(mask)\n        self.mask = torch.LongTensor(mask)\n        \n\n    def __len__(self):\n        return len(self.dataset) \n    \n    def __getitem__(self, idx):\n        imgs, labels = self.dataset.__getitem__(idx)\n        if self.is_train_split():\n            return imgs, labels, self.mask[idx]\n        return imgs, labels\n\n\n# get data loaders and view a batch of images \ndef get_loaders(batch_size=32, image_size=32, mask_size=1000):\n    data_loader_train = torch.utils.data.DataLoader(SVHN_Dataset(image_size,split='train',label_mask_size=mask_size),\n                                                    batch_size=batch_size,\n                                                    shuffle=True,\n                                                    num_workers = 1)\n    data_loader_test = torch.utils.data.DataLoader(SVHN_Dataset(image_size, split='test'),\n                                                   batch_size=batch_size, num_workers=2)\n    return data_loader_train, data_loader_test\n\ndataloader_train, dataloader_test = get_loaders(batch_size=32, image_size=32, mask_size=1000) \n\ndef visualize(imgs, rows=2, cols=10):\n    fig = plt.figure(figsize=(cols,rows))\n    plt.subplots_adjust(wspace=0,hspace=0)\n    imgs_numpy = imgs.detach().cpu().numpy().transpose(0,2,3,1)\n    for i in range(imgs.shape[0]):\n        ax = fig.add_subplot(rows, cols, i+1, xticks=[], yticks=[])\n        #denormalize \n        #imgs = (imgs\/0.5) + 0.5\n        img = (imgs[i] + imgs[i].min())*255 \/\/ (imgs[i].max() - imgs[i].min())\n        \n        ax.imshow(img)\n\nimgs, labels, masks = next(iter(dataloader_train))\nvisualize(imgs)\n\nimgs, labels = next(iter(dataloader_test))\nvisualize(imgs)\n<\/code>\nThis is the full stack-trace :\n<code class=\"lang-auto\">Using downloaded and verified file: SVHN\\train_32x32.mat\nUsing downloaded and verified file: SVHN\\test_32x32.mat\n---------------------------------------------------------------------------\nBrokenPipeError                           Traceback (most recent call last)\n in ()\n     23         ax.imshow(img)\n     24 \n---> 25 imgs, labels, masks = next(iter(dataloader_train))\n     26 visualize(imgs)\n     27 \n\nC:\\Users\\userx\\Anaconda3\\lib\\site-packages\\torch\\utils\\data\\dataloader.py in __iter__(self)\n    191 \n    192     def __iter__(self):\n--> 193         return _DataLoaderIter(self)\n    194 \n    195     def __len__(self):\n\nC:\\Users\\userx\\Anaconda3\\lib\\site-packages\\torch\\utils\\data\\dataloader.py in __init__(self, loader)\n    467                 #     before it starts, and __del__ tries to join but will get:\n    468                 #     AssertionError: can only join a started process.\n--> 469                 w.start()\n    470                 self.index_queues.append(index_queue)\n    471                 self.workers.append(w)\n\nC:\\Users\\userx\\Anaconda3\\lib\\multiprocessing\\process.py in start(self)\n    103                'daemonic processes are not allowed to have children'\n    104         _cleanup()\n--> 105         self._popen = self._Popen(self)\n    106         self._sentinel = self._popen.sentinel\n    107         # Avoid a refcycle if the target function holds an indirect\n\nC:\\Users\\userx\\Anaconda3\\lib\\multiprocessing\\context.py in _Popen(process_obj)\n    221     \n    222     def _Popen(process_obj):\n--> 223         return _default_context.get_context().Process._Popen(process_obj)\n    224 \n    225 class DefaultContext(BaseContext):\n\nC:\\Users\\userx\\Anaconda3\\lib\\multiprocessing\\context.py in _Popen(process_obj)\n    320         def _Popen(process_obj):\n    321             from .popen_spawn_win32 import Popen\n--> 322             return Popen(process_obj)\n    323 \n    324     class SpawnContext(BaseContext):\n\nC:\\Users\\userx\\Anaconda3\\lib\\multiprocessing\\popen_spawn_win32.py in __init__(self, process_obj)\n     63             try:\n     64                 reduction.dump(prep_data, to_child)\n---> 65                 reduction.dump(process_obj, to_child)\n     66             finally:\n     67                 set_spawning_popen(None)\n\nC:\\Users\\userx\\Anaconda3\\lib\\multiprocessing\\reduction.py in dump(obj, file, protocol)\n     58 def dump(obj, file, protocol=None):\n     59     '''Replacement for pickle.dump() using ForkingPickler.'''\n---> 60     ForkingPickler(file, protocol).dump(obj)\n     61 \n     62 #\n\nBrokenPipeError: [Errno 32] Broken pipe\n[13]\n<\/code>","y":"Setting num_workers=0 actually showed the real problem which simply was :\nAttributeError: 'SVHN_Dataset' object has no attribute 'is_train_split'\nI simply had a misspelling and that was causing the issue. basically as noted here with link \"https:\/\/github.com\/pytorch\/pytorch\/issues\/2341\" and here with link \"https:\/\/discuss.pytorch.org\/t\/brokenpipeerror-errno-32-broken-pipe-when-i-run-cifar10-tutorial-py\/6224\" :\n\nthis error only occurs when you try to do multiprocessing on some code with errors in it.\n\nhope this is useful for somebody out there!","z":"Setting num_workers=0 actually showed the real problem which simply was :\nAttributeError: 'SVHN_Dataset' object has no attribute 'is_train_split'\nI simply had a misspelling and that was causing the issue. basically as noted here with link \"https:\/\/github.com\/pytorch\/pytorch\/issues\/2341\" and here with link \"https:\/\/discuss.pytorch.org\/t\/brokenpipeerror-errno-32-broken-pipe-when-i-run-cifar10-tutorial-py\/6224\" :\n\nthis error only occurs when you try to do multiprocessing on some code with errors in it.\n\nhope this is useful for somebody out there!"},{"x":"I am trying to use NasNet to do feature extraction for an OCR use case.\nHowever after like 2300 steps I have the following error:\n\n\u2026\nFile \u201cC:\\Users\\tryck\\Documents\\OCR_from_scratch\\model_custom\\NasNet_layers.py\u201d, line 159, in forward\ncomb_iter_1 = comb_iter_1_left + comb_iter_1_right\nRuntimeError: The size of tensor a (16) must match the size of tensor b (9) at non-singleton dimension 0\n\nMy code is deterministic I set a seed and deactivate non deterministic cuda calculation.\nThe error is always the same but do not always appear at the same step.\nWhat should I do to be able to solve this issue ?","y":"Hi, there are several approaches that I can think of.\n\nAssign each data item with a unique identifier (e.g. image path) and log that when the error occurs. Perhaps this datapoint is a bit funky?\nBuilt a testcase for your dataloader where you loop through all the data and assert that it is the correct shape, datatype etc\u2026\nMake your code saveable\/resumable so that you don\u2019t have to redo all the 2000 steps before debugging, but maybe just 100.\n\nIn your case it seems like comb_iter_1_left has the batch size of 16 whilst the right one has 9. As to why is impossible to say for me but perhaps your dataset isn\u2019t evenly dividable with 16 so the last batch is just 9?\nEdit: What steps are you taking to make sure that your program is deterministic?","z":"Hi, there are several approaches that I can think of.\n\nAssign each data item with a unique identifier (e.g. image path) and log that when the error occurs. Perhaps this datapoint is a bit funky?\nBuilt a testcase for your dataloader where you loop through all the data and assert that it is the correct shape, datatype etc\u2026\nMake your code saveable\/resumable so that you don\u2019t have to redo all the 2000 steps before debugging, but maybe just 100.\n\nIn your case it seems like comb_iter_1_left has the batch size of 16 whilst the right one has 9. As to why is impossible to say for me but perhaps your dataset isn\u2019t evenly dividable with 16 so the last batch is just 9?\nEdit: What steps are you taking to make sure that your program is deterministic?\nTo assert that the code is deterministic I set the following values, to arbitrary values:\n\nrandom.seed(opt.manualSeed)\nnp.random.seed(opt.manualSeed)\ntorch.manual_seed(opt.manualSeed)\ntorch.cuda.manual_seed(opt.manualSeed)\n\ncudnn.benchmark = True\ncudnn.deterministic = True\n\nOk, that looks good. But change the benchmark to False in accordance to the docs with link \"https:\/\/pytorch.org\/docs\/stable\/notes\/randomness.html\". Any thoughts to my reply?\nI have checked and my dataset is not divisible by 16. So I modified it in order to. I changed the cudnn.benchmark value to false as given by the linked doc.\nThanks for your help"},{"x":"I am trying to move the image to the GPU during transforms.Compose but I am getting the error\n<code class=\"lang-auto\">transforms.Lambda(lambda x: x.to(device)),\nRuntimeError: CUDA error: initialization error\n<\/code>\nI need to do this as I am using one of the trained models during Compose.transformation; something like:\n<code class=\"lang-auto\">device = torch.device('cuda') \ntransform = transforms.Compose([\n        transforms.Resize([256, 256]),       \n        transforms.ToTensor(),         \n        transforms.Lambda(lambda x: x.to(device)),\n        transforms.Lambda(lambda x: my_model(x.unsqueeze(0)).squeeze(0) )\n        ])\n<\/code>\nAny ideas of how can I resolve this issue?\nThanks in advance","y":"If you are using multiple workers in your DataLoader, multiple initializations of the CUDA context might yield this error.\nTry to use the spawn method for multiprocessing as described here with link \"https:\/\/pytorch.org\/docs\/stable\/notes\/multiprocessing.html#sharing-cuda-tensors\" or alternatively use a single worker (num_workers=0).","z":"If you are using multiple workers in your DataLoader, multiple initializations of the CUDA context might yield this error.\nTry to use the spawn method for multiprocessing as described here with link \"https:\/\/pytorch.org\/docs\/stable\/notes\/multiprocessing.html#sharing-cuda-tensors\" or alternatively use a single worker (num_workers=0)."},{"x":"The current load_state_dict API with strict=False returns an IncompatibleKeys named tuple which is misleading in the case of successful loading of weights. I don\u2019t think IncompatibleKeys is the correct name for a return object for loading the state dict. I was thrown off for a while trying to figure out why I was repeatedly getting the same IncompatibleKeys error with strict=False before realizing that was actually the return object itself.","y":"This issue was tracked here with link \"https:\/\/github.com\/pytorch\/pytorch\/issues\/20128\" and should be fixed in the current master or nightly build by now.\nCould you check it again with one of the current versions?","z":"This issue was tracked here with link \"https:\/\/github.com\/pytorch\/pytorch\/issues\/20128\" and should be fixed in the current master or nightly build by now.\nCould you check it again with one of the current versions?"},{"x":"I\u2019m practicing a semantic segmentation task.\nAs there are two class for each pixel, the output of the model is of shape(1, 2, 512, 512). See there are two channels, but i need to convert it into shape(1, 1, 512, 512), to match the shape of my label. Wondering what method should i take to complete this transformation?","y":"Hi,\n\nThis line worked for me.\n<code class=\"lang-python\">\n_, pred = torch.max(scores, dim=1)\n<\/code>\nhttps:\/\/pytorch.org\/docs\/stable\/torch.html#torch.max\nWhere scores is your ultimate tensor containing probs.\nFor instance, scores tensor has size of [10, 150, 256, 256] which means I have 150 classes to segment and using above code give me [10, 256, 256] tensor. Then you can .unsqueeze(1) to get your desired dimension.\nPS: torch.max return a tuple which the second value is the target tensor.","z":"Hello,\nI think shape of output of the network is depending on which loss function employed.\nIn binary classification, you can use both CELoss and BCELoss.\nFor CELoss, the shape of the output should be [batch_size, nb_classes, H, W] then produce a probability map by softmax.\nFor BCELoss, the shape should be [batch_size, H, W] which is the same to label, and it should incorperate with sigmoid.\nNote: CELoss has contain logsoftmax, so you could only pass the model output and label.\nBCELossWithLogits = BCELoss+sigmoid\nfor more details with link \"https:\/\/pytorch.org\/docs\/master\/nn.html#torch.nn.CrossEntropyLoss\"\nHow to get  the ultimate output tensor ,i.e. (1, 2, 512, 512) to transform  into the ground truth shape (1, 1, 512, 512)? I know how to use CELoss and BCELoss\nHave you solved the problem?\nHi,\n\nThis line worked for me.\n<code class=\"lang-python\">\n_, pred = torch.max(scores, dim=1)\n<\/code>\nhttps:\/\/pytorch.org\/docs\/stable\/torch.html#torch.max\nWhere scores is your ultimate tensor containing probs.\nFor instance, scores tensor has size of [10, 150, 256, 256] which means I have 150 classes to segment and using above code give me [10, 256, 256] tensor. Then you can .unsqueeze(1) to get your desired dimension.\nPS: torch.max return a tuple which the second value is the target tensor.\nthank you very much!"},{"x":"Hi team,\nI am using MTCNN in pytorch, and it looks like pure non-max suppression implementation in pytorch(cuda) is waaay slower than numpy implementation on cpu. I ended up running just the nms part on cpu to get decent frame rates. Could someone please correct any obvious mistakes, here is the code,\n<code class=\"lang-auto\">def nms(boxes, _keep, overlap_threshold=0.5, min_mode=False):\n    x1 = boxes[:, 0]\n    y1 = boxes[:, 1]\n    x2 = boxes[:, 2]\n    y2 = boxes[:, 3]\n    scores = boxes[:, 4]\n\n    areas = (x2 - x1 + 1) * (y2 - y1 + 1)\n    _, order = scores.sort(dim=0, descending=True)\n    cnt = 0\n\n    while order.size()[0] > 1 and cnt < _keep.shape[0]:\n        _keep[cnt] = order[0]\n        cnt += 1\n        xx1 = torch.max(x1[order[0]], x1[order[1:]])\n        yy1 = torch.max(y1[order[0]], y1[order[1:]])\n        xx2 = torch.min(x2[order[0]], x2[order[1:]])\n        yy2 = torch.min(y2[order[0]], y2[order[1:]])\n\n        w = torch.clamp(xx2-xx1, min=0)\n        h = torch.clamp(yy2-yy1, min=0)\n        inter = w * h\n        if min_mode:\n            ovr = inter \/ torch.min(areas[order[0]], areas[order[1:]])\n        else:\n            ovr = inter \/ (areas[order[0]] + areas[order[1:]] - inter)\n\n        inds = torch.nonzero(ovr <= overlap_threshold).squeeze()\n        if inds.dim():\n            order = order[inds + 1]\n        else:\n            break\n\n    return _keep[:cnt]\n<\/code>\nAnd here\u2019s the same in numpy\n<code class=\"lang-auto\">def nms_cpu(boxes,  overlap_threshold=0.5, min_mode=False):\n    boxes = boxes.cpu().numpy()\n    x1 = boxes[:, 0]\n    y1 = boxes[:, 1]\n    x2 = boxes[:, 2]\n    y2 = boxes[:, 3]\n    scores = boxes[:, 4]\n\n    areas = (x2 - x1 + 1) * (y2 - y1 + 1)\n    order = scores.argsort()[::-1]\n\n    keep = []\n    while order.size > 0:\n        keep.append(order[0])\n        xx1 = np.maximum(x1[order[0]], x1[order[1:]])\n        yy1 = np.maximum(y1[order[0]], y1[order[1:]])\n        xx2 = np.minimum(x2[order[0]], x2[order[1:]])\n        yy2 = np.minimum(y2[order[0]], y2[order[1:]])\n\n        w = np.maximum(0.0, xx2 - xx1 + 1)\n        h = np.maximum(0.0, yy2 - yy1 + 1)\n        inter = w * h\n\n        if min_mode:\n            ovr = inter \/ np.minimum(areas[order[0]], areas[order[1:]])\n        else:\n            ovr = inter \/ (areas[order[0]] + areas[order[1:]] - inter)\n\n        inds = np.where(ovr <= overlap_threshold)[0]\n        order = order[inds + 1]\n    return keep\n<\/code>\nHere is the output of profiling for pytorch\n<code class=\"lang-auto\">Total time: 55.6077 s\nFile: utils\/box_utils.py\nFunction: nms at line 4\n\nLine #      Hits         Time  Per Hit   % Time  Line Contents\n==============================================================\n     4                                           \n     5                                           def nms(boxes, _keep, overlap_threshold=0.5, min_mode=False):\n     6       506       3933.0      7.8      0.0      x1 = boxes[:, 0]\n     7       506       2744.0      5.4      0.0      y1 = boxes[:, 1]\n     8       506       2433.0      4.8      0.0      x2 = boxes[:, 2]\n     9       506       2558.0      5.1      0.0      y2 = boxes[:, 3]\n    10       506       2363.0      4.7      0.0      scores = boxes[:, 4]\n    11                                           \n    12       506      38317.0     75.7      0.1      areas = (x2 - x1 + 1) * (y2 - y1 + 1)\n    13       506      19902.0     39.3      0.0      _, order = scores.sort(dim=0, descending=True)\n    14       506        327.0      0.6      0.0      cnt = 0\n    15                                           \n    16     39420     132472.0      3.4      0.2      while order.size()[0] > 1 and cnt < _keep.shape[0]:\n    17     39296     984623.0     25.1      1.8          _keep[cnt] = order[0]\n    18     39296      31360.0      0.8      0.1          cnt += 1\n    19     39296    8083133.0    205.7     14.5          xx1 = torch.max(x1[order[0]], x1[order[1:]])\n    20     39296    7976850.0    203.0     14.3          yy1 = torch.max(y1[order[0]], y1[order[1:]])\n    21     39296    7997772.0    203.5     14.4          xx2 = torch.min(x2[order[0]], x2[order[1:]])\n    22     39296    7949390.0    202.3     14.3          yy2 = torch.min(y2[order[0]], y2[order[1:]])\n    23                                           \n    24     39296    1270632.0     32.3      2.3          w = torch.clamp(xx2-xx1, min=0)\n    25     39296    1048029.0     26.7      1.9          h = torch.clamp(yy2-yy1, min=0)\n    26                                                   # print (w, h)\n    27     39296     662499.0     16.9      1.2          inter = w * h\n    28     39296      23280.0      0.6      0.0          if min_mode:\n    29        58      13687.0    236.0      0.0              ovr = inter \/ torch.min(areas[order[0]], areas[order[1:]])\n    30                                                   else:\n    31     39238    9429904.0    240.3     17.0              ovr = inter \/ (areas[order[0]] + areas[order[1:]] - inter)\n    32                                           \n    33     39296    3483842.0     88.7      6.3          inds = torch.nonzero(ovr <= overlap_threshold).squeeze()\n    34     39296      54212.0      1.4      0.1          if inds.dim():\n    35     38914    6390010.0    164.2     11.5              order = order[inds + 1]\n    36                                                   else:\n    37       382        267.0      0.7      0.0              break\n    38                                           \n    39       506       3197.0      6.3      0.0      return _keep[:cnt]\n<\/code>\nsame for numpy\n<code class=\"lang-auto\">Total time: 13.3619 s\nFile: utils\/box_utils.py\nFunction: nms_cpu at line 40\n\nLine #      Hits         Time  Per Hit   % Time  Line Contents\n==============================================================\n    40                                           \n    41                                           def nms_cpu(boxes, _keep, overlap_threshold=0.5, min_mode=False):\n    42                                               # This is run on CPU (numpy) as it runs slower on GPU\n    43      4895     255245.0     52.1      1.9      boxes = boxes.cpu().numpy()\n    44      4895      15433.0      3.2      0.1      x1 = boxes[:, 0]\n    45      4895       4332.0      0.9      0.0      y1 = boxes[:, 1]\n    46      4895       3376.0      0.7      0.0      x2 = boxes[:, 2]\n    47      4895       3341.0      0.7      0.0      y2 = boxes[:, 3]\n    48      4895       3260.0      0.7      0.0      scores = boxes[:, 4]\n    49                                           \n    50      4895     112262.0     22.9      0.8      areas = (x2 - x1 + 1) * (y2 - y1 + 1)\n    51      4895      55061.0     11.2      0.4      order = scores.argsort()[::-1]\n    52                                           \n    53      4895       3019.0      0.6      0.0      keep = []\n    54    375565     226824.0      0.6      1.7      while order.size > 0:\n    55    370670     272610.0      0.7      2.0          keep.append(order[0])\n    56    370670    1394733.0      3.8     10.4          xx1 = np.maximum(x1[order[0]], x1[order[1:]])\n    57    370670    1234165.0      3.3      9.2          yy1 = np.maximum(y1[order[0]], y1[order[1:]])\n    58    370670    1222226.0      3.3      9.1          xx2 = np.minimum(x2[order[0]], x2[order[1:]])\n    59    370670    1209276.0      3.3      9.1          yy2 = np.minimum(y2[order[0]], y2[order[1:]])\n    60                                           \n    61    370670    1558863.0      4.2     11.7          w = np.maximum(0.0, xx2 - xx1 + 1)\n    62    370670    1438743.0      3.9     10.8          h = np.maximum(0.0, yy2 - yy1 + 1)\n    63    370670     442606.0      1.2      3.3          inter = w * h\n    64                                           \n    65    370670     169062.0      0.5      1.3          if min_mode:\n    66       642       3642.0      5.7      0.0              ovr = inter \/ np.minimum(areas[order[0]], areas[order[1:]])\n    67                                                   else:\n    68    370028    1811625.0      4.9     13.6              ovr = inter \/ (areas[order[0]] + areas[order[1:]] - inter)\n    69                                           \n    70    370670    1086337.0      2.9      8.1          inds = np.where(ovr <= overlap_threshold)[0]\n    71    370670     833359.0      2.2      6.2          order = order[inds + 1]\n    72      4895       2533.0      0.5      0.0      return keep\n<\/code>\nBoth of them were profiled for 60s. Pytorch NMS took 55s as opposed to numpy\u2019s mere 13s.\nThank you so much for your time!","y":"NMS is one of those operations where it is common to write custom kernels because, as you note, implementing this in PyTorch directly is not so fast.\nIn the MaskRCNN-Benchmark-Implementation there is are CPU and GPU kernels for NMS, e.g.:\n\n\ngithub.com with link \"https:\/\/github.com\/facebookresearch\/maskrcnn-benchmark\/blob\/master\/maskrcnn_benchmark\/csrc\/cpu\/nms_cpu.cpp\"\n\n\nfacebookresearch\/maskrcnn-benchmark\/blob\/master\/maskrcnn_benchmark\/csrc\/cpu\/nms_cpu.cpp with link \"https:\/\/github.com\/facebookresearch\/maskrcnn-benchmark\/blob\/master\/maskrcnn_benchmark\/csrc\/cpu\/nms_cpu.cpp\"\n<code class=\"lang-cpp\">\/\/ Copyright (c) Facebook, Inc. and its affiliates. All Rights Reserved.\n#include \"cpu\/vision.h\"\n\n\ntemplate <typename scalar_t>\nat::Tensor nms_cpu_kernel(const at::Tensor&amp; dets,\n                          const at::Tensor&amp; scores,\n                          const float threshold) {\n  AT_ASSERTM(!dets.type().is_cuda(), \"dets must be a CPU tensor\");\n  AT_ASSERTM(!scores.type().is_cuda(), \"scores must be a CPU tensor\");\n  AT_ASSERTM(dets.type() == scores.type(), \"dets should have the same type as scores\");\n\n  if (dets.numel() == 0) {\n    return at::empty({0}, dets.options().dtype(at::kLong).device(at::kCPU));\n  }\n\n  auto x1_t = dets.select(1, 0).contiguous();\n  auto y1_t = dets.select(1, 1).contiguous();\n  auto x2_t = dets.select(1, 2).contiguous();\n  auto y2_t = dets.select(1, 3).contiguous();\n<\/code>\n\n  This file has been truncated. show original with link \"https:\/\/github.com\/facebookresearch\/maskrcnn-benchmark\/blob\/master\/maskrcnn_benchmark\/csrc\/cpu\/nms_cpu.cpp\"\n\n\n\n\n\n\nBest regards\nThomas","z":"How did you install PyTorch and NumPy? Are you using conda? I also observe that NumPy is faster on one of my machines (actually, my laptop where I use conda) whereas it isn\u2019t when I compiled it from source (on my work station). I think it has sth to do with MKL and the intel-specific optimizations conda (and\/or intel) does for the conda NumPy package:\nhttps:\/\/docs.anaconda.com\/mkl-optimizations\/\nI installed both of them using pip install \u2026 . My laptop is on Cuda 9.0 and pytorch 0.4.1 if that makes a difference.\nIt baffles me that pytorch on cuda is slower that numpy on cpu.\nI see. I assumed you were comparing both on the CPU for some reason. Regarding GPU, have you subtracted the time it takes to transfer data from main memory to the GPU?\nPlus, your operations don\u2019t seem to be well suited for taking advantage of GPUs. You want to compare things like matrix multiplications etc. where you have parallelism and is based on CUDA and cuDNN stuff more heavily.\n\nRegarding GPU, have you subtracted the time it takes to transfer data from main memory to the GPU?\n\nYes, 13seconds include that\n\nPlus, your operations don\u2019t seem to be well suited for taking advantage of GPUs\n\nOh, if that\u2019s the case I\u2019ll have to stick with numpy for this bit then. Is there a better implementation of NMS in Pytorch, I know a cuda NMS is being worked on, but anything else that I can use in the meantime?\nGPU is optimized for large matrix multiplication, so it might be not too surprising if it\u2019s slower in other scenarios? \nPS: that\u2019s why loop hits like crazy, that might be why, can you batch it or something\nNMS is one of those operations where it is common to write custom kernels because, as you note, implementing this in PyTorch directly is not so fast.\nIn the MaskRCNN-Benchmark-Implementation there is are CPU and GPU kernels for NMS, e.g.:\n\n\ngithub.com with link \"https:\/\/github.com\/facebookresearch\/maskrcnn-benchmark\/blob\/master\/maskrcnn_benchmark\/csrc\/cpu\/nms_cpu.cpp\"\n\n\nfacebookresearch\/maskrcnn-benchmark\/blob\/master\/maskrcnn_benchmark\/csrc\/cpu\/nms_cpu.cpp with link \"https:\/\/github.com\/facebookresearch\/maskrcnn-benchmark\/blob\/master\/maskrcnn_benchmark\/csrc\/cpu\/nms_cpu.cpp\"\n<code class=\"lang-cpp\">\/\/ Copyright (c) Facebook, Inc. and its affiliates. All Rights Reserved.\n#include \"cpu\/vision.h\"\n\n\ntemplate <typename scalar_t>\nat::Tensor nms_cpu_kernel(const at::Tensor&amp; dets,\n                          const at::Tensor&amp; scores,\n                          const float threshold) {\n  AT_ASSERTM(!dets.type().is_cuda(), \"dets must be a CPU tensor\");\n  AT_ASSERTM(!scores.type().is_cuda(), \"scores must be a CPU tensor\");\n  AT_ASSERTM(dets.type() == scores.type(), \"dets should have the same type as scores\");\n\n  if (dets.numel() == 0) {\n    return at::empty({0}, dets.options().dtype(at::kLong).device(at::kCPU));\n  }\n\n  auto x1_t = dets.select(1, 0).contiguous();\n  auto y1_t = dets.select(1, 1).contiguous();\n  auto x2_t = dets.select(1, 2).contiguous();\n  auto y2_t = dets.select(1, 3).contiguous();\n<\/code>\n\n  This file has been truncated. show original with link \"https:\/\/github.com\/facebookresearch\/maskrcnn-benchmark\/blob\/master\/maskrcnn_benchmark\/csrc\/cpu\/nms_cpu.cpp\"\n\n\n\n\n\n\nBest regards\nThomas\nThanks , I\u2019ll check that out\nHi, have you compared the GPU time spent on NMS in caffe and pytorch? I recently transfer a model from caffe to libtorch. And I found that the main time spent by libtorch is concentrated on the NMS operation. the time is 160ms, which takes 80% of all forward time. but the overall time consumption of caffe is only 150ms!\nThe nms implementation I used is copying from torchvision0.3, and the maskrcnn-benchmark is also around 160ms. so Is there an operation that can continue to optimize nms? thank you\nSee here for continuation:\n\n\n\n\nNms gpu cost time so more than caffe with link \"https:\/\/discuss.pytorch.org\/t\/nms-gpu-cost-time-so-more-than-caffe\/51620\/2\" vision with link \"\/c\/vision\"\n\n\n    How did you post this twice? \nI thought that the PyTorch NMS was ultimately derived from the (caffe2) detectron, so it seems strange that it should be that much slower. \nYou would have to show your benchmarking code before I would try to find out why it is different. \nBest regards \nThomas\n  \n\n"},{"x":"when i work on tutorial object detection and implementing this multiplication, i found it gives this errorScreenshot_2019-07-25 pytorch Object detection Kaggle.png1299\u00d7241 15.1 KB with link \"https:\/\/discuss.pytorch.org\/uploads\/default\/original\/2X\/0\/0b74a763cce2607c0e94c2905a69047b8e46f596.png\"","y":"int16 can only store values from -32768 to 32767 (2^15 bits + one for the sign). 35358 overflows the upper limit and the result gets negative.","z":"Hi,\nWhat error? There is no error in the screenshot.\nit gives a negative number,  although it is an area and can\u2019t be negative\nScreenshot_2019-07-25 pytorch Object detection Kaggle(1).png1299\u00d7506 25.1 KB with link \"https:\/\/discuss.pytorch.org\/uploads\/default\/original\/2X\/3\/3933d64e5ccb3332f13e11d9c25259925f876c6c.png\"\nint16 can only store values from -32768 to 32767 (2^15 bits + one for the sign). 35358 overflows the upper limit and the result gets negative.\ni made it int32, and it had solved the problem. thanks alot"},{"x":"Hi,\nImplementing an autoregressor in Keras is something like the following code snippet.\n<code class=\"lang-auto\">def network_autoregressive(x):\n\n    ''' Define the network that integrates information along the sequence '''\n\n    x = keras.layers.GRU(units=256, return_sequences=False, name='ar_context')(x)\n\n\n    return x\n<\/code>\nHere the input x is 32x4x128. And the output is 32x256. I am not able to produce a similar behaviour using pytorch GRU. Any help is much appreciated.\nThanks\nRegards\nPranavan","y":"Thanks for the people who read this topic. I found the solution. In keras, return_sequences=False actually returns the final output in the sequence dimension. Hence, it can be sliced from the GRU output of pytorch.","z":"Thanks for the people who read this topic. I found the solution. In keras, return_sequences=False actually returns the final output in the sequence dimension. Hence, it can be sliced from the GRU output of pytorch."},{"x":"Hi, have anyone compared the GPU time spent on NMS in caffe and pytorch? I recently transfer a model from caffe to libtorch. And I found that the main time spent by libtorch is concentrated on the NMS operation. the time is 160ms, which takes 80% of all forward time. but the overall time consumption of caffe is only 150ms!\nThe nms implementation I used is copying from torchvision0.3, and the maskrcnn-benchmark is also around 160ms. so Is there an operation that can continue to optimize nms? thank you","y":"Yeah, well, the usual way to do these things is to grab an input and try to measure the function in isolation.\nBenchmarking has quite a few pitfalls, in particular with CUDA asynchronous computation involved, so it\u2019s hard to say whether you found something where PyTorch is indeed terribly slow or whether you\u2019ve just screwed up your benchmarking (I have certainly done that before, too).\nBest regards\nThomas","z":"How did you post this twice?\nI thought that the PyTorch NMS was ultimately derived from the (caffe2) detectron, so it seems strange that it should be that much slower.\nYou would have to show your benchmarking code before I would try to find out why it is different.\nBest regards\nThomas\nHi,Tommas. I also want to post it, but unfortunately I am doing pvanet\u2019s libtorch implementation, nms is only a small part of it. it may not be possible to upload the entire project. I just used the c++ chrono library to record the time spent in the nms function and compare it with the caffe version of pvanet. sorry for can\u2019t provide you with more information. \nYeah, well, the usual way to do these things is to grab an input and try to measure the function in isolation.\nBenchmarking has quite a few pitfalls, in particular with CUDA asynchronous computation involved, so it\u2019s hard to say whether you found something where PyTorch is indeed terribly slow or whether you\u2019ve just screwed up your benchmarking (I have certainly done that before, too).\nBest regards\nThomas\nok,I understand, Thanks a lot!\nHi,tom. this is nms code that i copy from torchvision 0.3.\nI found nms_kernel function is very fast, it\u2019s only cost 0.01ms, but this move data operation\n\nat::Tensor mask_cpu = mask.to(at::kCPU);\n\nfrom gpu to cpu is very slow, because it spent 222.432ms. can i do something to reduce time consumption?\n<code class=\"lang-auto\">\/\/ boxes is a N x 5 tensor\nat::Tensor nms_cuda(const at::Tensor boxes, float nms_overlap_thresh) {\n  using scalar_t = float;\n  AT_ASSERTM(boxes.type().is_cuda(), \"boxes must be a CUDA tensor\");\n  at::cuda::CUDAGuard device_guard(boxes.device());\n\n  auto scores = boxes.select(1, 4);\n  auto order_t = std::get<1>(scores.sort(0, \/* descending=*\/true));\n  auto boxes_sorted = boxes.index_select(0, order_t);\n\n  int boxes_num = boxes.size(0);\n\n  const int col_blocks = at::cuda::ATenCeilDiv(boxes_num, threadsPerBlock);\n\n  at::Tensor mask =\n      at::empty({boxes_num * col_blocks}, boxes.options().dtype(at::kLong));\n\n  dim3 blocks(col_blocks, col_blocks);\n  dim3 threads(threadsPerBlock);\n  cudaStream_t stream = at::cuda::getCurrentCUDAStream();\n\n  AT_DISPATCH_FLOATING_TYPES_AND_HALF(\n      boxes_sorted.type(), \"nms_kernel_cuda\", [&amp;] {\n        nms_kernel<scalar_t><<<blocks, threads, 0, stream>>>(\n            boxes_num,\n            nms_overlap_thresh,\n            boxes_sorted.data<scalar_t>(),\n            (unsigned long long*)mask.data<int64_t>());\n      });\n\n  at::Tensor mask_cpu = mask.to(at::kCPU);\n  unsigned long long* mask_host = (unsigned long long*)mask_cpu.data<int64_t>();\n\n  std::vector<unsigned long long> remv(col_blocks);\n  memset(&amp;remv[0], 0, sizeof(unsigned long long) * col_blocks);\n\n  at::Tensor keep =\n      at::empty({boxes_num}, boxes.options().dtype(at::kLong).device(at::kCPU));\n  int64_t* keep_out = keep.data<int64_t>();\n\n  int num_to_keep = 0;\n  for (int i = 0; i < boxes_num; i++) {\n    int nblock = i \/ threadsPerBlock;\n    int inblock = i % threadsPerBlock;\n\n    if (!(remv[nblock] &amp; (1ULL << inblock))) {\n      keep_out[num_to_keep++] = i;\n      unsigned long long* p = mask_host + i * col_blocks;\n      for (int j = nblock; j < col_blocks; j++) {\n        remv[j] |= p[j];\n      }\n    }\n  }\n\n  AT_CUDA_CHECK(cudaGetLastError());\n  return\n      order_t\n          .index({keep.narrow(\/*dim=*\/0, \/*start=*\/0, \/*length=*\/num_to_keep)\n                      .to(order_t.device(), keep.scalar_type())});\n}\n\n<\/code>\nDid you use torch.cuda.synchronize() before timing starts and ends?\nBest regards\nThomas\nhow can i do it in libtorch?\n\n\n\n\nAny analog of torch.cuda.syncronize() in C++ API? with link \"https:\/\/discuss.pytorch.org\/t\/any-analog-of-torch-cuda-syncronize-in-c-api\/36763\/2\" C++ with link \"\/c\/c\"\n\n\n    Hi, \nYou can call directly the cudaDeviceSynchronize() method as is done by the python code here with link \"https:\/\/github.com\/pytorch\/pytorch\/blob\/202eaa4ef40824d08ae88b3062b9c82d78794432\/torch\/csrc\/cuda\/Module.cpp#L212-L218\".\n  \n\n"},{"x":"I\u2019m new to Pytorch, I feel puzzled when I run the code:\nfrom torch.nn.init import zeros_\nand use it to initialize some veriables:\nzeros_(model.bias)\nand I got an error:  ImportError: cannot import name \u2018zeros_\u2019 \nwhere torch.version is 0.4.0","y":"Hi,\nThanks for your reply!\nI will try to use the approach to init with constant values.\nI solve this problem by updating the torch version.\nwhen the torch version is 0.4.0\n<code class=\"lang-auto\">import torch\ndir(torch.nn.init)\n<\/code>\nand can not find \u2018zeros_\u2019 in the result.  So I updata the torch version(0.4.1).\nThanks ! ","z":"Hi,\nI ran your code, and I did not get any error.\nIf you want to init with constant values, here is a better approach:\n<code class=\"lang-python\">torch.nn.init.constant_(tensor, 0)\n<\/code>\nBests\nNik\nHi,\nThanks for your reply!\nI will try to use the approach to init with constant values.\nI solve this problem by updating the torch version.\nwhen the torch version is 0.4.0\n<code class=\"lang-auto\">import torch\ndir(torch.nn.init)\n<\/code>\nand can not find \u2018zeros_\u2019 in the result.  So I updata the torch version(0.4.1).\nThanks ! \nYou are welcome.\nI think it is great idea to use latest version. There are a lot of optimization and development now. Furthermore, your code will be usable for different purposes.\nGood luck"},{"x":"Hello every one.\nI am working on video processing and as data samples, i need short video clips (around 10 frame per each).\nI have a dataset, composed of consecutive images, and what i want to do is combining consecutive 10 frames to make a cuboid. Now the problem is: for big datasets or even for multiple normal datasets, when i create numpy arrays containing plenty of cuboids (10 * 227* 227 : 10 frames of size 227*227), the Computer RAM gets fully occupied and kernel dies!!\nMaybe my algorithm has a bad mistake and i am unaware of a simple way, but i would be haapy if anyone give me useful hints to manage my RAM\nThank you very much","y":"Hi,\nYou should not load whole datasets into your RAM if you do not have enough RAM to do that. You should use lazy loading provided in DataLoaders in PyTorch. As you mentioned, You have to prepare next data when your GPU is processing.\nBy the way, This is best and most efficient solution to task such this regarding huge datasets.","z":"The storage of 10 frames of shape 227x227 shouldn\u2019t cause any problems itself.\nAssuming that you are dealing with RGB images, you would only use approx. 10*227*227*3*4 \/ 1024**2 = 5.9MB, if you store the images in FP32.\nDo you run out of memory directly after loading the data or could some other code part (e.g. model forward\/backward) cause this issue?\nThank you very much   for your reply.\nFirst, there are plenty of these cuboids  ( 4 datasets such as UCSD, AVenue,\u2026). Even considering the possible augmentations, it gets even higher.\nyes in fact i read, process the datasets first, to make cuboid and it makes me run out of memory even before  training! (whole 32GB RAM)\nsome times i think about loading dataset while training ( not pre-loading the all dataset at once) but i think it makes the training so slow ( because GPU should wait for new data samples from CPU, if i am not wrong!).\nwhat is the standard approach for large datasets?\nHi,\nYou should not load whole datasets into your RAM if you do not have enough RAM to do that. You should use lazy loading provided in DataLoaders in PyTorch. As you mentioned, You have to prepare next data when your GPU is processing.\nBy the way, This is best and most efficient solution to task such this regarding huge datasets.\nThan you very much .\nthanks for your great information."},{"x":"Hello everyone,\nI\u2019m trying to build YOLO on pytorch, both for Academic reasons(I\u2019m rather new to deep learning) and because I\u2019m trying to use it to control a robot. However when I run the following line to create a new layer\n<code class=\"lang-auto\">Self.conv10 = nn.Conv3d(9,4,1024)\n<\/code>\nWhich is one of the layers detailed in the paper, I receive an error telling me that I allocated 114gb of ram and that ends with Buy New Ram! If you need to know my computer specs, I have a SurfacePro with 16gb ram, and I8, and a NVIDIA 1069","y":" - You should be using conv2d here instead of conv3d.\nFor 3x3x1024, it means that you have 1024 kernel filters of 3x3 size each. So your out_channels here is 1024, kernel_size is 3x3 and in_channels is the number of channels from the previous layer.","z":"Hi Ferruolo,\nThe conv3d parameters are input channels, output channels and kernel size in that order. Please ensure this is consistent with your code.\nAs you have given kernel size as 1024, the resulting weight tensor is of the size 9 x 4x 1024 x 1024 x 1024. That is a highly unusual kernel size in current literature.\n344B14B1-A931-4183-BE20-70BD7E9C4D36.jpeg828\u00d7393 54.6 KB with link \"https:\/\/discuss.pytorch.org\/uploads\/default\/original\/2X\/a\/a28059cacb043a058ed8e00532b7efe7c24801e0.jpeg\"\nI pulled that diagram from the Yolo paper. As far as I am aware, the notation of the kernels is INxOUTxKERNEL_SIZE. However, I am self taught, so that may be where I am wrong. What would I want to use for the 3x3x1024_kernel layer then?\n - You should be using conv2d here instead of conv3d.\nFor 3x3x1024, it means that you have 1024 kernel filters of 3x3 size each. So your out_channels here is 1024, kernel_size is 3x3 and in_channels is the number of channels from the previous layer.\nThanks a lot for your help  and .\n - Pytorch Docs are very detailed and easy to follow, and there are plenty of tutorials on github. You can follow them to learn more.\nI think every time someone opens PyTorch forum, at first should be redirected to this phrase."},{"x":"Hi,\nI\u2019m trying to train some part of a model like below:\nIMG_5BC31395F3B4-1.jpeg1349\u00d7880 733 KB with link \"https:\/\/discuss.pytorch.org\/uploads\/default\/original\/2X\/0\/0b775416d94b28a23a029f999c6d4b088729bbbd.jpeg\"\ninput is high dimensional and I\u2019m using a simple NN on a small partition of the data (f0) to reduce dimension and then concatenate with the rest of the data (f1) on which nothing has been applied. then I pass them to a frozen model M (pretrained Keras model) and then get the binary output and then I calculate loss. I just want to train the NN model and do backward on its parameters. Is this possible?","y":"A dummy approach would be:\n<code class=\"lang-python\">class MyModel(nn.Module):\n    def __init__(self):\n        super(MyModel, self).__init__()\n        self.fc = nn.Linear(4, 2)\n        self.base = models.resnet50()\n        \n    def forward(self, x):\n        # split x to x0, x1\n        x0, x1 = x[:, :4], x[:, 4:]\n        x0 = self.fc(x0)\n        # Concatenate\n        x = torch.cat((x0, x1), 1)\n        # reshape to fit resnet50 input shape\n        x = x.view(x.size(0), 3, 224, 224)\n        x = self.base(x)\n        return x\n\nmodel = MyModel()\nx = torch.randn(1, 3*224*224 + 2)\noutput = model(x)\n<\/code>\nNote that I\u2019ve used resnet50, which expects image tensors as the input, thus I had to reshape the concatenated tensor to [batch_size, 3, 224, 224].\nIf you are dealing with a pretrained model using only linear layers, you wouldn\u2019t have to reshape it.\n\n\n\n amirhf:\n\nAlso, is it because the output of the Keras model is a numpy array and doesn\u2019t keep the grad_fn and requires_grad that this is not possible?\n\n\nYes, basically you would have to convert the concatenated PyTorch tensor to a numpy array, which will detach the computation graph, so that you won\u2019t be able to calculate the gradients using the final loss.","z":"The workflow would be possible in PyTorch.\nHowever, if seems you would like to mix PyTorch with a pretrained Keras model afterwards, which won\u2019t work (at least I\u2019m not aware of such a workflow and if someone actually has used it before).\nWould it be possible to port the Keras parameters to a PyTorch model?\nThank you for the response.\nSo there are some layers in my Keras model that are custom defined and the existing libraries don\u2019t support custom layers when converting. But, how would I do that if M was a pretrained PyTorch model?\nAlso, is it because the output of the Keras model is a numpy array and doesn\u2019t keep the grad_fn and requires_grad that this is not possible?\nA dummy approach would be:\n<code class=\"lang-python\">class MyModel(nn.Module):\n    def __init__(self):\n        super(MyModel, self).__init__()\n        self.fc = nn.Linear(4, 2)\n        self.base = models.resnet50()\n        \n    def forward(self, x):\n        # split x to x0, x1\n        x0, x1 = x[:, :4], x[:, 4:]\n        x0 = self.fc(x0)\n        # Concatenate\n        x = torch.cat((x0, x1), 1)\n        # reshape to fit resnet50 input shape\n        x = x.view(x.size(0), 3, 224, 224)\n        x = self.base(x)\n        return x\n\nmodel = MyModel()\nx = torch.randn(1, 3*224*224 + 2)\noutput = model(x)\n<\/code>\nNote that I\u2019ve used resnet50, which expects image tensors as the input, thus I had to reshape the concatenated tensor to [batch_size, 3, 224, 224].\nIf you are dealing with a pretrained model using only linear layers, you wouldn\u2019t have to reshape it.\n\n\n\n amirhf:\n\nAlso, is it because the output of the Keras model is a numpy array and doesn\u2019t keep the grad_fn and requires_grad that this is not possible?\n\n\nYes, basically you would have to convert the concatenated PyTorch tensor to a numpy array, which will detach the computation graph, so that you won\u2019t be able to calculate the gradients using the final loss.\nThank you so much. This clarified a lot of things and helps me a lot!\nYou\u2019re welcome.\nLet us know, if you need help porting the code to train the model end-to-end \nDo you mean porting the Keras model? Do you think it\u2019s possible to port a trained Keras model (with custom layers) to PyTorch?\nIt depends, what kind of custom layers there are, but as long as there are equivalent PyTorch methods, it should work.\nThere might be some pitfalls, e.g. flipped kernels etc., but users in this forum ported successfully models before.\nSounds like a good project. I\u2019ll research a bit more (this forum and web) to see what I can do. Thanks for the help again. I\u2019ll reach out if I needed help "},{"x":"My images have 6 channels. I would like to apply some transformations to them. The problem is that if I apply, for instance, a RandomHorizontalFlip, then the resulting image has 3 channels only.\nI thus tried to use a Lambda function to apply the same transformation to each channel. I came up with the following class:\n<code class=\"lang-auto\">class HorizontalFlipChannels(object):\n    \n    def transform(self, img):\n        \n        transformed_channels = []\n        \n        for idx, channel in enumerate(img):\n            print(idx)\n            channel = transforms.ToPILImage()(channel)\n            channel = transforms.RandomHorizontalFlip()(channel)\n            channel = transforms.ToTensor()(channel)\n            \n            transformed_channels.append(channel)\n        \n        img = torch.cat(transformed_channels)\n        \n        return img\n<\/code>\nThe problem is that if I print the index of the channel (when I iterate over the images in one batch), then I get something like the following\n<code class=\"lang-auto\">0\n0\n1\n2\n1\n0\n2\n1\n0\n3\n3\n4\n2\n1\n5\n3\n4\n2\n4\n5\n3\n...\n<\/code>\nIs this normal? I was expecting to see something like 0 1 2 3 4 5 0 1 2 3 4 5 ....","y":"Yes it will, so no worries  It\u2019s just the multiprocessing that\u2019s screwing with the printouts.","z":"Do you by any chance has 4 workers in your dataloader? If more than 0, then the program can do these things\nYes, I have 4 workers. But I was wondering whether the final order of the channels for all the images will be the same.\nYes it will, so no worries  It\u2019s just the multiprocessing that\u2019s screwing with the printouts."},{"x":"Hi,\nI want to mimic behavior of PIL.Image.resize(\u2026,interpolation=PIL.Image.ANTIALIAS). What I would like to expect, is to have matching pixel vales (~99.9% exact match should be fine) . How can I do this with OpenCV ? I tried Opencv resize with available interpolation methods (including LANCZOS4) , but result does differ from the one I got from PIL.\nWhy I\u2019m doing this? Well I 'm developing machine learning program in C that is using (infer) pre-trained model (done in Python+PIL with PIL.Image.ANTIALIAS). Even small differences in preprocessing of images has an impact on how machine learning model behave.\nSo either I find a way to achieve same resize behavior in OpenCV as used in PIL.Image.ANTIALIAS or how to use PIL in C.\nSo how can I get over this difference?\nThanks in advance for answering my question.\nPlease advice","y":"In any case, you can easily convert PIL format to OpenCv format. Here\u2019s how\n\n\ngithub.com with link \"https:\/\/github.com\/amrit-das\/helper_scripts\/blob\/master\/cv-pil.py\"\n\n\namrit-das\/helper_scripts\/blob\/master\/cv-pil.py with link \"https:\/\/github.com\/amrit-das\/helper_scripts\/blob\/master\/cv-pil.py\"\n<code class=\"lang-py\">import cv2\nfrom PIL import Image\nimport numpy as np\nimport sys\n\n# PIL to openCV conversion\nim = Image.open(\"\/home\/amrit\/Desktop\/SRM\/codes\/{}\".format(sys.argv[1]))\nim.show()\ncv = cv2.cvtColor(numpy.array(im), cv2.COLOR_RGB2BGR)\ncv2.imshow(\"cv\",cv)\ncv2.waitKey(0)\n\n# OpenCV to PIL conversion\nim = cv2.imread(\"\/home\/amrit\/Desktop\/SRM\/codes\/{}\".format(sys.argv[1]))\ncv = cv2.cvtColor(numpy.array(im), cv2.COLOR_BGR2RGB)\nim = Image.fromarray(cv)\nim.show()\n\n\n\n<\/code>\n\n\n\n\n\n","z":"In any case, you can easily convert PIL format to OpenCv format. Here\u2019s how\n\n\ngithub.com with link \"https:\/\/github.com\/amrit-das\/helper_scripts\/blob\/master\/cv-pil.py\"\n\n\namrit-das\/helper_scripts\/blob\/master\/cv-pil.py with link \"https:\/\/github.com\/amrit-das\/helper_scripts\/blob\/master\/cv-pil.py\"\n<code class=\"lang-py\">import cv2\nfrom PIL import Image\nimport numpy as np\nimport sys\n\n# PIL to openCV conversion\nim = Image.open(\"\/home\/amrit\/Desktop\/SRM\/codes\/{}\".format(sys.argv[1]))\nim.show()\ncv = cv2.cvtColor(numpy.array(im), cv2.COLOR_RGB2BGR)\ncv2.imshow(\"cv\",cv)\ncv2.waitKey(0)\n\n# OpenCV to PIL conversion\nim = cv2.imread(\"\/home\/amrit\/Desktop\/SRM\/codes\/{}\".format(sys.argv[1]))\ncv = cv2.cvtColor(numpy.array(im), cv2.COLOR_BGR2RGB)\nim = Image.fromarray(cv)\nim.show()\n\n\n\n<\/code>\n\n\n\n\n\n\nOne major difference: The output of operations is completely different. In Pillow, \u200byou set radius, while in cv you set kernel size, which is literally diameter. You should use ksize=(7, 7) to achieve the same result.\nBut Dataloader could only support PIL Image as its input.\nWhat I want to do is that:\n\nUse OpenCV to read, and preprocess image like transforms.\nUse data.Dataloader to create train dataloader and test dataloader. But it cannot accept data that is resized by OpenCV.\n\nRead with opencv, convert it to PIL format, process the data and then you can revert it back to cv2 if you want.\nah neat. we must be on the same team.\nOK Finally, our team rewrites the whole torchvision to enable it to support opencv readings, including the transform file.\nNow it works well.\n: How to check that transform file uses opencv for resizing an image?"},{"x":"I\u2019m running a kenel that uses pytorch and CNN for mnist digit classification. The model is fairly simple but I\u2019m still getting CUDA OUT  OF MEMORY error.\nHere is the link to the kernel :     https:\/\/www.kaggle.com\/whizzkid\/training-best-cnn-model-pytorch\nI think I\u2019m using pytorch is some wrong way. Because I\u2019ve seen more complex model getting trained on kaggle successfully. I tried same model with keras and it worked as well. Even batch size is 16\nKaggle provides 16GB gpu. That\u2019s a lot to train my model. Still i\u2019m getting the error I don\u2019t know why.\nCan anyone tell me what am I doing wrong ?","y":"make the following change in your code in both the places\n<code class=\"lang-auto\">net_loss += loss.item()\n<\/code>\nloss.item() would just sum up the loss value, where as if you add loss it would keep on adding the whole graphs.","z":"The link is broken.\nAre you getting out of memory right at the start of training or does it train for sometime and then you get OOM ?\nAlso, can you check with batch_size=1 and see if the model runs ?\nSorry for the link it\u2019s working now.\nI\u2019m getting out of memory after 5-6 epochs or sometimes 9-10 epochs.\nmake the following change in your code in both the places\n<code class=\"lang-auto\">net_loss += loss.item()\n<\/code>\nloss.item() would just sum up the loss value, where as if you add loss it would keep on adding the whole graphs.\nthanks  , it\u2019s working now \nAlthough I understood the problem a bit, could you please explain in detail what was happening.\nBecause earlier also when I was printing the loss value, it was working the same way as of now. But now I\u2019m not getting out of memory error \nwhen you call .item() it returns the value of the tensor which is just a number. But in your earlier case you are adding loss which is a reference(not a value of tensor). So, you are holding that the reference to that computational graph every time you are doing total_loss += loss, which otherwise would have been destroyed after the update. To see the difference yourself,\n<code class=\"lang-auto\">print(loss)\nprint(loss.item())\n\nprint(type(loss))\nprint(type(loss.item()))\n\n<\/code>\nThe first prints a tensor which is the graph(as everything is connected) where as the second is just a float.\nPS: If your problem is solved mark the above comment as solution so that others may find it helpful."},{"x":"Hi,\nI am a beginner in Pytorch. I am currently trying to use the recently released (torchvision 0.3) models for object detection.\nI have read the examples here with link \"https:\/\/colab.research.google.com\/github\/pytorch\/vision\/blob\/temp-tutorial\/tutorials\/torchvision_finetuning_instance_segmentation.ipynb#scrollTo=at-h4OWK0aoc\" and here with link \"https:\/\/github.com\/pytorch\/vision\/blob\/v0.3.0\/references\/detection\/train.py\", but I am unable to make it work. Specifically, the example do segmentation, and I only want to do object detection. I think that is the reason why I cannot adequately adapt the examples to my script.\nThe error I constantly get is:\n<code class=\"lang-auto\">\/pytorch\/aten\/src\/THCUNN\/ClassNLLCriterion.cu:105: void cunn_ClassNLLCriterion_updateOutput_kernel(Dtype *, Dtype *, Dtype *, long *, Dtype *, int, int, int, int, long) [with Dtype = float, Acctype = float]: block: [0,0,0], thread: [28,0,0] Assertion `t >= 0 &amp;&amp; t < n_classes` failed.\n\/pytorch\/aten\/src\/THCUNN\/ClassNLLCriterion.cu:105: void cunn_ClassNLLCriterion_updateOutput_kernel(Dtype *, Dtype *, Dtype *, long *, Dtype *, int, int, int, int, long) [with Dtype = float, Acctype = float]: block: [0,0,0], thread: [29,0,0] Assertion `t >= 0 &amp;&amp; t < n_classes` failed.\n\/pytorch\/aten\/src\/THCUNN\/ClassNLLCriterion.cu:105: void cunn_ClassNLLCriterion_updateOutput_kernel(Dtype *, Dtype *, Dtype *, long *, Dtype *, int, int, int, int, long) [with Dtype = float, Acctype = float]: block: [0,0,0], thread: [30,0,0] Assertion `t >= 0 &amp;&amp; t < n_classes` failed.\n\/pytorch\/aten\/src\/THCUNN\/ClassNLLCriterion.cu:105: void cunn_ClassNLLCriterion_updateOutput_kernel(Dtype *, Dtype *, Dtype *, long *, Dtype *, int, int, int, int, long) [with Dtype = float, Acctype = float]: block: [0,0,0], thread: [31,0,0] Assertion `t >= 0 &amp;&amp; t < n_classes` failed.\nTraceback (most recent call last):\n  File \"detector_totales.py\", line 222, in <module>\n    main()\n  File \"detector_totales.py\", line 186, in main\n    train_one_epoch(model, optimizer, train_loader, device, epoch, print_freq=1)\n  File \"\/home\/fer\/git_clone\/eleccionesGT2019\/engine.py\", line 30, in train_one_epoch\n    loss_dict = model(images, targets)\n  File \"\/home\/fer\/git_clone\/eleccionesGT2019\/.tse\/lib\/python3.5\/site-packages\/torch\/nn\/modules\/module.py\", line 493, in __call__\n    result = self.forward(*input, **kwargs)\n  File \"\/home\/fer\/git_clone\/eleccionesGT2019\/.tse\/lib\/python3.5\/site-packages\/torchvision\/models\/detection\/generalized_rcnn.py\", line 52, in forward\n    detections, detector_losses = self.roi_heads(features, proposals, images.image_sizes, targets)\n  File \"\/home\/fer\/git_clone\/eleccionesGT2019\/.tse\/lib\/python3.5\/site-packages\/torch\/nn\/modules\/module.py\", line 493, in __call__\n    result = self.forward(*input, **kwargs)\n  File \"\/home\/fer\/git_clone\/eleccionesGT2019\/.tse\/lib\/python3.5\/site-packages\/torchvision\/models\/detection\/roi_heads.py\", line 534, in forward\n    class_logits, box_regression, labels, regression_targets)\n  File \"\/home\/fer\/git_clone\/eleccionesGT2019\/.tse\/lib\/python3.5\/site-packages\/torchvision\/models\/detection\/roi_heads.py\", line 34, in fastrcnn_loss\n    sampled_pos_inds_subset = torch.nonzero(labels > 0).squeeze(1)\nRuntimeError: copy_if failed to synchronize: device-side assert triggered\n<\/code>\nI have tried creating the network following the documentation in h**ps:\/\/github.com\/pytorch\/vision\/blob\/master\/torchvision\/models\/detection\/faster_rcnn.py like this:\n<code class=\"lang-auto\">def get_mobilenet_model(num_classes):\n    # Seguir ejemplo en \n\n    backbone = torchvision.models.mobilenet_v2(pretrained=True).features\n    backbone.out_channels = 1280\n\n    anchor_generator = AnchorGenerator(sizes=((32, 64, 128, 256, 512),), aspect_ratios=((0.5, 1.0, 2.0),))\n    roi_pooler = torchvision.ops.MultiScaleRoIAlign(featmap_names=[0], output_size=7,sampling_ratio=2)\n\n    model = FasterRCNN(backbone, num_classes=num_classes,rpn_anchor_generator=anchor_generator, box_roi_pool=roi_pooler)\n    return model\n<\/code>\nand also like this:\n<code class=\"lang-auto\">model = torchvision.models.detection.fasterrcnn_resnet50_fpn(pretrained=True) \nin_features = model.roi_heads.box_predictor.cls_score.in_features\nmodel.roi_heads.box_predictor = FastRCNNPredictor(in_features, num_classes)\n<\/code>\nI have in total 31 classes, but they do not start in 0 and are not sequential. I am not sure if this can be an issue.\nDo anyone has any idea on what the issue could be? The full script is available in (h**ps:\/\/github.com\/juanmed\/eleccionesGT2019\/blob\/master\/detector_totales.py).  I will appreciate any help.","y":"I was able to overcome the issue with CUDA Out of memory error. I did not solve it since I have not yet tried moving IOU calculation to CPU but at least now I can train with batch_size > 1.\nIn the end I simple reduced the number of anchors created from 5 to 3, and reduced the size of the input images. This allowed me to increase the batch size from 1 to 8.\nWhile doing this, I needed to resize both input images and targets (bounding box, area). Luckily, the detection models released with torchvision 0.3 already include resizing transforms that take care of both images and targets. Please take a look at this thread with link \"https:\/\/github.com\/pytorch\/vision\/issues\/329#issuecomment-504975409\".\nI will close this issue now.","z":"I was able to get a little bit further. It seems the problem did was with the classes definition. Originally the classes did not start in 0 and were not sequential. I defined them from 0 and sequential and the previous error seems to be resolved.\nHowever, after trying to run I got a CUDA out of memory error even in the on an NVIDIA RTX 24Gb GPU, which seems very strange since the model is based on mobilenet:\n<code class=\"lang-auto\">Traceback (most recent call last):\n  File \"detector_totales.py\", line 230, in <module>\n    main()\n  File \"detector_totales.py\", line 166, in main\n    model = model.to(device)\n  File \"\/home\/jfmy\/Repositories\/eleccionesGT2019\/.tse\/lib\/python3.6\/site-packages\/torch\/nn\/modules\/module.py\", line 386, in to\n    return self._apply(convert)\n  File \"\/home\/jfmy\/Repositories\/eleccionesGT2019\/.tse\/lib\/python3.6\/site-packages\/torch\/nn\/modules\/module.py\", line 193, in _apply\n    module._apply(fn)\n  File \"\/home\/jfmy\/Repositories\/eleccionesGT2019\/.tse\/lib\/python3.6\/site-packages\/torch\/nn\/modules\/module.py\", line 193, in _apply\n    module._apply(fn)\n  File \"\/home\/jfmy\/Repositories\/eleccionesGT2019\/.tse\/lib\/python3.6\/site-packages\/torch\/nn\/modules\/module.py\", line 193, in _apply\n    module._apply(fn)\n  File \"\/home\/jfmy\/Repositories\/eleccionesGT2019\/.tse\/lib\/python3.6\/site-packages\/torch\/nn\/modules\/module.py\", line 199, in _apply\n    param.data = fn(param.data)\n  File \"\/home\/jfmy\/Repositories\/eleccionesGT2019\/.tse\/lib\/python3.6\/site-packages\/torch\/nn\/modules\/module.py\", line 384, in convert\n    return t.to(device, dtype if t.is_floating_point() else None, non_blocking)\nRuntimeError: CUDA out of memory. Tried to allocate 246.00 MiB (GPU 0; 23.62 GiB total capacity; 65.31 MiB already\nallocated; 60.75 MiB free; 20.69 MiB cached)\n<\/code>\nAfter trying to run on CPU, the model does start training and there is some output but a new error, which I do not understand, appears:\n<code class=\"lang-auto\">Traceback (most recent call last):\n  File \"detector_totales.py\", line 230, in <module>\n    main()\n  File \"detector_totales.py\", line 196, in main\n    evaluate(model, test_loader, device=device)\n  File \"\/home\/fer\/git_clone\/eleccionesGT2019\/.tse\/lib\/python3.5\/site-packages\/torch\/autograd\/grad_mode.py\", line 43, in decorate_no_grad\n    return func(*args, **kwargs)\n  File \"\/home\/fer\/git_clone\/eleccionesGT2019\/engine.py\", line 78, in evaluate\n    coco = get_coco_api_from_dataset(data_loader.dataset)\n  File \"\/home\/fer\/git_clone\/eleccionesGT2019\/coco_utils.py\", line 205, in get_coco_api_from_dataset\n    return convert_to_coco_api(dataset)\n  File \"\/home\/fer\/git_clone\/eleccionesGT2019\/coco_utils.py\", line 155, in convert_to_coco_api\n    image_id = targets[\"image_id\"].item()\nKeyError: 'image_id'\n<\/code>\nAny help on both errors will be appreciated.\nI was able to solve the 2nd issue related to a key \u2018image_id\u2019 not available. It was my error and I noticed after double reading the example here with link \"https:\/\/pytorch.org\/tutorials\/intermediate\/torchvision_tutorial.html\". I only added the \u2018boxes\u2019 and \u2018labels\u2019 keys to the target dictionary but the keys \u2018image_id\u2019, \u2018area\u2019 and \u2018iscrowd\u2019 are also necessary. I created them by following the same example.\nAny help on the CUDA Out of memory error will be welcomed.\nAfter running  training  for 100 epochs, all IOU results are 0.0 and -1.0 which seems quite strange. I\u2019ve got a learning rate = 0.005, momentum = 0.5 and weight_decay = 0.005, which, after reading some posts related to such results (AP = 0.0) might solve such results.  Could anything in the model definition be leading to these results? Any comments will be appreciated.\n<code class=\"lang-auto\">Test:  [0\/1]  eta: 0:00:27  model_time: 26.7939 (26.7939)  evaluator_time: 0.0214 (0.0214)  time: 27.9693  data: 1.1538  max mem: 0\nTest: Total time: 0:00:28 (28.1102 s \/ it)\nAveraged stats: model_time: 26.7939 (26.7939)  evaluator_time: 0.0214 (0.0214)\nAccumulating evaluation results...\nDONE (t=0.06s).\nIoU metric: bbox\n Average Precision  (AP) @[ IoU=0.50:0.95 | area=   all | maxDets=100 ] = 0.000\n Average Precision  (AP) @[ IoU=0.50      | area=   all | maxDets=100 ] = 0.000\n Average Precision  (AP) @[ IoU=0.75      | area=   all | maxDets=100 ] = 0.000\n Average Precision  (AP) @[ IoU=0.50:0.95 | area= small | maxDets=100 ] = -1.000\n Average Precision  (AP) @[ IoU=0.50:0.95 | area=medium | maxDets=100 ] = -1.000\n Average Precision  (AP) @[ IoU=0.50:0.95 | area= large | maxDets=100 ] = 0.000\n Average Recall     (AR) @[ IoU=0.50:0.95 | area=   all | maxDets=  1 ] = 0.000\n Average Recall     (AR) @[ IoU=0.50:0.95 | area=   all | maxDets= 10 ] = 0.000\n Average Recall     (AR) @[ IoU=0.50:0.95 | area=   all | maxDets=100 ] = 0.000\n Average Recall     (AR) @[ IoU=0.50:0.95 | area= small | maxDets=100 ] = -1.000\n Average Recall     (AR) @[ IoU=0.50:0.95 | area=medium | maxDets=100 ] = -1.000\n Average Recall     (AR) @[ IoU=0.50:0.95 | area= large | maxDets=100 ] = 0.000\n<\/code>\nI think I might have some clues on what is the problem related to the CUDA Out of Memory error. My dataset has several bounding boxes (>20)  per image and according to this with link \"https:\/\/github.com\/facebookresearch\/maskrcnn-benchmark\/issues\/120\" andthis with link \"https:\/\/github.com\/facebookresearch\/maskrcnn-benchmark\/issues\/18\", calculation of IOU in the GPU is memory expensive.\nIn those issues, they suggest reducing batch size. So I though my batch size of 2 was low enough, but after I changed it to 1, the training process did started and has not stopped for 70 epochs. I wanted to implement thesuggestion in the thread with link \"https:\/\/github.com\/facebookresearch\/maskrcnn-benchmark\/issues\/120#issuecomment-438674823\" on moving IOU calculation to CPU (which can severely slow down training).\nDo anyone knows where the IOU calculation code is performed for the detection models? Any comments will be appreciated.\nI was able to overcome the issue with CUDA Out of memory error. I did not solve it since I have not yet tried moving IOU calculation to CPU but at least now I can train with batch_size > 1.\nIn the end I simple reduced the number of anchors created from 5 to 3, and reduced the size of the input images. This allowed me to increase the batch size from 1 to 8.\nWhile doing this, I needed to resize both input images and targets (bounding box, area). Luckily, the detection models released with torchvision 0.3 already include resizing transforms that take care of both images and targets. Please take a look at this thread with link \"https:\/\/github.com\/pytorch\/vision\/issues\/329#issuecomment-504975409\".\nI will close this issue now."},{"x":"Hello, I would like to have the MNIST data loader (torchvision package) to return an extra item for eg, Mean pixel value of the image along with the Image and the target. I have created a subclass which override the get_item method and returns the extra item.\n<code class=\"lang-auto\">import torchvision.datasets as dataset\nfrom torch.utils.data import  DataLoader\n\nclass MNIST1(dataset.MNIST):\n    \n    def __getitem__(self, index):\n        \"\"\"\n        Args:\n            index (int): Index\n\n        Returns:\n            tuple: (image, target,mean_pixel) where target is index of the target class.\n        \"\"\"\n        img, target = super(MNIST1,self).__getitem__(index)\n\n        # doing this so that it is consistent with all other datasets\n        # to return a PIL Image\n        img = Image.fromarray(img.numpy(), mode='L')\n       # the exta item to be returned\n        mean_pixel = PIL.ImageStat.Stat(img).mean\n        if self.transform is not None:\n            img = self.transform(img)\n\n        if self.target_transform is not None:\n            target = self.target_transform(target)\n        \n        sample ={\"image\":img,\"target\":target,\"mean_pixel\",mean_pixel}\n        return sample\n<\/code>\nthe data loading part\u2026\n<code class=\"lang-auto\">trans = transforms.Compose([transforms.ToTensor(), transforms.Normalize((0.5,), (1.0,))])\ntrain_set = MNIST1(root=root, train=True, transform=trans, download=True)\ntest_set = MNIST1(root=root, train=False, transform=trans, download=True)\n\nbatch_size = 100\ntrain_loader = DataLoader(dataset=train_set,\n                 batch_size=batch_size,\n                shuffle=True,pin_memory=True)\ntest_loader = DataLoader(dataset=test_set,\n                 batch_size=batch_size,\n                shuffle=True,pin_memory=True)\n<\/code>\nAm i doing it correct , I am facing errors while trying to iterate over the dataloader.\n<code class=\"lang-auto\">ValueError: Too many dimensions: 3 > 2.\n<\/code>\nthis is from  Image.fromarray(img.numpy(), mode=\u2018L\u2019) part. Please suggest a solution","y":"img will have 3 dimensions ([1, 28, 28]), which will yield this error.\nThe reason for the additional channel dimension is, that each sample will already be transformed in the super().__getitem__() method.\nIn particular Normalize will add the channel dimension, if you pass the MNIST images to this method.\nThe solution would be to override the complete __getitem__ method or to skip the additional transformation.","z":"img will have 3 dimensions ([1, 28, 28]), which will yield this error.\nThe reason for the additional channel dimension is, that each sample will already be transformed in the super().__getitem__() method.\nIn particular Normalize will add the channel dimension, if you pass the MNIST images to this method.\nThe solution would be to override the complete __getitem__ method or to skip the additional transformation.\nThanks for the clarification and it worked. I would like  to examine the performance of an Auto encoder  in MNIST dataset with an addition of extra loss function on mean pixel target. But I calculate the target mean pixels like this .\n<code class=\"lang-auto\">class MNIST1(dataset.MNIST):\n    \n    def __getitem__(self, index):\n        \"\"\"\n        Args:\n            index (int): Index\n\n        Returns:\n            tuple: (image, target,mean_pixel) where target is index of the target class.\n        \"\"\"\n        img, target = self.data[index], int(self.targets[index])\n        \n\n        # doing this so that it is consistent with all other datasets\n        # to return a PIL Image\n        img = Image.fromarray(img.numpy(), mode='L')\n        \n        # Calculatine the mean pixel value\n        stat = ImageStat.Stat(img)\n        mean_pixel = stat.mean\n        if self.transform is not None:\n            img = self.transform(img)\n            \n        if self.target_transform is not None:\n            target = self.target_transform(target)\n        \n        \n        return img, target,mean_pixel\n<\/code>\nI have doubt regarding the mean calculation, I am doing it on the PIL image where the pixel value ranges between 0 and 255. However I normalize the input image as follows\n<code class=\"lang-auto\">trans = transforms.Compose([transforms.ToTensor(), transforms.Normalize((0.5,), (1.0,))])\n\n<\/code>\nIf I want to calculate the loss on the mean pixel(assume that we have encoded the  predicted mean pixel value on one of the entries of the encoder decoder bottleneck part) target mean pixel values should be calculated on the normalized input or not?"},{"x":"Hi,\nI have created the following optimizer.\noptimizer = optim.SGD(model.parameters(), lr=learning_rate, momentum=momentum)\nBased on a condition, I am updating the weights for the new inputs. Whenever I update the weight, I am running through the following code body.\n<code class=\"lang-auto\">optimizer = optim.SGD(model.parameters(), lr=learning_rate, momentum=0.9) # doubt here\noptimizer.zero_grad()\ninputs_new, _ = data_list[max_index]\noutputs_new = model(inputs_new)\nloss = criterion(outputs_new, torch.tensor([y]))\nloss.backward()\noptimizer.step()\n<\/code>\nIs it correct to instantiate optimizer again and again (the line with the comment)? Is it a wrong practice? Is it logically wrong?\nThanks in advance.","y":"This should help. I had a similar question recently\u2026\n\n\n\n\nClarification on re-initializing optimizer in every epoch with link \"https:\/\/discuss.pytorch.org\/t\/clarification-on-re-initializing-optimizer-in-every-epoch\/48997\/\"\n\n\n    You shouldn\u2019t be doing that, right? fmi as well as my issue here with link \"https:\/\/github.com\/psyec1\/Lipreading-PyTorch\/issues\/7\"\n\n\n","z":"i dont see anything wrong here,it should work fine.\nR u facing any problems mate?\nI guess the momentum wouldn\u2019t work very well. Never seen anyone doing that before. Most people don\u2019t reinitialise it\nHi ,\nThe accuracy has gone down by nearly 20 % in the first case. (With one initializer)\nThanks\nThanks for the input. Actually, the momentum works well in that case.\naccuracy depends on other parameters also, i dont think re-instantiate optimizer again and again is a good practice but you then there is nothing in doing that.\ncan you post the full code and tell what exactly you r doing?\nThis should help. I had a similar question recently\u2026\n\n\n\n\nClarification on re-initializing optimizer in every epoch with link \"https:\/\/discuss.pytorch.org\/t\/clarification-on-re-initializing-optimizer-in-every-epoch\/48997\/\"\n\n\n    You shouldn\u2019t be doing that, right? fmi as well as my issue here with link \"https:\/\/github.com\/psyec1\/Lipreading-PyTorch\/issues\/7\"\n\n\n"},{"x":"I want to compute a pixel-wise loss by summing up distances to pixels neighbors. Is there a \u201csliding window\u201d manner to do that with Pytorch?","y":"You could probably use unfold to get the image patches and calculate the loss on these.\nHave a look at this post with link \"https:\/\/discuss.pytorch.org\/t\/how-to-extract-smaller-image-patches-3d\/16837\/4\" for an example.","z":"You could probably use unfold to get the image patches and calculate the loss on these.\nHave a look at this post with link \"https:\/\/discuss.pytorch.org\/t\/how-to-extract-smaller-image-patches-3d\/16837\/4\" for an example.\nThank u so much! :))"},{"x":"I\u2019ve 2 doubts in the official formula for calculating shape of output after applying 2d convolutional operation :\n\nWhy do we add 1 and take floor, why don\u2019t just take ceiling ?\nWhy do we add 1 in the numerator ?\n","y":"\n\nfloor + 1 is different to ceil for the case of a zero, which is needed in the calculation:\nfloor(0) + 1 = 1, while ceil(0) = 0\n\n\nWe subtract 1 in the numerator. Have a look at the Conv arithmetic with link \"http:\/\/deeplearning.net\/software\/theano\/tutorial\/conv_arithmetic.html#dilated-convolutions\" to derive the formula. Basically if you reformulate the dilated conv output shape formula, you\u2019ll get to the one used in the PyTorch docs.\n\n","z":"\n\nfloor + 1 is different to ceil for the case of a zero, which is needed in the calculation:\nfloor(0) + 1 = 1, while ceil(0) = 0\n\n\nWe subtract 1 in the numerator. Have a look at the Conv arithmetic with link \"http:\/\/deeplearning.net\/software\/theano\/tutorial\/conv_arithmetic.html#dilated-convolutions\" to derive the formula. Basically if you reformulate the dilated conv output shape formula, you\u2019ll get to the one used in the PyTorch docs.\n\n"},{"x":"Let\u2019s say i have to train an image classifier an a highly unbalanced dataset. Then I would like to penalize the losses belonging to the dominating classes less and vice versa !\nCan you pls show with a few lines of code how exactly  weights  in nn.BCEWithLogitsLossis passed ?\nImagine we have a dataset in which we have three classes with the following number of examples:\nclassA: 900\nclassB: 90\nclassC: 10\nnow how would you define ur loss function and how would you pass the weights argument?\nwould it be like\nloss_fn = nn.CrossEntropyLoss(weight = [1- 900\/1000, 1- 90\/1000, 1 - 10\/1000]) ???","y":"Hello Gurgaon!\n\n\n\n n0obcoder:\n\nImagine we have a dataset in which we have three classes with the following number of examples:\nclassA: 900\nclassB: 90\nclassC: 10\nnow how would you define ur loss function and how would you pass the weights argument?\nwould it be like\nloss_fn = nn.CrossEntropyLoss(weight = [1- 900\/1000, 1- 90\/1000, 1 - 10\/1000]) ???\n\n\nYou are correct that the named weight argument is the way to\nprovide class weights to nn.CrossEntropyLoss.\nBut the weights you use in your example don\u2019t make a lot of\nsense to me.  The weights you give for classes B and C are\nnot very different \u2013 they are 0.91 and 0.99, respectively, even\nthough you have nine times as many class-B samples as\nclass-C samples.\nIf you want your weights to fully compensate for the unbalanced\nnumber of samples per class, you would want relative weights of:\n<code class=\"lang-python\">    weight = [ 1\/900, 1\/90, 1\/10 ]\n<\/code>\nFurthermore , if you want your weights to have \u2013 on average \u2013 an\noverall scale of 1, you would want:\n<code class=\"lang-python\">    weight = (1000 \/ 3) * [ 1\/900, 1\/90, 1\/10 ]\n<\/code>\n(Also, could you edit the title and body of your post to change\n\u201cnn.BCEWithLogitsLoss\u201d to \u201cnn.CrossEntropyLoss\u201d where it\noccurs.)\nBest regards.\nK. Frank","z":"loss_fn = nn.CrossEntropyLoss(weight = [1- 900\/1000, 1- 90\/1000, 1 - 10\/1000])\nThis will work\nHello Gurgaon!\n\n\n\n n0obcoder:\n\nImagine we have a dataset in which we have three classes with the following number of examples:\nclassA: 900\nclassB: 90\nclassC: 10\nnow how would you define ur loss function and how would you pass the weights argument?\nwould it be like\nloss_fn = nn.CrossEntropyLoss(weight = [1- 900\/1000, 1- 90\/1000, 1 - 10\/1000]) ???\n\n\nYou are correct that the named weight argument is the way to\nprovide class weights to nn.CrossEntropyLoss.\nBut the weights you use in your example don\u2019t make a lot of\nsense to me.  The weights you give for classes B and C are\nnot very different \u2013 they are 0.91 and 0.99, respectively, even\nthough you have nine times as many class-B samples as\nclass-C samples.\nIf you want your weights to fully compensate for the unbalanced\nnumber of samples per class, you would want relative weights of:\n<code class=\"lang-python\">    weight = [ 1\/900, 1\/90, 1\/10 ]\n<\/code>\nFurthermore , if you want your weights to have \u2013 on average \u2013 an\noverall scale of 1, you would want:\n<code class=\"lang-python\">    weight = (1000 \/ 3) * [ 1\/900, 1\/90, 1\/10 ]\n<\/code>\n(Also, could you edit the title and body of your post to change\n\u201cnn.BCEWithLogitsLoss\u201d to \u201cnn.CrossEntropyLoss\u201d where it\noccurs.)\nBest regards.\nK. Frank"},{"x":"**The model is  79.82897162437439 % certain that the image has a predicted class of  2\nHow to make the output show the real name if the class instead of the index ?\n<code class=\"lang-auto\">def predict(image, model):\n    # Pass the image through our model\n    image = image.to(device)\n    output = model.forward(image)\n    \n    # Reverse the log function in our output\n    output = torch.exp(output)\n    \n    # Get the top predicted class, and the output percentage for\n    # that class\n    probs, classes = output.topk(1, dim=1)\n    \n   \n    \n    return probs.item(), classes.item()\n<\/code>","y":"You could create a mapping between the class indices and the corresponding names using a dict:\n<code class=\"lang-python\">idx_to_name = {\n  0: 'class0',\n  1: 'class1',\n  ...\n}\n\nprint(idx_to_name[target])\n<\/code>","z":"You could create a mapping between the class indices and the corresponding names using a dict:\n<code class=\"lang-python\">idx_to_name = {\n  0: 'class0',\n  1: 'class1',\n  ...\n}\n\nprint(idx_to_name[target])\n<\/code>"},{"x":"I am trying to change a public code with link \"https:\/\/github.com\/ThibaultGROUEIX\/3D-CODED\" so that I can have data parallel along two GPUs. I have seen this topic with link \"https:\/\/discuss.pytorch.org\/t\/runtimeerror-function-catbackward-returned-an-invalid-gradient-at-index-1-expected-device-1-but-got-0\/33958\", but it is for Model Parallelism, rather than DataParallel.\nThe code for creating and training the encoder-decoder network is as follows (Please look at the lines marked by #MY CHANGE):\n<code class=\"lang-auto\">from __future__ import print_function\nimport argparse\nimport random\nimport torch\nimport torch.optim as optim\nimport sys\nimport os\nsys.path.append(os.path.join(os.path.dirname(sys.path[0]), 'auxiliary'))\nfrom datasetSMPL2 import *\nfrom model_sample import *\nfrom utils import *\nfrom ply import *\nimport torch.nn as nn\n\nparser = argparse.ArgumentParser()\nparser.add_argument('--batchSize', type=int, default=32, help='input batch size')\nparser.add_argument('--workers', type=int, help='number of data loading workers', default=8)\nparser.add_argument('--nepoch', type=int, default=100, help='number of epochs to train for')\n\nopt = parser.parse_args()\n# ========================================================== #\nimport os\nsys.path.append(os.path.join(os.path.dirname(sys.path[0]), 'extension'))\n\nblue = lambda x: '\\033[94m' + x + '\\033[0m'\n\nopt.manualSeed = random.randint(1, 10000)  # fix seed\nrandom.seed(opt.manualSeed)\ntorch.manual_seed(opt.manualSeed)\nL2curve_train_smpl = []\nL2curve_val_smlp = []\n\n# meters to record stats on learning\ntrain_loss_L2_smpl = AverageValueMeter()\nval_loss_L2_smpl = AverageValueMeter()\ntmp_val_loss = AverageValueMeter()\n\n# ===================CREATE DATASET============================#\ndataset = SMPL(train=True, regular = True)\ndataloader = torch.utils.data.DataLoader(dataset, batch_size=opt.batchSize,\n                                         shuffle=True, num_workers=int(opt.workers))\n# ===================CREATE network==============================#\nnetwork = AE_AtlasNet_Humans()\ngpu_num = torch.cuda.device_count()  # MY CHANGE\nif torch.cuda.device_count() > 1:  # MY CHANGE\n    print(\"Let's use\", gpu_num, \"GPUs!\")  # MY CHANGE\n    network = nn.DataParallel(network, list(range(gpu_num))).cuda() # MY CHANGE\n\nnetwork.apply(weights_init)  # initialization of the weight\n# ===================CREATE optimizer=============================#\nlrate = 0.001  # learning rate\noptimizer = optim.Adam(network.parameters(), lr=lrate)\n# =============start of the learning loop =============================== #\nfor epoch in range(0, opt.nepoch):\n    if epoch==80:\n        lrate = lrate\/10.0  # learning rate scheduled decay\n        optimizer = optim.Adam(network.parameters(), lr=lrate)\n    if epoch==90:\n        lrate = lrate\/10.0  # learning rate scheduled decay\n        optimizer = optim.Adam(network.parameters(), lr=lrate)\n\n    # TRAIN MODE\n    train_loss_L2_smpl.reset()\n    network.train()\n    for i, data in enumerate(dataloader, 0):\n        optimizer.zero_grad()\n        points, idx,_ = data\n        points = points.transpose(2, 1).contiguous()\n        points = points.cuda()\n        pointsReconstructed = network(points, idx)  # MY CHANGE: replacing \"network.forward_idx\" with \"network\"\n        # target = points.transpose(2, 1).contiguous().cuda(non_blocking=True)  # tried to use DataParallel with loss; does not fix the error!\n        # criterion = nn.DataParallel(nn.MSELoss())\n        # criterion.cuda()\n        # loss_net = criterion(pointsReconstructed, target)\n        # loss_net.backward(torch.Tensor([1, 1]).cuda())\n        loss_net = torch.mean((pointsReconstructed - points.transpose(2, 1).contiguous()) ** 2)\n        loss_net.backward()  # RUNTIMEERROR\n        train_loss_L2_smpl.update(loss_net.item())\n        optimizer.step()  # gradient update\n<\/code>\nThe code for building the model is as follows (Please look at the lines marked by #MY CHANGE):\n<code class=\"lang-auto\">from __future__ import print_function\nimport torch\nimport torch.nn as nn\nimport torch.nn.parallel\nimport torch.utils.data\nfrom torch.autograd import Variable\nimport numpy as np\nimport torch.nn.functional as F\nimport trimesh\n\nclass STN3d(nn.Module):\n    def __init__(self, num_points = 2500):\n        super(STN3d, self).__init__()\n        self.num_points = num_points\n        self.conv1 = torch.nn.Conv1d(3, 64, 1)\n        self.conv2 = torch.nn.Conv1d(64, 128, 1)\n        self.conv3 = torch.nn.Conv1d(128, 1024, 1)\n        self.fc1 = nn.Linear(1024, 512)\n        self.fc2 = nn.Linear(512, 256)\n        self.fc3 = nn.Linear(256, 9)\n        self.relu = nn.ReLU()\n\n    def forward(self, x):\n        batchsize = x.size()[0]\n        x = F.relu(self.conv1(x))\n        x = F.relu(self.conv2(x))\n        x = F.relu(self.conv3(x))\n        x,_ = torch.max(x, 2)\n        x = x.view(-1, 1024)\n\n        x = F.relu(self.fc1(x))\n        x = F.relu(self.fc2(x))\n        x = self.fc3(x)\n\n        iden = Variable(torch.from_numpy(np.array([1,0,0,0,1,0,0,0,1]).astype(np.float32))).view(1,9).repeat(batchsize,1)\n        if x.is_cuda:\n            iden = iden.cuda()\n        x = x + iden\n        x = x.view(-1, 3, 3)\n        return x\n\n\nclass PointNetfeat(nn.Module):\n    def __init__(self, num_points = 2500, global_feat = True, trans = False):\n        super(PointNetfeat, self).__init__()\n        self.stn = STN3d(num_points = num_points)\n        self.conv1 = torch.nn.Conv1d(3, 64, 1)\n        self.conv2 = torch.nn.Conv1d(64, 128, 1)\n        self.conv3 = torch.nn.Conv1d(128, 1024, 1)\n\n        self.bn1 = torch.nn.BatchNorm1d(64)\n        self.bn2 = torch.nn.BatchNorm1d(128)\n        self.bn3 = torch.nn.BatchNorm1d(1024)\n        self.trans = trans\n\n        self.num_points = num_points\n        self.global_feat = global_feat\n\n    def forward(self, x):\n        batchsize = x.size()[0]\n        if self.trans:\n            trans = self.stn(x)\n            x = x.transpose(2,1)\n            x = torch.bmm(x, trans)\n            x = x.transpose(2,1)\n        x = F.relu(self.bn1(self.conv1(x)))\n        pointfeat = x\n        x = F.relu(self.bn2(self.conv2(x)))\n        x = self.bn3(self.conv3(x))\n        x,_ = torch.max(x, 2)\n        x = x.view(-1, 1024)\n        if self.trans:\n            if self.global_feat:\n                return x, trans\n            else:\n                x = x.view(-1, 1024, 1).repeat(1, 1, self.num_points)\n                return torch.cat([x, pointfeat], 1), trans\n        else:\n            return x\n\n\nclass PointGenCon(nn.Module):\n    def __init__(self, bottleneck_size = 2500):\n        self.bottleneck_size = bottleneck_size\n        super(PointGenCon, self).__init__()\n\n        self.conv1 = torch.nn.Conv1d(bottleneck_size, bottleneck_size, 1)\n        self.conv2 = torch.nn.Conv1d(bottleneck_size, bottleneck_size\/\/2, 1)\n        self.conv3 = torch.nn.Conv1d(bottleneck_size\/\/2, bottleneck_size\/\/4, 1)\n        self.conv4 = torch.nn.Conv1d(bottleneck_size\/\/4, 3, 1)\n\n        self.th = nn.Tanh()\n        self.bn1 = torch.nn.BatchNorm1d(bottleneck_size)\n        self.bn2 = torch.nn.BatchNorm1d(bottleneck_size\/\/2)\n        self.bn3 = torch.nn.BatchNorm1d(bottleneck_size\/\/4)\n\n    def forward(self, x):\n        batchsize = x.size()[0]\n        # print(x.size())\n        x = F.relu(self.bn1(self.conv1(x)))\n        x = F.relu(self.bn2(self.conv2(x)))\n        x = F.relu(self.bn3(self.conv3(x)))\n        x = 2*self.th(self.conv4(x))\n        return x\n\n\nclass AE_AtlasNet_Humans(nn.Module):\n    def __init__(self, num_points = 6890, bottleneck_size = 1024, nb_primitives = 1):\n        super(AE_AtlasNet_Humans, self).__init__()\n        self.num_points = num_points\n        self.bottleneck_size = bottleneck_size\n        self.nb_primitives = nb_primitives\n        self.encoder = nn.Sequential(\n        PointNetfeat(num_points, global_feat=True, trans = False),\n        nn.Linear(1024, self.bottleneck_size),\n        nn.BatchNorm1d(self.bottleneck_size),\n        nn.ReLU()\n        )\n\n        self.decoder = nn.ModuleList([PointGenCon(bottleneck_size = 3 +self.bottleneck_size) for i in range(0,self.nb_primitives)])\n\n        import sys\n        import os\n        mesh = trimesh.load(os.path.join(os.path.dirname(sys.path[0]), 'data', 'template', 'male_template.ply'), process=False)\n        self.mesh = mesh\n        mesh_HR = trimesh.load(os.path.join(os.path.dirname(sys.path[0]), 'data', 'template', 'male_template_dense.ply'), process=False)\n        self.mesh_HR = mesh_HR\n        point_set = mesh.vertices\n\n        bbox = np.array([[np.max(point_set[:,0]), np.max(point_set[:,1]), np.max(point_set[:,2])], [np.min(point_set[:,0]), np.min(point_set[:,1]), np.min(point_set[:,2])]])\n        tranlation = (bbox[0] + bbox[1]) \/ 2\n        point_set = point_set - tranlation\n\n        point_set_HR = mesh_HR.vertices\n        bbox = np.array([[np.max(point_set_HR[:,0]), np.max(point_set_HR[:,1]), np.max(point_set_HR[:,2])], [np.min(point_set_HR[:,0]), np.min(point_set_HR[:,1]), np.min(point_set_HR[:,2])]])\n        tranlation = (bbox[0] + bbox[1]) \/ 2\n        point_set_HR = point_set_HR - tranlation\n\n        self.vertex = torch.from_numpy(point_set).cuda().float()\n        self.vertex_HR = torch.from_numpy(point_set_HR).cuda().float()\n        self.num_vertex = self.vertex.size(0)\n        self.num_vertex_HR = self.vertex_HR.size(0)\n\n    def forward(self, x, idx): # MY CHANGE: replacing \"forward_idx\" with \"forward\"\n        x = self.encoder(x)\n        outs = []\n        for i in range(0,self.nb_primitives):\n            idx = idx.view(-1)\n            idx = idx.cpu().data.numpy().astype(np.int)\n            rand_grid = self.vertex[idx,:]\n            rand_grid = rand_grid.view(x.size(0), -1, 3).transpose(1,2).contiguous()\n            rand_grid = Variable(rand_grid)\n            y = x.unsqueeze(2).expand(x.size(0),x.size(1), rand_grid.size(2)).contiguous()\n            y = torch.cat( (rand_grid, y), 1).contiguous()\n            if x.is_cuda: # MY CHANGE: adding \".cuda()\"\n                y = y.cuda() # MY CHANGE: adding \".cuda()\"\n            outs.append(self.decoder[i](y))\n        # res = torch.cat(outs,2).contiguous().transpose(2,1).contiguous().to(\"cuda:0\") # tried to solve the RuntimeError; does not fix the error!\n        # return res\n        return torch.cat(outs,2).contiguous().transpose(2,1).contiguous()\n<\/code>\nI have a Runtime error as written in the title by running loss_net.backward(). I guess it might be related to the change I have made in the forward function of AE_Atlasnet_Humans (adding \u201c.cuda()\u201d to y). I did this change to fix an error that I had in forward pass when I wanted to use DataParallel.\nI tired different things to fix the error in the backward pass, but they did not solve the error. You can find them in the comments.","y":"Thank you. Good suggestion. Having debugged the code and watched the tensors, I have noticed that rand_grid is on the different GPU which affects y as well. By fixing that, the problem got solved.\nI replaced self.vertex = torch.from_numpy(point_set).cuda().float() with self.vertex = torch.from_numpy(point_set).float(), and rand_grid = self.vertex[idx,:] with rand_grid = self.vertex[idx,:].cuda(). I also deleted the following lines:\n<code class=\"lang-auto\">if x.is_cuda: # MY CHANGE: adding \".cuda()\"\n                y = y.cuda() # MY CHANGE: adding \".cuda()\"\nouts.append(self.decoder[i](y))\n<\/code>\nThe reason behind this is that DataParallel devides the input tensors to two sets and copies each set to a different GPU. So, we always need to be sure that the tensors which are not the input arguments of the forward function are at the devise where the input arguments are.","z":"Have you tried sending all variables to cuda in the same place in the code?  I see that you are sending different variables to devices both outside the loop:\n\n\n\n Nasi_Kaashki:\n\npoints = points.cuda()\n\n\nand then also inside the loop:\n\n\n\n Nasi_Kaashki:\n\nif x.is_cuda: # MY CHANGE: adding \u201c.cuda()\u201d y = y.cuda() # MY CHANGE: adding \u201c.cuda()\u201d\n\n\nI can\u2019t verify this but it\u2019s possible that .cuda() (which I believe defaults to cuda:0) may be sending to different devices in different sections of the code, especially given that one is called within the DataParallel module and one is outside.\nIt\u2019s also a good idea to use a debugger to check which device the variables used to calculate loss (pointsReconstructed, points.transpose(2, 1).contiguous()) are on: .contiguous() creates a copy of the tensor, for example, but on which device? A debugger also should help you check the gradients (or whether they exist in the first place).\nThank you. Good suggestion. Having debugged the code and watched the tensors, I have noticed that rand_grid is on the different GPU which affects y as well. By fixing that, the problem got solved.\nI replaced self.vertex = torch.from_numpy(point_set).cuda().float() with self.vertex = torch.from_numpy(point_set).float(), and rand_grid = self.vertex[idx,:] with rand_grid = self.vertex[idx,:].cuda(). I also deleted the following lines:\n<code class=\"lang-auto\">if x.is_cuda: # MY CHANGE: adding \".cuda()\"\n                y = y.cuda() # MY CHANGE: adding \".cuda()\"\nouts.append(self.decoder[i](y))\n<\/code>\nThe reason behind this is that DataParallel devides the input tensors to two sets and copies each set to a different GPU. So, we always need to be sure that the tensors which are not the input arguments of the forward function are at the devise where the input arguments are."},{"x":"Consider applications, e. g. Neural Style Transfer with link \"https:\/\/pytorch.org\/tutorials\/advanced\/neural_style_tutorial.html\", where the input is not batched but rather a single image. Can an application like this benefit from multiple GPUs if the model is wrapped in a nn.DataParallel?\nI\u2019m asking, because I want to be sure before I buy new hardware. If I understand the multiprocessing correctly, for such applications multiple GPUs are useless and thus a single GPU with high memory would be the better choice.","y":"Interesting, thanks for the clarification! In that case, I think using DataParallel will add useless overhead ","z":"Indeed, nn.DataParallel works by splitting batches on the available GPUs. multiprocessing however is used generally for loading data with multiple workers (CPU based) onto the GPU, therefore speeding training considerably.\nI\u2019m not familiar with style transfer training, but I don\u2019t see a reason why training would happen with single image batches . Even for inference, you might want to process multiple images at once\u2026\nIn style transfer you do not train a model, but use the output of the intermediate layers of a pretrained model to optimize \/ train a single image. Thus, during the optimization nothing is loaded from the drive.\nInteresting, thanks for the clarification! In that case, I think using DataParallel will add useless overhead \nThanks for the input, but since you \u2018only\u2019 think that multiple GPUs would be useless, I will keep this thread open for a little longer hoping someone who knows weighs in \nAs  said, if you are only using a single image, the data parallel approach won\u2019t speed up anything.\nI see no reason why you couldn\u2019t add a batch dimension and process multiple images at a time and thus be able to leverage DataParallel. Just make sure you don\u2019t reduce any of the losses across the batch dim.\n Thanks for the clarification.\n As explained above, during the whole training (maybe optimisation is a better fit here) you only process a single image. You do not train a model, but rather train (optimise) the pixel values of a single image to fit a given loss function."},{"x":"Using the following code:\n<code class=\"lang-auto\">import torch\nimport numpy as np\nfrom vgg_unet_aspp_detection import UNetVgg\nimport os\nimport psutil\nimport gc\nprocess = psutil.Process(os.getpid())\n\n\ndevice_str = \"cpu\"\ndevice = torch.device(device_str)\n    \nmodel = UNetVgg(4, device)\nmodel = model.eval()\nmodel = model.to(device)\n\n\nfor i in range(1000):\n    x = np.random.randint(500, 900)\n    y = np.random.randint(500, 1000)\n    img = np.random.rand(x, y, 3)\n    img_pt = img.astype(np.float32) \/ 255.0\n    img_pt = img_pt.transpose(2,0,1)\n    img_pt = torch.from_numpy(img_pt[None, ...]).to(device)\n    \n    with torch.no_grad():\n        output, _ = model(img_pt)\n        \n    print(output[0, 0, 0, 0])\n    \n    \n    gc.collect()\n    print('Loop %d - Memory: %f' % (i, process.memory_percent()))\n<\/code>\nI have the following output:\n\ntensor(-0.0305)\nLoop 0 - Memory: 3.526018\ntensor(-0.0306)\nLoop 1 - Memory: 4.146981\ntensor(-0.0306)\nLoop 2 - Memory: 4.146662\ntensor(-0.0305)\nLoop 3 - Memory: 4.390995\ntensor(-0.0306)\nLoop 4 - Memory: 4.518435\ntensor(-0.0304)\nLoop 5 - Memory: 4.517454\ntensor(-0.0306)\nLoop 6 - Memory: 4.534013\ntensor(-0.0306)\nLoop 7 - Memory: 4.279794\ntensor(-0.0305)\nLoop 8 - Memory: 4.967409\ntensor(-0.0305)\nLoop 9 - Memory: 4.966697\ntensor(-0.0305)\nLoop 10 - Memory: 5.320857\ntensor(-0.0306)\nLoop 11 - Memory: 5.320440\ntensor(-0.0305)\nLoop 12 - Memory: 5.326524\ntensor(-0.0304)\nLoop 13 - Memory: 5.353901\n\nWhy does the Memory changes in this accumulative way? I\u2019ve tried with Pytorch 1.0.1 and Pytorch 1.1, always using CPU. It seems like there is an upper bound, but still, I find this behaviour puzzling and it eats up a reasonable amount of memory.\nThe model is this one:\n<code class=\"lang-auto\">import torch\nimport torchvision\nimport numpy as np\nfrom torch import nn\nimport torch.nn.init as init\n\nclass ASPPModule(nn.Module):\n\n    def __init__(self, features, inner_features=256, out_features=512, dilations=(3, 5, 8)):\n        super(ASPPModule, self).__init__()\n\n        self.conv1 = nn.Sequential(nn.AdaptiveAvgPool2d((1,1)),\n                                   nn.Conv2d(features, inner_features, kernel_size=1, padding=0, dilation=1, bias=False))\n        self.conv2 = nn.Conv2d(features, inner_features, kernel_size=3, padding=1, dilation=1, bias=False)\n        self.conv3 = nn.Conv2d(features, inner_features, kernel_size=3, padding=dilations[0], dilation=dilations[0], bias=False)\n        self.conv4 = nn.Conv2d(features, inner_features, kernel_size=3, padding=dilations[1], dilation=dilations[1], bias=False)\n        self.conv5 = nn.Conv2d(features, inner_features, kernel_size=3, padding=dilations[2], dilation=dilations[2], bias=False)\n\n        self.bottleneck = nn.Sequential(\n            nn.Conv2d(inner_features * 5, out_features, kernel_size=1, padding=0, dilation=1, bias=False),\n            torch.nn.ReLU(True),\n            nn.Conv2d(out_features, out_features, kernel_size=1, padding=0, dilation=1),\n            torch.nn.ReLU(True)\n            )\n        \n    def forward(self, x):\n\n        _, _, h, w = x.size()\n\n        feat1 = torch.nn.functional.interpolate(self.conv1(x), size=(h, w), mode='bilinear', align_corners=True)\n\n        feat2 = self.conv2(x)\n        feat3 = self.conv3(x)\n        feat4 = self.conv4(x)\n        feat5 = self.conv5(x)\n        out = torch.cat((feat1, feat2, feat3, feat4, feat5), 1)\n\n        bottle = self.bottleneck(out)\n        return bottle\n\nclass UNetVgg(torch.nn.Module):\n    \"\"\"\n    Combines UNet (VGG based) with the ASPP module for segmentation.\n    \"\"\"\n    \n    def __init__(self, nClasses, device):\n        super(UNetVgg, self).__init__()\n        \n        vgg16pre = torchvision.models.vgg16(pretrained=True)\n        self.vgg0 = torch.nn.Sequential(*list(vgg16pre.features.children())[:4])\n        self.vgg1 = torch.nn.Sequential(*list(vgg16pre.features.children())[4:9])\n        self.vgg2 = torch.nn.Sequential(*list(vgg16pre.features.children())[9:16])\n        self.vgg3 = torch.nn.Sequential(*list(vgg16pre.features.children())[16:23])\n        self.vgg4 = torch.nn.Sequential(*list(vgg16pre.features.children())[23:30])\n        \n        \n        \n        \n        self.bottom = torch.nn.Sequential(\n                torch.nn.MaxPool2d(2, 2),\n                ASPPModule(512)\n                )\n        \n        self.aux_path = torch.nn.Sequential(\n                torch.nn.Conv2d(128, 64, kernel_size=(3,3), stride=1, padding=(1, 1)),\n                torch.nn.ReLU(True),\n                torch.nn.Conv2d(64, nClasses, kernel_size=1, stride=1, padding=0),\n                )\n        \n        self.smooth0 = torch.nn.Sequential(\n                torch.nn.Conv2d(128, 64, kernel_size=(3,3), stride=1, padding=(1, 1)),\n                torch.nn.ReLU(True),\n                torch.nn.Conv2d(64, 64, kernel_size=(3,3), stride=1, padding=(1, 1)),\n                torch.nn.ReLU(True)\n                )\n        self.smooth1 = torch.nn.Sequential(\n                torch.nn.Conv2d(384, 64, kernel_size=(3,3), stride=1, padding=(1, 1)),\n                torch.nn.ReLU(True),\n                torch.nn.Conv2d(64, 64, kernel_size=(3,3), stride=1, padding=(1, 1)),\n                torch.nn.ReLU(True)\n                )\n        self.smooth2 = torch.nn.Sequential(\n                torch.nn.Conv2d(512, 128, kernel_size=(3,3), stride=1, padding=(1, 1)),\n                torch.nn.ReLU(True),\n                torch.nn.Conv2d(128, 128, kernel_size=(3,3), stride=1, padding=(1, 1)),\n                torch.nn.ReLU(True)\n                )\n        self.smooth3 = torch.nn.Sequential(\n                torch.nn.Conv2d(768, 256, kernel_size=(3,3), stride=1, padding=(1, 1)),\n                torch.nn.ReLU(True),\n                torch.nn.Conv2d(256, 256, kernel_size=(3,3), stride=1, padding=(1, 1)),\n                torch.nn.ReLU(True)\n                )\n        self.smooth4 = torch.nn.Sequential(\n                torch.nn.Conv2d(1024, 256, kernel_size=(3,3), stride=1, padding=(1, 1)),\n                torch.nn.ReLU(True),\n                torch.nn.Conv2d(256, 256, kernel_size=(3,3), stride=1, padding=(1, 1)),\n                torch.nn.ReLU(True)\n                )\n        \n        \n        self.pass0 = torch.nn.Sequential(\n                torch.nn.Conv2d(64, 64, kernel_size=(1,1), stride=1, padding=(0, 0)),\n                torch.nn.ReLU(True)\n                )\n        self.pass1 = torch.nn.Sequential(\n                torch.nn.Conv2d(128, 128, kernel_size=(1,1), stride=1, padding=(0, 0)),\n                torch.nn.ReLU(True)\n                )\n        \n        self.bottom_up = torch.nn.Sequential(\n                torch.nn.Conv2d(512, 128, kernel_size=(1,1), stride=1, padding=(0, 0)),\n                torch.nn.ReLU(True)\n                )\n        \n        self.final = torch.nn.Conv2d(64, nClasses, kernel_size=1, stride=1, padding=0)\n\n    def forward(self, x):\n        \n        x = self.vgg0(x)\n        feat0 = self.pass0(x)\n        x = self.vgg1(x)\n        feat1 = self.pass1(x)\n        \n        feat2 = self.vgg2(x)\n        feat3 = self.vgg3(feat2)\n        feat4 = self.vgg4(feat3)\n        feat5 = self.bottom(feat4)\n        \n        btp = self.bottom_up(feat5)\n        \n        \n        _,_,H,W = feat4.size()\n        up4 = torch.nn.functional.interpolate(feat5, size=(H,W), mode='bilinear', align_corners=True)\n        concat4 = torch.cat([feat4, up4], 1)\n        end4 = self.smooth4(concat4)\n        \n        _,_,H,W = feat3.size()\n        up3 = torch.nn.functional.interpolate(end4, size=(H,W), mode='bilinear', align_corners=True)\n        concat3 = torch.cat([feat3, up3], 1)\n        end3 = self.smooth3(concat3)\n        \n        _,_,H,W = feat2.size()\n        up2 = torch.nn.functional.interpolate(end3, size=(H,W), mode='bilinear', align_corners=True)\n        concat2 = torch.cat([feat2, up2], 1)\n        end2 = self.smooth2(concat2)\n        \n        aux_out = self.aux_path(end2)\n        \n        _,_,H,W = feat1.size()\n        up1 = torch.nn.functional.interpolate(end2, size=(H,W), mode='bilinear', align_corners=True)\n        bottom_up = torch.nn.functional.interpolate(btp, size=(H,W), mode='bilinear', align_corners=True)\n        concat1 = torch.cat([feat1, up1, bottom_up], 1)\n        end1 = self.smooth1(concat1)\n        \n        _,_,H,W = feat0.size()\n        up0 = torch.nn.functional.interpolate(end1, size=(H,W), mode='bilinear', align_corners=True)\n        concat0 = torch.cat([feat0, up0], 1)\n        end0 = self.smooth0(concat0)\n        \n        aux_out = torch.nn.functional.interpolate(aux_out, size=(H,W), mode='bilinear', align_corners=True)\n        \n        return self.final(end0), aux_out\n<\/code>","y":"Could this be related to 22127 with link \"https:\/\/github.com\/pytorch\/pytorch\/issues\/22127\"?","z":"Thanks for the code!\nI could reproduce this behavior and the memory usage is growing ~1GB each epoch on my machine.\nI\u2019ll take a closer look at it.\nThank you!\nFrom my tests, it is not Python\/GC related. The accumulated memory is indeed lost somewhere\u2026\nIt could be just memory fragmentation\u2026\nBest regards,\nCould this be related to 22127 with link \"https:\/\/github.com\/pytorch\/pytorch\/issues\/22127\"?\nIt is indeed. Using\n\nLD_PRELOAD=\/usr\/lib\/x86_64-linux-gnu\/libjemalloc.so.1 python test_leak.py\n\nsolves the issue."},{"x":"Hi all, let\u2019s say I have a 4 dims tensor:\ninputs = tensor.randn(N, C, H, W)\nI want to get:\ninputs = [input_1, input_2, \u2026, input_n]\ninput_i.shape = [C, H, W]\nlen(inputs) = N\nIs there any elegant way to make this conversion? I can make it by a for loop but just feel not good.\nThis might be a weird conversion, but I do need it because the upper level of my project needs it.\nThank you for all for any suggestion!","y":"Using a list comprehension is probably faster: inputs = [i for i in inputs]\nHowever, since Pytorch tensors can virtually be used as lists would (iterating, item assignment\u2026 I think), I\u2019m really curious about your requirements. Can you reveal more about the \u201cupper level\u201d or is this information too sensitive?","z":"Using a list comprehension is probably faster: inputs = [i for i in inputs]\nHowever, since Pytorch tensors can virtually be used as lists would (iterating, item assignment\u2026 I think), I\u2019m really curious about your requirements. Can you reveal more about the \u201cupper level\u201d or is this information too sensitive?\nHi Alex, thanks for the answer!\nIt is because the upper level will zip() my inputs with another list. But I think you are right! I have never thought that zip() could directly work on a tensor (indeed it works)! Thanks a lot!"},{"x":"Dataloader code:\n<code class=\"lang-auto\">class HRFDataset(Dataset):\n    def __init__(self, image_paths, target_paths, train=True):\n        self.image_paths = image_paths\n        self.target_paths = target_paths\n\n    def transform(self, image, mask):\n    # Resize\n\n        imageTransform= transforms.Compose([\n        transforms.Resize(size=(256, 256)),\n        transforms.ToTensor(),\n        transforms.Normalize([0.485, 0.456, 0.406], [0.229, 0.224, 0.225]) # imagenet\n        ])\n\n        maskTransform= transforms.Compose([\n        transforms.Resize(size=(256, 256)),\n        transforms.ToTensor()\n        ])\n\n        image= imageTransform(image)\n        mask= maskTransform(mask)\n\n        return image, mask\n\n    def __getitem__(self, index):\n        image = Image.open(self.image_paths[index])\n        mask = Image.open(self.target_paths[index])\n        x, y = self.transform(image, mask)\n        return x, y\n<\/code>\nInitializing Dataset\n<code class=\"lang-auto\"># Creating training paths for dataset\nextra_val=\"0\"\ntrain_dir=\".\/dataset\/images\/\"\nmask_dir=\".\/dataset\/mask\/\"\ntrain_array=[]\nval_array=[]\nfor i in range(1,11):\n    if(i<10):\n        train_array.append(train_dir+extra_val+str(i)+\"_dr.JPG\")\n        train_array.append(train_dir+extra_val+str(i)+\"_g.jpg\")\n        train_array.append(train_dir+extra_val+str(i)+\"_h.jpg\")\n    else:\n        train_array.append(train_dir+str(i)+\"_dr.JPG\")\n        train_array.append(train_dir+str(i)+\"_g.jpg\")\n        train_array.append(train_dir+str(i)+\"_h.jpg\")\n\nfor i in range(1,11):\n    if(i<10):\n        val_array.append(mask_dir+extra_val+str(i)+\"_dr_mask.tif\")\n        val_array.append(mask_dir+extra_val+str(i)+\"_g_mask.tif\")\n        val_array.append(mask_dir+extra_val+str(i)+\"_h_mask.tif\")\n    else:\n        val_array.append(mask_dir+str(i)+\"_dr_mask.tif\")\n        val_array.append(mask_dir+str(i)+\"_g_mask.tif\")\n        val_array.append(mask_dir+str(i)+\"_h_mask.tif\")\n\n# Creating Validation paths dataset\n\n\n\nvalid_image=[]\nvalid_mask=[]\nfor i in range(1,11):\n    if(i<10):\n        valid_image.append(train_dir+extra_val+str(i)+\"_dr.JPG\")\n        valid_image.append(train_dir+extra_val+str(i)+\"_g.jpg\")\n        valid_image.append(train_dir+extra_val+str(i)+\"_h.jpg\")\n    else:\n        valid_image.append(train_dir+str(i)+\"_dr.JPG\")\n        valid_image.append(train_dir+str(i)+\"_g.jpg\")\n        valid_image.append(train_dir+str(i)+\"_h.jpg\")\n\nfor i in range(1,11):\n    if(i<10):\n        valid_mask.append(mask_dir+extra_val+str(i)+\"_dr_mask.tif\")\n        valid_mask.append(mask_dir+extra_val+str(i)+\"_g_mask.tif\")\n        valid_mask.append(mask_dir+extra_val+str(i)+\"_h_mask.tif\")\n    else:\n        valid_mask.append(mask_dir+str(i)+\"_dr_mask.tif\")\n        valid_mask.append(mask_dir+str(i)+\"_g_mask.tif\")\n        valid_mask.append(mask_dir+str(i)+\"_h_mask.tif\")\n\ntrain_set= HRFDataset(train_array,val_array)\nval_set= HRFDataset(valid_image,valid_mask)\nprint train_set\n<\/code>\nOutput of print statement:\nOut[45]: <main.HRFDataset object at 0x7fe7d659d550>\nImplementing Dataloader\n<code class=\"lang-auto\">from torch.utils.data import Dataset, DataLoader\n\nimage_datasets = {\n    'train': train_set, 'val': val_set\n}\n\nbatch_size = 10\n\ndataloaders = {\n    'train': DataLoader(train_set, batch_size=batch_size, shuffle=True, num_workers=0),\n    'val': DataLoader(val_set, batch_size=batch_size, shuffle=True, num_workers=0)\n}\n\ndataset_sizes = {\n    x: len(image_datasets[x]) for x in image_datasets.keys()\n}\n\ndataset_sizes\n<\/code>\nBut at this stage i am getting an error:\n\nNotImplementedError                       Traceback (most recent call last)\n in ()\n8\n9 dataloaders = {\n\u2014> 10     \u2018train\u2019: DataLoader(train_set, batch_size=batch_size, shuffle=True, num_workers=0),\n11     \u2018val\u2019: DataLoader(val_set, batch_size=batch_size, shuffle=True, num_workers=0)\n12 }\n\/home\/gul\/anaconda2\/lib\/python2.7\/site-packages\/torch\/utils\/data\/dataloader.pyc in init(self, dataset, batch_size, shuffle, sampler, batch_sampler, num_workers, collate_fn, pin_memory, drop_last, timeout, worker_init_fn)\n800             if sampler is None:\n801                 if shuffle:\n\u2013> 802                     sampler = RandomSampler(dataset)\n803                 else:\n804                     sampler = SequentialSampler(dataset)\n\/home\/gul\/anaconda2\/lib\/python2.7\/site-packages\/torch\/utils\/data\/sampler.pyc in init(self, data_source, replacement, num_samples)\n58\n59         if self.num_samples is None:\n\u2014> 60             self.num_samples = len(self.data_source)\n61\n62         if not isinstance(self.num_samples, int) or self.num_samples <= 0:\n\/home\/gul\/anaconda2\/lib\/python2.7\/site-packages\/torch\/utils\/data\/dataset.pyc in len(self)\n18\n19     def len(self):\n\u2014> 20         raise NotImplementedError\n21\n22     def add(self, other):\nNotImplementedError:\n\nCan\u2019t seem to find the problem. Looking forward to quick help.","y":"Okay. So all the problems above mentioned are solved by implementing len(self) function in the dataloader. My final dataloader is this.\n<code class=\"lang-auto\">class HRFDataset(Dataset):\n    def __init__(self, image_paths, target_paths, train=True):\n        self.image_paths = image_paths\n        self.target_paths = target_paths\n\n    def transform(self, image, mask):\n    # Resize\n\n        imageTransform= transforms.Compose([\n        transforms.Resize(size=(256, 256)),\n        transforms.ToTensor(),\n        transforms.Normalize([0.485, 0.456, 0.406], [0.229, 0.224, 0.225]) # imagenet\n        ])\n\n        maskTransform= transforms.Compose([\n        transforms.Resize(size=(256, 256)),\n        transforms.ToTensor()\n        ])\n\n        image= imageTransform(image)\n        mask= maskTransform(mask)\n\n        return image, mask\n\n    def __getitem__(self, index):\n        image = Image.open(self.image_paths[index])\n        mask = Image.open(self.target_paths[index])\n        x, y = self.transform(image, mask)\n        return x, y\n    def __len__(self):\n    \n        return len(self.image_paths)\n\n\n<\/code>","z":"i resolved the upper given problem by making shuffle bool false in DataLoader function. But now i am getting following error.\n<code class=\"lang-auto\">NotImplementedError                       Traceback (most recent call last)\n<ipython-input-54-2dda8680dbda> in <module>()\n     13 \n     14 dataset_sizes = {\n---> 15     x: len(image_datasets[x]) for x in image_datasets.keys()\n     16 }\n     17 \n\n<ipython-input-54-2dda8680dbda> in <dictcomp>((x,))\n     13 \n     14 dataset_sizes = {\n---> 15     x: len(image_datasets[x]) for x in image_datasets.keys()\n     16 }\n     17 \n\n\/home\/gul\/anaconda2\/lib\/python2.7\/site-packages\/torch\/utils\/data\/dataset.pyc in __len__(self)\n     18 \n     19     def __len__(self):\n---> 20         raise NotImplementedError\n     21 \n     22     def __add__(self, other):\n\nNotImplementedError: \n<\/code>\non following code:\n<code class=\"lang-auto\">from torch.utils.data import Dataset, DataLoader\n\nimage_datasets = {\n    'train': train_set, 'val': val_set\n}\n\nbatch_size = 10\n\ndataloaders = {\n    'train': DataLoader(train_set, batch_size=batch_size, shuffle=False, num_workers=0),\n    'val': DataLoader(val_set, batch_size=batch_size, shuffle=False, num_workers=0)\n}\n\ndataset_sizes = {\n    x: len(image_datasets[x]) for x in image_datasets.keys()\n}\n\ndataset_sizes\n<\/code>\nEdit 1\nResolved this error by making image_datasets[x] to image_datasets[x].image_paths in the third last statement\nI can\u2019t fetch batch from dataloader now.\n<code class=\"lang-auto\">inputs, masks = next(iter(dataloaders['train']))\n<\/code>\nI am getting the following error:\n<code class=\"lang-auto\">NotImplementedError                       Traceback (most recent call last)\n<ipython-input-19-fbb099c4b6b9> in <module>()\n     13 # Get a batch of training data\n     14 \n---> 15 inputs, masks = next(iter(dataloaders['train']))\n     16 \n     17 print(inputs.shape, masks.shape)\n\n\/home\/gul\/anaconda2\/lib\/python2.7\/site-packages\/torch\/utils\/data\/dataloader.pyc in __iter__(self)\n    817 \n    818     def __iter__(self):\n--> 819         return _DataLoaderIter(self)\n    820 \n    821     def __len__(self):\n\n\/home\/gul\/anaconda2\/lib\/python2.7\/site-packages\/torch\/utils\/data\/dataloader.pyc in __init__(self, loader)\n    582             # prime the prefetch loop\n    583             for _ in range(2 * self.num_workers):\n--> 584                 self._put_indices()\n    585 \n    586     def __len__(self):\n\n\/home\/gul\/anaconda2\/lib\/python2.7\/site-packages\/torch\/utils\/data\/dataloader.pyc in _put_indices(self)\n    644     def _put_indices(self):\n    645         assert self.batches_outstanding < 2 * self.num_workers\n--> 646         indices = next(self.sample_iter, None)\n    647         if indices is None:\n    648             return\n\n\/home\/gul\/anaconda2\/lib\/python2.7\/site-packages\/torch\/utils\/data\/sampler.pyc in __iter__(self)\n    158     def __iter__(self):\n    159         batch = []\n--> 160         for idx in self.sampler:\n    161             batch.append(idx)\n    162             if len(batch) == self.batch_size:\n\n\/home\/gul\/anaconda2\/lib\/python2.7\/site-packages\/torch\/utils\/data\/sampler.pyc in __iter__(self)\n     32 \n     33     def __iter__(self):\n---> 34         return iter(range(len(self.data_source)))\n     35 \n     36     def __len__(self):\n\n\/home\/gul\/anaconda2\/lib\/python2.7\/site-packages\/torch\/utils\/data\/dataset.pyc in __len__(self)\n     18 \n     19     def __len__(self):\n---> 20         raise NotImplementedError\n     21 \n     22     def __add__(self, other):\n\nNotImplementedError: \n<\/code>\nOkay. So all the problems above mentioned are solved by implementing len(self) function in the dataloader. My final dataloader is this.\n<code class=\"lang-auto\">class HRFDataset(Dataset):\n    def __init__(self, image_paths, target_paths, train=True):\n        self.image_paths = image_paths\n        self.target_paths = target_paths\n\n    def transform(self, image, mask):\n    # Resize\n\n        imageTransform= transforms.Compose([\n        transforms.Resize(size=(256, 256)),\n        transforms.ToTensor(),\n        transforms.Normalize([0.485, 0.456, 0.406], [0.229, 0.224, 0.225]) # imagenet\n        ])\n\n        maskTransform= transforms.Compose([\n        transforms.Resize(size=(256, 256)),\n        transforms.ToTensor()\n        ])\n\n        image= imageTransform(image)\n        mask= maskTransform(mask)\n\n        return image, mask\n\n    def __getitem__(self, index):\n        image = Image.open(self.image_paths[index])\n        mask = Image.open(self.target_paths[index])\n        x, y = self.transform(image, mask)\n        return x, y\n    def __len__(self):\n    \n        return len(self.image_paths)\n\n\n<\/code>\n It was nice reading your soliloquy, but I really loved it. And above anything, it solved the same issue i had. Though I also found out that this with link \"https:\/\/stanford.edu\/~shervine\/blog\/pytorch-how-to-generate-data-parallel\" tutorial on DataLoader class says about the len function. OMG, if only pytorch had good documentation and tutorials which explicitly mentions this. Even if it does, I somehow can never find one when I need one. Its like these important things are hidden somewhere deep inside a broad \u201cintro to pytorch\u201d tutorial\u2026\nThank you. Yes searching for bugs can be really difficult. I hope what you said wasn\u2019t sarcasm.\nThe document is quite clear\u2026\nversion: torch1.0.1\nIn:\nprint(torch.utils.data.Dataset.doc)\nOut:\nAn abstract class representing a Dataset.\n\nAll other datasets should subclass it. All subclasses should override\n``__len__``, that provides the size of the dataset, and ``__getitem__``,\nsupporting integer indexing in range from 0 to len(self) exclusive.\n\nOne can also use predefined subclasses of torch.utils.data.Dataset, e.g. tensorDataset, IterableDataset, all available after 1.0.1"},{"x":"I am going through the tutorial below, but I am confused as to how the right image shape is created via the generator.\nhttps:\/\/pytorch.org\/tutorials\/beginner\/dcgan_faces_tutorial.html\nIt seems the latent vector created is a one dimensional vector of size 100:\nWe are feeding that into ConvTranspose2D as getting back a  512, 8x8?\nConvTranspose2d(100, 512, kernel_size=(4, 4), stride=(1, 1), bias=False)\nHow exactly is this happening?\nIn the diagram shown within tutorial, 100z is being projected into a 1024, 4, 4 vector then converted to\n512,8,8 via ConvTranspore.\nAre we skipping this step?  I am assuming the projection would take place using a fc layer and then reshape?\nSo what exactly is happening here:\nConvTranspose2d(100, 512, kernel_size=(4, 4), stride=(1, 1), bias=False)","y":"By feeding the noise tensor of [batch_size, nz, 1, 1] into the first transposed conv layer, the spatial size will be increased.\nLet\u2019s have a look at a simple use case using just a single pixel value:\n<code class=\"lang-python\">b = 1\nnz = 1\nnoise = torch.ones(b, nz, 1, 1)\nconv = nn.ConvTranspose2d( nz, nz, 4, 1, 0, bias=False)\n\noutput = conv(noise)\nprint(output.shape)\n> torch.Size([1, 1, 4, 4])\nprint(conv.weight)\n> Parameter containing:\ntensor([[[[-0.0689, -0.0482,  0.1806,  0.0298],\n          [-0.1211,  0.1254, -0.1988, -0.1285],\n          [ 0.0184,  0.1757,  0.1835, -0.0602],\n          [ 0.0267, -0.0453, -0.0595,  0.0140]]]], requires_grad=True)\nprint(output)\n> tensor([[[[-0.0689, -0.0482,  0.1806,  0.0298],\n            [-0.1211,  0.1254, -0.1988, -0.1285],\n            [ 0.0184,  0.1757,  0.1835, -0.0602],\n            [ 0.0267, -0.0453, -0.0595,  0.0140]]]],\n         grad_fn=<ConvTranspose2DBackward>)\n<\/code>\nAs you can see in this example the kernel will just be multiplied with the single input value.\nSince I\u2019ve initialized it as a 1, you\u2019ll get the same output value.\nNote that if you are using more than a single input channel, the output will be the sum over the input channels of the weight kernels in this example.","z":"By feeding the noise tensor of [batch_size, nz, 1, 1] into the first transposed conv layer, the spatial size will be increased.\nLet\u2019s have a look at a simple use case using just a single pixel value:\n<code class=\"lang-python\">b = 1\nnz = 1\nnoise = torch.ones(b, nz, 1, 1)\nconv = nn.ConvTranspose2d( nz, nz, 4, 1, 0, bias=False)\n\noutput = conv(noise)\nprint(output.shape)\n> torch.Size([1, 1, 4, 4])\nprint(conv.weight)\n> Parameter containing:\ntensor([[[[-0.0689, -0.0482,  0.1806,  0.0298],\n          [-0.1211,  0.1254, -0.1988, -0.1285],\n          [ 0.0184,  0.1757,  0.1835, -0.0602],\n          [ 0.0267, -0.0453, -0.0595,  0.0140]]]], requires_grad=True)\nprint(output)\n> tensor([[[[-0.0689, -0.0482,  0.1806,  0.0298],\n            [-0.1211,  0.1254, -0.1988, -0.1285],\n            [ 0.0184,  0.1757,  0.1835, -0.0602],\n            [ 0.0267, -0.0453, -0.0595,  0.0140]]]],\n         grad_fn=<ConvTranspose2DBackward>)\n<\/code>\nAs you can see in this example the kernel will just be multiplied with the single input value.\nSince I\u2019ve initialized it as a 1, you\u2019ll get the same output value.\nNote that if you are using more than a single input channel, the output will be the sum over the input channels of the weight kernels in this example.\nThank you so much for this example. I forgot I could just do tests on my own especially since Pytorch graphs are dynamic. I will play around with strides as well."},{"x":"Hi,\nI\u2019m bit puzzled on how scatter_add(https:\/\/pytorch.org\/docs\/stable\/tensors.html#torch.Tensor.scatter_add_) works.\nProblem\nWhy can\u2019t I reproduce the same result with for-loops?\n<code class=\"lang-auto\">self[index[i][j][k]][j][k] += other[i][j][k]  # if dim == 0\nself[i][index[i][j][k]][k] += other[i][j][k]  # if dim == 1\nself[i][j][index[i][j][k]] += other[i][j][k]  # if dim == 2\n<\/code>\nExamples\nI\u2019m testing the method progressively starting from a\n1-D tensor.\n<code class=\"lang-python\">a = tf.Variable([1,2,3], dtype=tf.float32)\nb = torch.tensor([1,2,3], dtype=torch.float32)\n\nindices = np.array([0,0,1,2,1,0,2], dtype=np.int)\nupdates = np.array(list(range(len(indices))), dtype=np.float32)\n\nb.scatter_add(0, torch.from_numpy(indices), torch.from_numpy(updates))\n# tensor([ 7.,  8., 12.])\n<\/code>\n2-D tensor.\n<code class=\"lang-python\">array = np.array([\n    [1,2,3],\n    [4,5,6]\n], dtype=np.float32)\n\nb = torch.tensor(array, dtype=torch.float32)\n\nindices = np.array([\n    [0,0], [0,1]\n], dtype=np.int)\nupdates = np.array(\n    [[1,2], [3,4]], \n    dtype=np.float32)\n<\/code>\nHere I run into an error:\n<code class=\"lang-python\">b.scatter_add(0, torch.from_numpy(indices), torch.from_numpy(updates))\n\n# Isn't the above the same as below?\nfor i in range(2):\n    for j in range(2):\n        b[indices[i,j]][j] += updates[i,j]\n        \n# tensor([[5., 4., 3.],\n#         [4., 9., 6.]])\n<\/code>\nThe following does not raise an error:\n<code class=\"lang-auto\">b.scatter_add(1, torch.from_numpy(indices), torch.from_numpy(updates))\n# tensor([[11.,  4.,  3.],\n#         [10., 17.,  6.]])\n\n\n# However, the result is not as same as \nfor i in range(2):\n    for j in range(2):\n        b[i, indices[i,j]] += updates[i,j]\n# tensor([[ 8.,  4.,  3.],\n#         [ 7., 13.,  6.]])\n<\/code>\nThanks!","y":"The second example yields the same result on my machine.\nCould you make sure to reinitialize b and check the result again?\nThe first call raises an error, since you have a dimension mismatch.\nIf you expand indices and updates, the code will also return the same values:\n<code class=\"lang-python\">array = np.array([[1,2,3],\n                  [4,5,6]],\n                 dtype=np.float32)\n\nb = torch.tensor(array, dtype=torch.float32)\n\nindices = np.array([[0,0,0],\n                    [0,1,1]],\n                    dtype=np.int)\nupdates = np.array([[1,2,3],\n                    [4,5,6]], \n                   dtype=np.float32)\n\nprint(b.scatter_add(0, torch.from_numpy(indices), torch.from_numpy(updates)))\n\nfor i in range(2):\n    for j in range(3):\n        b[indices[i,j]][j] += updates[i,j]\nprint(b)\n\n> tensor([[ 6.,  4.,  6.],\n          [ 4., 10., 12.]])\n> tensor([[ 6.,  4.,  6.],\n          [ 4., 10., 12.]])\n<\/code>","z":"Plus, is scatter_add compatible with tf.scatter_add(https:\/\/www.tensorflow.org\/api_docs\/python\/tf\/scatter_add)?\nThe second example yields the same result on my machine.\nCould you make sure to reinitialize b and check the result again?\nThe first call raises an error, since you have a dimension mismatch.\nIf you expand indices and updates, the code will also return the same values:\n<code class=\"lang-python\">array = np.array([[1,2,3],\n                  [4,5,6]],\n                 dtype=np.float32)\n\nb = torch.tensor(array, dtype=torch.float32)\n\nindices = np.array([[0,0,0],\n                    [0,1,1]],\n                    dtype=np.int)\nupdates = np.array([[1,2,3],\n                    [4,5,6]], \n                   dtype=np.float32)\n\nprint(b.scatter_add(0, torch.from_numpy(indices), torch.from_numpy(updates)))\n\nfor i in range(2):\n    for j in range(3):\n        b[indices[i,j]][j] += updates[i,j]\nprint(b)\n\n> tensor([[ 6.,  4.,  6.],\n          [ 4., 10., 12.]])\n> tensor([[ 6.,  4.,  6.],\n          [ 4., 10., 12.]])\n<\/code>\nI see. So for a 2-D tensor of shape (x, y), y has to be the same for all tensors involved in.\nAs of PyTorch 1.1.0 and TF 1.14.0, their logic for scatter_add differ. So I just created a custom ops in TF for my project."},{"x":"Hi everyone, i have this error when training my network. I don\u2019t see where is the error in my code\n<code class=\"lang-python\">THCudaCheck FAIL file=\/opt\/conda\/conda-bld\/pytorch_1532581333611\/work\/aten\/src\/THC\/generated\/..\/generic\/THCTensorMathPointwise.cu line=464 error=59 : device-side assert triggered\nTraceback (most recent call last):\n  File \"train.py\", line 213, in <module>\n    train()\n  File \"train.py\", line 154, in train\n    loss_id.backward()\n  File \"\/home\/fstu1\/miniconda3\/lib\/python3.6\/site-packages\/torch\/tensor.py\", line 93, in backward\n    torch.autograd.backward(self, gradient, retain_graph, create_graph)\n  File \"\/home\/fstu1\/miniconda3\/lib\/python3.6\/site-packages\/torch\/autograd\/__init__.py\", line 90, in backward\n    allow_unreachable=True)  # allow_unreachable flag\nRuntimeError: cuda runtime error (59) : device-side assert triggered at \/opt\/conda\/conda-bld\/pytorch_1532581333611\/work\/aten\/src\/THC\/generated\/..\/generic\/THCTensorMathPointwise.cu:464\n<\/code>\nThe train() function\n<code class=\"lang-python\">model = Model(pretrained=True)\nmodel = model.cuda()\nfor i, (inputs, targets) in enumerate(train_loader):\n     inputs = inputs.cuda()\n     inputs = Variable(inputs)\n     outputs = model(inputs)\n     targets = targets.cuda()\n     targets = Variable(targets)\n     loss_id = criterion(ipnuts, targets)\n     optimizer.zero_grad()\n     loss_id.backward()\n     optimizer.step()\n<\/code>\nThank you for helping me debug this","y":"ok, thank you. my mistake, the output of the model was (batch_size, 512). I  had to change it to 751\nThank you","z":"Running with CUDA_LAUNCH_BLOCKING=1 gives this output:\n<code class=\"lang-python\">\/opt\/conda\/conda-bld\/pytorch_1532581333611\/work\/aten\/src\/THCUNN\/ClassNLLCriterion.cu:105: void cunn_ClassNLLCriterion_updateOutput_kernel(Dtype *, Dtype *, Dtype *, long *, Dtype *, int, int, int, int, long) [with Dtype = float, Acctype = float]: block: [0,0,0], thread: [0,0,0] Assertion `t >= 0 &amp;&amp; t < n_classes` failed.\n\/opt\/conda\/conda-bld\/pytorch_1532581333611\/work\/aten\/src\/THCUNN\/ClassNLLCriterion.cu:105: void cunn_ClassNLLCriterion_updateOutput_kernel(Dtype *, Dtype *, Dtype *, long *, Dtype *, int, int, int, int, long) [with Dtype = float, Acctype = float]: block: [0,0,0], thread: [1,0,0] Assertion `t >= 0 &amp;&amp; t < n_classes` failed.\n\/opt\/conda\/conda-bld\/pytorch_1532581333611\/work\/aten\/src\/THCUNN\/ClassNLLCriterion.cu:105: void cunn_ClassNLLCriterion_updateOutput_kernel(Dtype *, Dtype *, Dtype *, long *, Dtype *, int, int, int, int, long) [with Dtype = float, Acctype = float]: block: [0,0,0], thread: [2,0,0] Assertion `t >= 0 &amp;&amp; t < n_classes` failed.\n\/opt\/conda\/conda-bld\/pytorch_1532581333611\/work\/aten\/src\/THCUNN\/ClassNLLCriterion.cu:105: void cunn_ClassNLLCriterion_updateOutput_kernel(Dtype *, Dtype *, Dtype *, long *, Dtype *, int, int, int, int, long) [with Dtype = float, Acctype = float]: block: [0,0,0], thread: [3,0,0] Assertion `t >= 0 &amp;&amp; t < n_classes` failed.\nTHCudaCheck FAIL file=\/opt\/conda\/conda-bld\/pytorch_1532581333611\/work\/aten\/src\/THCUNN\/generic\/ClassNLLCriterion.cu line=111 error=59 : device-side assert triggered\nException ignored in: <bound method _DataLoaderIter.__del__ of <torch.utils.data.dataloader._DataLoaderIter object at 0x2b78253c1630>>\nTraceback (most recent call last):\n  File \"\/home\/fstu1\/miniconda3\/lib\/python3.6\/site-packages\/torch\/utils\/data\/dataloader.py\", line 399, in __del__\n    self._shutdown_workers()\n  File \"\/home\/fstu1\/miniconda3\/lib\/python3.6\/site-packages\/torch\/utils\/data\/dataloader.py\", line 378, in _shutdown_workers\n    self.worker_result_queue.get()\n  File \"\/home\/fstu1\/miniconda3\/lib\/python3.6\/multiprocessing\/queues.py\", line 337, in get\n    return _ForkingPickler.loads(res)\n  File \"\/home\/fstu1\/miniconda3\/lib\/python3.6\/site-packages\/torch\/multiprocessing\/reductions.py\", line 151, in rebuild_storage_fd\n    fd = df.detach()\n  File \"\/home\/fstu1\/miniconda3\/lib\/python3.6\/multiprocessing\/resource_sharer.py\", line 57, in detach\n    with _resource_sharer.get_connection(self._id) as conn:\n  File \"\/home\/fstu1\/miniconda3\/lib\/python3.6\/multiprocessing\/resource_sharer.py\", line 87, in get_connection\n    c = Client(address, authkey=process.current_process().authkey)\n  File \"\/home\/fstu1\/miniconda3\/lib\/python3.6\/multiprocessing\/connection.py\", line 494, in Client\n    deliver_challenge(c, authkey)\n  File \"\/home\/fstu1\/miniconda3\/lib\/python3.6\/multiprocessing\/connection.py\", line 722, in deliver_challenge\n    response = connection.recv_bytes(256)        # reject large message\n  File \"\/home\/fstu1\/miniconda3\/lib\/python3.6\/multiprocessing\/connection.py\", line 216, in recv_bytes\n    buf = self._recv_bytes(maxlength)\n  File \"\/home\/fstu1\/miniconda3\/lib\/python3.6\/multiprocessing\/connection.py\", line 407, in _recv_bytes\n    buf = self._recv(4)\n  File \"\/home\/fstu1\/miniconda3\/lib\/python3.6\/multiprocessing\/connection.py\", line 379, in _recv\n    chunk = read(handle, remaining)\nConnectionResetError: [Errno 104] Connection reset by peer\nTraceback (most recent call last):\n  File \"train.py\", line 213, in <module>\n    train()\n  File \"train.py\", line 140, in train\n    loss_id = criterion(outputs, targets)\n  File \"\/home\/fstu1\/miniconda3\/lib\/python3.6\/site-packages\/torch\/nn\/modules\/module.py\", line 477, in __call__\n    result = self.forward(*input, **kwargs)\n  File \"\/home\/fstu1\/miniconda3\/lib\/python3.6\/site-packages\/torch\/nn\/modules\/loss.py\", line 862, in forward\n    ignore_index=self.ignore_index, reduction=self.reduction)\n  File \"\/home\/fstu1\/miniconda3\/lib\/python3.6\/site-packages\/torch\/nn\/functional.py\", line 1550, in cross_entropy\n    return nll_loss(log_softmax(input, 1), target, weight, None, ignore_index, None, reduction)\n  File \"\/home\/fstu1\/miniconda3\/lib\/python3.6\/site-packages\/torch\/nn\/functional.py\", line 1407, in nll_loss\n    return torch._C._nn.nll_loss(input, target, weight, _Reduction.get_enum(reduction), ignore_index)\nRuntimeError: cuda runtime error (59) : device-side assert triggered at \/opt\/conda\/conda-bld\/pytorch_1532581333611\/work\/aten\/src\/THCUNN\/generic\/ClassNLLCriterion.cu:111\n\n<\/code>\nCould you check the target values, since they are supposed to be in the range [0, nb_classes-1].\nMaybe they are outside of this range, which will throw an error.\nAlso, you might want to run the code on CPU to get a (hopefully better) error message.\nThank you for your help\nI printed the targets values for the first epoch\n<code class=\"lang-python\">tensor([111, 695, 202, 720, 217, 434, 471, 625, 379, 201,   3, 273, 469, 147,\n        180, 141, 378, 326, 418, 725,  32, 225, 176, 297, 452, 328, 264, 208,\n        441, 689, 580, 155], device='cuda:0')\ntensor([260, 129, 505, 586, 529, 662, 570, 289, 391, 702, 433, 228, 139,  66,\n         52, 732, 678, 437, 619, 526, 369, 100, 709, 181, 402, 460, 231, 492,\n        516, 374, 440, 299], device='cuda:0')\ntensor([157, 412, 149, 651, 669,   5,  31, 632, 680, 145, 103, 478, 213, 115,\n        576, 738, 253,  17, 182, 327, 464, 252, 590, 706,  51, 238,   6, 553,\n        107, 302, 240, 721], device='cuda:0')\ntensor([532, 491, 693, 664, 567, 530, 135, 150, 515, 436, 119, 300, 655, 628,\n        604,  23, 683, 332, 204, 481, 387, 315, 153, 750, 518, 456, 222, 511,\n        200, 218,  36, 271], device='cuda:0')\ntensor([739, 631, 599, 110, 601, 746, 183, 400, 520,  91, 169,   2, 239, 422,\n        286, 455, 639, 483, 280, 479, 741, 340,  59, 524, 510, 396, 211, 144,\n        419, 508, 577, 288], device='cuda:0')\ntensor([125,  39,  63,  96, 519, 366, 463, 417,  55,  81, 574, 350, 197, 684,\n         18, 334, 428, 597, 185, 338, 295, 493, 215,  76, 159, 488, 188, 536,\n        555, 710,  38, 408], device='cuda:0')\ntensor([283, 151,   9, 548, 486,  82, 186, 236, 652, 623, 170,  45, 626, 457,\n        504, 609, 514, 281, 163, 318, 591, 177, 560, 672, 588, 267, 700, 542,\n        195, 444, 256, 635], device='cuda:0')\ntensor([353, 311, 376, 371, 677,  14, 321, 320, 282,  72, 291, 665, 543, 259,\n        551, 584, 733,  97, 301,  86, 679, 410, 600,  48,  19, 336, 189, 317,\n        346, 716, 610, 735], device='cuda:0')\ntensor([ 24, 223, 521, 257, 382, 569, 415, 587, 290,  69, 137, 312, 337, 734,\n        232, 160, 717, 503, 713, 612,  21, 199, 158, 643, 575, 620, 594, 388,\n        210,  64, 568, 556], device='cuda:0')\ntensor([262,  79, 657,  92, 461,  54, 166, 187, 429, 308,   1, 485, 666,  42,\n        496, 352,  37, 497, 675, 356, 203, 431, 482, 697, 243, 547, 305, 196,\n        468, 314, 432,   8], device='cuda:0')\ntensor([644,  11, 737, 322, 355, 383, 581,  95, 642, 335,  56, 743, 636, 686,\n        659, 673, 397, 138, 292,  50,  98, 613, 279,  78, 358, 132, 549, 707,\n        633, 681, 194, 390], device='cuda:0')\ntensor([389, 454, 539, 540, 130, 480, 624, 426, 233, 167, 362, 459, 660, 123,\n         88,  71, 699, 685, 435, 172, 219, 255, 127, 365, 656, 668, 269, 490,\n        618, 310, 101,  44], device='cuda:0')\ntensor([650, 152, 105, 506, 663, 168, 298, 270, 641, 142, 611, 446, 489, 647,\n        407, 126, 607, 670,  70, 373, 557, 726, 598, 209,  87, 494, 227, 339,\n        319, 714, 304, 272], device='cuda:0')\ntensor([416, 525, 124, 722, 424, 671, 140,  35, 276, 667, 254, 622, 544, 742,\n        582, 731,  58, 381,  74, 592, 438, 285, 749, 361, 206, 370, 487,  65,\n        447, 134, 747, 414], device='cuda:0')\ntensor([661, 349, 637, 698, 629, 676, 405,  10, 104, 143, 708, 694, 701, 146,\n         89, 306, 303, 224, 534, 404, 451, 559, 207, 263,  43, 242, 538, 646,\n        727, 386, 658, 385], device='cuda:0')\ntensor([248, 357,  90, 564, 120, 164, 498, 608, 522, 472, 220, 736, 345, 509,\n        293, 212, 234, 325, 616, 606, 589, 393, 528, 348, 102, 347,  61, 423,\n        705, 718, 696, 596], device='cuda:0')\ntensor([719, 343,  99, 192, 499, 674, 740, 723, 284, 122, 430, 501, 605, 109,\n         77, 113, 579, 537, 112, 171, 333, 545, 561,  85,  34, 341,  84, 406,\n        654,  20, 323, 316], device='cuda:0')\nTime taken: 14.64 sec.\n\n-----------------------------------\nEpoch: [2\/150]\ntensor([ 57, 159, 486, 182,  69, 625, 607, 370, 742, 559, 406, 271, 174, 173,\n        347, 338, 427, 743,  62,  74, 396, 185, 693, 603, 168, 731, 462, 266,\n         32, 143,  70, 122], device='cuda:0')\ntensor([ 68, 386, 648,  54, 456, 301, 366, 536, 204, 732, 523, 748, 481, 653,\n        435, 468, 155, 272, 141,  50, 690, 516, 440, 264, 429, 639, 258,  41,\n        539, 545, 205, 333], device='cuda:0')\n\n<\/code>\nNo target value is less than 0 or greater than 750. I have 751 classes.\nI don\u2019t understand how criterion will know about n_classes. It\u2019s neither defined during the initilization, criterion = nn.CrossEntropyLoss()  and the error happens during the first call of loss = criterion(outputs, targets)\nOn CPU, it gives this error\n<code class=\"lang-python\">  File \"train.py\", line 142, in train\n    loss_id = criterion(outputs, targets)\n  File \"\/home\/fstu1\/miniconda3\/lib\/python3.6\/site-packages\/torch\/nn\/modules\/module.py\", line 477, in __call__\n    result = self.forward(*input, **kwargs)\n  File \"\/home\/fstu1\/miniconda3\/lib\/python3.6\/site-packages\/torch\/nn\/modules\/loss.py\", line 862, in forward\n    ignore_index=self.ignore_index, reduction=self.reduction)\n  File \"\/home\/fstu1\/miniconda3\/lib\/python3.6\/site-packages\/torch\/nn\/functional.py\", line 1550, in cross_entropy\n    return nll_loss(log_softmax(input, 1), target, weight, None, ignore_index, None, reduction)\n  File \"\/home\/fstu1\/miniconda3\/lib\/python3.6\/site-packages\/torch\/nn\/functional.py\", line 1407, in nll_loss\n    return torch._C._nn.nll_loss(input, target, weight, _Reduction.get_enum(reduction), ignore_index)\nRuntimeError: Assertion `cur_target >= 0 &amp;&amp; cur_target < n_classes' failed.  at \/opt\/conda\/conda-bld\/pytorch_1532581333611\/work\/aten\/src\/THNN\/generic\/ClassNLLCriterion.c:93\n<\/code>\nThe number of classes is defined by the shape of your model\u2019s output.\nE.g. an output of the shape [batch_size, 10] will correspond to 10 different classes.\nTry to put an assert statement or any other check inside your training loop to check for invalid values in your target.\nok, thank you. my mistake, the output of the model was (batch_size, 512). I  had to change it to 751\nThank you"},{"x":"Hello fellows,\nI do have an issue to properly display images with CIFAR10 and PIL.\nMy dataset is loaded using torchvision.datasets as follow (mean and std values are from the CIFAR 10 standard mean and std values):\n        dataset=datasets.CIFAR10('CIFAR10', train=False, \n                   download=True,\n                   transform=transforms.Compose([\n                       transforms.ToTensor(),\n                       transforms.Normalize(mean=[0.485, 0.456, 0.406],\n                                            std=[0.229, 0.224, 0.225])\n                   ]))\n\nLater in my code, I want to unnormalize some image to save them on disk and visualize them later:\nI thus follow the definition of transforms.Normalize at https:\/\/pytorch.org\/docs\/master\/torchvision\/transforms.html#torchvision.transforms.Normalize to obtain my original images:\ndef convert_cifar10(t,pil):\n    \"\"\"Function to convert a cifar10 image tensor (already normalized)\n    onto a plotable image.\n    \n    :param t: image tensor of size (3,32,23)\n    :type t: torch.Tensor\n    :param pil: output is of size (3,32,32) if True, else (32,32,3)\n    :type pil: bool\n    \"\"\"\n    im = t.detach().cpu()\n    # approximate unnormalization \n    im[0] = im[0]*0.229 + 0.485\n    im[1] = im[1]*0.224 + 0.456\n    im[2] = im[2]*0.225 + 0.406\n    if not pil:\n        im = im.numpy()\n        im = np.transpose(im,(1,2,0))\n    return im\n\nThe minimal example is the following:\ndataiter = iter(dataset)\ndata, label = dataiter.next()\noriginal_img = convert_cifar10(data[0],pil=False)\nplt.imshow(original_img)\nplt.show()\nsecond = convert_cifar10(data[0],pil=False)\nplt.imshow(second)\nplt.show()\n\nThe first plot displays something normal (cannot diplay it as a new user, but they are for sure the original dataset images).\nThe second one is much less coloured:\nFigure_2.png956\u00d7241 8.13 KB with link \"https:\/\/discuss.pytorch.org\/uploads\/default\/original\/2X\/1\/1f0c057320a3d5a9e821da6a88d2682a4b56c2c9.png\"\nI may have applied twice the transformation on the input tensor, but the documentation explicitly states that the operation is made out of place:\nThis transform acts out of place, i.e., it does not mutates the input tensor.\nIs there anything I have misunderstood?\nThank you in advance ","y":"My mistake, I double checked my code today and I realized I used the .cpu() routine. According to the documentation at https:\/\/pytorch.org\/docs\/stable\/tensors.html#torch.Tensor.cpu:\nReturns a copy of this object in CPU memory. If this object is already in CPU memory and on the correct device, then no copy is performed and the original object is returned.\nSo I was just normalizing twice my inputs, since the second call returned me the already unnormalized image.\nA solution to work on a copy of the tensor is to use torch.Tensor.clone().","z":"My mistake, I double checked my code today and I realized I used the .cpu() routine. According to the documentation at https:\/\/pytorch.org\/docs\/stable\/tensors.html#torch.Tensor.cpu:\nReturns a copy of this object in CPU memory. If this object is already in CPU memory and on the correct device, then no copy is performed and the original object is returned.\nSo I was just normalizing twice my inputs, since the second call returned me the already unnormalized image.\nA solution to work on a copy of the tensor is to use torch.Tensor.clone()."},{"x":"I retained a densenet121 model for binary classification. please How do I save and reload this model to fit with the architecture and the saved parameters.\nThanks in advance","y":"You should you the model\u2019s state_dict to save and load it again. I\u2019d recommend you to take a look at this tutorial with link \"https:\/\/pytorch.org\/tutorials\/beginner\/saving_loading_models.html\".","z":"You should you the model\u2019s state_dict to save and load it again. I\u2019d recommend you to take a look at this tutorial with link \"https:\/\/pytorch.org\/tutorials\/beginner\/saving_loading_models.html\"."},{"x":"I\u2019m training my model but the prediction is being broadcasted over the whole tensor\n<code class=\"lang-auto\">\nclass CNN(nn.Module):\n    def __init__(self):\n        super(CNN, self).__init__()\n        self.conv1 = nn.Conv2d(1, 6, kernel_size=4, padding=1)\n        self.conv2 = nn.Conv2d(6, 12, kernel_size=4)\n        #self.conv2_drop = nn.Dropout2d()\n        #self.fc1 = nn.Linear(32* 193 * 7, args.output_dim* 2)\n        #self.fc2 = nn.Linear( args.output_dim* 2, args.output_dim)\"\"\n\n    def forward(self, x):\n        x = F.relu(F.max_pool2d(self.conv1(x), 2))\n        x = F.relu(F.max_pool2d(self.conv2(x), 2))\n        #x = x.view(-1, 1369720)\n        #x = F.relu(self.fc1(x))\n        #x = F.dropout(x,p = 0.4, training=self.training)\n        #x = self.fc2(x)\n        #return F.log_softmax(x, dim=1)\n        return x\n\n\nclass Combine(nn.Module):\n    def __init__(self):\n        super(Combine, self).__init__()\n        self.cnn = CNN()\n        self.rnn = nn.LSTM(\n            input_size=56856, \n            hidden_size=args.unit_dim, \n            num_layers=args.layer_dim,\n            batch_first=True)\n        self.linear = nn.Linear(args.unit_dim,args.output_dim)\n\n    def forward(self, x):\n        #print(x.size())\n        batch_size, C, H, W = x.size()\n        \n        timesteps = W\n        \n        c_in = x.view(batch_size , C, H, W)\n        c_out = self.cnn(c_in)\n        batch_size, C, H, W = c_out.size()\n        \n        timesteps = W\n        #print(c_out.shape)\n        r_in = c_out.view(batch_size,timesteps,  -1)\n        # Initialize hidden state with zeros\n        h0 = torch.zeros(args.layer_dim, r_in.size(0), args.unit_dim).requires_grad_().cuda()\n\n        # Initialize cell state\n        c0 = torch.zeros(args.layer_dim, x.size(0), args.unit_dim).requires_grad_().cuda()\n        r_out, (h_n, h_c) = self.rnn(r_in, (h0.detach(), c0.detach()))\n        r_out2 = self.linear(r_out[:, -1, :])\n        \n        return F.log_softmax(r_out2, dim=1)\n        #return r_out2\n\n\n\n  \nmodel = Combine()\nif args.cuda:\n    model.cuda()\n\noptimizer = optim.SGD(model.parameters(), lr=args.lr, momentum=args.momentum)\n# Cross Entropy Loss \ncriterion = nn.CrossEntropyLoss()\n#optimizer = torch.optim.Adam(model.parameters(), lr = args.lr)\ndef train(epoch):\n    model.train()\n    correct = 0\n    total_train = 0\n    correct_train = 0\n    for batch_idx, (data, target,length) in enumerate(train_loader):\n        \n        data = np.expand_dims(data, axis=1)\n        data = torch.FloatTensor(data)\n        if args.cuda:\n            data, target = data.cuda(), target.cuda()\n            \n\n        \n        data, target = data, target\n        optimizer.zero_grad()\n        \n        output = model(data)\n    \n        loss = F.nll_loss(output, torch.max(target.data, 1)[1])\n        \n        loss.backward()\n        optimizer.step()\n        \n        \n        pred = output.data.max(\n            1, keepdim=True)[1]  # get the index of the max log-probability\n        print('prediction: ',pred)\n        print('target: ',target.data.max(1, keepdim=True )[1])\n        correct += (pred.cpu() == torch.max(target.data, 1)[1].cpu()).sum()\n        if batch_idx % args.log_interval == 0:\n            print('Train Epoch: {} [{}\/{} ({:.0f}%)]\\tLoss: {:.6f}'.format(\n                epoch, batch_idx * len(data), len(train_loader.dataset),\n                100. * batch_idx \/ len(train_loader), loss.item()))\n\n\ndef test():\n    model.eval()\n    test_loss = 0\n    correct = 0\n    total = 0\n    for data, target, length in validation_loader:\n        \n        data = np.expand_dims(data, axis=1)\n        data = torch.FloatTensor(data)\n        print(target.size())\n        \n        if args.cuda:\n            data, target = data.cuda(), target.cuda()\n        data, target = data, target\n        output = model(data)\n        test_loss += F.nll_loss(\n            output, torch.max(target.data, 1)[1], size_average=False).item()  # sum up batch loss\n        pred = output.data.max(\n            1, keepdim=True)[1]  # get the index of the max log-probability\n        \n        total += target.size(0)\n        \n        correct += (pred.cpu() == target.data.max(\n            1, keepdim=True)[1] .cpu()).sum()\n        \n        print(pred)\n        print( torch.max(target.data, 1)[1])\n    total = target.size(0)\n    test_loss \/= len(validation_loader.dataset)\n    print(\n        '\\nTest set: Average loss: {:.4f}, Accuracy: {}\/{} ({:.0f}%)\\n'.format(\n            test_loss, correct, total,\n            100. * correct \/ total))\n    torch.save({\n            'drive\/My Drive\/epoch': args.epochs,\n            'drive\/My Drive\/model_state_dict': model.state_dict(),\n            'drive\/My Drive\/optimizer_state_dict': optimizer.state_dict(),\n            'drive\/My Drive\/loss': test_loss,\n          \n            }, 'drive\/My Drive\/weights.pth')\n\n\nfor epoch in range(1, args.epochs + 1):\n    train(epoch)\n    test()\n<\/code>\nand here\u2019s the output\n<code class=\"lang-auto\">prediction:  tensor([[4],\n        [4],\n        [4],\n        [4],\n        [4],\n        [4],\n        [4],\n        [4],\n        [4],\n        [4],\n        [4],\n        [4],\n        [4],\n        [4],\n        [4],\n        [4],\n        [4],\n        [4],\n        [4],\n        [4],\n        [4],\n        [4],\n        [4],\n        [4],\n        [4],\n        [4],\n        [4],\n        [4],\n        [4]], device='cuda:0')\ntarget:  tensor([[ 5],\n        [ 6],\n        [14],\n        [ 3],\n        [ 6],\n        [ 9],\n        [10],\n        [12],\n        [ 3],\n        [12],\n        [12],\n        [12],\n        [ 6],\n        [ 6],\n        [ 5],\n        [10],\n        [ 3],\n        [ 1],\n        [13],\n        [ 5],\n        [11],\n        [ 6],\n        [ 2],\n        [ 1],\n        [ 0],\n        [ 5],\n        [ 5],\n        [ 0],\n        [ 6]], device='cuda:0')\nTrain Epoch: 1 [0\/148 (0%)]\tLoss: 2.725649\nprediction:  tensor([[4],\n        [4],\n        [4],\n        [4],\n        [4],\n        [4],\n        [4],\n        [4],\n        [4],\n        [4],\n        [4],\n        [4],\n        [4],\n        [4],\n        [4],\n        [4],\n        [4],\n        [4],\n        [4],\n        [4],\n        [4],\n        [4],\n        [4],\n        [4],\n        [4],\n        [4],\n        [4],\n        [4],\n        [4]], device='cuda:0')\ntarget:  tensor([[ 7],\n        [ 5],\n        [ 1],\n        [ 9],\n        [13],\n        [11],\n        [ 0],\n        [ 9],\n        [ 4],\n        [ 3],\n        [10],\n        [ 1],\n        [10],\n        [ 2],\n        [12],\n        [ 1],\n        [ 9],\n        [ 0],\n        [10],\n        [ 8],\n        [13],\n        [ 7],\n        [ 8],\n        [ 9],\n        [ 4],\n        [ 9],\n        [13],\n        [ 7],\n        [11]], device='cuda:0')\nTrain Epoch: 1 [29\/148 (20%)]\tLoss: 2.724051\nprediction:  tensor([[4],\n        [4],\n        [4],\n        [4],\n        [4],\n        [4],\n        [4],\n        [4],\n        [4],\n        [4],\n        [4],\n        [4],\n        [4],\n        [4],\n        [4],\n        [4],\n        [4],\n        [4],\n        [4],\n        [4],\n        [4],\n        [4],\n        [4],\n        [4],\n        [4],\n        [4],\n        [4],\n        [4],\n        [4]], device='cuda:0')\ntarget:  tensor([[ 7],\n        [ 9],\n        [ 5],\n        [ 3],\n        [ 7],\n        [ 0],\n        [13],\n        [13],\n        [ 2],\n        [ 7],\n        [14],\n        [ 2],\n        [11],\n        [ 7],\n        [14],\n        [14],\n        [13],\n        [ 2],\n        [ 8],\n        [ 9],\n        [ 4],\n        [11],\n        [ 0],\n        [ 4],\n        [12],\n        [ 6],\n        [14],\n        [13],\n        [ 2]], device='cuda:0')\nTrain Epoch: 1 [58\/148 (40%)]\tLoss: 2.707235\nprediction:  tensor([[4],\n        [4],\n        [4],\n        [4],\n        [4],\n        [4],\n        [4],\n        [4],\n        [4],\n        [4],\n        [4],\n        [4],\n        [4],\n        [4],\n        [4],\n        [4],\n        [4],\n        [4],\n        [4],\n        [4],\n        [4],\n        [4],\n        [4],\n        [4],\n        [4],\n        [4],\n        [4],\n        [4],\n        [4]], device='cuda:0')\ntarget:  tensor([[ 2],\n        [14],\n        [ 8],\n        [11],\n        [ 6],\n        [ 7],\n        [ 4],\n        [14],\n        [ 8],\n        [ 9],\n        [12],\n        [ 2],\n        [ 3],\n        [ 2],\n        [ 3],\n        [ 1],\n        [10],\n        [12],\n        [ 0],\n        [ 3],\n        [12],\n        [10],\n        [10],\n        [ 1],\n        [ 4],\n        [13],\n        [ 8],\n        [ 8],\n        [12]], device='cuda:0')\nTrain Epoch: 1 [87\/148 (60%)]\tLoss: 2.686659\n<\/code>\ncan any one please tell me what\u2019s wrong with my code","y":"I assume the number of input features is 103.\nIn that case, your output is not being broadcasted, but apparently your model is currently overfitting to class4, since the output shape of [29, 1] matches r_out's shape after slicing and the linear layer.","z":"Could you print the shape or r_out before passing it to the linear layer?\nr_out is:  (tensor([[[ 0.0263, -0.0128, -0.0243,  \u2026, -0.0243, -0.0108,  0.0277],\n[ 0.0374, -0.0202, -0.0350,  \u2026, -0.0389, -0.0158,  0.0412],\n[ 0.0428, -0.0245, -0.0397,  \u2026, -0.0464, -0.0181,  0.0478],\n\u2026,\n[ 0.0823, -0.0406, -0.0463,  \u2026, -0.0200, -0.0264,  0.0489],\n[ 0.0881, -0.0432, -0.0463,  \u2026, -0.0086, -0.0246,  0.0496],\n[ 0.0923, -0.0446, -0.0466,  \u2026,  0.0030, -0.0218,  0.0502]],\n    [[ 0.0264, -0.0130, -0.0243,  ..., -0.0244, -0.0109,  0.0279],\n     [ 0.0375, -0.0203, -0.0352,  ..., -0.0390, -0.0160,  0.0412],\n     [ 0.0429, -0.0244, -0.0396,  ..., -0.0464, -0.0184,  0.0474],\n     ...,\n     [ 0.0835, -0.0419, -0.0457,  ..., -0.0199, -0.0265,  0.0497],\n     [ 0.0886, -0.0442, -0.0456,  ..., -0.0093, -0.0238,  0.0505],\n     [ 0.0927, -0.0456, -0.0455,  ...,  0.0015, -0.0202,  0.0511]],\n\n    [[ 0.0263, -0.0127, -0.0244,  ..., -0.0243, -0.0107,  0.0278],\n     [ 0.0376, -0.0198, -0.0356,  ..., -0.0390, -0.0157,  0.0411],\n     [ 0.0433, -0.0240, -0.0403,  ..., -0.0463, -0.0180,  0.0476],\n     ...,\n     [ 0.0835, -0.0395, -0.0475,  ..., -0.0185, -0.0253,  0.0494],\n     [ 0.0876, -0.0420, -0.0475,  ..., -0.0072, -0.0233,  0.0504],\n     [ 0.0913, -0.0440, -0.0485,  ...,  0.0036, -0.0202,  0.0512]],\n\n    ...,\n\n    [[ 0.0265, -0.0130, -0.0243,  ..., -0.0241, -0.0107,  0.0279],\n     [ 0.0379, -0.0203, -0.0350,  ..., -0.0385, -0.0157,  0.0414],\n     [ 0.0434, -0.0246, -0.0397,  ..., -0.0455, -0.0183,  0.0477],\n     ...,\n     [ 0.0823, -0.0416, -0.0494,  ..., -0.0207, -0.0257,  0.0505],\n     [ 0.0871, -0.0436, -0.0497,  ..., -0.0094, -0.0233,  0.0528],\n     [ 0.0911, -0.0451, -0.0493,  ...,  0.0019, -0.0201,  0.0527]],\n\n    [[ 0.0265, -0.0133, -0.0241,  ..., -0.0238, -0.0101,  0.0279],\n     [ 0.0376, -0.0205, -0.0352,  ..., -0.0381, -0.0145,  0.0416],\n     [ 0.0432, -0.0249, -0.0397,  ..., -0.0453, -0.0169,  0.0482],\n     ...,\n     [ 0.0844, -0.0405, -0.0468,  ..., -0.0213, -0.0263,  0.0502],\n     [ 0.0888, -0.0421, -0.0473,  ..., -0.0109, -0.0249,  0.0511],\n     [ 0.0924, -0.0438, -0.0476,  ...,  0.0009, -0.0214,  0.0524]],\n\n    [[ 0.0263, -0.0129, -0.0245,  ..., -0.0243, -0.0108,  0.0277],\n     [ 0.0375, -0.0202, -0.0354,  ..., -0.0389, -0.0158,  0.0410],\n     [ 0.0428, -0.0243, -0.0399,  ..., -0.0466, -0.0182,  0.0472],\n     ...,\n     [ 0.0829, -0.0420, -0.0473,  ..., -0.0196, -0.0278,  0.0496],\n     [ 0.0871, -0.0443, -0.0473,  ..., -0.0086, -0.0253,  0.0511],\n     [ 0.0909, -0.0455, -0.0477,  ...,  0.0026, -0.0223,  0.0528]]],\n   device='cuda:0', grad_fn=<CudnnRnnBackward>), (tensor([[[ 0.0200,  0.6911, -0.6832,  ..., -0.0039,  0.3721,  0.2176],\n     [ 0.0397,  0.7053, -0.6309,  ...,  0.0570,  0.3670,  0.2009],\n     [ 0.0348,  0.6988, -0.6598,  ...,  0.0264,  0.3939,  0.1875],\n     ...,\n     [ 0.0281,  0.6888, -0.5802,  ...,  0.0085,  0.4267,  0.2134],\n     [ 0.0193,  0.6896, -0.6339,  ...,  0.0306,  0.3935,  0.1763],\n     [ 0.0362,  0.6898, -0.6385,  ...,  0.0406,  0.3817,  0.1875]],\n\n    [[-0.1313, -0.0347,  0.1207,  ..., -0.1216, -0.0032,  0.1396],\n     [-0.1303, -0.0374,  0.1208,  ..., -0.1178, -0.0191,  0.1357],\n     [-0.1278, -0.0331,  0.1149,  ..., -0.1220, -0.0053,  0.1441],\n     ...,\n     [-0.1424, -0.0219,  0.1096,  ..., -0.1120, -0.0118,  0.1349],\n     [-0.1239, -0.0300,  0.1122,  ..., -0.1167, -0.0069,  0.1394],\n     [-0.1242, -0.0261,  0.1106,  ..., -0.1144, -0.0133,  0.1327]],\n\n    [[ 0.0923, -0.0446, -0.0466,  ...,  0.0030, -0.0218,  0.0502],\n     [ 0.0927, -0.0456, -0.0455,  ...,  0.0015, -0.0202,  0.0511],\n     [ 0.0913, -0.0440, -0.0485,  ...,  0.0036, -0.0202,  0.0512],\n     ...,\n     [ 0.0911, -0.0451, -0.0493,  ...,  0.0019, -0.0201,  0.0527],\n     [ 0.0924, -0.0438, -0.0476,  ...,  0.0009, -0.0214,  0.0524],\n     [ 0.0909, -0.0455, -0.0477,  ...,  0.0026, -0.0223,  0.0528]]],\n   device='cuda:0', grad_fn=<CudnnRnnBackward>), tensor([[[ 3.5384e-01,  1.0493e+00, -2.2504e+00,  ..., -8.9872e-03,\n       9.8480e-01,  1.0078e+00],\n     [ 5.4717e-01,  1.0476e+00, -2.1838e+00,  ...,  1.3820e-01,\n       9.4552e-01,  1.0322e+00],\n     [ 6.0499e-01,  1.0538e+00, -2.4513e+00,  ...,  6.8737e-02,\n       1.0223e+00,  9.5667e-01],\n     ...,\n     [ 5.0454e-01,  1.0370e+00, -2.2494e+00,  ...,  2.2548e-02,\n       9.9235e-01,  1.0062e+00],\n     [ 2.7099e-01,  1.0496e+00, -2.2780e+00,  ...,  8.2996e-02,\n       9.9398e-01,  9.6303e-01],\n     [ 4.7176e-01,  1.0196e+00, -2.3526e+00,  ...,  9.2828e-02,\n       9.9370e-01,  1.0548e+00]],\n\n    [[-2.7644e-01, -7.5817e-02,  3.0098e-01,  ..., -2.6721e-01,\n      -7.3597e-03,  3.1150e-01],\n     [-2.7400e-01, -8.2529e-02,  3.0173e-01,  ..., -2.5754e-01,\n      -4.3918e-02,  3.0245e-01],\n     [-2.6914e-01, -7.1011e-02,  2.8584e-01,  ..., -2.6781e-01,\n      -1.2103e-02,  3.2070e-01],\n     ...,\n     [-2.9461e-01, -4.7481e-02,  2.6912e-01,  ..., -2.4372e-01,\n      -2.7129e-02,  3.0116e-01],\n     [-2.5839e-01, -6.4935e-02,  2.7611e-01,  ..., -2.5553e-01,\n      -1.5848e-02,  3.0867e-01],\n     [-2.6162e-01, -5.6842e-02,  2.7326e-01,  ..., -2.5025e-01,\n      -3.0504e-02,  2.9524e-01]],\n\n    [[ 1.8682e-01, -8.5577e-02, -8.7757e-02,  ...,  6.2227e-03,\n      -4.3620e-02,  1.0997e-01],\n     [ 1.8689e-01, -8.7545e-02, -8.5590e-02,  ...,  3.0909e-03,\n      -4.0423e-02,  1.1175e-01],\n     [ 1.8515e-01, -8.4282e-02, -9.1440e-02,  ...,  7.4527e-03,\n      -4.0361e-02,  1.1215e-01],\n     ...,\n     [ 1.8369e-01, -8.6465e-02, -9.2885e-02,  ...,  3.8705e-03,\n      -4.0138e-02,  1.1528e-01],\n     [ 1.8638e-01, -8.3885e-02, -8.9513e-02,  ...,  1.8936e-03,\n      -4.2861e-02,  1.1478e-01],\n     [ 1.8395e-01, -8.7310e-02, -9.0022e-02,  ...,  5.3197e-03,\n      -4.4569e-02,  1.1564e-01]]], device='cuda:0',\n   grad_fn=<CudnnRnnBackward>)))\nI meant the shape of the tensor.\nYou can get it using print(r_out.shape).\naha I\u2019m sorry\nr_out shape is:  torch.Size([29, 44, 103])\nI assume the number of input features is 103.\nIn that case, your output is not being broadcasted, but apparently your model is currently overfitting to class4, since the output shape of [29, 1] matches r_out's shape after slicing and the linear layer.\nAha thank you so much then I should I add  drop out to avoid the overfitting  right!\nIt might help.\nHowever, this also looks as if you are dealing with an imbalanced dataset. Is that also the case, i.e. do you have a lot more class4 samples than others?\nI\u2019m dealing with video dataset each class with 10 observations"},{"x":"Hello i\u2019m trying assign list of strings into tensor but it gives me this error\nthis is the code\n\n<code class=\"lang-auto\"># Merge captions (from tuple of 1D tensor to 2D tensor).\nlengths = [len(cap) for cap in captions]\n    targets = torch.zeros(len(captions), max(lengths)).long()\n    for i, cap in enumerate(captions):\n        end = lengths[i]\n        targets[i, :end] = cap[:end]\n<\/code>","y":"for people who might want the answer I converted the string to it\u2019s unicode and stored it in the tensor\nhere\u2019s the answer\n<code class=\"lang-auto\">targets[i, :end] = torch.from_numpy(np.array(list(map(ord, cap[:end])))).to(torch.long)\n<\/code>","z":"for people who might want the answer I converted the string to it\u2019s unicode and stored it in the tensor\nhere\u2019s the answer\n<code class=\"lang-auto\">targets[i, :end] = torch.from_numpy(np.array(list(map(ord, cap[:end])))).to(torch.long)\n<\/code>"},{"x":"This idea comes to be randomly.\nOne of the advantage of CNN is the fact that it captures the locality in the input space through receptive fields.\nIf the input data does not have such relationship among features,\nWould DNN provide better results?\nIs there any previous study on this? I would like to take a look","y":"Hi Brandon!\n\n\n\n ljj7975:\n\n\u2026\nOne of the advantage of CNN is the fact that it captures the locality in the input space through receptive fields.\nIf the input data does not have such relationship among features,\nWould DNN provide better results?\n\u2026\n\n\nFirst, by \u201cCNN\u201d I assume you mean a \u201cconvolutional neural\nnetwork,\u201d that is, a network whose first few layers consist of\nconvolutions.  And by \u201cnaive DNN\u201d I assume you mean a\nnetwork consisting of fully-connected layers.\nYes, your expectation is correct \u2013 a CNN will not work well\non a problem whose input doesn\u2019t have \u201clocality.\u201d\nLet\u2019s say that your input consists of 1024 values, and that\nthe relationship between, say, inputs #7 and #505 is just as\nlikely to carry useful information as the relationship between\ninputs #7 and #8.  The constrained structure of the CNN \u2013 its\n\u201cassumption\u201d of locality \u2013 is going to make it difficult for the\nnetwork to discover the relationship between inputs #7 and\n#505.\n(You can implement a convolutional layer as a fully-connected\nlayer where the fully-connected weight matrix consists of a\nbunch of copies of the convolutional block, with zeros elsewhere.\nThe weights that would mix inputs #7 and #505 together and\npotentially discover these inputs\u2019 relationships will be zero in\nthis case \u2013 so no mixing, at least in this layer.)\nAlso note that in addition to this constraint of locality, a\nconvolutional layer also has translational invariance.  (That\nis, the convolutional block is translated unchanged across\nthe input, which is to say, it is convolved with the  input.)\nFor example, let\u2019s say your input is a 2-d image.  We expect\nthat edges within the image that form potentially useful\nfeatures have more or less the same structure and meaning\nin the lower left corner of the image as in the upper right.\nOr we may expect to be able to detect the image of a dog\nequally well in the lower left and upper right corners of the\nimage.\nIf the local, translationally-invariant structure of the convolutional\nlayers match the structure of the input data, then you come out\nahead.  If it doesn\u2019t match, then you come out behind, and you\nare better off training fully-connected layers that can learn the\n(non-local, non-translationally-invariant) structure of the input\ndata.\nBest.\nK. Frank","z":"Hi Brandon!\n\n\n\n ljj7975:\n\n\u2026\nOne of the advantage of CNN is the fact that it captures the locality in the input space through receptive fields.\nIf the input data does not have such relationship among features,\nWould DNN provide better results?\n\u2026\n\n\nFirst, by \u201cCNN\u201d I assume you mean a \u201cconvolutional neural\nnetwork,\u201d that is, a network whose first few layers consist of\nconvolutions.  And by \u201cnaive DNN\u201d I assume you mean a\nnetwork consisting of fully-connected layers.\nYes, your expectation is correct \u2013 a CNN will not work well\non a problem whose input doesn\u2019t have \u201clocality.\u201d\nLet\u2019s say that your input consists of 1024 values, and that\nthe relationship between, say, inputs #7 and #505 is just as\nlikely to carry useful information as the relationship between\ninputs #7 and #8.  The constrained structure of the CNN \u2013 its\n\u201cassumption\u201d of locality \u2013 is going to make it difficult for the\nnetwork to discover the relationship between inputs #7 and\n#505.\n(You can implement a convolutional layer as a fully-connected\nlayer where the fully-connected weight matrix consists of a\nbunch of copies of the convolutional block, with zeros elsewhere.\nThe weights that would mix inputs #7 and #505 together and\npotentially discover these inputs\u2019 relationships will be zero in\nthis case \u2013 so no mixing, at least in this layer.)\nAlso note that in addition to this constraint of locality, a\nconvolutional layer also has translational invariance.  (That\nis, the convolutional block is translated unchanged across\nthe input, which is to say, it is convolved with the  input.)\nFor example, let\u2019s say your input is a 2-d image.  We expect\nthat edges within the image that form potentially useful\nfeatures have more or less the same structure and meaning\nin the lower left corner of the image as in the upper right.\nOr we may expect to be able to detect the image of a dog\nequally well in the lower left and upper right corners of the\nimage.\nIf the local, translationally-invariant structure of the convolutional\nlayers match the structure of the input data, then you come out\nahead.  If it doesn\u2019t match, then you come out behind, and you\nare better off training fully-connected layers that can learn the\n(non-local, non-translationally-invariant) structure of the input\ndata.\nBest.\nK. Frank\nThank you very much !\nIt\u2019s the best answer"},{"x":"I\u2019m using VGG16 pre-trained model as my base model. However, I need to add a new layer (quantizing layer) before every convolution layer in VGG 16. How can I add this new layer before conv layers if I want to keep the value of VGG16 parameters (weights\/Biases)?","y":"It\u2019s a kinda tedious task, but you have to map the layer\u2019s weights now, like what was the later in the actual VGG16 and what\u2019s in your own implementation, just access the .weight\/.bias attribute and load the weights from the pretrained version to yours\u2026","z":"you need to write your own VGG16 then (better) and add the intermediate layers plus loading the weights for those specific original vgg layers as well?\nThank you for your help.\nthe name of layers parameters are changed in my VGG16 model. How can I load parameter from pretrained VGG16 model with different parameter names ?\nHere is pretrained parameter names\n\n<table>\n<thead>\n<tr>\n<th>features.0.weight<\/th>\n<th>torch.Size([64, 3, 3, 3])<\/th>\n<\/tr>\n<\/thead>\n<tbody>\n<tr>\n<td>features.0.bias            torch.Size([64])<\/td>\n<td><\/td>\n<\/tr>\n<tr>\n<td>features.2.weight        torch.Size([64, 64, 3, 3])<\/td>\n<td><\/td>\n<\/tr>\n<tr>\n<td>features.2.bias            torch.Size([64])<\/td>\n<td><\/td>\n<\/tr>\n<tr>\n<td>features.5.weight        torch.Size([128, 64, 3, 3])<\/td>\n<td><\/td>\n<\/tr>\n<tr>\n<td>features.5.bias            torch.Size([128])<\/td>\n<td><\/td>\n<\/tr>\n<tr>\n<td>features.7.weight        torch.Size([128, 128, 3, 3])<\/td>\n<td><\/td>\n<\/tr>\n<tr>\n<td>features.7.bias            torch.Size([128])<\/td>\n<td><\/td>\n<\/tr>\n<tr>\n<td>features.10.weight      torch.Size([256, 128, 3, 3])<\/td>\n<td><\/td>\n<\/tr>\n<\/tbody>\n<\/table>\nAnd Here my VGG16 parameter names - Step size is the name of parameter in my quantization layer\n|features.0.step_size | torch.Size([1])|\n|features.1.weight         torch.Size([64, 3, 3, 3])|\n|features.1.bias             torch.Size([64])|\n|features.3.step_size    torch.Size([1])|\n|features.4.weight         torch.Size([64, 64, 3, 3])|\n|features.4.bias             torch.Size([64])|\n|features.7.step_size    torch.Size([1])|\n|features.8.weight         torch.Size([128, 64, 3, 3])|\n|features.8.bias             torch.Size([128])|\n|features.10.step_size  torch.Size([1])|\nIt\u2019s a kinda tedious task, but you have to map the layer\u2019s weights now, like what was the later in the actual VGG16 and what\u2019s in your own implementation, just access the .weight\/.bias attribute and load the weights from the pretrained version to yours\u2026"},{"x":"Hi. I am going over the transfer learning tutorial with ants and bees, and I am coming from Keras with no experience in Pytorch.\nIs there a built-in equivalent of model.summary in pytorch?\nI found this medium article but it requires installation of another framework:\n\n\n\nTowards Data Science \u2013 27 Sep 18 with link \"https:\/\/towardsdatascience.com\/model-summary-in-pytorch-b5a1e4b64d25\"\n\n\nModel summary in PyTorch with link \"https:\/\/towardsdatascience.com\/model-summary-in-pytorch-b5a1e4b64d25\"\nKeras style model.summary() in PyTorch\nReading time: 3 min read\n\n\n\n\n\n\nJust wondering if that is indeed what I should be doing.\nThank you.","y":"you can use modelsummary which is a python library \uff0c\u6709\u4e2apython\u5305\uff0c\u662f\u53ef\u4ee5\u7684\uff0cgithub\uff1ahttps:\/\/github.com\/graykode\/modelsummary","z":"you can use modelsummary which is a python library \uff0c\u6709\u4e2apython\u5305\uff0c\u662f\u53ef\u4ee5\u7684\uff0cgithub\uff1ahttps:\/\/github.com\/graykode\/modelsummary"},{"x":"I am going through the ant bees transfer learning tutorial, and I am trying to get a deep understanding of preparing data in Pytorch. I removed all of the transformations except ToTensor, but it seems\nyou need to make sure images need to be resized?\nSo I am trying this:\n\ntrain_data = ImageFolder(root = os.path.join(root_dir, \u2018train\u2019),    transform=transforms.Compose([Resize(256),ToTensor()]))\ntrain_loader = iter(dataloader)\nx,y = next(train_loader)\n\nHowever I am still getting a runtime Error:\n`Sizes of tensors must match except in dimension 0. Got 371 and 375 in dimension 2 at ..\\aten    \\src\\TH\/generic\/THTensor.cpp:711`\n\nPreformatted textIs there another transformation I need to add?\nPS: Is there no preview button on these boards? I am trying out different formating options but I can\u2019t seem to find a way to preview the options. Thank you. .","y":"If size is a sequence like (h, w), output size will be matched to this.\nIf size is an int, smaller edge of the image will be matched to this number. i.e, if height > width, then image will be rescaled to (size * height \/ width, size)\nI think this is the problem. Instead of Resize(256) try, Resize(256,256). This will alter the aspect ratio. But, first check and let us know the outcome.","z":"If size is a sequence like (h, w), output size will be matched to this.\nIf size is an int, smaller edge of the image will be matched to this number. i.e, if height > width, then image will be rescaled to (size * height \/ width, size)\nI think this is the problem. Instead of Resize(256) try, Resize(256,256). This will alter the aspect ratio. But, first check and let us know the outcome.\nYes that seems to be case. Thank you so much."},{"x":"Hi,\nI have a custom_transform variable defined as follows:\n<code class=\"lang-python\">custom_transforms = Compose([\n    RandomResizedCrop(size=224, scale=(0.8, 1.2)),\n    RandomRotation(degrees=(-30, 30)),\n    RandomHorizontalFlip(p=0.5),\n    ToTensor(),\n    Normalize((0.5, 0.5, 0.5), (0.5, 0.5, 0.5)),\n    RandomNoise(p=0.5, mean=0, std=0.1)])\n<\/code>\nI use this to apply transformations on input images through __getitem__ method. I have 3 images in each sample returned by __getitem__.\nThe first image is the target image which I generate it dynamically from input and my input is the ground truth. Finally, the third image is also generated using some modification on the target image.\nNow the question is, I do not want to use RandomNoise and Normalize on the second and third images, but will same random transforms like Crop, Flip, etc happen on ground truth?\nI have read some issues on Github and it seems random seed resets somehow.\nHere is what I have done:\n<code class=\"lang-python\">transform_gt = Compose(custom_transform.transforms[:-1])\n<\/code>\nThanks for any advices","y":"The problem solved using feeding same seed value before applying each Compose of transforms.\n\n\ngithub.com\/pytorch\/vision with link \"https:\/\/github.com\/pytorch\/vision\/issues\/9#issuecomment-304224800\"\n\n\n with link \"https:\/\/github.com\/fmassa\"\nIssue: Random transforms for both input and target? with link \"https:\/\/github.com\/pytorch\/vision\/issues\/9#issuecomment-304224800\"\n\n\n\topened by fmassa with link \"https:\/\/github.com\/fmassa\"\n\ton 2016-11-28 with link \"https:\/\/github.com\/pytorch\/vision\/issues\/9#issuecomment-304224800\"\n\n\n\tclosed by fmassa with link \"https:\/\/github.com\/fmassa\"\n\ton 2017-09-03 with link \"https:\/\/github.com\/pytorch\/vision\/issues\/9#issuecomment-304224800\"\n\n\nIn some scenarios (like semantic segmentation), we might want to apply the same random transform to both the input and the...\n\n\n\n\n\n\n\n<code class=\"lang-auto\">def __getitem__(self,index):      \n        img = Image.open(self.data[index]).convert('RGB')\n        target = Image.open(self.data_labels[index])\n        \n        seed = np.random.randint(2147483647) # make a seed with numpy generator \n        random.seed(seed) # apply this seed to img tranfsorms\n        if self.transform is not None:\n            img = self.transform(img)\n            \n        random.seed(seed) # apply this seed to target tranfsorms\n        if self.target_transform is not None:\n            target = self.target_transform(target)\n\n        target = torch.ByteTensor(np.array(target))\n    \n        return img, target\n<\/code>\nBy the way, it works completely fine on a subset of transforms.","z":"Githhub transformations applies transformations frame-wise. Thus, different transformations will be applied to both gt and images. It\u2019s not a matter of seed rather than the way torchvision is done. https:\/\/github.com\/JuanFMontesinos\/flerken\/blob\/master\/flerken\/dataloaders\/transforms\/transforms.py\nThis file reimplements transforms such that you can apply same transformation to a list a frames.\nAbout subsets try to apply common part in one transform and specific parts in other transforms\nI tried this code using your transforms.py:\n<code class=\"lang-python\">custom_transforms = Compose([\n    RandomRotation(degrees=(-30, 30)),\n    RandomHorizontalFlip(p=0.5),\n])\n\ni1 = Image.open('dataset\/sub_test\/data\/Places365_val_00000002.jpg')\ni2 = Image.open('dataset\/sub_test\/data\/Places365_val_00000003.jpg')\n\nit12 = custom_transforms([i1, i2])\n<\/code>\nBut I got these errors:\n<code class=\"lang-python\"> File \"C:\\Users\\NIkan\\Desktop\\Deep Halftoning\\Github\\Deep-Halftoning\\lib\\transforms.py\", line 69, in __call__\n    return self.apply_sequence(inpt)\n  File \"C:\\Users\\NIkan\\Desktop\\Deep Halftoning\\Github\\Deep-Halftoning\\lib\\transforms.py\", line 79, in apply_sequence\n    output = list(map(self.apply_img, seq))\n  File \"C:\\Users\\NIkan\\Desktop\\Deep Halftoning\\Github\\Deep-Halftoning\\lib\\transforms.py\", line 75, in apply_img\n    img = t(img)\n  File \"C:\\Users\\NIkan\\Desktop\\Deep Halftoning\\Github\\Deep-Halftoning\\lib\\transforms.py\", line 990, in __call__\n    self.get_params()\nTypeError: get_params() missing 1 required positional argument: 'degrees'\n<\/code>\nIt seems it calls self.get_params() without passing provided degrees.\n\n\ngithub.com with link \"https:\/\/github.com\/JuanFMontesinos\/flerken\/blob\/dae0d5f33e1b3d7cf3dc7c159ffec326179f9f8f\/flerken\/dataloaders\/transforms\/transforms.py#L971-L990\"\n\n\nJuanFMontesinos\/flerken\/blob\/dae0d5f33e1b3d7cf3dc7c159ffec326179f9f8f\/flerken\/dataloaders\/transforms\/transforms.py#L971-L990 with link \"https:\/\/github.com\/JuanFMontesinos\/flerken\/blob\/dae0d5f33e1b3d7cf3dc7c159ffec326179f9f8f\/flerken\/dataloaders\/transforms\/transforms.py#L971-L990\"\n<code class=\"lang-py\">\ndef get_params(self, degrees):\n    \"\"\"Get parameters for ``rotate`` for a random rotation.\n\n\n    Returns:\n        sequence: params to be passed to ``rotate`` for random rotation.\n    \"\"\"\n    self.angle = random.uniform(degrees[0], degrees[1])\n\n\ndef reset_params(self):\n    self.angle = None\n\n\ndef __call__(self, img):\n    \"\"\"\n        img (PIL Image): Image to be rotated.\n\n\n    Returns:\n        PIL Image: Rotated image.\n    \"\"\"\n    if self.angle is None:\n        self.get_params()\n<\/code>\n\n\n\n\n\nThe problem solved using feeding same seed value before applying each Compose of transforms.\n\n\ngithub.com\/pytorch\/vision with link \"https:\/\/github.com\/pytorch\/vision\/issues\/9#issuecomment-304224800\"\n\n\n with link \"https:\/\/github.com\/fmassa\"\nIssue: Random transforms for both input and target? with link \"https:\/\/github.com\/pytorch\/vision\/issues\/9#issuecomment-304224800\"\n\n\n\topened by fmassa with link \"https:\/\/github.com\/fmassa\"\n\ton 2016-11-28 with link \"https:\/\/github.com\/pytorch\/vision\/issues\/9#issuecomment-304224800\"\n\n\n\tclosed by fmassa with link \"https:\/\/github.com\/fmassa\"\n\ton 2017-09-03 with link \"https:\/\/github.com\/pytorch\/vision\/issues\/9#issuecomment-304224800\"\n\n\nIn some scenarios (like semantic segmentation), we might want to apply the same random transform to both the input and the...\n\n\n\n\n\n\n\n<code class=\"lang-auto\">def __getitem__(self,index):      \n        img = Image.open(self.data[index]).convert('RGB')\n        target = Image.open(self.data_labels[index])\n        \n        seed = np.random.randint(2147483647) # make a seed with numpy generator \n        random.seed(seed) # apply this seed to img tranfsorms\n        if self.transform is not None:\n            img = self.transform(img)\n            \n        random.seed(seed) # apply this seed to target tranfsorms\n        if self.target_transform is not None:\n            target = self.target_transform(target)\n\n        target = torch.ByteTensor(np.array(target))\n    \n        return img, target\n<\/code>\nBy the way, it works completely fine on a subset of transforms.\nI just rewrote everything so there may be minor issues\nOw, sorry.\nThanks for your help."},{"x":"Hi,\nI am doing an experiment where I use the output of intermediate values of batchnorm layer from resnet50. So, I forward hook at different layers and work on the output. But, the weird thing I observed is the min value of the batchnorm output clamps to zero at all the layers. To reproduce the result I wrote a toy program:\n<code class=\"lang-auto\">import torch\nfrom torchvision import models, datasets\nimport torchvision\nimport datasets\nfrom tqdm import tqdm, trange\n\n\nactivation_student = {}\ndef get_activation_student(name):\n    def hook(self, input_, output):\n        activation_student[name]=  output.data\n    return hook\n\namazonData = datasets.amazon_load('train', 16)\nmodel = models.resnet50(pretrained=True)\nfor n, m in model.named_modules():\n    if n == 'layer2.1.bn1' or 'layer3.2.bn2' or 'layer4.1.bn3':\n        m.register_forward_hook(get_activation_student(n))\n\nfor i in trange(5, leave=False):\n    source_x, source_y = next(iter(amazonData))\n    out = model(source_x)\n    print(\"min: \", torch.min(activation_student['layer2.1.bn1']))\n    print(\"min: \", torch.min(activation_student['layer3.2.bn2']))\n    print(\"min: \", torch.min(activation_student['layer4.1.bn3']))\n\n<\/code>\noutput:\nmin: tensor(0.)\nmin: tensor(0.)\nmin: tensor(0.)\ncan someone help me in fixing this, or is this expected ?","y":"The output of the Bottleneck layer might be misleading:\n<code class=\"lang-python\">model.layer2[1]\nOut[42]: \nBottleneck(\n  (conv1): Conv2d(512, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)\n  (bn1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n  (conv2): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n  (bn2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n  (conv3): Conv2d(128, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)\n  (bn3): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n  (relu): ReLU(inplace)\n)\n<\/code>\nThis output does not mean, that these modules will be executed sequentially, as they are not wrapped in an nn.Sequential module.\nIn fact the output just lists all modules as they were initialized in __init__, while the forward method defines the execution.\nAs you can see from the link I\u2019ve posted, self.relu is used three times inside the Bottleneck module (every time operating on the activation inplace).","z":" any idea?\nP.S. I apologize for tagging.\nThe activation values are a bit tricky in this use case.\nAs you can see in this line of code with link \"https:\/\/github.com\/pytorch\/vision\/blob\/8a64dbcd2ecb5d55b45e1946dd8dac56307e60be\/torchvision\/models\/resnet.py#L96\", the Bottleneck module uses self.relu directly after the batch norm layers.\nSince self.relu is defined as an inplace operation, the output of the batchnorm layers will be manipulated inplace, which is why you actually see the \u201coutput\u201d of the self.relu layer.\nHi,\nThanks for your reply. I have gone through the docs. Had it been \u2018layer2.1.bn3\u2019 then I am convinced with your reason as there is a relu operation following it but after the batchnorm layer \u2018layer2.1.bn1\u2019 there is no relu operation. In resnet \u201cconv-bn-conv-bn-conv-bn-relu\u201d is the pattern followed and I am looking at the output of the first batchnorm where there is no ReLU(inplace=True). I hope I made it clear ?\nBut still, I get minimum value as \u201c0\u201d ? or am I missing something ?\nThe output of the Bottleneck layer might be misleading:\n<code class=\"lang-python\">model.layer2[1]\nOut[42]: \nBottleneck(\n  (conv1): Conv2d(512, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)\n  (bn1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n  (conv2): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n  (bn2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n  (conv3): Conv2d(128, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)\n  (bn3): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n  (relu): ReLU(inplace)\n)\n<\/code>\nThis output does not mean, that these modules will be executed sequentially, as they are not wrapped in an nn.Sequential module.\nIn fact the output just lists all modules as they were initialized in __init__, while the forward method defines the execution.\nAs you can see from the link I\u2019ve posted, self.relu is used three times inside the Bottleneck module (every time operating on the activation inplace).\nThat was very clear and to the point. Thanks a lot.\nOne final question, is there a way I could hook somehow and get the output of the batchnorm layer and not the relu output ? I cannot use conv layer\u2019s output because my loss function involves a margin  where I can set the margin to one std. dev if only I get output of batchnorm layer.\nThanks again.\nThe clean approach would be to derive a custom class from ResNet and use vanilla nn.ReLU modules instead of the inplace version.\nThe hacky way would be to just replace the \u201cunwanted\u201d inplace ReLUs:\n<code class=\"lang-python\">model = models.resnet50(pretrained=True)\nmodel.layer2[1].relu = nn.ReLU()\nmodel.layer3[2].relu = nn.ReLU()\nmodel.layer4[1].relu = nn.ReLU()\nfor n, m in model.named_modules():\n    if n == 'layer2.1.bn1' or 'layer3.2.bn2' or 'layer4.1.bn3':   \n        m.register_forward_hook(get_activation_student(n))\n<\/code>\n Thanks my issue is solved. I will mark the above comment as solution."},{"x":"I\u2019m using Pytorch version 1.1.0 installed with fast.ai library and trying to get to know more about the framework about how it works.\nI\u2019ve been going through the blitz tutorial and on the one with training a classifier with CIFAR10 dataset, I\u2019m stuck at a certain point where we use the Data Loader on the training sample :\n<code class=\"lang-auto\"># get some random training images\ndataiter = iter(trainloader)\nimages, labels = dataiter.next()\n<\/code>\nWhen I want to run this code, it gives me the following error :\n<code class=\"lang-auto\">TypeError: object() takes no parameters\n<\/code>\nI don\u2019t understand why it does this error, I\u2019ve tried to uninstall and re-install yet it still keep doing the same error","y":"Thank you for your answer!\nThe code comes from the official PyTorch training a classifier tutorial here with link \"https:\/\/pytorch.org\/tutorials\/beginner\/blitz\/cifar10_tutorial.html#sphx-glr-beginner-blitz-cifar10-tutorial-py\"\nEDIT : Just found the mistake\u2026\nIn the code below, I\u2019ve not put \u201c()\u201d after the function ToTensor\n<code class=\"lang-auto\">transform = transforms.Compose([transforms.ToTensor, \n                               transforms.Normalize((0.5,0.5,0.5), (0.5,0.5,0.5))])\n<\/code>\nWhich led to the TypeError. But I\u2019ve got confused as the error pointed at the lines I\u2019ve mentioned before on my first post.\nSorry for the inconvenience and thank you for your answer!","z":"Welcome to the PyTorch community! \nIt\u2019s a bit hard to tell the context without seeing the rest of your code. Also, I\u2019m not sure what kind of wrapping fast.ai has done, but this could be coming from their library.\nThe typical way to interact with datasets in PyTorch looks like something like this:\n<code class=\"lang-auto\">>>> mnist = MNIST(r\"some root directory\", download=True, train=True, transform=ToTensor())\n>>> data = DataLoader(mnist)\n>>> for num, (input, label) in enumerate(data):\n...     print(num)\n<\/code>\ndata in this case, an instance of the DataLoader class is itself an iterator. You can use normal python looping constructs on this. You shouldn\u2019t have to call .next() directly.\nThank you for your answer!\nThe code comes from the official PyTorch training a classifier tutorial here with link \"https:\/\/pytorch.org\/tutorials\/beginner\/blitz\/cifar10_tutorial.html#sphx-glr-beginner-blitz-cifar10-tutorial-py\"\nEDIT : Just found the mistake\u2026\nIn the code below, I\u2019ve not put \u201c()\u201d after the function ToTensor\n<code class=\"lang-auto\">transform = transforms.Compose([transforms.ToTensor, \n                               transforms.Normalize((0.5,0.5,0.5), (0.5,0.5,0.5))])\n<\/code>\nWhich led to the TypeError. But I\u2019ve got confused as the error pointed at the lines I\u2019ve mentioned before on my first post.\nSorry for the inconvenience and thank you for your answer!"},{"x":"Hello,\nI am very new to pytorch and DNNs in general.\nI\u2019ve created a classifier on top of a pretrained densenet161, to classify images of flowers, into the groups: daisy(0), dandelion(1), rose(2), sunflower(3) and tulip(4).\nThe training-process works fine and in order to test the model I went on implementing the example from chaper 5 of the tutorial from the PyTorch site with link \"https:\/\/pytorch.org\/tutorials\/beginner\/blitz\/cifar10_tutorial.html\".\nAll this seems to work, the classifier predicts a defined set of images with an acceptable performance but the next thing I wanted to do, was to test it with a single image.\nThe first step was to simply set the batch-size to one but at this point the output of my network is wrong.\n\nThis is the output tensor of the working example, the index of these classes is mapped like shown above.\nAfter this worked, i tried to load a single image like this:\n<code class=\"lang-auto\">test_transforms = transforms.Compose([\n                          transforms.Resize((224,224)), \n                          transforms.ToTensor(),\n                          transforms.Normalize([0.485, 0.456, 0.406],\n                             [0.229, 0.224, 0.225])])\ndef make_image(image):\n    image_tensor = test_transforms(image)\n    image_tensor.unsqueeze_(0)\n    return image_tensor\n\nimg = make_image(Image.open(\"path-to-a-sunflower\"))\noutput = torch.exp(model.forward(img)) # exp because of LogSoftMax as output layer \nprint(output)\nval, pos = out.topk(1,dim=1)\n<\/code>\nVia the topk I wanted to get the position of the highest value in order to display the class.\nThe output of the model was:\ntensor([[0.1126, 0.2525, 0.1020, 0.0674, 0.4655]], grad_fn=)\nwhich means that it has classified the picture as a tulip.\nThe strange thing is, that if I try the same images within a dataloader with identical transformation, the results are just right.\nI have no idea where to start with this problem, so I wonder if there is a reason for this behavior and, in the best case a solution.\nTo provide more information I will post some of the Code here, I hope this helps.\nClassifier, Criterion and Optimizer:\n<code class=\"lang-auto\">   model.classifier = nn.Sequential(nn.Linear(model.classifier.in_features, 512),\n                                     nn.ReLU(),\n                                     nn.Dropout(0.2),\n                                     nn.Linear(512, 265),\n                                     nn.ReLU(),\n                                     nn.Linear(265,5),\n                                     nn.LogSoftmax(dim=1))\n    criterion = nn.NLLLoss()\n    optimizer = optim.Adam(model.classifier.parameters(), lr=0.003)\n<\/code>\nThe Epoch Loop\n<code class=\"lang-auto\">for epoch in range(epochs):\n        for inputs, labels in tqdm_notebook(trainloader,desc=\"Training batches\"):\n            steps += 1\n            inputs, labels = inputs.to(device), labels.to(device)\n            optimizer.zero_grad()\n            logps = model.forward(inputs)\n            loss = criterion(logps, labels)\n            loss.backward()\n            optimizer.step()\n            running_loss += loss.item()\n\n            if steps % print_every == 0:\n                test_loss = 0\n                accuracy = 0\n                model.eval()\n                with torch.no_grad():\n                    for inputs, labels in tqdm_notebook(testloader,desc=\"Testing batches\",leave = False):\n                        inputs, labels = inputs.to(device), labels.to(device)\n                        logps = model.forward(inputs)\n                        batch_loss = criterion(logps, labels)\n                        test_loss += batch_loss.item()\n\n                        ps = torch.exp(logps)\n                        top_p, top_class = ps.topk(1, dim=1)\n                        equals = top_class == labels.view(*top_class.shape)\n                        accuracy += torch.mean(equals.type(torch.FloatTensor)).item()\n                # plotting\n                train_losses.append(running_loss \/ len(trainloader))\n                test_losses.append(test_loss \/ len(testloader))\n                print(f\"Epoch {epoch + 1}\/{epochs}.. \"\n                  f\"Train loss: {running_loss \/ print_every:.3f}.. \"\n                  f\"Test loss: {test_loss \/ len(testloader):.3f}.. \"\n                  f\"Test accuracy: {accuracy \/ len(testloader):.3f}\")\n                running_loss = 0\n                model.train()\n<\/code>\nI hope these informations could describe my problem and I appreciate any idea concerning this topic. Thank you very much in advance.","y":"Did you also call model.eval() before testing the single image?","z":"Did you also call model.eval() before testing the single image?\nThat does the trick!\nIn my first model I didn\u2019t do that but maybe it had not that much of an impact than in the model here, since I use a dropout-layer.\nThank you very much for your answer!\nMany freshmen just started using pytorch and didn\u2019t know to use model.train() and model.eval().  If you train the model, you can add a model.train() in front. If you test the model, you must add a model.eval()."},{"x":"Hey all! I\u2019m working with a dataset for segmentation that has your typical RGB images as well as index-based masks (so the pixel values in the mask represent the classes at those positions), and I\u2019m wondering what the best way to train this dataset would be with a simple U-Net. Would it be better to do standard CrossEntropy loss using the index masks, or would it be better to split the masks up into one hot vector format and use BCELoss with each channel? Or is there a better loss for this type of dataset entirely? I\u2019ve seen dice loss, but no efficient implementations for multi-class scenarios.","y":"Hello Yuerno -\n\n\n\n Yuerno:\n\nHey all! I\u2019m working with a dataset for segmentation that has your typical RGB images as well as index-based masks (so the pixel values in the mask represent the classes at those positions), and I\u2019m wondering what the best way to train this dataset would be with a simple U-Net. Would it be better to do standard CrossEntropy loss using the index masks, or would it be better to split the masks up into one hot vector format and use BCELoss with each channel? Or is there a better loss for this type of dataset entirely? I\u2019ve seen dice loss, but no efficient implementations for multi-class scenarios.\n\n\nConventional wisdom and common practice suggest that you\nshould use CrossEntropyLoss for a multi-class classification\nproblem.  Reading between the lines, I understand your\nsegmentation problem to be a straightforward (multi-label)\nclassification problem, so I would think that CrossEntropyLoss\nwould be your preferred choice.\nAlso, if you propose using BCELoss, to be concrete, you need\nto specify how you intend to combine the BCELoss for each\nof the classes together to get a single loss function to optimize.\nLastly, people argue that dice loss works well (better than\nCrossEntropyLoss?) when your classes are imbalanced, but\ndice loss \u2013 its gradients, etc. \u2013 are mathematically unwieldy,\nand this is a practical disadvantage.\nI would recommend that you start with CrossEntropyLoss,\nand only if that isn\u2019t working well on your problem (or maybe\nif your classes are highly imbalanced) also try dice loss and\ncompare its performance to that which you achieved with\nCrossEntropyLoss.  (I don\u2019t think you should use just dice loss\nwithout comparing it to CrossEntropyLoss.)\n(I\u2019m not aware of any scheme for combining together multiple\nBCELosses that people prefer to CrossEntropyLoss.)\nGood luck.\nK. Frank","z":"Hello Yuerno -\n\n\n\n Yuerno:\n\nHey all! I\u2019m working with a dataset for segmentation that has your typical RGB images as well as index-based masks (so the pixel values in the mask represent the classes at those positions), and I\u2019m wondering what the best way to train this dataset would be with a simple U-Net. Would it be better to do standard CrossEntropy loss using the index masks, or would it be better to split the masks up into one hot vector format and use BCELoss with each channel? Or is there a better loss for this type of dataset entirely? I\u2019ve seen dice loss, but no efficient implementations for multi-class scenarios.\n\n\nConventional wisdom and common practice suggest that you\nshould use CrossEntropyLoss for a multi-class classification\nproblem.  Reading between the lines, I understand your\nsegmentation problem to be a straightforward (multi-label)\nclassification problem, so I would think that CrossEntropyLoss\nwould be your preferred choice.\nAlso, if you propose using BCELoss, to be concrete, you need\nto specify how you intend to combine the BCELoss for each\nof the classes together to get a single loss function to optimize.\nLastly, people argue that dice loss works well (better than\nCrossEntropyLoss?) when your classes are imbalanced, but\ndice loss \u2013 its gradients, etc. \u2013 are mathematically unwieldy,\nand this is a practical disadvantage.\nI would recommend that you start with CrossEntropyLoss,\nand only if that isn\u2019t working well on your problem (or maybe\nif your classes are highly imbalanced) also try dice loss and\ncompare its performance to that which you achieved with\nCrossEntropyLoss.  (I don\u2019t think you should use just dice loss\nwithout comparing it to CrossEntropyLoss.)\n(I\u2019m not aware of any scheme for combining together multiple\nBCELosses that people prefer to CrossEntropyLoss.)\nGood luck.\nK. Frank\nHi Frank! Thanks so much for this detailed response, it really helped clarify things for me. Your assessment in reading between the lines was absolutely correct, so I think I\u2019ll be sticking with just regular CrossEntropyLoss for now and see how that goes."},{"x":"Does group norm maintain an running average of mean and variance ?","y":"Looking at the code here: https:\/\/pytorch.org\/docs\/stable\/_modules\/torch\/nn\/modules\/normalization.html\nNeither group norm nor layer norm seem to maintain running averages. The description of them suggests they might: https:\/\/pytorch.org\/docs\/stable\/nn.html?highlight=group%20norm#torch.nn.GroupNorm\n\u201cthis layer uses statistics computed from input data in both training and evaluation modes\u201d\nWhether or not they are supposed to I don\u2019t know. I don\u2019t see running averages in the tensorflow version of group norm either: https:\/\/github.com\/tensorflow\/tensorflow\/blob\/r1.13\/tensorflow\/contrib\/layers\/python\/layers\/normalization.py (group_norm)\nOr layer norm for that matter:\nhttps:\/\/github.com\/tensorflow\/tensorflow\/blob\/r1.13\/tensorflow\/contrib\/layers\/python\/layers\/layers.py (layer_norm)\nAs both compute the mean and std for the batch dim, i.e the mean\u2019s shape is (N, 1) in layer norm, tracking a running average doesn\u2019t make sense. Who is to say something similar will be at that exact position in your validation batch?","z":"Looking at the code here: https:\/\/pytorch.org\/docs\/stable\/_modules\/torch\/nn\/modules\/normalization.html\nNeither group norm nor layer norm seem to maintain running averages. The description of them suggests they might: https:\/\/pytorch.org\/docs\/stable\/nn.html?highlight=group%20norm#torch.nn.GroupNorm\n\u201cthis layer uses statistics computed from input data in both training and evaluation modes\u201d\nWhether or not they are supposed to I don\u2019t know. I don\u2019t see running averages in the tensorflow version of group norm either: https:\/\/github.com\/tensorflow\/tensorflow\/blob\/r1.13\/tensorflow\/contrib\/layers\/python\/layers\/normalization.py (group_norm)\nOr layer norm for that matter:\nhttps:\/\/github.com\/tensorflow\/tensorflow\/blob\/r1.13\/tensorflow\/contrib\/layers\/python\/layers\/layers.py (layer_norm)\nAs both compute the mean and std for the batch dim, i.e the mean\u2019s shape is (N, 1) in layer norm, tracking a running average doesn\u2019t make sense. Who is to say something similar will be at that exact position in your validation batch?"},{"x":"I have encountered a problem about the forward propagation, the full code is as follows:\nI want to accurately record the runing time of nn.conv2d function, where time1 is about\n0.00009901s, but time2 is about 0.00011235s. As can be seen in the code, the convolution flops is identical,  I don\u2019t know what cause the time difference with time1 and time2.\n<code class=\"lang-auto\">import os\nos.environ[\"CUDA_VISIBLE_DEVICES\"] = '7'\n\nimport torch\nimport torch.nn as nn\nfrom torch.autograd import Variable\n\nimport time\nimport numpy as np\nimport random\n\ndef main():\n    \n    test_input1 = Variable(torch.rand(1,36,56,56)).cuda()\n    test_input2 = Variable(torch.rand(1,64,56,56)).cuda()\n    m1 = torch.nn.Conv2d(36, 36, kernel_size=3, stride=1, padding=1).cuda()\n\n    time1_list = []\n    for i in range(10000):\n        torch.cuda.synchronize()\n        t1 =time.time()\n        temp_out = m1(test_input1)\n        torch.cuda.synchronize()\n        t2=time.time()\n        time1_list.append(t2-t1)\n    print('time1:%.8f' %(sum(time1_list[50:])\/len(time1_list[50:])))\n\n\n\n    output = Variable(torch.zeros(1, 64, 56, 56)).cuda()\n    k_in_mask = torch.from_numpy(np.array(random.sample(range(0,64), 36))).cuda()\n    k_out_mask = torch.from_numpy(np.array(random.sample(range(0,64), 36))).cuda()\n    \n    time2_list = []\n    for i in range(10000):\n        temp_in = torch.index_select(test_input2, 1, Variable(k_in_mask))\n       \n        torch.cuda.synchronize()\n        t1=time.time()\n        temp_out = m1(temp_in)\n        torch.cuda.synchronize()\n        t2=time.time()\n        time2_list.append(t2-t1)\n        \n        output.index_copy(1, Variable(k_out_mask), temp_out)\n    print('time2:%.8f' %(sum(time2_list[50:])\/len(time2_list[50:])))\n\nif __name__=='__main__':\n    main()\n<\/code>","y":"Unfortunately, GPUs are very good at doing large simple operations like element-wise ops or mm but very bad at smart things like indexing.\nFor the time noise, I don\u2019t see any way around this, as I said, this is most likely some hardware quirks and I don\u2019t know GPU internals enough to have an idea why this happens ","z":"Hi,\nI\u2019m not sure what is the reason for this small difference.\nWhy do you repeatedely perform the index select\/copy in the loop? It might be adding some noise?\nI run 10000 times just to reduce the randomness of the model. I find that when I run just few times (about 3 times), the runing time several times longer than the normal runing time, so I run 10000 times and abandon the first 50 times to obbtain a more accurate time. But as shown on the code, the flops of time1 and time2 are identical, but the runing time of these two function is different, I don\u2019t know what brings additional time cost ?\nHi,\nThe problem is that the two following lines:\n<code class=\"lang-auto\">temp_in = torch.index_select(test_input2, 1, Variable(k_in_mask))\n# and\noutput.index_copy(1, Variable(k_out_mask), temp_out)\n<\/code>\nAdd some noise to the runtime computation.\nIf you put them outside of the loop, the runtime goes back to the same thing.\nIf you replace them with other ops line temp_in = temp_in + 1, you will see the same noise.\nI also find this phenomenon. But my project is as follows:\n<code class=\"lang-auto\">temp_in = torch.index_select(test_input2, 1, Variable(k_in_mask))\nother functions\noutput.index_copy(1, Variable(k_out_mask), temp_out)\n<\/code>\nAnd I do need to record the real runtime, is there any way to calculate the time ?\nThans so much for your reply !\nI think that you hit a case where there is no \u201creal runtime\u201d: doing other ops will influence the runtime a little for some reason. The difference is fairly small and very dependant on the hardware you use: I tried on a Titan Black and both runtime are almost exactly the same.\nPS: You don\u2019t need to use Variables anymore, you can just remove all of them as every Tensor is a Variable now.\nalbanD, thanks so much!\nI tested the runtime on Tesla M40 before, and I used the Variable just because my code was written based on Pytorch 0.3.1.\nDo you mean that the operation on Tensor instead of variable and maybe try other machine such as Titan Black can get the \u2018real runtime\u2019 (or just make the difference fairly small) ?\nI do need to calculate the \u2018real runtime\u2019 to verify the validity of my algorithm.\nI mean that the difference is too small to be meaningful for your algorithm: This is most likely due to some hardware details.\nThe \u201creal runtime\u201d is whatever time it actually takes to run. But that might vary depending on your hardware\/software versions and if other things are runnning on your machine. So it\u2019s a very subjective thing and there is no true \u201creal runtime\u201d.\nRight! I know what you mean, I just want to make the convolution time of time1 and time2 to be identical, then I can reduce the addtional time cost brounght by the\n<code class=\"lang-auto\">temp_in = torch.index_select(test_input2, 1, Variable(k_in_mask))\n<\/code>\nand\n<code class=\"lang-auto\">output.index_copy(1, Variable(k_out_mask), temp_out)\n<\/code>\nMaybe this is meaningless\uff0c because I just want to use the index_select, small convolution and index_copy to replace large convolution, but I don\u2019t find that the small convolution brings additional noise time cost.\nMaybe the operation is not so meaningful\nI am sorry to bother you again !\nAs for the  index_select  and  index_copy, which cost too much addtional time, is there any other way to accelerate this two process ?\nIf you just do one of them you can\u2019t really do much better.\nUnless you can remove them altogether and apply your function to the full input and ignore some of the results.\nIt\u2019s a very sad story, thank you for your answer anyway.\nRegarding the time noise, is this problem also no solution?\nUnfortunately, GPUs are very good at doing large simple operations like element-wise ops or mm but very bad at smart things like indexing.\nFor the time noise, I don\u2019t see any way around this, as I said, this is most likely some hardware quirks and I don\u2019t know GPU internals enough to have an idea why this happens \nThank you so much ! ! !"},{"x":"Hi everyone,\nI am using the pertained models from torchvision and there are 3 cases: feature extraction, fine-tuning pre-trained model, and optimise it yourself.\nFirst case, to extract feature from images:\n\nset model=resnet18(pretrained=True)\nset param.requires_grad = False\nand optimise only the Fully Connected layer, the last one.\n\nSecond case, to fine-tune pre-trained model:\n\nset model=resnet18(pretrained=True)\nset param.requires_grad = True\nand optimise Convolutional, Pooling and Fully Connected layers\n\nThird case, to optimise model from scratch :\n\nset model=resnet18(pretrained=False)\nset param.requires_grad = True\nand optimise Convolutional, Pooling and Fully Connected layers\n\nFrom my understanding, when you optimise parameters in CNN, you do it for Convolution, Pooling and Fully Connected layers. I can not get the difference between the 2nd and 3d cases. For me in both cases you optimise all layers.\nThank you for your considered time.","y":"You are fine-tuning all parameters using the pretrained weights as the initial parameters.\nThe difference is that you won\u2019t initialize them randomly using a torch.nn.init method, but start from the already pretrained parameters.\nIf you are using a similar dataset to the one used for pretraining, the model parameters will be already quite good. Changing all parameters \u201ca bit\u201d will fine-tune the model so that it hopefully works better for your new dataset.","z":"The number of layers you are training in case of fine-tuning depends on the amount of data you have and also how similar the new dataset is to the one used for pretraining (ImageNet in case of torchvision).\nHave a look at the Fine-tuning notes of CS231n with link \"http:\/\/cs231n.github.io\/transfer-learning\/\" for a good overview.\nptrblck,\nThank you for your response.\nI read this article about transfer learning and I understood how it is working. My question was how different is case 2 and and case 3.\nMaybe I expressed my question in incomprehensible way. In other words, how it is possible to use pre-trained model and optimise parameters of all layers setting \u201cset param.requires_grad = True\u201d ? If you optimise params of all layers you train your model from scratch so you set \u201cpretrained=False\u201d.\nYou are fine-tuning all parameters using the pretrained weights as the initial parameters.\nThe difference is that you won\u2019t initialize them randomly using a torch.nn.init method, but start from the already pretrained parameters.\nIf you are using a similar dataset to the one used for pretraining, the model parameters will be already quite good. Changing all parameters \u201ca bit\u201d will fine-tune the model so that it hopefully works better for your new dataset.\nptrblck,\nAs always great answer! Now, it is clear.\nThanks for your time and have a great day "},{"x":"Theoretically, a loss function should return 0 when target=prediction, and some positive number when target and prediction and different. In the simple example below, when the prediction=target, I am getting a loss value of -1, and when prediction and target are different, I am getting a loss value of 0. How are these values calculated? What is the intuition behind these values?\nScreenshot from 2019-04-22 21-16-33.png1820\u00d7318 29.4 KB with link \"https:\/\/discuss.pytorch.org\/uploads\/default\/original\/2X\/b\/b7310a7fac7714c5935049ed06d3211f95a10dc5.png\"\nI also tried passing the log_softmax of input, but then also, I am getting a loss value of 2.1232. Again, intuitively, the value of loss in this case should be 0.\nimage.png1132\u00d7519 30.3 KB with link \"https:\/\/discuss.pytorch.org\/uploads\/default\/original\/2X\/2\/23409a9c445d09d9c515d317144fe0ad245170a7.png\"\nWhat is the intuition\/reason behind these loss values?","y":"You will get a loss close to zero (in this case you\u2019ll see the print outputs an exact zero, although the value might just underflow), if the logit for the target class is very high:\n<code class=\"lang-python\">x = torch.ones(1, 21) * (-10000)\nx[0, 3] = 10000\ntarget = torch.tensor([3])\n\nloss = F.nll_loss(F.log_softmax(x, 1), target)\nprint(loss)\n> tensor(0.)\n<\/code>","z":"F.log_softmax expects logits which might have arbitrarily large and small numbers.\nIf you just call F.softmax on your input tensor, you\u2019ll see that the probability is not exactly one for the desired class:\n<code class=\"lang-python\">x = torch.zeros(1, 21)\nx[0, 3] = 1.\nprint(F.softmax(x, 1))\n> tensor([[0.0440, 0.0440, 0.0440, 0.1197, 0.0440, 0.0440, 0.0440, 0.0440, 0.0440,\n         0.0440, 0.0440, 0.0440, 0.0440, 0.0440, 0.0440, 0.0440, 0.0440, 0.0440,\n         0.0440, 0.0440, 0.0440]])\n<\/code>\nAs you can see, class3 has a probability of 0.1197, so you\u2019ll get a positive loss.\n\n\n\n ptrblck:\n\nF.log_softmax expects logits which might have arbitrarily large and small numbers.\n\n\nAhh, got it.\n\n\n\n ptrblck:\n\nAs you can see, class3 has a probability of 0.1197 , so you\u2019ll get a positive loss.\n\n\nAfter applying F.nll_loss to this softmax output, I get a loss value of -0.1197 which is close to 0 but not exactly zero. Shouldn\u2019t loss functions return exactly zero when prediction equals target like in this case?\nYou will get a loss close to zero (in this case you\u2019ll see the print outputs an exact zero, although the value might just underflow), if the logit for the target class is very high:\n<code class=\"lang-python\">x = torch.ones(1, 21) * (-10000)\nx[0, 3] = 10000\ntarget = torch.tensor([3])\n\nloss = F.nll_loss(F.log_softmax(x, 1), target)\nprint(loss)\n> tensor(0.)\n<\/code>\n\n\n\n ptrblck:\n\nif the logit for the target class is very high\n\n\nGreat, understood it! This also forces the network to output larger logit value for the corresponding neuron to reduce the loss during backprop!\nThanks for the explanation "},{"x":"I checked a video explanation of batch norm and the focal point of that video is the idea that we \u201cput our data on known or standard scale\u201d. This is often scale [0, 1] as in the next image:\nimage.jpg1029\u00d7512 209 KB with link \"https:\/\/discuss.pytorch.org\/uploads\/default\/original\/2X\/7\/700976acfc840187fb0b382a2d5d724ba3904367.jpeg\"\nOn the other hand it is unclear to me what is the standard scale for BatchNorm1d. Is this the gamma parameter (\u03b3)?\nimage.png1073\u00d7591 51.5 KB with link \"https:\/\/discuss.pytorch.org\/uploads\/default\/original\/2X\/4\/4f34ee1b33411faf0a5a6690f101fd2b34822b60.png\"","y":"\n\n\n Intel_Novel:\n\nWith affine=False this is not the case, in the example with link \"https:\/\/pytorch.org\/docs\/master\/nn.html?highlight=batch%20norm#torch.nn.BatchNorm1d\" it states \u201cwithout learnable parameters\u201d so we have some predefined (hardcoded) batch normalization.\n\n\nThe mean and variance are not hardcoded. Both will be initialized and updated during training, i.e. the current batch will be normalized using its mean and variance, so that mean=0 and var=1, while the running_mean and running_var will be updated using the momentum term. During evaluation (calling model.eval()), the running estimates will be used.\n\n\n\n Intel_Novel:\n\nI am not sure, but the only parameter from the example m = nn.BatchNorm1d(100) is 100. Is this the batch size?\n\n\nNo, it\u2019s the number if features, e.g. the number of channels of the output activation of a conv layer. The running estimates will thus have the same dimension, i.e. for 100 channels the batch norm layer will have 100 running_mean and running_var values.\n\n\n\n Intel_Novel:\n\nI analyzed further even without learning parameters the output from a batch norm will have standard deviation of 1 and mean of 0 which rises the question of why do we need learnable parameters at the first place?\n\n\nIt depends on your model, data etc., like so many things.\nAs I tried to explain, the affine parameters might \u201cundo\u201d the standardization, if it would be the ideal activation output.\nFrom the BatchNorm paper with link \"https:\/\/arxiv.org\/pdf\/1502.03167.pdf\":\n\nNote that simply normalizing each input of a layer maychange what the layer can represent.  For instance,  nor-malizing the inputs of a sigmoid would constrain them tothe linear regime of the nonlinearity.  To address this, wemake sure thatthe transformation inserted in the networkcan represent the identity transform.  To accomplish this, we introduce, for each activationx(k), a pair of parameters\u03b3(k), \u03b2(k), which scale and shift the normalized value:y(k)=\u03b3(k)\u0302x(k)+\u03b2(k).These  parameters  are  learned  along  with  the  originalmodel parameters,  and  restore the  representation powerof the network. Indeed, by setting\u03b3(k)=\u221aVar[x(k)] and \u03b2(k)=E[x(k)], we could recover the original activations,if that were the optimal thing to do.\n","z":"BatchNorm will standardize the data by default using the mean and variance (or running mean and running variance during eval). The affine parameters gamma and beta (weight and bias in nn.BatchNorm2d, respectively) allow this layer to undo the standardization or to manipulate it in any way beneficial for the model and training.\nE.g. if standardizing the activation might not be beneficial for the training, gamma might be pushed towards the value of the standard deviation, and beta might be learned to be close to the mean value of the data, which will eventually create an identity layer.\nLet me know, if this makes things clearer or if I\u2019ve missed some points.\nYou are somehow close enough, but let me explain what I am particularly unclear.\nThe batch normalization applies to a layer that we can represent as a tensor of activations A. Values of A range somewhere between [r1, r2] this means that all the activations are in that interval.\nAfter the batch norm which is just a transformation of the activations A, we will get activations tensor B. What would be the range of the B?\nIf you are using the affine parameters, you can\u2019t limit the value range of the output activations, since e.g. a high weight value might scale the values to an arbitrary large range.\nHowever, if you set affine=False, during training each batch will be standardized using your formula, which might create a standard normal distribution. The values would therefore be determined by the 68-95-99.7 rule with link \"https:\/\/en.wikipedia.org\/wiki\/68%E2%80%9395%E2%80%9399.7_rule\". So you will not limit the range to particular values. However, take these explanations with a grain of salt, as the distribution of your input activations might be completely different.\nI am assuming with affine=True, the batch norm should be capable of learning all 4 parameters exactly. The mean, the standard deviation, the gamma, and the beta.\nWith affine=False this is not the case, in the example with link \"https:\/\/pytorch.org\/docs\/master\/nn.html?highlight=batch%20norm#torch.nn.BatchNorm1d\" it states \u201cwithout learnable parameters\u201d so we have some predefined (hardcoded) batch normalization.\nI am not sure, but the only parameter from the example m = nn.BatchNorm1d(100) is 100. Is this the batch size?\nThis is modified example from before\u2026\n<code class=\"lang-auto\">import torch\nimport torch.nn as nn\n\n# With Learnable Parameters\nm = nn.BatchNorm1d(10)\n# Without Learnable Parameters\nm = nn.BatchNorm1d(10, affine=False)\ninput =  1000* torch.randn(3, 10)\nprint(input)\noutput = m(input)\nprint(output)\n<\/code>\nAnd it provides the output:\n<code class=\"lang-auto\">tensor([[  553.9385,   358.0948,  -311.0905,   946.6582,   320.4365,  1158.4661,\n           610.9046,  -300.3665,  -730.4023,  -432.1760],\n        [ -428.9000,  -373.9978,   304.2072,  -230.9816,   246.2242,   757.4435,\n          -489.5371, -2545.6099,  2042.6073,  -763.0421],\n        [ -350.7178,  1166.3325,  -511.8971, -1168.5955,  1719.1627,   -95.6929,\n          1275.7457,  2684.2368, -1186.4659, -1935.6162]])\ntensor([[ 1.4106, -0.0403, -0.3979,  1.2684, -0.6516,  1.0550,  0.1995, -0.1150,\n         -0.5413,  0.9479],\n        [-0.7929, -1.2041,  1.3742, -0.0925, -0.7612,  0.2882, -1.3122, -1.1632,\n          1.4021,  0.4350],\n        [-0.6177,  1.2444, -0.9763, -1.1759,  1.4128, -1.3431,  1.1128,  1.2782,\n         -0.8609, -1.3829]])\n<\/code>\nLooks like it will try to squeeze the outputs to (-1,1) range with as few as possible elements out of that range.\nAny comments?\nThe idea all 4 parameters can be learnable I found in video: https:\/\/youtu.be\/dXB-KQYkzNU?t=270\nimage.jpg508\u00d7576 138 KB with link \"https:\/\/discuss.pytorch.org\/uploads\/default\/original\/2X\/e\/eed22940caf31d95c8477fb45ba6e5257a818df1.jpeg\"\n as I analyzed further even without learning parameters the output from a batch norm will have standard deviation of 1 and mean of 0 which rises the question of why do we need learnable parameters at the first place?\n<code class=\"lang-auto\">import torch\nimport torch.nn as nn\n\n# Without Learnable Parameters\nm = nn.BatchNorm1d(10, affine=False)\ninput =  1000* torch.randn(3, 10)\nprint(input)\noutput = m(input)\nprint(output)\nprint(output.mean()) # will be close to 0\nprint(output.std()) # will be close to 1\n<\/code>\n\n\n\n Intel_Novel:\n\nWith affine=False this is not the case, in the example with link \"https:\/\/pytorch.org\/docs\/master\/nn.html?highlight=batch%20norm#torch.nn.BatchNorm1d\" it states \u201cwithout learnable parameters\u201d so we have some predefined (hardcoded) batch normalization.\n\n\nThe mean and variance are not hardcoded. Both will be initialized and updated during training, i.e. the current batch will be normalized using its mean and variance, so that mean=0 and var=1, while the running_mean and running_var will be updated using the momentum term. During evaluation (calling model.eval()), the running estimates will be used.\n\n\n\n Intel_Novel:\n\nI am not sure, but the only parameter from the example m = nn.BatchNorm1d(100) is 100. Is this the batch size?\n\n\nNo, it\u2019s the number if features, e.g. the number of channels of the output activation of a conv layer. The running estimates will thus have the same dimension, i.e. for 100 channels the batch norm layer will have 100 running_mean and running_var values.\n\n\n\n Intel_Novel:\n\nI analyzed further even without learning parameters the output from a batch norm will have standard deviation of 1 and mean of 0 which rises the question of why do we need learnable parameters at the first place?\n\n\nIt depends on your model, data etc., like so many things.\nAs I tried to explain, the affine parameters might \u201cundo\u201d the standardization, if it would be the ideal activation output.\nFrom the BatchNorm paper with link \"https:\/\/arxiv.org\/pdf\/1502.03167.pdf\":\n\nNote that simply normalizing each input of a layer maychange what the layer can represent.  For instance,  nor-malizing the inputs of a sigmoid would constrain them tothe linear regime of the nonlinearity.  To address this, wemake sure thatthe transformation inserted in the networkcan represent the identity transform.  To accomplish this, we introduce, for each activationx(k), a pair of parameters\u03b3(k), \u03b2(k), which scale and shift the normalized value:y(k)=\u03b3(k)\u0302x(k)+\u03b2(k).These  parameters  are  learned  along  with  the  originalmodel parameters,  and  restore the  representation powerof the network. Indeed, by setting\u03b3(k)=\u221aVar[x(k)] and \u03b2(k)=E[x(k)], we could recover the original activations,if that were the optimal thing to do.\n\nOK, I got more than enough information. I have some intriguing questions, but I will ask them in another thread."},{"x":"Hi, im using a resnet34 on the stanford car dataset(https:\/\/www.kaggle.com\/jutrera\/stanford-car-dataset-by-classes-folder).\nThe things that are happening\n\nTraining loss is decreasing, training acc is increasing\nTest accuracy stays the same throughout the training, doesnt matter how many epochs\n\nWith about 5 epochs the models peaks to 90% acc and then doesn\u2019t increase that much.\nBut the validation accuracy is still around 4%, doesn\u2019t matter when i measure it (1, 2, \u2026, 6 epochs)\nCan somebody help me find a problem with my code or what i might improve upon?\n<code class=\"lang-auto\">import torch\nimport torch.nn as nn\nimport torch.optim as optim\nimport torchvision\nimport torchvision.models as models\nimport torchvision.transforms as transforms\nimport time\ndevice = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\nprint(device)\n\ntfms = transforms.Compose([transforms.Resize((300, 300)),\n                           transforms.ToTensor()])\n\ndataset = torchvision.datasets.ImageFolder(root=\"..\/..\/Data\/car_data2\/train\", transform = tfms)\ntrainloader = torch.utils.data.DataLoader(dataset, batch_size = 16, shuffle=True)\n\ndataset2 = torchvision.datasets.ImageFolder(root=\"..\/..\/Data\/car_data2\/test\", transform = tfms)\ntestloader = torch.utils.data.DataLoader(dataset2, batch_size = 16, shuffle=False)\n\ndef train_model(model, criterion, optimizer, n_epochs = 5):\n    \n    for epoch in range(n_epochs):\n        running_loss = 0.0\n        running_correct = 0.0\n        for i, data in enumerate(trainloader, 0):\n\n            # get the inputs\n            inputs, labels = data\n            inputs = inputs.to(device)\n            labels = labels.to(device)\n            optimizer.zero_grad()\n            \n            # forward + backward + optimize\n            outputs = model(inputs)\n            _, predicted = torch.max(outputs.data, 1)\n            loss = criterion(outputs, labels)\n            loss.backward()\n            optimizer.step()\n            \n            running_loss += loss.item()\n            running_correct += (labels==predicted).sum().item()\n            \n            if i % 10 == 9:    # print every 10 mini-batches (change according to data)\n                print('[Epoch %d, %5d \/ %5d] loss: %.3f acc: %.3f' %\n                      (epoch + 1, i + 1, len(trainloader), running_loss \/ 10, (100\/16)*running_correct\/10))\n                running_loss = 0.0\n                running_correct = 0.0\n\n    print('Finished Training')\n    return model\n\nmodel_ft = models.resnet34(pretrained=True)\nnum_ftrs = model_ft.fc.in_features\nmodel_ft.fc = nn.Linear(num_ftrs, 196)\n\nmodel_ft = model_ft.to(device)\n\ncriterion = nn.CrossEntropyLoss()\n\noptimizer = optim.SGD(model_ft.parameters(), lr=0.001, momentum=0.9)\n\nmodel_ft = train_model(model_ft, criterion, optimizer, n_epochs=5)\n\n# Test the model\ncorrect = 0\ntotal = 0\nwith torch.no_grad():\n    for i, data in enumerate(testloader):\n        images, labels = data\n        images = images.to(device)\n        labels = labels.to(device)\n        outputs = model_ft(images)\n        _, predicted = torch.max(outputs.data, 1)\n        total += labels.size(0)\n        correct += (predicted == labels).sum().item()\nprint('Accuracy of the network on the test images: %d %%' % (\n    100 * correct \/ total))\n<\/code>","y":"Could you try to add model.eval() before testing your model?","z":"Could you try to add model.eval() before testing your model?\nYES! It seemed to work, may i ask why that is? I understand it disables dropout and things amongst other things. So when im using a net that has no batch norm\/dropout, model.eval() isnt necessary?\nBasically me using a resnet requires the model.eval()?\nBut besides that, thank you very much, it really helped me, been wondering about it for days :).\nmodel.eval() might change all modules using self.training internally to change the behavior.\nUsually it\u2019s used in nn.Dropout and nn.BatchNorm layers.\nYes, ResNet uses batch norm layers, so you should call model.eval() before testing the model."},{"x":"I previously used https:\/\/github.com\/christiansafka\/img2vec to create resnet50 feature vectors for each of my categories to create some statistics. Now I am noticing running the same code throws me error. I am rather much confused how to fix it as it was working and I have changed nothing (and it is the same version of PyTorch as before 0.4)\n<code class=\"lang-auto\">import numpy as np\nfrom numpy import linalg as LA\n\ninfile1 = np.loadtxt('T1_resnet50_feature_vectors.txt')\ninfile1_reshaped = infile1.reshape(2048, 21)\ncov1 = np.cov(infile1_reshaped, rowvar=False)\n<\/code>\ninfile1.shape is (38, 2048)\nI get error for reshaping but I need reshaping for calculating the covariance.\nThe code in github is using torch.\n<code class=\"lang-auto\">Traceback (most recent call last):\n  File \"feature_vectors_statistics.py\", line XYZ, in <module>\n    infile1_reshaped = infile1.reshape(2048, 21)\nValueError: cannot reshape array of size 77824 into shape (2048,21)\n<\/code>\nPlease suggest how to calculate the covariance matrix of a file that includes feature vectors for each image in each line.","y":"This error is not attributed to PyTorch. You used reshape in Numpy.\nI have a couple of questions. Why did you use 21 instead of 38 when reshaping infile1? Why does the reshaping be necessary for calculating the covariance?","z":"This error is not attributed to PyTorch. You used reshape in Numpy.\nI have a couple of questions. Why did you use 21 instead of 38 when reshaping infile1? Why does the reshaping be necessary for calculating the covariance?\nthanks a lot Tony. I made a mistake on my own end regarding 21. Your questions helped me figure out and my problem is resolved.\n21 was shape of my data. My data size had increased. It is silly but I forgot to change it to 38."},{"x":"BatchNorm1d with link \"https:\/\/pytorch.org\/docs\/master\/_modules\/torch\/nn\/modules\/batchnorm.html#BatchNorm1d\" is not that obvious when I checked the source code.\nLet\u2019s check this:\n<code class=\"lang-auto\">if self.affine:\n            self.weight = Parameter(torch.Tensor(num_features))\n            self.bias = Parameter(torch.Tensor(num_features))\nelse:\n            self.register_parameter('weight', None)\n            self.register_parameter('bias', None)\n<\/code>\nFirst Parameter is the instruction that weight will be the learnable parameter. This means we will learn the weight.\nThen register_parameter adds a parameter to the module. But can you tell me what is the difference?\nI just assume this has to do something with making parameters that are shared for every mini batch and that are specific for every mini batch.\nThere is also similar question with link \"https:\/\/discuss.pytorch.org\/t\/nn-parameters-vs-nn-module-register-parameter\/18641\" without a clear answer.\nThe general question is: How many parameters in total we can learn in BatchNorm1d.\nI think 4 with link \"https:\/\/discuss.pytorch.org\/t\/what-is-standard-scale-of-batchnorm1d\/43230\/6\", but I am not sure.\nI know if we set affine=True we will learn weight and bias parameters.\nApart from these two, I somehow think we can also learn  mean and std. However, I am not sure.\nMaybe the current implementation of _BatchNorm (which is the base class for BatchNorm1d) is not learning mean and std(var).\nHere is the last bit of code I will share with you:\n<code class=\"lang-auto\">self.register_buffer('running_mean', torch.zeros(num_features))\n            self.register_buffer('running_var', torch.ones(num_features))\n            self.register_buffer('num_batches_tracked', torch.tensor(0, dtype=torch.long))\nelse:\n            self.register_parameter('running_mean', None)\n            self.register_parameter('running_var', None)\n            self.register_parameter('num_batches_tracked', None)\n<\/code>\nFrom this code I can understand that we can track running_mean and running_var (which should be variance I guess) and number of mini batches (num_batches_tracked) we processed so far.  The first two are tensors, and the second is a number of mini batches tracked. So at least these two are not the constants. Right?","y":"Hi,\nCheck this post,\n\n\n\n\n[SOLVED] Register_parameter vs register_buffer vs nn.Parameter with link \"https:\/\/discuss.pytorch.org\/t\/solved-register-parameter-vs-register-buffer-vs-nn-parameter\/31953\" autograd with link \"\/c\/autograd\"\n\n\n    I was looking at the code of batchnorm \n    def __init__(self, num_features, eps=1e-5, momentum=0.1, affine=True,\n                 track_running_stats=True):\n        super(_BatchNorm, self).__init__()\n        self.num_features = num_features\n        self.eps = eps\n        self.momentum = momentum\n        self.affine = affine\n        self.track_running_stats = track_running_stats\n        if self.affine:\n            self.weight = Parameter(torch.Tensor(num_features))\n            self.bias = Parame\u2026\n  \n\n\nWith respect to batch normalization the best you can do is to read the paper:\n\n\n\narXiv.org with link \"https:\/\/arxiv.org\/abs\/1502.03167\"\n\n\nBatch Normalization: Accelerating Deep Network Training by Reducing Internal... with link \"https:\/\/arxiv.org\/abs\/1502.03167\"\nTraining Deep Neural Networks is complicated by the fact that the\ndistribution of each layer's inputs changes during training, as the parameters\nof the previous layers change. This slows down the training by requiring lower\nlearning rates and careful...\n\n\n\n\n\n","z":"Hi,\nCheck this post,\n\n\n\n\n[SOLVED] Register_parameter vs register_buffer vs nn.Parameter with link \"https:\/\/discuss.pytorch.org\/t\/solved-register-parameter-vs-register-buffer-vs-nn-parameter\/31953\" autograd with link \"\/c\/autograd\"\n\n\n    I was looking at the code of batchnorm \n    def __init__(self, num_features, eps=1e-5, momentum=0.1, affine=True,\n                 track_running_stats=True):\n        super(_BatchNorm, self).__init__()\n        self.num_features = num_features\n        self.eps = eps\n        self.momentum = momentum\n        self.affine = affine\n        self.track_running_stats = track_running_stats\n        if self.affine:\n            self.weight = Parameter(torch.Tensor(num_features))\n            self.bias = Parame\u2026\n  \n\n\nWith respect to batch normalization the best you can do is to read the paper:\n\n\n\narXiv.org with link \"https:\/\/arxiv.org\/abs\/1502.03167\"\n\n\nBatch Normalization: Accelerating Deep Network Training by Reducing Internal... with link \"https:\/\/arxiv.org\/abs\/1502.03167\"\nTraining Deep Neural Networks is complicated by the fact that the\ndistribution of each layer's inputs changes during training, as the parameters\nof the previous layers change. This slows down the training by requiring lower\nlearning rates and careful...\n\n\n\n\n\n\nI am providing this for my better understanding. So far I understood number of batches tracked will increase with every batch.\n<code class=\"lang-auto\"> if self.training and self.track_running_stats:\n            # TODO: if statement only here to tell the jit to skip emitting this when it is None\n            if self.num_batches_tracked is not None:\n                self.num_batches_tracked += 1\n                if self.momentum is None:  # use cumulative moving average\n                    exponential_average_factor = 1.0 \/ float(self.num_batches_tracked)\n                else:  # use exponential moving average\n                    exponential_average_factor = self.momentum\n<\/code>\nweight and bias are learnable parameter of choice, and they can scale the output activation range effectively else it will be std (-1,1) as we saw with the mean of 0.\nWhat are the running mean and running var?\nThey are used for the moving average calculation with link \"https:\/\/en.wikipedia.org\/wiki\/Moving_average\"\nTwo types of moving averages we support as you can see:\n\nexponential moving average\ncumulative moving average\n\nAt the very end they will smoothen the curve which is what the batch norm do.\n\nSo the running mean and running var are calculated as average, not learned like Parameter or some parameter in the nn.Linear module using the loss function."},{"x":"with different kernel sizes ,feature maps also have different size ,like (1,1,62,62\uff09and (1,1,67,67), in such condition, torch.cat doesn\u2019t work as their dimensions don\u2019t match. is there any way can make them together? Thanks a lot","y":"I don\u2019t think so  You can always flatten them to (1,-1) where -1=62x62 or 67x67.\nOtherwise you\u2019re stuck with padding or cropping the feature maps","z":"I don\u2019t think so  You can always flatten them to (1,-1) where -1=62x62 or 67x67.\nOtherwise you\u2019re stuck with padding or cropping the feature maps\nit works. great appreciated"},{"x":", Hi!\nI\u2019m trying to visualize the adversarial images generated by this script:\nhttps:\/\/pytorch.org\/tutorials\/beginner\/fgsm_tutorial.html\nThis tutorial is used for the mnist data. Now I want to use for other data which is trained using the inception_v1 architecture, below is the gist for that one:\n<code class=\"lang-auto\">from __future__ import print_function\nimport torch\nimport torch.nn as nn\nimport torch.nn.functional as F\nimport torch.optim as optim\nfrom torchvision import datasets, models, transforms\nimport numpy as np\nimport matplotlib.pyplot as plt\nimport os\n\n\n\nepsilons = [0, .05, .1, .15, .2, .25, .3]\npretrained_model = \"googlenet\/googlenet_aux03_ep30.pth\"\nuse_cuda=True\n\nNet = models.googlenet()\n\n#Even if set aux_logits to False, the state_dict throws the unexpected aux arguments. \nnum_ftrs = Net.aux2.fc2.in_features\nNet.aux2.fc2 = nn.Linear(num_ftrs, 5)\n\nnum_ftrs = Net.fc.in_features\nNet.fc = nn.Linear(num_ftrs, 5)\n\n\ntest_loader = torch.utils.data.DataLoader( \n            datasets.ImageFolder(os.path.join('data\/val'), transform=transforms.Compose([\n                 transforms.Resize(224),\n                 transforms.RandomCrop(224),\n                 transforms.ToTensor(),\n                 transforms.Normalize([0.485, 0.456, 0.406], [0.229, 0.224, 0.225]) \n            ])),\n               batch_size=1, shuffle=True)\n\n\nprint(\"CUDA Available: \",torch.cuda.is_available())\ndevice = torch.device(\"cuda:1\" if (use_cuda and torch.cuda.is_available()) else \"cpu\")\n\nmodel = Net.to(device)\n\nmodel.load_state_dict(torch.load(pretrained_model, map_location='cpu'))\n\nmodel.eval()\n\ndef fgsm_attack(image, epsilon, data_grad):\n    # Collect the element-wise sign of the data gradient\n    sign_data_grad = data_grad.sign()\n    # Create the perturbed image by adjusting each pixel of the input image\n    perturbed_image = image + epsilon*sign_data_grad\n    # Adding clipping to maintain [0,1] range\n    perturbed_image = torch.clamp(perturbed_image, 0, 1)\n    # Return the perturbed image\n    return perturbed_image\n\n\n\n\ndef test( model, device, test_loader, epsilon ):\n\n    # Accuracy counter\n    correct = 0\n    adv_examples = []\n\n    # Loop over all examples in test set\n    for data, target in test_loader:\n\n        # Send the data and label to the device\n        data, target = data.to(device), target.to(device)\n\n        # Set requires_grad attribute of tensor. Important for Attack\n        data.requires_grad = True\n\n        # Forward pass the data through the model\n        output = model(data)\n        init_pred = output.max(1, keepdim=True)[1] # get the index of the max log-probability\n\n        # If the initial prediction is wrong, dont bother attacking, just move on\n        if init_pred.item() != target.item():\n            continue\n\n        # Calculate the loss\n        loss = F.nll_loss(output, target)\n\n        # Zero all existing gradients\n        model.zero_grad()\n\n        # Calculate gradients of model in backward pass\n        loss.backward()\n\n        # Collect datagrad\n        data_grad = data.grad.data\n\n        # Call FGSM Attack\n        perturbed_data = fgsm_attack(data, epsilon, data_grad)\n\n        # Re-classify the perturbed image\n        output = model(perturbed_data)\n\n        # Check for success\n        final_pred = output.max(1, keepdim=True)[1] # get the index of the max log-probability\n        if final_pred.item() == target.item():\n            correct += 1\n            # Special case for saving 0 epsilon examples\n            if (epsilon == 0) and (len(adv_examples) < 5):\n                adv_ex = perturbed_data.squeeze().detach().cpu().numpy()\n                adv_examples.append( (init_pred.item(), final_pred.item(), adv_ex) )\n        else:\n            # Save some adv examples for visualization later\n            if len(adv_examples) < 5:\n                adv_ex = perturbed_data.squeeze().detach().cpu().numpy()\n                adv_examples.append( (init_pred.item(), final_pred.item(), adv_ex) )\n\n    # Calculate final accuracy for this epsilon\n    final_acc = correct\/float(len(test_loader))\n    print(\"Epsilon: {}\\tTest Accuracy = {} \/ {} = {}\".format(epsilon, correct, len(test_loader), final_acc))\n\n    # Return the accuracy and an adversarial example\n    return final_acc, adv_examples\n\n\n\n\naccuracies = []\nexamples = []\n\n# Run test for each epsilon\nfor eps in epsilons:\n    acc, ex = test(model, device, test_loader, eps)\n    accuracies.append(acc)\n    examples.append(ex)\n\n\n\nplt.figure(figsize=(5,5))\nplt.plot(epsilons, accuracies, \"*-\")\nplt.yticks(np.arange(0, 1.1, step=0.1))\nplt.xticks(np.arange(0, .35, step=0.05))\nplt.title(\"Accuracy vs Epsilon\")\nplt.xlabel(\"Epsilon\")\nplt.ylabel(\"Accuracy\")\nplt.show()\n\n\ncnt = 0\nplt.figure(figsize=(8,10)) \nfor i in range(len(epsilons)):\n    for j in range(len(examples[i])):\n        cnt += 1\n        plt.subplot(len(epsilons),len(examples[0]),cnt)\n        plt.xticks([], [])\n        plt.yticks([], [])\n        if j == 0:\n            plt.ylabel(\"Eps: {}\".format(epsilons[i]), fontsize=14)\n        orig,adv,ex = examples[i][j]\n        ex = ex.numpy().transpose((1, 2, 0))\n        mean = np.array([0.485, 0.456, 0.406])\n        std = np.array([0.229, 0.224, 0.225]) \n        ex = std * ex + mean\n        ex = np.clip(ex, 0, 1)\n        #ex = ex.permute(1, 2, 0)\n        plt.title(\"{} -> {}\".format(orig, adv))\n        plt.imshow(ex)\nplt.tight_layout()\nplt.show()\n\n<\/code>\nUpto the accuracy graph, the code ran well, but for the visualizing images it throws this error:\n<code class=\"lang-auto\">Traceback (most recent call last):\n  File \"fgsm.py\", line 156, in <module>\n    ex = ex.numpy().transpose((1, 2, 0))\nAttributeError: 'numpy.ndarray' object has no attribute 'numpy'\n\n<\/code>\nany thoughts?","y":"adv_ex is already a numpy array, so you can\u2019t call .numpy() again on it (which is a tensor method).\nStore adv_ex as a tensor or avoid calling numpy on it:\n<code class=\"lang-python\">adv_ex = perturbed_data.squeeze().detach().cpu()\nadv_examples.append( (init_pred.item(), final_pred.item(), adv_ex) )\n<\/code>","z":"adv_ex is already a numpy array, so you can\u2019t call .numpy() again on it (which is a tensor method).\nStore adv_ex as a tensor or avoid calling numpy on it:\n<code class=\"lang-python\">adv_ex = perturbed_data.squeeze().detach().cpu()\nadv_examples.append( (init_pred.item(), final_pred.item(), adv_ex) )\n<\/code>\nThanks!. My bad, I haven\u2019t noticed that it is already called.\n<code class=\"lang-auto\">cnt = 0\nplt.figure(figsize=(8,10)) \nfor i in range(len(epsilons)):\n    for j in range(len(examples[i])):\n        cnt += 1\n        plt.subplot(len(epsilons),len(examples[0]),cnt)\n        plt.xticks([], [])\n        plt.yticks([], [])\n        if j == 0:\n            plt.ylabel(\"Eps: {}\".format(epsilons[i]), fontsize=14)\n        orig,adv,ex = examples[i][j]\n        ex = ex.transpose((1, 2, 0))\n        plt.title(\"{} -> {}\".format(orig, adv))\n        plt.imshow(ex)\nplt.tight_layout()\nplt.show()\n<\/code>\nNow, i\u2019m looking for print the class names, as of now, it prints the number order for the categories.\n<code class=\"lang-auto\">plt.subplot(len(epsilons),len(examples[0]),cnt)\n....\nplt.title(\"{} -> {}\".format(orig, adv))\n<\/code>\n: This gives prediction as corresponding label_index, but i want to print prediction as corresponding class_names. How can i do that?\nExample plot: Screenshot from 2019-04-09 23-36-47.png750\u00d7127 32.5 KB with link \"https:\/\/discuss.pytorch.org\/uploads\/default\/original\/2X\/2\/20353d9c50bf4f44543ff5b114349faf0e4f1e34.png\"\nI want to plot predictions as bus -> car.\nYou could create a dict containing an index to class mapping. Then just use this dict to get the corresponding names:\n<code class=\"lang-python\">mapping = {\n    0: 'bus',\n    1: 'car',\n    ...\n}\n\nplt.title(\"{} -> {}\".format(mapping[orig], mapping[adv]))\n<\/code>\nThanks!! worked like a charm! \nThere is an another problem with the Epsilon Vs. Accuracy graph. For eps=0, the graph should start from actual validation accuracy. I have a 15K samples with 5 categories and took 20% for validation. When I run the attack, for eps=0, it is printing like this, which is quiet weird:\n<code class=\"lang-auto\">Epsilon: 0\tTest Accuracy = 596 \/ 3564 = 0.16722783389450055\n\n<\/code>\nIt should be the 88%, that reported at training the network time.\nCould you share some thoughts?\nThis time changed the loss criterion:\n<code class=\"lang-auto\">loss = F.nll_loss(output, target)\n<\/code>\nto\n<code class=\"lang-auto\">loss = criterion(output, target)\n...\n...\ncriterion = nn.CrossEntropyLoss()\n<\/code>\nAs far as I understand the issue, it seems that you are seeing a discrepancy between the validation and test accuracy?\nIf so, could you post the validation code so that we could search for some differences?\nscript:\n<code class=\"lang-auto\">from __future__ import print_function\nimport torch\nimport torch.nn as nn\nimport torch.nn.functional as F\nimport torch.optim as optim\nfrom torchvision import datasets, models, transforms\nimport numpy as np\nimport matplotlib.pyplot as plt\nimport os\n\n\n\nepsilons = [0, .05, .1, .15, .2, .25, .3]\npretrained_model = \"googlenet\/googlenet_aux03_ep30.pth\"\nuse_cuda=True\n\nNet = models.googlenet()\n\n#Even if set aux_logits to False, the state_dict throws the unexpected aux arguments. \nnum_ftrs = Net.aux2.fc2.in_features\nNet.aux2.fc2 = nn.Linear(num_ftrs, 5)\n\nnum_ftrs = Net.fc.in_features\nNet.fc = nn.Linear(num_ftrs, 5)\n\n\ntest_loader = torch.utils.data.DataLoader( \n            datasets.ImageFolder(os.path.join('data\/val'), transform=transforms.Compose([\n                 transforms.Resize(224),\n                 transforms.RandomCrop(224),\n                 transforms.ToTensor(),\n                 transforms.Normalize([0.485, 0.456, 0.406], [0.229, 0.224, 0.225]) \n            ])),\n               batch_size=1, shuffle=True)\n\n\nprint(\"CUDA Available: \",torch.cuda.is_available())\ndevice = torch.device(\"cuda:1\" if (use_cuda and torch.cuda.is_available()) else \"cpu\")\n\nmodel = Net.to(device)\n\nmodel.load_state_dict(torch.load(pretrained_model, map_location='cpu'))\n\nmodel.eval()\n\ndef fgsm_attack(image, epsilon, data_grad):\n    # Collect the element-wise sign of the data gradient\n    sign_data_grad = data_grad.sign()\n    # Create the perturbed image by adjusting each pixel of the input image\n    perturbed_image = image + epsilon*sign_data_grad\n    # Adding clipping to maintain [0,1] range\n    perturbed_image = torch.clamp(perturbed_image, 0, 1)\n    # Return the perturbed image\n    return perturbed_image\n\n\n\n\ndef test( model, device, test_loader, epsilon ):\n\n    # Accuracy counter\n    correct = 0\n    adv_examples = []\n\n    # Loop over all examples in test set\n    for data, target in test_loader:\n\n        # Send the data and label to the device\n        data, target = data.to(device), target.to(device)\n\n        # Set requires_grad attribute of tensor. Important for Attack\n        data.requires_grad = True\n\n        # Forward pass the data through the model\n        output = model(data)\n        init_pred = output.max(1, keepdim=True)[1] # get the index of the max log-probability\n\n        # If the initial prediction is wrong, dont bother attacking, just move on\n        if init_pred.item() != target.item():\n            continue\n\n        # Calculate the loss\n        loss = F.nll_loss(output, target)\n\n        # Zero all existing gradients\n        model.zero_grad()\n\n        # Calculate gradients of model in backward pass\n        loss.backward()\n\n        # Collect datagrad\n        data_grad = data.grad.data\n\n        # Call FGSM Attack\n        perturbed_data = fgsm_attack(data, epsilon, data_grad)\n\n        # Re-classify the perturbed image\n        output = model(perturbed_data)\n\n        # Check for success\n        final_pred = output.max(1, keepdim=True)[1] # get the index of the max log-probability\n        if final_pred.item() == target.item():\n            correct += 1\n            # Special case for saving 0 epsilon examples\n            if (epsilon == 0) and (len(adv_examples) < 5):\n                adv_ex = perturbed_data.squeeze().detach().cpu().numpy()\n                adv_examples.append( (init_pred.item(), final_pred.item(), adv_ex) )\n        else:\n            # Save some adv examples for visualization later\n            if len(adv_examples) < 5:\n                adv_ex = perturbed_data.squeeze().detach().cpu().numpy()\n                adv_examples.append( (init_pred.item(), final_pred.item(), adv_ex) )\n\n    # Calculate final accuracy for this epsilon\n    final_acc = correct\/float(len(test_loader))\n    print(\"Epsilon: {}\\tTest Accuracy = {} \/ {} = {}\".format(epsilon, correct, len(test_loader), final_acc))\n\n    # Return the accuracy and an adversarial example\n    return final_acc, adv_examples\n\n\n\n\naccuracies = []\nexamples = []\n\n# Run test for each epsilon\nfor eps in epsilons:\n    acc, ex = test(model, device, test_loader, eps)\n    accuracies.append(acc)\n    examples.append(ex)\n\n\n\nplt.figure(figsize=(5,5))\nplt.plot(epsilons, accuracies, \"*-\")\nplt.yticks(np.arange(0, 1.1, step=0.1))\nplt.xticks(np.arange(0, .35, step=0.05))\nplt.title(\"Accuracy vs Epsilon\")\nplt.xlabel(\"Epsilon\")\nplt.ylabel(\"Accuracy\")\nplt.show()\n\n\ncnt = 0\nplt.figure(figsize=(8,10)) \nfor i in range(len(epsilons)):\n    for j in range(len(examples[i])):\n        cnt += 1\n        plt.subplot(len(epsilons),len(examples[0]),cnt)\n        plt.xticks([], [])\n        plt.yticks([], [])\n        if j == 0:\n            plt.ylabel(\"Eps: {}\".format(epsilons[i]), fontsize=14)\n        orig,adv,ex = examples[i][j]\n        ex = ex.numpy().transpose((1, 2, 0))\n        mean = np.array([0.485, 0.456, 0.406])\n        std = np.array([0.229, 0.224, 0.225]) \n        ex = std * ex + mean\n        ex = np.clip(ex, 0, 1)\n        #ex = ex.permute(1, 2, 0)\n        plt.title(\"{} -> {}\".format(orig, adv))\n        plt.imshow(ex)\nplt.tight_layout()\nplt.show()\n<\/code>\nThis is the same code I\u2019m following. and when I change the batch_size above 1, the code throws this error.\n<code class=\"lang-auto\">Traceback (most recent call last):\nIn def test( model, device, test_loader, epsilon ):\nLine:  if init_pred.item() != target.item(): \n            continue\n\nValue Error:only one element tensors can be converted to Python scalars\n<\/code>\nFor test accuracy, it even if the batch_size=1 it should take all samples from 3K right?\nor i\u2019m incorrect?\nCould you check the shape of init_pred and target?\nBased on your code, it should be a single element tensor, but apparently that\u2019s not the case.\nYes, your code currently should only work with batch_size=1, since you are using .item() ops and calculate the dataset length based on the length of your DataLoader, which will be wrong for another batch size.\nThere a power failure with my server.  I will check once its up. But the problem is why it\u2019s giving such a low accuracy with epsilon =0. When I comment  transforms.Normalize(), it rasied to 40%pc.\n\nis there any problem with the NxCxHxW?, since the original script is used for MNIST data and model, which based on the 1-channel.\n<code class=\"lang-auto\">target shape: torch.Size([1])\ndata shape: torch.Size([1, 3, 224, 224])\ninit_pred shape: torch.Size([1, 1])\n<\/code>"},{"x":"I was wondering if you could help me define a torch.nn.ReflectionPad3d layer?\nI only found this post:\nhttps:\/\/discuss.pytorch.org\/t\/reflectionpad3d\/20403\nAnd the answer doesn\u2019t make any sense. Thank you.","y":"Found the answer in the following link:\nhttps:\/\/discuss.pytorch.org\/t\/how-to-pad-one-side-in-pytorch\/21212\/2\ntorch.nn.functional.pad ( input, pad , mode='reflect' , value=(1, 1, 1, 1, 1, 1) ) will do the job. Can be rapped around a module as well:\n<code class=\"lang-auto\">import torch\nimport torch.nn.functional as F\n\nclass CustomPad(torch.nn.module):\n  def __init__(self, padding):\n    self.padding = padding\n  def forward(self, x):\n    return F.pad(x. self.padding, mode='reflect')\n<\/code>","z":"Even this answer doesn\u2019t work correctly, since this ReflectionPadNd function does not work correctly.\nhttps:\/\/github.com\/c22n\/unet-pytorch\/blob\/master\/models\/custom_layers.py\nFound the answer in the following link:\nhttps:\/\/discuss.pytorch.org\/t\/how-to-pad-one-side-in-pytorch\/21212\/2\ntorch.nn.functional.pad ( input, pad , mode='reflect' , value=(1, 1, 1, 1, 1, 1) ) will do the job. Can be rapped around a module as well:\n<code class=\"lang-auto\">import torch\nimport torch.nn.functional as F\n\nclass CustomPad(torch.nn.module):\n  def __init__(self, padding):\n    self.padding = padding\n  def forward(self, x):\n    return F.pad(x. self.padding, mode='reflect')\n<\/code>"},{"x":"\n\n\nGitHub with link \"https:\/\/github.com\/bastulli\/AutoCarJetsonNano\"\n\n\n\nbastulli\/AutoCarJetsonNano with link \"https:\/\/github.com\/bastulli\/AutoCarJetsonNano\"\nPyTorch Python Neural Network Autonomous 1\/10 Car for Nvidia Jetson Nano - bastulli\/AutoCarJetsonNano\n\n\n\n\n\nI\u2019m trying to control the steering of a car by getting an output between -1 ,1.  Currently, I trained my network with this model.\n<code class=\"lang-auto\">import torch.nn as nn\nimport torch.nn.functional as F\n\n# define the CNN architecture\nclass Net(nn.Module):\n    def __init__(self):\n        super(Net, self).__init__()\n        # convolutional layer (sees 32x32x3 image tensor)\n        self.conv1 = nn.Conv2d(3, 16, 3, padding=1)\n        # convolutional layer (sees 16x16x16 tensor)\n        self.conv2 = nn.Conv2d(16, 32, 3, padding=1)\n        self.pool = nn.MaxPool2d(2, 2)\n        self.fc1 = nn.Linear(32 * 4 * 4, 256)\n        self.fc2 = nn.Linear(256, 1)\n        self.dropout = nn.Dropout(0.25)\n\n    def forward(self, x):\n        x = self.pool(F.relu(self.conv1(x)))\n        x = self.pool(F.relu(self.conv2(x)))\n        # flatten image input\n        x = x.view(-1, 32 * 4 * 4)\n        x = self.dropout(x)\n        x = F.relu(self.fc1(x))\n        x = self.dropout(x)\n        x = self.fc2(x)\n        return x\n\n# create a complete CNN\nmodel = Net()\nprint(model)\n\n# move tensors to GPU if CUDA is available\nif train_on_gpu:\n    model.cuda()\n<\/code>\nAlso you can see the jupyter notebook here with link \"https:\/\/github.com\/bastulli\/AutoCarJetsonNano\/blob\/master\/train\/autocar.ipynb\"  The problem is i\u2019m not sure what to use to get the output in real time such as np.argmax, np.max, ect\u2026\n<code class=\"lang-auto\">    def autopilot(self):\n        \n        img = self.preprocess(self.cam.value)\n        count = self.cam.count\n        \n        if count!= self.temp:\n            print('RUN!')\n            self.model.eval()\n            with torch.no_grad():\n                output = self.model(img)\n            _, angle_tensor = torch.max(output,1)\n            self.angle_out = angle_tensor.cpu().data.numpy()\n            #self.angle_out = np.argmax(output.cpu().data.numpy())#angle[0].cpu().numpy()\n            self.temp = count\n            print(self.angle_out)\n            \n        else:\n            pass\n<\/code>\nI\u2019m new to pytorch and i\u2019m stuck trying to get the correct output like I did in jupyter notebook","y":"The problem is that your output before applying reshaping is Batchx32x56x56\nWhy do you reshape that to -1,512?\nIt makes no sense and that\u2019s why it does not work.\nYou are squeezing basically maping around 100k elements into a 196*512 matrix","z":"In teory you are not doing anything wrong.\nTo get outputs you just have to do the same than for training but adding under the decoration wtih torch.no_grad():\nis it not what you tried?\nCOuld you provide few more details about what does it fail?\nThe issue that I\u2019m having is that my output is a array that is 195 in length for each image I pass through my model.  I just want a single numpy output between -1,1.  I think I set up my output function incorrectly.  Here is a project very similar to what i\u2019m trying to do https:\/\/github.com\/autorope\/donkeycar\/blob\/dev\/donkeycar\/parts\/keras.py\nBut as far as I can see you have a fc layer which goes from 256 to 1 feature. I find it\u2019s not possible you to get 195 elements on a tensor. If you does not modify the network it should be a tensor of size batch x 1 or similar to that.\nCould you print output shape? Or, are you running batch size 195 or getting 195 items in the batch dimension when you apply that x = x.view(-1, 32 * 4 * 4)?\nHere is my output\n Screenshot from 2019-05-02 16-37-25.png799\u00d7314 34.9 KB with link \"https:\/\/discuss.pytorch.org\/uploads\/default\/original\/2X\/1\/1fd68f53b55a03f0d1d941bd6fa2e5c29443eaac.png\"\nThe problem is that your output before applying reshaping is Batchx32x56x56\nWhy do you reshape that to -1,512?\nIt makes no sense and that\u2019s why it does not work.\nYou are squeezing basically maping around 100k elements into a 196*512 matrix\nThanks, I never changed the flatten size after modifying a the network.  I forgot my input is 244x244 > maxpool  > 122x122 > maxpool  > 56x56.  Then multiplied by out_channel size of 32"},{"x":"Hi,\nI\u2019m wondering if trilinear and bilinear interpolation would be the same if I retained one of the dimensions. For example:\n<code class=\"lang-auto\">x = torch.rand([5, 6, 3, 30, 50]) # B C T H W\nF.upsample(x, size=(3, 60, 100), mode='trilinear') # 5 6 3 60 100\n<\/code>\nIs the second tensor equivalent to bilinear upsampling of each tensor along the temporal axis?","y":"Looks like it is.\n<code class=\"lang-auto\">import torch, torch.nn.functional as F\nx = torch.rand([5, 6, 3, 30, 50]) # B C T H W\nz = F.upsample(x, size=(3, 60, 100), mode='trilinear') # 5 6 3 60 100\n\nz2 =  []\nfor t in range(3):\n   x_t  = x[:, :, t, :, :]\n   z2.append(F.upsample(x_t, size=(60, 100), mode='bilinear')) # 5 6 60 100\nz2 = torch.stack(z2, dim=2) # 5 6 3 60 100\n\nprint(bool((z==z2).all())) # True\n\n<\/code>","z":"Looks like it is.\n<code class=\"lang-auto\">import torch, torch.nn.functional as F\nx = torch.rand([5, 6, 3, 30, 50]) # B C T H W\nz = F.upsample(x, size=(3, 60, 100), mode='trilinear') # 5 6 3 60 100\n\nz2 =  []\nfor t in range(3):\n   x_t  = x[:, :, t, :, :]\n   z2.append(F.upsample(x_t, size=(60, 100), mode='bilinear')) # 5 6 60 100\nz2 = torch.stack(z2, dim=2) # 5 6 3 60 100\n\nprint(bool((z==z2).all())) # True\n\n<\/code>"},{"x":"Hi all,\nI am trying to resume training from a pretrained Resnet-50 model, which is a 3dcnn model that has been initialized with Kinetics weights https:\/\/github.com\/kenshohara\/3D-ResNets-PyTorch. Initial training (5 epochs) has been done on a cuda device which has PyTorch version 1.0.0. My model and loss function is sent to current device, before starting training.\nI am unable to resume training on the same cuda machine, but I am able to resume it on a cpu device.\nThe exact error output is as follows,\n<code class=\"lang-auto\">Traceback (most recent call last):\n  File \"\/opt\/anaconda3\/lib\/python3.7\/site-packages\/torch\/optim\/sgd.py\", line 101, in step\n    buf.mul_(momentum).add_(1 - dampening, d_p)\nRuntimeError: expected type torch.FloatTensor but got torch.cuda.FloatTensor\n<\/code>\nI save states as follows,\n<code class=\"lang-auto\">        best_ckpt_path = os.path.join('checkpoint-best.tar')\n        states = {\n            'epoch': epoch + 1,\n            'optimizer': optimizer.state_dict(),\n            'state_dict': model.state_dict(),\n        }\n        torch.save(states, best_ckpt_path)\n<\/code>\nand I load,\n<code class=\"lang-auto\">        device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n        parameters = self.get_fine_tuning_parameters(opt.param_dict_list,\n                                                     opt.learning_rate,\n                                                     opt.weight_decay)\n\n        if opt.optim.lower() in ['adam']:\n            optimizer = optim.Adam(parameters, lr=opt.learning_rate, weight_decay=opt.weight_decay)\n        elif opt.optim.lower() in ['sgd']:\n            optimizer = optim.SGD(parameters, lr=opt.learning_rate, momentum=0.9, weight_decay=opt.weight_decay)\n        else:\n            raise ValueError('Invalid optimizer type string.')\n\n        self.optimizer = optimizer\n\n        if os.path.isfile(self.resume_path):\n            print('resuming model from checkpoint {}'.format(self.resume_path))\n\n            if self.device.type in ['cpu']:\n                checkpoint = torch.load(self.resume_path, map_location=self.device)\n            else:\n                checkpoint = torch.load(self.resume_path)\n\n            self.model.load_state_dict(checkpoint['state_dict'])\n            self.optimizer.load_state_dict(checkpoint['optimizer'])\n            for state in self.optimizer.state.values():\n                for k, v in state.items():\n                    if isinstance(v, torch.Tensor):\n                        print(\"device: {}\".format(v.device))\n\n            self.begin_epoch = checkpoint['epoch']\n<\/code>\nThe print output of the above code block is cpu for each optimizer tensor, both for the cuda device and cpu device. So this shows that optimizer related tensors are already placed on cpu. So, how can I receive a got torch.cuda.FloatTensor error ?","y":"Hi\nBased on your error message, I guess that the data-types of the tensors in the momentum_buffer of the optimizer are on the wrong device.\n\n\n\n kirtac83:\n\nTraceback (most recent call last): File \u201c\/opt\/anaconda3\/lib\/python3.7\/site-packages\/torch\/optim\/sgd.py\u201d, line 101, in step buf.mul_(momentum).add_(1 - dampening, d_p) RuntimeError: expected type torch.FloatTensor but got torch.cuda.FloatTensor\n\n\nYou can try to run this snippet to move the buffers:\n<code class=\"lang-python\">print(optimizer.state[list(optimizer.state.keys())[0]])\n\nfor p in optimizer.state.keys():\n    param_state = optimizer.state[p]\n    buf = param_state[\"momentum_buffer\"]\n    param_state[\"momentum_buffer\"] = buf.cuda()  # move buf to device\n\nprint(optimizer.state[list(optimizer.state.keys())[0]])\n<\/code>\nSee here with link \"https:\/\/pytorch.org\/docs\/stable\/_modules\/torch\/optim\/sgd.html#SGD\" how momentum_buffers are used in the SGD optimizer.\nHope this helps.","z":"Hi\nBased on your error message, I guess that the data-types of the tensors in the momentum_buffer of the optimizer are on the wrong device.\n\n\n\n kirtac83:\n\nTraceback (most recent call last): File \u201c\/opt\/anaconda3\/lib\/python3.7\/site-packages\/torch\/optim\/sgd.py\u201d, line 101, in step buf.mul_(momentum).add_(1 - dampening, d_p) RuntimeError: expected type torch.FloatTensor but got torch.cuda.FloatTensor\n\n\nYou can try to run this snippet to move the buffers:\n<code class=\"lang-python\">print(optimizer.state[list(optimizer.state.keys())[0]])\n\nfor p in optimizer.state.keys():\n    param_state = optimizer.state[p]\n    buf = param_state[\"momentum_buffer\"]\n    param_state[\"momentum_buffer\"] = buf.cuda()  # move buf to device\n\nprint(optimizer.state[list(optimizer.state.keys())[0]])\n<\/code>\nSee here with link \"https:\/\/pytorch.org\/docs\/stable\/_modules\/torch\/optim\/sgd.html#SGD\" how momentum_buffers are used in the SGD optimizer.\nHope this helps.\nHi,\nthanks a lot for the solution. I was not aware of the momentum buffers. And I haven\u2019t specially treated them in my previous work. Why do you think this happened? To my knowledge, I only need to push the model and loss function to current device. Could you describe the best practice in this case please?\nI\u2019m not sure. If you call torch.load it should restore the tensors on the device prior to saving.\nOptimizer with link \"https:\/\/pytorch.org\/docs\/stable\/_modules\/torch\/optim\/optimizer.html#Optimizer\" also casts the tensors to the respective device.\nI can\u2019t really tell without the complete code.\nI had a similar issue and I think it is related to your optimizer referencing model parameters that are not yet loaded on GPU. In my case I fixed the issue by changing the following:\n<code class=\"lang-auto\">optimizer = torch.optim.Adam(model.parameters(), lr=params.lr)\ntry:\n       checkpoint = torch.load(\"checkpoints\/\"+model_folder+\"checkpoint.pth.tar\")\n       start_epoch = checkpoint['epoch']\n       scheduler.load_state_dict(checkpoint['scheduler'])\n       model.load_state_dict(checkpoint['state_dict'])\n       model.to(device) # moving this line here fixed the issue\n       optimizer.load_state_dict(checkpoint['optimizer'])\n       \u2026\n       # it used to be somewhere around here\n<\/code>\nSo in your case a cleaner solution would be to send that parameters object to GPU (not sure how but there must be some one-liner that does that) BEFORE loading the dictionary, else it seems that it will be loaded to the usual memory instead of GPU memory."},{"x":"Hello, I am a Pytorch beginner.\nI\u2019m trying to make a custom Dataset which is not simply like \u201cone image: one target\" mode.\nI want to have a \u201c5 input images : one target image\u201d mode dataset. So how can I do that ?\nThanks very much !","y":"Hey,\nYou should look at this tutorial with link \"https:\/\/pytorch.org\/tutorials\/beginner\/data_loading_tutorial.html\". I think it\u2019s what you\u2019re looking for.\nBut basically you need to create your own dataset class like:\n<code class=\"lang-auto\">class Dataset(Dataset):\n    \"\"\"dataset.\"\"\"\n\n    def __init__(self, csv_file, root_dir, transform=None):\n        \"\"\"\n        Args:\n            csv_file (string): Path to the csv file with names of all images\n            root_dir (string): Directory with all the images.\n            transform (callable, optional): Optional transform to be applied\n                on a sample.\n        \"\"\"\n        self.names = pd.read_csv(csv_file)\n        self.root_dir = root_dir\n        self.transform = transform\n\n    def __len__(self):\n        return len(self.names)\n\n    def __getitem__(self, idx):\n        img_name1 = os.path.join(self.root_dir,\n                                self.names.iloc[idx, 0])\n        image1 = io.imread(img_name1)\n        img_name2 = os.path.join(self.root_dir,\n                                self.names.iloc[idx, 1])\n        image2 = io.imread(img_name2)\n        img_name3 = os.path.join(self.root_dir,\n                                self.names.iloc[idx, 2])\n        image3 = io.imread(img_name3)\n        target_name = os.path.join(self.root_dir,\n                                self.names.iloc[idx, 3])\n        target = io.imread(target_name)\n        sample = {'image1': image1, 'image2': image2, 'image3': image3, 'target': target}\n\n        if self.transform:\n            sample = self.transform(sample)\n\n        return sample\n<\/code>","z":"Hey,\nYou should look at this tutorial with link \"https:\/\/pytorch.org\/tutorials\/beginner\/data_loading_tutorial.html\". I think it\u2019s what you\u2019re looking for.\nBut basically you need to create your own dataset class like:\n<code class=\"lang-auto\">class Dataset(Dataset):\n    \"\"\"dataset.\"\"\"\n\n    def __init__(self, csv_file, root_dir, transform=None):\n        \"\"\"\n        Args:\n            csv_file (string): Path to the csv file with names of all images\n            root_dir (string): Directory with all the images.\n            transform (callable, optional): Optional transform to be applied\n                on a sample.\n        \"\"\"\n        self.names = pd.read_csv(csv_file)\n        self.root_dir = root_dir\n        self.transform = transform\n\n    def __len__(self):\n        return len(self.names)\n\n    def __getitem__(self, idx):\n        img_name1 = os.path.join(self.root_dir,\n                                self.names.iloc[idx, 0])\n        image1 = io.imread(img_name1)\n        img_name2 = os.path.join(self.root_dir,\n                                self.names.iloc[idx, 1])\n        image2 = io.imread(img_name2)\n        img_name3 = os.path.join(self.root_dir,\n                                self.names.iloc[idx, 2])\n        image3 = io.imread(img_name3)\n        target_name = os.path.join(self.root_dir,\n                                self.names.iloc[idx, 3])\n        target = io.imread(target_name)\n        sample = {'image1': image1, 'image2': image2, 'image3': image3, 'target': target}\n\n        if self.transform:\n            sample = self.transform(sample)\n\n        return sample\n<\/code>\nNice! I never thought about using dictionary and csv file \u2026\nActually following the tutorial, I just know how to load an exist dataset and some other specific examples.\nI will try it ! Thank you !"},{"x":"I am getting this error when I am loading the weights for a model I trained on a GPU.  I am trying to load the model on a CPU.  Not sure what this error means.  I did check the checkpoint[\u2018state_dict\u2019] keys and it does contain this \u2018num_batches_tracked\u2019 key for all the batch normalization layers.  I am using the same model on the CPU as I trained on the GPU.  I am training a UNet model.\ncheckpoint = torch.load(\u2018model_best.pth-zeromask-notconsider.tar\u2019,map_location=lambda storage, loc: storage)\ntrain_conv.model.load_state_dict(checkpoint[\u2018state_dict\u2019])\nRuntimeError                              Traceback (most recent call last)\n in ()\n----> 1 train_conv.model.load_state_dict(checkpoint[\u2018state_dict\u2019])\n~\\AppData\\Local\\conda\\conda\\envs\\myfastai1\\lib\\site-packages\\torch\\nn\\modules\\module.py in load_state_dict(self, state_dict, strict)\n719         if len(error_msgs) > 0:\n720             raise RuntimeError(\u2018Error(s) in loading state_dict for {}:\\n\\t{}\u2019.format(\n\u2013> 721                                self.class.name, \u201c\\n\\t\u201d.join(error_msgs)))\n722\n723     def parameters(self):\nRuntimeError: Error(s) in loading state_dict for Unet34:\nUnexpected key(s) in state_dict: \u201crn.1.num_batches_tracked\u201d, \u201crn.4.0.bn1.num_batches_tracked\u201d, \u201crn.4.0.bn2.num_batches_tracked\u201d, \u201crn.4.1.bn1.num_batches_tracked\u201d, \u201crn.4.1.bn2.num_batches_tracked\u201d, \u201crn.4.2.bn1.num_batches_tracked\u201d, \u201crn.4.2.bn2.num_batches_tracked\u201d, \u201crn.5.0.bn1.num_batches_tracked\u201d, \u201crn.5.0.bn2.num_batches_tracked\u201d, \u201crn.5.0.downsample.1.num_batches_tracked\u201d, \u201crn.5.1.bn1.num_batches_tracked\u201d, \u201crn.5.1.bn2.num_batches_tracked\u201d, \u201crn.5.2.bn1.num_batches_tracked\u201d, \u201crn.5.2.bn2.num_batches_tracked\u201d, \u201crn.5.3.bn1.num_batches_tracked\u201d, \u201crn.5.3.bn2.num_batches_tracked\u201d, \u201crn.6.0.bn1.num_batches_tracked\u201d, \u201crn.6.0.bn2.num_batches_tracked\u201d, \u201crn.6.0.downsample.1.num_batches_tracked\u201d, \u201crn.6.1.bn1.num_batches_tracked\u201d, \u201crn.6.1.bn2.num_batches_tracked\u201d, \u201crn.6.2.bn1.num_batches_tracked\u201d, \u201crn.6.2.bn2.num_batches_tracked\u201d, \u201crn.6.3.bn1.num_batches_tracked\u201d, \u201crn.6.3.bn2.num_batches_tracked\u201d, \u201crn.6.4.bn1.num_batches_tracked\u201d, \u201crn.6.4.bn2.num_batches_tracked\u201d, \u201crn.6.5.bn1.num_batches_tracked\u201d, \u201crn.6.5.bn2.num_batches_tracked\u201d, \u201crn.7.0.bn1.num_batches_tracked\u201d, \u201crn.7.0.bn2.num_batches_tracked\u201d, \u201crn.7.0.downsample.1.num_batches_tracked\u201d, \u201crn.7.1.bn1.num_batches_tracked\u201d, \u201crn.7.1.bn2.num_batches_tracked\u201d, \u201crn.7.2.bn1.num_batches_tracked\u201d, \u201crn.7.2.bn2.num_batches_tracked\u201d, \u201cup1.bn.num_batches_tracked\u201d, \u201cup2.bn.num_batches_tracked\u201d, \u201cup3.bn.num_batches_tracked\u201d, \u201cup4.bn.num_batches_tracked\u201d.","y":"Looks like it was a version problem - I trained the model in 0.4.1 and tried to load the dict in 0.4.0.  Once I upgraded the pytorch version, everything worked fine.","z":"Looks like it was a version problem - I trained the model in 0.4.1 and tried to load the dict in 0.4.0.  Once I upgraded the pytorch version, everything worked fine.\nHi, I meet the same question. I have some models in 0.4.1, at the same time some models in 0.4.0. And my pytorch is 0.4.0. As you say, after upgrading the pytorch version, pytorch can work fine loading 0.4.1\u2019s model. But how about loading 0.4.0\u2019s model?\nThanks !\nI have the same problem with you, my model and pytorch is 0.4.1, but the problem is not solved, the same error still occurs.\nI trained my model in pytorch 0.4.1, in classroom I have 0.4.0 and I can\u2019t upgrade classroom version. it is possible that I could load 0.4.1 model in 0.4.0 or I have to downgrade my pytorch version?\nThanks.\nAre there any updates or solutions yet?\nI am having a similar trouble loading a model.\nEdit: I don\u2019t think that this should be marked as solved, since the solution refers to a pytorch version imcompatibility. This sounds more like a workaround than a solution. To me it should be compatible with multiple versions of pytorch since not everybody can up- or downgrade their version.\nYou can check pytorch densenet with link \"https:\/\/github.com\/pytorch\/vision\/blob\/1f8f7ea71c8807408a7860b8ae9ca63177975b64\/torchvision\/models\/densenet.py#L120-L144\" for the solution. The internal state_dict have changed from something like features.denseblock1.denselayer1.norm.1.weight to features.denseblock1.denselayer1.norm1.weight\nAlso, you can write a simple function that load the model, modify state_dict and save the model.\n<code class=\"lang-auto\">def modify_state_dict(state_dict):\n    pattern = re.compile(\n                r'^(.*denselayer\\d+\\.(?:norm|relu|conv))\\.((?:[12])\\.(?:weight|bias|running_mean|running_var))$')\n    state_dict = checkpoint['state_dict']\n    for key in list(state_dict.keys()):\n        res = pattern.match(key)\n        if res:\n            new_key = res.group(1) + res.group(2)\n            state_dict[new_key] = state_dict[key]\n            del state_dict[key]\n\ncheckpoint = torch.load(model_path)\nmodify_state_dict(checkpoint['state_dict'])\ntorch.save(checkpoint, model_path)\n<\/code>\nFor the num_batches_tracked, pytorch has added in later version. I have checked the value of these key in densenet layer and they are all tensor(0, device='cuda:0'). I think you can add that missing key to the state_dict when modifying it. Btw, I load my trained densenet model on v0.4.1 without that num_batches_tracked key.\nAgree with you!!Just downgrade the version to avoid this error is not elegant."},{"x":"class Decode3d(nn.Module):\ndef __init__(self, input_channels, output_channels, conv_bias=True,\n             lrelu_inplace=True, Trilinear = True, res = False):\n    super(Decode3d, self).__init__()\n    if Trilinear:\n        self.up = nn.Upsample(scale_factor = 3, mode = 'trilinear', align_corners = True)\n    else:\n        self.up = nn.ConvTranspose3d(input_channels, input_channels,\n                    kernel_size = 3, stride =2, output_padding = 1)\n    self.lrelu_inplace = lrelu_inplace\n    self.conv_bias = conv_bias\n    self.input_channels = input_channels\n    self.output_channels= output_channels\n    self.residual = res\n    self.conv1 = nn.Conv3d(input_channels, input_channels, kernel_size = 3,\n                           stride = 1,padding = 1, bias=conv_bias)\n    self.bn_1 = nn.InstanceNorm3d(input_channels, affine=True)\n    self.conv2 = nn.Conv3d(output_channels, output_channels, kernel_size = 1,\n                           stride = 1,padding = 0, bias=conv_bias)\n    self.bn_2 = nn.InstanceNorm3d(output_channels, affine=True)\n\ndef forward(self, x1, x2):\n    print(\"Decoding\", x1.shape, x2.shape)\n    x = self.up(x1)\n    print(\"After upconv, X1 shape:\", x1.shape)\n    x = torch.cat([x1, x2], dim = 1)\n    if self.residual == True:\n        skip = x\n    x = self.bn_1(x)\n    x = F.leaky_relu(x)\n    x = self.conv1(x)\n    x = self.bn_2(x)\n    x = F.leaky_relu(x)\n    x = self.conv2(x)\n    if self.residual == True:\n        x = x + skip\n    print(\"Exiting deconding\", x.shape)\n    return x\n\nHi,\nI am trying to upsample and concatenate but for some reason Upsample does not seem to work? Is there something wrong with the syntax?","y":"\n\n\n Geeks_Sid:\n\nprint(\u201cAfter upconv, X1 shape:\u201d, x1.shape)\n\n\nYou are printing the wrong variable. print(\"After upconv, X1 shape:\", x1.shape) should be print(\"After upconv, X shape:\", x.shape)","z":"\n\n\n Geeks_Sid:\n\nprint(\u201cAfter upconv, X1 shape:\u201d, x1.shape)\n\n\nYou are printing the wrong variable. print(\"After upconv, X1 shape:\", x1.shape) should be print(\"After upconv, X shape:\", x.shape)\nHi, I have solved it. You guys mentioned the wrong error but it lead me down a good road.\n\nx = torch.cat([x1, x2], dim = 1) ###!Wrong\nx = torch.cat([x, x2], dim = 1) ###Correct.\n\nCheers~"},{"x":"dataset = datasets.ImageFolder(root='\/data_path', transform=transform)\n\nAfter the above code, how can I split the dataset into 20 percent for testing and 80 percent for training and load into torch.utils.data.DataLoader?","y":"Here\u2019s an example with link \"https:\/\/gist.github.com\/kevinzakka\/d33bf8d6c7f06a9d8c76d97a7879f5cb\" from somewhere else.","z":"Here\u2019s an example with link \"https:\/\/gist.github.com\/kevinzakka\/d33bf8d6c7f06a9d8c76d97a7879f5cb\" from somewhere else.\nwhat should be the root directory parameter in torchvision.datasets.ImageFolder on google colab\nwhat should be the root= \u2018data_path\u2019 on google colab\u2026?\nperhaps the code changed but that link has no mention of ImageFolder"},{"x":"How can I change the datatype of a tensor without changing the device type.\nIf I use .type() then it would also require the device (cpu or gpu). Is there any way to change just datatype of the tensor","y":"You could just use .to(), .type() or call the type method directly without specifying the device:\n<code class=\"lang-python\">x = torch.randn(1, device='cuda')\nprint(x.to(torch.double))\n> tensor([1.4169], device='cuda:0', dtype=torch.float64)\nprint(x.double())\n> tensor([1.4169], device='cuda:0', dtype=torch.float64)\nprint(x.type(dtype=torch.double))\ntensor([1.4169], device='cuda:0', dtype=torch.float64)\n<\/code>\nAs you can see, the device is still the same.","z":"You could just use .to(), .type() or call the type method directly without specifying the device:\n<code class=\"lang-python\">x = torch.randn(1, device='cuda')\nprint(x.to(torch.double))\n> tensor([1.4169], device='cuda:0', dtype=torch.float64)\nprint(x.double())\n> tensor([1.4169], device='cuda:0', dtype=torch.float64)\nprint(x.type(dtype=torch.double))\ntensor([1.4169], device='cuda:0', dtype=torch.float64)\n<\/code>\nAs you can see, the device is still the same."},{"x":"Cross entropy and BCELoss give average over all pixels. How can I compute a loss having same shape as the class prediction matrix where each pixel has loss value.\nI want to get a loss output of shape (H,W) where each pixel has loss for thst particular pixel.","y":"Based on BCELoss with link \"https:\/\/pytorch.org\/docs\/stable\/nn.html#torch.nn.BCELoss\" documentation, it seems if you set reduction ='none', you will get your output.\nAnd as you mentioned in your question, the BCELoss averages, because the default value of reduction is mean.\nSo here is the code you should use:\n<code class=\"lang-python\">criterion = BCELoss(reduction='none')\n<\/code>","z":"You can define your own loss function by extending nn.Module class.\n<code class=\"lang-python\">\nimport torch.nn as nn\nimport torch\n\n\nclass Loss(nn.Module):\n    def __init__(self):\n        \n        super(Loss, self).__init__()\n        \n\n    def forward(self, y, y_pred):\n        loss = # DO YOUR CALCULATION ON THE TENSOR OF YOUR IMAGE\n        return loss\n\n<\/code>\nIs there any way or attribute available in the predefined loss functions?\nRedution parameter does the job!\nBased on BCELoss with link \"https:\/\/pytorch.org\/docs\/stable\/nn.html#torch.nn.BCELoss\" documentation, it seems if you set reduction ='none', you will get your output.\nAnd as you mentioned in your question, the BCELoss averages, because the default value of reduction is mean.\nSo here is the code you should use:\n<code class=\"lang-python\">criterion = BCELoss(reduction='none')\n<\/code>"},{"x":"I\u2019m having this issue when computing losses. It stops at the following line, output_o and loss_gen dtype is torch.float32. The model is in GPU, the line is in training loop not in model. I\u2019m not sure what is happening.\n<code class=\"lang-auto\">Traceback (most recent call last):\n\nFile \"train.py\", line 73, in &amp;lt;module&amp;gt;\n\nloss_g = 0.01 * torch.log(1 - output_o) + loss_gen\n\nRuntimeError: expected type torch.cuda.FloatTensor but got torch.FloatTensor\n<\/code>\nEdit: I think I\u2019ve solved it.Output_o was in CPU.","y":"Yes, exactly. When you transfer your model to GPU, your model output will be on GPU so you have to convert one to another type.\nSo you can send your tensor to GPU by .to(device) method.","z":"Yes, exactly. When you transfer your model to GPU, your model output will be on GPU so you have to convert one to another type.\nSo you can send your tensor to GPU by .to(device) method."},{"x":"Using this: nn.ConvTranspose3d(64, 3, kernel_size=4, stride=2, padding=(1, 1, 1)) I get correct the output channels and the number of frames, but the frames size are 64x64: torch.Size([1, 3, 16, 64, 64])\nWhat should I change in order to get torch.Size([1, 3, 16, 112, 112]) ?","y":"The solution to this is:\nConvTranspose3d(64, 3, kernel_size=4, stride=(2, 4, 4), bias=False, padding=(1, 8, 8))","z":"The solution to this is:\nConvTranspose3d(64, 3, kernel_size=4, stride=(2, 4, 4), bias=False, padding=(1, 8, 8))"},{"x":"I have extracted an intermediate layer from VGG16 using my 256x256 image input. The dimension of this intermediate input is [512, 8, 8], therefore 512 channels.\nI want to pass this intermediate output into a decoder that expects an input of [32, 8, 8], how can I do this without loss of information e.g. I guess it would not be a good idea to take a 32 channel slice of the 512 channel input?\ntl;dr how do I reduce 512 channels to 32?\nThanks!","y":"Add a transform conv layer, e.g.\nself.tsf = nn.Conv2d(512,32,1)","z":"Add a transform conv layer, e.g.\nself.tsf = nn.Conv2d(512,32,1)\n\nThanks, that worked! I added a new module called transform and passed the input into it."},{"x":"Hello,\nI would like to load the weights of the densenet121 network like that :\nnet = models.densenet121(pretrained=False)\nnet.load_state_dict(torch.load(\u2019.\/densenet121-a639ec97.pth\u2019))\nit return me this error :\nRuntimeError: Error(s) in loading state_dict for DenseNet:\nMissing key(s) in state_dict: \u201cfeatures.denseblock1.denselayer1.norm1.weight\u201d, \u201cfeatures.denseblock1.denselayer1.norm1.bias\u201d, \u201cfeatures.denseblock1.denselayer1.norm1.running_mean\u201d, \u201cfeatures.denseblock1.denselayer1.norm1.running_var\u201d, \u201cfeatures.denseblock1.denselayer1.conv1.weight\u201d, \u201cfeatures.denseblock1.denselayer1.norm2.weight\u201d, \u201cfeatures.denseblock1.denselayer1.norm2.bias\u201d, \u201cfeatures.denseblock1.denselayer1.norm2.running_mean\u201d, \u201cfeatures.denseblock1.denselayer1.norm2.running_var\u201d, \u201cfeatures.denseblock1.denselayer1.conv2.weight\u201d\netc\u2026\n\u2026\nI use this method instead of :\nnet = models.densenet121(pretrained=True)\nbecause the proxy of my company don\u2019t allow to do it\u2026\nCould someone help me ?\nThank you","y":"Thank you, you answer guided me to look at why my network don\u2019t know state dict so is decide to use the constructor code from pytorch densenet and it works :\nimport re\n<h1>Code modified from torchvision densenet source for loading from pre .4 densenet weights.<\/h1>\ncheckpoint = torch.load(\u2019.\/model.pth.tar\u2019)\nstate_dict = checkpoint[\u2018state_dict\u2019]\nremove_data_parallel = False # Change if you don\u2019t want to use nn.DataParallel(model)\npattern = re.compile(\nr\u2019^(.*denselayer\\d+.(?:norm|relu|conv)).((?:[12]).(?:weight|bias|running_mean|running_var))$\u2019)\nfor key in list(state_dict.keys()):\nmatch = pattern.match(key)\nnew_key = match.group(1) + match.group(2) if match else key\nnew_key = new_key[7:] if remove_data_parallel else new_key\nstate_dict[new_key] = state_dict[key]\n# Delete old key only if modified.\nif match or remove_data_parallel:\ndel state_dict[key]","z":"I used to load model\u2019s weights like this:\nnet = models.densenet121(pretrained=False).cuda()\nnet.load_state_dict(torch.load('path\/to\/saved_model.pth.tar')['state_dict'])\nThank you, you answer guided me to look at why my network don\u2019t know state dict so is decide to use the constructor code from pytorch densenet and it works :\nimport re\n<h1>Code modified from torchvision densenet source for loading from pre .4 densenet weights.<\/h1>\ncheckpoint = torch.load(\u2019.\/model.pth.tar\u2019)\nstate_dict = checkpoint[\u2018state_dict\u2019]\nremove_data_parallel = False # Change if you don\u2019t want to use nn.DataParallel(model)\npattern = re.compile(\nr\u2019^(.*denselayer\\d+.(?:norm|relu|conv)).((?:[12]).(?:weight|bias|running_mean|running_var))$\u2019)\nfor key in list(state_dict.keys()):\nmatch = pattern.match(key)\nnew_key = match.group(1) + match.group(2) if match else key\nnew_key = new_key[7:] if remove_data_parallel else new_key\nstate_dict[new_key] = state_dict[key]\n# Delete old key only if modified.\nif match or remove_data_parallel:\ndel state_dict[key]"},{"x":"I have a tiny problem where the folder names that are selected for class named are confused for the transfer learning tutorial. Could you please guide how to fix it?\n*Adding the ipynb here because it could not be loaded in gist even though I uploaded it\nMy category names are 1,2, \u2026 , 16\n.\/test                    : 135\n.\/test\/1                  : 5\n.\/test\/2                  : 2\n.\/test\/3                  : 4\n.\/test\/4                  : 63\n.\/test\/5                  : 2\n.\/test\/6                  : 0\n.\/test\/7                  : 12\n.\/test\/8                  : 8\n.\/test\/9                  : 13\n.\/test\/10                 : 3\n.\/test\/11                 : 1\n.\/test\/12                 : 3\n.\/test\/13                 : 3\n.\/test\/14                 : 6\n.\/test\/15                 : 2\n.\/test\/16                 : 8\nAnd the accuracies I get from the tutorial are:\nclass 1 --> accuracy: 80.00, correct predictions: 4, all: 5\nclass 2 --> accuracy: 33.33, correct predictions: 1, all: 3\nclass 3 --> accuracy: 0.00, correct predictions: 0, all: 1\nclass 4 --> accuracy: 0.00, correct predictions: 0, all: 3\nclass 5 --> accuracy: 0.00, correct predictions: 0, all: 3\nclass 6 --> accuracy: 33.33, correct predictions: 2, all: 6\nclass 7 --> accuracy: 0.00, correct predictions: 0, all: 2\nclass 8 --> accuracy: 12.50, correct predictions: 1, all: 8\nclass 9 --> accuracy: 50.00, correct predictions: 1, all: 2\nclass 10 --> accuracy: 25.00, correct predictions: 1, all: 4\nclass 11 --> accuracy: 93.65, correct predictions: 59, all: 63\nclass 12 --> accuracy: 50.00, correct predictions: 1, all: 2\nclass 13 --> accuracy: nan, correct predictions: 0, all: 0\nclass 14 --> accuracy: 91.67, correct predictions: 11, all: 12\nclass 15 --> accuracy: 37.50, correct predictions: 3, all: 8\nclass 16 --> accuracy: 69.23, correct predictions: 9, all: 13\ntotal correct: 93, total samples: 135\nAs you see, here class 4 is considered to be 11.\nThe following is the part of the code used for calculating accuracies:\n<code class=\"lang-auto\">\nimport ntpath\nfrom torch.utils.data.sampler import WeightedRandomSampler\n\nmodel_ft.eval()\n\nnb_classes = 16\n\nimport torch.nn.functional as F\n\nconfusion_matrix = torch.zeros(nb_classes, nb_classes)\n\n_classes = []\n_preds = []\npredicted_labels = []\n\nclass_probs = torch.Tensor()\n\n\n\n\nim_paths = []\nwith torch.no_grad():\n    for i, (inputs, classes, im_path) in enumerate(dataloaders['test']):\n       \n\n        im_paths.append(im_path)\n        inputs = inputs.to(device)\n        \n        classes = classes.to(device)\n        classes_list = classes.cpu().detach().numpy().tolist()\n        _classes[:]=[i+1 for i in classes_list]\n        outputs = model_ft(inputs)\n        \n  \n\n        class_probs = class_probs.cuda()\n        \n        class_probs = torch.cat((class_probs, F.softmax(outputs, 1)))\n            \n        _, preds = torch.max(outputs, 1)\n        preds_list = preds.cpu().detach().numpy().tolist()\n        _preds[:]=[i+1 for i in preds_list]\n          \n        predicted_labels.append(preds.cpu().detach().numpy().tolist())\n        for t, p in zip(classes.view(-1), preds.view(-1)):\n                confusion_matrix[t.long(), p.long()] += 1\n                \nprint(confusion_matrix)\nper_class_accuracies = (confusion_matrix.diag()\/confusion_matrix.sum(1)).cpu().detach().numpy().tolist()\n\nprint(','.join(\"{:2.04f}\".format(x) for x in per_class_accuracies))\ntotal_correct = 0\ntotal = 0\nfor i in range(nb_classes):\n    total_correct += int(confusion_matrix[i][i].numpy())\n    total += int(confusion_matrix.sum(dim=1)[i].numpy())\n    print(\"class {:d} --> accuracy: {:.2f}, correct predictions: {:d}, all: {:d}\".format(i+1, (confusion_matrix.diag()\/confusion_matrix.sum(1))[i]*100, int(confusion_matrix[i][i].numpy()), int(confusion_matrix.sum(dim=1)[i].numpy())))\n    \n\nprint(\"total correct: {}, total samples: {}\".format(total_correct, total))\n\nflattened_im_paths = flattened = [item for sublist in im_paths for item in sublist]\n\nprint(\"length is: \", len(flattened_im_paths))\nfor i in range(len(flattened_im_paths)):\n    class_p = class_probs[i].cpu().detach().numpy().tolist()\n\n    print('{}, {}'.format(ntpath.basename(flattened_im_paths[i]), class_p))\n<\/code>","y":"Regarding the mismatch between the folders and the class predictions:\nIf you are using ImageFolder, the folders will be sorted internally, such that\n<code class=\"lang-python\">1 - class0\n10 - class1\n11 - class2\n...\n2 - ...\n20 - ...\n21 - ...\n<\/code>\nYou could append zeros in front of your folder names:\n<code class=\"lang-python\">01 - class0\n02 - class1\n03 - ...\n<\/code>\nor write a custom Dataset with your custom folder - class mapping.","z":"Regarding the mismatch between the folders and the class predictions:\nIf you are using ImageFolder, the folders will be sorted internally, such that\n<code class=\"lang-python\">1 - class0\n10 - class1\n11 - class2\n...\n2 - ...\n20 - ...\n21 - ...\n<\/code>\nYou could append zeros in front of your folder names:\n<code class=\"lang-python\">01 - class0\n02 - class1\n03 - ...\n<\/code>\nor write a custom Dataset with your custom folder - class mapping.\nThank you so much. I confirm that zero-padding fixed the problem "},{"x":"Strange things happened, look at the simple code:\n<code class=\"lang-auto\">       mask_miss = mask_miss.expand_as(sxing)             # type: torch.Tensor\n       mask_miss[:, :, -2, :, :] = 1   \n<\/code>\nI want to reset a channel of mask_miss to 1, but whenever I did this, all the elements will become 1.\n<code class=\"lang-auto\">        print(torch.max(mask_miss), torch.min(mask_miss))\n<\/code>\nI print the range of mask_miss and I get tensor(1), tensor(1). If I change mask_miss[:, :, -2, :, :] = 0.5, I get tensor(0.5), tensor(0.5).","y":"Pytorch tensors can share the same memory. If you create a tensor once and put that many times into mask_miss this sort of thing can happen.\nIt works the same way in numpy if you\u2019re curious. Could you show us how you create the mask_miss tensor?","z":"Pytorch tensors can share the same memory. If you create a tensor once and put that many times into mask_miss this sort of thing can happen.\nIt works the same way in numpy if you\u2019re curious. Could you show us how you create the mask_miss tensor?\nThe reason is that expand does not allocate more memory but just make the tensor look as if you expanded it.\nYou should call .clone() on the resulting tensor if you plan on modifying it to avoid such behaviours.\nHi, thank you for your help. The mask_miss is created from numpy array and I have checked it already.\nThank you so much for your explain and instructions, the problem is solved. I missed the doc file of expand before. By the way, as  said, due to the memory saving mechanism, does the following code look good?\n<code class=\"lang-auto\">for i in range(self.nstack):\n            preds_instack = []\n            # return 5 scales of feature maps\n            hourglass_feature = self.hourglass[i](x)\n\n            if i == 0:  # cache for smaller feature maps produced by hourglass block\n                features_cache = [torch.zeros_like(hourglass_feature[scale]) for scale in range(5)]\n                for s in range(5):  # channel attention before heatmap regression\n                    hourglass_feature[s] = self.channel_attention[i][s](hourglass_feature[s])\n            else:  # residual connection across stacks\n                for k in range(5):\n                \n                    hourglass_feature_attention = self.channel_attention[i][k](hourglass_feature[k])\n\n                    hourglass_feature[k] = hourglass_feature_attention + features_cache[k]\n            # feature maps before heatmap regression\n            features_instack = self.features[i](hourglass_feature)\n<\/code>\nI save the cache named   hourglass_feature and use and reset it in the next iteration. Even though I just place the cache in the same location hourglass_feature[s] = self.channel_attention[i][s](hourglass_feature[s]), the graph connection is correct  and is just what I desire after I draw the network using some visualization tool such as Netron."},{"x":"Hi, this is my code for training an image dataset.\n<code class=\"lang-auto\">batch_size=20\n\n    data_loader = data.DataLoader(dataset, batch_size,\n                                  num_workers=args.num_workers,\n                                  shuffle=True,\n                                  collate_fn=collate_fn,\n                                  pin_memory=False)\n    \n<\/code>\n<code class=\"lang-auto\">for step,(img_pre, img_next, boxes_pre, boxes_next, labels, valid_pre, valid_next) in enumerate(data_loader):\n        if args.cuda:\n            print(\"cuda is true..\")\n            img_pre = Variable(img_pre.cuda())\n            img_next = Variable(img_next.cuda())\n            boxes_pre = Variable(boxes_pre.cuda())\n            boxes_next = Variable(boxes_next.cuda())\n            valid_pre = Variable(valid_pre.cuda(), volatile=True)\n            valid_next = Variable(valid_next.cuda(), volatile=True)\n            labels = Variable(labels.cuda(), volatile=True)\n\n        else:\n            img_pre = Variable(img_pre)\n            img_next = Variable(img_next)\n            boxes_pre = Variable(boxes_pre)\n            boxes_next = Variable(boxes_next)\n            valid_pre = Variable(valid_pre)\n            valid_next = Variable(valid_next)\n            labels = Variable(labels, volatile=True)\n\n\n        # forward\n        t0 = time.time()\n        out = net(img_pre, img_next, boxes_pre, boxes_next, valid_pre, valid_next)\n\n        optimizer.zero_grad()\n        loss_pre, loss_next, loss_similarity, loss, accuracy_pre, accuracy_next, accuracy, predict_indexes = criterion(out, labels, valid_pre, valid_next)\n\n        loss.backward()\n        optimizer.step()\n        t1 = time.time()\n\n        all_epoch_loss += [loss.data.cpu()]\n\n<\/code>\nI am getting an error after one iteration :\n<code class=\"lang-auto\">TypeError                                 Traceback (most recent call last)\n<ipython-input-25-418aca395a0e> in <module>()\n   327 \n   328 if __name__ == '__main__':\n--> 329     train()\n\n<ipython-input-25-418aca395a0e> in train()\n   154     #img_pre, img_next, boxes_pre, boxes_next, labels, valid_pre, valid_next=next(batch_iterator)\n   155 \n--> 156     for step,(img_pre, img_next, boxes_pre, boxes_next, labels, valid_pre, valid_next) in enumerate(data_loader):\n   157         if args.cuda:\n   158             print(\"cuda is true..\")\n\n~\/anaconda3\/lib\/python3.6\/site-packages\/torch\/utils\/data\/dataloader.py in __next__(self)\n   334                 self.reorder_dict[idx] = batch\n   335                 continue\n--> 336             return self._process_next_batch(batch)\n   337 \n   338     next = __next__  # Python 2 compatibility\n\n~\/anaconda3\/lib\/python3.6\/site-packages\/torch\/utils\/data\/dataloader.py in _process_next_batch(self, batch)\n   355         self._put_indices()\n   356         if isinstance(batch, ExceptionWrapper):\n--> 357             raise batch.exc_type(batch.exc_msg)\n   358         return batch\n   359 \n\nTypeError: Traceback (most recent call last):\n File \"\/home\/shounak\/anaconda3\/lib\/python3.6\/site-packages\/torch\/utils\/data\/dataloader.py\", line 106, in _worker_loop\n   samples = collate_fn([dataset[i] for i in batch_indices])\n File \"\/home\/shounak\/anaconda3\/lib\/python3.6\/site-packages\/torch\/utils\/data\/dataloader.py\", line 106, in <listcomp>\n   samples = collate_fn([dataset[i] for i in batch_indices])\n File \"<ipython-input-8-101613e4c615>\", line 45, in __getitem__\n   return self.transform(current_image, next_image, current_box, next_box, labels)\n File \"<ipython-input-16-7e912b97874a>\", line 558, in __call__\n   return self.augment(img_pre, img_next, boxes_pre, boxes_next, labels)\n File \"<ipython-input-16-7e912b97874a>\", line 45, in __call__\n   img_pre, img_next, boxes_pre, boxes_next, labels =                 t(img_pre, img_next, boxes_pre, boxes_next, labels)\n File \"<ipython-input-16-7e912b97874a>\", line 408, in __call__\n   im_pre, im_next, boxes_pre, boxes_next, labels =             self.rand_brightness(im_pre, im_next, boxes_pre, boxes_next, labels)\n File \"<ipython-input-16-7e912b97874a>\", line 184, in __call__\n   if random.randint(2):\nTypeError: randint() missing 1 required positional argument: 'b\n<\/code>\nCan you please help ?","y":"It seems you are using a custom collate_fn with an if-condition:\n<code class=\"lang-python\">if random.randint(2)\n<\/code>\nThe random.randint method takes two arguments: the lower bound a and the upper bound b.\nIf you would like to sample in [0, 2], you can use random.randint(0, 2) instead.\nNote that the Python randint method will sample with an inclusive upper bound, so you\u2019ll get an integer in [0, 1, 2]!","z":"It seems you are using a custom collate_fn with an if-condition:\n<code class=\"lang-python\">if random.randint(2)\n<\/code>\nThe random.randint method takes two arguments: the lower bound a and the upper bound b.\nIf you would like to sample in [0, 2], you can use random.randint(0, 2) instead.\nNote that the Python randint method will sample with an inclusive upper bound, so you\u2019ll get an integer in [0, 1, 2]!"},{"x":"I am working on semantic Segmentation on Pascal VOC 2012 dataset and my model is not working.\nPlease help.\nMy model is like.\nScreenshot_20190313-002300-1.jpg916\u00d7351 59.7 KB with link \"https:\/\/discuss.pytorch.org\/uploads\/default\/original\/2X\/a\/a8e8ffa09e32b959f567d9234fe6d055f59ef44e.jpeg\"\n(I took the final decoder output dimension as 1 as binary so 1 dimension labelmap)\n<code class=\"lang-auto\">class Net(nn.Module):\n  def __init__(self):\n        print('\\nGruInitializing')\n        super(GRUNet,self).__init__()\n        \n        self.is_x_tensor4 = False\n        \n        self.batch_size, self.img_w, self.img_h=10,128,128\n        \n        self.input_shape = (self.batch_size, 3, self.img_w, self.img_h)\n        \n        #number of filters for each convolution layer in the encoder\n        self.n_convfilter = [96, 128, 256, 256, 256, 256]  \n        \n        #the dimension of the fully connected layer\n        self.n_fc_filters = [1024]    \n        #number of filters for each 2d convolution layer in the decoder\n        self.n_deconvfilter = [256, 256, 256, 128, 96, 1]\n        self.encoder=encoder(self.input_shape,self.n_convfilter,\\\n                            self.n_fc_filters)\n        #HERE THE PROBELM IS idx1,idx2,idx3\n        self.decoder=decoder(self.n_deconvfilter,n_class=1)\n    \n  def forward(self,x):\n    output,c1,c2,c3,c4,c5= self.encoder(x)\n    ok=self.decoder(output,c1,c2,c3,c4,c5)\n    \n    return ok\n<\/code>\nI have used the encoder-decoder model and my code is as follows\u2013\n<code class=\"lang-auto\">class encoder(nn.Module):\n  def __init__(self,input_shape,n_convfilter,\\\n               n_fc_filters):\n        print(\"\\ninitalizing \\\"encoder\\\"\")\n        super(encoder,self).__init__()\n        #conv1\n        self.conv1a = Conv2d(input_shape[1], n_convfilter[0], 7, padding=3,stride=1)#\n        self.conv1b = Conv2d(n_convfilter[0], n_convfilter[0], 3, padding=1,stride=1)\n        \n        #conv2\n        self.conv2a = Conv2d(n_convfilter[0], n_convfilter[1], 3, padding=1,stride=1)\n        self.conv2b = Conv2d(n_convfilter[1], n_convfilter[1], 3, padding=1,stride=1)\n        self.conv2c = Conv2d(n_convfilter[0], n_convfilter[1], 1)\n        \n        #conv3\n        self.conv3a = Conv2d(n_convfilter[1], n_convfilter[2], 3, padding=1,stride=1)\n        self.conv3b = Conv2d(n_convfilter[2], n_convfilter[2], 3, padding=1,stride=1)\n        self.conv3c = Conv2d(n_convfilter[1], n_convfilter[2], 1)\n        \n        #conv4\n        self.conv4a = Conv2d(n_convfilter[2], n_convfilter[3], 3, padding=1,stride=1)\n        self.conv4b = Conv2d(n_convfilter[3], n_convfilter[3], 3, padding=1,stride=1)\n        \n        #conv5\n        self.conv5a = Conv2d(n_convfilter[3], n_convfilter[4], 3, padding=1,stride=1)\n        self.conv5b = Conv2d(n_convfilter[4], n_convfilter[4], 3, padding=1,stride=1)\n        self.conv5c = Conv2d(n_convfilter[3], n_convfilter[4], 1)\n        \n        #conv6\n        self.conv6a = Conv2d(n_convfilter[4], n_convfilter[5], 3, padding=1,stride=1)\n        self.conv6b = Conv2d(n_convfilter[5], n_convfilter[5], 3, padding=1,stride=1)\n        \n        #conv6\n        self.conv6a = Conv2d(n_convfilter[4], n_convfilter[5], 3, padding=1,stride=1)\n        self.conv6b = Conv2d(n_convfilter[5], n_convfilter[5], 3, padding=1,stride=1)\n        \n        #conv7\n        self.conv7a = Conv2d(n_convfilter[5], n_convfilter[5], 3, padding=1,stride=1)\n        self.conv7b = Conv2d(n_convfilter[5], n_convfilter[5], 3, padding=1,stride=1)\n        #self.conv7c = Conv2d(n_convfilter[3], n_convfilter[4], 1)\n        ########################### n_convfilter[5]=256###################all\n        \n        \n        \n        #pooling layer\n        self.pool1 = MaxPool2d(kernel_size= 2,stride=2)#,return_indices=True)\n        self.pool2 = MaxPool2d(kernel_size=1,stride=2)#,return_indices=True)\n        self.gpool=nn.AvgPool2d(kernel_size=2)\n        \n        #nonlinearities of the network\n        self.leaky_relu = LeakyReLU(negative_slope= 0.01)\n        self.sigmoid = Sigmoid()\n        self.tanh = Tanh()\n        \n        self.conv8a = Conv2d(n_convfilter[5], 1024, 1, padding=0,stride=1)\n        \n        #self.fc7 = Linear(1*1*256, 1024) \n            \n        \n            \n  def forward(self, x):\n        idx1,idx2,idx3=0,0,0\n        #x is the input and the size of x is (batch_size, channels, heights, widths).\n        \n        conv1a = self.conv1a(x)\n        rect1a = self.leaky_relu(conv1a)\n        conv1b = self.conv1b(rect1a)\n        \n        rect1 = self.leaky_relu(conv1b)\n        pool1,idx1 = self.pool1(rect1),0\n        \n        \n        conv2a = self.conv2a(pool1)\n        rect2a = self.leaky_relu(conv2a)\n        conv2b = self.conv2b(rect2a)\n        rect2 = self.leaky_relu(conv2b)\n        conv2c = self.conv2c(pool1)\n        res2 = conv2c + rect2\n        pool2,idx2 = self.pool2(res2),0\n        \n        \n        \n        conv3a = self.conv3a(pool2)\n        rect3a = self.leaky_relu(conv3a)\n        conv3b = self.conv3b(rect3a)\n        rect3 = self.leaky_relu(conv3b)\n        conv3c = self.conv3c(pool2)\n        res3 = conv3c + rect3\n        pool3,idx3 = self.pool2(res3),0\n        \n        conv4a = self.conv4a(pool3)\n        rect4a = self.leaky_relu(conv4a)\n        conv4b = self.conv4b(rect4a)\n        rect4 = self.leaky_relu(conv4b)\n        pool4,idx4 = self.pool2(rect4),0\n        \n        \n        conv5a = self.conv5a(pool4)\n        rect5a = self.leaky_relu(conv5a)\n        conv5b = self.conv5b(rect5a)\n        rect5 = self.leaky_relu(conv5b)\n        conv5c = self.conv5c(pool4)\n        res5 = conv5c + rect5\n        pool5,idx5 = self.pool2(res5),0\n        \n        \n        conv6a = self.conv6a(pool5)\n        rect6a = self.leaky_relu(conv6a)\n        conv6b = self.conv6b(rect6a)\n        rect6 = self.leaky_relu(conv6b)\n        res6 = pool5 + rect6\n        pool6,idx6 = self.pool2(res6),0\n        \n        conv7a = self.conv6a(pool6)\n        rect7a = self.leaky_relu(conv7a)\n        conv7b = self.conv6b(rect7a)\n        rect7 = self.leaky_relu(conv7b)\n        res7 = pool6 + rect7\n        pool7,idx6 = self.pool2(res7),0\n        \n        pool8=self.gpool(pool7)\n        \n        \n        #pool9 = pool8.view(pool8.size(0), -1)\n        #print(pool8.shape)\n        \n        fc7 = self.conv8a(pool8)\n        rect7 = self.leaky_relu(fc7)\n        \n        #print(rect7.shape)\n        \n        return rect7,pool1,pool2,pool3,pool4,pool5\n<\/code>\nDecoder\u2013\n<code class=\"lang-auto\">class decoder(nn.Module):\n    def __init__(self, n_deconvfilter,n_class):\n        self.n_class=n_class\n        print(\"\\ninitializing \\\"decoder\\\"\")\n        super(decoder, self).__init__()\n        #2d conv10\n        self.conv10 = ConvTranspose2d(1024, n_deconvfilter[0], 3,stride=2, output_padding=1)#n_deconvfilter[0](we have to replace it with 256)\n        #self.conv7b = ConvTranspose2d(n_deconvfilter[1], 256, 3, padding=1)#n_deconvfilter[0](we have to replace it with 256)\n\n        \n        \n        #2d conv11\n        self.conv11 = ConvTranspose2d(n_deconvfilter[0], n_deconvfilter[1], 3, padding=1,stride=2,output_padding=1)#((4-1)*2+3-2*1)\n        #self.conv8b = ConvTranspose2d(n_deconvfilter[2], n_deconvfilter[2], 3, padding=1)\n        \n        \n        #2d conv12\n        self.conv12 = ConvTranspose2d(n_deconvfilter[1], n_deconvfilter[2], 3, padding=1,stride=2,output_padding=1)\n        #self.conv9b = ConvTranspose2d(n_deconvfilter[3], n_deconvfilter[3], 3, padding=1)\n        #self.conv9c = ConvTranspose2d(n_deconvfilter[2], n_deconvfilter[3], 1)\n        \n        \n        #2d conv13\n        self.conv13 = ConvTranspose2d(n_deconvfilter[2], n_deconvfilter[3], 3, padding=1,stride=2,output_padding=1)\n        #self.conv10b = ConvTranspose2d(n_deconvfilter[4], n_deconvfilter[4], 3, padding=1)\n        #self.conv10c = ConvTranspose2d(n_deconvfilter[4], n_deconvfilter[4], 3, padding=1)\n        \n        \n        #2d conv14\n        self.conv14 = ConvTranspose2d(n_deconvfilter[3], n_deconvfilter[4], 4, padding=1,stride=2)\n        \n        self.conv15 = ConvTranspose2d(n_deconvfilter[4], n_deconvfilter[5], 4, padding=1,stride=2)\n        \n        \n        self.leaky_relu = LeakyReLU(negative_slope= 0.01)\n        self.softmax=nn.Softmax2d()\n        \n    def forward(self, rect7,c1,c2,c3,c4,c5):\n        \n        #rect7=rect7.view([rect7.shape[0],1024,1 , 1])#idx3\n        #unpool7 = self.unpool2d(rect7)###HERE FACING PROBLEM\n        #unpool7=unpool7.cuda()\n        print(rect7.shape)\n  \n        conv10 = self.conv10(rect7)\n        rect10 = self.leaky_relu(conv10)\n        rect10=rect10+c5\n        print(rect10.shape)\n        #resp=res7[0:,0:,:5,:5]\n        \n        #unpool8 = self.unpool2d(res7)#Here is the probelm\n        conv11 = self.conv11(rect10)\n        rect11 = self.leaky_relu(conv11)\n        rect11=rect11+c4 \n      \n        conv12 = self.conv12(rect11)\n        rect12 = self.leaky_relu(conv12)\n        rect12=rect12+c3\n        \n        \n        \n        conv13 = self.conv13(rect12)\n        rect13 = self.leaky_relu(conv13)\n        rect13=rect13+c2\n        \n        conv14 = self.conv14(rect13)\n        rect14 = self.leaky_relu(conv14)\n        rect14=rect14+c1\n        \n        conv15=self.conv15(rect14)\n        #rect10=rect10+c5\n        h=conv15\n        h=self.softmax(conv15)\n        #print(h.shape)\n        \n        soft=h.view(h.shape[0]*self.n_class,h.shape[2]*h.shape[3]*h.shape[1])\n\n        return soft\n      \n      \n<\/code>\nMy images are of size(10 as batch size)\n<code class=\"lang-auto\">[10,3,128,128]\n<\/code>\ntraining\u2013\n<code class=\"lang-auto\">n_epochs = 5\n#model=model.cuda()\nvalid_loss_min = np.Inf # track change in validation loss\nmodel.train()\ntrain_loss = 0.0\nvalid_loss=0.0\nimport datetime\nfor epoch in range(1, n_epochs+1):\n    print(\"ALL ABOUT LOSS(training)--------\",(train_loss\/len(images)),\"----------\/n\")\n    print(\"ALL ABOUT LOSS(valid)--------\",(valid_loss\/len(valid_target)),\"----------\/n\")\n    train_loss = 0.0 \n    valid_loss=0.0\n    model.train()\n    for i in range(len(train_img)):\n          \n          data=train_img[i]#.cuda()\n          tar=train_target[i]#.cuda()\n          #tar=tar.long()\n          \n          print(tar.size())\n            # clear the gradients of all optimized variables\n          optimizer.zero_grad()\n            # forward pass: compute predicted outputs by passing inputs to the model\n            \n          output=model(data)\n          print(output.size())\n          loss = criterion(output, tar)\n         loss.backward()\n            # perform a single optimization step (parameter update)\n          optimizer.step()\n            # update training loss\n          train_loss += loss.item()*data.size(0)\n          if(i==1):\n            print(\"about me 1 \",train_loss)\n            #print(\"time\", datetime.datetime.now().time())\n\n          if(i==43):\n            print(\"\\nabout me 43 \",train_loss)\n            print(\"time\", datetime.datetime.now().time())\n          \"\"\"\n          if(i==200):\n            print(\"\\nabout me 200 \",train_loss)\n            print(\"time\", datetime.datetime.now().time())\n\"\"\"\n    model.eval() \n    \n    for i in range(len(valid_img)):\n          data=valid_img[i]#.cuda()\n          tar=valid_target[i]#.cuda()\n          loss = torch.mean(criterion(model(data), tar))\n          valid_loss += loss.item()*data.size(0)\n<\/code>\nAnd i am using Optimizers and loss as-\n<code class=\"lang-auto\">import torch.optim as optim\ncriterion =  nn.BCELoss()\noptimizer = optim.Adam(model.parameters(), lr=1e-5,eps=1e-08)\n<\/code>\nI think the problem is in the Loss function\nMy images are Binary in nature\u2013\n\nSo i create label map as-\n<code class=\"lang-auto\">tensor([[0., 0., 0.,  ..., 0., 0., 0.],\n        [0., 0., 0.,  ..., 0., 0., 0.],\n        [0., 0., 0.,  ..., 1., 1., 1.],\n        ...,\n        [0., 0., 0.,  ..., 0., 0., 0.],\n        [0., 0., 0.,  ..., 1., 1., 1.],\n        [0., 0., 0.,  ..., 0., 0., 0.]])\n<\/code>\nI guess the probelm is with the last layer.\nUsing softmax 2d all my output converts to 1(each cell).\nI am not getting where i am lacking.\nPlease Help.","y":"What about having some modification on lr_scheduler?","z":"Use Cross Entropy Loss instead.\n Using CrossEntropyLoss it throws error-\n<code class=\"lang-auto\">RuntimeError: multi-target not supported at \/pytorch\/aten\/src\/THCUNN\/generic\/ClassNLLCriterion.cu:15\n<\/code>\nPlease revisit decoder code\nWell thats because you must have one-hot encoded it, just use a matrix having class labels for each sample (batch_size,1,H,W). Have a look at the documentation of Cross Entropy Loss, It would be much more clear to you. Your prediction output should be having 2 channels.\n My target Image contains only 1(white) and 0(black) . So i created a label map where background is 0 and area of interest is 1.Hence my output dimension is 1(Please rectify me if i am going the wrong way).\nAnd i dint fully understand to use one-hot vectors in this case(I used one hot in multi class where the dimension is equal to the number of classes).\nPlease tell me how can i use one hot encoding in this case for the representation of target.\nwhat are the dimension of your prediction and target which your are passing in the loss function?\ntarget-\n[10,16384]\nHere 10 is the batch size\nprediction(output by the model)-\n[10,16384]\nIt just flattening the decoder output- 128* 128 *1\nfor segmentation task, you will be appying softmax over each pixel. So your prediction should be (batch_size,channel,H,W) and target should be(batch_size,H,W). In your case channel will be 2.\n I am not fully understand what you are saying.\nWill it not lead to size mismatch.\nAs prediction-\n10* 2 *128 *128\nAnd target-\n10* 128 *128\nif its about ,that we need to convert our target to one hot. So Please explain it, how i can convert it in 2 channel dimension with only 1 and 0 as labels.\nhttps:\/\/pytorch.org\/docs\/stable\/nn.html#crossentropyloss\nI don\u2019t think so it will cause any problem. Are you getting any error on doing so?\n Hey i worked.Thank You.\nPlease rectify what mistake i was doing(I am not fully understood).(i have to convert the prediction again back to image)\nAnd my training loss is also decreasing to slowly.\n<code class=\"lang-auto\">criterion =  nn.CrossEntropyLoss()\n# specify optimizer\noptimizer = optim.Adam(model.parameters(), lr=1e-5,eps=1e-08)\n<\/code>\nAny help u can provide with this.\nWell there was no harm in even using BCELoss, just you had to read the what exactly are inputs  it expects. For getting prediction labels, you can use torch.argmax. If you think your issue is solved, please mark the answer as solution.\nOne-hot encoding is required to compute the loss but that is taken care of by the pytorch loss function itself. So I would say your understanding was right but you need to change data representation as expected by the function :).\n Thanks for replying.\nWhat i basically follow.\nFor semantic segmentation, if you have only one label then you can just have a label map where at each pixel you have a 1 where the object of interest is and 0s everywhere else. If you have >1 label of interest, you should use one hot encoding to generate your label map.\n(Please provide the important info that i am lacking)\nSo in this please explain what actually is happening in the last layer of the decoder as we r using cross entropy loss.\nWhat i basically understand is if we have multiclass then each dimesion holds like\u2026 For the first layer of that dimension I would have ones for each pixel that belongs to class 1, for layer 2 I would have ones at all of the pixels that belong to class 2, etc. etc.\nIf there are more than 2 labels also you don\u2019t need one-hot as it is taken care of by the framework.\n Okk. Thank you very much\n Can u please help me as my semantic segmentation on Pascal voc dataset as it is not working. I have changed the model to FCN8 but it dint work.\nI Have trained my model but the loss reduces very small.\nI am providing you the link to the google collab file and also of the model checkpoint.\nPlease help. and also suggest where i am lacking.\nCollab Link-\nhttps:\/\/colab.research.google.com\/drive\/1PQVOwf4yBan0Rt9yDZS9_mCYksc59COV\nModel Checkpoint-\nhttps:\/\/drive.google.com\/file\/d\/1r_xOEXgmDLiSgk6VgH3AiSjxFX279C11\/view?usp=sharing\n if possible please help.\nThe learning rate seems to be very low with 1e-7. Was the model not training at all with higher learning rates?\n Thanks for replying. I have tried learning rate 0.0001, the loss decreased upto 4 epochs but then the loss almost stops decreasing(very less decrement in loss).\nThen i tried 0.001 and also 0.000001 , but no good results.I have tried different eps values and different lr values but its like the loss decreased significant like 0.4012343 to 0.3512435 and then after some some epoch(2 or 4 epochs) the loss started decreasing very very less.\nPlease also rectify me if my approach is right  towards semantic(basically what i have coded is right or i am doing something wrong).\nWhat about having some modification on lr_scheduler?"},{"x":"So I have simple test network for MNIST data as following\n<code class=\"lang-auto\">def forward(self, x):\n        x = F.relu(F.max_pool2d(self.conv1(x), 2))\n        x = F.relu(F.max_pool2d(self.conv2_drop(self.conv2(x)), 2))\n        x = x.view(-1, 80)\n        x = F.relu(self.fc1(x))\n        x = F.dropout(x, training=self.training)\n        x = self.fc2(x)\n        return F.XXXX(x)\n<\/code>\nI am using NLLLoss function to implement cross entropy loss explicitly\nI wanted to understand how different act function affects the accuracy,\nso I tried LogSoftmax and verify that the network trains but for some reason when I used LogSigmoid, the network fails to train.\n(note that NLLLoss expects log probability)\nSince softmax and sigmoid both have output value between 0 and 1.\nI thought there shouldn\u2019t be an issue.\nCan anyone explain the detail I am not catching here?","y":"To me, the logsigmoid+NLLLoss combination hardly makes any sense, because the objective function only tries to promote the gt_class, but no suppression on the negative ones. Maybe you wanna try sigmoid+bceloss.","z":"\nSince both outputs the value between 0 and 1. I thought there shouldn\u2019t be an issue.\n\nthe output of log sigmoid isn\u2019t between 0 and 1, pls refer to docs with link \"https:\/\/pytorch.org\/docs\/stable\/nn.html?highlight=logsigmoid#torch.nn.LogSigmoid\"\nSorry that was a mistake. I knew they are not.\nI meant to say sigmod and softmax both output the value between 0 and 1.\npost is updated\nTo me, the logsigmoid+NLLLoss combination hardly makes any sense, because the objective function only tries to promote the gt_class, but no suppression on the negative ones. Maybe you wanna try sigmoid+bceloss.\nThank you so much for your advice.\nthe model trains with bce even with sigmoid\nsoftmax works as well with bce but only up to a certain point and the training collapse.\nI am not sure why but I guess it has something to do with the dependency among classes\u2026\nDo you know if there is a loss function which will be good for both activation function.\na naive answer: If you really want to test with a single loss function for both activation functions,  what about L2 loss with one-hot vectors as target?\nI\u2019m not sure if it will give good performance though.\nSoftmax is actually not an activation function\u2026\nlogsigmoid+nllloss doesn\u2019t make sense mathematically (if you derive the gradients, you\u2019ll find it.)\nPyTroch documentation says NLLLoss expects log probability though\nand that is logsigmoid + nllloss is how CrossEntropyLoss is constructed for PyTorch\nnn.CrossEntropyLoss uses F.log_softmax and nn.NLLLoss internally as shown here with link \"https:\/\/github.com\/pytorch\/pytorch\/blob\/2e97c82470966df6942f364102690460ea58403e\/torch\/nn\/functional.py#L2028\"."},{"x":"I\u2019m trying to train a classifier on 15k images over five categories using googlenet architecture.\nI followed the fine-tune tutorial (but used as pretrained=false just to train from scratch).\nBut the training is only possible if i set the \u2018aux logits as false\u2019\n\u2018\u2019\u2019\nmodel.aux.logits=False\n\u2018\u2019\u2019\nCan someone explain why I have to do this for training?","y":"For GoogLeNet, there are 2 aux branches. So we have to do this way:\n<code class=\"lang-auto\">aux1, aux2, output = model(inputs)     \nloss1 = criterion(outputs, target)\nloss2 = criterion(aux1, target)\nloss3 = criterion(aux2, target)\nloss = loss1 + 0.3*(loss2+loss3)\n<\/code>\nFor Inception v3, it has only one aux branch.\n<code class=\"lang-auto\">outputs, aux_outputs = model(inputs)\nloss1 = criterion(outputs, target)\nloss2 = criterion(aux_outputs, target)\nloss = loss1 + 0.4*loss2\n<\/code>\nNow, it\u2019s working!. Thanks for the followup.","z":"Do you get an error if you leave aux_logits=True?\nAs far as I remember they were in fact only used during training in the original paper.\nHi thanks. I\u2019m following the pytorch\/examples\/imagenet script for training. I also seen your similar comments after posting this topic. \nWhen I set true and changed\n<code class=\"lang-auto\">loss = criterion(output[0], target)\n<\/code>\nthen in the script, wherever there is an  output it gives dimension error.\nIf i don\u2019t use aux_logits=False, will that effect the val_acc?\nCould you post the dimension error?\nSince the Inception model is quite deep, the auxiliary loss was used to stabilize the training.\nIf you are training from scratch, using the aux_loss might help.\nThanks. I used this approach from the pytorch tutorial on fine-tune inception and modified the examples\/imagenet\/train.py\n<code class=\"lang-auto\">outputs, aux_outputs = model(inputs)\nloss1 = criterion(outputs, target)\nloss2 = criterion(aux_outputs, target)\nloss = loss1 + 0.4*loss2\n<\/code>\nnow the error :\n<code class=\"lang-auto\">Traceback (most recent call last):\n  File \"imagenet.py\", line 406, in <module>\n    main()\n  File \"imagenet.py\", line 113, in main\n    main_worker(args.gpu, ngpus_per_node, args)\n  File \"imagenet.py\", line 239, in main_worker\n    train(train_loader, model, criterion, optimizer, epoch, args)\n  File \"imagenet.py\", line 279, in train\n    output, aux_outputs = model(input)\nValueError: too many values to unpack (expected 2)\n\n<\/code>\nIs your model is train() and did you leave aux_logits=True?\nHi. Yes. It is in train() and leave aux_logits=True.\nimagenet based script process over the batch iteration for one epoch, then before second epoch it gave me that error. It tries to go the eval() for the first epoch to give the acc over that batch right?\nBelow is the snippet for main_worker function.\n<code class=\"lang-auto\">def main_worker(gpu, ngpus_per_node, args):\n.....\n.....\n.....\nfor epoch in range(args.start_epoch, args.epochs):\n        if args.distributed:\n            train_sampler.set_epoch(epoch)\n        adjust_learning_rate(optimizer, epoch, args)\n\n        # train for one epoch\n        train(train_loader, model, criterion, optimizer, epoch, args)\n\n        # evaluate on validation set\n        acc1 = validate(val_loader, model, criterion, args)\n\n        # remember best acc and save checkpoint\n        is_best = acc1 > best_acc1\n        best_acc1 = max(acc1, best_acc1)\n\n        if not args.multiprocessing_distributed or (args.multiprocessing_distributed\n                and args.rank % ngpus_per_node == 0):\n            save_checkpoint({\n                'epoch': epoch + 1,\n                'arch': args.arch,\n                'state_dict': model.state_dict(),\n                'best_acc1': best_acc1,\n                'optimizer' : optimizer.state_dict(),\n            }, is_best)\n\n\n<\/code>\nIt seems that the model might still be in eval() after the first epoch.\nThe aux_logits will only be returned in train() mode, so make sure to activate it before the next epoch.\nNo, it didn\u2019t went to eval(), its just done with the train(), before going to eval(), error thrown.\nI just chaged the def train() params, set def validate() as it is. am I incorrect?\nimagenet script organization:\n<code class=\"lang-auto\">def main()\ndef main_worker()\ndef train()\n   outputs, aux_outputs = model(inputs)\n   loss1 = criterion(outputs, target)\n   loss2 = criterion(aux_outputs, target)\n   loss = loss1 + 0.4*loss2\ndef validate()\n   outputs = model(inputs)\n   loss = criterion(outputs, target)\ndef adjust_learning_rate()\ndef accuracy()\n\n<\/code>\nTry to set the desired mode specifically in both functions:\n<code class=\"lang-python\">def train():\n    model.train()\n    ...\n\ndef validate():\n    model.eval()\n    ...\n<\/code>\nYeah, set to desired mode in both functions.  and gave below command\npython imagenet.py -a googlenet --epochs 30 --batch-size 96 --gpu 1 data\/\nAnd you still get the ValueError?\nCould you create a gist so that I could have a look at the complete code?\nYes, I still get the error. ;\/ , usually why this error raises\nValueError: too many values to unpack (expected 2).\nDoes it says, in outputs, aux_outputs = model(inputs), model(1, 2) where 2 expected?\ngist: https:\/\/gist.github.com\/rajasekharponakala\/80514484ea7a38dc444cdc244dc9f950\n(same but just the loss modification for googlenet, script from pytorch\/examples\/imagenet\/main.py)\nThere should be some problem with the yielding values of both sides. not sure ;\/\noutputs, aux_outputs = model(inputs)\nFor GoogLeNet, there are 2 aux branches. So we have to do this way:\n<code class=\"lang-auto\">aux1, aux2, output = model(inputs)     \nloss1 = criterion(outputs, target)\nloss2 = criterion(aux1, target)\nloss3 = criterion(aux2, target)\nloss = loss1 + 0.3*(loss2+loss3)\n<\/code>\nFor Inception v3, it has only one aux branch.\n<code class=\"lang-auto\">outputs, aux_outputs = model(inputs)\nloss1 = criterion(outputs, target)\nloss2 = criterion(aux_outputs, target)\nloss = loss1 + 0.4*loss2\n<\/code>\nNow, it\u2019s working!. Thanks for the followup.\nThanks for the information and sorry for missing that you are using GoogleNet and not Inception_v3.\nI\u2019m glad you figured it out! \nYeah, I haven\u2019t noticed too, someone from the github\/pytorch\/issues on the same topic has figured it out \n\nHi!\nThis time, there is little confusion with the fc layer. I followed the finetune tutorial (just want to run with aux_logits=True): for inception as there is only one aux_logit below snippet working fine.\n<code class=\"lang-auto\">    elif model_name == \"inception\":\n        \"\"\" Inception v3 \n        Be careful, expects (299,299) sized images and has auxiliary output\n        \"\"\"\n        model_ft = models.inception_v3(pretrained=use_pretrained)\n        set_parameter_requires_grad(model_ft, feature_extract)\n        # Handle the auxilary net\n        num_ftrs = model_ft.AuxLogits.fc.in_features\n        model_ft.AuxLogits.fc = nn.Linear(num_ftrs, num_classes)\n        # Handle the primary net\n        num_ftrs = model_ft.fc.in_features\n        model_ft.fc = nn.Linear(num_ftrs,num_classes)\n        input_size = 299\n<\/code>\ncorrespoing inception_v3 net file snippet:\n<code class=\"lang-auto\">if self.training and self.aux_logits:\naux = self.AuxLogits(x)\n<\/code>\nand the fc snippet:\n<code class=\"lang-auto\">self.fc = nn.Linear(768, num_classes)\n<\/code>\nWhereas for GoogLeNet has two auxilary outputs, the net file snippet has:\n<code class=\"lang-auto\">if self.training and self.aux_logits:\naux1 = self.aux1(x)\n.....\nif self.training and self.aux_logits:\naux2 = self.aux2(x)\n<\/code>\nand the fc snippets:\n<code class=\"lang-auto\">self.fc1 = nn.Linear(2048, 1024)\nself.fc2 = nn.Linear(1024, num_classes)\n<\/code>\nNow, my confusion is about using the fc in finetuning script, how to embed?\n<code class=\"lang-auto\">num_ftrs = model_ft.(aux1\/aux2).(fc1\/fc2).in_features\nmodel_ft.(aux1\/aux2).(fc1\/fc2) = nn.Linear(num_ftrs, num_classes)\n<\/code>\nany thoughts?"},{"x":"Hi! I am trying to implement the neural style transfer model from the original Gatys\u2019 paper from scratch. I am aware of the tutorial on the website, but I am trying to implement it myself to see if I understand the model right, also, I am trying to stay as close as possible to the paper.\nI have come across some problems, specifically a weird mixture of the content and the style. I can see that something is happening but the results are not pleasing, so I would appreciate any help.\nAs far as I understand, we need to minimize the sum of the content loss and the style loss. The content loss is the MSE between the activations of the content image and the generated image (noise) from one of the deeper layers of the VGG19 model. The style loss is the MSE between the Gramian matrices (across multiple layers) of style image and the generated image.\nI am trying to use Adam optimizer and theoretically it should work as we are performing the gradient descent in the image space.\nHere are the images I am trying to merge:\nContent and Style images:\ncontent_img_1.jpg2160\u00d72016 240 KB with link \"https:\/\/discuss.pytorch.org\/uploads\/default\/original\/2X\/1\/15027c525c6b3cedc9074317ecb33596c5daa2ce.jpeg\" style_img.jpeg1204\u00d7893 223 KB with link \"https:\/\/discuss.pytorch.org\/uploads\/default\/original\/2X\/b\/b9477aef3c9969257bad540361ca6b2efd74ff00.jpeg\"\nResult:\n\nObviously, surely something is not right in my implementation. Here is my code, in case if anyone is able to point me in the right direction.\n<code class=\"lang-auto\">import torch\nimport torch.nn as nn\nimport torchvision.models as models\nfrom PIL import Image\nfrom torchvision import transforms\nfrom torch import optim\nimport torch.nn.functional as F\nimport numpy as np\nimport matplotlib.pyplot as plt\nimport warnings\nfrom torchvision.utils import save_image\n\nwarnings.simplefilter('ignore')\n\ndef gram(tensor):\n    return torch.mm(tensor, tensor.t()) \n\ndef gram_loss(noise_img_gram, style_img_gram, N, M):\n    return torch.sum(torch.pow(noise_img_gram - style_img_gram, 2)).div((np.power(N*M*2, 2, dtype=np.float64)))\n\n# read the images\ncont_img = Image.open('.\/content_img_1.jpg')\nstyle_img = Image.open('.\/style_img.jpeg')\n\n# define the transform\ntransform = transforms.Compose([transforms.Resize((224, 224)), \n                                transforms.ToTensor(), \n                                transforms.Normalize([0.485, 0.456, 0.406],\n                                                     [0.229, 0.224, 0.225])])\n\n# get the tensor of the image\ncontent_image = transform(cont_img).unsqueeze(0).cuda()\nstyle_image = transform(style_img).unsqueeze(0).cuda()\n\n# define the VGG\nclass VGG(nn.Module):\n    def __init__(self):\n        super(VGG, self).__init__()\n        \n        # load the vgg model's features\n        self.vgg = models.vgg19(pretrained=True).features\n    \n    def get_content_activations(self, x):\n        return self.vgg[:32](x)\n    \n    def get_style_activations(self, x):\n        # block1_conv1, block2_conv1, block3_conv1, block4_conv1, block5_conv1\n        return [self.vgg[:30](x)] + [self.vgg[:21](x)] + [self.vgg[:12](x)] + [self.vgg[:7](x)] + [self.vgg[:4](x)]\n    \n    def forward(self, x):\n        return self.vgg(x)\n\n# init the network\nvgg = VGG().cuda().eval()\n\n# lock the gradient\nfor param in vgg.parameters():\n    param.requires_grad = False\n\n# get the content activations of the content image and detach them from the graph\ncontent_activations = vgg.get_content_activations(content_image).detach()\n\n# get the style activations of the style image\nstyle_activations = vgg.get_style_activations(style_image)\n\n# unroll the content activations\ncontent_F = content_activations.view(512, -1)\n\n# for every layer in the style activations\nfor i in range(len(style_activations)):\n    \n    # unroll the activations and detach them from the graph\n    style_activations[i] = style_activations[i].squeeze().view(style_activations[i].shape[1], -1).detach()\n\n# calculate the gram matrices of the style image\ngram_matrices = [gram(style_activations[i]) for i in range(len(style_activations))]\n\n# generate the Gaussian noise\nnoise = torch.randn(1, 3, 224, 224, device='cuda', requires_grad=True)\n\n# define the adam optimizer\n# pass the noise image pixels to the optimnizer as parameters\nadam = optim.Adam(params=[noise], lr=0.01)\n\n# run the iteration\nfor iteration in range(10000):\n    \n    # zero grad\n    adam.zero_grad()\n    \n    # get the content activations of the Gaussian noise\n    noise_content_activations = vgg.get_content_activations(noise)\n    \n    # unroll the feature maps of the noise\n    noise_content_F = noise_content_activations.view(512, -1)\n    \n    # calculate the loss\n    content_loss = 1\/2. * torch.sum(torch.pow(noise_content_F - content_F, 2))\n    \n    # get the style activations of the noise image\n    noise_style_activations = vgg.get_style_activations(noise)\n    \n    # for every noise style activation layer\n    for i in range(len(noise_style_activations)):\n        \n        # unroll the the noise style activations\n        noise_style_activations[i] = noise_style_activations[i].squeeze().view(noise_style_activations[i].shape[1], -1)\n    \n    # calculate the noise gram matrices\n    noise_gram_matrices = [gram(noise_style_activations[i]) for i in range(len(noise_style_activations))]\n    \n    # calculate the total style loss\n    style_loss = 0\n    for i in range(len(style_activations)):\n        N, M = noise_style_activations[i].shape[0], noise_style_activations[i].shape[1]\n        style_loss += (gram_loss(noise_gram_matrices[i], gram_matrices[i], N, M) \/ 5.)\n\n    style_loss = style_loss.cuda()\n    total_loss = content_loss + 10000 * style_loss\n    \n    if iteration % 1000 == 0:\n        print(\"Iteration: {}, Content Loss: {}, Style Loss: {}\".format(iteration, content_loss.item(), 10000 * style_loss.item()))\n        save_image(noise, filename='.\/generated\/iter_{}.png'.format(iteration))\n        \n    total_loss.backward()\n    adam.step()\n<\/code>","y":"Looks like, ultimately, the problem was with the content - style loss balance. I also increased the resolution of the images and got some good results.\niter_9500.png512\u00d7512 259 KB with link \"https:\/\/discuss.pytorch.org\/uploads\/default\/original\/2X\/9\/945e03a96afdf6b0c5d891d41e50907494c475ef.png\" iter_14800.png512\u00d7512 244 KB with link \"https:\/\/discuss.pytorch.org\/uploads\/default\/original\/2X\/5\/50a8c29404db35cb528b51042bc76e523f3b4860.png\"\nThank you, everyone!","z":"Congrats! That doesn\u2019t look all that bad! And I do think that implementing things yourself is one of the best ways to learn.\nNow regarding your question: vgg does have \u201cimplied\u201d or learned length scales, so maybe your images are too large to have the same length scale?\nI think I saw someone experiment with rescaling and style transfer, maybe on the fast.ai forum?\nBest regards\nThomas\nHi Tom, I am not sure what you mean by length scales? Do you mean feature map sizes?\nTry scaling your two inputs, e.g half the size (well the lengths, so size will be a quarter) before you start.\n hi, would you mind helping me with this issue by chance?\nThe style reconstructions have some weird artifacts, like this green pixel tint.\n with link \"https:\/\/media.giphy.com\/media\/YSTqbvsP9hi1DUIVH2\/giphy.gif\"\n\nTom, you mean  decrease the size of the input images?\n<code class=\"lang-auto\">def total_variation_loss(image):\n    # shift one pixel and get difference (for both x and y direction)\n    loss = torch.mean(torch.abs(image[:, :, :, :-1] - image[:, :, :, 1:])) + \\\n            torch.mean(torch.abs(image[:, :, :-1, :] - image[:, :, 1:, :]))\nreturn loss\n<\/code>\nAdding total_variation_loss will be better\nThank you for your suggestion, I added it to the model. I also replaced all the max pooling layers with the average pooling layers.\nMaybe you can add tv_loss to the tutorial of neural style transfer if you are interested.\n\n\ngithub.com\/pytorch\/tutorials with link \"https:\/\/github.com\/pytorch\/tutorials\/issues\/168\"\n\n\n with link \"https:\/\/github.com\/Naruto-Sasuke\"\nIssue: Lack of TV loss in Neural Style tutorial. with link \"https:\/\/github.com\/pytorch\/tutorials\/issues\/168\"\n\n\n\topened by Naruto-Sasuke with link \"https:\/\/github.com\/Naruto-Sasuke\"\n\ton 2017-10-24 with link \"https:\/\/github.com\/pytorch\/tutorials\/issues\/168\"\n\n\n\tclosed by chsasank with link \"https:\/\/github.com\/chsasank\"\n\ton 2018-01-22 with link \"https:\/\/github.com\/pytorch\/tutorials\/issues\/168\"\n\n\nIn original paper, tv loss is added to help generate more clear images. I wonder if it deserves adding.\n\n\n\n\n\n\n\n\nThank you for the suggestion\nLooks like, ultimately, the problem was with the content - style loss balance. I also increased the resolution of the images and got some good results.\niter_9500.png512\u00d7512 259 KB with link \"https:\/\/discuss.pytorch.org\/uploads\/default\/original\/2X\/9\/945e03a96afdf6b0c5d891d41e50907494c475ef.png\" iter_14800.png512\u00d7512 244 KB with link \"https:\/\/discuss.pytorch.org\/uploads\/default\/original\/2X\/5\/50a8c29404db35cb528b51042bc76e523f3b4860.png\"\nThank you, everyone!"},{"x":"Hi there,\nNow I have a tensor A with the shape (n, c, h, w). Suppose the first channel of the tensor is like:\n[[1, 2, 3, 4],\n[5, 6, 7, 8],\n[9, 10, 11, 12],\n[13, 14, 15, 16]]\nI want to reshape the tensor into the shape (n, c, h*w) by grids. If the grid is 2x2, then the reshaped tensor should be:\n[1, 2, 5, 6,\n3, 4, 7, 8,\n9, 10, 13, 14,\n11, 12, 15, 16]\nHow can I implement this using existing functions in PyTorch?","y":"For a general use case using a grid you could use .fold():\n<code class=\"lang-python\">x = torch.arange(1, 17).float().view(1, 1, 4, 4)\n\nkh, kw = 2, 2  # kernel_size\ndh, dw = 2, 2  # stride\n# get all image windows of size (kh, kw) and stride (dh, dw)\ninput_windows = x.unfold(2, kh, dh).unfold(3, kw, dw)\noutput = input_windows.contiguous().view(x.size())\n<\/code>","z":"try this:\n<code class=\"lang-auto\">A.view(A.size()[0], A.size()[1], A.size(2)*A.size(3))\n<\/code>\nThank you for your kind reply!\nHowever, this definitely does not work as it reshapes the tensor by row instead of the grid. In this case, the output should be like\n[1, 2, 3, 4,\n5, 6, 7, 8,\n9, 10, 11, 12,\n13, 14, 15, 16]\nwhich is not what I want.\nFor a general use case using a grid you could use .fold():\n<code class=\"lang-python\">x = torch.arange(1, 17).float().view(1, 1, 4, 4)\n\nkh, kw = 2, 2  # kernel_size\ndh, dw = 2, 2  # stride\n# get all image windows of size (kh, kw) and stride (dh, dw)\ninput_windows = x.unfold(2, kh, dh).unfold(3, kw, dw)\noutput = input_windows.contiguous().view(x.size())\n<\/code>\nThank you sooooooo much for your help! This works for me XD\nI found the mem cost is a little bit high when plugged this part of code into my own. I guess it\u2019s because the operation contiguous that copies the tensor input_windows twice in the memory. Is there any way to avoid the memory copy?\nI don\u2019t think you can avoid the copy, but you can free some memory if you assign the result back to input_windows.\nPS: by .fold() I obviously meant .unfold() "},{"x":"CPU usage is around 250%(ubuntu top command) was using torchvision transforms to convert cv2 image to torch\n<code class=\"lang-auto\">normalize_transform = transforms.Compose([ transforms.ToTensor(), transforms.Normalize([0.5, 0.5, 0.5], [0.5, 0.5, 0.5]) ])\n\ndef normalizeCvImage(image_cv, device):\n    return normalize_transform(image_cv).unsqueeze(0).to(device)\n<\/code>\nBut usage drops to 100% when I do the operation manually,\n<code class=\"lang-auto\">def normalizeCvImage(image_cv, device):\n    image = torch.Tensor(image_cv).to(device)\n    image = image.permute(2, 0, 1).unsqueeze(0)\n    image = (image - 127.5) \/ 127.5\n    return image\n<\/code>\nMore interestingly, this happens only on  version 1.0.1.post2 (Cuda 10), whereas if I stick with version 1.0.0 (Cuda 10)  the difference is minimal.","y":"I see. So you are not using DataLoader or anything like that but just calling that function on each image separately?\nIn your function\n<code class=\"lang-python\">def normalizeCvImage(image_cv, device):\n    image = torch.Tensor(image_cv).to(device)\n    image = image.permute(2, 0, 1).unsqueeze(0)\n    image = (image - 127.5) \/ 127.5\n    return image\n<\/code>\nyou have this \u201cearly\u201d to(device) whereas for the other, you do it at the end of the pipeline. Since you are using CUDA, this could explain the difference maybe. I.e., for the second \u201clower CPU\u201d usage version, you could do\n<code class=\"lang-auto\">def normalizeCvImage(image_cv, device):\n    image = torch.Tensor(image_cv)\n    image = image.permute(2, 0, 1).unsqueeze(0)\n    image = (image - 127.5) \/ 127.5\n    return image.to(device)\n<\/code>\nand see what happens","z":"Maybe it\u2019s doing some more multithreading or so? Have you benchmarked for speed? Maybe the higher CPU utilization is desirable if you have multiple CPUs?\nActually the speed (FPS) remains the same! Both run ~30fps (which is my cam\u2019s max speed). Eventually I\u2019ll run this on a Jetson TX2 which has a weak CPU compared to conventional ones. So if its 250% on my i7 + 1070, it would be worse on my TX2\nNot that it makes a difference, probably, but conceptually, you also have an extra step in normalize_transform compared to you other function. I.e., transforms.ToTensor() converts tensors to [0, 1] range. So, it\u2019s basically first doing that conversion, and then in an additional operation, it is doing (x-0.5) * 2.\nHow exactly are you running the two function to compare them, in a Python for-loop?\nI\u2019ve implemented MTCNN face detection and this is the part which feeds images from the camera to the network. The high CPU usage was bugging me (even though everything is on CUDA) and was playing around until I stumbled upon this. Everything else was kept the same except this function and surprise CPU usage dropped to 100%.\nI see. So you are not using DataLoader or anything like that but just calling that function on each image separately?\nIn your function\n<code class=\"lang-python\">def normalizeCvImage(image_cv, device):\n    image = torch.Tensor(image_cv).to(device)\n    image = image.permute(2, 0, 1).unsqueeze(0)\n    image = (image - 127.5) \/ 127.5\n    return image\n<\/code>\nyou have this \u201cearly\u201d to(device) whereas for the other, you do it at the end of the pipeline. Since you are using CUDA, this could explain the difference maybe. I.e., for the second \u201clower CPU\u201d usage version, you could do\n<code class=\"lang-auto\">def normalizeCvImage(image_cv, device):\n    image = torch.Tensor(image_cv)\n    image = image.permute(2, 0, 1).unsqueeze(0)\n    image = (image - 127.5) \/ 127.5\n    return image.to(device)\n<\/code>\nand see what happens\nWohoo, You are right , when I do that CPU usage jumps back to 250s. That makes sense as well. Is there better way to implement this in torchvision.transforms?\n\nI see. So you are not using DataLoader or anything like that but just calling that function on each image separately?\n\nYes\nI am not sure what the issue with the high CPU usage is exactly. The reason why you got lower CPU usage in the other case is not because the implementation was more efficient but because it was done on the GPU instead. What really matter, I\u2019d say, is how fast your processing step finishes altogether. If you do steps on the GPU, sure, your CPU usage will drop, but doesn\u2019t mean it\u2019s faster or more efficient as these may be steps that a CPU is better at (also you need to consider that data transfer to GPU is slow as well)\nAgreed, In my case, my TX2 has ample GPU to spare, but not CPU. So this serves the purpose as long as the performance isn\u2019t hurt (And looks like it is not in this case) . What I was asking - Is there a way to move the tensor to GPU and then do normalize in torchvision.transforms or is it not meant to do this altogether?\noh and I forgot to thank you, Thank you!!\nHm, I am not sure if that\u2019s possible since the API is mainly targeted for DL training where the GPU is busy running the model (also, some people like to put transforms in there that are not implemented on GPUs, e.g., PIL or OpenCV stuff). I think in your case, it\u2019s maybe not a good use case for the data loader with custom transform. I would use maybe the data loader just for iterating and do the processing then via a manual function like you did."},{"x":"this is the shape of numpy array, (35628, 1, 16000)\n\n\ni train\/test split it\nfrom sklearn.model_selection import train_test_split\nx_train, x_test ,y_train, y_test = train_test_split(datan, label, test_size=0.2,  shuffle=True, random_state=40)\n(this works fine)\n\n\nmake dataset from split arrays\nimport torch.utils.data as utils\n\n\ntensor_x = torch.stack([torch.Tensor(i) for i in list(x_train)])\ntensor_y = torch.Tensor(y_train)\nmy_dataset = utils.TensorDataset(tensor_x, tensor_y)\ntrainloader = utils.DataLoader(my_dataset, batch_size = 1)\ntensor_xte = torch.stack([torch.Tensor(i) for i in list(x_test)])\ntensor_yte = torch.Tensor(y_test)\nmy_datasette = utils.TensorDataset(tensor_xte, tensor_yte)\ntestloader = utils.DataLoader(my_datasette, batch_size = 1)\nERROR HERE!!! (session crashes in google colab)\nThis works well for all other dataset, but its not working only for this dataset.\nis it becoz the data is large?? if yes, how do i do it.","y":"It should work with any size as long as your system can handle it properly.\nIf you are using np.float32 values, the data should take approx 2GB of RAM.\nAre you using multiple workers in your DataLoaders?\nI\u2019m not sure, what limitations Colab has on the RAM.\nAlso, could you try to use torch.from_numpy instead of the list comprehension with torch.stack?\nThe former approach would avoid a copy of the data.","z":"Try to lower the size of your Dataset and run the code again.\nI haven\u2019t used Colab that often, but do you get any error message?\nTo lower the size, try to slice both numpy arrays:\n<code class=\"lang-python\">datan = datan[:100]\nlabel = label[:100]\n<\/code>\nhi  , yeah it works normally with numpy array of shape(100,1,16000). How do i add large numpy to dataloader?\nIt should work with any size as long as your system can handle it properly.\nIf you are using np.float32 values, the data should take approx 2GB of RAM.\nAre you using multiple workers in your DataLoaders?\nI\u2019m not sure, what limitations Colab has on the RAM.\nAlso, could you try to use torch.from_numpy instead of the list comprehension with torch.stack?\nThe former approach would avoid a copy of the data.\nI converted numpy array to float32 and used torch.from_numpy , it worked.\nThank you "},{"x":"I\u2019m trying to implement a masked loss function which only works on a certain region of the output. Over the training phase, the regions not considered in the loss are getting blown\/vanish.\n<code class=\"lang-python\">    def masked_l1_loss(self, x, trgt):\n        mask = (trgt > -1).detach()\n        diff = trgt - x\n        diff = diff[mask]\n        return diff.abs().mean()\n<\/code>","y":"If I understand your problem. You only want to backpropagate through only those pixels that are True in the mask. In that case, you cannot use the approach, you defined in the question as that would backpropagate through all the pixels.\nUse this code instead\n<code class=\"lang-auto\">class myModel(nn.Module):\n    def __init__(self):\n        super().__init__()\n        self.linear = nn.Linear(1, 10)\n    def forward(self, x, trgt):\n        x = self.linear(x)\n        \n        # Mask code\n        mask = trgt > -1\n        x[mask==0] = 0\n        return x\nmodel = myModel()\n\n# Dummy inputs:- Assume mask is 10 units\nx = torch.rand(1, 1)\ntrgt = torch.arange(-4, 6).view(1, 10).to(dtype=torch.float)\n\n# Initial param valuies\nprint('Before update')\nprint(model.state_dict())\n\n# Run a epoch step\n# Large lr to visualize easily\noptimizer = optim.SGD(model.parameters(), lr=100)\noptimizer.zero_grad()\n\noutput = model(x, trgt)\n# This step is copied from your question as you take diff[mask]\ntrgt[trgt<=-1]=0\n\nloss = torch.mean(torch.abs(output - trgt))\nloss.backward()\noptimizer.step()\n\nprint('\\nAfter update')\nprint(model.state_dict())\n<\/code>\n<code class=\"lang-auto\">Before update\nOrderedDict([('linear.weight', tensor([[ 0.2001],\n        [ 0.8265],\n        [ 0.4066],\n        [ 0.8853],\n        [ 0.3886],\n        [-0.1819],\n        [ 0.4687],\n        [-0.8151],\n        [ 0.6224],\n        [-0.1107]])), ('linear.bias', tensor([ 0.1837,  0.0316, -0.2824,  0.9075,  0.6819,  0.6282, -0.9894, -0.2376,\n         0.2664, -0.9535]))])\n\nAfter update\nOrderedDict([('linear.weight', tensor([[ 0.2001],\n        [ 0.8265],\n        [ 0.4066],\n        [ 0.8853],\n        [-7.4897],\n        [ 7.6964],\n        [ 8.3469],\n        [ 7.0632],\n        [ 8.5007],\n        [ 7.7676]])), ('linear.bias', tensor([ 0.1837,  0.0316, -0.2824,  0.9075, -9.3181, 10.6282,  9.0106,  9.7624,\n        10.2664,  9.0465]))])\n<\/code>\nAs you see only those parameters gor upgraded that were part of the mask.","z":"Can you elaborate a bit more? Are you referring to the regions of output as \u2018regions\u2019 in your questions .\nyes, the regions\/section of image in the models output. The loss is calculated only for a region of valid pixels in the target output.\nSomething like in this with link \"http:\/\/openaccess.thecvf.com\/content_cvpr_2017\/papers\/Kuznietsov_Semi-Supervised_Deep_Learning_CVPR_2017_paper.pdf\", they define the supervised loss for only valid depth values in the target. rest are masked out.\nIf I understand your problem. You only want to backpropagate through only those pixels that are True in the mask. In that case, you cannot use the approach, you defined in the question as that would backpropagate through all the pixels.\nUse this code instead\n<code class=\"lang-auto\">class myModel(nn.Module):\n    def __init__(self):\n        super().__init__()\n        self.linear = nn.Linear(1, 10)\n    def forward(self, x, trgt):\n        x = self.linear(x)\n        \n        # Mask code\n        mask = trgt > -1\n        x[mask==0] = 0\n        return x\nmodel = myModel()\n\n# Dummy inputs:- Assume mask is 10 units\nx = torch.rand(1, 1)\ntrgt = torch.arange(-4, 6).view(1, 10).to(dtype=torch.float)\n\n# Initial param valuies\nprint('Before update')\nprint(model.state_dict())\n\n# Run a epoch step\n# Large lr to visualize easily\noptimizer = optim.SGD(model.parameters(), lr=100)\noptimizer.zero_grad()\n\noutput = model(x, trgt)\n# This step is copied from your question as you take diff[mask]\ntrgt[trgt<=-1]=0\n\nloss = torch.mean(torch.abs(output - trgt))\nloss.backward()\noptimizer.step()\n\nprint('\\nAfter update')\nprint(model.state_dict())\n<\/code>\n<code class=\"lang-auto\">Before update\nOrderedDict([('linear.weight', tensor([[ 0.2001],\n        [ 0.8265],\n        [ 0.4066],\n        [ 0.8853],\n        [ 0.3886],\n        [-0.1819],\n        [ 0.4687],\n        [-0.8151],\n        [ 0.6224],\n        [-0.1107]])), ('linear.bias', tensor([ 0.1837,  0.0316, -0.2824,  0.9075,  0.6819,  0.6282, -0.9894, -0.2376,\n         0.2664, -0.9535]))])\n\nAfter update\nOrderedDict([('linear.weight', tensor([[ 0.2001],\n        [ 0.8265],\n        [ 0.4066],\n        [ 0.8853],\n        [-7.4897],\n        [ 7.6964],\n        [ 8.3469],\n        [ 7.0632],\n        [ 8.5007],\n        [ 7.7676]])), ('linear.bias', tensor([ 0.1837,  0.0316, -0.2824,  0.9075, -9.3181, 10.6282,  9.0106,  9.7624,\n        10.2664,  9.0465]))])\n<\/code>\nAs you see only those parameters gor upgraded that were part of the mask.\nhow do I scale this up to a larger model? (say resnet) Hardcoding in the mask for each step isn\u2019t an optimal approach.\nBy scale are you referring to the cost of computing the mask=trgt>-1. You can precompute them and give them as output of the dataloader.\nNope, by scale I meant a network with more than one layer. say in an auto-encoder.\nYou don\u2019t have to worry about that. The concept is similar to Dropout, you zero the activations in the top layers and as a result the activations are not carries in the previous layers.\nSay you have 2 layers. And you apply the mask in the last layer removing 4 neurons from the last layer. During the back prop their activations would be zero and as a result the neurons in layers 1 would be updated by the other (n-4) neurons.\nHope it helps."},{"x":"Hi, i ma follow cs230-code-examples with link \"https:\/\/github.com\/cs230-stanford\/cs230-code-examples\"\nand I have miss understanding the loss function\nhow dose this line works\n\" return -torch.sum(outputs[range(num_examples), labels])\/num_examples\"\nwhere outputs is result of the model with shape = batch_size  x num_classes , labels with shape = batch_size\nThanks in advance","y":"Let\u2019s first have a look at outputs[range(num_examples), labels].\nAs you said, outputs contains the model outputs and has the shape [batch_size, num_classes].\nlabels seems to be a torch.LongTensor, containing the ground truth class for each sample in the batch.\nIn this code we are basically indexing outputs, such that the result will contain the model output corresponding to the label.\nHere is a small example:\n<code class=\"lang-python\">batch_size = 5\nnb_classes = 3\noutputs = torch.randn(batch_size, nb_classes)\nlabels = torch.randint(0, nb_classes, (batch_size,))\nprint(outputs)\n> tensor([[ 0.6579, -2.1024, -0.4000],\n          [-0.3348, -0.4195, -1.5200],\n          [-0.3317,  0.6184,  1.7048],\n          [-0.1368, -1.1512, -0.6306],\n          [-0.3990, -1.2909, -0.8157]])\nprint(labels)\n> tensor([0, 0, 1, 2, 1])\nprint(outputs[torch.arange(batch_size), labels])\n> tensor([ 0.6579, -0.3348,  0.6184, -0.6306, -1.2909])\n\n# This would yield the same result\noutputs.gather(1, labels[:, None])\n<\/code>\nAs you can see, labels is used as an index to get the class logit of your output.\nIn the next step, these values are just summed and divided by the batch size, thus averaged, and multiplied by -1.","z":"Let\u2019s first have a look at outputs[range(num_examples), labels].\nAs you said, outputs contains the model outputs and has the shape [batch_size, num_classes].\nlabels seems to be a torch.LongTensor, containing the ground truth class for each sample in the batch.\nIn this code we are basically indexing outputs, such that the result will contain the model output corresponding to the label.\nHere is a small example:\n<code class=\"lang-python\">batch_size = 5\nnb_classes = 3\noutputs = torch.randn(batch_size, nb_classes)\nlabels = torch.randint(0, nb_classes, (batch_size,))\nprint(outputs)\n> tensor([[ 0.6579, -2.1024, -0.4000],\n          [-0.3348, -0.4195, -1.5200],\n          [-0.3317,  0.6184,  1.7048],\n          [-0.1368, -1.1512, -0.6306],\n          [-0.3990, -1.2909, -0.8157]])\nprint(labels)\n> tensor([0, 0, 1, 2, 1])\nprint(outputs[torch.arange(batch_size), labels])\n> tensor([ 0.6579, -0.3348,  0.6184, -0.6306, -1.2909])\n\n# This would yield the same result\noutputs.gather(1, labels[:, None])\n<\/code>\nAs you can see, labels is used as an index to get the class logit of your output.\nIn the next step, these values are just summed and divided by the batch size, thus averaged, and multiplied by -1."},{"x":"when I want to use nn.Sequential() in this way, it turns out a TypeError\n<code class=\"lang-auto\">class Discrim(nn.Module):\n    channels, maxpool_mask = [13, 64, 192, 384, 256, 256], [1, 1, 0, 0, 1]\n    ker_size, strd, pad = [2, 5, 3, 3, 3], [2, 1, 1, 1, 1], [0, 2, 1, 1, 1]\n    \n    def __init__(self, classes=13, conv_layers=5):\n        super(Discrim, self).__init__()\n        self.classes = classes\n        conv_features = []\n        for index in range(conv_layers):\n            conv_features.append(nn.Conv2d(Discrim.channels[index], Discrim.channels[index+1],\n                                           kernel_size=Discrim.ker_size[index],\n                                           stride=Discrim.strd[index],\n                                           padding=Discrim.pad[index],\n                                           bias=False))\n            conv_features.append(nn.BatchNorm2d(Discrim.channels[index+1]))\n            conv_features.append(nn.ReLU(inplace=True))\n            if Discrim.maxpool_mask[index] == 1:\n                conv_features.append(nn.MaxPool2d(3, stride=2, padding=1))\n            else:\n                conv_features.append(nn.ReLU(inplace=True))\n        self.features = nn.Sequential(conv_features[i] for layer_num in range(4*conv_layers))\n\n    def forward(self, x):\n        out = self.features(x)\n        return out\n\n\nnet = Discrim()\n<\/code>\nthen the output is:\n<code class=\"lang-auto\">File \"test.py\", line 184, in <module>\n    net = Discrim()\n  File \"test.py\", line 177, in __init__\n    self.features = nn.Sequential(conv_features[i] for layer_num in range(4*conv_layers))\n  File \"\/usr\/local\/lib\/python3.5\/dist-packages\/torch\/nn\/modules\/container.py\", line 52, in __init__\n    self.add_module(str(idx), module)\n  File \"\/usr\/local\/lib\/python3.5\/dist-packages\/torch\/nn\/modules\/module.py\", line 171, in add_module\n    torch.typename(module)))\nTypeError: Discrim.__init__.<locals>.<genexpr> is not a Module subclass\n<\/code>\ndo I make a mistake here?","y":"The following is working for me:\n<code class=\"lang-auto\">class Discrim(nn.Module):\n    channels, maxpool_mask = [13, 64, 192, 384, 256, 256], [1, 1, 0, 0, 1]\n    ker_size, strd, pad = [2, 5, 3, 3, 3], [2, 1, 1, 1, 1], [0, 2, 1, 1, 1]\n\n\n    def __init__(self, classes=13, conv_layers=5):\n        super(Discrim, self).__init__()\n        self.classes = classes\n        conv_features = []\n        for index in range(conv_layers):\n            conv_features.append(nn.Conv2d(Discrim.channels[index], Discrim.channels[index + 1],\n                                           kernel_size=Discrim.ker_size[index],\n                                           stride=Discrim.strd[index],\n                                           padding=Discrim.pad[index],\n                                           bias=False))\n            conv_features.append(nn.BatchNorm2d(Discrim.channels[index + 1]))\n            conv_features.append(nn.ReLU(inplace=True))\n            if Discrim.maxpool_mask[index] == 1:\n                conv_features.append(nn.MaxPool2d(3, stride=2, padding=1))\n            else:\n                conv_features.append(nn.ReLU(inplace=True))\n        self.features = nn.Sequential(*conv_features)\n    \n    \n    def forward(self, x):\n        out = self.features(x)\n        return out\n\n\nnet = Discrim()\n<\/code>\nI only changed the line self.features = nn.Sequential(*conv_features). The Problem here is, that you index with i but i is not defined. Since you iterate over all layers, I just unpacked the list, which should basically do the same.","z":"The following is working for me:\n<code class=\"lang-auto\">class Discrim(nn.Module):\n    channels, maxpool_mask = [13, 64, 192, 384, 256, 256], [1, 1, 0, 0, 1]\n    ker_size, strd, pad = [2, 5, 3, 3, 3], [2, 1, 1, 1, 1], [0, 2, 1, 1, 1]\n\n\n    def __init__(self, classes=13, conv_layers=5):\n        super(Discrim, self).__init__()\n        self.classes = classes\n        conv_features = []\n        for index in range(conv_layers):\n            conv_features.append(nn.Conv2d(Discrim.channels[index], Discrim.channels[index + 1],\n                                           kernel_size=Discrim.ker_size[index],\n                                           stride=Discrim.strd[index],\n                                           padding=Discrim.pad[index],\n                                           bias=False))\n            conv_features.append(nn.BatchNorm2d(Discrim.channels[index + 1]))\n            conv_features.append(nn.ReLU(inplace=True))\n            if Discrim.maxpool_mask[index] == 1:\n                conv_features.append(nn.MaxPool2d(3, stride=2, padding=1))\n            else:\n                conv_features.append(nn.ReLU(inplace=True))\n        self.features = nn.Sequential(*conv_features)\n    \n    \n    def forward(self, x):\n        out = self.features(x)\n        return out\n\n\nnet = Discrim()\n<\/code>\nI only changed the line self.features = nn.Sequential(*conv_features). The Problem here is, that you index with i but i is not defined. Since you iterate over all layers, I just unpacked the list, which should basically do the same.\nThank you so much, how stupid I am!"},{"x":"All of my images are put in the same folder, and I want to load them.\n<code class=\"lang-auto\">data\n    img\n        0.png\n        1.png\n        2.png\n        3.png\n        ...\n<\/code>","y":"In case you want them to be a part of dataloader, then you can use ImageFolder.\nBut if you want to load them to memory, the most efficient way would be to\n<code class=\"lang-auto\">import os\nimport matplotlib.pyplot as plt\npath = 'data\/img\/'\nimg_names = os.listdir(path)\n\nimages = np.empty(num_images, width, height, channels)\nfor idx, name in enumerate(img_names):\n    img_name = path + name\n    # Use you favourite library to load the image\n    image = plt.imread(img_name)\n    images[idx] = image\n<\/code>","z":"In case you want them to be a part of dataloader, then you can use ImageFolder.\nBut if you want to load them to memory, the most efficient way would be to\n<code class=\"lang-auto\">import os\nimport matplotlib.pyplot as plt\npath = 'data\/img\/'\nimg_names = os.listdir(path)\n\nimages = np.empty(num_images, width, height, channels)\nfor idx, name in enumerate(img_names):\n    img_name = path + name\n    # Use you favourite library to load the image\n    image = plt.imread(img_name)\n    images[idx] = image\n<\/code>"},{"x":"Hi,\nI noticed when I run the following piece of code the model outputs at each time slightly different, what is going on?\n<code class=\"lang-auto\">import random\nimport os\nimport numpy as np\nfrom PIL import Image\nimport torch\nimport torchvision\nimport torchvision.transforms as transforms\nfrom torch.utils.data.sampler import SubsetRandomSampler\nfrom torch.autograd import Variable\nimport utils\nimport model.resnet as teacher\nfrom torchsummary import summary\n\nnormalize = transforms.Normalize(mean=[0.4554, 0.4384, 0.4112],\n                             std=[0.2373, 0.2321, 0.2346])\n\t\t\t\t\t\t\t \ntrain_transformer = transforms.Compose([\n    transforms.RandomResizedCrop(224),\n    transforms.RandomHorizontalFlip(),  # randomly flip image horizontally\n    transforms.ToTensor(),\n    normalize])\n\ntrainset = torchvision.datasets.ImageFolder('\/cluster1\/train\/', transform=train_transformer)\ntrainloader = torch.utils.data.DataLoader(trainset, batch_size=8,\nshuffle=False, num_workers=4, pin_memory=1)\n\n\nbase_model = '\/base_resnet152\/best.pth.tar'\n\nother_model = teacher.ResNet(teacher.Bottleneck, [3,8,36,3], num_classes=1000).cuda()\nother_model.eval()\t\t\t\n#for loading parameter from other cluster\nutils.load_checkpoint(base_model, other_model)\n\n\npredicted = np.array([])\nlegit = np.array([])\n\n\nteacher_outputs = []\nfor i, (data_batch, labels_batch) in enumerate(trainloader):\n    data_batch, labels_batch = Variable(data_batch.cuda(async=True)),Variable(labels_batch.cuda(async=True))\n\n    output_teacher_batch = other_model(data_batch).data.cpu().numpy()\n    teacher_outputs.append(output_teacher_batch)\n    legit = np.append(legit, np.argmax(output_teacher_batch, axis=1))\n\t\n\nnp.savetxt('\/base_resnet152\/legit.txt', legit)\n\n<\/code>\n<code class=\"lang-auto\">this is part of legit file at first run:\n3.390000000000000000e+02\n3.390000000000000000e+02\n2.740000000000000000e+02\n3.390000000000000000e+02\n3.390000000000000000e+02\n3.390000000000000000e+02\n3.390000000000000000e+02\n3.390000000000000000e+02\n3.390000000000000000e+02\n3.390000000000000000e+02\n3.390000000000000000e+02\n3.390000000000000000e+02\n3.390000000000000000e+02\n3.390000000000000000e+02\n3.390000000000000000e+02\n3.390000000000000000e+02\n3.390000000000000000e+02\n3.390000000000000000e+02\n3.390000000000000000e+02\n3.390000000000000000e+02\n3.390000000000000000e+02\n3.450000000000000000e+02\n6.450000000000000000e+02\n...\n<\/code>\nand this is for second run:\n<code class=\"lang-auto\">3.390000000000000000e+02\n3.390000000000000000e+02\n**3.460000000000000000e+02**\n3.390000000000000000e+02\n3.390000000000000000e+02\n3.390000000000000000e+02\n3.390000000000000000e+02\n3.390000000000000000e+02\n**9.120000000000000000e+02**\n3.390000000000000000e+02\n3.390000000000000000e+02\n3.390000000000000000e+02\n3.390000000000000000e+02\n3.390000000000000000e+02\n**2.110000000000000000e+02**\n3.390000000000000000e+02\n3.390000000000000000e+02\n3.390000000000000000e+02\n3.390000000000000000e+02\n3.390000000000000000e+02\n3.390000000000000000e+02\n3.450000000000000000e+02\n6.450000000000000000e+02\n...\n<\/code>\nas you see some values are different, like those I bolded. I thought maybe this happens because the calculations are in floating point mode, but I\u2019m not sure whether its the case or not. I don\u2019t have any source of randomness in the code so what could be wrong.","y":"apparently the dataloader is not deterministic and in this post shows hows to make it deterministic:\n\n\n\n\nDataLoader is not deterministic with link \"https:\/\/discuss.pytorch.org\/t\/dataloader-is-not-deterministic\/19250\" vision with link \"\/c\/vision\"\n\n\n    Hi, \nI am using DataLoader and even with single worker getting different generation of image order.  How can I make it generate the same image order every time ?  (and does it depend on #workers?) \nCode is initialized with: \nrandom.seed(1)\ntorch.manual_seed(1)\ntorch.backends.cudnn.deterministic = True\n\nIn addition, I would like to be able to serialize DataLoader\u2019s internal state to file so that if I stop the run at the middle of an epoch and resume it later then I can continue from the same plac\u2026\n  \n\n","z":"apparently the dataloader is not deterministic and in this post shows hows to make it deterministic:\n\n\n\n\nDataLoader is not deterministic with link \"https:\/\/discuss.pytorch.org\/t\/dataloader-is-not-deterministic\/19250\" vision with link \"\/c\/vision\"\n\n\n    Hi, \nI am using DataLoader and even with single worker getting different generation of image order.  How can I make it generate the same image order every time ?  (and does it depend on #workers?) \nCode is initialized with: \nrandom.seed(1)\ntorch.manual_seed(1)\ntorch.backends.cudnn.deterministic = True\n\nIn addition, I would like to be able to serialize DataLoader\u2019s internal state to file so that if I stop the run at the middle of an epoch and resume it later then I can continue from the same plac\u2026\n  \n\n"},{"x":"Hi,\nI was trying to fine-tune resent 152 and I was using this piece of code:\n<code class=\"lang-auto\">> for param in list(model.parameters())[:-1]:\n>     param.requires_grad = False\n<\/code>\nbut I notice this was not working I changed it to :\n<code class=\"lang-auto\">> child_counter = 0\n>  for child in model.children():\n>        if child_counter < 9:\n>            for param in child.parameters():\n>                 param.requires_grad = False\n>       child_counter += 1\n<\/code>\nit seems the second piece of code does the job, but I\u2019m confiused, aren\u2019t these two pieces of code doing the same thing ?","y":"In the first code snippet you are setting all requires_grad flags to False except the model.fc.bias, while the second code snippet additionally keeps model.fc.weight.requires_grad as True.","z":"In the first code snippet you are setting all requires_grad flags to False except the model.fc.bias, while the second code snippet additionally keeps model.fc.weight.requires_grad as True."},{"x":"Hello ,everyone, I was trying to do some segenmentation jobs with Unet. I have only 10 Images , and I use 8 of them as train Images, 2 of them as test. The images are greyscale images, and the mask(groundtruth) are binarized images(black stand for backgrounds, white for objects). I am using almost the most basic Unet. The test result is not so good but works, most of the pixels can be correctly predicted. So I decide to rotate the original Images and perform random crops on the original Images to enlarge the training set so that the test result can be improved. Unfortunatly, the network does not work after those operations. With random crops, the trained network cant even predict a single part(the predicted mask is all black). And with vertical or horizontal flip, the predicted result seems to be wrong, it is a combination of original mask and flipped mask. I have no idea what\u2019s wrong with my implementation. Is Unet not working with those augmentations or something else?\nHope someone can give me some ideas.\nThanks","y":"You can find a small example of using the same \u201crandom\u201d transformations here with link \"https:\/\/discuss.pytorch.org\/t\/torchvision-transfors-how-to-perform-identical-transform-on-both-image-and-target\/10606\/7?u=ptrblck\", which would be important as  described.","z":"Firstly check if you have performed the exact same augmentation on the masks also. Otherwise, it is clear to me that data augmentation cannot improve generalization cause your model never generalised in the first place. With only 8 train images, it over fitted the data and because the validation was so similar it performed alight. 10 images is far too less for any good outcome.\nYou can find a small example of using the same \u201crandom\u201d transformations here with link \"https:\/\/discuss.pytorch.org\/t\/torchvision-transfors-how-to-perform-identical-transform-on-both-image-and-target\/10606\/7?u=ptrblck\", which would be important as  described.\nI have doubt if I call the transform function twice, the operation will change or not.This is what I have:\n<code class=\"lang-auto\">class Data_set():\n    def __init__(self,transform):\n        imgs,masks = make_dataset(imgpath,maskpath)\n        self.transform = transform\n        self.imgs = imgs\n        self.masks = masks\n    def __len__(self):\n        return len(self.imgs)\n    def __getitem__(self,index):\n        imgpath,maskpath = self.imgs[index],self.masks[index]\n        img_x = Image.open(imgpath)\n        mask_x = Image.open(maskpath)\n        img_x = self.transform(img_x)#.float()\n        mask_x = self.transform(mask_x)#.long()\n        return img_x,mask_x\n<\/code>\nI have earlier tried this way:\n<code class=\"lang-auto\">x, y = self.transform(image, mask)\n<\/code>\nbut I dont know why that comes an error, so I call that twice.\nIf self.transform applies random transformations, both calls will use different random transformations, which will mess up the data to target correspondence.\nIn my example, I\u2019ve used the functional API to only draw the random parameters once and apply them on the image as well as on the target.\nThis solve the problem on vertical\/horizontal flip. Thanks a lot.\nBut with crop, the same problem happens again. I am not sure if it\u2019s caused by lacking of training or something else.\nI assume you\u2019ve used the example code to crop your data.\nIf so, maybe try to relax the cropping a bit, i.e. just sample small numbers and try to see, if the cropping operation might be causing trouble training your model.\nI just tested, the problem is not caused by the cropping. Thanks for help."},{"x":"I want to call a function defined in my dataset class at the end of every epoch of training. I\u2019m not sure if it\u2019s the right thing to do and I wanted some feedback. The current structure looks something like below:\n<code class=\"lang-auto\">class my_dataset(nn.data.utils.Dataset):\n    __init__(self, ...)\n    __len__(self)\n    __getitem__(self, idx)\n    my_func(self)\ndata = my_dataset()\ndata_loader = torch.utils.data.DataLoader(data)\ndef training():\n    for i, sample in enumerate(train_loader):\n        do something\n    data.my_func()\n<\/code>\nI know that I can call data.my_func() but will it be reflected in the data_loader ? I guess I\u2019m trying to copy the on_epoch_end function provided in the Keras Dataset structure. Is this the right way to do it ?\nThanks !","y":"What would you like to achieve?\nYou could also call train_loader.dataset.my_func(). Both ways should work though.\nIgnite with link \"https:\/\/github.com\/pytorch\/ignite\" provides some event handling like .on(Events.EPOCH_COMPLETED). Maybe the code would be cleaner, if you need a lot on these events. ","z":"What would you like to achieve?\nYou could also call train_loader.dataset.my_func(). Both ways should work though.\nIgnite with link \"https:\/\/github.com\/pytorch\/ignite\" provides some event handling like .on(Events.EPOCH_COMPLETED). Maybe the code would be cleaner, if you need a lot on these events. \nThank you for the reply. At a high level, I\u2019m just looking at shuffling data. I know that there is randomization in the order of samples selected (with shuffle=True in the data loader) via the sampling procedure but due to how I implement __getitem__, there is a likely correlation between the mini-batch of images loaded. A random shuffling of my image set after every epoch would break this correlation.\nI\u2019ll look into Ignite. I did come across Torchsample with link \"https:\/\/github.com\/ncullen93\/torchsample\" but it wasn\u2019t obvious how to do what I wanted in it the way it seems to be in Ignite. But I think the native pytorch solution of just calling it after the iterator of train_loader completes is what I\u2019ll go with, for now.\nThanks again for your help !\nMay I ask, how you are shuffling the data in the Dataset?\nSince the __getitem__ function receives an index, I assume you are shuffling by copying your data?\nMaybe you could implement your own Sampler with link \"http:\/\/pytorch.org\/docs\/master\/data.html#torch.utils.data.sampler.Sampler\", which could be faster?\nI have the following two variables in my dataset which is a list of image names per object category: self.list_image_names and self.num_images. The __getitem__ does something like this\nimage_names = [self.image_names[i][idx % self.num_images[i]] for i in range(len(self.list_image_names))]. The order in which my model sees the images changes due to the default sampler but the images inside a particular batch are the same every time (for a given idx). I plan to shuffle data at the end of every epoch using self.image_names[i] = np.random.permutation(self.image_names[i]) to break this correlation.\nImplementing my own Sampler might be faster or the correct thing to do, but I think this shuffling will do what I want.\nYeah, I would do the same, since you are just shuffling names\/paths it\u2019s not a big deal.\nI had my concerns with shuffling (large) data, but that seems just be fine. "},{"x":"I am experiencing some inconsistent predictions when running the same file multiple times. I followed the https:\/\/pytorch.org\/docs\/stable\/notes\/randomness.html guide to set all the seeds and enable\/disable the respective cudnn flags.\nTraining a small 3D network with some pooling layers on the CPU runs smoothly and the results between multiple runs are consistent. When training on the GPU the network predictions change after a few iterations even though the exact same network (and initialization) is used. Excluding the pooling layers fixes the problem. Furthermore, I can not observe such inconsistencies when using an equivalent 2D model (with pooling layers).\nBelow is a generic 3D examples which replicates my observations on my system.\nNote: I\u2019m not referring to differences between the CPU and the GPU run, I\u2019m referring to multiple runs on the CPU\/GPU respectively.\n\n\ngist.github.com with link \"https:\/\/gist.github.com\/mibaumgartner\/59f0be2e0a667bb9c1bec1106e49989e\"\n\n\nhttps:\/\/gist.github.com\/mibaumgartner\/59f0be2e0a667bb9c1bec1106e49989e\nExample\n<code class=\"\">set_seed = 0\nDEVICE = 'cuda'\n\nimport torch\ntorch.manual_seed(set_seed)\ntorch.backends.cudnn.deterministic = True\ntorch.backends.cudnn.benchmark = False\nimport torch.nn as nn\n\nimport random<\/code>\nThis file has been truncated. show original with link \"https:\/\/gist.github.com\/mibaumgartner\/59f0be2e0a667bb9c1bec1106e49989e\"\n\n\n\n\n\n\n\nTested on:\nUbuntu 18.04\nTried with: Torch 1.0post2 with cuda 10\nTorch 0.4.1 with cuda 9\nNvidia GTX 980\nEdit: updated description and replaced 3d resnet examples with much simpler\/smaller network.","y":"Thanks for the code snippet!\nI debugged it a bit and think you are seeing some non-determinism due to some atomic operations.\nFrom the reproducibility docs:\n\nThere are some PyTorch functions that use CUDA functions that can be a source of non-determinism. One class of such CUDA functions are atomic operations, in particular atomicAdd , where the order of parallel additions to the same value is undetermined and, for floating-point variables, a source of variance in the result. [\u2026]\nA number of operations have backwards that use atomicAdd , in particular torch.nn.functional.embedding_bag() with link \"https:\/\/pytorch.org\/docs\/stable\/nn.html#torch.nn.functional.embedding_bag\", torch.nn.functional.ctc_loss() with link \"https:\/\/pytorch.org\/docs\/stable\/nn.html#torch.nn.functional.ctc_loss\" and many forms of pooling, padding, and sampling. There currently is no simple way of avoiding non-determinism in these functions.\n\nI\u2019ve compared two runs for a single epoch and since the differences are increasing I assume this might be due to the atomicAdd in the backward function of your pooling layers.\nHere are the differences for the predictions and losses:\n<code class=\"lang-python\">print((preds1 - preds2).abs())\n> tensor([0.0000e+00, 0.0000e+00, 1.1921e-07, 2.6822e-07, 2.9802e-08, 1.6391e-07,\n        1.3411e-07, 1.4901e-07, 2.9802e-07, 4.7684e-07, 8.3447e-07, 2.3842e-07,\n        1.3113e-06, 5.9605e-07, 2.3842e-07, 4.7684e-07, 4.1723e-06, 6.8545e-06,\n        9.6858e-06, 1.2815e-05, 4.9993e-06, 6.1840e-06, 5.8860e-06, 6.7577e-06,\n        6.6683e-06, 5.5283e-06, 5.8264e-06, 6.8545e-06, 7.7151e-06, 8.6427e-06,\n        7.5623e-06, 6.1095e-06, 5.7220e-06, 1.9968e-06, 2.0415e-06, 9.5367e-07,\n        1.5795e-06, 2.2650e-06, 1.2517e-06, 4.8578e-06, 5.9009e-06, 1.0103e-05,\n        1.5825e-05, 2.7329e-05, 2.5600e-05, 3.5465e-05, 2.5690e-05, 1.8418e-05,\n        5.1737e-05, 9.8228e-05])\nprint((losses1 - losses2).abs())\n> tensor([0.0000e+00, 0.0000e+00, 0.0000e+00, 5.9605e-08, 0.0000e+00, 5.9605e-08,\n        5.9605e-08, 5.9605e-08, 1.4901e-07, 8.9407e-08, 1.4901e-07, 1.1921e-07,\n        1.0729e-06, 1.0431e-07, 2.3842e-07, 2.3842e-07, 2.9802e-06, 4.5300e-06,\n        5.9009e-06, 5.7220e-06, 2.6226e-06, 3.0398e-06, 3.0398e-06, 3.3379e-06,\n        3.2783e-06, 2.9802e-06, 3.0994e-06, 3.6359e-06, 3.9935e-06, 4.3511e-06,\n        3.6359e-06, 3.2187e-06, 2.6822e-06, 1.1325e-06, 8.3447e-07, 3.5763e-07,\n        8.9407e-07, 8.9407e-07, 5.9605e-07, 2.8014e-06, 3.6955e-06, 3.9935e-06,\n        9.5367e-06, 1.1146e-05, 1.0610e-05, 1.4484e-05, 1.0252e-05, 7.1228e-06,\n        3.2902e-05, 3.5226e-05])\n<\/code>","z":"Thanks for the code snippet!\nI debugged it a bit and think you are seeing some non-determinism due to some atomic operations.\nFrom the reproducibility docs:\n\nThere are some PyTorch functions that use CUDA functions that can be a source of non-determinism. One class of such CUDA functions are atomic operations, in particular atomicAdd , where the order of parallel additions to the same value is undetermined and, for floating-point variables, a source of variance in the result. [\u2026]\nA number of operations have backwards that use atomicAdd , in particular torch.nn.functional.embedding_bag() with link \"https:\/\/pytorch.org\/docs\/stable\/nn.html#torch.nn.functional.embedding_bag\", torch.nn.functional.ctc_loss() with link \"https:\/\/pytorch.org\/docs\/stable\/nn.html#torch.nn.functional.ctc_loss\" and many forms of pooling, padding, and sampling. There currently is no simple way of avoiding non-determinism in these functions.\n\nI\u2019ve compared two runs for a single epoch and since the differences are increasing I assume this might be due to the atomicAdd in the backward function of your pooling layers.\nHere are the differences for the predictions and losses:\n<code class=\"lang-python\">print((preds1 - preds2).abs())\n> tensor([0.0000e+00, 0.0000e+00, 1.1921e-07, 2.6822e-07, 2.9802e-08, 1.6391e-07,\n        1.3411e-07, 1.4901e-07, 2.9802e-07, 4.7684e-07, 8.3447e-07, 2.3842e-07,\n        1.3113e-06, 5.9605e-07, 2.3842e-07, 4.7684e-07, 4.1723e-06, 6.8545e-06,\n        9.6858e-06, 1.2815e-05, 4.9993e-06, 6.1840e-06, 5.8860e-06, 6.7577e-06,\n        6.6683e-06, 5.5283e-06, 5.8264e-06, 6.8545e-06, 7.7151e-06, 8.6427e-06,\n        7.5623e-06, 6.1095e-06, 5.7220e-06, 1.9968e-06, 2.0415e-06, 9.5367e-07,\n        1.5795e-06, 2.2650e-06, 1.2517e-06, 4.8578e-06, 5.9009e-06, 1.0103e-05,\n        1.5825e-05, 2.7329e-05, 2.5600e-05, 3.5465e-05, 2.5690e-05, 1.8418e-05,\n        5.1737e-05, 9.8228e-05])\nprint((losses1 - losses2).abs())\n> tensor([0.0000e+00, 0.0000e+00, 0.0000e+00, 5.9605e-08, 0.0000e+00, 5.9605e-08,\n        5.9605e-08, 5.9605e-08, 1.4901e-07, 8.9407e-08, 1.4901e-07, 1.1921e-07,\n        1.0729e-06, 1.0431e-07, 2.3842e-07, 2.3842e-07, 2.9802e-06, 4.5300e-06,\n        5.9009e-06, 5.7220e-06, 2.6226e-06, 3.0398e-06, 3.0398e-06, 3.3379e-06,\n        3.2783e-06, 2.9802e-06, 3.0994e-06, 3.6359e-06, 3.9935e-06, 4.3511e-06,\n        3.6359e-06, 3.2187e-06, 2.6822e-06, 1.1325e-06, 8.3447e-07, 3.5763e-07,\n        8.9407e-07, 8.9407e-07, 5.9605e-07, 2.8014e-06, 3.6955e-06, 3.9935e-06,\n        9.5367e-06, 1.1146e-05, 1.0610e-05, 1.4484e-05, 1.0252e-05, 7.1228e-06,\n        3.2902e-05, 3.5226e-05])\n<\/code>\nThank you for your quick response!\nI guess you are right, just wanted to double check. I was a bit surprised by the fact, that this only happens when 3d pooling is used and not with 2d pooling (or an equivalent 2d version of the network).\nNot, sure why this happens in the 3d case but not in 2d, but atomicAdd will be used, if the kernel_size is not equal to the stride as described in these lines of code with link \"https:\/\/github.com\/pytorch\/pytorch\/blob\/80927fc068548bd3fd4621c79b9475439d751f3e\/aten\/src\/THCUNN\/SpatialAdaptiveMaxPooling.cu#L130\".\nIf you change your nn.MaxPool3d layers to nn.MaxPool3d(kernel_size=2, stride=2, padding=0), you should get deterministic results.\nThank you, good to know "},{"x":"Hi, I\u2019m confused and lost in convNN parameters\nsome example code that i forked before:\n<code class=\"lang-auto\">nn.Conv2d(3, 6, 3)\npool = nn.MaxPool2d(2, 2) \nconv2 = nn.Conv2d(6, 16, 3)\n<\/code>\nWhat determines the change of the numbers ?\nFirst 3 is our channels and 6 our filters and the last 3 is 3x3 filter size as i remembered but i couldn\u2019t understand the math behind that how to determine the (6,16,3) for the second conv layer after maxPooling layer.\nthanks for your help.","y":"In conv2\n\n6 -> refers to the number of in_channels, after first conv your output had 6 out_channels, so they become input in conv2\n16 -> refers to the number of out_channels. It can be any number. It is your choice. You can set it to 100, 10 anything.\n3 -> kernel_size again is your choice. But kernel_size value 2 and 3 are common, so don\u2019t need to experiment much\n","z":"In conv2\n\n6 -> refers to the number of in_channels, after first conv your output had 6 out_channels, so they become input in conv2\n16 -> refers to the number of out_channels. It can be any number. It is your choice. You can set it to 100, 10 anything.\n3 -> kernel_size again is your choice. But kernel_size value 2 and 3 are common, so don\u2019t need to experiment much\n"},{"x":"Hi,\nthe following picture is a snippet of resnet 18 structure. I got confused about the dimensions. I thought the input size of a layer should be the same as the output size of the previous layer. I wonder those highlighted numbers, shouldn\u2019t have the same value?\ndownsample.PNG754\u00d7390 11.5 KB with link \"https:\/\/discuss.pytorch.org\/uploads\/default\/original\/2X\/4\/467ce3cb29619e98bc00b1bc41b5142dda78d33f.png\"","y":"you should take a look at the logic of the forward function, the structure of layers do not represent the flow of tensors.","z":"you should take a look at the logic of the forward function, the structure of layers do not represent the flow of tensors.\nthis is the forward path:\n\ndef forward(self, x):\n    x = self.conv1(x)\n    x = self.bn1(x)\n    x = self.relu(x)\n    x = self.maxpool(x)\n\n    x = self.layer1(x)\n    x = self.layer2(x)\n    x = self.layer3(x)\n    x = self.layer4(x)\n\n    x = self.avgpool(x)\n    x = x.view(x.size(0), -1)\n    x = self.fc(x)\n\n    return x\n\n\nit seems the snippet(layer4) will run as it is.\nimage.png1538\u00d7694 63.7 KB with link \"https:\/\/discuss.pytorch.org\/uploads\/default\/original\/2X\/d\/d36330ffab01281405644187fb624451d56e0852.png\"\nThis is mine resnet18 output, did you use the resnet18 from torchvision or some other implementation?\nI took it from here:\n\n\ngithub.com with link \"https:\/\/github.com\/pytorch\/vision\/blob\/master\/torchvision\/models\/resnet.py\"\n\n\npytorch\/vision\/blob\/master\/torchvision\/models\/resnet.py with link \"https:\/\/github.com\/pytorch\/vision\/blob\/master\/torchvision\/models\/resnet.py\"\n<code class=\"lang-py\">import torch.nn as nn\nimport torch.utils.model_zoo as model_zoo\n\n\n__all__ = ['ResNet', 'resnet18', 'resnet34', 'resnet50', 'resnet101',\n           'resnet152']\n\n\nmodel_urls = {\n    'resnet18': 'https:\/\/download.pytorch.org\/models\/resnet18-5c106cde.pth',\n    'resnet34': 'https:\/\/download.pytorch.org\/models\/resnet34-333f7ec4.pth',\n    'resnet50': 'https:\/\/download.pytorch.org\/models\/resnet50-19c8e357.pth',\n    'resnet101': 'https:\/\/download.pytorch.org\/models\/resnet101-5d3b4d8f.pth',\n    'resnet152': 'https:\/\/download.pytorch.org\/models\/resnet152-b121ed2d.pth',\n}\n\n\ndef conv3x3(in_planes, out_planes, stride=1):\n    \"\"\"3x3 convolution with padding\"\"\"\n    return nn.Conv2d(in_planes, out_planes, kernel_size=3, stride=stride,\n<\/code>\n\n  This file has been truncated. show original with link \"https:\/\/github.com\/pytorch\/vision\/blob\/master\/torchvision\/models\/resnet.py\"\n\n\n\n\n\n\napparently, I mistakenly initialize resnet with Bottleneck instead of BasicBlock. However, the problem is still there:\ndownsample.PNG757\u00d7368 14.8 KB with link \"https:\/\/discuss.pytorch.org\/uploads\/default\/original\/2X\/a\/affe4923e1069f409ac4c37247b6688c35676806.png\"\nAs  said, the  forward logic might differ from the order of the stored modules.\nIf you look at this line of code with link \"https:\/\/github.com\/pytorch\/vision\/blob\/6cc4970b3a01d3357af038c3d4b6a81f9fb74357\/torchvision\/models\/resnet.py#L52\", you\u2019ll see, that self.downsample is appiled on x, which differs from the output of self.bn2."},{"x":"Hi,\nI\u2019m trying to change the used size of channels at layer4 of resnet 18, I wrote something like the follwoing piece of code. but when I run it complains about the size mismatch at the fc layer, I don\u2019t know exactly how should I realize which size should I use.\n\nclass Inception(nn.Module):\ndef __init__(self, in_channels=2048):\n    super(Inception, self).__init__()\n\n    self.paral_0 = nn.Sequential(\n        nn.Conv2d(256, 64, kernel_size=3, stride=2, padding=1, bias=False),\n        nn.BatchNorm2d(64),\n        nn.ReLU(inplace=True),\t\t\t\n        nn.Conv2d(64, 64, kernel_size=3, stride=1,  padding=1, bias=False),\n        nn.BatchNorm2d(64),\n  \t\n        nn.Conv2d(64, 64, kernel_size=3, stride=1,  padding=1, bias=False),\n        nn.BatchNorm2d(64),\n        nn.ReLU(inplace=True),\t\t\t\n        nn.Conv2d(64, 64, kernel_size=3, stride=1,  padding=1,  bias=False),\n        nn.BatchNorm2d(64),\n  \t\n        nn.AdaptiveAvgPool2d(output_size=(1, 1))\n    )\n    self.fc0    = nn.Linear(in_features=256, out_features=83, bias=True)\n    ....\n\n\n\ndef forward(self, x):\n    y0 = self.paral_0(x)\n    y0 = y0.view(y0.size(0), -1)\n    y0 = self.fc0(y0)\n....\n\n\nand this is the error:\n\nTraceback (most recent call last):\nFile \" generated_train\/0.py\", line 471, in \nsummary(model, (3, 224, 224))\nFile \" lib\/python3.6\/site-packages\/torchsummary\/torchsummary.py\", line 72, in summary\nmodel(*x)\nFile \" lib\/python3.6\/site-packages\/torch\/nn\/modules\/module.py\", line 489, in call\nresult = self.forward(*input, **kwargs)\nFile \" generated_train\/model\/net.py\", line 533, in forward\nx = self.layer4(x)\nFile \" lib\/python3.6\/site-packages\/torch\/nn\/modules\/module.py\", line 489, in call\nresult = self.forward(*input, **kwargs)\nFile \" generated_train\/model\/net.py\", line 303, in forward\ny0 = self.fc0(y0)\nFile \" lib\/python3.6\/site-packages\/torch\/nn\/modules\/module.py\", line 489, in call\nresult = self.forward(*input, **kwargs)\nFile \" lib\/python3.6\/site-packages\/torch\/nn\/modules\/linear.py\", line 67, in forward\nreturn F.linear(input, self.weight, self.bias)\nFile \" lib\/python3.6\/site-packages\/torch\/nn\/functional.py\", line 1352, in linear\nret = torch.addmm(torch.jit._unwrap_optional(bias), input, weight.t())\nRuntimeError: size mismatch, m1: [2 x 64], m2: [256 x 83] at \/pytorch\/aten\/src\/THC\/generic\/THCTensorMathBlas.cu:266\n","y":"Static frameworks can infer the input size based on the computation graph.\nI.e. if we know which activation will be passed to the linear layer in advance, we can set the number of input features based on the activation shape.\nHowever, as the forward pass is not known in advance in PyTorch and the computation graph is dynamically created in each forward pass, we cannot pre-compute the input shape without losing the flexibility of a dynamic approach.\nAnyway, a linear layer is basically a matrix multiplication and an addition. The matrix size has to be set before the actual forward pass takes place.","z":"If calculating the shapes is too cumbersome, you could just print the shape of y0 before passing it to the linear layer:\n<code class=\"lang-python\">print(y0.shape)\ny0 = self.fc0(y0)\n<\/code>\nAfter the next crash, you\u2019ll get the shape of your tensor and can adapt the number of input features in your layer accordingly.\nwell, it prints the size as torch.Size([2, 64]), what should I change the input feature into? I tried different combinations and all give a similar error. should I use reshape?\nin_features should be set to 64 to match the input.\nyes apparently 64 was its magic number, thanks\nI got a little confused, why the input features of Linear layer depends on previous layers ? isn\u2019t it the case that each neuron at FC layer will connect to all neurons at previous layers, so why it would matter what is the size of FC layer.\nCould you run the code on the CPU and see if you get a proper error message?\nThat\u2019s basically right. Since PyTorch is a dynamic framework, you have to specify the in and out features of the linear layer, so that the weight matrix can be properly initialized.\nI understand this part that we should specify the input feature size, but I don\u2019t understand why it should be specific, like in my case why it should be 64 ? isn\u2019t it against the definition of FC layers as I mentioned earlier ? did I miss something here\nStatic frameworks can infer the input size based on the computation graph.\nI.e. if we know which activation will be passed to the linear layer in advance, we can set the number of input features based on the activation shape.\nHowever, as the forward pass is not known in advance in PyTorch and the computation graph is dynamically created in each forward pass, we cannot pre-compute the input shape without losing the flexibility of a dynamic approach.\nAnyway, a linear layer is basically a matrix multiplication and an addition. The matrix size has to be set before the actual forward pass takes place."},{"x":"hello everybody,\nI am new to pytorch and had a problem with channels in AlexNet.\nI am using it for a \u2018gta san andreas self driving car\u2019 project, I collected the dataset from a black and white image that has one channel and trying to train AlexNet using the script:\n<code class=\"lang-auto\">\nfrom AlexNetPytorch import*\nimport torchvision\nimport torchvision.transforms as transforms\nimport torch.optim as optim\nimport torch.utils.data\nimport numpy as np\nimport torch\nfrom IPython.core.debugger import set_trace\n\nAlexNet = AlexNet()\n\ncriterion = nn.CrossEntropyLoss()\noptimizer = optim.SGD(AlexNet.parameters(), lr=0.001, momentum=0.9)\n\nall_data = np.load('training_data.npy')\ninputs= all_data[:,0]\nlabels= all_data[:,1]\ninputs_tensors = torch.stack([torch.Tensor(i) for i in inputs])\nlabels_tensors = torch.stack([torch.Tensor(i) for i in labels])\n\ndata_set = torch.utils.data.TensorDataset(inputs_tensors,labels_tensors)\ndata_loader = torch.utils.data.DataLoader(data_set, batch_size=3,shuffle=True, num_workers=2)\n\n\n\n\nif __name__ == '__main__':\n for epoch in range(8):\n  runing_loss = 0.0\n  for i,data in enumerate(data_loader , 0):\n     inputs= data[0]\n     inputs = torch.FloatTensor(inputs)\n     labels= data[1]\n     labels = torch.FloatTensor(labels)\n     optimizer.zero_grad()\n     # set_trace()\n     inputs = torch.unsqueeze(inputs, 1)\n     outputs = AlexNet(inputs)\n     loss = criterion(outputs , labels)\n     loss.backward()\n     optimizer.step()\n     \n     runing_loss +=loss.item()\n     if i % 2000 == 1999:    # print every 2000 mini-batches\n            print('[%d, %5d] loss: %.3f' %\n                  (epoch + 1, i + 1, running_loss \/ 2000))\n            running_loss = 0.0\n print('finished')\n<\/code>\nI am using AlexNet from the link\n\n\ngithub.com with link \"https:\/\/github.com\/pytorch\/vision\/blob\/master\/torchvision\/models\/alexnet.py\"\n\n\npytorch\/vision\/blob\/master\/torchvision\/models\/alexnet.py with link \"https:\/\/github.com\/pytorch\/vision\/blob\/master\/torchvision\/models\/alexnet.py\"\n<code class=\"lang-py\">import torch.nn as nn\nimport torch.utils.model_zoo as model_zoo\n\n\n__all__ = ['AlexNet', 'alexnet']\n\n\nmodel_urls = {\n    'alexnet': 'https:\/\/download.pytorch.org\/models\/alexnet-owt-4df8aa71.pth',\n}\n\n\nclass AlexNet(nn.Module):\n\n    def __init__(self, num_classes=1000):\n        super(AlexNet, self).__init__()\n        self.features = nn.Sequential(\n            nn.Conv2d(3, 64, kernel_size=11, stride=4, padding=2),\n            nn.ReLU(inplace=True),\n            nn.MaxPool2d(kernel_size=3, stride=2),\n<\/code>\n\n  This file has been truncated. show original with link \"https:\/\/github.com\/pytorch\/vision\/blob\/master\/torchvision\/models\/alexnet.py\"\n\n\n\n\n\n\nbut changed line 18 from\nnn.Conv2d(3, 64, kernel_size=11, stride=4, padding=2)\nto\nnn.Conv2d(1, 64, kernel_size=11, stride=4, padding=2)\nbecause i am using oly one channel in training images,but i get this error\n<code class=\"lang-auto\">Traceback (most recent call last):\n  File \"training_script.py\", line 44, in <module>\n    outputs = AlexNet(inputs)\n  File \"C:\\Users\\Mukhtar\\Anaconda3\\lib\\site-packages\\torch\\nn\\modules\\module.py\", line 477, in __call__\n    result = self.forward(*input, **kwargs)\n  File \"C:\\Users\\Mukhtar\\Documents\\AI_projects\\gta\\AlexNetPytorch.py\", line 34, in forward\n    x = self.features(x)\n  File \"C:\\Users\\Mukhtar\\Anaconda3\\lib\\site-packages\\torch\\nn\\modules\\module.py\", line 477, in __call__\n    result = self.forward(*input, **kwargs)\n  File \"C:\\Users\\Mukhtar\\Anaconda3\\lib\\site-packages\\torch\\nn\\modules\\container.py\", line 91, in forward\n    input = module(input)\n  File \"C:\\Users\\Mukhtar\\Anaconda3\\lib\\site-packages\\torch\\nn\\modules\\module.py\", line 477, in __call__\n    result = self.forward(*input, **kwargs)\n  File \"C:\\Users\\Mukhtar\\Anaconda3\\lib\\site-packages\\torch\\nn\\modules\\pooling.py\", line 142, in forward\n    self.return_indices)\n  File \"C:\\Users\\Mukhtar\\Anaconda3\\lib\\site-packages\\torch\\nn\\functional.py\", line 396, in max_pool2d\n    ret = torch._C._nn.max_pool2d_with_indices(input, kernel_size, stride, padding, dilation, ceil_mode)\nRuntimeError: Given input size: (256x1x1). Calculated output size: (256x0x0). Output size is too small at c:\\programdata\\miniconda3\\conda-bld\\pytorch-cpu_1532499824793\\work\\aten\\src\\thnn\\generic\/SpatialDilatedMaxPooling.c:67\n<\/code>\nI dont know what is wrong, is it wrong to change the channel size like this , and if it is wrong can you please lead me to a neural network that work with one channel , as i said i am a newbie in pytorch and i dont want to write the nn myself.","y":"It looks like your fix regarding the input channels is right, but the spatial size might be too small.\nHow large is your input? AlexNet should work with 224x224 sized images.","z":"It looks like your fix regarding the input channels is right, but the spatial size might be too small.\nHow large is your input? AlexNet should work with 224x224 sized images.\nYes, my input size was the problem i gave it a (32x32) images rather than a (224x224) .\nI reshaped the input to (224x224) and now i am training the CNN.\nthanks for the help."},{"x":"I use Visdom vis.line() to draw the loss during training. But after a period of time, the loss window disappear in the env and not shown in the browser.\nThe loss draw function is as follows,\n<code class=\"lang-python\">    def plot_current_losses(self, iter_id, losses, opt):\n        \"\"\"\n        Plot losses in one figure current epoch\n        Args:\n            losses: dict\n                {'loss_1': value, 'loss_2': value}\n        \"\"\"\n        if not hasattr(self, 'plot_data'):\n            self.plot_data = {'x': [], 'y': [], 'legend': list(losses.keys())}\n        # : Here is different from CycleGAN\n        self.plot_data['x'].append(iter_id)\n        self.plot_data['y'].append([losses[k]\n                                    for k in self.plot_data['legend']])\n        try:\n            self.vis.line(\n                # Here X shape is [n_losses, n_epoch]\n                X=np.array(self.plot_data['x']),\n                Y=np.array(self.plot_data['y']),\n                opts={\n                    'title': opt.name,\n                    'legend': self.plot_data['legend'],\n                    'xlabel': 'iter_id',\n                    'ylabel': 'loss'},\n                win=self.win_id,\n                env=self.env\n            )\n        except ConnectionError:\n            self._throw_visdom_connection_error()\n<\/code>\nThere is no Error raised, but the loss window disappear\u2026Why?","y":"This is my snippet to display the loss curve in visdom which is working fine.\n<code class=\"lang-auto\">    vis = visdom.Visdom()\n    loss_window = vis.line(X=torch.zeros((1,)).cpu(),\n                           Y=torch.zeros((1)).cpu(),\n                           opts=dict(xlabel='epoch',\n                                     ylabel='Loss',\n                                     title='Training Loss',\n                                     legend=['Loss']))    \n    vis = visdom.Visdom()\n    accuracy_window = vis.line(X=torch.zeros((1,)).cpu(),\n                           Y=torch.zeros((1)).cpu(),\n                           opts=dict(xlabel='epoch',\n                                     ylabel='accuracy',\n                                     title='Training accuracy',\n                                     legend=['accuracy']))    \n#    \n    for epoch in range(200):  # loop over the dataset multiple times\n        \n        t0 = time.time()\n        \n        running_loss = 0\n        total_train = 0\n        correct_train = 0\n        \n        model.train()\n        for i, data in enumerate(train_loader, 0):\n            # get the inputs\n            t_image, mask = data\n            t_image, mask = Variable(t_image.to(device)), Variable(mask.to(device))\n        \n\n            # zeroes the gradient buffers of all parameters\n            optimizer.zero_grad()\n            # forward + backward + optimize\n            outputs = model(t_image) # forward\n            loss = criterion(outputs, mask) # calculate the loss\n            loss.backward() # back propagation\n            optimizer.step() # update gradients\n            running_loss += loss.item()\n            \n            # accuracy\n            _, predicted = torch.max(outputs.data, 1)\n            total_train += mask.nelement()  # number of pixel in the batch\n            correct_train += predicted.eq(mask.data).sum().item() # sum all precited pixel values\n            train_accuracy = 100 * correct_train \/ total_train\n           #avg_accuracy = train_accuracy \/ len(train_loader)                                     \n            \n            print('Epoch {}, train Loss: {:.3f}'.format(epoch, loss.item()), \"Training Accuracy: %d %%\" % (train_accuracy), '{:.3f} seconds'.format(time.time() - t0))\n            \n            \n            values = [epoch+1, running_loss, train_accuracy]\n            export_history(header, values, save_dir, save_file_name)\n\n            vis.line(\n                    X=torch.ones((1, 1)).cpu()*epoch,\n                    Y=torch.Tensor([loss]).unsqueeze(0).cpu(),\n                    win=loss_window,\n                    update='append')\n            \n            vis.line(\n                    X=torch.ones((1, 1)).cpu()*epoch,\n                    Y=torch.Tensor([train_accuracy]).unsqueeze(0).cpu(),\n                    win=accuracy_window,\n                    update='append')\n\n<\/code>\nAlso, there is some good example here with link \"https:\/\/www.programcreek.com\/python\/example\/92356\/visdom.Visdom\".","z":"This is my snippet to display the loss curve in visdom which is working fine.\n<code class=\"lang-auto\">    vis = visdom.Visdom()\n    loss_window = vis.line(X=torch.zeros((1,)).cpu(),\n                           Y=torch.zeros((1)).cpu(),\n                           opts=dict(xlabel='epoch',\n                                     ylabel='Loss',\n                                     title='Training Loss',\n                                     legend=['Loss']))    \n    vis = visdom.Visdom()\n    accuracy_window = vis.line(X=torch.zeros((1,)).cpu(),\n                           Y=torch.zeros((1)).cpu(),\n                           opts=dict(xlabel='epoch',\n                                     ylabel='accuracy',\n                                     title='Training accuracy',\n                                     legend=['accuracy']))    \n#    \n    for epoch in range(200):  # loop over the dataset multiple times\n        \n        t0 = time.time()\n        \n        running_loss = 0\n        total_train = 0\n        correct_train = 0\n        \n        model.train()\n        for i, data in enumerate(train_loader, 0):\n            # get the inputs\n            t_image, mask = data\n            t_image, mask = Variable(t_image.to(device)), Variable(mask.to(device))\n        \n\n            # zeroes the gradient buffers of all parameters\n            optimizer.zero_grad()\n            # forward + backward + optimize\n            outputs = model(t_image) # forward\n            loss = criterion(outputs, mask) # calculate the loss\n            loss.backward() # back propagation\n            optimizer.step() # update gradients\n            running_loss += loss.item()\n            \n            # accuracy\n            _, predicted = torch.max(outputs.data, 1)\n            total_train += mask.nelement()  # number of pixel in the batch\n            correct_train += predicted.eq(mask.data).sum().item() # sum all precited pixel values\n            train_accuracy = 100 * correct_train \/ total_train\n           #avg_accuracy = train_accuracy \/ len(train_loader)                                     \n            \n            print('Epoch {}, train Loss: {:.3f}'.format(epoch, loss.item()), \"Training Accuracy: %d %%\" % (train_accuracy), '{:.3f} seconds'.format(time.time() - t0))\n            \n            \n            values = [epoch+1, running_loss, train_accuracy]\n            export_history(header, values, save_dir, save_file_name)\n\n            vis.line(\n                    X=torch.ones((1, 1)).cpu()*epoch,\n                    Y=torch.Tensor([loss]).unsqueeze(0).cpu(),\n                    win=loss_window,\n                    update='append')\n            \n            vis.line(\n                    X=torch.ones((1, 1)).cpu()*epoch,\n                    Y=torch.Tensor([train_accuracy]).unsqueeze(0).cpu(),\n                    win=accuracy_window,\n                    update='append')\n\n<\/code>\nAlso, there is some good example here with link \"https:\/\/www.programcreek.com\/python\/example\/92356\/visdom.Visdom\"."},{"x":"I have the image dataset  include :\nfrom sklearn.model_selection import  train_test_split\ntrain_set_x,test_set_x,train_set_y,test_set_y=train_test_split(n_train_samples,n_train_labels,test_size=0.2,random_state=0)\ntrain_set_x shape: (12000, 128, 128, 3)\ntrain_set_y shape: (1, 12000)\ntest_set_x shape: (3000, 128, 128, 3)\ntest_set_y shape: (1, 3000)\nHow  do I  load the images into  pytorch for training?","y":"You can make use of Dataset module and further DataLoader module provided by Pytorch for feeding images to your designed network.","z":"You can make use of Dataset module and further DataLoader module provided by Pytorch for feeding images to your designed network.\nI found downloaded imagenet and found it painful to figure out to train a model using it\u2026\nIt would be VERY useful if pytorch people provide an example (or tutorial) just to let us know how should we use imagenet and load the data"},{"x":"<code class=\"lang-auto\">class exampleNet(nn.Module):\n    def __init__(self):\n        super().__init__()\n        self.layer = torch.nn.Sequential(\n            torch.nn.Conv2d(3, 3, 9)\n\n        )\n    def forward(self, x):\n        x = self.layer(x)\n        x = self.layer(x)\n        x = self.layer(x)\n        x = self.layer(x)\n        return x\n<\/code>","y":"Your model contains one trainable layer which apparently should be used 4 times in a loop.\nSince the number of output channels does not match the number on input channels, your code won\u2019t work currently. Also you\u2019ve forgot to return x in your forward method.","z":"It has 4 conv layers\nYour model contains one trainable layer which apparently should be used 4 times in a loop.\nSince the number of output channels does not match the number on input channels, your code won\u2019t work currently. Also you\u2019ve forgot to return x in your forward method.\nPlease correct me if im wrong.\nin other words, your networks has FLOP of a network with 4 layer and Parameters of network with one layer"},{"x":"Hi,\nI want to convolve some images with some known kernels. I thought I could use torch.nn.functional.conv2d for this. So suppose I have some images with shape [2048, 1, 12, 12] (that is, a batch of 2048 images with 1 channel of size 12x12 pixels) and a kernel like kernel = torch.tensor([[-1, 1],[-1, 1]]), i.e. a 2x2 kernel. How can I convolve every image in my batch with this kernel?\nI thought this could be done with torch.nn.functional.conv2d(images, kernel.expand(2048, 1, 2, 2)) but this results a final matrix of size [2048, 2048, 11, 11]. If I read the docs correctly, this is due to having the number of groups the same as the amount of out-channels, so every input is convolved with every filter.\nThis is not what I want. What I want is that every image is convolved with the same kernel. Also, I want the output size to match the input size, as if I am calling scipy.signal.convolve2d(image, kernel, mode='same', boundary='symm') for each image in the batch (this is what I am doing now, but this is slow because it involves many copies from GPU to CPU).\nCan I do this operation with pytorch\u2019s functional.conv2d as to avoid copying data from the GPU to CPU? If so what kind of parameters do I need to pass to match my expected result?\nThanks in advance.","y":"\n\n\n mlterpstra-umc:\n\nI thought this could be done with torch.nn.functional.conv2d(images, kernel.expand(2048, 1, 2, 2)) but this results a final matrix of size [2048, 2048, 11, 11].\n\n\nYes, that\u2019s right. I did not notice that the output size should be [2048, 1, 12, 12], so specifically 1 output channel. So the convolution kernel shape is [output_channels, input_channels, kernel_dim_0, kernel_dim_1].\nSo determining the kernel size:\n\nDesired output channels: 1\n\nInput channels: 1\n\nFilter size: 2x2\n\n\nTherefore,  the kernel size for this should just be [1, 1, 2, 2]:\n<code class=\"lang-auto\">>>> a = torch.randn(2048, 1, 12, 12)\n>>> pad = nn.ZeroPad2d((1, 0, 1, 0))\n>>> ap = pad(a)\n>>> ap.shape\ntorch.Size([2048, 1, 13, 13])\n>>> kernel = torch.randn(1, 1, 2, 2)\n>>> kernel.shape\ntorch.Size([1, 1, 2, 2])\n>>> b = F.conv2d(ap, kernel)\n>>> b.shape\ntorch.Size([2048, 1, 12, 12])\n<\/code>","z":"I think you just need to add padding. Given that your kernel size is 2x2 (even kernel-size),I think you can add manual padding only at one side of each dimension using torch.nn.ZeroPad2d\nIf you just use padding=1, that will increase the output size to 13x13:\n<code class=\"lang-python\">>>> import torch\n>>> import torch.nn.functional as F\n>>> a = torch.randn(2048, 1, 12, 12)\n>>> a.shape\ntorch.Size([2048, 1, 12, 12])\n>>> kernel = torch.randn(2048, 1, 2, 2)\n>>> b = F.conv2d(a, kernel, padding=1)\n>>> b.shape\ntorch.Size([2048, 2048, 13, 13])\n<\/code>\nBut if you manually add padding, your output will be the same shape as input:\n<code class=\"lang-python\">>>> import torch.nn as nn\n>>> pad = nn.ZeroPad2d((1, 0, 1, 0))\n>>> ap = pad(a)\n>>> ap.shape\ntorch.Size([2048, 1, 13, 13])\n>>> b = F.conv2d(ap, kernel)\n>>> b.shape\ntorch.Size([2048, 2048, 12, 12])\n<\/code>\nHi, thanks for you answer. I read up on how scipy.signal.convolve2d with same mode. This works indeed with padding, but is \u2018centered\u2019 with respect to \u2018full\u2019, so I guess in my case that means using the ReflectionPad2d layer and follow the same approach you suggest.\nThe other question still remains because the final output is still [2048, 2048, 12, 12] because every image is convolved with every mask rather than each image with each corresponding mask, which would make the final output [2048, 1, 12, 12]. Do you also have a solution for that?\nThanks!\nI think I might have a solution, if I do torch.nn.functional.conv2d(pad(data).view(1, 2048, 13, 13), kernel.expand(2048, 1, 2, 2), groups=2048) I get a result of shape [1, 2048, 12, 12], where most of the data is the same as convolving with convolve2d, except for the first row, and there rows where every value is nan.\nI\u2019ll just look into that and keep you updated.\nThanks!\n\n\n\n mlterpstra-umc:\n\nI thought this could be done with torch.nn.functional.conv2d(images, kernel.expand(2048, 1, 2, 2)) but this results a final matrix of size [2048, 2048, 11, 11].\n\n\nYes, that\u2019s right. I did not notice that the output size should be [2048, 1, 12, 12], so specifically 1 output channel. So the convolution kernel shape is [output_channels, input_channels, kernel_dim_0, kernel_dim_1].\nSo determining the kernel size:\n\nDesired output channels: 1\n\nInput channels: 1\n\nFilter size: 2x2\n\n\nTherefore,  the kernel size for this should just be [1, 1, 2, 2]:\n<code class=\"lang-auto\">>>> a = torch.randn(2048, 1, 12, 12)\n>>> pad = nn.ZeroPad2d((1, 0, 1, 0))\n>>> ap = pad(a)\n>>> ap.shape\ntorch.Size([2048, 1, 13, 13])\n>>> kernel = torch.randn(1, 1, 2, 2)\n>>> kernel.shape\ntorch.Size([1, 1, 2, 2])\n>>> b = F.conv2d(ap, kernel)\n>>> b.shape\ntorch.Size([2048, 1, 12, 12])\n<\/code>\nHi, thank you very much! I was able to match the result from scipy.signal.convolve2d(img, mask, mode='same', boundary='symm') with the following Pytorch code:\n<code class=\"lang-auto\">imgs = torch.rand(2048, 1, 12, 12, dtype=torch.float32, device='cuda')\n# F.conv2d does a cross correlation so be sure to flip the mask to do a convolution\nmask = torch.flip(torch.tensor([[-1, 1],[-1, 1]], dtype=torch.float32), [0, 1]).expand(1, 1, 2, 2).to('cuda')\npad = torch.nn.ReplicationPad2d((1, 0, 1, 0))\nreturn F.conv2d(pad(imgs), mask)\n<\/code>\nAgain, thank you very much!"},{"x":"I\u2019m getting the following error:\nAttributeError: cannot assign module before Module.__init__() call.\nI\u2019m trying to create an instance of my class :\n<code class=\"lang-auto\">class ResNetGenerator(nn.Module):\n    def __init__(self, input_nc=3, output_nc=3, n_residual_blocks=9, use_dropout=False):\n        # super(ResNetGenerator, self).__init__()\n        super().__init__()\n<\/code>\nI\u2019m calling super().__init__() but in vain.\nWhat I am doing wrong here?","y":"Oh, it seems I\u2019ve missed an important line in the stack trace.\n<code class=\"lang-python\">File \"train.py\", line 40, in <module>\n    model = ColorizationCycleGAN(args)\n<\/code>\nCould you check your definition of ColorizationCycleGAN?\nI would always recommend to update to the latest release, but this error seems unrelated to your PyTorch version.","z":"Your posted code looks alright and is working on my machine.\nDid you check the right code indent?\n Thanks for your response.  Yes, I checked the indentation. Indeed, my code runs normally and executes __init__ until the end, the problem rises when the __setattr__ function is called.\nHere is the code of my Class:\n<code class=\"lang-auto\">class ResNetGenerator(nn.Module):\n    def __init__(self, input_nc=3, output_nc=3, n_residual_blocks=9, use_dropout=False):\n        print(\"*\"*50)\n        super(ResNetGenerator, self).__init__()\n        # super().__init__()\n        # nn.Module.__init__(self)\n\n        # Input layer\n        model = [nn.ReflectionPad2d(3),\n                 nn.Conv2d(input_nc, 64, kernel_size=7, padding=0),\n                 nn.InstanceNorm2d(64),\n                 nn.ReLU(True)]\n\n        # Encoding layers\n        in_features = 64\n        out_features = in_features * 2\n\n        for _ in range(2):\n            model += [nn.Conv2d(in_features, out_features, kernel_size=3, stride=2, padding=1),\n                      nn.InstanceNorm2d(out_features),\n                      nn.ReLU(True)]\n            in_features = out_features\n            out_features = in_features * 2\n\n        # Transformations layers (Residual blocks)\n        for _ in range(n_residual_blocks):\n            model += [ResidualBlock(in_features, use_dropout)]\n        # Decoding layers\n        out_features = in_features \/\/ 2\n\n        for _ in range(2):\n            model += [nn.ConvTranspose2d(in_features, out_features, kernel_size=3, stride=2, padding=1, output_padding=1),\n                      nn.InstanceNorm2d(out_features),\n                      nn.ReLU(True)]\n            in_features = out_features\n            out_features = in_features \/\/ 2\n\n        # Output layer\n        model += [nn.ReflectionPad2d(3),\n                  nn.Conv2d(64, output_nc, kernel_size=7, padding=0),\n                  nn.Tanh()]\n\n        self.model = nn.Sequential(*model)\n        print(\"#\"*50)\n\n    def forward(self, x):\n        return self.model(x)\n<\/code>\nAnd Here is the complete Traceback:\n<code class=\"lang-auto\">File \"train.py\", line 40, in <module>\n    model = ColorizationCycleGAN(args)\nFile \"\/path\/cycle_gan.py\", line 27, in __init__\n    self.G_A2B = ResNetGenerator(input_nc=self.input_nc, output_nc=self.output_nc, n_residual_blocks=9, use_dropout=False)\nFile \"\/path\/.local\/lib\/python3.6\/site packages\/torch\/nn\/modules\/module.py\", line 544, in __setattr__\n    \"cannot assign module before Module.__init__() call\")\nAttributeError: cannot assign module before Module.__init__() call\n<\/code>\nCould you post the definition of ResidualBlock?\nIf I remove it, the code still works, so the error might be in the definition of ResidualBlock.\n , Here is the definition of ResidualBlock:\n<code class=\"lang-auto\">class ResidualBlock(nn.Module):\n    def __init__(self, in_features, use_dropout):\n        # super(ResidualBlock, self).__init__()\n        super().__init__()\n        conv_block = [nn.ReflectionPad2d(1),\n                      nn.Conv2d(in_features, in_features, 3, padding=0),\n                      nn.InstanceNorm2d(in_features),\n                      nn.ReLU(inplace=True),\n                      nn.ReflectionPad2d(1),\n                      nn.Conv2d(in_features, in_features, 3, padding=0),\n                      nn.InstanceNorm2d(in_features)]\n        if use_dropout:\n            conv_block.insert(4, nn.Dropout(0.5))\n\n        self.conv_block = nn.Sequential(*conv_block)\n\n    def forward(self, x):\n        return x + self.conv_block(x)\n<\/code>\nThanks for the code. It still works on my machine with PyTorch 1.0.0.dev20181217 and Python3.7.\nAs far as I know, you would have to use the first super call with the class name and self in Python2.x.\nCould this be the reason for the error you are getting?\nThanks for your response. I checked my Python and Pytorch versions. I\u2019m using Python 3.6.7 and Pytorch 0.4.1, maybe should I update my Pytorch version?\nConcerning the super call, I tried the two methods but in vain.\nOh, it seems I\u2019ve missed an important line in the stack trace.\n<code class=\"lang-python\">File \"train.py\", line 40, in <module>\n    model = ColorizationCycleGAN(args)\n<\/code>\nCould you check your definition of ColorizationCycleGAN?\nI would always recommend to update to the latest release, but this error seems unrelated to your PyTorch version.\n, Thank you very much for your support. Indeed, I realized that I did not call super().__init__() in ColorizationCycleGAN.\nThis mistake took me a long time, I thank you very much for your help.\nGood to hear it\u2019s working!\nNow you should update PyTorch to 1.0. \nYou\u2019ll find the install instructions here with link \"https:\/\/pytorch.org\/get-started\/locally\/\"."},{"x":"I\u2019m trying to modify my image classifier by adding decoder and reconstruction loss as autoencoder.\nI want to use the BCELoss which requires targets range from 0 to 1.\nBut my classifier has input normalization at the data loader as in usual so the input range is not fitted.\nSo I want to get it back to the original range by inverse normalize.\nI\u2019m working on cifar10, so I have mean(0.4914, 0.4822, 0.4465) and std(0.2023, 0.1994, 0.2010).\nand the input shape is (batch, channel, H, W).\nWhat is the best way to inverse normalize in this case?\nOr, Is there any way to get both normalized and unnormalized input from the train loader?","y":"y = (x - mean) \/ std\nx = (y * std) + mean\nJust do such an operation per-channel on your output:\n<code class=\"lang-python\"># assuming x and y are Batch x 3 x H x W and mean = (0.4914, 0.4822, 0.4465), std = (0.2023, 0.1994, 0.2010)\nx = y.new(*y.size())\nx[:, 0, :, :] = y[:, 0, :, :] * std[0] + mean[0]\nx[:, 1, :, :] = y[:, 1, :, :] * std[1] + mean[1]\nx[:, 2, :, :] = y[:, 2, :, :] * std[2] + mean[2]\n<\/code>","z":"y = (x - mean) \/ std\nx = (y * std) + mean\nJust do such an operation per-channel on your output:\n<code class=\"lang-python\"># assuming x and y are Batch x 3 x H x W and mean = (0.4914, 0.4822, 0.4465), std = (0.2023, 0.1994, 0.2010)\nx = y.new(*y.size())\nx[:, 0, :, :] = y[:, 0, :, :] * std[0] + mean[0]\nx[:, 1, :, :] = y[:, 1, :, :] * std[1] + mean[1]\nx[:, 2, :, :] = y[:, 2, :, :] * std[2] + mean[2]\n<\/code>\nThanks for the solution !\nIf I can change the per-channel operation to one element-wise tensor multiplication by expanding the mean and std in proper way, then is it gonna be faster? Or since its graph, it will not affect much?\nSomething like below,\nt_mean = torch.FloatTensor(mean).view(3,1,1).expand(3,H,W).contiguous().view(1,3,H,W)\nt_std = torch.FloatTensor(std).view(3,1,1).expand(3,H,W).contiguous().view(1,3,H,W)\nx = y * t_std.expand(B,3,H,W) +t_mean.expand(B,3,H,W)\n\nI\u2019m not sure what is most simple way to change the shape properly though\u2026\nbecause it\u2019s only 3 channels, it shouldn\u2019t matter either ways.\nHello,\nI had the same problem, but came up with a somewhat different way to approach it which may be of interest. I have defined the following transform which lets you return both the normalised and the unnormalised version of the data from data loader:\n<code class=\"lang-auto\">class Branch(object):\n\n    def __init__(self, transforms):\n        self.transforms = transforms\n\n    def __call__(self, img):\n        outputs = []\n        for t in self.transforms:\n            outputs.append(t(img))\n        return outputs\n\n    def __repr__(self):\n        format_string = self.__class__.__name__ + '('\n        for t in self.transforms:\n            format_string += '\\n'\n            format_string += '    {0}'.format(t)\n        format_string += '\\n)'\n        return format_string\n<\/code>\nUse for example like this:\n<code class=\"lang-auto\">    train_set = datasets.MNIST(\n        '..\/data\/mnist',\n        train=True,\n        download=True,\n        transform=Branch([\n            transforms.Compose([\n                transforms.ToTensor(),\n                transforms.Normalize((0.1307,), (0.3081,))]),\n            transforms.ToTensor()]))\n<\/code>"},{"x":"Hi! I\u2019m trying to implement the baseline model described in the Stanford MURA paper. The following is the code for my model:\n<code class=\"lang-python\">model = densenet169(pretrained=True)\n    \nmodel.classifier = nn.Sequential(\n                        nn.Linear(in_features=1664, out_features=1),\n                        nn.Sigmoid()\n                   )\n\nmodel = model.cuda()\n<\/code>\nThis works fine with the default input size of 224x224. However, the paper describes the input size to be 320x320. However, when input of this shape is fed to the model defined in the code above, it unsurprisingly throws a shape mismatch error. I\u2019m new to PyTorch (and deep learning in general) and I\u2019m having trouble implementing this. Can someone please help? I\u2019ve looked at other solutions proposed for ResNet and other networks but I haven\u2019t been able to apply any of those to my DenseNet169.","y":"Most likely you\u2019ll get the tensor shapes for the tensors where the mismatch occurred.\nIn your case, it\u2019s:\n\nRuntimeError: size mismatch, m1: [1 x 26624], m2: [1664 x 1] at \u2026\n\nIf you change the in_features of your linear layer to 26624, your code will run successfully with 320x320 shaped inputs.\nAnother approach would be to print the shape of the incoming tensors using this small module:\n<code class=\"lang-python\">class Print(nn.Module):\n    def __init__(self):\n        super(Print, self).__init__()\n        \n    def forward(self, x):\n        print(x.shape)\n        return x\n\nmodel.classifier = nn.Sequential(\n    Print(),\n    nn.Linear(in_features=26624, out_features=1),\n    nn.Sigmoid()\n)\n<\/code>","z":"Most likely you\u2019ll get the tensor shapes for the tensors where the mismatch occurred.\nIn your case, it\u2019s:\n\nRuntimeError: size mismatch, m1: [1 x 26624], m2: [1664 x 1] at \u2026\n\nIf you change the in_features of your linear layer to 26624, your code will run successfully with 320x320 shaped inputs.\nAnother approach would be to print the shape of the incoming tensors using this small module:\n<code class=\"lang-python\">class Print(nn.Module):\n    def __init__(self):\n        super(Print, self).__init__()\n        \n    def forward(self, x):\n        print(x.shape)\n        return x\n\nmodel.classifier = nn.Sequential(\n    Print(),\n    nn.Linear(in_features=26624, out_features=1),\n    nn.Sigmoid()\n)\n<\/code>\nHi, . Thanks for the prompt response. This worked! Thank you so much."},{"x":"mask_pred = net(imgs)\nIt is mapped to values of 0.96, 0.006, and so on. (because of normalization: x\/ 255)\nI\u2019d like to see if this format was put in imshow() to see if it was well segmented\nso I try\u2026\nimgs # size(1,1,640,640) = size(batch_size, channel, width, height)\n    mask_pred = net(imgs)[0]\n\n    mask_pred = (mask_pred >0.5).float()\n\n    np_img = mask_pred.cpu().detach().numpy() # GPU Tensor -> CPU Tensor -> numpy\n\n    np_img = np.transpose(np_img, (1, 2, 0)) # <class 'tuple'>: (640,640,1), dtype=float32\n\n    plt.imshow(np_img)\n\nTypeError: Invalid dimensions for image data\nhow to solve it?","y":"Since your image has only 1 channel, then you should remove the last channel. So instead of having shape (640, 640, 1) you should have (640, 640):\n<code class=\"lang-python\">np_img = np_img[:, :, 0]\nplt.imshow(np_img)\n<\/code>","z":"Since your image has only 1 channel, then you should remove the last channel. So instead of having shape (640, 640, 1) you should have (640, 640):\n<code class=\"lang-python\">np_img = np_img[:, :, 0]\nplt.imshow(np_img)\n<\/code>"},{"x":"Hi, there:\nI am training a multilabel annotation predictor, with 50 labels for each image.\nIn the first try, I just employ BCEWithLogitsLoss without any extra arguments:\n<code class=\"lang-auto\">criterion = nn.BCEWithLogitsLoss().cuda()\n<\/code>\nand it works fine. After this, I wanna feed some weight term for positive samples, just like the way described in the official docs, with link \"https:\/\/pytorch.org\/docs\/stable\/nn.html#torch.nn.BCEWithLogitsLoss\" with a torch.Tensor with the size of 50. But I received the following error:\n<code class=\"lang-auto\">TypeError: __init__() got an unexpected keyword argument 'pos_weight'\n<\/code>\nCould anyone give some advice? Thanks in advance!\nI used a copy of pytorch 0.4.0, on a 64-bit linux machine.","y":"This problem is because there\u2019s no pos_weight argument in BCEWithLogitsLoss in 0.4.0 \u2026 SORRY for the inconvenience.","z":"From the official documents, it is said:\n\n\n\npos_weight  \u2013 a weight of positive examples. Must be a vector with length equal to the number of classes.\n\n\nSo, I\u2019ve no idea what\u2019s wrong with my code\u2026\nThis problem is because there\u2019s no pos_weight argument in BCEWithLogitsLoss in 0.4.0 \u2026 SORRY for the inconvenience.\nThanks for your answer !\nBut,how to add pos_weight to BceWithLogitsLoss in the new version?\nI try to use the method to solve some data-imbalance problems.\nThanks a lot !"},{"x":"I ran into a rather curious problem while trying to implement this semi-supervised representation learning algorithm with link \"https:\/\/arxiv.org\/abs\/1603.09246\"\n(tl;dr Pretrain a CNN by letting it solve jigsaw puzzles. Each image gets cut into nine tiles. These tiles are permutated with a precalculated permutation and the net has to predict the index in the permutation set)\nI am using a custom Permutator object to hold the permutations, which I load from a file.\nThis object is given to my wrapper class around torchvision.datasets.ImageFolder().\nWhen I set num_workers > 0 in torch.utils.data.DataLoader(), the whole script gets executed multiple times before each epoch.\nHere is some pseudo-code of my setup:\n<code class=\"lang-python\">from __future__ import print_function, division\n\nimport torch\nimport torch.nn as nn\nimport torch.optim as optim\nfrom torch.utils.data import Dataset\nfrom torchvision import datasets, transforms\nfrom pathlib import Path\n\nimport permutation\nimport jigsaw_model\n\n\ntorch.backends.cudnn.benchmark = True\ndevice = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n\n# Define hyperparameters\nimage_size = (99, 99)\nbatch_size = 256\nn = 3\nlr = 10e-3\nn_epochs = 100\n\n# Create custom permutation object\n# This object holds a list of 100 permutations and can select one at random\npermutator = permutation.Permutation(filename=Path(\"data\", \"permutations_max_100.csv\"))\nprint(\"Permutator created.\")\n\n\n# Training transforms\ntransforms_train = transforms.Compose([\n    ... # Normalize the image and cut it into n*n puzzle tiles\n])\n\n# Image files\nimages_train = datasets.ImageFolder(root=\"my_path\/to\/images\", transform=None)\n\n# This is just a wrapper on torchvision.datasets.ImageFolder(). It loads an image, cuts it into n*n puzzle tiles\n#  and permutates the tiles with a permutation obtained from the permutator object\ndataset_train = jigsaw_model.JigsawTileDataset(dataset=images_train, transform=transforms_train, n=n,\n                                               permutator=permutator)\n\n# If I set num_workers > 0 here, all the above code is executed multiple times\nnum_workers = 0\nloader_train = torch.utils.data.DataLoader(dataset_train, num_workers=num_workers, shuffle=True, pin_memory=True,\n                                           batch_size=batch_size)\n\n# Define the model\nmodel = ...\n\ncriterion = nn.CrossEntropyLoss()\noptimizer = optim.Adam(model.parameters(), lr=lr)\n\nif __name__ == '__main__':\n    for epoch in range(n_epochs):\n        train(...)\n        validate(...)\n<\/code>\nA minimal working example can be found here. with link \"https:\/\/github.com\/bewagner\/jigsaw-puzzler\"\nHow often the code is executed can be tracked by the output print(\"Permutator created.\").\nIt seems to be num_workers+1, which makes sense, since it\u2019s one for the base process and one for each worker.\nNevertheless I am puzzled by this behaviour, since I want to avoid multiple creation of the Permutator object before each epoch, but still I wish to use multiple workers for speed considerations.\nAny help on this would be very much appreciated \nI am using the following setup:\nPytorch 0.4\nPython 3.6\nWindows 7\nWether I run the code from Pycharm or the Anaconda console, the same behaviour is observed.","y":"I think it\u2019s related to the multiprocessing behavior in Windows.\nYou should wrap your whole code in another function and guard it:\n<code class=\"lang-auto\">def run():\n    for epoch in ...\n        train(...)\n\n\nif __name__=='__main__':\n    run()\n<\/code>\nI\u2019m wondering why you won\u2019t get a Broken Pipe error, which is usually thrown in such a case.\nCould you try that?","z":"Here is my output for num_workers=3:\n<code class=\"lang-python\">Permutator created.\nPermutator created.\nPermutator created.\nEpoch 0 [0\/196]\t\tLoss 4.6297\tAcc 1.172\tTime 2.37\nEpoch 0 [20\/196]\t\tLoss 3.9302\tAcc 13.672\tTime 0.19\nEpoch 0 [40\/196]\t\tLoss 3.2485\tAcc 24.609\tTime 0.11\nEpoch 0 [60\/196]\t\tLoss 2.9858\tAcc 32.812\tTime 0.11\nEpoch 0 [80\/196]\t\tLoss 2.6841\tAcc 40.625\tTime 0.12\nEpoch 0 [100\/196]\t\tLoss 2.2454\tAcc 44.922\tTime 0.11\nEpoch 0 [120\/196]\t\tLoss 2.1016\tAcc 48.828\tTime 0.11\nEpoch 0 [140\/196]\t\tLoss 2.0078\tAcc 50.781\tTime 0.12\nEpoch 0 [160\/196]\t\tLoss 2.1359\tAcc 50.000\tTime 0.11\nEpoch 0 [180\/196]\t\tLoss 2.0075\tAcc 49.609\tTime 0.11\n\nFinished Epoch 0 in 69.74s\tAvg. Loss 2.6975\tAvg. Accuracy 38.306\n\nPermutator created.\nPermutator created.\nPermutator created.\nPermutator created.\nEpoch 1 [0\/196]\t\tLoss 1.6530\tAcc 60.547\tTime 0.33\nEpoch 1 [20\/196]\t\tLoss 2.0716\tAcc 53.125\tTime 0.11\nEpoch 1 [40\/196]\t\tLoss 1.8582\tAcc 56.250\tTime 0.12\nEpoch 1 [60\/196]\t\tLoss 1.7714\tAcc 58.594\tTime 0.12\nEpoch 1 [80\/196]\t\tLoss 1.5090\tAcc 62.500\tTime 0.18\nEpoch 1 [100\/196]\t\tLoss 1.5322\tAcc 63.281\tTime 0.11\nEpoch 1 [120\/196]\t\tLoss 1.4636\tAcc 65.234\tTime 0.12\n<\/code>\nYou can see that after the first epoch the permutator is created four times (two times for training and two times for validation).\nI think it\u2019s related to the multiprocessing behavior in Windows.\nYou should wrap your whole code in another function and guard it:\n<code class=\"lang-auto\">def run():\n    for epoch in ...\n        train(...)\n\n\nif __name__=='__main__':\n    run()\n<\/code>\nI\u2019m wondering why you won\u2019t get a Broken Pipe error, which is usually thrown in such a case.\nCould you try that?\nYeah, I\u2019ll try that and tell you if it worked!\nThat was it! Thank you very much! \nFor anyone that runs into the same issue, I\u2019ll add that I had to wrap the whole code in a function to make the issue disappear. That means, the above code example should be:\n<code class=\"lang-auto\">def run():\n    torch.backends.cudnn.benchmark = True\n    device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n\n    ... \n    # (Load data, define loader and model)\n\n    for epoch in ...\n        train(...)\n\n\nif __name__=='__main__':\n    run()\n<\/code>\nTried  but  still getting the same errorbroken_pipe.PNG974\u00d7727 32.9 KB with link \"https:\/\/discuss.pytorch.org\/uploads\/default\/original\/2X\/2\/2d494afad3846cf2da487038404a4eadcc8ef672.png\""},{"x":"I am implementing a custom dataset class. If I set batchsize>=1, the code doesn\u2019t work, please help me.\n<code class=\"lang-auto\">import glob\nimport os\n\nfrom torch.utils.data import Dataset\nfrom PIL import Image\nimport torchvision.transforms as transforms\nfrom torch.utils.data import DataLoader\n\n\nclass ImageDataset(Dataset):\n    def __init__(self, root, transforms_=None):\n        self.transform = transforms.Compose(transforms_)\n\n        self.files_A = sorted(glob.glob(os.path.join(root, 'train\/A') + '\/*.*'))\n        self.files_B = sorted(glob.glob(os.path.join(root, 'train\/B') + '\/*.*'))\n\n    def __getitem__(self, index):\n        item_A = self.transform(Image.open(self.files_A[index % len(self.files_A)]))\n        item_B = self.transform(Image.open(self.files_B[index % len(self.files_B)]))\n\n        return {'A': item_A, 'B': item_B}\n\n    def __len__(self):\n        return max(len(self.files_A), len(self.files_B))\n\n\ntransforms_ = [transforms.ToTensor()]\ndataloader = DataLoader(ImageDataset('datasets\/horse2zebra\/', transforms_=transforms_),\n                        batch_size=2,\n                        shuffle=True,\n                        num_workers=0,\n                        drop_last=True)\n\nfor i, batch in enumerate(dataloader):\n    print('it works.')\n<\/code>\n<code class=\"lang-auto\">Traceback (most recent call last):\n  File \"C:\/N\/tt.py\", line 34, in <module>\n    for i, batch in enumerate(dataloader):\n  File \"C:\\ProgramData\\Anaconda3\\lib\\site-packages\\torch\\utils\\data\\dataloader.py\", line 615, in __next__\n    batch = self.collate_fn([self.dataset[i] for i in indices])\n  File \"C:\\ProgramData\\Anaconda3\\lib\\site-packages\\torch\\utils\\data\\dataloader.py\", line 229, in default_collate\n    return {key: default_collate([d[key] for d in batch]) for key in batch[0]}\n  File \"C:\\ProgramData\\Anaconda3\\lib\\site-packages\\torch\\utils\\data\\dataloader.py\", line 229, in <dictcomp>\n    return {key: default_collate([d[key] for d in batch]) for key in batch[0]}\n  File \"C:\\ProgramData\\Anaconda3\\lib\\site-packages\\torch\\utils\\data\\dataloader.py\", line 209, in default_collate\n    return torch.stack(batch, 0, out=out)\nRuntimeError: invalid argument 0: Sizes of tensors must match except in dimension 0. Got 1 and 3 in dimension 1 at c:\\a\\w\\1\\s\\tmp_conda_3.6_091443\\conda\\conda-bld\\pytorch_1544087948354\\work\\aten\\src\\th\\generic\/THTensorMoreMath.cpp:1333\n\nProcess finished with exit code 1\n\n<\/code>\nDirectory structure:\n<code class=\"lang-auto\">    .\n    \u251c\u2500\u2500 datasets                   \n    |   \u251c\u2500\u2500 <dataset_name>         # i.e. brucewayne2batman\n    |   |   \u251c\u2500\u2500 train              # Training\n    |   |   |   \u251c\u2500\u2500 A              # Contains domain A images (i.e. Bruce Wayne)\n    |   |   |   \u2514\u2500\u2500 B              # Contains domain B images (i.e. Batman)\n    |   |   \u2514\u2500\u2500 test               # Testing\n    |   |   |   \u251c\u2500\u2500 A              # Contains domain A images (i.e. Bruce Wayne)\n    |   |   |   \u2514\u2500\u2500 B              # Contains domain B images (i.e. Batman)\n    \n<\/code>","y":"It looks like some of your images are RGB (3 channels) images while others might be in grayscale format (1 channel).\nCould you try to convert all the RGB using:\n<code class=\"lang-python\">item_A = self.transform(Image.open(self.files_A[index % len(self.files_A)]).convert('RGB'))\nitem_B = self.transform(Image.open(self.files_B[index % len(self.files_B)]).convert('RGB'))\n<\/code>\nand run it again?","z":"It looks like some of your images are RGB (3 channels) images while others might be in grayscale format (1 channel).\nCould you try to convert all the RGB using:\n<code class=\"lang-python\">item_A = self.transform(Image.open(self.files_A[index % len(self.files_A)]).convert('RGB'))\nitem_B = self.transform(Image.open(self.files_B[index % len(self.files_B)]).convert('RGB'))\n<\/code>\nand run it again?\nThank you very much, there are some images with 1 channel indeed."},{"x":"In Pytorch, when using torchvision MNIST dataset, we can get a digit as follow :\n<code class=\"lang-auto\">   import torchvision\n    import torchvision.transforms as transforms\n    from torch.utils.data import DataLoader, Dataset, TensorDataset\n    \n    tsfm = transforms.Compose([transforms.Resize((16, 16)),\n                               transforms.ToTensor(),\n                               transforms.Normalize((0.1307,), (0.3081,))])\n    \n    mnist_ds = torchvision.datasets.MNIST(root='..\/..\/..\/_data\/mnist',train=True,\n                                    download=True, transform=tsfm)\n    \n    \n    digit_12 = mnist_ds[12]\n<\/code>\nThough it is possible to slice on most datasets, we can not slice on this one:\ndigit_12_to_14 = mnist_ds[12:15]\nwill return\n\nValueError: Too many dimensions: 3 > 2.\n\n\nThis is due to a Image.fromarray() in the getItem()\nIs it possible to use MNIST dataset without using a Dataloader ? How ?\nPS: The reason why I would like to avoid using Dataloader is that sending batches one at a time to the GPU slow down the training. I prefer to send to the GPU the whole data just one time. For this I need to have access to the whole TRANSFORMED dataset.","y":"Yeah, I\u2019ve built torchvision from source, where these names were modified (source with link \"https:\/\/github.com\/pytorch\/vision\/blob\/98ca260bc834ec94a8143e4b5cfe9516b0b951a2\/torchvision\/datasets\/mnist.py#L57\").\nYou are right, if you need to transform the PIL.Images, my suggestion won\u2019t work.\nI had in mind to push the already transformed tensors onto the GPU.\nHowever, I\u2019m not even sure if you\u2019ll really see a performance benefit compared to push the data onto the GPU in your training loop using multiple workers.","z":"Would a workaround of setting the batch_size to the size of the complete dataset solve this issue?\nIt did. I I came up with 2 solutions in fact.\n<code class=\"lang-auto\">print(\"\\nFirst...\")\nst = time()\nx_all_ts = torch.tensor([mnist_ds[i][0].numpy() for i in range(0, len(mnist_ds))])\nt_all_ts = torch.tensor([mnist_ds[i][1] for i in range(0, len(mnist_ds))])\nprint(f\"{time()-st}   images:{x_all_ts.size()}  targets:{t_all_ts.size()} \")\n\nprint(\"\\nSecond...\")\nst = time()\ndl = DataLoader(dataset=mnist_ds, batch_size=len(mnist_ds))\nX, T = list(dl)[0]\nprint(f\"{time()-st}   images:{X.size()}  targets:{T.size()} \")\n\nFirst...\n42.57414126396179   images:torch.Size([60000, 1, 16, 16])  targets:torch.Size([60000]) \nSecond...\n17.22612690925598   images:torch.Size([60000, 1, 16, 16])  targets:torch.Size([60000])\n<\/code>\nBy the way, it makes the training much faster to put MNIST on the GPU just once and  manually create batches in the training loop.\nI am looking forward a transparent way to do so from pytorch, that is a way to put the dataset on the GPU before training and be able to use the DataLoader in the training loop.\nIt should work, if you push all the data in your Dataset onto the GPU, and use num_workers=0 as well as pin_memory=False in your DataLoader.\nYou could write your own Dataset of if you would like to use the torchvision.dataset.MNIST one, you could manipulate the underlying dataset.data and dataset.targets.\nI guess you meant dataset.train_data and dataset.train_labels\nUsing dataset.train_labels is a great idea. It\u2019s exaltly what I wanted.\nBut as you  know, dataset.train_data is not transformed. MNIST images are transformed in getitem()  and though I could develop my own dataset, I like the simplicity of torchvision MNIST.\nSo my updated solution is:\n<code class=\"lang-auto\">x_all_ts = torch.tensor([mnist_ds[i][0].numpy() for i in range(0, len(mnist_ds))])\nt_all_ts = mnist_ds.train_labels\n<\/code>\nRegarding your first sentence , I am a bit confused. How to push the MNIST dataset onto the GPU? did you mean push dataset.train_data ?\nYeah, I\u2019ve built torchvision from source, where these names were modified (source with link \"https:\/\/github.com\/pytorch\/vision\/blob\/98ca260bc834ec94a8143e4b5cfe9516b0b951a2\/torchvision\/datasets\/mnist.py#L57\").\nYou are right, if you need to transform the PIL.Images, my suggestion won\u2019t work.\nI had in mind to push the already transformed tensors onto the GPU.\nHowever, I\u2019m not even sure if you\u2019ll really see a performance benefit compared to push the data onto the GPU in your training loop using multiple workers.\nOn my tiny laptop GPU, in a training loop, pushing each batch one at a time onto the GPU with default number of workers (I guess 1) takes 2 or 3 more time than pushing the whole dataset once then manually creating batches in the traning loop. Same thing on google cobab GPU.\nI will try to play with mutliples workers and pin_memory on google colab.\nThanks for you help. Take care.\nI agree with you. I have tried to push my whole validation one time onto the GPU  and then push each batch one (bacth size is 100)at a time onto the GPU, the result shows what you have said is correct, the former is exactly faster than the latter\nand most tutorials  I read use the slow Dataloader  solution whereas using something like:\n<code class=\"lang-auto\"> for i in range((n - 1) \/\/ bs + 1):\n`   x = x_ts[i * bs: i * bs + bs]`     # x_ts  in GPU\n<\/code>\nmakes it much faster (with one worker of course).\nWhile this approach might be faster for tiny datasets and small models, you are using your precious GPU memory to store all the data.\nIn my opinion the standard workflow is to save the GPU memory for your (large) model and to only push the current batch of data as needed. If tutorials teach you to push all data onto the GPU I would consider that an edge case (small data, small model).\n(Tutorials I read teach NOT to push all data onto the GPU)\nI understand things better now. In fact, you convinced me to give up my approach and adopt best practices right now (even on MNIST or CIFAR10). Thanks a million for your explanation and time."},{"x":"I am working on semantic segmentation task and I have to make a customized dataset.\nThe images are 24-bit per pixel and the masks are 8-bit per pixel.\nMy customized dataset as follows:\n<code class=\"lang-auto\">lass MyDataset(Dataset):\n    def __init__(self, root, set_name,):\n        super(MyDataset, self).__init__()\n        assert set_name == 'train' or set_name == 'val' or set_name == 'test'\n        self.root = root\n        self.set_name = set_name\n        self.image_list = glob.glob(os.path.join(\n            root,\n            set_name,\n            args.images_folder,\n            \"*.tif\",\n        ))\n        self.label_list = glob.glob(os.path.join(\n            root,\n            set_name,\n            args.labels_folder,\n            \"*.tif\",\n        ))\n\n    def __getitem__(self, index):\n\n        images = Image.open(self.image_list[index])\n        masks = Image.open(self.label_list[index])\n        t_images = TF.to_tensor(images)\n        t_masks = TF.to_tensor(masks)\n        return t_images, t_masks\n\n    def __len__(self):\n\n        return len(self.image_list)\n<\/code>\nBut it will occur an error like this:\n\nRuntimeError: invalid argument 0: Sizes of tensors must match except in dimension 0. Got 3 and 1 in dimension 1 at \/pytorch\/aten\/src\/TH\/generic\/THTensorMath.cpp:3616\n\nI think it because the masks only have a single channel so I change the code:\n<code class=\"lang-auto\">        images = Image.open(self.image_list[index]).convert('RGB')\n        masks = Image.open(self.label_list[index]).convert('RGB')\n<\/code>\nThen it works, but I have to reshape the masks when I feed data and target to the network.\nSo I want to know is there any solution that can avoid change masks format when reading the dataset, I do not know whether it will have bad effects when I use operations above.\nThanks in advance ","y":"Oh, it is too stupid.\nIn my method transforms, I made mistakes mask = resize(image), so that in the DataLoader the dimension mismatch error will occur in the default_collate, it is not the DataLoader\u2019s problem.\nMy code as follows:\n<code class=\"lang-auto\">rgb_mean = (0.4353, 0.4452, 0.4131)\nrgb_std = (0.2044, 0.1924, 0.2013)\n\nclass MyDataset(Dataset):\n    def __init__(self,\n                 config,\n                 subset,\n                 data_transforms= None,\n                 target_transforms=None):\n        super(MyDataset, self).__init__()\n        assert subset == 'train' or subset == 'valid' or subset == 'test'\n        self.config = config\n        self.root = self.config.root_dir\n        self.subset = subset\n        self.data = self.config.data_folder_name\n        self.target = self.config.target_folder_name\n\n        self.data_transforms = data_transforms if data_transforms!=None else TF.to_tensor\n        self.target_transforms = target_transforms if target_transforms!= None else TF.to_tensor\n\n        self.mapping = {\n            0: 0,\n            255: 1,\n        }\n        self.data_list = glob.glob(os.path.join(\n            self.root,\n            subset,\n            self.data,\n            '*.tif'\n        ))\n        self.target_list = glob.glob(os.path.join(\n            self.root,\n            subset,\n            self.target,\n            '*.tif'\n        ))\n    def mask_to_class(self, mask):\n        for k in self.mapping:\n            mask[mask == k] = self.mapping[k]\n        return mask\n    def transfroms(self, image, mask):\n        resize = transforms.Resize(size=(self.config.input_size, self.config.input_size))\n        image = resize(image)\n        mask = resize(mask)\n        if ranom.random() > 0.5:\n            image = TF.hflip(image)\n            mask = TF.hflip(mask)\n        if random.random() > 0.5:\n            image = TF.vflip(image)\n            mask = TF.vflip(mask)\n        image = TF.to_tensor(image)\n        image = TF.normalize(image, mean=rgb_mean, std=rgb_std)\n        mask = torch.from_numpy(np.array(mask, dtype=np.uint8))\n        mask = self.mask_to_class(mask)\n        mask = mask.long()\n        return image, mask\n    def __getitem__(self, index):\n        datas = Image.open(self.data_list[index]).convert('RGB')\n        targets = Image.open(self.target_list[index]).convert('P')\n        t_datas, t_targets = self.transfroms(datas, targets)\n        return t_datas, t_targets\n    def __len__(self):\n        return len(self.data_list)\n<\/code>\nAnd there are something to pay attention:\n\nIf you want to change grayscale mask to class, the Image method from PIL doesn\u2019t work, we should read the masks in P or L\n\nTF.to_tensor() will normalize images or masks to [0,1], so if change masks to class and tensor, we should convert mask to tensor by torch.from_numpy() and thus we can change masks to class and make them Tensor. Don\u2019t foget convert the dtype to long.\n","z":"I don\u2019t think this problem is related to the DataLoader. Because in this class, you are not mixing the two tensors t_images and t_masks. I think the problem happens later on in your code. For a test, can you just get the size of tensors loaded from the data_loader:\n<code class=\"lang-python\">batch_x, batch_y = next(iter(data_loade))\nprint(batch_x.shape, batch_y.shape)\n<\/code>\nIf this works, then we can confirm that there is no problem with MyDataset class that you have defined.\nHi,  Thanks for your response.\nI have tried the code you offered as follows :\n<code class=\"lang-auto\">    dst = MyDataset(root = args.dataset_root_dir, set_name=\"train\")\n    #print(len(dst)) \n    data_loader = DataLoader(dataset=dst, batch_size=args.batch_size, shuffle=False)\n    batch_x, batch_y = next(iter(data_loader))\n    print(batch_x.shape, batch_y.shape)\n<\/code>\nAnd the shape of data and target like this:\n\ntorch.Size([10, 3, 512, 512])   torch.Size([10, 1, 512, 512])\n\nIt confirms that MyDataset has no problem.\nI found that  the dimension mismatch error occurs when I do TF.vflip() and TF.hflip() transforms, and I solve the problem by this:\n<code class=\"lang-auto\">def transform(self, image, mask):\n        # Resize\n        resize = transforms.Resize(size=(args.input_size, args.input_size))\n        image = resize(image)\n        mask = resize(mask)\n        # Random horizontal flipping\n        if random.random() > 0.5:\n            image = TF.hflip(image)\n            mask = TF.hflip(image)\n        # Random vertical flipping\n        if random.random() > 0.5:\n            image = TF.vflip(image)\n            mask = TF.vflip(mask)\n        mask = TF.to_grayscale(mask)\n        # Transform to Tensor\n        image = TF.to_tensor(image)\n        mask = TF.to_tensor(mask)\n        image = TF.normalize(image, mean=[0.4353, 0.4452, 0.4131],\n                                    std=[0.2044, 0.1924, 0.2013])\n        return image, mask\n<\/code>\nIt means I should read the images  and masks  in RGB and after transfroms the masks then convert to grayscale, and then I can get masks [batch_size, 1, height, width] in DataLoader. Is there better solutions?\nOh, I will go to re-edit the topic name. Thanks a lot \nOkay, so now we know that the MyDataset class is ok.\nBut now, why do you have this other transform() function? So have you removed the transformations in __getitem__, and instead you call his function inside __getitem__?\nAlso, youshouldn\u2019t have to read the masks in RGB. You can directly read them as Grayscale.\nOh yes, I removed the transformations in __getitem__, I defined a method transform in class MyDataset and call transform in __getitem__.\nThe code as follows:\n<code class=\"lang-auto\">class MyDataset(Dataset):\n    def __init__(self, root,set_name,):\n        super(MyDataset, self).__init__()\n        assert set_name == 'train' or set_name == 'val' or set_name == 'test'\n        self.root = root\n        self.set_name = set_name\n        self.mapping = {\n            0: 0,\n            255: 1\n        }\n        self.image_list = glob.glob(os.path.join(\n            root,\n            set_name,\n            args.images_folder,\n            \"*.tif\",\n        ))\n        self.label_list = glob.glob(os.path.join(\n            root,\n            set_name,\n            args.labels_folder,\n            \"*.tif\",\n        ))\n\n    def mask_to_class(self, mask):\n        for k in self.mapping:\n            mask[mask == k] = self.mapping[k]\n        return mask\n\n    def transform(self, image, mask):\n        # Resize\n        resize = transforms.Resize(size=(args.input_size, args.input_size))\n        image = resize(image)\n        mask = resize(mask)\n        # Random horizontal flipping\n        if random.random() > 0.5:\n            image = TF.hflip(image)\n            mask = TF.hflip(image)\n        # Random vertical flipping\n        if random.random() > 0.5:\n            image = TF.vflip(image)\n            mask = TF.vflip(mask)\n        # masks to gray_scale\n        mask = TF.to_grayscale(mask)\n        # Transform to Tensor\n        image = TF.to_tensor(image)\n        mask = TF.to_tensor(mask)\n        mask = self.mask_to_class(mask)\n        # Normalized only images? Yes\n        image = TF.normalize(image, mean=[0.4353, 0.4452, 0.4131],\n                             std=[0.2044, 0.1924, 0.2013])\n        return image, mask\n\n    def __getitem__(self, index):\n\n        images = Image.open(self.image_list[index]).convert('RGB')\n        masks = Image.open(self.label_list[index]).convert('RGB')\n        t_images, t_masks = self.transform(images, masks)\n        return t_images, t_masks\n\n    def __len__(self):\n\n        return len(self.image_list)\n<\/code>\nAnd test dataset and dataloader like this:\n<code class=\"lang-auto\">    dst = MyDataset(root = args.dataset_root_dir, set_name=\"train\")\n    #print(len(dst)) \n    data_loader = DataLoader(dataset=dst, batch_size=args.batch_size, shuffle=False)\n    for i, (data, target) in enumerate(data_loader):\n        print(i)\n        print(data.shape)\n        print(target.shape)\n        break\n<\/code>\n\ndata.shape is [batch_size, 3, height, width]\ntarget.shape is [batch_size, 1, height, width]\n\nBut when I read masks as Grayscale, when do a vertical and horizontal flip there will be a dimension mismatch error.\nHow can I read the images as RGB and masks as Grayscale directly without these so many conversions?\nThank you very much ! \nBut I am able to read a gray-scale image as is, without needing to convert that to RGB:\n<code class=\"lang-python\">>>> img = Image.open('img-gray.jpg')\n>>> img_hflip = TF.hflip(img)\n>>> img_tensor = TF.to_tensor(img_hflip)\n>>> img_tensor.shape\ntorch.Size([1, 218, 178])\n<\/code>\nI still believe this error is related to things outside the dataloader.\nOkay, you are right. It doesn\u2019t need convert to RGB to do some transforms, I removed convert('RGB') for images and masks, it still works, but when I removed\n<code class=\"lang-auto\">mask = TF.to_grayscale(mask)\n<\/code>\nin the transform method above. Trackback and error as follows:\n<code class=\"lang-auto\">Traceback (most recent call last):\n  File \"data_loader_test.py\", line 16, in <module>\n    for idx, (data, target) in enumerate(dataloader):\n  File \"\/home\/user\/.local\/lib\/python3.5\/site-packages\/torch\/utils\/data\/dataloader.py\", line 314, in __next__\n    batch = self.collate_fn([self.dataset[i] for i in indices])\n  File \"\/home\/user\/.local\/lib\/python3.5\/site-packages\/torch\/utils\/data\/dataloader.py\", line 187, in default_collate\n    return [default_collate(samples) for samples in transposed]\n  File \"\/home\/user\/.local\/lib\/python3.5\/site-packages\/torch\/utils\/data\/dataloader.py\", line 187, in <listcomp>\n    return [default_collate(samples) for samples in transposed]\n  File \"\/home\/user\/.local\/lib\/python3.5\/site-packages\/torch\/utils\/data\/dataloader.py\", line 164, in default_collate\n    return torch.stack(batch, 0, out=out)\nRuntimeError: invalid argument 0: Sizes of tensors must match except in dimension 0. Got 1 and 3 in dimension 1 at \/pytorch\/aten\/src\n\/TH\/generic\/THTensorMath.cpp:3616\n<\/code>\nEmm, masks shape as [1,512, 512], it seems that they are already grayscale? and don\u2019t need to convert to \u2018L\u2019 or \u2018P\u2019? And the function TF.to_grayscale doesn\u2019t make masks shape changed, so where the problems are \u2026\n\nOh, it is too stupid.\nIn my method transforms, I made mistakes mask = resize(image), so that in the DataLoader the dimension mismatch error will occur in the default_collate, it is not the DataLoader\u2019s problem.\nMy code as follows:\n<code class=\"lang-auto\">rgb_mean = (0.4353, 0.4452, 0.4131)\nrgb_std = (0.2044, 0.1924, 0.2013)\n\nclass MyDataset(Dataset):\n    def __init__(self,\n                 config,\n                 subset,\n                 data_transforms= None,\n                 target_transforms=None):\n        super(MyDataset, self).__init__()\n        assert subset == 'train' or subset == 'valid' or subset == 'test'\n        self.config = config\n        self.root = self.config.root_dir\n        self.subset = subset\n        self.data = self.config.data_folder_name\n        self.target = self.config.target_folder_name\n\n        self.data_transforms = data_transforms if data_transforms!=None else TF.to_tensor\n        self.target_transforms = target_transforms if target_transforms!= None else TF.to_tensor\n\n        self.mapping = {\n            0: 0,\n            255: 1,\n        }\n        self.data_list = glob.glob(os.path.join(\n            self.root,\n            subset,\n            self.data,\n            '*.tif'\n        ))\n        self.target_list = glob.glob(os.path.join(\n            self.root,\n            subset,\n            self.target,\n            '*.tif'\n        ))\n    def mask_to_class(self, mask):\n        for k in self.mapping:\n            mask[mask == k] = self.mapping[k]\n        return mask\n    def transfroms(self, image, mask):\n        resize = transforms.Resize(size=(self.config.input_size, self.config.input_size))\n        image = resize(image)\n        mask = resize(mask)\n        if ranom.random() > 0.5:\n            image = TF.hflip(image)\n            mask = TF.hflip(mask)\n        if random.random() > 0.5:\n            image = TF.vflip(image)\n            mask = TF.vflip(mask)\n        image = TF.to_tensor(image)\n        image = TF.normalize(image, mean=rgb_mean, std=rgb_std)\n        mask = torch.from_numpy(np.array(mask, dtype=np.uint8))\n        mask = self.mask_to_class(mask)\n        mask = mask.long()\n        return image, mask\n    def __getitem__(self, index):\n        datas = Image.open(self.data_list[index]).convert('RGB')\n        targets = Image.open(self.target_list[index]).convert('P')\n        t_datas, t_targets = self.transfroms(datas, targets)\n        return t_datas, t_targets\n    def __len__(self):\n        return len(self.data_list)\n<\/code>\nAnd there are something to pay attention:\n\nIf you want to change grayscale mask to class, the Image method from PIL doesn\u2019t work, we should read the masks in P or L\n\nTF.to_tensor() will normalize images or masks to [0,1], so if change masks to class and tensor, we should convert mask to tensor by torch.from_numpy() and thus we can change masks to class and make them Tensor. Don\u2019t foget convert the dtype to long.\n\nHi, I\u2019m trying to do the semantic segmentation too, could you show me your github?\nI\u2019m confused about the loss function\nOh, sorry. :\\\nI have not completed the entir project, and after testing, there are something wrong in it, train phase on GPU is too slow, and I will upload my project to github, thanks for your attention.  In addition, Im new in this domain \nI\u2019m new too. keep going\nThe same to you. "},{"x":"In the following code, how can I modify it such that I can iterate over x, y, and im_path for test images?\n<code class=\"lang-auto\">import torchvision.datasets as datasets\nclass MonaDataset(datasets.folder.ImageFolder):\n    def __init__(self, root, transform=None, target_transform=None,\n                 loader=datasets.folder.default_loader):\n        super(MonaDataset, self).__init__(root, transform, target_transform, loader)\n\n    def __getitem__(self, index):\n        path, target = self.samples[index]\n        sample = self.loader(path)\n        if self.transform is not None:\n            sample = self.transform(sample)\n        if self.target_transform is not None:\n            target = self.target_transform(target)\n        return sample, target, path\n\ndataset = MonaDataset('10folds\/10folds_9')\nprint(len(dataset))\nx, y, im_path = dataset[0]\n\n\nprint(\"x is: {}, y is: {}, im_path is: {}\".format(x, y, im_path))\n\nimage_datasets = {x: datasets.ImageFolder(os.path.join(data_dir, x),\n                                          data_transforms[x])\n                  for x in ['train', 'test']}\ndataloaders = {x: torch.utils.data.DataLoader(image_datasets[x], batch_size=4,\n                                             shuffle=True, num_workers=4)\n              for x in ['train', 'test']}\ndataset_sizes = {x: len(image_datasets[x]) for x in ['train', 'test']}\n\n\nclass_names = image_datasets['train'].classes\n<\/code>\nI need it for the following code snippet. When I am printing the class probabilities, I need to know which image do they belong to from the test folder.\n<code class=\"lang-auto\">nb_classes = 9\n\nimport torch.nn.functional as F\n\nconfusion_matrix = torch.zeros(nb_classes, nb_classes)\n\n_classes = []\n_preds = []\npredicted_labels = []\nloocv_probs = []\n\nwith torch.no_grad():\n    for i, (inputs, classes) in enumerate(dataloaders['test']):\n    #for i, (inputs, classes, im_path) in enumerate(dataset):\n        \n      \n        inputs = inputs.to(device)\n        tmp_labels = model_ft(inputs)\n        \n        classes = classes.to(device)\n        classes_list = classes.cpu().detach().numpy().tolist()\n        _classes[:]=[i+1 for i in classes_list]\n        outputs = model_ft(inputs)\n        \n        gpu_tensor_probs = F.softmax(outputs, 1)\n        cpu_numpy_probs = gpu_tensor_probs.data.cpu().numpy()\n        loocv_probs.append(cpu_numpy_probs.tolist())\n    \n        _, preds = torch.max(outputs, 1)\n        preds_list = preds.cpu().detach().numpy().tolist()\n        _preds[:]=[i+1 for i in preds_list]\n          \n        predicted_labels.append(preds.cpu().detach().numpy().tolist())\n        for t, p in zip(classes.view(-1), preds.view(-1)):\n                confusion_matrix[t.long(), p.long()] += 1\n                \nprint(confusion_matrix)\nprint(confusion_matrix.diag()\/confusion_matrix.sum(1))\n#print('Class probabilities:', loocv_probs)\n\nfor i in range(len(loocv_probs)): #21\n    for j in range(len(loocv_probs[0])): #4\n        print(*[f\"{element:.2f}\" for element in loocv_probs[i][j]], sep=', ', end='\\n')\n\n\nfor i in range(9):\n    print(\"class {:d} --> accuracy: {:.2f}, correct predictions: {:d}, all: {:d}\".format(i+1, (confusion_matrix.diag()\/confusion_matrix.sum(1))[i]*100, int(confusion_matrix[i][i].numpy()), int(confusion_matrix.sum(dim=1)[i].numpy())))\n<\/code>","y":"Thanks a lot to  for helping with most parts of the following code and guidance:\n<code class=\"lang-auto\">import torchvision.datasets as datasets\nclass MonaDataset(datasets.folder.ImageFolder):\n    def __init__(self, root, transform=None, target_transform=None,\n                 loader=datasets.folder.default_loader):\n        super(MonaDataset, self).__init__(root, transform, target_transform, loader)\n\n    def __getitem__(self, index):\n        path, target = self.samples[index]\n        sample = self.loader(path)\n        if self.transform is not None:\n            sample = self.transform(sample)\n        if self.target_transform is not None:\n            target = self.target_transform(target)\n        return sample, target, path\n\ndataset = MonaDataset('10folds\/10fold_9')\nprint(len(dataset))\nx, y, im_path = dataset[0]\n\n\nprint(\"x is: {}, y is: {}, im_path is: {}\".format(x, y, im_path))\n\nimage_datasets = {x: MonaDataset(os.path.join(data_dir, x),\n                                          data_transforms[x])\n                  for x in ['train', 'test']}\ndataloaders = {x: torch.utils.data.DataLoader(image_datasets[x], batch_size=4,\n                                             shuffle=True, num_workers=4)\n              for x in ['train', 'test']}\ndataset_sizes = {x: len(image_datasets[x]) for x in ['train', 'test']}\n\n\nclass_names = image_datasets['train'].classes\n<\/code>\nand\n<code class=\"lang-auto\">import ntpath\n\nnb_classes = 9\n\nimport torch.nn.functional as F\n\nconfusion_matrix = torch.zeros(nb_classes, nb_classes)\n\n_classes = []\n_preds = []\npredicted_labels = []\n\nclass_probs = torch.Tensor()\n\nim_paths = []\nwith torch.no_grad():\n    for i, (inputs, classes, im_path) in enumerate(dataloaders['test']):\n       \n\n        im_paths.append(im_path)\n        inputs = inputs.to(device)\n        tmp_labels = model_ft(inputs)\n        \n        classes = classes.to(device)\n        classes_list = classes.cpu().detach().numpy().tolist()\n        _classes[:]=[i+1 for i in classes_list]\n        outputs = model_ft(inputs)\n        \n  \n\n        class_probs = class_probs.cuda()\n        \n        class_probs = torch.cat((class_probs, F.softmax(outputs, 1)))\n\n\n    \n        _, preds = torch.max(outputs, 1)\n        preds_list = preds.cpu().detach().numpy().tolist()\n        _preds[:]=[i+1 for i in preds_list]\n          \n        predicted_labels.append(preds.cpu().detach().numpy().tolist())\n        for t, p in zip(classes.view(-1), preds.view(-1)):\n                confusion_matrix[t.long(), p.long()] += 1\n                \nprint(confusion_matrix)\nprint(confusion_matrix.diag()\/confusion_matrix.sum(1))\n\n\n\nflattened_im_paths = flattened = [item for sublist in im_paths for item in sublist]\n\nfor i in range(len(flattened_im_paths)):\n    print('img {}, class_prob is: {}'.format(ntpath.basename(flattened_im_paths[i]), class_probs[i]))\n\n\nfor i in range(nb_classes):\n    print(\"class {:d} --> accuracy: {:.2f}, correct predictions: {:d}, all: {:d}\".format(i+1, (confusion_matrix.diag()\/confusion_matrix.sum(1))[i]*100, int(confusion_matrix[i][i].numpy()), int(confusion_matrix.sum(dim=1)[i].numpy())))\n<\/code>\nYou get something like:\n<code class=\"lang-auto\">tensor([[ 0.,  0.,  0.,  0.,  0.,  1.,  1.,  0.,  0.],\n        [ 0.,  4.,  3.,  0.,  2.,  0.,  3.,  0.,  1.],\n        [ 0.,  1., 22.,  0.,  1.,  0.,  1.,  0.,  1.],\n        [ 0.,  1.,  1.,  0.,  1.,  0.,  1.,  0.,  0.],\n        [ 0.,  2.,  5.,  0.,  3.,  0.,  0.,  0.,  0.],\n        [ 0.,  1.,  1.,  0.,  0.,  2.,  1.,  0.,  0.],\n        [ 0.,  1.,  3.,  0.,  0.,  0., 11.,  0.,  0.],\n        [ 0.,  0.,  0.,  0.,  1.,  0.,  1.,  0.,  0.],\n        [ 0.,  3.,  1.,  0.,  0.,  0.,  1.,  0.,  1.]])\ntensor([0.0000, 0.3077, 0.8462, 0.0000, 0.3000, 0.4000, 0.7333, 0.0000, 0.1667])\nimg 352.jpg, class_prob is: tensor([0.0021, 0.8629, 0.0088, 0.0019, 0.0700, 0.0005, 0.0299, 0.0043, 0.0196],\n       device='cuda:0')\nimg 90258.jpg, class_prob is: tensor([0.0253, 0.0341, 0.0541, 0.1367, 0.2127, 0.2026, 0.1849, 0.1008, 0.0488],\n       device='cuda:0')\nimg 183.jpg, class_prob is: tensor([0.0172, 0.0292, 0.7435, 0.0120, 0.0217, 0.0128, 0.1457, 0.0098, 0.0080],\n       device='cuda:0')\nimg 100342.jpg, class_prob is: tensor([0.0309, 0.0268, 0.7362, 0.0402, 0.0391, 0.0463, 0.0256, 0.0194, 0.0356],\n       device='cuda:0')\nimg 10149.jpg, class_prob is: tensor([0.0515, 0.0811, 0.0959, 0.1061, 0.1484, 0.0629, 0.2921, 0.0598, 0.1021],\n       device='cuda:0')\nimg 100083.jpg, class_prob is: tensor([0.0032, 0.0497, 0.0892, 0.0604, 0.0291, 0.7322, 0.0188, 0.0114, 0.0059],\n       device='cuda:0')\nimg 10274.jpg, class_prob is: tensor([0.0436, 0.1276, 0.3260, 0.0033, 0.1529, 0.0061, 0.0842, 0.0191, 0.2371],\n       device='cuda:0')\nimg 10659.jpg, class_prob is: tensor([0.0133, 0.2016, 0.3928, 0.0270, 0.0962, 0.0011, 0.2383, 0.0228, 0.0070],\n       device='cuda:0')\nimg 100044.jpg, class_prob is: tensor([0.0149, 0.0259, 0.9173, 0.0053, 0.0095, 0.0070, 0.0062, 0.0087, 0.0051],\n       device='cuda:0')\nimg 10316.jpg, class_prob is: tensor([0.0033, 0.3977, 0.1576, 0.0450, 0.1999, 0.0621, 0.0940, 0.0207, 0.0197],\n       device='cuda:0')\nimg 10590.jpg, class_prob is: tensor([0.0228, 0.0571, 0.0350, 0.0687, 0.1637, 0.0527, 0.5209, 0.0453, 0.0339],\n       device='cuda:0')\nimg 10132.jpg, class_prob is: tensor([0.0686, 0.1016, 0.1488, 0.0164, 0.1288, 0.0062, 0.2033, 0.0319, 0.2945],\n       device='cuda:0')\nimg 19.jpg, class_prob is: tensor([0.0327, 0.1787, 0.6138, 0.0145, 0.0874, 0.0198, 0.0199, 0.0140, 0.0191],\n       device='cuda:0')\nimg 399.jpg, class_prob is: tensor([0.0379, 0.0256, 0.0970, 0.0300, 0.2091, 0.0182, 0.4842, 0.0303, 0.0676],\n       device='cuda:0')\nimg 10608.jpg, class_prob is: tensor([0.0052, 0.0664, 0.2319, 0.0453, 0.1924, 0.0128, 0.3560, 0.0528, 0.0372],\n       device='cuda:0')\nimg 90054.jpg, class_prob is: tensor([0.0274, 0.5300, 0.1293, 0.0415, 0.0300, 0.1012, 0.0657, 0.0305, 0.0444],\n       device='cuda:0')\nimg 45.jpg, class_prob is: tensor([0.0131, 0.1924, 0.4044, 0.0258, 0.0733, 0.1507, 0.0925, 0.0127, 0.0352],\n       device='cuda:0')\nimg 387.jpg, class_prob is: tensor([0.0391, 0.1142, 0.0487, 0.0249, 0.1007, 0.0144, 0.5883, 0.0420, 0.0277],\n       device='cuda:0')\nimg 372.jpg, class_prob is: tensor([0.0664, 0.1080, 0.1106, 0.0049, 0.0611, 0.0033, 0.0603, 0.0159, 0.5694],\n       device='cuda:0')\nimg 100375.jpg, class_prob is: tensor([0.0034, 0.0530, 0.5269, 0.1435, 0.1574, 0.0389, 0.0343, 0.0401, 0.0025],\n       device='cuda:0')\nimg 10157.jpg, class_prob is: tensor([0.0052, 0.0323, 0.0511, 0.1004, 0.4424, 0.0094, 0.3234, 0.0167, 0.0190],\n       device='cuda:0')\nimg 10318.jpg, class_prob is: tensor([0.0641, 0.1038, 0.6826, 0.0068, 0.0093, 0.0084, 0.0210, 0.0150, 0.0890],\n       device='cuda:0')\nimg 100103.jpg, class_prob is: tensor([0.0062, 0.3813, 0.0362, 0.0286, 0.0615, 0.2253, 0.1815, 0.0474, 0.0319],\n       device='cuda:0')\nimg 10022.jpg, class_prob is: tensor([0.0326, 0.0547, 0.6418, 0.0130, 0.1654, 0.0071, 0.0519, 0.0166, 0.0170],\n       device='cuda:0')\nimg 10053.jpg, class_prob is: tensor([0.0083, 0.3553, 0.0077, 0.0558, 0.0891, 0.0594, 0.3704, 0.0235, 0.0305],\n       device='cuda:0')\nimg 10697.jpg, class_prob is: tensor([0.0203, 0.0668, 0.6432, 0.0108, 0.1396, 0.0080, 0.0292, 0.0236, 0.0586],\n       device='cuda:0')\nimg 10001.jpg, class_prob is: tensor([0.0400, 0.0805, 0.1437, 0.1071, 0.1454, 0.0374, 0.3614, 0.0459, 0.0387],\n       device='cuda:0')\nimg 10509.jpg, class_prob is: tensor([0.0083, 0.0296, 0.9042, 0.0037, 0.0149, 0.0076, 0.0152, 0.0071, 0.0093],\n       device='cuda:0')\nimg 100153.jpg, class_prob is: tensor([0.1175, 0.1032, 0.6336, 0.0068, 0.0617, 0.0087, 0.0295, 0.0194, 0.0195],\n       device='cuda:0')\nimg 100280.jpg, class_prob is: tensor([0.0079, 0.5634, 0.1119, 0.0226, 0.0699, 0.0133, 0.1406, 0.0187, 0.0518],\n       device='cuda:0')\nimg 194.jpg, class_prob is: tensor([0.0156, 0.0336, 0.1321, 0.0290, 0.4090, 0.0443, 0.1912, 0.0355, 0.1096],\n       device='cuda:0')\nimg 100088.jpg, class_prob is: tensor([0.0150, 0.1104, 0.2590, 0.1620, 0.0683, 0.0826, 0.2287, 0.0478, 0.0262],\n       device='cuda:0')\nimg 76.jpg, class_prob is: tensor([0.0219, 0.0049, 0.0389, 0.1124, 0.1977, 0.0165, 0.5528, 0.0421, 0.0127],\n       device='cuda:0')\nimg 10577.jpg, class_prob is: tensor([0.0219, 0.1587, 0.6273, 0.0056, 0.0379, 0.0022, 0.0369, 0.0511, 0.0584],\n       device='cuda:0')\nimg 10017.jpg, class_prob is: tensor([0.0438, 0.3121, 0.2318, 0.0161, 0.1514, 0.0423, 0.1211, 0.0112, 0.0701],\n       device='cuda:0')\nimg 10479.jpg, class_prob is: tensor([0.0058, 0.4157, 0.2131, 0.0400, 0.0546, 0.1839, 0.0434, 0.0145, 0.0292],\n       device='cuda:0')\nimg 10604.jpg, class_prob is: tensor([0.0056, 0.0993, 0.4794, 0.0245, 0.0576, 0.0898, 0.1823, 0.0163, 0.0453],\n       device='cuda:0')\nimg 10285.jpg, class_prob is: tensor([0.0112, 0.3222, 0.4843, 0.0180, 0.0692, 0.0371, 0.0162, 0.0214, 0.0204],\n       device='cuda:0')\nimg 90202.jpg, class_prob is: tensor([0.1037, 0.0657, 0.1044, 0.0224, 0.0853, 0.0015, 0.5637, 0.0370, 0.0163],\n       device='cuda:0')\nimg 10160.jpg, class_prob is: tensor([0.0244, 0.1033, 0.0709, 0.0644, 0.3132, 0.0861, 0.1217, 0.0413, 0.1747],\n       device='cuda:0')\nimg 100198.jpg, class_prob is: tensor([0.0153, 0.2387, 0.2866, 0.0096, 0.0584, 0.0060, 0.2811, 0.0601, 0.0442],\n       device='cuda:0')\nimg 100034.jpg, class_prob is: tensor([0.0070, 0.0240, 0.7375, 0.0332, 0.1346, 0.0126, 0.0248, 0.0218, 0.0044],\n       device='cuda:0')\nimg 10331.jpg, class_prob is: tensor([0.1055, 0.0490, 0.1435, 0.0637, 0.1599, 0.0504, 0.3353, 0.0253, 0.0675],\n       device='cuda:0')\nimg 10214.jpg, class_prob is: tensor([0.0126, 0.5641, 0.0477, 0.0295, 0.0540, 0.0945, 0.0506, 0.0165, 0.1305],\n       device='cuda:0')\nimg 240.jpg, class_prob is: tensor([0.0088, 0.4677, 0.0158, 0.0085, 0.0853, 0.0041, 0.0192, 0.0207, 0.3699],\n       device='cuda:0')\nimg 10410.jpg, class_prob is: tensor([0.0368, 0.0628, 0.3392, 0.0753, 0.1717, 0.1086, 0.1817, 0.0195, 0.0044],\n       device='cuda:0')\nimg 10236.jpg, class_prob is: tensor([0.0050, 0.0423, 0.9172, 0.0018, 0.0114, 0.0092, 0.0022, 0.0043, 0.0067],\n       device='cuda:0')\nimg 42.jpg, class_prob is: tensor([0.0046, 0.0063, 0.0165, 0.0250, 0.0251, 0.0035, 0.8946, 0.0142, 0.0102],\n       device='cuda:0')\nimg 90007.jpg, class_prob is: tensor([0.0287, 0.0517, 0.7361, 0.0202, 0.0359, 0.0685, 0.0261, 0.0238, 0.0089],\n       device='cuda:0')\nimg 100373.jpg, class_prob is: tensor([0.0079, 0.0319, 0.5325, 0.0326, 0.2785, 0.0101, 0.0664, 0.0291, 0.0109],\n       device='cuda:0')\nimg 100146.jpg, class_prob is: tensor([0.0297, 0.3012, 0.0568, 0.0731, 0.1679, 0.0060, 0.1998, 0.0690, 0.0966],\n       device='cuda:0')\nimg 10543.jpg, class_prob is: tensor([0.0215, 0.2594, 0.0614, 0.0112, 0.0428, 0.0802, 0.3085, 0.0083, 0.2068],\n       device='cuda:0')\nimg 64.jpg, class_prob is: tensor([0.0365, 0.0918, 0.1058, 0.0447, 0.2888, 0.0066, 0.3342, 0.0651, 0.0265],\n       device='cuda:0')\nimg 100080.jpg, class_prob is: tensor([0.0074, 0.0326, 0.9456, 0.0017, 0.0026, 0.0025, 0.0031, 0.0026, 0.0019],\n       device='cuda:0')\nimg 10388.jpg, class_prob is: tensor([0.0029, 0.0464, 0.0259, 0.0762, 0.1816, 0.6206, 0.0306, 0.0087, 0.0072],\n       device='cuda:0')\nimg 10238.jpg, class_prob is: tensor([0.0139, 0.0817, 0.0415, 0.0073, 0.0454, 0.0025, 0.3419, 0.0251, 0.4407],\n       device='cuda:0')\nimg 10085.jpg, class_prob is: tensor([0.0110, 0.0998, 0.8299, 0.0025, 0.0164, 0.0122, 0.0017, 0.0068, 0.0197],\n       device='cuda:0')\nimg 10392.jpg, class_prob is: tensor([0.0074, 0.0144, 0.0278, 0.0164, 0.0157, 0.0028, 0.8931, 0.0142, 0.0081],\n       device='cuda:0')\nimg 221.jpg, class_prob is: tensor([0.0228, 0.0883, 0.0344, 0.0408, 0.2939, 0.0865, 0.1654, 0.0312, 0.2367],\n       device='cuda:0')\nimg 100222.jpg, class_prob is: tensor([0.0131, 0.1702, 0.4185, 0.0532, 0.1874, 0.0181, 0.1034, 0.0271, 0.0089],\n       device='cuda:0')\nimg 10484.jpg, class_prob is: tensor([0.0190, 0.2676, 0.0652, 0.0931, 0.2483, 0.0554, 0.1531, 0.0608, 0.0376],\n       device='cuda:0')\nimg 100263.jpg, class_prob is: tensor([0.0513, 0.1218, 0.3362, 0.0292, 0.1180, 0.0055, 0.2519, 0.0687, 0.0175],\n       device='cuda:0')\nimg 100379.jpg, class_prob is: tensor([0.0060, 0.1531, 0.0784, 0.0539, 0.0652, 0.4153, 0.1938, 0.0075, 0.0268],\n       device='cuda:0')\nimg 10058.jpg, class_prob is: tensor([0.0217, 0.0225, 0.8157, 0.0028, 0.0259, 0.0020, 0.0130, 0.0110, 0.0854],\n       device='cuda:0')\nimg 44.jpg, class_prob is: tensor([0.0304, 0.3533, 0.1229, 0.0171, 0.0375, 0.1888, 0.0284, 0.0253, 0.1963],\n       device='cuda:0')\nimg 356.jpg, class_prob is: tensor([0.0118, 0.0481, 0.7284, 0.0614, 0.0839, 0.0207, 0.0144, 0.0287, 0.0026],\n       device='cuda:0')\nimg 10184.jpg, class_prob is: tensor([0.0180, 0.0630, 0.0571, 0.0114, 0.0653, 0.0024, 0.7470, 0.0141, 0.0217],\n       device='cuda:0')\nimg 90038.jpg, class_prob is: tensor([0.0130, 0.0851, 0.1648, 0.0328, 0.2578, 0.0216, 0.2993, 0.0263, 0.0994],\n       device='cuda:0')\nimg 100293.jpg, class_prob is: tensor([0.0163, 0.1146, 0.3101, 0.0808, 0.1430, 0.0223, 0.2678, 0.0280, 0.0172],\n       device='cuda:0')\nimg 164.jpg, class_prob is: tensor([0.0387, 0.2200, 0.0386, 0.0152, 0.0999, 0.0106, 0.4888, 0.0199, 0.0684],\n       device='cuda:0')\nimg 10213.jpg, class_prob is: tensor([0.0772, 0.1158, 0.5259, 0.0054, 0.0325, 0.0158, 0.0499, 0.0247, 0.1527],\n       device='cuda:0')\nimg 10596.jpg, class_prob is: tensor([0.0039, 0.0704, 0.3960, 0.1109, 0.2341, 0.1079, 0.0249, 0.0409, 0.0109],\n       device='cuda:0')\nimg 90097.jpg, class_prob is: tensor([0.0285, 0.1540, 0.0092, 0.0161, 0.1429, 0.0141, 0.5176, 0.0336, 0.0839],\n       device='cuda:0')\nimg 10701.jpg, class_prob is: tensor([0.0345, 0.0778, 0.8156, 0.0084, 0.0159, 0.0194, 0.0072, 0.0050, 0.0160],\n       device='cuda:0')\nimg 332.jpg, class_prob is: tensor([0.0041, 0.0296, 0.7350, 0.0356, 0.0441, 0.0344, 0.0867, 0.0198, 0.0108],\n       device='cuda:0')\nimg 358.jpg, class_prob is: tensor([0.0130, 0.1856, 0.1176, 0.0475, 0.3586, 0.0123, 0.1686, 0.0506, 0.0462],\n       device='cuda:0')\nimg 2.jpg, class_prob is: tensor([0.0856, 0.1166, 0.0873, 0.1253, 0.1327, 0.1465, 0.2541, 0.0437, 0.0083],\n       device='cuda:0')\nimg 10321.jpg, class_prob is: tensor([0.0080, 0.0503, 0.8827, 0.0031, 0.0210, 0.0095, 0.0060, 0.0038, 0.0156],\n       device='cuda:0')\nimg 100366.jpg, class_prob is: tensor([0.0052, 0.1807, 0.1158, 0.0151, 0.1243, 0.0334, 0.3535, 0.0323, 0.1396],\n       device='cuda:0')\nimg 10389.jpg, class_prob is: tensor([0.0405, 0.1310, 0.1676, 0.0828, 0.2095, 0.0051, 0.1929, 0.0779, 0.0927],\n       device='cuda:0')\nimg 10578.jpg, class_prob is: tensor([0.0149, 0.0446, 0.8703, 0.0042, 0.0121, 0.0011, 0.0149, 0.0032, 0.0348],\n       device='cuda:0')\nimg 100242.jpg, class_prob is: tensor([0.0066, 0.2517, 0.0405, 0.0711, 0.1588, 0.1981, 0.2039, 0.0347, 0.0347],\n       device='cuda:0')\nimg 10060.jpg, class_prob is: tensor([0.0459, 0.0720, 0.1534, 0.0360, 0.2780, 0.0389, 0.2456, 0.0995, 0.0308],\n       device='cuda:0')\nclass 1 --> accuracy: 0.00, correct predictions: 0, all: 2\nclass 2 --> accuracy: 30.77, correct predictions: 4, all: 13\nclass 3 --> accuracy: 84.62, correct predictions: 22, all: 26\nclass 4 --> accuracy: 0.00, correct predictions: 0, all: 4\nclass 5 --> accuracy: 30.00, correct predictions: 3, all: 10\nclass 6 --> accuracy: 40.00, correct predictions: 2, all: 5\nclass 7 --> accuracy: 73.33, correct predictions: 11, all: 15\nclass 8 --> accuracy: 0.00, correct predictions: 0, all: 2\nclass 9 --> accuracy: 16.67, correct predictions: 1, all: 6\n<\/code>","z":"Thanks a lot to  for helping with most parts of the following code and guidance:\n<code class=\"lang-auto\">import torchvision.datasets as datasets\nclass MonaDataset(datasets.folder.ImageFolder):\n    def __init__(self, root, transform=None, target_transform=None,\n                 loader=datasets.folder.default_loader):\n        super(MonaDataset, self).__init__(root, transform, target_transform, loader)\n\n    def __getitem__(self, index):\n        path, target = self.samples[index]\n        sample = self.loader(path)\n        if self.transform is not None:\n            sample = self.transform(sample)\n        if self.target_transform is not None:\n            target = self.target_transform(target)\n        return sample, target, path\n\ndataset = MonaDataset('10folds\/10fold_9')\nprint(len(dataset))\nx, y, im_path = dataset[0]\n\n\nprint(\"x is: {}, y is: {}, im_path is: {}\".format(x, y, im_path))\n\nimage_datasets = {x: MonaDataset(os.path.join(data_dir, x),\n                                          data_transforms[x])\n                  for x in ['train', 'test']}\ndataloaders = {x: torch.utils.data.DataLoader(image_datasets[x], batch_size=4,\n                                             shuffle=True, num_workers=4)\n              for x in ['train', 'test']}\ndataset_sizes = {x: len(image_datasets[x]) for x in ['train', 'test']}\n\n\nclass_names = image_datasets['train'].classes\n<\/code>\nand\n<code class=\"lang-auto\">import ntpath\n\nnb_classes = 9\n\nimport torch.nn.functional as F\n\nconfusion_matrix = torch.zeros(nb_classes, nb_classes)\n\n_classes = []\n_preds = []\npredicted_labels = []\n\nclass_probs = torch.Tensor()\n\nim_paths = []\nwith torch.no_grad():\n    for i, (inputs, classes, im_path) in enumerate(dataloaders['test']):\n       \n\n        im_paths.append(im_path)\n        inputs = inputs.to(device)\n        tmp_labels = model_ft(inputs)\n        \n        classes = classes.to(device)\n        classes_list = classes.cpu().detach().numpy().tolist()\n        _classes[:]=[i+1 for i in classes_list]\n        outputs = model_ft(inputs)\n        \n  \n\n        class_probs = class_probs.cuda()\n        \n        class_probs = torch.cat((class_probs, F.softmax(outputs, 1)))\n\n\n    \n        _, preds = torch.max(outputs, 1)\n        preds_list = preds.cpu().detach().numpy().tolist()\n        _preds[:]=[i+1 for i in preds_list]\n          \n        predicted_labels.append(preds.cpu().detach().numpy().tolist())\n        for t, p in zip(classes.view(-1), preds.view(-1)):\n                confusion_matrix[t.long(), p.long()] += 1\n                \nprint(confusion_matrix)\nprint(confusion_matrix.diag()\/confusion_matrix.sum(1))\n\n\n\nflattened_im_paths = flattened = [item for sublist in im_paths for item in sublist]\n\nfor i in range(len(flattened_im_paths)):\n    print('img {}, class_prob is: {}'.format(ntpath.basename(flattened_im_paths[i]), class_probs[i]))\n\n\nfor i in range(nb_classes):\n    print(\"class {:d} --> accuracy: {:.2f}, correct predictions: {:d}, all: {:d}\".format(i+1, (confusion_matrix.diag()\/confusion_matrix.sum(1))[i]*100, int(confusion_matrix[i][i].numpy()), int(confusion_matrix.sum(dim=1)[i].numpy())))\n<\/code>\nYou get something like:\n<code class=\"lang-auto\">tensor([[ 0.,  0.,  0.,  0.,  0.,  1.,  1.,  0.,  0.],\n        [ 0.,  4.,  3.,  0.,  2.,  0.,  3.,  0.,  1.],\n        [ 0.,  1., 22.,  0.,  1.,  0.,  1.,  0.,  1.],\n        [ 0.,  1.,  1.,  0.,  1.,  0.,  1.,  0.,  0.],\n        [ 0.,  2.,  5.,  0.,  3.,  0.,  0.,  0.,  0.],\n        [ 0.,  1.,  1.,  0.,  0.,  2.,  1.,  0.,  0.],\n        [ 0.,  1.,  3.,  0.,  0.,  0., 11.,  0.,  0.],\n        [ 0.,  0.,  0.,  0.,  1.,  0.,  1.,  0.,  0.],\n        [ 0.,  3.,  1.,  0.,  0.,  0.,  1.,  0.,  1.]])\ntensor([0.0000, 0.3077, 0.8462, 0.0000, 0.3000, 0.4000, 0.7333, 0.0000, 0.1667])\nimg 352.jpg, class_prob is: tensor([0.0021, 0.8629, 0.0088, 0.0019, 0.0700, 0.0005, 0.0299, 0.0043, 0.0196],\n       device='cuda:0')\nimg 90258.jpg, class_prob is: tensor([0.0253, 0.0341, 0.0541, 0.1367, 0.2127, 0.2026, 0.1849, 0.1008, 0.0488],\n       device='cuda:0')\nimg 183.jpg, class_prob is: tensor([0.0172, 0.0292, 0.7435, 0.0120, 0.0217, 0.0128, 0.1457, 0.0098, 0.0080],\n       device='cuda:0')\nimg 100342.jpg, class_prob is: tensor([0.0309, 0.0268, 0.7362, 0.0402, 0.0391, 0.0463, 0.0256, 0.0194, 0.0356],\n       device='cuda:0')\nimg 10149.jpg, class_prob is: tensor([0.0515, 0.0811, 0.0959, 0.1061, 0.1484, 0.0629, 0.2921, 0.0598, 0.1021],\n       device='cuda:0')\nimg 100083.jpg, class_prob is: tensor([0.0032, 0.0497, 0.0892, 0.0604, 0.0291, 0.7322, 0.0188, 0.0114, 0.0059],\n       device='cuda:0')\nimg 10274.jpg, class_prob is: tensor([0.0436, 0.1276, 0.3260, 0.0033, 0.1529, 0.0061, 0.0842, 0.0191, 0.2371],\n       device='cuda:0')\nimg 10659.jpg, class_prob is: tensor([0.0133, 0.2016, 0.3928, 0.0270, 0.0962, 0.0011, 0.2383, 0.0228, 0.0070],\n       device='cuda:0')\nimg 100044.jpg, class_prob is: tensor([0.0149, 0.0259, 0.9173, 0.0053, 0.0095, 0.0070, 0.0062, 0.0087, 0.0051],\n       device='cuda:0')\nimg 10316.jpg, class_prob is: tensor([0.0033, 0.3977, 0.1576, 0.0450, 0.1999, 0.0621, 0.0940, 0.0207, 0.0197],\n       device='cuda:0')\nimg 10590.jpg, class_prob is: tensor([0.0228, 0.0571, 0.0350, 0.0687, 0.1637, 0.0527, 0.5209, 0.0453, 0.0339],\n       device='cuda:0')\nimg 10132.jpg, class_prob is: tensor([0.0686, 0.1016, 0.1488, 0.0164, 0.1288, 0.0062, 0.2033, 0.0319, 0.2945],\n       device='cuda:0')\nimg 19.jpg, class_prob is: tensor([0.0327, 0.1787, 0.6138, 0.0145, 0.0874, 0.0198, 0.0199, 0.0140, 0.0191],\n       device='cuda:0')\nimg 399.jpg, class_prob is: tensor([0.0379, 0.0256, 0.0970, 0.0300, 0.2091, 0.0182, 0.4842, 0.0303, 0.0676],\n       device='cuda:0')\nimg 10608.jpg, class_prob is: tensor([0.0052, 0.0664, 0.2319, 0.0453, 0.1924, 0.0128, 0.3560, 0.0528, 0.0372],\n       device='cuda:0')\nimg 90054.jpg, class_prob is: tensor([0.0274, 0.5300, 0.1293, 0.0415, 0.0300, 0.1012, 0.0657, 0.0305, 0.0444],\n       device='cuda:0')\nimg 45.jpg, class_prob is: tensor([0.0131, 0.1924, 0.4044, 0.0258, 0.0733, 0.1507, 0.0925, 0.0127, 0.0352],\n       device='cuda:0')\nimg 387.jpg, class_prob is: tensor([0.0391, 0.1142, 0.0487, 0.0249, 0.1007, 0.0144, 0.5883, 0.0420, 0.0277],\n       device='cuda:0')\nimg 372.jpg, class_prob is: tensor([0.0664, 0.1080, 0.1106, 0.0049, 0.0611, 0.0033, 0.0603, 0.0159, 0.5694],\n       device='cuda:0')\nimg 100375.jpg, class_prob is: tensor([0.0034, 0.0530, 0.5269, 0.1435, 0.1574, 0.0389, 0.0343, 0.0401, 0.0025],\n       device='cuda:0')\nimg 10157.jpg, class_prob is: tensor([0.0052, 0.0323, 0.0511, 0.1004, 0.4424, 0.0094, 0.3234, 0.0167, 0.0190],\n       device='cuda:0')\nimg 10318.jpg, class_prob is: tensor([0.0641, 0.1038, 0.6826, 0.0068, 0.0093, 0.0084, 0.0210, 0.0150, 0.0890],\n       device='cuda:0')\nimg 100103.jpg, class_prob is: tensor([0.0062, 0.3813, 0.0362, 0.0286, 0.0615, 0.2253, 0.1815, 0.0474, 0.0319],\n       device='cuda:0')\nimg 10022.jpg, class_prob is: tensor([0.0326, 0.0547, 0.6418, 0.0130, 0.1654, 0.0071, 0.0519, 0.0166, 0.0170],\n       device='cuda:0')\nimg 10053.jpg, class_prob is: tensor([0.0083, 0.3553, 0.0077, 0.0558, 0.0891, 0.0594, 0.3704, 0.0235, 0.0305],\n       device='cuda:0')\nimg 10697.jpg, class_prob is: tensor([0.0203, 0.0668, 0.6432, 0.0108, 0.1396, 0.0080, 0.0292, 0.0236, 0.0586],\n       device='cuda:0')\nimg 10001.jpg, class_prob is: tensor([0.0400, 0.0805, 0.1437, 0.1071, 0.1454, 0.0374, 0.3614, 0.0459, 0.0387],\n       device='cuda:0')\nimg 10509.jpg, class_prob is: tensor([0.0083, 0.0296, 0.9042, 0.0037, 0.0149, 0.0076, 0.0152, 0.0071, 0.0093],\n       device='cuda:0')\nimg 100153.jpg, class_prob is: tensor([0.1175, 0.1032, 0.6336, 0.0068, 0.0617, 0.0087, 0.0295, 0.0194, 0.0195],\n       device='cuda:0')\nimg 100280.jpg, class_prob is: tensor([0.0079, 0.5634, 0.1119, 0.0226, 0.0699, 0.0133, 0.1406, 0.0187, 0.0518],\n       device='cuda:0')\nimg 194.jpg, class_prob is: tensor([0.0156, 0.0336, 0.1321, 0.0290, 0.4090, 0.0443, 0.1912, 0.0355, 0.1096],\n       device='cuda:0')\nimg 100088.jpg, class_prob is: tensor([0.0150, 0.1104, 0.2590, 0.1620, 0.0683, 0.0826, 0.2287, 0.0478, 0.0262],\n       device='cuda:0')\nimg 76.jpg, class_prob is: tensor([0.0219, 0.0049, 0.0389, 0.1124, 0.1977, 0.0165, 0.5528, 0.0421, 0.0127],\n       device='cuda:0')\nimg 10577.jpg, class_prob is: tensor([0.0219, 0.1587, 0.6273, 0.0056, 0.0379, 0.0022, 0.0369, 0.0511, 0.0584],\n       device='cuda:0')\nimg 10017.jpg, class_prob is: tensor([0.0438, 0.3121, 0.2318, 0.0161, 0.1514, 0.0423, 0.1211, 0.0112, 0.0701],\n       device='cuda:0')\nimg 10479.jpg, class_prob is: tensor([0.0058, 0.4157, 0.2131, 0.0400, 0.0546, 0.1839, 0.0434, 0.0145, 0.0292],\n       device='cuda:0')\nimg 10604.jpg, class_prob is: tensor([0.0056, 0.0993, 0.4794, 0.0245, 0.0576, 0.0898, 0.1823, 0.0163, 0.0453],\n       device='cuda:0')\nimg 10285.jpg, class_prob is: tensor([0.0112, 0.3222, 0.4843, 0.0180, 0.0692, 0.0371, 0.0162, 0.0214, 0.0204],\n       device='cuda:0')\nimg 90202.jpg, class_prob is: tensor([0.1037, 0.0657, 0.1044, 0.0224, 0.0853, 0.0015, 0.5637, 0.0370, 0.0163],\n       device='cuda:0')\nimg 10160.jpg, class_prob is: tensor([0.0244, 0.1033, 0.0709, 0.0644, 0.3132, 0.0861, 0.1217, 0.0413, 0.1747],\n       device='cuda:0')\nimg 100198.jpg, class_prob is: tensor([0.0153, 0.2387, 0.2866, 0.0096, 0.0584, 0.0060, 0.2811, 0.0601, 0.0442],\n       device='cuda:0')\nimg 100034.jpg, class_prob is: tensor([0.0070, 0.0240, 0.7375, 0.0332, 0.1346, 0.0126, 0.0248, 0.0218, 0.0044],\n       device='cuda:0')\nimg 10331.jpg, class_prob is: tensor([0.1055, 0.0490, 0.1435, 0.0637, 0.1599, 0.0504, 0.3353, 0.0253, 0.0675],\n       device='cuda:0')\nimg 10214.jpg, class_prob is: tensor([0.0126, 0.5641, 0.0477, 0.0295, 0.0540, 0.0945, 0.0506, 0.0165, 0.1305],\n       device='cuda:0')\nimg 240.jpg, class_prob is: tensor([0.0088, 0.4677, 0.0158, 0.0085, 0.0853, 0.0041, 0.0192, 0.0207, 0.3699],\n       device='cuda:0')\nimg 10410.jpg, class_prob is: tensor([0.0368, 0.0628, 0.3392, 0.0753, 0.1717, 0.1086, 0.1817, 0.0195, 0.0044],\n       device='cuda:0')\nimg 10236.jpg, class_prob is: tensor([0.0050, 0.0423, 0.9172, 0.0018, 0.0114, 0.0092, 0.0022, 0.0043, 0.0067],\n       device='cuda:0')\nimg 42.jpg, class_prob is: tensor([0.0046, 0.0063, 0.0165, 0.0250, 0.0251, 0.0035, 0.8946, 0.0142, 0.0102],\n       device='cuda:0')\nimg 90007.jpg, class_prob is: tensor([0.0287, 0.0517, 0.7361, 0.0202, 0.0359, 0.0685, 0.0261, 0.0238, 0.0089],\n       device='cuda:0')\nimg 100373.jpg, class_prob is: tensor([0.0079, 0.0319, 0.5325, 0.0326, 0.2785, 0.0101, 0.0664, 0.0291, 0.0109],\n       device='cuda:0')\nimg 100146.jpg, class_prob is: tensor([0.0297, 0.3012, 0.0568, 0.0731, 0.1679, 0.0060, 0.1998, 0.0690, 0.0966],\n       device='cuda:0')\nimg 10543.jpg, class_prob is: tensor([0.0215, 0.2594, 0.0614, 0.0112, 0.0428, 0.0802, 0.3085, 0.0083, 0.2068],\n       device='cuda:0')\nimg 64.jpg, class_prob is: tensor([0.0365, 0.0918, 0.1058, 0.0447, 0.2888, 0.0066, 0.3342, 0.0651, 0.0265],\n       device='cuda:0')\nimg 100080.jpg, class_prob is: tensor([0.0074, 0.0326, 0.9456, 0.0017, 0.0026, 0.0025, 0.0031, 0.0026, 0.0019],\n       device='cuda:0')\nimg 10388.jpg, class_prob is: tensor([0.0029, 0.0464, 0.0259, 0.0762, 0.1816, 0.6206, 0.0306, 0.0087, 0.0072],\n       device='cuda:0')\nimg 10238.jpg, class_prob is: tensor([0.0139, 0.0817, 0.0415, 0.0073, 0.0454, 0.0025, 0.3419, 0.0251, 0.4407],\n       device='cuda:0')\nimg 10085.jpg, class_prob is: tensor([0.0110, 0.0998, 0.8299, 0.0025, 0.0164, 0.0122, 0.0017, 0.0068, 0.0197],\n       device='cuda:0')\nimg 10392.jpg, class_prob is: tensor([0.0074, 0.0144, 0.0278, 0.0164, 0.0157, 0.0028, 0.8931, 0.0142, 0.0081],\n       device='cuda:0')\nimg 221.jpg, class_prob is: tensor([0.0228, 0.0883, 0.0344, 0.0408, 0.2939, 0.0865, 0.1654, 0.0312, 0.2367],\n       device='cuda:0')\nimg 100222.jpg, class_prob is: tensor([0.0131, 0.1702, 0.4185, 0.0532, 0.1874, 0.0181, 0.1034, 0.0271, 0.0089],\n       device='cuda:0')\nimg 10484.jpg, class_prob is: tensor([0.0190, 0.2676, 0.0652, 0.0931, 0.2483, 0.0554, 0.1531, 0.0608, 0.0376],\n       device='cuda:0')\nimg 100263.jpg, class_prob is: tensor([0.0513, 0.1218, 0.3362, 0.0292, 0.1180, 0.0055, 0.2519, 0.0687, 0.0175],\n       device='cuda:0')\nimg 100379.jpg, class_prob is: tensor([0.0060, 0.1531, 0.0784, 0.0539, 0.0652, 0.4153, 0.1938, 0.0075, 0.0268],\n       device='cuda:0')\nimg 10058.jpg, class_prob is: tensor([0.0217, 0.0225, 0.8157, 0.0028, 0.0259, 0.0020, 0.0130, 0.0110, 0.0854],\n       device='cuda:0')\nimg 44.jpg, class_prob is: tensor([0.0304, 0.3533, 0.1229, 0.0171, 0.0375, 0.1888, 0.0284, 0.0253, 0.1963],\n       device='cuda:0')\nimg 356.jpg, class_prob is: tensor([0.0118, 0.0481, 0.7284, 0.0614, 0.0839, 0.0207, 0.0144, 0.0287, 0.0026],\n       device='cuda:0')\nimg 10184.jpg, class_prob is: tensor([0.0180, 0.0630, 0.0571, 0.0114, 0.0653, 0.0024, 0.7470, 0.0141, 0.0217],\n       device='cuda:0')\nimg 90038.jpg, class_prob is: tensor([0.0130, 0.0851, 0.1648, 0.0328, 0.2578, 0.0216, 0.2993, 0.0263, 0.0994],\n       device='cuda:0')\nimg 100293.jpg, class_prob is: tensor([0.0163, 0.1146, 0.3101, 0.0808, 0.1430, 0.0223, 0.2678, 0.0280, 0.0172],\n       device='cuda:0')\nimg 164.jpg, class_prob is: tensor([0.0387, 0.2200, 0.0386, 0.0152, 0.0999, 0.0106, 0.4888, 0.0199, 0.0684],\n       device='cuda:0')\nimg 10213.jpg, class_prob is: tensor([0.0772, 0.1158, 0.5259, 0.0054, 0.0325, 0.0158, 0.0499, 0.0247, 0.1527],\n       device='cuda:0')\nimg 10596.jpg, class_prob is: tensor([0.0039, 0.0704, 0.3960, 0.1109, 0.2341, 0.1079, 0.0249, 0.0409, 0.0109],\n       device='cuda:0')\nimg 90097.jpg, class_prob is: tensor([0.0285, 0.1540, 0.0092, 0.0161, 0.1429, 0.0141, 0.5176, 0.0336, 0.0839],\n       device='cuda:0')\nimg 10701.jpg, class_prob is: tensor([0.0345, 0.0778, 0.8156, 0.0084, 0.0159, 0.0194, 0.0072, 0.0050, 0.0160],\n       device='cuda:0')\nimg 332.jpg, class_prob is: tensor([0.0041, 0.0296, 0.7350, 0.0356, 0.0441, 0.0344, 0.0867, 0.0198, 0.0108],\n       device='cuda:0')\nimg 358.jpg, class_prob is: tensor([0.0130, 0.1856, 0.1176, 0.0475, 0.3586, 0.0123, 0.1686, 0.0506, 0.0462],\n       device='cuda:0')\nimg 2.jpg, class_prob is: tensor([0.0856, 0.1166, 0.0873, 0.1253, 0.1327, 0.1465, 0.2541, 0.0437, 0.0083],\n       device='cuda:0')\nimg 10321.jpg, class_prob is: tensor([0.0080, 0.0503, 0.8827, 0.0031, 0.0210, 0.0095, 0.0060, 0.0038, 0.0156],\n       device='cuda:0')\nimg 100366.jpg, class_prob is: tensor([0.0052, 0.1807, 0.1158, 0.0151, 0.1243, 0.0334, 0.3535, 0.0323, 0.1396],\n       device='cuda:0')\nimg 10389.jpg, class_prob is: tensor([0.0405, 0.1310, 0.1676, 0.0828, 0.2095, 0.0051, 0.1929, 0.0779, 0.0927],\n       device='cuda:0')\nimg 10578.jpg, class_prob is: tensor([0.0149, 0.0446, 0.8703, 0.0042, 0.0121, 0.0011, 0.0149, 0.0032, 0.0348],\n       device='cuda:0')\nimg 100242.jpg, class_prob is: tensor([0.0066, 0.2517, 0.0405, 0.0711, 0.1588, 0.1981, 0.2039, 0.0347, 0.0347],\n       device='cuda:0')\nimg 10060.jpg, class_prob is: tensor([0.0459, 0.0720, 0.1534, 0.0360, 0.2780, 0.0389, 0.2456, 0.0995, 0.0308],\n       device='cuda:0')\nclass 1 --> accuracy: 0.00, correct predictions: 0, all: 2\nclass 2 --> accuracy: 30.77, correct predictions: 4, all: 13\nclass 3 --> accuracy: 84.62, correct predictions: 22, all: 26\nclass 4 --> accuracy: 0.00, correct predictions: 0, all: 4\nclass 5 --> accuracy: 30.00, correct predictions: 3, all: 10\nclass 6 --> accuracy: 40.00, correct predictions: 2, all: 5\nclass 7 --> accuracy: 73.33, correct predictions: 11, all: 15\nclass 8 --> accuracy: 0.00, correct predictions: 0, all: 2\nclass 9 --> accuracy: 16.67, correct predictions: 1, all: 6\n<\/code>\nI have the same requirement but based on yours customised dataset couldn\u2019t figure out how can I print the image name to be able to make sure I am getting the probability class of related image. do you have any idea how can I print the image name ?\nthis is snippet for the dataset.\n<code class=\"lang-auto\">        \nclass CustomDataset(Dataset):\n    def __init__(self, image_paths, target_paths):   # initial logic happens like transform\n\n        self.image_paths = image_paths\n        self.target_paths = target_paths\n        self.transforms = transforms.ToTensor()\n        self.mapping = {\n            0: 0,\n            255: 1\n     \n        }\n    def mask_to_class(self, mask):\n        for k in self.mapping:\n            mask[mask==k] = self.mapping[k]\n        return mask\n    \n    def __getitem__(self, index):\n\n        image = Image.open(self.image_paths[index])\n        mask = Image.open(self.target_paths[index])\n        t_image = image.convert('L')\n        t_image = self.transforms(t_image)\n        mask = torch.from_numpy(np.array(mask, dtype=np.uint8)) # this is for my dataset(lv)\n        mask = self.mask_to_class(mask)\n        mask = mask.long()\n        return t_image, mask\n\n    def __len__(self):  # return count of sample we have\n\n        return len(self.image_paths)\n\n\n\nfolder_data = glob.glob(\"D:\\\\Neda\\\\Pytorch\\\\U-net\\\\my_data\\\\imagesResized\\\\*.png\")\nfolder_mask = glob.glob(\"D:\\\\Neda\\\\Pytorch\\\\U-net\\\\my_data\\\\labelsResized\\\\*.png\")\n\n# split these path using a certain percentage\nlen_data = len(folder_data)\nprint(\"count of dataset: \", len_data)\ntrain_size = 0.6\n\ntrain_image_paths = folder_data[:int(len_data*train_size)]\nprint(\"count of train images is: \", len(train_image_paths)) # output is 55 image for train\ntest_image_paths = folder_data[int(len_data*train_size):]\nprint(\"count of test images is: \", len(test_image_paths)) # output is 37 image for test\n\ntrain_mask_paths = folder_mask[:int(len_data*train_size)]\ntest_mask_paths = folder_mask[int(len_data*train_size):]\n\n\ntrain_dataset = CustomDataset(train_image_paths, train_mask_paths)\ntrain_loader = torch.utils.data.DataLoader(train_dataset, batch_size=1, shuffle=True, num_workers=2)\n\ntest_dataset = CustomDataset(test_image_paths, test_mask_paths)\ntest_loader = torch.utils.data.DataLoader(test_dataset, batch_size=1, shuffle=False, num_workers=2)\n\n<\/code>\nand this is the snippet when I test the test_loader\n<code class=\"lang-auto\">\ndef test():\n    \n    model_load = torch.load('model.pth')\n    \n         #test model\n    model_load.eval()\n    total = 0\n    test_loss = 0\n    correct = 0\n    count = 0\n    #iterate through test dataset\n    for ii, data in enumerate(test_loader):\n                \n                t_image, mask = data\n                #print(t_image.shape) # torch.Size([1, 1, 240, 320])\n                t_image, mask = t_image.to(device), mask.to(device)\n                with torch.no_grad():\n\n                    outputs = model_load(t_image)\n                    #print(outputs.shape) # torch.Size([1, 2, 240, 320])\n                    test_loss += criterion(outputs, mask).item() \/ len(test_loader)\n\n                    probs = torch.exp(outputs)\n                    probs_numpy = probs.detach().cpu().numpy()\n                    b = probs_numpy.tolist()\n                    file_path = \"\/Neda\/Pytorch\/U-net\/output.json\" ## your path variable\n                    json.dump(b, codecs.open(file_path, 'w', encoding='utf-8'), separators=(',', ':'), sort_keys=True, indent=4) ### this saves the array in .json format\n\n\n                    _, predicted = torch.max(outputs.data, 1)  \n                    \n                    total += mask.nelement()\n                    correct += predicted.eq(mask.data).sum().item()\n                    accuracy = 100 * correct \/ total\n                    \n                    count +=1                \n                    print(count, \"Test Loss: {:.3f}\".format(test_loss), \"Test Accuracy: %d %%\" % (accuracy))\n<\/code>\nI can print out image name now by manipulating the print in test function\n<code class=\"lang-auto\">print(count, \"Test Loss: {:.3f}\".format(test_loss), \"Test Accuracy: %d %%\" % (accuracy), os.path.basename(os.path.normpath(test_image_paths[ii])))\n<\/code>\nI would recommend to stick to the other approach, i.e. to return the image paths along with the data and target as shown in \u2019s code snippet.\nThe ii index in your test_loader loop does not necessarily correspond to the index which is used in your __getitem__, so your code might yield the wrong paths, e.g. if you use shuffle=True in your test_loader.\nYou could return change your __getitem__(self, index) to this:\n<code class=\"lang-python\">def __getitem__(self, index):\n    ...\n    return t_image, mask, self.image_paths[index]\n<\/code>\n Thanks a lot for your advice. I also did return the target path from getitem to plot the target as well when I load test_loader.\nhi\nI get BrokenPipeError\nHow can I get something like your results?\nThanks in advance for the help"},{"x":"I\u2019m trying to pass one more input to ResBlocks, and the ResNet I implemented is basically from torchvision ResNet with link \"https:\/\/github.com\/pytorch\/vision\/blob\/master\/torchvision\/models\/resnet.py\" as below. The only difference is that there\u2019s one more input to be forwarded in BasicBlocks.\n<code class=\"lang-auto\">model = ResNet(BasicBlock, [2, 2, 2, 2])\n\nclass ResNet(nn.Module):\n    def __init__(self, block, layers, num_classes=10):\n        self.layer1 = self._make_layer(block, 64, layers[0])\n        ...\n\n    def _make_layer(self, block, planes, blocks, stride=1):\n        downsample = None\n        if stride != 1 or self.inplanes != planes * block.expansion:\n            downsample = nn.Sequential(\n                conv1x1(self.inplanes, planes * block.expansion, stride),\n                nn.BatchNorm2d(planes * block.expansion),\n            )\n\n        layers = []\n        layers.append(block(self.inplanes, planes, stride, downsample))\n        self.inplanes = planes * block.expansion\n        for _ in range(1, blocks):\n            layers.append(block(self.inplanes, planes))\n\n        return nn.Sequential(*layers)\n    def forward(self, x, alpha):\n        ...\n        x = self.layer1(x, alpha) ## HERE\n        ...\nclass BasicBlock(nn.Module):\n    expansion = 1\n\n    def __init__(self, inplanes, planes, stride=1, downsample=None):\n        super(BasicBlock, self).__init__()\n        self.conv1 = conv3x3(inplanes, planes, stride)\n        self.bn1 = nn.BatchNorm2d(planes)\n        ...\n\n    def forward(self, x, alpha):\n        residual = x\n\n        out = self.conv1(x)\n        out = self.addi(out, alpha) ## Here needs 'alpha'\n        out = self.relu(out)\n        ...\n        return out\n<\/code>\nHowever, this causes an error,\n<code class=\"lang-auto\">TypeError: forward() takes 2 positional arguments but 3 were given,\n<\/code>\nor If I replace  x=self.layer1(x,alpha) with x=self.layer1(x)\n<code class=\"lang-auto\">TypeError: forward() missing 1 required positional argument: 'alpha'\n<\/code>\nHow can I fix it? Thanks!","y":"The problem is because nn.Sequential with link \"https:\/\/pytorch.org\/docs\/stable\/_modules\/torch\/nn\/modules\/container.html#Sequential\" (in ResBlock) takes only one input to be forwarded.\nThus, I solved it by override nn.Sequential function as\n<code class=\"lang-auto\">class MySequential(nn.Sequential):\n    def forward(self, x, alpha):\n        for module in self._modules.values():\n            x = module(x, alpha)\n        return x\n<\/code>","z":"The problem is because nn.Sequential with link \"https:\/\/pytorch.org\/docs\/stable\/_modules\/torch\/nn\/modules\/container.html#Sequential\" (in ResBlock) takes only one input to be forwarded.\nThus, I solved it by override nn.Sequential function as\n<code class=\"lang-auto\">class MySequential(nn.Sequential):\n    def forward(self, x, alpha):\n        for module in self._modules.values():\n            x = module(x, alpha)\n        return x\n<\/code>"},{"x":"I am trying to use my custom dataset and hence downloaded the the code data_loading_tutorial available in the Pytorch tutorials.\nThe code works fine till the line:\ntransformed_dataset = FaceLandmarksDataset(csv_file=\u2018faces\/face_landmarks.csv\u2019,\n_                                           root_dir=\u2018faces\/\u2019,\n_                                           transform=transforms.Compose([\nRescale(256),\n_                                               RandomCrop(224),\n_                                               ToTensor()\n_                                           ]))\nI skipped the part of manual iteration after that using the for loop and tried to run the rest of the code from\ndataloader = DataLoader(transformed_dataset, batch_size=4,\nshuffle=True, num_workers=4)\nIt shows the error : BrokenPipeError: [Errno 32] Broken pipe\nTo point out more precisely ,I would like to inform that the dataloader object is created. The error is basically created from enumerate(dataloader)\nI cannot understand what might be the reason of this. Please suggest a way around this . Thanks !","y":"You could add this line at the bottom of your code, if you wrap the other code into the run() function.\nAlternatively, you could wrap your whole code into the if-statement.\nThe important point is, that your main code should be executed inside this if-statement.\nOtherwise Windows will execute the whole script, which will create processes recursively.\nExample:\n<code class=\"lang-auto\">import torch\n...\n\ndef run():\n    model = ...\n    # training\n    for batch_idx, (data, target) in enumerate(loader):\n        ...\n\nif __name__=='__main__':\n    run()\n<\/code>","z":"Are you using Windows for this code?\nIf so, you should protect your code with\nif __name__=='__main__':\n    run()\n\nOtherwise Windows might try to create processes recursively, yielding your error.\nLet me know, if this is the case.\nYes I am using Windows 10.\nCan you please tell me where I should include the two lines ?\nYou could add this line at the bottom of your code, if you wrap the other code into the run() function.\nAlternatively, you could wrap your whole code into the if-statement.\nThe important point is, that your main code should be executed inside this if-statement.\nOtherwise Windows will execute the whole script, which will create processes recursively.\nExample:\n<code class=\"lang-auto\">import torch\n...\n\ndef run():\n    model = ...\n    # training\n    for batch_idx, (data, target) in enumerate(loader):\n        ...\n\nif __name__=='__main__':\n    run()\n<\/code>\nIts working , but a new issue  is that the images which where supposed to be shown , are not being shown \nIn fact, I suspect that the part in the if block is not being run at all.\nHow are you visualizing the images?\n<code class=\"lang-auto\">\nThis is the code:\n\nif __name__=='main':\n    \n    def show_landmarks_batch(sample_batched):\n        \"\"\"Show image with landmarks for a batch of samples.\"\"\"\n        images_batch, landmarks_batch = \\\n                sample_batched['image'], sample_batched['landmarks']\n        batch_size = len(images_batch)\n        im_size = images_batch.size(2)\n    \n        grid = utils.make_grid(images_batch)\n        plt.imshow(grid.numpy().transpose((1, 2, 0)))\n    \n        for i in range(batch_size):\n            plt.scatter(landmarks_batch[i, :, 0].numpy() + i * im_size,\n                        landmarks_batch[i, :, 1].numpy(),\n                        s=10, marker='.', c='r')\n    \n            plt.title('Batch from dataloader')\n    \n    \n    for i_batch, sample_batched in enumerate(dataloader):\n      print(i_batch, sample_batched['image'].size(),\n          sample_batched['landmarks'].size())\n    \n    # observe 4th batch and stop\n      if i_batch == 3:\n        plt.figure()\n        show_landmarks_batch(sample_batched)\n        plt.axis('off')\n        plt.ioff()\n        plt.show()\n        break\n<\/code>\nBasically it shows a batch of 4 images using matplotlib with the help of the function  show_landmarks_batch.\nIts a part of the code in Pytorch tutorials in the Pytorch website.\nI did this, but nothing shows up\u2026"},{"x":"Hi guys,\nI\u2019m trying to load point cloud data with dataloader. These data has different size like tensor (100,3) tensor (200, 3). I\u2019m able to load them with my custom collate function,  but there will be no batch dimension in this case. Could someone please help me with it? Thanks.","y":"Thanks for the info!\nDo you want to use all 300 samples at once?\nI see the disadvantage of this approach, since your batch size is not flexible anymore.\nI\u2019ve created a small example to use a list of different sized point clouds and load each sample in the Dataset:\n<code class=\"lang-python\">class MyDataset(Dataset):\n    def __init__(self, data):\n        self.data = data\n        self.lengths = [len(d) for d in self.data]\n        self.len = sum(self.lengths)\n        \n    def __getdata__(self, index):\n        for idx, length in enumerate(self.lengths):\n            if (index - length) < 0:\n                print('Using data[{}][{}]'.format(idx, index))\n                x = self.data[idx][index]\n                break\n            index -= length\n        return x\n    \n    def __getitem__(self, index):\n        x = self.__getdata__(index)\n        return x\n    \n    def __len__(self):\n        return self.len\n\n\ndata = []\ndata.append(torch.randn(100, 3))\ndata.append(torch.randn(200, 3))\ndata.append(torch.randn(150, 3))\n\ndataset = MyDataset(data)\nprint(len(dataset))\n# Check a few indices\ndataset[0]\ndataset[50]\ndataset[99]\ndataset[100]\ndataset[299]\ndataset[300]\ndataset[449]\n\nloader = DataLoader(\n    dataset,\n    batch_size=10,\n    shuffle=True,\n    num_workers=2\n)\n\nfor data in loader:\n    pass\n<\/code>\nThis will make sure to load only a single point, so that you can use the DataLoader as usual.\nLet me know, if that works for you!","z":"Could you share the code you are using to load the data?\nYou collate function would be of interest.\nI assume the first dimension corresponds to the number of samples in the current file?\nIs it not possible to load the samples in a Dataset without a custom collate_fn?\nIn short, the code looks like this\n<code class=\"lang-auto\">result = []\nfor i in range(len(data)):\n    result.append(data[i])\nresult = torch.FloatTensor(np.concatenate(result), axis = 0)\n<\/code>\nSo, the two tensor (100,3) (200,3) will combine into one tensor(300,3). I use another variable to count the number of rows of each tensor. As you can see, I lose the batch dimension by doing this. It\u2019s ok for single GPU, but batch dimension is necessary for multi-GPU case.\nThanks for the info!\nDo you want to use all 300 samples at once?\nI see the disadvantage of this approach, since your batch size is not flexible anymore.\nI\u2019ve created a small example to use a list of different sized point clouds and load each sample in the Dataset:\n<code class=\"lang-python\">class MyDataset(Dataset):\n    def __init__(self, data):\n        self.data = data\n        self.lengths = [len(d) for d in self.data]\n        self.len = sum(self.lengths)\n        \n    def __getdata__(self, index):\n        for idx, length in enumerate(self.lengths):\n            if (index - length) < 0:\n                print('Using data[{}][{}]'.format(idx, index))\n                x = self.data[idx][index]\n                break\n            index -= length\n        return x\n    \n    def __getitem__(self, index):\n        x = self.__getdata__(index)\n        return x\n    \n    def __len__(self):\n        return self.len\n\n\ndata = []\ndata.append(torch.randn(100, 3))\ndata.append(torch.randn(200, 3))\ndata.append(torch.randn(150, 3))\n\ndataset = MyDataset(data)\nprint(len(dataset))\n# Check a few indices\ndataset[0]\ndataset[50]\ndataset[99]\ndataset[100]\ndataset[299]\ndataset[300]\ndataset[449]\n\nloader = DataLoader(\n    dataset,\n    batch_size=10,\n    shuffle=True,\n    num_workers=2\n)\n\nfor data in loader:\n    pass\n<\/code>\nThis will make sure to load only a single point, so that you can use the DataLoader as usual.\nLet me know, if that works for you!\nThanks for the detailed solution. This is really helpful."},{"x":"I am trying to do CIFAR10 tutorial on GPU. since, I am worling on windows machien I was getting an error BrokenPipeError: [Errno 32] Broken pipe which solved aftre wrap the code in if __name__=='__main__': main() , then I did check that is cuda available or not which is available.\n<code class=\"lang-auto\">device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n\n# Assume that we are on a CUDA machine, then this should print a CUDA device:\nprint(device)\n<\/code>\nand I added the net.to(device). then I added the second line to send the inputs and targets at every step to the GPU too,\n<code class=\"lang-auto\">inputs, labels = data\ninputs, labels = inputs.to(device), labels.to(device)\n\n<\/code>\nbut I got an error\n<code class=\"lang-auto\">\n  File \"C:\\Users\\Neda\\Anaconda3\\lib\\site-packages\\torch\\nn\\modules\\conv.py\", line 301, in forward\n    self.padding, self.dilation, self.groups)\n\nRuntimeError: Input type (torch.cuda.FloatTensor) and weight type (torch.FloatTensor) should be the same\n<\/code>\nthank you in advance.","y":"It should be alright. You could put the net creation, datasets, and loaders into main, but this shouldn\u2019t yield this error.\nHowever, it looks like you\u2019re not pushing the data and target onto the GPU in your eval loops:\n<code class=\"lang-python\">with torch.no_grad():\n    for data in testloader:\n        images, labels = data\n        outputs = net(images)\n<\/code>\nAdd the .to(device) calls to both tensors like in the training loop.\nAlso, the code to get some predictions for a single batch is missing the push to GPU:\n<code class=\"lang-python\">dataiter = iter(testloader)\nimages, labels = dataiter.next()\n...\noutputs = net(images)\n<\/code>","z":"It looks like your model is still on the CPU.\nCould you call net = net.to(device) and run it again?\n I did that, and I got this error\n<code class=\"lang-auto\">\n  File \"C:\\Users\\Neda\\Anaconda3\\lib\\site-packages\\torch\\nn\\modules\\conv.py\", line 301, in forward\n    self.padding, self.dilation, self.groups)\n\nRuntimeError: Expected object of type torch.FloatTensor but found type torch.cuda.FloatTensor for argument #2 'weight'\n<\/code>\nNow it seems your input is on the CPU again?\nCould you check the dtypes of your input and model?\nSomething like this should work:\n<code class=\"lang-python\">print(input.type())\nprint(model.conv1.weight.type())  # Change \"conv1\" to your layer name\n<\/code>\n I am not sure about when I wrap the code in def main(). maybe I am doing something mistake here\n<code class=\"lang-auto\">import torch\nimport torchvision\nimport torchvision.transforms as transforms\nimport torch.nn as nn\nimport torch.nn.functional as F\nimport torch.optim as optim\n\n\ndevice = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\nprint(device)\n\nclass Net(nn.Module):\n    def __init__(self):\n        super(Net, self).__init__()\n        \n        self.conv1 = nn.Conv2d(3, 6, 5)\n        self.pool = nn.MaxPool2d(2, 2)\n        self.conv2 = nn.Conv2d(6, 16, 5)\n        \n        self.fc1 = nn.Linear(16 * 5 * 5, 120)\n        self.fc2 = nn.Linear(120, 84)\n        self.fc3 = nn.Linear(84, 10)\n\n    def forward(self, x):\n        x = self.pool(F.relu(self.conv1(x)))\n        x = self.pool(F.relu(self.conv2(x)))\n        x = x.view(-1, 16 * 5 * 5)\n        x = F.relu(self.fc1(x))\n        x = F.relu(self.fc2(x))\n        x = self.fc3(x)\n        return x\n\nnet = Net()\nnet = net.to(device)\nprint(net)\n\n\n   \ntransform = transforms.Compose(\n    [transforms.ToTensor(),\n     transforms.Normalize((0.5, 0.5, 0.5), (0.5, 0.5, 0.5))])\n\ntrainset = torchvision.datasets.CIFAR10(root='.\/data', train=True,\n                                        download=True, transform=transform)\ntrainloader = torch.utils.data.DataLoader(trainset, batch_size=4,\n                                          shuffle=True, num_workers=2)\n\ntestset = torchvision.datasets.CIFAR10(root='.\/data', train=False,\n                                       download=True, transform=transform)\ntestloader = torch.utils.data.DataLoader(testset, batch_size=4,\n                                         shuffle=False, num_workers=2)\n\nclasses = ('plane', 'car', 'bird', 'cat',\n           'deer', 'dog', 'frog', 'horse', 'ship', 'truck')\n\ncriterion = nn.CrossEntropyLoss()\noptimizer = optim.SGD(net.parameters(), lr=0.001, momentum=0.9)\n\ndef main():\n    for epoch in range(2):\n    \n        running_loss = 0.0\n     \n        for i, data in enumerate(trainloader, 0):\n            inputs, labels = data\n            inputs, labels = inputs.to(device), labels.to(device)\n            optimizer.zero_grad()\n    \n            outputs = net(inputs)\n            loss = criterion(outputs, labels)\n    \n            loss.backward()\n            optimizer.step()\n    \n            running_loss += loss.item()\n            if i % 2000 == 1999:  \n                print('[%d, %5d] loss: %.3f' %\n                      (epoch + 1, i + 1, running_loss \/ 2000))\n                running_loss = 0.0\n    print('Finished Training')\n    \n\n    dataiter = iter(testloader)\n    images, labels = dataiter.next()\n    \n    print('Ground truth: ', ' '.join('%5s' % classes[labels[j]] for j in\n                                     range(4)))\n    \n    outputs = net(images)\n    _, predicted = torch.max(outputs, 1)\n    print('Predicted: ', ' '.join('%5s' % classes[predicted[j]]\n                                  for j in range(4)))\n    \n    correct = 0\n    total = 0\n    with torch.no_grad():\n        for data in testloader:\n            images, labels = data\n            outputs = net(images)\n            _, predicted = torch.max(outputs.data, 1)\n            total += labels.size(0)\n            correct += (predicted == labels).sum().item()\n    print('Accuracy of the network on the 10000 test images: %d %%' % (\n            100 * correct \/ total))\n    \n    \n    class_correct = list(0. for i in range(10))\n    class_total = list(0. for i in range(10))\n    with torch.no_grad():\n        for data in testloader:\n            images, labels = data\n            outputs = net(images)\n            _, predicted = torch.max(outputs, 1)\n            c = (predicted == labels).squeeze()\n            for i in range(4):\n                label = labels[i]\n                class_correct[label] += c[i].item()\n                class_total[label] += 1\n    \n    \n    for i in range(10):\n     print('Accuracy of %5s : %2d %%' % (\n            classes[i], 100 * class_correct[i] \/ class_total[i]))\n    \n\nif __name__=='__main__':\n    main()\n<\/code>\nIt should be alright. You could put the net creation, datasets, and loaders into main, but this shouldn\u2019t yield this error.\nHowever, it looks like you\u2019re not pushing the data and target onto the GPU in your eval loops:\n<code class=\"lang-python\">with torch.no_grad():\n    for data in testloader:\n        images, labels = data\n        outputs = net(images)\n<\/code>\nAdd the .to(device) calls to both tensors like in the training loop.\nAlso, the code to get some predictions for a single batch is missing the push to GPU:\n<code class=\"lang-python\">dataiter = iter(testloader)\nimages, labels = dataiter.next()\n...\noutputs = net(images)\n<\/code>\n. Yes. I did and it works. I didn\u2019t notice any speed up. how can I be sure is done on GPU?\ndo you think is this correct?\n<code class=\"lang-auto\">device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\nprint(device)\n\ntransform = transforms.Compose(\n    [transforms.ToTensor(),\n     transforms.Normalize((0.5, 0.5, 0.5), (0.5, 0.5, 0.5))])\n\ntrainset = torchvision.datasets.CIFAR10(root='.\/data', train=True,\n                                        download=True, transform=transform)\ntrainloader = torch.utils.data.DataLoader(trainset, batch_size=4,\n                                          shuffle=True, num_workers=2)\n\ntestset = torchvision.datasets.CIFAR10(root='.\/data', train=False,\n                                       download=True, transform=transform)\ntestloader = torch.utils.data.DataLoader(testset, batch_size=4,\n                                         shuffle=False, num_workers=2)\n\nclasses = ('plane', 'car', 'bird', 'cat',\n           'deer', 'dog', 'frog', 'horse', 'ship', 'truck')\n\nclass Net(nn.Module):\n    def __init__(self):\n        super(Net, self).__init__()\n        \n        self.conv1 = nn.Conv2d(3, 6, 5)\n        self.pool = nn.MaxPool2d(2, 2)\n        self.conv2 = nn.Conv2d(6, 16, 5)\n        \n        self.fc1 = nn.Linear(16 * 5 * 5, 120)\n        self.fc2 = nn.Linear(120, 84)\n        self.fc3 = nn.Linear(84, 10)\n\n    def forward(self, x):\n        x = self.pool(F.relu(self.conv1(x)))\n        x = self.pool(F.relu(self.conv2(x)))\n        x = x.view(-1, 16 * 5 * 5)\n        x = F.relu(self.fc1(x))\n        x = F.relu(self.fc2(x))\n        x = self.fc3(x)\n        return x\n\nnet = Net()\nnet = net.to(device)\nprint(net)\n\ncriterion = nn.CrossEntropyLoss()\noptimizer = optim.SGD(net.parameters(), lr=0.001, momentum=0.9)\n\ndef main():\n    for epoch in range(2):\n    \n        running_loss = 0.0\n     \n        for i, data in enumerate(trainloader, 0):\n            inputs, labels = data\n            inputs, labels = inputs.to(device), labels.to(device)\n            optimizer.zero_grad()\n    \n            outputs = net(inputs)\n            loss = criterion(outputs, labels)\n    \n            loss.backward()\n            optimizer.step()\n    \n            running_loss += loss.item()\n            if i % 2000 == 1999:  \n                print('[%d, %5d] loss: %.3f' %\n                      (epoch + 1, i + 1, running_loss \/ 2000))\n                running_loss = 0.0\n    print('Finished Training')\n    \n\n    dataiter = iter(testloader)\n    images, labels = dataiter.next()\n    images, labels = images.to(device), labels.to(device)\n\n    \n    print('Ground truth: ', ' '.join('%5s' % classes[labels[j]] for j in\n                                     range(4)))\n    \n    outputs = net(images)\n    _, predicted = torch.max(outputs, 1)\n    print('Predicted: ', ' '.join('%5s' % classes[predicted[j]]\n                                  for j in range(4)))\n    \n    correct = 0\n    total = 0\n    with torch.no_grad():\n        for data in testloader:\n            images, labels = data\n            images, labels = images.to(device), labels.to(device)\n            outputs = net(images)\n            _, predicted = torch.max(outputs.data, 1)\n            total += labels.size(0)\n            correct += (predicted == labels).sum().item()\n    print('Accuracy of the network on the 10000 test images: %d %%' % (\n            100 * correct \/ total))\n    \n    \n    class_correct = list(0. for i in range(10))\n    class_total = list(0. for i in range(10))\n    with torch.no_grad():\n        for data in testloader:\n            images, labels = data\n            images, labels = images.to(device), labels.to(device)\n            outputs = net(images)\n            _, predicted = torch.max(outputs, 1)\n            c = (predicted == labels).squeeze()\n            for i in range(4):\n                label = labels[i]\n                class_correct[label] += c[i].item()\n                class_total[label] += 1\n    \n    \n    for i in range(10):\n     print('Accuracy of %5s : %2d %%' % (\n            classes[i], 100 * class_correct[i] \/ class_total[i]))\n    \n\nif __name__=='__main__':\n    main()\n<\/code>\nYes, it looks good!\nWhat is print(device) saying?\nThe workload might be a bit small to fully utilize your GPU.\nYou could increase the number of parameters in your model (conv filters, linear weights) and you\u2019ll probably see some performance gain.\n you. the print(device) output is cuda:0 ."},{"x":"Where weight initialised in the Pytorch tutorial. In the self.conv1 = nn.Conv2d(3, 6, 5) layer, there are 6 different kernels (output-feature) but how the value of the kernel assigned? is it weight initialised in criterion = nn.CrossEntropyLoss(). any comment will be appreciated.","y":"They are initialized at the time the Module is instantiated, i.e. conv1 = nn.Conv2d(3, 6, 5), through the reset_parameters() method, which is executed as part of the Module\u2019s __init__ with link \"https:\/\/pytorch.org\/docs\/master\/_modules\/torch\/nn\/modules\/conv.html#_ConvNd\". The dimensions of the kernel and bias are set through the module\u2019s constructor arguments with link \"https:\/\/pytorch.org\/docs\/master\/nn.html#conv2d\". If you want to change the parameters\u2019 initialization you can do so explicitly with the functions defined in torch.nn.init with link \"https:\/\/pytorch.org\/docs\/master\/nn.html#torch-nn-init\" and the Module\u2019s apply(fn) method, e.g.\n<code class=\"lang-auto\">def init_weights(m):\n    if type(m) == nn.Linear:\n        torch.nn.init.xavier_uniform(m.weight)\n    elif type(m) == nn.Conv2d:\n        # Some other initalization\n\nnet = Net()\nnet.apply(init_weights)\n<\/code>","z":"The weight initialization happens here:\n\n\ngithub.com with link \"https:\/\/github.com\/pytorch\/pytorch\/blob\/master\/torch\/nn\/modules\/conv.py#L41-L47\"\n\n\npytorch\/pytorch\/blob\/master\/torch\/nn\/modules\/conv.py#L41-L47 with link \"https:\/\/github.com\/pytorch\/pytorch\/blob\/master\/torch\/nn\/modules\/conv.py#L41-L47\"\n<code class=\"lang-py\">\ndef reset_parameters(self):\n    n = self.in_channels\n    init.kaiming_uniform_(self.weight, a=math.sqrt(5))\n    if self.bias is not None:\n        fan_in, _ = init._calculate_fan_in_and_fan_out(self.weight)\n        bound = 1 \/ math.sqrt(fan_in)\n        init.uniform_(self.bias, -bound, bound)\n<\/code>\n\n\n\n\n\n thank you for the link. I still cannot understand how weight initialization happened.  For example in the Tutorial CIFAR10 with link \"https:\/\/pytorch.org\/tutorials\/beginner\/blitz\/cifar10_tutorial.html#sphx-glr-beginner-blitz-cifar10-tutorial-py\" could you please tell me where weight initialization happened. I know in the first layer which is conv1 we use 6 filters with the kernel size of 5*5, but my question is how the values of these kernels specified?\nThey are initialized at the time the Module is instantiated, i.e. conv1 = nn.Conv2d(3, 6, 5), through the reset_parameters() method, which is executed as part of the Module\u2019s __init__ with link \"https:\/\/pytorch.org\/docs\/master\/_modules\/torch\/nn\/modules\/conv.html#_ConvNd\". The dimensions of the kernel and bias are set through the module\u2019s constructor arguments with link \"https:\/\/pytorch.org\/docs\/master\/nn.html#conv2d\". If you want to change the parameters\u2019 initialization you can do so explicitly with the functions defined in torch.nn.init with link \"https:\/\/pytorch.org\/docs\/master\/nn.html#torch-nn-init\" and the Module\u2019s apply(fn) method, e.g.\n<code class=\"lang-auto\">def init_weights(m):\n    if type(m) == nn.Linear:\n        torch.nn.init.xavier_uniform(m.weight)\n    elif type(m) == nn.Conv2d:\n        # Some other initalization\n\nnet = Net()\nnet.apply(init_weights)\n<\/code>"},{"x":"Some people get the error on the second GPU here: Problem of running on the second GPU on Ubuntu with link \"https:\/\/discuss.pytorch.org\/t\/problem-of-running-on-the-second-gpu-on-ubuntu\/20601\"\nI am using a single tesla k-80. I get this error when doing a linear operation on a 4 by 2048 matrix\nimage.png806\u00d767 3.96 KB with link \"https:\/\/discuss.pytorch.org\/uploads\/default\/original\/2X\/d\/d702333a23d65714fb15e49be25dafda6743ca8d.png\"\nWhy I am getting this error?","y":"I already screwed up the environment here that this error \u201cRuntimeError: cuda runtime error (77) : an illegal memory access was encountered\u201d was returned on everything. I screwed it up by not following the guideline here:\n\n\n\n\nConverting numpy array to tensor on GPU with link \"https:\/\/discuss.pytorch.org\/t\/converting-numpy-array-to-tensor-on-gpu\/19423\"\n\n\n    import torch\nfrom skimage import io\n\nimg = io.imread('input.png')\n\ndevice = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\nprint(device)\n\nimg =torch.tensor(img, device=device).float()\nprint(img.device)\n\nOutput \ncuda:0\ncpu\n\nI am not able to convert a numpy array into a torch tensor on GPU. What is the right way to do this?\n  \n\n","z":"I already screwed up the environment here that this error \u201cRuntimeError: cuda runtime error (77) : an illegal memory access was encountered\u201d was returned on everything. I screwed it up by not following the guideline here:\n\n\n\n\nConverting numpy array to tensor on GPU with link \"https:\/\/discuss.pytorch.org\/t\/converting-numpy-array-to-tensor-on-gpu\/19423\"\n\n\n    import torch\nfrom skimage import io\n\nimg = io.imread('input.png')\n\ndevice = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\nprint(device)\n\nimg =torch.tensor(img, device=device).float()\nprint(img.device)\n\nOutput \ncuda:0\ncpu\n\nI am not able to convert a numpy array into a torch tensor on GPU. What is the right way to do this?\n  \n\n"},{"x":"Hello,\nI\u2019m currently a student doing offline character recognition, but there is an issue with the model.parameters() not updating during training. Here is a copy of my train_function and forward. I used F.log_softmax() for the forward, and nll_loss() for the loss function. All the intermediate tensors (output, input, loss, etc) appear to be correct. However, when computing \u201cpost - pre\u201d, there is no update to the model parameters, which is very confusing.\n\nprint(\"shape coming in is \"+str(x.shape))\nx = F.max_pool2d(F.relu(self.conv1(x)), self.kernel)\nprint(\"shape after round 1 is \"+ str(x.shape))\nx = F.max_pool2d(F.relu(self.conv2(x)), self.kernel)\nprint(\"shape after round 2 is \"+str(x.shape))\nx = F.max_pool2d(F.relu(self.conv3(x)), self.kernel)\nprint(\"shape after round 3 is \"+str(x.shape))\nx = x.view(-1, self.flatten_features(x))\nprint(\"shape after round 4 view is \"+str(x.shape))\nx = F.relu(self.fc1(x))\nprint(\"shape after round 5 linear 1 is \"+str(x.shape))\nx = self.fc2(x)\nprint(\"shape after round 6 linear 2 is \"+str(x.shape))\nreturn F.log_softmax(x)\n\n\noptimizer.zero_grad()\nprint(\u201cOUTPUT: {}{}\".format(output, output.shape))\nloss = F.nll_loss(output, chin_char)\nprint(\"LOSS: {}{}\u201d.format(loss,loss.shape))\npre = list(model.parameters())[0]\nloss.backward()\noptimizer.step()\npost = list(model.parameters())[0]\n","y":"I have a simple example for this:\n<code class=\"lang-auto\">>>> model = nn.Linear(4, 2)\n>>> x = torch.randn(10, 4)\n>>> y = torch.LongTensor([1, 0, 1, 1, 0, 1, 0, 1, 1, 1])\n>>> optimizer = torch.optim.SGD(model.parameters(), lr=0.1, momentum=0.9)\n>>> # compute the output\n>>> out = F.log_softmax(model(x), dim=1)\n>>> pre = list(model.parameters())[0]\n>>> # print the parameters before update:\n>>> print(pre)\nParameter containing:\ntensor([[ 0.4893, -0.4020,  0.3605, -0.3534],\n        [-0.3695,  0.3736,  0.1374,  0.3506]], requires_grad=True)\n<\/code>\nNow, we do the update step:\n<code class=\"lang-auto\">>>> optimizer.zero_grad()\n>>> loss = F.nll_loss(out, y)\n>>> loss.backward()\n>>> optimizer.step()\n>>> post = list(model.parameters())[0]\n>>> print(pre)\nParameter containing:\ntensor([[ 0.4812, -0.3906,  0.3314, -0.3524],\n        [-0.3614,  0.3621,  0.1665,  0.3496]], requires_grad=True)\n>>> print(post)\nParameter containing:\ntensor([[ 0.4812, -0.3906,  0.3314, -0.3524],\n        [-0.3614,  0.3621,  0.1665,  0.3496]], requires_grad=True)\n<\/code>\nSo you can see that pre and post have the same values after the update step is performed, but the printed values of paramaters before the update are different.","z":"Can you provide the way you defined the optimizer? Maybe the parameters are not passed to the optimizer correctly. Also, what is the learning rate?\n\noptimizer = optim.SGD(model.parameters(),lr=args.l_rate,momentum=args.momentum)\n\nLearning rate is 0.01.\n\n\n\n karnation22:\n\nF.nll_loss(output, chin_char)\n\n\nIt actually chanegs, but pre and post are the same because it\u2019s not creating a new copy of the tensor values. Try printing them before and after the optimizer.step() line.\nI have a simple example for this:\n<code class=\"lang-auto\">>>> model = nn.Linear(4, 2)\n>>> x = torch.randn(10, 4)\n>>> y = torch.LongTensor([1, 0, 1, 1, 0, 1, 0, 1, 1, 1])\n>>> optimizer = torch.optim.SGD(model.parameters(), lr=0.1, momentum=0.9)\n>>> # compute the output\n>>> out = F.log_softmax(model(x), dim=1)\n>>> pre = list(model.parameters())[0]\n>>> # print the parameters before update:\n>>> print(pre)\nParameter containing:\ntensor([[ 0.4893, -0.4020,  0.3605, -0.3534],\n        [-0.3695,  0.3736,  0.1374,  0.3506]], requires_grad=True)\n<\/code>\nNow, we do the update step:\n<code class=\"lang-auto\">>>> optimizer.zero_grad()\n>>> loss = F.nll_loss(out, y)\n>>> loss.backward()\n>>> optimizer.step()\n>>> post = list(model.parameters())[0]\n>>> print(pre)\nParameter containing:\ntensor([[ 0.4812, -0.3906,  0.3314, -0.3524],\n        [-0.3614,  0.3621,  0.1665,  0.3496]], requires_grad=True)\n>>> print(post)\nParameter containing:\ntensor([[ 0.4812, -0.3906,  0.3314, -0.3524],\n        [-0.3614,  0.3621,  0.1665,  0.3496]], requires_grad=True)\n<\/code>\nSo you can see that pre and post have the same values after the update step is performed, but the printed values of paramaters before the update are different."},{"x":"Hello guys !\nI am working on a problem in which I have the coordinates to slice the image like [y, y+height, x, x+width]. So if if I have torch image obtained using\n img = Variable(img.cuda())\n\nhow can we slice the image to get that specific area of image [y:y+height, x:x+width] .\nThanks","y":"You can directly index your image tensor:\n<code class=\"lang-python\">img = torch.randn(1, 3, 10, 10, device='cuda')\nx, y = 1, 1\nwidth, height = 5, 5\nimg[:, :, y:y+height, x:x+width]\n<\/code>","z":"You can directly index your image tensor:\n<code class=\"lang-python\">img = torch.randn(1, 3, 10, 10, device='cuda')\nx, y = 1, 1\nwidth, height = 5, 5\nimg[:, :, y:y+height, x:x+width]\n<\/code>"},{"x":"Hello! Why there is no global pooling in Pytorch Framework.\nI just notice there are normal pooling method like Maxpool  or  Avgpool\nbut no Global pool there, why?~\nAnd if you may, how does global pooling works actually, I just saw it in the paper about CNN, but a little bit confuse about its backend mechanism:hushed:","y":"use torch.mean or torch.max operator","z":"use torch.mean or torch.max operator\nAs  says, you can use torch,mean for global average pooling. Another way is by torch.nn.AvgPool2d(kernel_size=feature_size).  This way you need to find out the size of the input features, and apply pooling with kernel size as big as your feature size.\nthx smth:+1:\nthx vmirly:+1:"},{"x":"I am trying to display an RGB image using imshow, but it does not let me display it and gives the error - \u201cInvalid dimensions for image data\u201d.\nThe dimensions of the image variable that I am trying to display are 33232.","y":"matplotlib usually expects the images to have the channel in dim2, so try to permute your image tensor using tensor = tensor.permute(1, 2, 0) before passing it to imshow.","z":"matplotlib usually expects the images to have the channel in dim2, so try to permute your image tensor using tensor = tensor.permute(1, 2, 0) before passing it to imshow."},{"x":"It\u2019s not a question rather a discussion. Suppose I have two directories, one with images[1 \u2026 N] and the second with a modified version of image[1 \u2026 N]. Image1 when modified forms modified_image_1, ImageN to modified_image_N. I want to train a network that would learn from the directories and generate a model. Unknown ImageX, when passed through this image, will provide me a output of modified_image_x.\nSo, I would appreciate if anyone could help me on how to, or a similiar example of some network etc.","y":"You have to understand that neural networks are trained to perform a task. With a non-trained network you cannot measure disparity.\nIf you would have, for example, a classifier, then you can compute a loss which measure the difference between two image in terms of how likely are they to belong to the same class (or whatever loss you use).\nNN recognize patters, difficult patters. But you need to \u201cteach\u201d which samples are closer and which samples are farther in terms of your criteria.\nAnyway the most closer thing you are looking for is metric learning, siamese networks with contrastive loss and hinge loss. In which NN learn to set pairs of samples closer of further, and then you can compare latent spaces to measure disparity. Anyway you will have to define which \u201ccloser\u201d means for you","z":"It depends on which modification you are performing.\nHave a look at Cycled GAN, style transfer, image-to-image translation.\nThank you   so  what I was planning is to creat Net which will be common for 2 inputs,\n<code class=\"lang-auto\">def forward(self,x):\n        output = self.cnn1(x)\n        output = output.view(output.size()[0], -1)\n        output = self.fc1(output)\n        return output\n\ndef calculate(self, input1, input2):\n        output1 = self.forward_once(input1)\n        output2 = self.forward_once(input2)\n        return output1, output2\n<\/code>\nand calculate the loss between output1 and output2 and based on the difference value, determine the modification\nThe problem is that you have no criteria about what \u201cdifferent\u201d means. You can determine the modification in terms of colors, shapes, semantic, texture\u2026 You have to define from which space to which other you want to map\/compare\nYes exactly. So should I just append all the loss\/difference betwwen Output1 and Output2 into a Variable and train the Variable to calculate a relation. But I highly doubt it might be of use.\nYou have to understand that neural networks are trained to perform a task. With a non-trained network you cannot measure disparity.\nIf you would have, for example, a classifier, then you can compute a loss which measure the difference between two image in terms of how likely are they to belong to the same class (or whatever loss you use).\nNN recognize patters, difficult patters. But you need to \u201cteach\u201d which samples are closer and which samples are farther in terms of your criteria.\nAnyway the most closer thing you are looking for is metric learning, siamese networks with contrastive loss and hinge loss. In which NN learn to set pairs of samples closer of further, and then you can compare latent spaces to measure disparity. Anyway you will have to define which \u201ccloser\u201d means for you"},{"x":"I was training my model using Yolo v3\nWhen I set my input size = 416, I can train my model with batch size = 9 without any errors.\nHowever, when I decrease my input size to 320, I  ran into Cuda memory error even when my batch size = 7.\nI found this particularly strange, has anyone encountered anything similar before?\nError happened when executing loss.backward()\nThank you in advance for helping me!","y":"Ho,\nWhat happens if you add torch.backends.cudnn.enabled=False at the beginning of your code? Does the error still occurs?","z":"Ok this is getting even more strange.\nKeeping my input size = 320\nI changed my batch size from 9 to 16, the model started training flawlessly without any errors\nHi,\nDo you use cudnn? Do you use it in benchmark mode? What is the memory usage values when it actually run and what is the memory on your gpu?\nHi albanD with link \"https:\/\/discuss.pytorch.org\/u\/albanD\"\n\nNo I did not explicitly use cudnn.\nI did not use it in benchmark mode.\nThese were my memory and gpu memory usage when I ran my training with input size = 320, batch size = 9:\n\n[2018-12-12 13:27:27,480 train.py] ram memory info before loss.backward(): svmem(total=17138393088, available=5950562304, percent=65.3, used=11187830784, free=5950562304)\n[2018-12-12 13:27:27,481 train.py] max_memory_allocated before loss.backward(): 2788280832\n[2018-12-12 13:27:27,481 train.py] memory_cached before loss.backward(): 2799042560\n[2018-12-12 13:27:27,482 train.py] max_memory_cached before loss.backward(): 2799042560\n[2018-12-12 13:27:27,482 train.py] memory_allocated before loss.backward(): 2772128768\nErros came right after this\nline 70, in train\nloss.backward()\nFile \u201cC:\\Users\\jayde\\Anaconda3\\envs\\new_env\\lib\\site-packages\\torch\\tensor.py\u201d, line 93, in backward\ntorch.autograd.backward(self, gradient, retain_graph, create_graph)\nFile \u201cC:\\Users\\jayde\\Anaconda3\\envs\\new_env\\lib\\site-packages\\torch\\autograd_init_.py\u201d, line 90, in backward\nallow_unreachable=True)  # allow_unreachable flag\nRuntimeError: CUDA error: out of memory\nHowever when I set input size = 320, batch size = 16,  this is what I get:\n[2018-12-12 13:32:11,560 train.py] ram memory info before loss.backward(): svmem(total=17138393088, available=5892562944, percent=65.6, used=11245830144, free=5892562944)\n[2018-12-12 13:32:11,561 train.py] max_memory_allocated before loss.backward(): 4837138432\n[2018-12-12 13:32:11,561 train.py] memory_cached before loss.backward(): 5020450816\n[2018-12-12 13:32:11,562 train.py] max_memory_cached before loss.backward(): 5020450816\n[2018-12-12 13:32:11,562 train.py] memory_allocated before loss.backward(): 4684070912\n[2018-12-12 13:32:11,749 train.py] ram memory info after loss.backward(): svmem(total=17138393088, available=5946978304, percent=65.3, used=11191414784, free=5946978304)\n[2018-12-12 13:32:11,749 train.py] max_memory_allocated after loss.backward(): 5460978176\n[2018-12-12 13:32:11,749 train.py] memory_cached after loss.backward(): 6174408704\n[2018-12-12 13:32:11,750 train.py] max_memory_cached after loss.backward(): 6174408704\n[2018-12-12 13:32:11,750 train.py] memory_allocated after loss.backward(): 518663680\n[2018-12-12 13:32:12,162 train.py] ram memory info before loss.backward(): svmem(total=17138393088, available=5980577792, percent=65.1, used=11157815296, free=5980577792)\n[2018-12-12 13:32:12,162 train.py] max_memory_allocated before loss.backward(): 5460978176\n[2018-12-12 13:32:12,163 train.py] memory_cached before loss.backward(): 6193283072\n[2018-12-12 13:32:12,163 train.py] max_memory_cached before loss.backward(): 6193283072\n[2018-12-12 13:32:12,163 train.py] memory_allocated before loss.backward(): 5199972864\n[2018-12-12 13:32:12,315 train.py] ram memory info after loss.backward(): svmem(total=17138393088, available=5965262848, percent=65.2, used=111731\nNo memory errors were raised.\nThank you again for helping me!\nHi,\nAre you sharing the GPU with other people?\nPytorch reports <3GB used but cuda report 11+GB in use on the device ( in the case where it fails).\nAs you can see, in the case where it works with a bigger batch size, pytorch actually uses much more memory: 5\/6GB.\nIf there is a lot of memory already used on the gpu even though nothing is running (you can check with nvidia-smi). You can use (sudo) lsof \/dev\/nvidiaX where X is the number of the gpu you want to check. This will list all process using the gpu more thoroughly than nvidia-smi. If there are some things that shouldn\u2019t be here like old python process, you can get rid of them by killing them.\nHi \nThank you so much for your reply!\nNo I did not share GPU with anyone.\nThe 11+GB is my local ram but gpu memory. I\u2019m using a 2080 so it has only 8 GB of vram.\nThe thing is it fails every single time if I set input size = 320, batch = 9. However if I do input size = 320, batch = 16 right after that, it runs perfectly every single time too.\nI tried to clear cache before back propagation too but the error incurred .\nThank you again for helping me!\nHo,\nWhat happens if you add torch.backends.cudnn.enabled=False at the beginning of your code? Does the error still occurs?\nHi \nYes it solves the problem!!\nThank you so much for helping!\nOn the other, I have a couple of question regarding to this.\n\nI have never set  torch.backends.cudnn.enabled=True, so is True the default setting in pytorch?\nWhy would setting it True resulting this error?\n3, In what occasion that setting torch.backends.cudnn.enabled=True would be beneficial?\n\nUpdate:\nSo I did one more round of test,\nif I set\ntorch.backends.cudnn.enabled = True\nand\ntorch.backends.cudnn.benchmark = True\nno errors were raised too.\nBut if I set\ntorch.backends.cudnn.benchmark = False\nThen error appeared again.\nPreviously I was having this impression that the error was caused by the benchmark as it continues to search for the best algorithm. Now I am even more confused.\nOnce again thank you so much for your help!!!\nHi,\nYes I just remembered, that there was a bug in some handling of cudnn algorithm.\nBasically cudnn choose the fastest possible algorithm but that sometimes require more memory than what you have. This should be handled properly and select a less memory hangry algorithm. There was a bug in this fixed in this pr with link \"https:\/\/github.com\/pytorch\/pytorch\/pull\/13665\".\nNot sure if it made it into 1.0 but I think it did. Enabling benchmark mode should fix the issue and keep the best runtime !\nThank you so much !\nJust one last question, do we need to install cudnn separately or it will be automatically installed when conda install torch?\nThank you again for helping me!\nIt comes with the conda install and is enabled by default."},{"x":"Hi everyone, I am running the Discriminator from here with link \"https:\/\/pytorch.org\/tutorials\/beginner\/dcgan_faces_tutorial.html\" with ndf = 32 and I am getting the following error:\n<code class=\"lang-auto\">Traceback (most recent call last):\n  File \"main.py\", line 152, in <module>\n    train(epoch, model, opt, dataset_train, train_logger)\n  File \"main.py\", line 88, in train\n    model.train_step(imgs_, labels)\n  File \"\/home\/darley\/Documents\/Repositories\/attention-based-classification\/models\/ad_attn_vae.py\", line 119, in train_step\n    output = self.disc(same_class).view(-1)\n  File \"\/home\/darley\/miniconda3\/lib\/python3.7\/site-packages\/torch\/nn\/modules\/module.py\", line 489, in __call__\n    result = self.forward(*input, **kwargs)\n  File \"\/home\/darley\/Documents\/Repositories\/attention-based-classification\/models\/disc.py\", line 51, in forward\n    output = self.conv5(output)\n  File \"\/home\/darley\/miniconda3\/lib\/python3.7\/site-packages\/torch\/nn\/modules\/module.py\", line 489, in __call__\n    result = self.forward(*input, **kwargs)\n  File \"\/home\/darley\/miniconda3\/lib\/python3.7\/site-packages\/torch\/nn\/modules\/conv.py\", line 320, in forward\n    self.padding, self.dilation, self.groups)\nRuntimeError: std::exception\n<\/code>\nMy Pytorch cpu version is 1.0.0 from conda on python 3.7.1. This error is happening at the last conv layer nn.Conv2d(ndf * 8, 1, 4, 1, 0, bias=False).\nWhat does this error mean?","y":"I found the error. Before the last Conv2d, my tensor had shape torch.Size([8, 128, 2, 2]), then when it goes to the last Conv2d it crashes. So I am not using the two last convs.","z":"I found the error. Before the last Conv2d, my tensor had shape torch.Size([8, 128, 2, 2]), then when it goes to the last Conv2d it crashes. So I am not using the two last convs."},{"x":"Looking through the tutorials, I am able to get the last layer vector (say, length 512 for ResNet18) (see here with link \"https:\/\/github.com\/monajalal\/img2vec\/blob\/master\/img_to_vec.py\") , and I am also separately able to do transfer learning on my own dataset.\nHow can I get the last layer feature vector from my own transferred learned network?\n<code class=\"lang-auto\"># -*- coding: utf-8 -*-\n\"\"\"\nTransfer Learning Tutorial\n==========================\n**Author**: `Sasank Chilamkurthy <https:\/\/chsasank.github.io>`_\n\nIn this tutorial, you will learn how to train your network using\ntransfer learning. You can read more about the transfer learning at `cs231n\nnotes <http:\/\/cs231n.github.io\/transfer-learning\/>`__\n\nQuoting these notes,\n\n    In practice, very few people train an entire Convolutional Network\n    from scratch (with random initialization), because it is relatively\n    rare to have a dataset of sufficient size. Instead, it is common to\n    pretrain a ConvNet on a very large dataset (e.g. ImageNet, which\n    contains 1.2 million images with 1000 categories), and then use the\n    ConvNet either as an initialization or a fixed feature extractor for\n    the task of interest.\n\nThese two major transfer learning scenarios look as follows:\n\n-  **Finetuning the convnet**: Instead of random initializaion, we\n   initialize the network with a pretrained network, like the one that is\n   trained on imagenet 1000 dataset. Rest of the training looks as\n   usual.\n-  **ConvNet as fixed feature extractor**: Here, we will freeze the weights\n   for all of the network except that of the final fully connected\n   layer. This last fully connected layer is replaced with a new one\n   with random weights and only this layer is trained.\n\n\"\"\"\n# License: BSD\n# Author: Sasank Chilamkurthy\n\nfrom __future__ import print_function, division\n\nimport torch\nimport torch.nn as nn\nimport torch.optim as optim\nfrom torch.optim import lr_scheduler\nimport numpy as np\nimport torchvision\nfrom torchvision import datasets, models, transforms\nimport matplotlib.pyplot as plt\nimport time\nimport os\nimport copy\n\nplt.ion()   # interactive mode\n\n######################################################################\n# Load Data\n# ---------\n#\n# We will use torchvision and torch.utils.data packages for loading the\n# data.\n#\n# The problem we're going to solve today is to train a model to classify\n# **ants** and **bees**. We have about 120 training images each for ants and bees.\n# There are 75 validation images for each class. Usually, this is a very\n# small dataset to generalize upon, if trained from scratch. Since we\n# are using transfer learning, we should be able to generalize reasonably\n# well.\n#\n# This dataset is a very small subset of imagenet.\n#\n# .. Note ::\n#    Download the data from\n#    `here <https:\/\/download.pytorch.org\/tutorial\/hymenoptera_data.zip>`_\n#    and extract it to the current directory.\n\n# Data augmentation and normalization for training\n# Just normalization for validation\ndata_transforms = {\n    'train': transforms.Compose([\n        transforms.RandomResizedCrop(224),\n        transforms.RandomHorizontalFlip(),\n        transforms.ToTensor(),\n        transforms.Normalize([0.485, 0.456, 0.406], [0.229, 0.224, 0.225])\n    ]),\n    'val': transforms.Compose([\n        transforms.Resize(256),\n        transforms.CenterCrop(224),\n        transforms.ToTensor(),\n        transforms.Normalize([0.485, 0.456, 0.406], [0.229, 0.224, 0.225])\n    ]),\n}\n\n#data_dir = 'hymenoptera_data'\ndata_dir = \"mona_data\"\nimage_datasets = {x: datasets.ImageFolder(os.path.join(data_dir, x),\n                                          data_transforms[x])\n                  for x in ['train', 'val']}\ndataloaders = {x: torch.utils.data.DataLoader(image_datasets[x], batch_size=4,\n                                             shuffle=True, num_workers=4)\n              for x in ['train', 'val']}\ndataset_sizes = {x: len(image_datasets[x]) for x in ['train', 'val']}\nclass_names = image_datasets['train'].classes\n\ndevice = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n\n######################################################################\n# Visualize a few images\n# ^^^^^^^^^^^^^^^^^^^^^^\n# Let's visualize a few training images so as to understand the data\n# augmentations.\n\ndef imshow(inp, title=None):\n    \"\"\"Imshow for Tensor.\"\"\"\n    inp = inp.numpy().transpose((1, 2, 0))\n    mean = np.array([0.485, 0.456, 0.406])\n    std = np.array([0.229, 0.224, 0.225])\n    inp = std * inp + mean\n    inp = np.clip(inp, 0, 1)\n    plt.imshow(inp)\n    if title is not None:\n        plt.title(title)\n    plt.pause(0.001)  # pause a bit so that plots are updated\n\n\n# Get a batch of training data\ninputs, classes = next(iter(dataloaders['train']))\n\n# Make a grid from batch\nout = torchvision.utils.make_grid(inputs)\n\nimshow(out, title=[class_names[x] for x in classes])\n\n\n######################################################################\n# Training the model\n# ------------------\n#\n# Now, let's write a general function to train a model. Here, we will\n# illustrate:\n#\n# -  Scheduling the learning rate\n# -  Saving the best model\n#\n# In the following, parameter ``scheduler`` is an LR scheduler object from\n# ``torch.optim.lr_scheduler``.\n\n\ndef train_model(model, criterion, optimizer, scheduler, num_epochs=25):\n    since = time.time()\n\n    best_model_wts = copy.deepcopy(model.state_dict())\n    best_acc = 0.0\n\n    for epoch in range(num_epochs):\n        print('Epoch {}\/{}'.format(epoch, num_epochs - 1))\n        print('-' * 10)\n\n        # Each epoch has a training and validation phase\n        for phase in ['train', 'val']:\n            if phase == 'train':\n                scheduler.step()\n                model.train()  # Set model to training mode\n            else:\n                model.eval()   # Set model to evaluate mode\n\n            running_loss = 0.0\n            running_corrects = 0\n\n            # Iterate over data.\n            for inputs, labels in dataloaders[phase]:\n                inputs = inputs.to(device)\n                labels = labels.to(device)\n\n                # zero the parameter gradients\n                optimizer.zero_grad()\n\n                # forward\n                # track history if only in train\n                with torch.set_grad_enabled(phase == 'train'):\n                    outputs = model(inputs)\n                    _, preds = torch.max(outputs, 1)\n                    loss = criterion(outputs, labels)\n\n                    # backward + optimize only if in training phase\n                    if phase == 'train':\n                        loss.backward()\n                        optimizer.step()\n\n                # statistics\n                running_loss += loss.item() * inputs.size(0)\n                running_corrects += torch.sum(preds == labels.data)\n\n            epoch_loss = running_loss \/ dataset_sizes[phase]\n            epoch_acc = running_corrects.double() \/ dataset_sizes[phase]\n\n            print('{} Loss: {:.4f} Acc: {:.4f}'.format(\n                phase, epoch_loss, epoch_acc))\n\n            # deep copy the model\n            if phase == 'val' and epoch_acc > best_acc:\n                best_acc = epoch_acc\n                best_model_wts = copy.deepcopy(model.state_dict())\n\n        print()\n\n    time_elapsed = time.time() - since\n    print('Training complete in {:.0f}m {:.0f}s'.format(\n        time_elapsed \/\/ 60, time_elapsed % 60))\n    print('Best val Acc: {:4f}'.format(best_acc))\n\n    # load best model weights\n    model.load_state_dict(best_model_wts)\n    return model\n\n\n######################################################################\n# Visualizing the model predictions\n# ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n#\n# Generic function to display predictions for a few images\n#\n\ndef visualize_model(model, num_images=6):\n    was_training = model.training\n    model.eval()\n    images_so_far = 0\n    fig = plt.figure()\n\n    with torch.no_grad():\n        for i, (inputs, labels) in enumerate(dataloaders['val']):\n            inputs = inputs.to(device)\n            labels = labels.to(device)\n\n            outputs = model(inputs)\n            _, preds = torch.max(outputs, 1)\n\n            for j in range(inputs.size()[0]):\n                images_so_far += 1\n                ax = plt.subplot(num_images\/\/2, 2, images_so_far)\n                ax.axis('off')\n                ax.set_title('predicted: {}'.format(class_names[preds[j]]))\n                imshow(inputs.cpu().data[j])\n\n                if images_so_far == num_images:\n                    model.train(mode=was_training)\n                    return\n        model.train(mode=was_training)\n\n######################################################################\n# Finetuning the convnet\n# ----------------------\n#\n# Load a pretrained model and reset final fully connected layer.\n#\n\nmodel_ft = models.resnet18(pretrained=True)\nnum_ftrs = model_ft.fc.in_features\nmodel_ft.fc = nn.Linear(num_ftrs, 7)\n\nmodel_ft = model_ft.to(device)\n\ncriterion = nn.CrossEntropyLoss()\n\n# Observe that all parameters are being optimized\noptimizer_ft = optim.SGD(model_ft.parameters(), lr=0.001, momentum=0.9)\n\n# Decay LR by a factor of 0.1 every 7 epochs\nexp_lr_scheduler = lr_scheduler.StepLR(optimizer_ft, step_size=7, gamma=0.1)\n\n######################################################################\n# Train and evaluate\n# ^^^^^^^^^^^^^^^^^^\n#\n# It should take around 15-25 min on CPU. On GPU though, it takes less than a\n# minute.\n#\n\nmodel_ft = train_model(model_ft, criterion, optimizer_ft, exp_lr_scheduler,\n                       num_epochs=25)\n\n######################################################################\n#\n\nvisualize_model(model_ft)\n\n\n######################################################################\n# ConvNet as fixed feature extractor\n# ----------------------------------\n#\n# Here, we need to freeze all the network except the final layer. We need\n# to set ``requires_grad == False`` to freeze the parameters so that the\n# gradients are not computed in ``backward()``.\n#\n# You can read more about this in the documentation\n# `here <http:\/\/pytorch.org\/docs\/notes\/autograd.html#excluding-subgraphs-from-backward>`__.\n#\n\nmodel_conv = torchvision.models.resnet18(pretrained=True)\nfor param in model_conv.parameters():\n    param.requires_grad = False\n\n# Parameters of newly constructed modules have requires_grad=True by default\nnum_ftrs = model_conv.fc.in_features\nmodel_conv.fc = nn.Linear(num_ftrs, 7)\n\nmodel_conv = model_conv.to(device)\n\ncriterion = nn.CrossEntropyLoss()\n\n# Observe that only parameters of final layer are being optimized as\n# opoosed to before.\noptimizer_conv = optim.SGD(model_conv.fc.parameters(), lr=0.001, momentum=0.9)\n\n# Decay LR by a factor of 0.1 every 7 epochs\nexp_lr_scheduler = lr_scheduler.StepLR(optimizer_conv, step_size=7, gamma=0.1)\n\n\n######################################################################\n# Train and evaluate\n# ^^^^^^^^^^^^^^^^^^\n#\n# On CPU this will take about half the time compared to previous scenario.\n# This is expected as gradients don't need to be computed for most of the\n# network. However, forward does need to be computed.\n#\n\nmodel_conv = train_model(model_conv, criterion, optimizer_conv,\n                         exp_lr_scheduler, num_epochs=25)\n\n######################################################################\n#\n\nvisualize_model(model_conv)\n\nplt.ioff()\nplt.show()\n<\/code>\nOur model here is model_conv and we are interested in getting the feature vector by dropping result of softmax.\nI am able to get the last layer using (I am not sure if it is correct):\n<code class=\"lang-auto\">tf_last_layer_chopped = nn.Sequential(*list(model_conv.children())[:-1])\nprint(tf_last_layer_chopped)\nSequential(\n  (0): Conv2d(3, 64, kernel_size=(7, 7), stride=(2, 2), padding=(3, 3), bias=False)\n  (1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n  (2): ReLU(inplace)\n  (3): MaxPool2d(kernel_size=3, stride=2, padding=1, dilation=1, ceil_mode=False)\n  (4): Sequential(\n    (0): BasicBlock(\n      (conv1): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n      (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n      (relu): ReLU(inplace)\n      (conv2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n      (bn2): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n    )\n    (1): BasicBlock(\n      (conv1): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n      (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n      (relu): ReLU(inplace)\n      (conv2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n      (bn2): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n    )\n  )\n  (5): Sequential(\n    (0): BasicBlock(\n      (conv1): Conv2d(64, 128, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\n      (bn1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n      (relu): ReLU(inplace)\n      (conv2): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n      (bn2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n      (downsample): Sequential(\n        (0): Conv2d(64, 128, kernel_size=(1, 1), stride=(2, 2), bias=False)\n        (1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n      )\n    )\n    (1): BasicBlock(\n      (conv1): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n      (bn1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n      (relu): ReLU(inplace)\n      (conv2): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n      (bn2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n    )\n  )\n  (6): Sequential(\n    (0): BasicBlock(\n      (conv1): Conv2d(128, 256, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\n      (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n      (relu): ReLU(inplace)\n      (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n      (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n      (downsample): Sequential(\n        (0): Conv2d(128, 256, kernel_size=(1, 1), stride=(2, 2), bias=False)\n        (1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n      )\n    )\n    (1): BasicBlock(\n      (conv1): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n      (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n      (relu): ReLU(inplace)\n      (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n      (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n    )\n  )\n  (7): Sequential(\n    (0): BasicBlock(\n      (conv1): Conv2d(256, 512, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\n      (bn1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n      (relu): ReLU(inplace)\n      (conv2): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n      (bn2): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n      (downsample): Sequential(\n        (0): Conv2d(256, 512, kernel_size=(1, 1), stride=(2, 2), bias=False)\n        (1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n      )\n    )\n    (1): BasicBlock(\n      (conv1): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n      (bn1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n      (relu): ReLU(inplace)\n      (conv2): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n      (bn2): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n    )\n  )\n  (8): AvgPool2d(kernel_size=7, stride=1, padding=0)\n  (9): Linear(in_features=512, out_features=7, bias=True)\n)\n<\/code>\nSo, how can I pass an image to the code above and get its feature vector of length 512?","y":"Thanks for the response. The error I had raises from not having this line that I added:\nmodel_conv = model_conv.cuda()\n<code class=\"lang-auto\">from PIL import Image\nscaler = transforms.Scale((224, 224))\nnormalize = transforms.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225])\nto_tensor = transforms.ToTensor()\nimg = Image.open('mona.jpg')\nimage = normalize(to_tensor(scaler(img))).unsqueeze(0).to(device)\nprint(image.shape)\nmodel_conv = model_conv.cuda()\ntf_last_layer_chopped = nn.Sequential(*list(model_conv.children())[:-1])\noutput = tf_last_layer_chopped(image)\nprint(output)\n<\/code>\nThere is a discussion in this page with link \"https:\/\/discuss.pytorch.org\/t\/input-type-cudafloattensor-and-weight-type-cpufloattensor-should-be-the-same\/15527\/2?u=mona_jalal\" about the error I got.","z":"tf_last_layer_chopped(image) is not working?\n\n\n\n Mona_Jalal:\n\ntf_last_layer_chopped = nn.Sequential(*list(model_conv.children())[:-1])\n\n\nHi thanks for the reply.\nNo, it gives me an error:\n<code class=\"lang-auto\">\nfrom PIL import Image\nscaler = transforms.Scale((224, 224))\nnormalize = transforms.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225])\nto_tensor = transforms.ToTensor()\nimg = Image.open('gun.jpg')\nimage = normalize(to_tensor(scaler(img))).unsqueeze(0).to(device)\nmy_embedding = torch.zeros(1,512, 1, 1)\ntf_last_layer_chopped = nn.Sequential(*list(model_conv.children())[:-1])\nprint(tf_last_layer_chopped(image))\n<\/code>\nError is:\n<code class=\"lang-auto\">\/scratch\/sjn-p3\/anaconda\/anaconda3\/lib\/python3.6\/site-packages\/torchvision-0.2.1-py3.6.egg\/torchvision\/transforms\/transforms.py:188: UserWarning: The use of the transforms.Scale transform is deprecated, please use transforms.Resize instead.\n  \"please use transforms.Resize instead.\")\n\n---------------------------------------------------------------------------\nRuntimeError                              Traceback (most recent call last)\n<ipython-input-54-58cf89cb5ae5> in <module>()\n      7 my_embedding = torch.zeros(1,512, 1, 1)\n      8 tf_last_layer_chopped = nn.Sequential(*list(model_conv.children())[:-1])\n----> 9 print(tf_last_layer_chopped(image))\n\n\/scratch\/sjn-p3\/anaconda\/anaconda3\/lib\/python3.6\/site-packages\/torch\/nn\/modules\/module.py in __call__(self, *input, **kwargs)\n    475             result = self._slow_forward(*input, **kwargs)\n    476         else:\n--> 477             result = self.forward(*input, **kwargs)\n    478         for hook in self._forward_hooks.values():\n    479             hook_result = hook(self, input, result)\n\n\/scratch\/sjn-p3\/anaconda\/anaconda3\/lib\/python3.6\/site-packages\/torch\/nn\/modules\/container.py in forward(self, input)\n     89     def forward(self, input):\n     90         for module in self._modules.values():\n---> 91             input = module(input)\n     92         return input\n     93 \n\n\/scratch\/sjn-p3\/anaconda\/anaconda3\/lib\/python3.6\/site-packages\/torch\/nn\/modules\/module.py in __call__(self, *input, **kwargs)\n    475             result = self._slow_forward(*input, **kwargs)\n    476         else:\n--> 477             result = self.forward(*input, **kwargs)\n    478         for hook in self._forward_hooks.values():\n    479             hook_result = hook(self, input, result)\n\n\/scratch\/sjn-p3\/anaconda\/anaconda3\/lib\/python3.6\/site-packages\/torch\/nn\/modules\/linear.py in forward(self, input)\n     53 \n     54     def forward(self, input):\n---> 55         return F.linear(input, self.weight, self.bias)\n     56 \n     57     def extra_repr(self):\n\n\/scratch\/sjn-p3\/anaconda\/anaconda3\/lib\/python3.6\/site-packages\/torch\/nn\/functional.py in linear(input, weight, bias)\n   1024         return torch.addmm(bias, input, weight.t())\n   1025 \n-> 1026     output = input.matmul(weight.t())\n   1027     if bias is not None:\n   1028         output += bias\n\nRuntimeError: size mismatch, m1: [512 x 1], m2: [512 x 7] at \/opt\/conda\/conda-bld\/pytorch_1535491974311\/work\/aten\/src\/THC\/generic\/THCTensorMathBlas.cu:249\n\n\n<\/code>\nThis code works for me:\n<code class=\"lang-python\">model_conv = torchvision.models.resnet18(pretrained=False)\nnum_ftrs = model_conv.fc.in_features\nmodel_conv.fc = nn.Linear(num_ftrs, 7)\ntf_last_layer_chopped = nn.Sequential(*list(model_conv.children())[:-1])\nx = torch.randn(1, 3, 224, 224)\noutput = tf_last_layer_chopped(x)\nprint(output.shape)\n> torch.Size([1, 512, 1, 1])\n<\/code>\nCould you check for differences between your code and this example?\n<code class=\"lang-auto\"><\/code>\nThanks for the response. The error I had raises from not having this line that I added:\nmodel_conv = model_conv.cuda()\n<code class=\"lang-auto\">from PIL import Image\nscaler = transforms.Scale((224, 224))\nnormalize = transforms.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225])\nto_tensor = transforms.ToTensor()\nimg = Image.open('mona.jpg')\nimage = normalize(to_tensor(scaler(img))).unsqueeze(0).to(device)\nprint(image.shape)\nmodel_conv = model_conv.cuda()\ntf_last_layer_chopped = nn.Sequential(*list(model_conv.children())[:-1])\noutput = tf_last_layer_chopped(image)\nprint(output)\n<\/code>\nThere is a discussion in this page with link \"https:\/\/discuss.pytorch.org\/t\/input-type-cudafloattensor-and-weight-type-cpufloattensor-should-be-the-same\/15527\/2?u=mona_jalal\" about the error I got.\nhi,\ncan you please explain me to how can i use inception_v3 instead of resnet18. i have changed input size to 299*299 and train the model. i have got below error\n\nYour code might work. Most likely you are initializing your inception model with the default settings, i.e. aux_logits=True, which will return a tuple (last layer and the aux_logits).\nSet aux_logits=False and run your code again.\nCan you please explain where should i make changes to get results,\n<code class=\"lang-auto\">model_ft = models.inception_v3(pretrained=True)\nnum_ftrs = model_ft.fc.in_features\nmodel_ft.fc = nn.Linear(num_ftrs, 2)\nmodel_ft = model_ft.to(device)\ncriterion = nn.CrossEntropyLoss()\n# Observe that all parameters are being optimized\noptimizer_ft = optim.SGD(model_ft.parameters(), lr=0.001, momentum=0.9)\n# Decay LR by a factor of 0.1 every 7 epochs\nexp_lr_scheduler = lr_scheduler.StepLR(optimizer_ft, step_size=7, gamma=0.1)\n<\/code>\nThanks its working.\nmodel_ft.aux_logits=False"},{"x":"I am a beginner in PyTorch and I am currently going through the official tutorials.\nIn the Transfer Learning tutorial with link \"https:\/\/pytorch.org\/tutorials\/beginner\/transfer_learning_tutorial.html\", the author used different transformation steps for Training and Validation data.\n<code class=\"lang-auto\">data_transforms = {\n    'train': transforms.Compose([\n        transforms.RandomResizedCrop(224),\n        transforms.RandomHorizontalFlip(),\n        transforms.ToTensor(),\n        transforms.Normalize([0.485, 0.456, 0.406], [0.229, 0.224, 0.225])\n    ]),\n    'val': transforms.Compose([\n        transforms.Resize(256),\n        transforms.CenterCrop(224),\n        transforms.ToTensor(),\n        transforms.Normalize([0.485, 0.456, 0.406], [0.229, 0.224, 0.225])\n    ]),\n}\n<\/code>\nI wanted to understand the intuition behind this decision.","y":"The general approach is to use some data augmentation in your training so artificially create \u201cnew\u201d samples of your data, and to use some \u201cconstant\u201d pre-processing in your validation\/test case.\nAs you can see the training transformation has some random transforms, e.g. RandomResizedCrop, which is fine for training, but could yield different predictions for the same sample in your test dataset.\nIt is thus preferred to use the non-random versions of the transformations for the validation\/test to get consistent predictions.","z":"The general approach is to use some data augmentation in your training so artificially create \u201cnew\u201d samples of your data, and to use some \u201cconstant\u201d pre-processing in your validation\/test case.\nAs you can see the training transformation has some random transforms, e.g. RandomResizedCrop, which is fine for training, but could yield different predictions for the same sample in your test dataset.\nIt is thus preferred to use the non-random versions of the transformations for the validation\/test to get consistent predictions."},{"x":"Hi guys,\njust want to introduce a tool of Pytorch which helps me with the kaggle TGS challenge with link \"https:\/\/www.kaggle.com\/c\/tgs-salt-identification-challenge\". It\u2019s called  Minetorch  and was built by me during this competition, my friends and I use this the whole time with the TGS challenge and it does bring some conveniences for us.\nHere\u2019s the link https:\/\/github.com\/louis-she\/minetorch , it\u2019s already on pypi so one can install it just with  pip install minetorch . Things out of box are:\n\nLogger\nTensorboard supported\nAuto resume\nAuto best model saving\nMany hook points for customization\n\nThere is also an mnist example of how to use this tool https:\/\/github.com\/louis-she\/minetorch\/blob\/master\/minetorch\/examples\/mnist.py , it\u2019s basiclly migrated from the official pytorch mnist example.\nThanks","y":" thanks for mentioning Ignite !\n sure that your project and Ignite have a lot of features in common. We are also working on more tight integration of Tensorboard into Ignite. If you have opportunity, give a try to Ignite and feel free to propose new features \nAnyway nice analogy in your project presentation:\n\u2026 So is data-mining, A special torch named pytorch with link \"http:\/\/pytorch.org\/\" can help us get the dimonds in data\u2026","z":"Sounds interesting? How does your tool compare to Ignite with link \"https:\/\/github.com\/pytorch\/ignite\"? Do you see any advantages \/ disadvantages using one or the other?\nHaven\u2019t known this until now, many features are same except the tensorboard and namespace.\nI write this for my own need, not sure if it\u2019s fit for everyone, just share it : )\nSure, thanks a lot for sharing it! Maybe some features would be nice to have in Ignite as well.\nCC \n thanks for mentioning Ignite !\n sure that your project and Ignite have a lot of features in common. We are also working on more tight integration of Tensorboard into Ignite. If you have opportunity, give a try to Ignite and feel free to propose new features \nAnyway nice analogy in your project presentation:\n\u2026 So is data-mining, A special torch named pytorch with link \"http:\/\/pytorch.org\/\" can help us get the dimonds in data\u2026\nThat\u2019s great that PyTorch has a official tool like this, I will dig more about the Ignite and see if I can contribute. : )"},{"x":"I noticed that other people are able to successfully avoid the \u201cstr\u2019 object has no attribute \u2018size\u2019\u201d error message, but I\u2019m getting it for some reason. What is the reason for that and how can I correct that?\nError message:\n\n\nAttributeError                            Traceback (most recent call last)\n in ()\n5 test_image_index = 28\n6 test_image = test_dir + \u201c\/\u201d + str(test_image_index) + \u201c\/image_05230.jpg\u201d\n----> 7 img = process_image(test_image)\n8 #test_image = images.to(\u2018cuda\u2019)\n9\n in process_image(image)\n7\n8     # scale\n----> 9     img_w, img_h = image.size\n10\n11     if(img_w > img_h):\nAttributeError: \u2018str\u2019 object has no attribute \u2018size\u2019\n\nCode Reference 1:\n\n<h1>TODO: Display an image along with the top 5 classes<\/h1>\n<h1>Get and process a Test Image<\/h1>\ntorch.set_default_tensor_type(\u2018torch.cuda.FloatTensor\u2019)\ntest_image_index = 28\ntest_image = test_dir + \u201c\/\u201d + str(test_image_index) + \u201c\/image_05230.jpg\u201d\nimg = process_image(test_image)\n#test_image = images.to(\u2018cuda\u2019)\n<h1>Display test image, with Label as title<\/h1>\nlabel = cat_to_name.get(str(test_image_index))\nprint(label)\nax = imshow(img, ax=plt).title(label)\n<h1>Run image through model<\/h1>\nprobs, classes = predict(test_image, model)\nprint(\u201cProbs:\u201d,probs)\nprint(\u201cClass:\u201d,classes)\n\nCode Reference 2:\n\ndef process_image(image):\n\u2018\u2019\u2019 Scales, crops, and normalizes a PIL image for a PyTorch model,\nreturns an Numpy array\n\u2018\u2019\u2019\n# TODO: Process a PIL image for use in a PyTorch model\n\n# scale\n\nimg_w, img_h = image.size\nif(img_w > img_h):\nimage = image.resize(size=(int((img_w256)\/img_h),256))\nelif(img_w < img_h):\nimage = image.resize(size=(256,int((img_w256)\/img_h)))\n# crop\nimg_w_new, img_h_new = image.size\nc1 = int(img_w_new\/2-112)\nc2 = int(img_h_new\/2-112)\nc3 = int(img_w_new\/2+112)\nc4 = int(img_h_new\/2+112)\n\nimage = image.crop((c1, c2, c3, c4)) # Getting (224, 224) image\n\n# normalize\nmean = np.array([0.485, 0.456, 0.406])\nstd = np.array([0.229, 0.224, 0.225])\nimage_array = np.array(image) \/ 255\nimage_norm = (image_array - mean) \/ std\n\n# reorder dimension\nimage_trans = image_norm.transpose((2,0,1))\n\nreturn torch.from_numpy(image_trans) # converting ndarray to tensor\n","y":"It seems you are passing the image path to process_image instead of an PIL.Image.\nTry to load the image and pass it to the function:\n<code class=\"lang-python\">from PIL import Image\n\ntest_image_path = ...\ntest_image = Image.open(test_image_path)\nprocess_image(test_image)\n<\/code>","z":"It seems you are passing the image path to process_image instead of an PIL.Image.\nTry to load the image and pass it to the function:\n<code class=\"lang-python\">from PIL import Image\n\ntest_image_path = ...\ntest_image = Image.open(test_image_path)\nprocess_image(test_image)\n<\/code>"},{"x":"I\u2019ve been trying to pin down the issue behind the constant increase of GPU usage but I\u2019ve been unable to.\nimage.png1030\u00d7478 48.3 KB with link \"https:\/\/discuss.pytorch.org\/uploads\/default\/original\/2X\/9\/9a6714e206b29093b9f9f1fb4de147bf77a07de7.png\"\n[it keeps increasing until the kernel dies]\nI\u2019ve tried solutions that were given before (del output, output_max, loss, data, targets, index, inputs, labels, make sure I use loss.item(), gc.collect(), torch.cuda.empty_cache()) but nothing worked. What is the best way to track down what\u2019s causing this?\nNot sure if this is relevant but one of the things that is different in my implementation is that although each batch is accessed initially using the dataloader, the dataloader also returns the indixes of the accessed data which are then used in conjunction with getitem(index) function to access \u2018subsets\u2019 of those data a couple of times.\nLet me know if you need further information.","y":"I believe I\u2019ve just solved this.\nIt turns out that a specific function was allocating memory that Python wasn\u2019t able to retrieve back and so the accumulation was the cause of this increase. So to solve this I spawn a new process to run the function and terminate it once I get what I want from it.\nFor anyone that might come across this issue:\n\nreader = mir.MultiResolutionImageReader()\nimage = reader.open(self.names[index])\ndef f(x, y, w, h, l, q):\n    img = image.getUCharPatch(x, y, w, h, l)\n    q.put(img)\nfrom multiprocessing import Process, Queue\nq = Queue()\np = Process(target=f, args=x, y, w, h, l,  q))\np.start()\nimg = q.get()\np.join()\np.terminate()\n","z":"I had the same issue and searched for a solution for a long time. I needed to do two things. Delete the forward return after the call (Or transfer to cpu) and wrap some tensors that were being stored, inside torch.autograd.Variables, (I did tbptt that time). This solution seems to me like a hack, because Variables is marked as deprecated and there surely is another approach to do this better. But I realized after I did that, that I didn\u2019t have to delete anything inside the forward method anymore.\nMaybe It will help you solve the problem.\nFYI: It would help to know what exactly you are doing.\nHey there, thanks for the reply.\nWhich tensors would you store as Variables? I\u2019ve tried deleting the forward returns right after but that didn\u2019t help.\nI\u2019ve also noticed something that may be of interest: when watching nvidia-smi I can see that during training the usage goes up to maximum and then back down and does that in cycles whereas during validation the usage is steady at 1\/4 of the available gpu ram.\nThe whole thing that I am trying to develop is a bit complicated but it boils down to: given an image X, have the model predict boundary boxes, use those boxes to extract patches and then recursively do that a number of times. Each patch goes through the same CNN and the extracted features are collected into a list. In the end all the extracted features are concatenated and a fcnn is used for classification.\nA colleague suggested http:\/\/tech.labs.oliverwyman.com\/blog\/2008\/11\/14\/tracing-python-memory-leaks\/ but i am not sure whether it will turn out to be useful. What do you think?\nStoring them as Variables only helps to control the backpropagation Flow. That doesn\u2018t seem to be an issue exept if you are backpropagating through the whole recursive cnns.\nCorrect me if im wrong, but during training the gradients have to be allocated, so its normal that the Memory usage increases.\nAre the extracted Features in the List Stored as cuda tensors? Maybe transferring them with .cpu() helps. You can call them back to cuda if you Need them.\nIts probably not a python memory leak. Have you tried to Train on cpu?\nOn phone so sry for Upper case.\nHi there.\nSo it turns out it\u2019s RAM issue rather than GPU memory issue. I\u2019ve been trying to allocate where is it coming from but no lack in solving it.\nI\u2019ve ran tracemalloc just now and it gives me the following output:\n<code class=\"lang-auto\">10 files allocating the most memory:\n<frozen importlib._bootstrap_external>;:487: size=500 KiB, count=5186, average=99 B \n\n\/home\/nd26\/anaconda3\/envs\/pytorch_env\/lib\/python3.6\/lib2to3\/pgen2\/grammar.py:108: size=345 KiB, count=5489, average=64 B \n\n\/home\/nd26\/anaconda3\/envs\/pytorch_env\/lib\/python3.6\/site-packages\/torch\/nn\/functional.py:1189: size=138 KiB, count=5040, average=28 B \n\n\/home\/nd26\/anaconda3\/envs\/pytorch_env\/lib\/python3.6\/site-packages\/torch\/serialization.py:469: size=98.0 KiB, count=1105, average=91 B \n\n\/home\/nd26\/anaconda3\/envs\/pytorch_env\/lib\/python3.6\/site-packages\/numpy\/lib\/arraypad.py:945: size=92.0 KiB, count=1311, average=72 B \n\n\/home\/nd26\/anaconda3\/envs\/pytorch_env\/lib\/python3.6\/site-packages\/torch\/tensor.py:33: size=84.7 KiB, count=1920, average=45 B \n\n\/home\/nd26\/anaconda3\/envs\/pytorch_env\/lib\/python3.6\/site-packages\/torch\/_utils.py:94: size=78.7 KiB, count=1007, average=80 B \n\n\/home\/nd26\/anaconda3\/envs\/pytorch_env\/lib\/python3.6\/site-packages\/torch\/tensor.py:36: size=65.8 KiB, count=702, average=96 B \n\n\/home\/nd26\/anaconda3\/envs\/pytorch_env\/lib\/python3.6\/site-packages\/torch\/serialization.py:213: size=64.4 KiB, count=687, average=96 B \n\n\/home\/nd26\/anaconda3\/envs\/pytorch_env\/lib\/python3.6\/lib2to3\/pgen2\/grammar.py:122: size=45.9 KiB, count=475, average=99 B\n<\/code>\nAnd, 25 frame traceback:\n<code class=\"lang-auto\">`5036 memory blocks: 317.3 KiB \nFile \/home\/nd26\/anaconda3\/envs\/pytorch_env\/lib\/python3.6\/lib2to3\/pgen2\/grammar.py\", line 108 d = pickle.load(f) \n\nFile \"\/home\/nd26\/anaconda3\/envs\/pytorch_env\/lib\/python3.6\/lib2to3\/pgen2\/driver.py\", line 134 g.load(gp) \n\nFile \"\/home\/nd26\/anaconda3\/envs\/pytorch_env\/lib\/python3.6\/lib2to3\/pgen2\/driver.py\", line 159 return load_grammar(grammar_source) \n\nFile \"\/home\/nd26\/anaconda3\/envs\/pytorch_env\/lib\/python3.6\/lib2to3\/pygram.py\", line 32 python_grammar = driver.load_packaged_grammar(\"lib2to3\", _GRAMMAR_FILE) \n\nFile \"&amp;lt;frozen importlib._bootstrap&amp;gt;\", line 219 \n\nFile \"&amp;lt;frozen importlib._bootstrap_external&amp;gt;\", line 678 \n\nFile \"&amp;lt;frozen importlib._bootstrap&amp;gt;\", line 665 \n\nFile \"&amp;lt;frozen importlib._bootstrap&amp;gt;\", line 955 \n\nFile \"&amp;lt;frozen importlib._bootstrap&amp;gt;\", line 971 \n\nFile \"\/home\/nd26\/anaconda3\/envs\/pytorch_env\/lib\/python3.6\/lib2to3\/fixer_util.py\", line 7 from .pygram import python_symbols as syms \n\nFile \"&amp;lt;frozen importlib._bootstrap&amp;gt;\", line 219 \n\nFile \"&amp;lt;frozen importlib._bootstrap_external&amp;gt;\", line 678 \n\nFile \"&amp;lt;frozen importlib._bootstrap&amp;gt;\", line 665 \n\nFile \"&amp;lt;frozen importlib._bootstrap&amp;gt;\", line 955 \n\nFile \"&amp;lt;frozen importlib._bootstrap&amp;gt;\", line 971 \n\nFile \"\/home\/nd26\/anaconda3\/envs\/pytorch_env\/lib\/python3.6\/lib2to3\/refactor.py\", line 25 from .fixer_util import find_root \n\nFile \"&amp;lt;frozen importlib._bootstrap&amp;gt;\", line 219 \n\nFile \"&amp;lt;frozen importlib._bootstrap_external&amp;gt;\", line 678\n\nFile \"&amp;lt;frozen importlib._bootstrap&amp;gt;\", line 665 \n\nFile \"&amp;lt;frozen importlib._bootstrap&amp;gt;\", line 955 \n\nFile \"&amp;lt;frozen importlib._bootstrap&amp;gt;\", line 971 \n\nFile \"\/home\/nd26\/anaconda3\/envs\/pytorch_env\/lib\/python3.6\/site-packages\/past\/translation\/__init__.py\", line 42 from lib2to3.refactor import RefactoringTool \n\nFile \"&amp;lt;frozen importlib._bootstrap&amp;gt;\", line 219 \n\nFile \"&amp;lt;frozen importlib._bootstrap_external&amp;gt;\", line 678\n\nFile \"&amp;lt;frozen importlib._bootstrap&amp;gt;\", line 665`\n<\/code>\nI am quite clueless as to what should I do next. Any advice?\nI believe I\u2019ve just solved this.\nIt turns out that a specific function was allocating memory that Python wasn\u2019t able to retrieve back and so the accumulation was the cause of this increase. So to solve this I spawn a new process to run the function and terminate it once I get what I want from it.\nFor anyone that might come across this issue:\n\nreader = mir.MultiResolutionImageReader()\nimage = reader.open(self.names[index])\ndef f(x, y, w, h, l, q):\n    img = image.getUCharPatch(x, y, w, h, l)\n    q.put(img)\nfrom multiprocessing import Process, Queue\nq = Queue()\np = Process(target=f, args=x, y, w, h, l,  q))\np.start()\nimg = q.get()\np.join()\np.terminate()\n"},{"x":"Assume that I create two datasets that differ by their \u201cgetitem\u201d protocol (for example, \u201cdataset1\u201d in the code below gives a denoised version of every image in the original dataset and \u201cdataset2\u201d in the code below gives the original version of the image), and want to create a new dataset, which consists of the first 10 images from the first dataset, and the first 10 images from the second dataset. Trying to run the code below indeed gives a new dataset, but it does not preserve all the variables of the original datasets (such as classes, samples, etc\u2026) that are required for the loading part.\nHow can I solve this problem?\nThanks!\n\ndataset1 = DatasetFolder1(parameters)\ndataset2 = DatasetFolder2(parameters)\ndataset = [dataset1[i] for i in range(20) ]\nfor i in [10:20]:\ndataset[i] =dataset2[i]\n","y":"I think you could create both Datasets and pass them to a custom Dataset, which concatenates the samples of both underlying Datasets.\nI\u2019ve created a small example using ImageFolder:\n<code class=\"lang-python\">\ndataset1 = datasets.ImageFolder(\n    root='YOUR_PATH',\n    transform=transforms.ToTensor())\ndataset2 = datasets.ImageFolder(\n    root='YOUR_PATH',\n    transform=transforms.ToTensor())\n\n\nclass MyDataset(Dataset):\n    def __init__(self, dataset1, dataset2):\n        self.dataset1 = dataset1\n        self.dataset2 = dataset2\n        \n    def __getitem__(self, index):\n        x1, y1 = self.dataset1[index]\n        x2, y2 = self.dataset2[index]\n        \n        x = torch.stack((x1, x2))\n        y = torch.stack((torch.tensor(y1), torch.tensor(y2)))\n        \n        return x, y\n    \n    def __len__(self):\n        return len(self.dataset1)\n\ndataset = MyDataset(dataset1, dataset2)\n\nloader = DataLoader(\n    dataset,\n    batch_size=10,\n    shuffle=False,\n    num_workers=2\n)\n\nfor data, target in loader:\n    data = data.view(-1, *data.size()[2:])\n    target = target.view(-1)\n    print(data.shape)\n    print(target.shape)\n<\/code>\nAs you can see, you will double the actual batch size.\nWould that work for you?","z":"If both Datasets store the same data in the noisy and original format, you might need to use the same indices.\nCurrently you are storing the first 20 samples in dataset, then overwrite [10:20] with the \u201csecond batch\u201d of dataset2.\nProbably this would work:\n<code class=\"lang-python\">dataset = [dataset1[i] for i in range(20) ]\nfor i in [10:20]:\n    dataset[i] =dataset2[i-10]\n<\/code>\nAlthough this doesn\u2019t answer my question, you are completely right\u2026 My code overwrites the last 10 samples of \u201cdataset\u201d with the second batch of dataset2, as opposed to what I was trying to do. Thanks for that.\nMy problem is that when I use the classes \u201cDatasetFolder1\u201d and \u201cDatasetFolder2\u201d that are derived from \u201ctorchvision.datasets.DatasetFolder\u201d, the objects \u201cdataset1\u201d and \u201cdataset2\u201d contain several variables (such as \u201cclasses\u201d, \u201cextensions\u201d, \u201csamples\u201d) that are omitted in the new object \u201cdataset\u201d when I use\n\ndataset = [dataset1[i] for i in range(20) ]\n\nAs far as I can see, the problem is that the object \u201cdataset\u201d is no longer of type \u201cDatasetFolder\u201d (it is just a list). Is there anyway of just creating a new \u201cDatasetFolder\u201d instance with some samples coming from dataset1 and some from dataset2?\nHope I made myself clear.\nThanks!\nI think you could create both Datasets and pass them to a custom Dataset, which concatenates the samples of both underlying Datasets.\nI\u2019ve created a small example using ImageFolder:\n<code class=\"lang-python\">\ndataset1 = datasets.ImageFolder(\n    root='YOUR_PATH',\n    transform=transforms.ToTensor())\ndataset2 = datasets.ImageFolder(\n    root='YOUR_PATH',\n    transform=transforms.ToTensor())\n\n\nclass MyDataset(Dataset):\n    def __init__(self, dataset1, dataset2):\n        self.dataset1 = dataset1\n        self.dataset2 = dataset2\n        \n    def __getitem__(self, index):\n        x1, y1 = self.dataset1[index]\n        x2, y2 = self.dataset2[index]\n        \n        x = torch.stack((x1, x2))\n        y = torch.stack((torch.tensor(y1), torch.tensor(y2)))\n        \n        return x, y\n    \n    def __len__(self):\n        return len(self.dataset1)\n\ndataset = MyDataset(dataset1, dataset2)\n\nloader = DataLoader(\n    dataset,\n    batch_size=10,\n    shuffle=False,\n    num_workers=2\n)\n\nfor data, target in loader:\n    data = data.view(-1, *data.size()[2:])\n    target = target.view(-1)\n    print(data.shape)\n    print(target.shape)\n<\/code>\nAs you can see, you will double the actual batch size.\nWould that work for you?\nYour idea worked. Thanks a lot!"},{"x":"I want to repeat the original fast-rcnn performance with Detectron with link \"https:\/\/github.com\/facebookresearch\/Detectron\". Since it does not provide selective search within its code, I obtained proposals of selective search using code here with link \"https:\/\/github.com\/rbgirshick\/fast-rcnn\/blob\/master\/matlab\/fast_rcnn_im_detect.m\".\nAccording to issue with link \"https:\/\/github.com\/facebookresearch\/Detectron\/issues\/164\" here, I could convert these proposals to some file that Detectron requires using the tool file with link \"https:\/\/github.com\/facebookresearch\/Detectron\/blob\/master\/tools\/convert_selective_search.py\": proposals->tool file-> some intermediate file->Detectron.\nMy question is: what parameters should I give to the tool file and Detectron RESPECTIVELY? If possible, examples are preferred.\nThank you in advance.","y":"Hi , thank you for your kind reply. To some extent, my problem was solved at this issue with link \"https:\/\/github.com\/facebookresearch\/Detectron\/issues\/710#issuecomment-431718989\".\nYou can try it using pairs of images and corresponding proposals at here with link \"https:\/\/github.com\/rbgirshick\/fast-rcnn\/tree\/master\/data\/demo\".  Thank you sincerely again.","z":"Sorry for the late reply. I tried to create some code to reproduce the issue.\nWould it be possible to provide your proposals, so that we could work with your proposals and some dummy data to debug this issue?\nHi , thank you for your kind reply. To some extent, my problem was solved at this issue with link \"https:\/\/github.com\/facebookresearch\/Detectron\/issues\/710#issuecomment-431718989\".\nYou can try it using pairs of images and corresponding proposals at here with link \"https:\/\/github.com\/rbgirshick\/fast-rcnn\/tree\/master\/data\/demo\".  Thank you sincerely again.\nI\u2019m glad it\u2019s working now! "},{"x":"I am using torchvision.Transforms to prepare images for a network but when I perform the operation I get strange scaling of the image.\nHere is an image before transforming which is just a numpy array:\n\nand now performing the transform\u2026\n<code class=\"lang-auto\">transform = transforms.Compose([transforms.ToTensor()])\ntensor = transform(image.reshape(*image.shape,1)).float().numpy()\n\nplt.figure()\nplt.imshow(tensor[0,:,:],cmap='jet')\nplt.colorbar()\n<\/code>\n\nIs this because the ToTensor class expects an image of a certain type or am I missing something in the data maybe?\nThanks in advance!","y":"Or you could do\n<code class=\"lang-auto\">img = LOAD_YOUR_IMAGE\nimg += img.min()\nimg *= 255\/img.max()\nimg = np.astype(np.uint8)\n<\/code>\nIf your image is a numpy array (which I assume from your plotting code). This would work for all image ranges and would always use the full range of [0,255]. The disadvantage however would be that if you have some effects with high intensity only in part of the images, they would be scaled differently. Another approach would be to calculate mean and std during your datasets init (would take some time for huge dataset)","z":"ToTensor transforms the image to a tensor with range [0,1].\nThus it already implies some kind of normalization. If you want to use the normalization transform afterwards you should keep in mind that a range of [0,1] usually implies mean and std to be around 0.5 (the real values depend on your data). These are the values you should pass to the normalization transform as mean and std.\n\n\n\n pytorch docs:\n\nConverts a PIL Image or numpy.ndarray (H x W x C) in the range [0, 255] to a torch.FloatTensor of shape (C x H x W) in the range [0.0, 1.0].\n\n\nWhich means you need to have your image in range of [0,255] before. Maybe your loaded image already lies in a range of [0,1]?\nThank makes sense but my colorbar has a range of ~1e36 so it doesn\u2019t seem to be normalising it between 0-1 strangely\nWhat is your image range directly after loading your image (inside your dataset)?\n277.83605046608886 -223.96290534932243\nIf I divide my images by 255 when they are loaded in, then maybe that will work.\nUpdate: That seems to work! I guess the transformation cannot handle data outside the 255 range\nOr you could do\n<code class=\"lang-auto\">img = LOAD_YOUR_IMAGE\nimg += img.min()\nimg *= 255\/img.max()\nimg = np.astype(np.uint8)\n<\/code>\nIf your image is a numpy array (which I assume from your plotting code). This would work for all image ranges and would always use the full range of [0,255]. The disadvantage however would be that if you have some effects with high intensity only in part of the images, they would be scaled differently. Another approach would be to calculate mean and std during your datasets init (would take some time for huge dataset)\ntake care of this issue as well.\n\n\ngithub.com\/pytorch\/vision with link \"https:\/\/github.com\/pytorch\/vision\/issues\/546\"\n\n\n with link \"https:\/\/github.com\/InnovArul\"\nIssue: transforms.ToTensor() for numpy float array in the range of [0.0, 255.0] with link \"https:\/\/github.com\/pytorch\/vision\/issues\/546\"\n\n\n\topened by InnovArul with link \"https:\/\/github.com\/InnovArul\"\n\ton 2018-07-13 with link \"https:\/\/github.com\/pytorch\/vision\/issues\/546\"\n\n\n\n\nI had come across a debugging scenario where the ToTensor() didn't convert the numpy float array in the range of [0.0,...\n\nenhancement\n\n\n\n\n\n\nWould that not be: img -= img.min() instead of +?\nIf I try that I start off with an image:\n\nand then get the following image after the transform:\n\nstrangely with a mean of nan:\n<code class=\"lang-auto\">tensor[0,:,:].mean()\n\nOut: nan\n<\/code>\nwhich is odd as I use tensor[tensor != tensor] = 0 to remove all nan values upon loading the file.\n\n\n\n spacemeerkat:\n\nWould that not be: img -= img.min() instead of +?\n\n\nYou\u2019re right.\nDo you try this before removing NaNs or after?\nCan you post your whole transformation code?\nTo avoid this, I added the conversion to unsigned ints, since ints are expected to be in range [0,255]\nAh I missed the uint8 line as Jupyter Notebooks doesn\u2019t support np.astype but I\u2019ve replaced it with img.astype(np.uint8) and it works perfectly now.\nSo for anyone who comes across this thread, here is the complete code:\n<code class=\"lang-auto\">img[img != img] = 0\nimg -= img.min()\nimg *= 255\/img.max()\nimg = img.astype(np.uint8)\ntransform = transforms.Compose([transforms.ToTensor(),transforms.Normalize([0], [1])])\ntensor = transform(img.reshape(*img.shape,1)).float()\n<\/code>\nAnd the final image after transform:\n\nMany thanks as always "},{"x":"I use tensorboardX.SummaryWriter.add_image to display some feature maps, but  it will regard some negative  number as positive and display confusion.\nFor example:\n\nthat image is a feature map, it have some irregular bright shots and some regular bright circles. Actually, the irregular shot are negtive and less than regular shot, but it looks so bright. Are there some bugs in tensorboardX?","y":"Did you normalize images?","z":"Did you normalize images?\nYou are right. I didn\u2019t normalize the feature maps. I think tensorboardX will normalize it, but it not."},{"x":"Hi there!\nDoes anybody have implementation of Mask R-CNN in PyTorch that has ability to fine-tuning on own dataset?","y":"From the Facebook Research team, just released, support for training on own dataset.\n\n\n\nGitHub with link \"https:\/\/github.com\/facebookresearch\/maskrcnn-benchmark\"\n\n\n\nfacebookresearch\/maskrcnn-benchmark with link \"https:\/\/github.com\/facebookresearch\/maskrcnn-benchmark\"\nFast, modular reference implementation of Semantic Segmentation and Object Detection algorithms in PyTorch. - facebookresearch\/maskrcnn-benchmark\n\n\n\n\n\n","z":"Hi, Oktai,\n\n\nFor inference only, please refer to the below implementation (reproduce).\nhttps:\/\/github.com\/ignacio-rocco\/detectorch\n\n\nFor training\/fine-tuning, you can refer to this implementation, but no benchmark performance has been reported,  meaning that it might has risks in reproducing paper\u2019s results.\nhttps:\/\/github.com\/roytseng-tw\/mask-rcnn.pytorch\n\n\nBest\n thanks a lot!\nDid you try to train on own dataset with second model?\nNot yet, I just saw a guy I stared follow the second implementation.\nThe author of the second implementation posts benchmark evaluation results, seems reasonable.\nFrom the Facebook Research team, just released, support for training on own dataset.\n\n\n\nGitHub with link \"https:\/\/github.com\/facebookresearch\/maskrcnn-benchmark\"\n\n\n\nfacebookresearch\/maskrcnn-benchmark with link \"https:\/\/github.com\/facebookresearch\/maskrcnn-benchmark\"\nFast, modular reference implementation of Semantic Segmentation and Object Detection algorithms in PyTorch. - facebookresearch\/maskrcnn-benchmark\n\n\n\n\n\n"},{"x":"1.PNG823\u00d7613 97.9 KB with link \"https:\/\/discuss.pytorch.org\/uploads\/default\/original\/2X\/a\/a5ff4ed4d0e2602b5eb9aa61c0698ca813983e90.png\"","y":"It seems clear. Is this code snippet right?\n<code class=\"lang-auto\"> \nmyNet=myNet.to(device)\nsig1=torch.tensor(1.0,requires_grad=True, device=device)\nsig2=torch.tensor(1.0,requires_grad=True, device=device)\ncriterion=nn.CrossEntropyLoss()\n\noptimizer=optim.Adam(myNet.parameters(),lr=lr,weight_decay = opt.weight_decay)\nscheduler = optim.lr_scheduler.ReduceLROnPlateau(optimizer, 'min',patience=10)\nopt_sig = torch.optim.SGD([sig1,sig2],lr=1e-4)\n\noptimizer.zero_grad()\nopt_sig.zero_grad()\noutline,outvp=myNet(data)\nlossline=(1.0\/sig1.pow(2))*criterion(outline,mask)+torch.log(sig1.pow(2))\nlossvp=(1.0\/sig2.pow(2))*criterion(outvp,vpmask)+torch.log(sig2.pow(2))\nloss=lossline+lossvp\n\nloss.backward()\noptimizer.step()\nopt_sig.step()\n<\/code>\nCan update the model\u2019s weight and the parameter sigma together with only one optimizer?\n<code class=\"lang-auto\">optimizer=optim.Adam([\n               {'params': myNet.parameters(), 'lr': lr},\n               {'params': [sig1,sig2], 'lr':1e-4}],\n               weight_decay = opt.weight_decay)\n<\/code>","z":"Hi any reason you can\u2019t do gradient descent over the parameter as well? Say the optimizer over your network is opt, make another optimizer for the parameter in question:\n<code class=\"lang-auto\">opt_sig = torch.optim.SGD([sigma],lr=1e-1)\n<\/code>\nand whenever you do a optimizer step for your network, also do a step with opt_sig:\n<code class=\"lang-auto\">opt.zero_grad()\nopt_sig.zero_grad()\nloss.backward()\nopt.step()\nopt_sig.step()\n<\/code>\nIt seems clear. Is this code snippet right?\n<code class=\"lang-auto\"> \nmyNet=myNet.to(device)\nsig1=torch.tensor(1.0,requires_grad=True, device=device)\nsig2=torch.tensor(1.0,requires_grad=True, device=device)\ncriterion=nn.CrossEntropyLoss()\n\noptimizer=optim.Adam(myNet.parameters(),lr=lr,weight_decay = opt.weight_decay)\nscheduler = optim.lr_scheduler.ReduceLROnPlateau(optimizer, 'min',patience=10)\nopt_sig = torch.optim.SGD([sig1,sig2],lr=1e-4)\n\noptimizer.zero_grad()\nopt_sig.zero_grad()\noutline,outvp=myNet(data)\nlossline=(1.0\/sig1.pow(2))*criterion(outline,mask)+torch.log(sig1.pow(2))\nlossvp=(1.0\/sig2.pow(2))*criterion(outvp,vpmask)+torch.log(sig2.pow(2))\nloss=lossline+lossvp\n\nloss.backward()\noptimizer.step()\nopt_sig.step()\n<\/code>\nCan update the model\u2019s weight and the parameter sigma together with only one optimizer?\n<code class=\"lang-auto\">optimizer=optim.Adam([\n               {'params': myNet.parameters(), 'lr': lr},\n               {'params': [sig1,sig2], 'lr':1e-4}],\n               weight_decay = opt.weight_decay)\n<\/code>\nYes, this with link \"https:\/\/pytorch.org\/docs\/stable\/optim.html#per-parameter-options\" seems to verify your approach."},{"x":"I have an unbalanced image dataset with the positive class being 1\/10 of the entire dataset. Classification models trained on this dataset tend to be biased toward the majority class (small false negative rate and bigger false positive rate). I would like to do some augmentation only on the minority class to deal with this. The way I understand, using transforms (random rotation, etc.) when creating torchvision.dataset will augment all of the data so the imbalance remains the same.\nWhat is the best way to go about only augmenting images from a specific class?","y":"You are correct regarding the transformation. The transformation will be applied on the fly on your minority class data.\nYou are also correct regarding the WeightedRandomSampler, if you are keeping the default replacement=True argument.","z":"You could write your own Dataset and apply the transformations in the __getitem__ method.\n<code class=\"lang-auto\">class MyData(Dataset):\n    def __init__(self, data, target, transform=None):\n        self.data = data\n        self.target = target\n        self.transform = transform\n        \n    def __len__(self):\n        return len(self.data)\n    \n    def __getitem__(self, index):\n        x = self.data[index]\n        y = self.target[index]\n        \n        if (y == 0) and self.transform: # check for minority class\n            x = self.transform(x)\n        \n        return x, y\n<\/code>\nEDIT: Another useful approach is to use the WeightedRandomSampler with link \"http:\/\/pytorch.org\/docs\/master\/data.html#torch.utils.data.sampler.WeightedRandomSampler\" and oversample the minority class.\nThank you very much . This will apply random transformations on the images within the minority class without physically changing the size of the minority class, right? I mean, if my dataset has 100 images, 90 of one class and 10 in the other class, I will get the effect of augmentation by iterating many times over it and not by physically having a 90:90 balanced set where 80 minority images were added through transformations. Am I right?\nAlso, oversampling through WeightedRandomSampler just adds exact copies of the minority to reach the target balanced weights, correct?\nYou are correct regarding the transformation. The transformation will be applied on the fly on your minority class data.\nYou are also correct regarding the WeightedRandomSampler, if you are keeping the default replacement=True argument.\n Can you give an example for how to select the weights?\nSure!\nI\u2019ve created some examples here with link \"https:\/\/discuss.pytorch.org\/t\/some-problems-with-weightedrandomsampler\/23242\/2?u=ptrblck\" and here with link \"https:\/\/discuss.pytorch.org\/t\/is-there-a-limit-on-how-disbalanced-a-train-set-can-be\/26334\/6?u=ptrblck\".\nLet me know, if you need some more explanation!"},{"x":"I am having serious speed issues using ImageFolder and DataLoader for feeding my model. I am loading 128x128 png image frames from the KTH dataset stored on my local HDD. Initially the training is relatively fast for a few iterations using about 50% of my CPU but then it crawls to a halt with just 5% CPU usage and very slow loading. I am not doing anything special other than the standard transformations below. I can see that my disk load is around 80% which would indicate my program is IO bound. Is there a recommended way in pytorch to preload large images, resize them and keep the result into memory to be used by the dataloader to avoid being starved by IO operations?\n<code class=\"lang-auto\">input_size = 60\ndevice = \"cuda\"\ntransform = transforms.Compose([torchvision.transforms.Grayscale(), transforms.Resize(input_size), transforms.ToTensor()])\ndataset = torchvision.datasets.ImageFolder('path\\to\\dataset', transform)\ntrain_loader = torch.utils.data.DataLoader(dataset,\n             batch_size=64, shuffle=True, num_workers=4, pin_memory=True)\n\nfor batch_idx, (data, target) in enumerate(train_loader):\n        data, target = data.to(device), target.to(device)\n<\/code>","y":"If your data is stored an a HDD, I doubt you can do a lot to avoid the IO bottleneck besides maybe reducing the batch size. Based on your explanation it seems that your code is IO bound so even pre-calculating the preprocessing would not change anything.\nAn SSD would provide a speedup, if that\u2019s an option.\nAlso, have a look at this thread with link \"https:\/\/discuss.pytorch.org\/t\/how-to-speed-up-the-data-loader\/13740\" about a similar issue. Maybe you could try some suggestions like using an HDF5 file etc.","z":"If your data is stored an a HDD, I doubt you can do a lot to avoid the IO bottleneck besides maybe reducing the batch size. Based on your explanation it seems that your code is IO bound so even pre-calculating the preprocessing would not change anything.\nAn SSD would provide a speedup, if that\u2019s an option.\nAlso, have a look at this thread with link \"https:\/\/discuss.pytorch.org\/t\/how-to-speed-up-the-data-loader\/13740\" about a similar issue. Maybe you could try some suggestions like using an HDF5 file etc."},{"x":"I am performing multi label image classification. I am using DataLoaders for this. How can I see the breakdown of the number of training and test images present for each class?","y":"I assume your labels are saved in a one-hot encoded format for a multi label classification.\nIf that\u2019s the case, you could iterate your Dataset once and just count all class occurrences:\n<code class=\"lang-python\">class MyDataset(Dataset):\n    def __init__(self, num_classes):\n        self.data = torch.randn(100, 2)\n        self.target = torch.empty(100, num_classes, dtype=torch.long).random_(2)\n        \n    def __getitem__(self, index):\n        x = self.data[index]\n        y = self.target[index]\n        return x, y\n        \n    def __len__(self):\n        return len(self.data)\n\n\nnum_classes = 10    \ndataset = MyDataset(num_classes)\nlabels = torch.zeros(num_classes, dtype=torch.long)\n\nfor _, target in dataset:\n    labels += target\n<\/code>","z":"I assume your labels are saved in a one-hot encoded format for a multi label classification.\nIf that\u2019s the case, you could iterate your Dataset once and just count all class occurrences:\n<code class=\"lang-python\">class MyDataset(Dataset):\n    def __init__(self, num_classes):\n        self.data = torch.randn(100, 2)\n        self.target = torch.empty(100, num_classes, dtype=torch.long).random_(2)\n        \n    def __getitem__(self, index):\n        x = self.data[index]\n        y = self.target[index]\n        return x, y\n        \n    def __len__(self):\n        return len(self.data)\n\n\nnum_classes = 10    \ndataset = MyDataset(num_classes)\nlabels = torch.zeros(num_classes, dtype=torch.long)\n\nfor _, target in dataset:\n    labels += target\n<\/code>\nThanks a lot. Seems easy after seeing the solution."},{"x":"I have some images \u201cX\u201d which I pass through a network that outputs some boundary boxes. Based on these boundary boxes I crop \u201cX\u201d and get \u201ccropped X\u201d. The \u201ccropped X\u201d is reduced to grayscale and the mean is computed (per image).\nIn case the mean is < 0.8, I change it to 0, otherwise, it keeps its value. This is then used with a SmoothL1Loss with the hopes that I can train the network to output boundary boxes that contain some content and it\u2019s not blank, i.e. their grayscale mean is lower than 0.8.\nUp until the computation of the boundary boxes, I understand the gradient flow. From there on, I would like some advice\/insights.\nI slice the images \u201cX\u201d based on the boundary boxes, i.e.:\ntemp_x_2 = temp_x[:, int(bbxes[i, j, 2]):int(bbxes[i, j, 2] + bbxes[i, j, 0]),\nint(bbxes[i, j, 3]):int(bbxes[i, j, 3] + bbxes[i, j, 1])]\n(A) Do I need to set the \u201crequires_grad\u201d to True for temp_x? Note that bbxes have requires_grad set to True.\nFollowing that, I manually convert them to grayscale and compute the mean, all using torch operations, i.e.:\ntemp_x_2[0, :, :] = temp_x_2[0, :, :] * 299\/1000\ntemp_x_2[1, :, :] = temp_x_2[1, :, :] * 587\/1000\ntemp_x_2[2, :, :] = temp_x_2[2, :, :] * 114\/1000\ntemp_x_2 = torch.sum(temp_x_2, dim=0)\n\u2026 = torch.mean(temp_x_2)\n(B) Does this look (1) right and (2) efficient? I avoid pillow operations to keep the gradient flow going.\nFinally, when I pass the means into SmoothL1Loss with the target set to 0, I get \u201cleaf variable has been moved into the graph interior\u201d. \u00a9 Why is this the case?\nAny insights &amp; advice would be greatly appreciated!","y":"Hi,\n\nIndexing operation is not differentiable with respect to the index. So no gradient will flow back bbxes. Is that expected?\nIndexing operation is differentiable with respect to the indexed tensor. So if it requires grad, then the output will too, otherwise it won\u2019t. You should not set it by hand.\nThis error occurs because of inplace modifications. I would replace your grascale conversion to tmp_x_2 = temp_x_2[0, :, :] * 299\/1000 + temp_x_2[1, :, :] * 587\/1000 + temp_x_2[2, :, :] * 114\/1000.\n","z":"Hi,\n\nIndexing operation is not differentiable with respect to the index. So no gradient will flow back bbxes. Is that expected?\nIndexing operation is differentiable with respect to the indexed tensor. So if it requires grad, then the output will too, otherwise it won\u2019t. You should not set it by hand.\nThis error occurs because of inplace modifications. I would replace your grascale conversion to tmp_x_2 = temp_x_2[0, :, :] * 299\/1000 + temp_x_2[1, :, :] * 587\/1000 + temp_x_2[2, :, :] * 114\/1000.\n\nUsing affine_grid and grid_sample will give you a behaviour like spatial transformer network. Which works well for some use cases.\nThe learning of the bounding boxes is a hard problem, especially if you don\u2019t have a ground truth bounding box to learn from.\nYou may want to look at different attention mechanisms that exist to find ideas.\nOf course. Thanks a lot, you\u2019ve just saved me a couple of days with that remark!"},{"x":"Hi! Now I want to pass a 5-d tensor to conv layers, where the the tensor is of [k, batch_size, C,H,W]. The later 4 dimension [batch_size,C,H,W] is just the usual 4-D tensor of image input, but this 4-D tensor is copied and expanded k times along the first dimension. Such an input is necessary in models like IWAE. I want to do usual 2D convolution on this tensor, while making sure that the forward pass output for each of the k copies are the same.  I think reshaping it to [k*batch_size,C,H,W] will make the forward output for each copy different, so is there any smart way to implement this?","y":"I think reshaping it to k*batch_size,C,H,W would work. you can check this out with for example batch_size =1, and k=3 , the output would be 3,C,H,W then, and you can verify that 0,:,:,: is the same as 1,:,:,:","z":"I think reshaping it to k*batch_size,C,H,W would work. you can check this out with for example batch_size =1, and k=3 , the output would be 3,C,H,W then, and you can verify that 0,:,:,: is the same as 1,:,:,:"},{"x":"Hi !\nI am creating a ladder like autoencoder network for video prediction.\nI create 2 class : one for the encoder and one for the decoder.\nI need these network be separate during the inference time to access some value.\nbut I need to train the 2 networks with the same loss :\n\nI pass the 2 net parameters to the same optimizer like :\n\n<code class=\"lang-auto\">param_S1 = list(E1.parameters()) + list(D1.parameters())\nnet_optimizer_S1 = torch.optim.SGD(param_S1, lr=0.01, momentum=0.9)\n<\/code>\n\nthen I train the network : The inference work fine, the loss calculation too but when the code get to the .backward() procedure, it gave :\n\n<code class=\"lang-auto\">  File \"RobNET_V2.py\", line 181, in <module>\n    loss_S1.backward()\n  File \"\/usr\/local\/lib\/python3.5\/dist-packages\/torch\/tensor.py\", line 93, in backward\n    torch.autograd.backward(self, gradient, retain_graph, create_graph)\n  File \"\/usr\/local\/lib\/python3.5\/dist-packages\/torch\/autograd\/__init__.py\", line 90, in backward\n    allow_unreachable=True)  # allow_unreachable flag\nRuntimeError: element 0 of tensors does not require grad and does not have a grad_fn\n<\/code>\ncan you help me please ^^\u2019 ?\nmy code :\n<code class=\"lang-auto\">import cv2\nimport numpy as np\n\nimport torch\nimport torch.nn.init as init\nimport torch.nn as nn\nfrom torch import optim\nfrom torchvision import transforms\n\n#-------------------------------------------------------------------------------\n#                           Predictive Network\n#-------------------------------------------------------------------------------\n# Local network size\nINPUT = 32*32\nHIDDEN = 8*8\nOUTPUT = INPUT\n# Readout size\nREAD = HIDDEN\nOUT_READ = 2\n\nclass INPUT_ENCODER(nn.Module):\n    def __init__(self, Encoder_size, Hidden_size):\n        super(INPUT_ENCODER, self).__init__()\n        # first encoder input Xt\n        self.E1 = nn.Linear(Encoder_size, Hidden_size)\n        # secondary feedback input\n        self.ES1 = nn.Linear(Encoder_size, Hidden_size)\n        # recurrent input H1-1\n        self.ER1 = nn.Linear(Hidden_size, Hidden_size)\n        # activation Sigmoid\n        self.ACT = nn.Sigmoid()\n\n    def forward(self, Xt, FB1, Last_H1):\n        # Encoder pass\n        out_E1 = self.E1(Xt)\n        out_ES1 = self.ES1(FB1)\n        out_ER1 = self.ER1(last_H1)\n        sum_H1 = out_E1 + out_ES1 + out_ER1\n        H1 = self.ACT(sum_H1)\n        # output stack\n        return H1\n\nclass LAYER_ENCODER(nn.Module):\n    def __init__(self, Encoder_size, Hidden_size):\n        super(LAYER_ENCODER, self).__init__()\n        # first encoder input Hl-1\n        self.EN = nn.Linear(Hidden_size, Hidden_size)\n        # secondary feedback input\n        self.ESN = nn.Linear(Encoder_size, Hidden_size)\n        # recurrent input Hn-1\n        self.ERN = nn.Linear(Hidden_size, Hidden_size)\n        # activation Sigmoid\n        self.ACT = nn.Sigmoid()\n\n    def forward(self, Ht, FBN, Last_Hn):\n        # Encoder pass\n        out_EN = self.EN(Ht)\n        out_ESN = self.ESN(FBN)\n        out_ERN = self.ERN(Last_Hn)\n        sum_Hn = out_EN + out_ESN + out_ERN\n        Hn = self.ACT(sum_Hn)\n        # output stack\n        return Hn\n\nclass LAYER_DECODER(nn.Module):\n    def __init__(self, Decoder_size, Hidden_size):\n        super(LAYER_DECODER, self).__init__()\n        # first Decoder input Hn\n        self.DN = nn.Linear(Hidden_size, Decoder_size)\n        # secondary superior input\n        self.DSN = nn.Linear(Decoder_size, Decoder_size)\n        # activation Sigmoid\n        self.ACT = nn.Sigmoid()\n\n    def forward(self, Ht, superior_in):\n        # Encoder pass\n        out_DN = self.DN(Ht)\n        out_DSN = self.DSN(superior_in)\n        sum_Yn = out_DN + out_DSN\n        Yn = self.ACT(sum_Yn)\n        # output stack\n        return Yn\n\n#-------------------------------------------------------------------------------\n#                      Motor\/action network\n#-------------------------------------------------------------------------------\nclass Readout(nn.Module):\n    def __init__(self, Hidden_size, external_out):\n        # supervised readout layer for task specific function\n        super(Readout, self).__init__()\n        # Simple 2 linear layers readout\n        self.readout = nn.Linear(Hidden_size, external_out)\n        self.act_readout = nn.Sigmoid()\n\n    def forward(self, input_read):\n        return self.readout(self.act_readout(input_read))\n\n#-------------------------------------------------------------------------------\n#                        Initialisation part :\n#-------------------------------------------------------------------------------\n\n# init stacked autoencoder => 3 stacks (3 encoder \/ 3 decoder)\nE1 = INPUT_ENCODER(INPUT, HIDDEN)\nE2 = LAYER_ENCODER(INPUT, HIDDEN)\nE3 = LAYER_ENCODER(INPUT, HIDDEN)\nD1 = LAYER_DECODER(OUTPUT, HIDDEN)\nD2 = LAYER_DECODER(OUTPUT, HIDDEN)\nD3 = LAYER_DECODER(OUTPUT, HIDDEN)\nprint(E1, E2, E3, D1, D2, D3)\n# init readout for high level command part\n# here we want to control a simple robot by given an UR\/Tau command to\n# the low-level trajectory\/speed\/torque controller\nREADOUT = Readout(HIDDEN, OUT_READ)\nprint(READOUT)\n\n# init the loss function\nloss_function = nn.MSELoss(reduction='elementwise_mean')\n# init stacked AE parameters\nparam_S1 = list(E1.parameters()) + list(D1.parameters())\nparam_S2 = list(E2.parameters()) + list(D2.parameters())\nparam_S3 = list(E3.parameters()) + list(D3.parameters())\n# init the optimization fonction\nnet_optimizer_S1 = torch.optim.SGD(param_S1, lr=0.01, momentum=0.9)\nnet_optimizer_S2 = torch.optim.SGD(param_S2, lr=0.01, momentum=0.9)\nnet_optimizer_S3 = torch.optim.SGD(param_S3, lr=0.01, momentum=0.9)\n#net_optimizer = optim.Adam(RobNET_ALPHA.parameters(), lr=1e-3)\nreadout_optimizer = torch.optim.SGD(READOUT.parameters(), lr=0.01, momentum=0.9)\n\n# init the recurrent part of the network\nlast_H1 = torch.zeros(HIDDEN)\nlast_H2 = torch.zeros(HIDDEN)\nlast_H3 = torch.zeros(HIDDEN)\n# init the feedback\nlast_Y1 = torch.zeros(OUTPUT)\nlast_Y2 = torch.zeros(OUTPUT)\nlast_Y3 = torch.zeros(OUTPUT)\n# no superior input for decoder 3\nSUP_L3 = torch.zeros(OUTPUT)\n\n#-------------------------------------------------------------------------------\n#                     Learning\/inference algorithm :\n#-------------------------------------------------------------------------------\n\ncam = cv2.VideoCapture(0)\n\nwhile(1):\n\n    # zero the parameter gradients\n    net_optimizer_S1.zero_grad()\n    net_optimizer_S2.zero_grad()\n    net_optimizer_S3.zero_grad()\n\n    # capture state Xt\n    ret, frame = cam.read()\n    frame = cv2.cvtColor(frame, cv2.COLOR_BGR2GRAY)\n    frame = cv2.resize(frame, (32,32))\n    Xt = torch.from_numpy(frame)\n    Xt = Xt.type(torch.FloatTensor)\n    Xt = Xt.view(INPUT)\n\n    # forward process\n    # decoder pass\n    H1 = E1.forward(Xt, last_Y1, last_H1)\n    H2 = E2.forward(H1, last_Y2, last_H2)\n    H3 = E3.forward(H2, last_Y3, last_H3)\n    # encoder pass\n    Y3 = D3.forward(H3, SUP_L3)\n    Y2 = D2.forward(H2, Y3)\n    Y1 = D1.forward(H1, Y2)\n\n    # calculate error loss\n    # train first AE\n\n    # TODO: element 0 of tensors does not require grad and does not have a grad_fn\n    # maybe => loss = Variable(loss, requires_grad = True) ?\n\n    loss_S1 = loss_function(last_Y1, Xt)\n    loss_S1.backward()\n    net_optimizer_S1.step()\n    # train second AE\n    loss_S2 = loss_function(last_Y2, Y2)\n    loss_S2.backward()\n    net_optimizer_S2.step()\n    # train third AE\n    loss_S3 = loss_function(last_Y3, Y3)\n    loss_S3.backward()\n    net_optimizer_S3.step()\n\n    # update recurrent element\n    last_H1 = H1\n    last_H2 = H2\n    last_H3 = H3\n    # update feedback\n    last_Y1 = Y1\n    last_Y2 = Y2\n    last_Y3 = Y3\n\n<\/code>","y":"Currently you are passing two tensors to your loss function, which don\u2019t require gradients and are decoupled of your models.\nlast_Y1 is still the zero tensor, while Xt is the image you\u2019ve captured using openCV.\nUsually you would pass some model output as the first input and a target as the second input to your loss function. Could you check, what should be calculated in loss_S1?\nAlso a small side note: you should call the model directly and not the forward method: H1 = E1(Xt, last_Y1, last_H1).","z":"Currently you are passing two tensors to your loss function, which don\u2019t require gradients and are decoupled of your models.\nlast_Y1 is still the zero tensor, while Xt is the image you\u2019ve captured using openCV.\nUsually you would pass some model output as the first input and a target as the second input to your loss function. Could you check, what should be calculated in loss_S1?\nAlso a small side note: you should call the model directly and not the forward method: H1 = E1(Xt, last_Y1, last_H1).\nHi !\nThank you for your answer and the tips.\nI solve my problem by using a variable when I calculate the loss."},{"x":"Hi,\nI\u2019m using Convolutional Autoencoder Network. While training my CPU RAM (30GB) is getting fully used in just 20 epochs but my GPU memory (8GB) is used only 5%.\nHow should I approach to use my GPU better and reduce CPU memory usage.\nThanks","y":"You are storing the computation graph using this line of code:\n<code class=\"lang-python\">running_loss += loss\n<\/code>\nChange it to running_loss += loss.item() and try it again.","z":"would be better if you comment on how you load your data,  if you are using standard data loaders of pytorch,  or writing your own,  what is your data(images,  videos,  text) etc. Basically if you give more info or better your code,  the awesome ppl here would be able to help you out\nHere is my code snippet which is possibly leading to CPU memory increase.\n<code class=\"lang-auto\">def train_cuda(device, dataset_name, audio_channel, data_feats_tensor, blueprint_input, criterion, num_epochs, learning_rates, lr_decay_rate, lr_decay_epoch_size, optim_hyperparams, reg_strengths=0.0, batch_size=1):\n\n    print('Network Initialization.......')\n    model_meenet1 = meenet1.MeeAutoEncoder()\n    model_meenet1.cuda()\n\n    # Initializing network weights and bias\n    model_meenet1.apply(weights_init) \n    loss_history = {}\n    num_train = data_feats_tensor.shape[1]\n\n    for m_lr in learning_rates:\n        loss_history[m_lr] = []\n        \n        for epoch in range(num_epochs):\n            optimizer = torch.optim.Adam(params=model_meenet1.parameters(), \n                                        lr=m_lr,\n                                        betas=(optim_hyperparams['adam']['beta_1'], optim_hyperparams['adam']['beta_2']),\n                                        eps=optim_hyperparams['adam']['epsilon'],\n                                        weight_decay=optim_hyperparams['adam']['weight_decay']\n                                        )\n\n            running_loss = 0.0\n            for i in range(num_train):\n                train_input_tensor = data_feats_tensor[0,i,:,:].cuda().view_as(blueprint_input)    # torch.Size([1, 1, 1025, 15])\n                train_label_tensor = data_feats_tensor[1,i,:,:].cuda().view_as(blueprint_input)    # torch.Size([1, 1, 1025, 15])\n               \n                optimizer.zero_grad()\n                \n                pred_label_tensor = model_meenet1(train_input_tensor)   # torch.Size([1, 1, 1025, 15])\n\n                loss = criterion(pred_label_tensor, train_label_tensor)\n                loss.backward()\n                optimizer.step()\n                \n                running_loss += loss\n\n                # del train_input_tensor\n                # del train_label_tensor\n                # gc.collect()    # garbage collection\n\n            loss_history[m_lr].append(running_loss\/num_train)\n            print('lr = {} : [epoch {}\/{}] : loss = {}'.format(m_lr, epoch, num_epochs, loss_history[m_lr][-1]))\n        \n        gc.collect()        # garbage collection\n\n    return loss_history\n\n##############################################\n<\/code>\nWhen I\u2019m calling this function the argument data_feats_tensor is a cpu tensor (i.e. it is the tensor was previously stored in CPU and now I\u2019m just loading it). Later (s you can see in code above) I\u2019m converting pieces of this tensor into CUDA tensor to use on GPU, but seems like it is never used in GPU.\nHere is a snapshot of my tmux session.\nramissues.jpg1600\u00d7838 843 KB with link \"https:\/\/discuss.pytorch.org\/uploads\/default\/original\/2X\/5\/5629ac47428166ae90c1473b514e9a92bbbeb39a.jpeg\"\nYou are storing the computation graph using this line of code:\n<code class=\"lang-python\">running_loss += loss\n<\/code>\nChange it to running_loss += loss.item() and try it again.\nThanks a lot, . It works like a charm. \nI tried everything but I was just missing loss. "},{"x":"Hi again,\nThis may seems as a stupid question but I have different outputs for the same network every time I train just for the forward pass.\nthe shape of the network looks like this :\n<code class=\"lang-auto\"> \n  self.features = torch.nn.Sequential(\n             # conv1\n             torch.nn.Conv2d(3,64,3,padding=35),\n             torch.nn.ReLU(),\n             torch.nn.Conv2d(64, 64, 3, padding=1),\n             torch.nn.ReLU(),\n             torch.nn.MaxPool2d(2, stride=2),\n             # conv2\n             torch.nn.Conv2d(64, 128, 3, padding=1),\n             torch.nn.ReLU(),\n             torch.nn.Conv2d(128, 128, 3, padding=1),\n             torch.nn.ReLU(),\n             torch.nn.MaxPool2d(2, stride=2),\n             # conv3\n             torch.nn.Conv2d(128, 256, 3, padding=1),\n             torch.nn.ReLU(),\n             torch.nn.Conv2d(256, 256, 3, padding=1),\n             torch.nn.ReLU(),\n             torch.nn.Conv2d(256, 256, 3, padding=1),\n             torch.nn.ReLU()\n )\n self.deconv1 = torch.nn.Sequential(\n             torch.nn.ConvTranspose2d(256, 128, 4, stride=2),\n             torch.nn.ReLU(),\n             torch.nn.ConvTranspose2d(128, 64, 4, stride=2),\n             torch.nn.ReLU(),\n             torch.nn.ConvTranspose2d(64, 1, 3, padding=0,stride=1)\n )\n<\/code>\nThe image go through the features and deconv blocks and then is cropped (slicing) before going through a sigmoid.\nThe ouput is never the same, for the same image.\nAm I not seing something stupid ?\nyours\nJustin","y":"Yes, weight initialization is one crucial step in training a network from scratch.\nPyTorch has a lot of different init functions with link \"http:\/\/pytorch.org\/docs\/master\/nn.html#torch-nn-init\".\nE.g. one popular method for conv layers is xavier_uniform. Depending on your architecture, different weight inits might speed up training of even make it possible.\nHave a look at the weight init section in CS231n with link \"http:\/\/cs231n.github.io\/neural-networks-2\/#init\".","z":"Do you run your model on your GPU?\nCould you post the difference\/error?\nI tried on GPU and CPU, same thing.\nHere are 2 different result obtained for the same  network and same input.\n\n\nCould you post your network definition, so that I could run it on my machine please?\nEDIT: I will just use your snippet from the first post and assume it\u2019s the complete network. \n\nclass VGGNet(nn.Module):\ndef init(self):\n\"\"\u201cSelect conv1_1 ~ conv5_1 activation maps.\u201d\"\"\nsuper(VGGNet, self).init()\nself.select = [15,22,29]\nself.features = torch.nn.Sequential(\n# conv1\ntorch.nn.Conv2d(3,64,3,padding=35),\ntorch.nn.ReLU(),\ntorch.nn.Conv2d(64, 64, 3, padding=1),\ntorch.nn.ReLU(),\ntorch.nn.MaxPool2d(2, stride=2),\n# conv2\ntorch.nn.Conv2d(64, 128, 3, padding=1),\ntorch.nn.ReLU(),\ntorch.nn.Conv2d(128, 128, 3, padding=1),\ntorch.nn.ReLU(),\ntorch.nn.MaxPool2d(2, stride=2),\n# conv3\ntorch.nn.Conv2d(128, 256, 3, padding=1),\ntorch.nn.ReLU(),\ntorch.nn.Conv2d(256, 256, 3, padding=1),\ntorch.nn.ReLU(),\ntorch.nn.Conv2d(256, 256, 3, padding=1),\ntorch.nn.ReLU(),\ntorch.nn.MaxPool2d(2, stride=2),\n# conv4\ntorch.nn.Conv2d(256, 512, 3, padding=1),\ntorch.nn.ReLU(),\ntorch.nn.Conv2d(512, 512, 3, padding=1),\ntorch.nn.ReLU(),\ntorch.nn.Conv2d(512, 512, 3, padding=1),\ntorch.nn.ReLU(),\ntorch.nn.MaxPool2d(2, stride=2),\n# conv5\ntorch.nn.Conv2d(512, 512, 3, padding=1),\ntorch.nn.ReLU(),\ntorch.nn.Conv2d(512, 512, 3, padding=1),\ntorch.nn.ReLU(),\ntorch.nn.Conv2d(512, 512, 3, padding=1),\ntorch.nn.ReLU(),\ntorch.nn.MaxPool2d(2, stride=2)\n)\nself.deconv1 = torch.nn.Sequential(\ntorch.nn.ConvTranspose2d(256, 128, 4, stride=2),\ntorch.nn.ReLU(),\ntorch.nn.ConvTranspose2d(128, 64, 4, stride=2),\ntorch.nn.ReLU(),\ntorch.nn.ConvTranspose2d(64, 1, 3, padding=0,stride=1),\ntorch.nn.ReLU(),\n)\nself.deconv2 = torch.nn.Sequential(\ntorch.nn.ConvTranspose2d(512, 256, 4, stride=2),\ntorch.nn.ReLU(),\ntorch.nn.ConvTranspose2d(256, 128, 4, stride=2),\ntorch.nn.ReLU(),\ntorch.nn.ConvTranspose2d(128, 64, 4, stride=2),\ntorch.nn.ReLU(),\ntorch.nn.ConvTranspose2d(64, 1, 3, padding=0,stride=1),\ntorch.nn.ReLU(),\n)\nself.deconv3 = torch.nn.Sequential(\ntorch.nn.ConvTranspose2d(512, 512, 4, stride=2),\ntorch.nn.ReLU(),\ntorch.nn.ConvTranspose2d(512, 256, 4, stride=2),\ntorch.nn.ReLU(),\ntorch.nn.ConvTranspose2d(256, 128, 4, stride=2),\ntorch.nn.ReLU(),\ntorch.nn.ConvTranspose2d(128, 64, 4, stride=2),\ntorch.nn.ReLU(),\ntorch.nn.ConvTranspose2d(64, 1, 3, padding=0,stride=1),\ntorch.nn.ReLU(),\n)\nself.final_attention_pred = torch.nn.Sequential(\ntorch.nn.ConvTranspose2d(9, 1, 3, stride=1,padding=1)\n)\nself._initialize_weights()\ndef _initialize_weights(self):\n    # initializing weights using ImageNet-trained model from PyTorch\n    for i, layer in enumerate(models.vgg16(pretrained=True).features):\n        if isinstance(layer, torch.nn.Conv2d):\n            self.features[i].weight.data = layer.weight.data\n            self.features[i].bias.data = layer.bias.data\n\n\ndef forward(self, x):\n    ##return list of feature map at different size\n    features = []\n    for i, layer in enumerate(self.features):\n        layer.register_backward_hook(printgradnorm)\n        if(i in self.select ):\n            x = layer(x)\n            features.append(x)\n        else:\n            x = layer(x)\n    for i in self.deconv1:\n        i.register_backward_hook(printgradnorm)\n    \n    for i in self.deconv2:\n        i.register_backward_hook(printgradnorm)\n        \n    for i in self.deconv3:\n        i.register_backward_hook(printgradnorm)\n        \n    self.final_attention_pred[0].register_backward_hook(printgradnorm)\n        \n    saliency = [] \n    m = nn.Sigmoid()\n    m1 = nn.Sigmoid()\n    m2 = nn.Sigmoid()\n    m3 = nn.Sigmoid()\n\n    m.register_backward_hook(printgradnorm)\n    attentionmap1 = self.deconv1(features[0])[:, :, 38:262, 38:262]\n    attentionmap1 = attentionmap1.expand(1, 3, 224, 224)\n    attentionmap2 = self.deconv2(features[1])[:, :, 38:262, 38:262]\n    attentionmap2 = attentionmap2.expand(1, 3, 224, 224)\n    attentionmap3 = self.deconv3(features[2])[:, :, 38:262, 38:262]\n    attentionmap3 = attentionmap3.expand(1, 3, 224, 224)\n    \n    saliency.append(m(attentionmap1))\n    display_image(saliency[0].data.cpu())\n    \n    saliency.append(m1(attentionmap2))\n    #display_image(saliency[0].data.cpu())\n\n    saliency.append(m2(attentionmap3))\n    #display_image(saliency[0].data.cpu())\n\n    output_data = torch.cat(saliency,1)\n    output  = m3(self.final_attention_pred(output_data))\n    return output\n\n\nHave fun ^ ^ you can remove the print functions or if you want it :\n\ndef display_image(input):\nx = input.permute(0,2,3,1)\nx = x.numpy()\nx = np.squeeze(x,axis = 0)\nif(x.shape[2]==1):\nx = np.squeeze(x,axis = 2)\nplt.figure()\nplt.imshow(x, cmap = matplotlib.cm.Greys_r)\n\nThis looks fine on my machine:\nCPU:\n<code class=\"lang-auto\">model = VGGNet()\nx = Variable(torch.randn(1, 3, 224, 224))\noutput = model(x)\no1 = output.clone()\noutput = model(x)\no2 = output.clone()\n(o1 - o2).abs().sum()\n>> Variable containing:\n 0\n[torch.FloatTensor of size 1]\n<\/code>\nGPU:\n<code class=\"lang-auto\">...\n(o1 - o2).abs().sum()\n>> Variable containing:\n1.00000e-06 *\n  7.5102\n[torch.cuda.FloatTensor of size 1 (GPU 0)]\n<\/code>\nmhh strange I tested my input and at various stage of the forward and every time I have something different \nEDIT: the only things which could lead to a problem on my input are :\nnp.expand_dims\ntorch.from_numpy\nj.permute\notherwise I don\u2019t see what could insert randomness in this model \nimage.png1100\u00d7740 109 KB with link \"https:\/\/discuss.pytorch.org\/uploads\/default\/original\/2X\/8\/85720ef73ecf242065f0dbd5ea03f790f5969a16.png\"\n  Alright so if I do one network with same input and try it twice I get the same output at two different stages of the forward. BUT if If recreate the same network and use the same input and compare its ouputs to the previous network, I dont have the same thing  (cf screen shot above\nEdit: if I create two models and check if their weight are the same using :\n\nfor p1, p2 in zip(model1.parameters(), model2.parameters()):\nif p1.data.ne(p2.data).sum() > 0:\nprint(\u201cnop\u201d)\nprint(\u201ctrue\u201d)\n\nI find that their weights are not the same, might be the reason of the difference. What is the correct way to copy weights ?\nYou are initializing all Conv2d layers, but skip the ConvTranspose2d layers, which are initialized using the default function.\nThat is why your models have different weights in the self.deconv modules.\nSo should I set their weights to 0  to produce the same results every time ?\nSetting all weights to zero will create an output containing all zeros.\nIt will be deterministic, but I doubt that\u2019s what you want.\nSince you don\u2019t have pre-trained weights for  the ConvTranspose layers, you could initialize them once in your first model and copy these weights to all other models.\nDo you really need different models with exactly the same weights?\nWell what I really need is that every time I use the model with the same input, my features map are the same once the input have been though the conv and deconv blocks.\nI think this will work using just one model or am I missing something?\nI mean yeah it will definitely works if I always use the same model. (train it, save it, reuse it  etc\u2026).\nBut if I lose the model, or decide to retrain everything from scratch, I will get completely different results, while I use the same model shape and same data.\nOk, what do you think about loading one model, copying the pre-trained weights, initializing all other weights and finally saving this model as your baseline.\nThen you would have to rewrite the initialize_weights function and load all weights from this baseline model.\nThat was my idea yeah. So there is no way to initialize my weights in a way that I can use all the time and any new network ?\nYou could try to set the seed with torch.manual_seed(SEED) before initializing the model.\nCould you try that? I think you would have to reset it every time before creating the model.\nAlright it does indeed works perfectly.\nI never thought about it as I always used an imagenet pretrained network\u2019s weights, but do we always set weights randomly when training a network from scratch ?\nYes, weight initialization is one crucial step in training a network from scratch.\nPyTorch has a lot of different init functions with link \"http:\/\/pytorch.org\/docs\/master\/nn.html#torch-nn-init\".\nE.g. one popular method for conv layers is xavier_uniform. Depending on your architecture, different weight inits might speed up training of even make it possible.\nHave a look at the weight init section in CS231n with link \"http:\/\/cs231n.github.io\/neural-networks-2\/#init\".\nThank you for those explanations, I will try to find the best way for my problem. The paper I am basing my research on uses a Gaussian distribution but I am not sure it will fit my problem perfectly.\nAnyway thanks a lot for you time and answers "},{"x":"I have two tensor:\nA: [8, 256, 32, 32], which is a feature-map extracted from images\nB: [8, 256], which is an intermediate tensor\nI want to perform dot product between each channel vector in A and B and output a tensor of size [8, 1, 32, 32]\nI use:\ntorch.matmul(A.permute(0, 3, 2, 1).contiguous().view(-1, 32*32, 256), B.view(-1, 256, 1)).view(-1, 32, 32).unsqueeze(1)\nAm I correct? Is there any other straight-forward operation in Pytorch to handle this? Thanks in advance~","y":"What about this?\n<code class=\"lang-auto\">(A * B.view(B.shape[0], B.shape[1], 1, 1)).sum(dim=1, keepdim=True)\n<\/code>","z":"What about this?\n<code class=\"lang-auto\">(A * B.view(B.shape[0], B.shape[1], 1, 1)).sum(dim=1, keepdim=True)\n<\/code>\nI test it and it seems correct. Thanks"},{"x":"Hello, I just can\u2019t figure out  the way nn.Conv2d calculate the output . The result calculated from torch is not the same as some machine learning course had taught.\nFor example, likes the code below:\n<code class=\"lang-python\">>> m = torch.nn.Conv2d(1, 1, 3, padding=0)\n>> m(input)\ntensor([[[[ 0.5142,  0.3803,  0.2687],\n          [-0.4321,  1.1637,  1.0675],\n          [ 0.1742,  0.0869, -0.4451]]]], grad_fn=<ThnnConv2DBackward>)\n>> input\ntensor([[[[ 0.7504,  0.1157,  1.4940, -0.2619, -0.4732],\n          [ 0.1497,  0.0805,  2.0829, -0.0925, -1.3367],\n          [ 1.7471,  0.5205, -0.8532, -0.7358, -1.3931],\n          [ 0.1159, -0.2376,  1.2683, -0.0959, -1.3171],\n          [-0.1620, -1.8539,  0.0893, -0.0568, -0.0758]]]])\n>> m.weight\nParameter containing:\ntensor([[[[ 0.2405,  0.3018,  0.0011],\n          [-0.1691, -0.0701, -0.0334],\n          [-0.0429,  0.2668, -0.2152]]]], requires_grad=True)\n\n<\/code>\nfor the left top element 0.5142, it\u2019s not the output equals to\n<code class=\"lang-python\">>> import numpy as np\n>> w = np.array([[0.2405,  0.3018,  0.0011], [-0.1691, -0.0701, -0.0334], [-0.0429,  0.2668, -0.2152]])\n# top-left 3x3 matrix of 5x5\n>> x = np.array([[ 0.7504,  0.1157,  1.4940], [ 0.1497,  0.0805,  2.0829], [1.7471,  0.5205, -0.8532]])\n>> print(np.sum(w*x))\n#  0.364034 != 0.5142\n0.36403412999999996\n<\/code>\nMy Question here is: Why Could the output not equal to 0.5142?\nFurther more, when i add paramter padding into nn.Conv2d, The outcome seems obscure to me as below, thanks a lot for explain that to me.Thank you!\n<code class=\"lang-python\">>> input\ntensor([[[[ 0.7504,  0.1157,  1.4940, -0.2619, -0.4732],\n          [ 0.1497,  0.0805,  2.0829, -0.0925, -1.3367],\n          [ 1.7471,  0.5205, -0.8532, -0.7358, -1.3931],\n          [ 0.1159, -0.2376,  1.2683, -0.0959, -1.3171],\n          [-0.1620, -1.8539,  0.0893, -0.0568, -0.0758]]]])\n# set padding from 0 to 1 equals to (1, 1)\n>> m1 = torch.nn.Conv2d(1, 1, 1, padding=1)\n>> m1(input)\ntensor([[[[0.9862, 0.9862, 0.9862, 0.9862, 0.9862, 0.9862, 0.9862],\n          [0.9862, 1.0771, 1.0002, 1.1672, 0.9544, 0.9288, 0.9862],\n          [0.9862, 1.0043, 0.9959, 1.2385, 0.9749, 0.8242, 0.9862],\n          [0.9862, 1.1978, 1.0492, 0.8828, 0.8970, 0.8174, 0.9862],\n          [0.9862, 1.0002, 0.9574, 1.1398, 0.9745, 0.8266, 0.9862],\n          [0.9862, 0.9665, 0.7615, 0.9970, 0.9793, 0.9770, 0.9862],\n          [0.9862, 0.9862, 0.9862, 0.9862, 0.9862, 0.9862, 0.9862]]]],\n       grad_fn=<ThnnConv2DBackward>)\n<\/code>\nThe confused point is that how 0.9862 be calculated? And what is the default padding strategy in nn.Conv2d?\nThank you for reading and answer!","y":"If you state down the documentation with link \"https:\/\/pytorch.org\/docs\/master\/nn.html#torch.nn.Conv2d\" hard enough, you find that\n\nConv2d has a bias parameter by default that your numpy calculation does not consider (see the formula in the doc),\npadding documented to be is (implicit) zero padding. This matches my experience.\n\nBest regards\nThomas","z":"If you state down the documentation with link \"https:\/\/pytorch.org\/docs\/master\/nn.html#torch.nn.Conv2d\" hard enough, you find that\n\nConv2d has a bias parameter by default that your numpy calculation does not consider (see the formula in the doc),\npadding documented to be is (implicit) zero padding. This matches my experience.\n\nBest regards\nThomas\nThank you very much! i am sorry for my careless."},{"x":"Hi,\nI got the following error when I do transfer learning using inception_v3.\nIndexError: index 1 is out of bounds for dimension 1 with size 1\nHere\u2019s part of my code:\n\nfor epoch in range(num_epochs):\n     for i,(images,labels) in enumerate(train_loader):\n\n             outputs = model(images) ### error in this line\n\n\nThe shape of my input images is [10,1,299,299] (batch size is 10, grayscale image).\nAm I doing wrong with input shape\u2026?? or is it the version issue? My torchvision version is 0.2.1, and pytorch 0.4.1\nMany thanks,","y":"inception_v3 expects a color image (3 channels).\nThe error message seems to be a bit strange and should be:\n<code class=\"lang-python\">RuntimeError: Given groups=1, weight of size [32, 3, 3, 3], expected input[10, 1, 299, 299] to have 3 channels, but got 1 channels instead\n<\/code>\nDid you modify the architecture somehow?\nAnyway, try to repeat your single channel image or cast it directly to RGB after loading:\n<code class=\"lang-python\">image = Image.open(PATH).convert('RGB')\n<\/code>","z":"inception_v3 expects a color image (3 channels).\nThe error message seems to be a bit strange and should be:\n<code class=\"lang-python\">RuntimeError: Given groups=1, weight of size [32, 3, 3, 3], expected input[10, 1, 299, 299] to have 3 channels, but got 1 channels instead\n<\/code>\nDid you modify the architecture somehow?\nAnyway, try to repeat your single channel image or cast it directly to RGB after loading:\n<code class=\"lang-python\">image = Image.open(PATH).convert('RGB')\n<\/code>\nThank you! I just changed my images from grayscale to 3 channels and it did work!\nThe error message was really misleading\u2026\nProblem solved."},{"x":"I have seen two ways to move module or tensor to GPU:\n\nUse the cuda() with link \"https:\/\/goo.gl\/iJbAQh\" method\nUse the to() with link \"https:\/\/goo.gl\/AYY4gr\" method\n\nIs there any difference between these two methods in terms of moving module or tensor to GPU? Which method is preferred?","y":"\n\n\n jdhao:\n\nIs there any difference between these two methods in terms of moving module or tensor to GPU?\n\n\nNo\n\n\n\n jdhao:\n\nWhich method is preferred?\n\n\n.to is preferred because you can define a device object and use it everywhere.","z":"\n\n\n jdhao:\n\nIs there any difference between these two methods in terms of moving module or tensor to GPU?\n\n\nNo\n\n\n\n jdhao:\n\nWhich method is preferred?\n\n\n.to is preferred because you can define a device object and use it everywhere."},{"x":"Sometimes, I found that some source code of pytorch are not listed in pytorch document website:https:\/\/pytorch.org\/docs\/stable\/index.html\nBut I found a Github repo of pytorch here:https:\/\/github.com\/pytorch\nHowever, I think the source code in that repo is not the real source code or the latest version. Some attributes of some class cannot find in source code in that repo.\nFor examples, I can\u2019t find attribute .train_labels in torchvision.dataset.ImageFloder class. But dataset.train_labels is availible which stores each img\u2019s labels.","y":"You are probably looking into the master branch of torchvision, which might differ from your current installed version.\nA few weeks ago there was a small refactoring, which added permanent data and target variables instead of train_data etc.\nHave a look at this diff with link \"https:\/\/github.com\/pytorch\/vision\/commit\/c74b79c83fc99d0b163d8381f7aa1296e4cb23d0#diff-b9008a8c20f5023c90d07d1683a51140\".","z":"You are probably looking into the master branch of torchvision, which might differ from your current installed version.\nA few weeks ago there was a small refactoring, which added permanent data and target variables instead of train_data etc.\nHave a look at this diff with link \"https:\/\/github.com\/pytorch\/vision\/commit\/c74b79c83fc99d0b163d8381f7aa1296e4cb23d0#diff-b9008a8c20f5023c90d07d1683a51140\"."},{"x":"Hello, everyone. I use deeplab-v2-resnet with link \"https:\/\/github.com\/speedinghzl\/Pytorch-Deeplab\" model for image segmentation. But due to the small batch size when training, I want to \u2018freeze\u2019 the parameters of BN layers which are loaded from pretrained  model.  I implement \u2018frozen\u2019 BN as follows:\nWhen training, I set momentum = 0 for all nn.BatchNorm2d, so I think the running mean and running var will keep still. Then I set requires_grad of parameters() of nn.BatchNorm2d false. so I think weight (gamma) and bias (beta) will keep still. I also add the following codes to further check the correctness, I save the parameters of BN layers in first step to critemp, then I check whether the parameters of BN layers of each step temp are unchanged.\n        temp = []\n        critemp = torch.load(\"bn_para.pt\")\n        def frozen_fn(m):\n            classname = m.__class__.__name__\n            if classname.find('BatchNorm2d') != -1:\n                temp.append([np.average(m.running_mean.data.cpu().numpy()),\n                      np.average(m.running_var.data.cpu().numpy()),\n                      np.average(m.weight.data.cpu().numpy()),\n                      np.average(m.bias.data.cpu().numpy())])\n        model.apply(frozen_fn)\n        assert temp == critemp\n\nAnd when testing, I directly use model.eval() and I also ensure that the parameters of BN layers are the same as those when training. But the results are quite terrible. And I change the mode from eval to train. And the results turn out to be much better. However, I think that both parameters of train and eval should be the totally same (I don\u2019t use drop out). But why I still get the different performance??\nCan anyone give me a help? Thanks!","y":"In train mode, you are using batch stats (not running stats). Since you disabled running stats update, in eval mode, the running stats are still the old stats, and since eval mode uses these old running stats, results will be bad.","z":"In train mode, you are using batch stats (not running stats). Since you disabled running stats update, in eval mode, the running stats are still the old stats, and since eval mode uses these old running stats, results will be bad.\nHello, Thanks for your reply! But I think that in in train mode, I set momentum to be 0, and running stats = (1-momentum)*history + momentum*current batch stats, thus running stats should just depend on history. So in either train() mode or eval() mode, they both uses the old (pretrained) running stats.\nAny problem with my understanding??\nBatch norm in training mode uses batch stats, not running stats.\nHello, The following content is directly copied from https:\/\/pytorch.org\/docs\/stable\/nn.html\n\nBy default, during training this layer keeps running estimates of its computed mean and variance, which are then used for normalization during evaluation. The running estimates are kept with a default  momentum  of 0.1.\nIf  track_running_stats  is set to  False , this layer then does not keep running estimates, and batch statistics are instead used during evaluation time as well.\n\nSo does it mean B\uff2e in train mode still uses runnning stats?? Thanks~\nIn training mode, it uses batch stats, i.e., the mean and variance computed using input data only in that batch, not the running average stats. Hope that this clarifies things.\nThanks for your reply! I will directly read the original paper about this and check whether I misunderstand this point. Thanks for your time~\nYes, you are right !!! Thank you very much!"},{"x":"Hi,\nI\u2019m a novice and am learning ML. I have loaded the pytorchvision dataset and trying to visualize the data through iterate method . How do I know that there are only parameters images and labels in the dataset? Where and how do I find this information? Any help is sincerely appreciated.\n<h1>Download and load the test data<\/h1>\ntestset = datasets.MNIST(\u2018MNIST_data\/\u2019, download=True, train=False, transform=transform)\ntestloader = torch.utils.data.DataLoader(testset, batch_size=64, shuffle=True)\n#Iterate\nTestdataiter = iter(trainloader)\nimages,labels =dataiter.next()","y":"You could have a look at the __getitem__ method of your Dataset to see all returned values.","z":"You could have a look at the __getitem__ method of your Dataset to see all returned values."},{"x":"I am trying to Implement Spectral Normalization Paper for GAN. I removed Spectral Normalization\nfrom  Discriminator, just to see its effect on the performance of GAN. Running this way the loss\nof Generator started going haywire. I got to know that, I need to add some regularizer in to\nDiscriminator, so I introduced BN layer, for each CONV layer.\nI am still getting losses such as this:\ndisc loss 0.0 \t\t\tgen loss 15.3624906539917\ndisc loss 0.0 \t\t\tgen loss 20.93044662475586\ndisc loss 0.0 \t\t\tgen loss 21.227508544921875\ndisc loss 9.47e-05 \t\tgen loss 21.953622817993164\ndisc loss 9.21943e-07 \t\tgen loss 19.166460037231445\ndisc loss 0.0 \t\t\tgen loss 11.971349716186523\ndisc loss 3.78e-07 \t\tgen loss 21.656723022460938\ndisc loss 2.293e-08 \t\tgen loss 24.056575775146484\ndisc loss 0.0 \t\t\tgen loss 23.89940643310547\ndisc loss 6.073e-05 \t\tgen loss 20.947172164916992\ndisc loss 1.82e-08 \t\tgen loss 27.6265258789062\nWhere Discriminator Loss (BCE) is reaching 0, where as Generator Loss is high.\nWith Spectral Normalization Loss of both Desc and Generator are in range (0,2).\nThe Observation definitely suggests that Discriminator has become \u2018too strong\u2019. Is\nthere some trick which can be employed.\nRegards,\nNitin BAnsal","y":" I just had a follow up question which is in regards to Inception Score and Frechet Inception Score used to verify the images quality. We usually use an inception model pre-trained on Imagenet. But considering that we use the GAN for CIFAR10\/100 dataset, which is different from Imagenet in terms of distribution, size. which means the Inception model used should be different for CIFAR datasets.\nAny comments or suggestion on this front would be really appreciated.\nRegards,\nNitin","z":"Hi Nitin,\ngreat observation!\nMy very biased (ha) view on this is the following:\nIn fancy talk, the spectral normalization limits the l_2 norm of the operators represented by each layer of the discriminator (i.e.with Euclidean (aka l_2) distance for both the inputs and the targets we have |f(x)-f(y)|_l2 <= C |x-y|_l2, and C could e.g. be one for linear layers). In that sense it is very similar to the original WGAN,which bounded the coefficients and thus limited the l_1-l_infinity norm of each layer (i.e. where you measure the input distance in l_1 and and the output distance in l_infinity). In this sense, one great achievement of the SNGAN authors is that they found an efficient way to match input and output norms in order to have more tight control over the activation norms (the mismatch really means you don\u2019t have good control). In contrast to that, WGAN-GP and SLOGAN with link \"https:\/\/lernapparat.de\/more-improved-wgan\/#slogan\" attempt to enforce a Lipschitz constraint between input and output directly (without resorting to the individual layers). That has the advantage of matching the theoretical interpretation of Wasserstein distance more closely, but the enforcement is much much weaker as it is only on a small sample rather than on the operators directly.\nI can also offer a very simple, self-contained SNGAN notebook with link \"https:\/\/github.com\/t-vi\/pytorch-tvmisc\/blob\/master\/wasserstein-distance\/sn_projection_cgan_64x64_143c.ipynb\" and in the same folder there are toy implementations of WGAN-GP and SLOGAN penalties.\nSo basically, I see the leading options as\n\nimplement a layerwise constraint as in WGAN and SNGAN,\nfor variety, one could try limiting the \u201crowwise\u201d absolute sums, i.e. the\nl_\\infty, l_\\infty - norm of the operators, I\u2019m sure someone tried that and published a paper and invented a new name, but I didn\u2019t look. When you do, be careful about PyTorch transposing weights etc.\nimplement a \u201cfull model\u201d constraint as in WGAN-GP and SLOGAN. People do all sorts of things here, e.g. DRAGAN only limits the norm in a neighbourhood of the data etc.\n\nBest regards\nThomas\nThanks Indeed Thomas!\nThat was an exhaustive explanation! I will definitely go through\nthe resources which you have pointed out! I can definitely  try some\noptions which you have suggested.\nRegards,\nNitin Bansal\n I just had a follow up question which is in regards to Inception Score and Frechet Inception Score used to verify the images quality. We usually use an inception model pre-trained on Imagenet. But considering that we use the GAN for CIFAR10\/100 dataset, which is different from Imagenet in terms of distribution, size. which means the Inception model used should be different for CIFAR datasets.\nAny comments or suggestion on this front would be really appreciated.\nRegards,\nNitin\nI\u2019m not much of an expert for the scoring, but if they are a proxy for \u201cimages quality\u201d, I\u2019d say that while it\u2019s clear that Imagenet has higher quality pictures than CIFAR10\/100, the criterion \u201cquality\u201d would be rather similar (\u201clooking like natural photographs\u201d as much as possible on the resolution). In that sense, I don\u2019t see that as the gravest imperfection of these metrics.\nBest regards\nThomas"},{"x":"I  am trying to pass features from a convolution layer to a Fully Connected layer, however I get a size mismatch error although the sizes are correct and fits a convolution transpose layer (i.e. 224 * 224). I get the following error with a mini-batch size set to 1 for debugging.\nRuntimeError: size mismatch, m1: [224 x 224], m2: [50176 x 1000] at \/opt\/conda\/conda-bld\/pytorch_1524582441669\/work\/aten\/src\/THC\/generic\/THCTensorMathBlas.cu:249\nBelow is the code:\n<code class=\"lang-auto\">        self.labelize = nn.Sequential(\n            # input is outs x 224 x 224\n            nn.Conv2d(outs, 64, kernel_size=1, stride=1),\n            nn.BatchNorm2d(64), nn.LeakyReLU(0.2, inplace=True),\n            # 64 x 224 x 224\n            nn.Conv2d(64, 1, kernel_size=1, stride=1),\n            nn.BatchNorm2d(1), nn.LeakyReLU(0.2, inplace=True),\n            # state size. 1 x 224 x 224\n            nn.Linear(1 * 224 * 224, num_of_classes),\n            )\n<\/code>","y":"Linear layer expects a 2D tensor,  where as conv2d outputs a 4D tensor.  Try to use .view() to change 4D tensor to 2D tensor.","z":"Linear layer expects a 2D tensor,  where as conv2d outputs a 4D tensor.  Try to use .view() to change 4D tensor to 2D tensor.\nThanks\u2026 I was assuming it will automatically convert it to 2D tensor"},{"x":"Hello, I\u2019m working on my first real classifier that isn\u2019t just a demo someone else made. I am at the point where I\u2019m starting the first forward pass with my data into the network during the training phase.\nTo be honest, I\u2019m not quite sure how to determine the values I should use within the various layers of this network. That was part of the incentive of trying to build a classifier myself. My network is currently defined as:\n<code class=\"lang-auto\">\n# Convolutional neural network (two convolutional layers)\nclass ConvNet(nn.Module):\n    def __init__(self, num_classes=3):\n        super(ConvNet, self).__init__()\n        self.layer1 = nn.Sequential(\n            nn.Conv2d(in_channels=3, out_channels=16, kernel_size=5, stride=1, padding=2),\n            nn.BatchNorm2d(num_features=16),\n            nn.ReLU(),\n            #nn.MaxPool2d(kernel_size=2, stride=2))\n            nn.AdaptiveMaxPool2d(output_size=16))\n        self.layer2 = nn.Sequential(\n            nn.Conv2d(in_channels=16, out_channels=32, kernel_size=5, stride=1, padding=2),\n            nn.BatchNorm2d(num_features=32),\n            nn.ReLU(),\n            #nn.MaxPool2d(kernel_size=2, stride=2))\n            nn.AdaptiveMaxPool2d(output_size=7*7*32))\n        self.fc = nn.Linear(in_features=7*7*32, out_features=num_classes)\n        \n    def forward(self, x):\n        out = self.layer1(x)\n        out = self.layer2(out)\n        out = out.reshape(out.size(0), -1)\n        out = self.fc(out)\n        return out\n\nmodel = ConvNet(num_classes).to(device)\nprint(\"Model defined\")\n<\/code>\nFor my first layer, I\u2019m using 3 input channels because my images are all RGB. The other parameter settings were from an example I\u2019m using as reference, but I want to learn how to know what to change them to myself.\nThe stack trace I\u2019m getting is below is a little too cryptic for me to know where to go from here:\n<code class=\"lang-auto\">Traceback (most recent call last):\n  File \"main.py\", line 97, in <module>\n    outputs = model(images)\n  File \"\/usr\/local\/lib\/python3.7\/site-packages\/torch\/nn\/modules\/module.py\", line 477, in __call__\n    result = self.forward(*input, **kwargs)\n  File \"main.py\", line 77, in forward\n    out = self.fc(out)\n  File \"\/usr\/local\/lib\/python3.7\/site-packages\/torch\/nn\/modules\/module.py\", line 477, in __call__\n    result = self.forward(*input, **kwargs)\n  File \"\/usr\/local\/lib\/python3.7\/site-packages\/torch\/nn\/modules\/linear.py\", line 55, in forward\n    return F.linear(input, self.weight, self.bias)\n  File \"\/usr\/local\/lib\/python3.7\/site-packages\/torch\/nn\/functional.py\", line 1024, in linear\n    return torch.addmm(bias, input, weight.t())\nRuntimeError: size mismatch, m1: [2 x 78675968], m2: [1568 x 3] at \/Users\/soumith\/code\/builder\/wheel\/pytorch-src\/aten\/src\/TH\/generic\/THTensorMath.cpp:2070\n<\/code>\nThanks all for any help!\n\nEDIT: After looking at the numbers in the runtime error for a couple hours I realized something.\nI\u2019m rescaling my input images to all be 224 x 224, which is a total of 50,176 pixels.\nIf I divide the first large number in the error message, 78,675,968, by 50,176, it = exactly 1,568, the second large number in the error message. I get the sense that something in my first layer is expecting the input to be of a dimensionality that is close to but not precisely equal to.","y":"\n\n\n static_cat:\n\nnn.AdaptiveMaxPool2d(output_size=7732)) self.fc = nn.Linear(in_features=7732, out_features=num_classes)\n\n\nRealized it was this code I needed to tinker with. Changed it to:\n        nn.AdaptiveMaxPool2d(output_size=6))\n    self.fc = nn.Linear(in_features=1152, out_features=3)\n\nAnd my train and test phases work successfullly! Only 77% accuracy, but I\u2019ll see if I can improve it now.","z":"\n\n\n static_cat:\n\nnn.AdaptiveMaxPool2d(output_size=7732)) self.fc = nn.Linear(in_features=7732, out_features=num_classes)\n\n\nRealized it was this code I needed to tinker with. Changed it to:\n        nn.AdaptiveMaxPool2d(output_size=6))\n    self.fc = nn.Linear(in_features=1152, out_features=3)\n\nAnd my train and test phases work successfullly! Only 77% accuracy, but I\u2019ll see if I can improve it now."},{"x":"Hi all,\nI\u2019m trying to solve a problem of video recognition using 3d cnn\u2019s.\nI want to classify the videos into 6 classes, I tried training an END-TO-END 3d cnn\u2019s model that didn\u2019t give me good results (around 40% accuracy) so I decided to try a different approach and training 6 models of binary classification for each class separately.\nEach individual model out of the 6 models I trained gave me good accuracy and low loss.\nNow I want to ensemble all the 6 models together but the problem is that each of the 6 models has it\u2019s own mean and std normalization.\nMy question is: I want to be able to load a batch of videos and to normalize them according to the model they are about to be processed with - I have 6 models so I need to normalize the videos 6 times.\nWhat is the best way for doing that? Do I need to create a different dataset for each of the models and normalize them accordingly or is there a much more simple and efficient way?\nBest regards,\nYana","y":"Ok, interesting idea.\nSo as far as I understand your approach, each models uses its mean and std, which were calculated on the positive samples for the appropriate class. Am I right?\nDid this approach outperform 6 different models using a global mean and std?\nHowever, you could relocate the standardization into the Dataset returning 6 differently normalized samples.\nThrough this, you could push some computation into a DataLoader, i.e. CPU, while your model ensemble calculates the predictions.\nWhat is the overall accuracy of the model ensemble compared to the first model (~40% accuracy)?","z":"What do you mean by \u201ceach model has its own mean and std normalization\u201d?\nUsually, you normalize the dataset and the statistics do not depend on the model, i.e. the mean and std is calculated from the training data.\nCould you explain a bit more about your use case?\nsure ,\nI\u2019m trying to classify 6 classes for video classification problem. I tried running end-2-end model (i3d) but it gave me bad results (42% accuracy) so I decided to solve 6 different classification problem (binary instead of multi).\n1 model for class A tells me 1 if the it\u2019s class A or 0 if it\u2019s not class A, same for class B i trained a model that predicts 1 if the video belongs to class B or 0 if it\u2019s not etc for all of my 6 classes.\nEach of these 6 models has their own mean and std (calculated on the training set) but i chose to do it a little differently\u2026 I decided for each model to calculate only the mean and std for the specific class - for example for model 1 that predicts class A i divided my training set into positive and negative samples (positive - 1 - all the videos belonging to class A and 0 - all the videos that are not classified as A) - so that\u2019s how i calculated my mean and std for each of my classes.\nI wrote a wrapper Module that takes all these 6 models and inference each one individually and concatenating the results into a Linear layer with 6 output channels (for the 6 classification i want to predict) when running my model in training it takes a lot of time to do the inference and a lot of memory for some reason.\ni\u2019m attaching my code for that:\n<code class=\"lang-auto\">import os,sys,inspect\n\ncurrentdir = os.path.dirname(os.path.abspath(inspect.getfile(inspect.currentframe())))\nparentdir = os.path.dirname(currentdir)\nsys.path.insert(0,parentdir)\n\nimport torch\nfrom torch import nn\nfrom Utils.Transforms import ToTensor\nfrom torch.autograd import Variable\n\n\nclass NetEnsemble(nn.Module):\n\t\"\"\"docstring for NetEnsemble\"\"\"\n\tdef __init__(self, batch_size, use_half, ensemble_models=None, normalizations=[]):\n\t\tsuper(ClarityNetEnsemble, self).__init__()\n\t\tto_tensor = ToTensor()\n\n\t\tself.ensemble_models = nn.ModuleList(ensemble_models)\n\t\tself.log_softmax = nn.LogSoftmax(1)\n\t\tself.fc = nn.Linear(12, 6)\n\n\t\tself.register_buffer('net1_mean', to_tensor(normalizations[0][0]).unsqueeze(0))\n\t\tself.register_buffer('net1_std', to_tensor(normalizations[0][1]).unsqueeze(0))\n\t\tself.register_buffer('net2_mean', to_tensor(normalizations[1][0]).unsqueeze(0))\n\t\tself.register_buffer('net2_std', to_tensor(normalizations[1][1]).unsqueeze(0))\n\t\tself.register_buffer('net3_mean', to_tensor(normalizations[2][0]).unsqueeze(0))\n\t\tself.register_buffer('net3_std', to_tensor(normalizations[2][1]).unsqueeze(0))\n\t\tself.register_buffer('net4_mean', to_tensor(normalizations[3][0]).unsqueeze(0))\n\t\tself.register_buffer('net4_std', to_tensor(normalizations[3][1]).unsqueeze(0))\n\t\tself.register_buffer('net5_mean', to_tensor(normalizations[4][0]).unsqueeze(0))\n\t\tself.register_buffer('net5_std', to_tensor(normalizations[4][1]).unsqueeze(0))\n\t\tself.register_buffer('net6_mean', to_tensor(normalizations[5][0]).unsqueeze(0))\n\t\tself.register_buffer('net6_std', to_tensor(normalizations[5][1]).unsqueeze(0))\n\n\tdef forward(self, x):\n\t\tlog_softmaxs = []\n\t\tsoftmaxs = []\n\n\t\t#normalize x according to the model mean and std\n\t\tx_normalized = (x - Variable(self.net1_mean, volatile=True))\/Variable(self.net1_std, volatile=True)\n\t\t#do inference on the normalized input\n\t\tlog_softmax, softmax = self.ensemble_models[0](x_normalized)\n\t\tlog_softmaxs.append(log_softmax)\n\t\tsoftmaxs.append(softmax)\n\n\t\t#normalize x according to the model mean and std\n\t\tx_normalized = (x - Variable(self.net2_mean, volatile=True))\/Variable(self.net2_std, volatile=True)\n\t\t#do inference on the normalized input\n\t\tlog_softmax, softmax = self.ensemble_models[1](x_normalized)\n\t\tlog_softmaxs.append(log_softmax)\n\t\tsoftmaxs.append(softmax)\n\n\t\t#normalize x according to the model mean and std\n\t\tx_normalized = (x - Variable(self.net3_mean, volatile=True))\/Variable(self.net3_std, volatile=True)\n\t\t#do inference on the normalized input\n\t\tlog_softmax, softmax = self.ensemble_models[2](x_normalized)\n\t\tlog_softmaxs.append(log_softmax)\n\t\tsoftmaxs.append(softmax)\n\n\t\t#normalize x according to the model mean and std\n\t\tx_normalized = (x - Variable(self.net4_mean, volatile=True))\/Variable(self.net4_std, volatile=True)\n\t\t#do inference on the normalized input\n\t\tlog_softmax, softmax = self.ensemble_models[3](x_normalized)\n\t\tlog_softmaxs.append(log_softmax)\n\t\tsoftmaxs.append(softmax)\n\n\t\t#normalize x according to the model mean and std\n\t\tx_normalized = (x - Variable(self.net5_mean, volatile=True))\/Variable(self.net5_std, volatile=True)\n\t\t#do inference on the normalized input\n\t\tlog_softmax, softmax = self.ensemble_models[4](x_normalized)\n\t\tlog_softmaxs.append(log_softmax)\n\t\tsoftmaxs.append(softmax)\n\n\t\t#normalize x according to the model mean and std\n\t\tx_normalized = (x - Variable(self.net6_mean, volatile=True))\/Variable(self.net6_std, volatile=True)\n\t\t#do inference on the normalized input\n\t\tlog_softmax, softmax = self.ensemble_models[5](x_normalized)\n\t\tlog_softmaxs.append(log_softmax)\n\t\tsoftmaxs.append(softmax)\n\n\t\tout = torch.cat(log_softmaxs, 1)\n\t\tout = out.view(out.size(0), -1) #flatten outputs\n\t\tout = self.fc(out)\n\t\t\n\t\tout.volatile = False\n\t\tout.requires_grad = True\n\t\t\n\t\treturn self.log_softmax(out)\n<\/code>\nOk, interesting idea.\nSo as far as I understand your approach, each models uses its mean and std, which were calculated on the positive samples for the appropriate class. Am I right?\nDid this approach outperform 6 different models using a global mean and std?\nHowever, you could relocate the standardization into the Dataset returning 6 differently normalized samples.\nThrough this, you could push some computation into a DataLoader, i.e. CPU, while your model ensemble calculates the predictions.\nWhat is the overall accuracy of the model ensemble compared to the first model (~40% accuracy)?\n, yes you are right!\nYes, the overall accuracy was around ~40% when i used a global mean and std of the entire training set, this works better since it highlights the difference between the videos in my positive and negative samples (Gives me ~75% accuracy for each model so the average is quite good i think).\nI thought about relocating the standardization into the Dataset but was afraid it will blow up my RAM (the machine i\u2019m using for experiments has only 64GB at the moment with 4 GPUs of 1080 ti).\nI tried to perform inference of my 6 models one after the other and then concatenate the results into another module that just have Linear layer that has 6 channels but my i ran out of memory on the GPU and the RAM almost blew up.\nI\u2019m just afraid that my 6 models are too big (i\u2019m using I3D) to use as inference together (maybe i\u2019m wrong) on the gpu.\nI can\u2019t tell the overall accuracy of the ensemble model yet since i\u2019m trying to make it work but I just keep getting out of memory errors.\nI assume you load all your models and push it on different GPUs?\nHow big is each model?\nSince you have 6 models and 4 GPUs, 2 GPUs will have 2 models on them?\nYou are probably running out of memory on these two?\nIf so, you could try to relocate the models after each operation, i.e.:\n<code class=\"lang-auto\">def forward(self, x):\n    ...\n    self.ensemble_models[0].cuda(0)\n    log_softmax, softmax = self.ensemble_models[0](x_normalized)\n    self.ensemble_models[0].cpu()\n    ...\n<\/code>\nThis will obviously slow down your ensemble, so you could try running it on CPU from the beginning.\nI\u2019m still interested in the final performance, since even though each model can predict its own class, the ensemble might fail when for example two models are predicting a very high probability.\nI assume you\u2019ve trained each model separately on its positive\/negative samples.\nI actually loaded all my models using DataParallel - so on all the my 4 GPUs.\nEach model is ~12M parameters.\nI tried to do the inference on the 6 models in the main training loop one after the other and collecting the scores into a tiny model that contains only 1 Linear layer and now my RAM is exploding while my GPUs RAM is releasing.\nHello, did you find a solution to this problem? I\u2019m facing the exact same problem of RAM exploding, while GPU RAM does release"},{"x":"Hello everyone!\nI am new in PyTorch and I tried to implement randomly scaling the input images (e.g. from 0.5 to 1.5) during training which is mentioned in the Deeplab paper. Here is the code.\n<code class=\"lang-auto\">class RandomScale(object):\n    def __init__(self, limit):\n        self.limit = limit\n\n    def __call__(self, sample):\n        img = sample['image']\n        mask = sample['label']\n        assert img.size == mask.size\n\n        scale = random.uniform(self.limit[0], self.limit[1])\n        w = int(scale * img.size[0])\n        h = int(scale * img.size[1])\n\n        img, mask = img.resize((w, h), Image.BILINEAR), mask.resize((w, h), Image.NEAREST)\n\n        return {'image': img, 'label': mask}\n\n# Augmentation\ncomposed_transforms_tr = transforms.Compose([\n        RandomScale((0.5, 1.5)),\n        RandomHorizontalFlip(),\n        ToTensor()])\n<\/code>\nHowever when just started to train there raised an error:\n<code class=\"lang-auto\">RuntimeError: invalid argument 0: Sizes of tensors must match except in dimension 0. Got 665 and 420 in dimension 2 at c:\\new-builder_3\\win-wheel\\pytorch\\aten\\src\\th\\generic\/THTensorMath.cpp:3616\n<\/code>\nThis is because images in the same batch need to be the same size. 665 and 420 in the error above are heights of input images after randomly scaling.\nSo I wonder if there is a way to transform batch of images simultaneously. Or can someone help to provide a suggestion to implement randomly scaling inputs?\nThank you very much!","y":"You could sample the random crop size once for the next batch and resample in each iteration of your DataLoader.\nMaybe not the most elegant approach, but should do the job.\nHere is a small dummy example:\n<code class=\"lang-python\">class MyDataset(Dataset):\n    def __init__(self, limit):\n        self.data = [TF.to_pil_image(x) for x in torch.randn(64, 3, 24, 24)]\n        self.limit = limit\n        self.resample_size(0)\n\n    def resample_size(self, index):\n        size = torch.empty(2).uniform_(self.limit[0], self.limit[1]).long().tolist()\n        self.crop_coord = transforms.RandomCrop.get_params(self.data[index], size)\n    \n    def __getitem__(self, index):\n        i, j, h, w = self.crop_coord\n        img = self.data[index]\n        img = TF.crop(img, i, j, h, w)\n        x = TF.to_tensor(img)\n        return x\n    \n    def __len__(self):\n        return len(self.data)\n\n\ndataset = MyDataset(limit=[18, 22])\nloader = DataLoader(\n    dataset,\n    batch_size=4\n)\n\n\nfor data in loader:\n    loader.dataset.resample_size(0)\n    print(data.shape)\n<\/code>","z":"You could sample the random crop size once for the next batch and resample in each iteration of your DataLoader.\nMaybe not the most elegant approach, but should do the job.\nHere is a small dummy example:\n<code class=\"lang-python\">class MyDataset(Dataset):\n    def __init__(self, limit):\n        self.data = [TF.to_pil_image(x) for x in torch.randn(64, 3, 24, 24)]\n        self.limit = limit\n        self.resample_size(0)\n\n    def resample_size(self, index):\n        size = torch.empty(2).uniform_(self.limit[0], self.limit[1]).long().tolist()\n        self.crop_coord = transforms.RandomCrop.get_params(self.data[index], size)\n    \n    def __getitem__(self, index):\n        i, j, h, w = self.crop_coord\n        img = self.data[index]\n        img = TF.crop(img, i, j, h, w)\n        x = TF.to_tensor(img)\n        return x\n    \n    def __len__(self):\n        return len(self.data)\n\n\ndataset = MyDataset(limit=[18, 22])\nloader = DataLoader(\n    dataset,\n    batch_size=4\n)\n\n\nfor data in loader:\n    loader.dataset.resample_size(0)\n    print(data.shape)\n<\/code>\nI cannot thank you enough for helping me."},{"x":"I am using pretrained Densenet121 from torch vision within my custom nn.module. However, the titled error happens even if I use .cuda(). I tried using .cuda within the init function of CustomNet but it still gives the same error. Please assume I would remove the last layer and replace the classifier of DenseNet with an identity layer before I add the fully-connected layers in the custom net. Thank you very much for your help in advance\n<code class=\"lang-auto\">class DenseNet121(nn.Module):\n    \"\"\"Model modified.\n    The architecture of our model is the same as standard DenseNet121\n    except the classifier layer which has an additional sigmoid function.\n    \"\"\"\n    def __init__(self, out_size):\n        super(DenseNet121, self).__init__()\n        self.densenet121 = torchvision.models.densenet121(pretrained=True)\n        num_ftrs = self.densenet121.classifier.in_features\n        self.in_features = num_ftrs\n        self.densenet121.classifier = nn.Sequential(\n            nn.Linear(num_ftrs, out_size),\n            nn.Sigmoid()\n        )\n\n    def forward(self, x):\n        x = self.densenet121(x)\n        return x\n<\/code>\n<code class=\"lang-auto\">class CustomNet(nn.Module):\n    def __init__(self, dense_net, num_ftrs):\n        super(CustomNet, self).__init__()\n        self.dense_net = dense_net\n        #Fully connected layer1\n        self.fc1 = nn.Linear(num_ftrs, 512)\n        self.fc2 = nn.Linear(512, 512)\n        self.output = nn.Linear(512, 9)\n        self.leakyRelu = nn.LeakyReLU(0.01)\n        \n    def forward(self, x):\n        features = self.dense_net(x)\n        fc1_output = self.leakyRelu(self.fc1(features ))\n        fc2_output = self.leakyRelu(self.fc2(fc1_output))\n        #Softmax?\n        output = self.output(fc2_output)\n        return output\n\n<\/code>\nEdit:\nApparently it\u2019s within the conv layers of densenet121, not sure how to solve this","y":"Did you forget to assign the tensor back?\n<code class=\"lang-python\">tensor = tensor.to('cuda')\n<\/code>","z":"The code looks basically alright.\nOne small side note: I\u2019m wondering why you are using a sigmoid in your DenseNet implementation, but that\u2019s unrelated to your problem.\nCould you post the code where you initialize your model and feed the data?\nThank you for the reply. That is for the implementation of CheXnet on X-ray images which uses a sigmoid. As for the initialisation. I simply instantiate the model and uses .cuda\nEdit: I almost forgot, I do remove the last layer of densenet121 and add an layer that doesn\u2019t change the input\n<code class=\"lang-auto\">class Identity(nn.Module):\n    def __init__(self):\n        super(Identity, self).__init__()\n        \n    def forward(self, x):\n        return x\n<\/code>\n<code class=\"lang-auto\">dense_net = DenseNet121(12)\ndense_net.classifier = nn.Sequential(Identity())\ncustom_net = CustomNet(dense_net, dense_net.in_features)\ncustom_net = custom_net.cuda(0)\n<\/code>\nI even checked the model parameters with is_cuda, they return true\u2026I checked the type of the input which is torch.cuda.FloatTensor. The error occurs within the conv layer of self.features(x) in densenet121. Thanks\nDid you also check the parameters of custom_net.dense_net.densenet121?\nI\u2019m still unsure where this error might come from.\nYes\u2026 I tried the line below and it still returns true\u2026\n<code class=\"lang-auto\">next(custom_net.dense_net.densenet121.parameters()).is_cuda\n<\/code>\nOK, thanks. Could you try to narrow down the location of the error?\nYou said the error is thrown in some conv layers of the densenet.\nCould you check, if all conv layers are on the GPU?\nDid you somehow manipulated the conv layers or changed them in another way?\nThank you for the prompt reply. I have copied torchvision\u2019s code for densenet121 to aid our discussion. I found that the error occurred at self.features(x). Within it, the error is at a \u2018conv2\u2019 of a _DenseLayer. I did not change anything within densenet121 as I intend to use it as a pretrained feature extractor. May I ask how do I check all the layers in this case? Thanks a lot!\n<code class=\"lang-auto\">def densenet121(pretrained=False, **kwargs):\n    r\"\"\"Densenet-121 model from\n    `\"Densely Connected Convolutional Networks\" <https:\/\/arxiv.org\/pdf\/1608.06993.pdf>`_\n    Args:\n        pretrained (bool): If True, returns a model pre-trained on ImageNet\n    \"\"\"\n    model = DenseNet(num_init_features=64, growth_rate=32, block_config=(6, 12, 24, 16),\n                     **kwargs)\n    if pretrained:\n        # '.'s are no longer allowed in module names, but pervious _DenseLayer\n        # has keys 'norm.1', 'relu.1', 'conv.1', 'norm.2', 'relu.2', 'conv.2'.\n        # They are also in the checkpoints in model_urls. This pattern is used\n        # to find such keys.\n        pattern = re.compile(\n            r'^(.*denselayer\\d+\\.(?:norm|relu|conv))\\.((?:[12])\\.(?:weight|bias|running_mean|running_var))$')\n        state_dict = model_zoo.load_url(model_urls['densenet121'])\n        for key in list(state_dict.keys()):\n            res = pattern.match(key)\n            if res:\n                new_key = res.group(1) + res.group(2)\n                state_dict[new_key] = state_dict[key]\n                del state_dict[key]\n        model.load_state_dict(state_dict)\n    return model\n<\/code>\n<code class=\"lang-auto\">class _DenseLayer(nn.Sequential):\n    def __init__(self, num_input_features, growth_rate, bn_size, drop_rate):\n        super(_DenseLayer, self).__init__()\n        self.add_module('norm1', nn.BatchNorm2d(num_input_features)),\n        self.add_module('relu1', nn.ReLU(inplace=True)),\n        self.add_module('conv1', nn.Conv2d(num_input_features, bn_size *\n                        growth_rate, kernel_size=1, stride=1, bias=False)),\n        self.add_module('norm2', nn.BatchNorm2d(bn_size * growth_rate)),\n        self.add_module('relu2', nn.ReLU(inplace=True)),\n        self.add_module('conv2', nn.Conv2d(bn_size * growth_rate, growth_rate,\n                        kernel_size=3, stride=1, padding=1, bias=False)),\n        self.drop_rate = drop_rate\n\n    def forward(self, x):\n        new_features = super(_DenseLayer, self).forward(x)\n        if self.drop_rate > 0:\n            new_features = F.dropout(new_features, p=self.drop_rate, training=self.training)\n        return torch.cat([x, new_features], 1)\n<\/code>\n<code class=\"lang-auto\">class DenseNet(nn.Module):\n    r\"\"\"Densenet-BC model class, based on\n    `\"Densely Connected Convolutional Networks\" <https:\/\/arxiv.org\/pdf\/1608.06993.pdf>`_\n    Args:\n        growth_rate (int) - how many filters to add each layer (`k` in paper)\n        block_config (list of 4 ints) - how many layers in each pooling block\n        num_init_features (int) - the number of filters to learn in the first convolution layer\n        bn_size (int) - multiplicative factor for number of bottle neck layers\n          (i.e. bn_size * k features in the bottleneck layer)\n        drop_rate (float) - dropout rate after each dense layer\n        num_classes (int) - number of classification classes\n    \"\"\"\n\n    def __init__(self, growth_rate=32, block_config=(6, 12, 24, 16),\n                 num_init_features=64, bn_size=4, drop_rate=0, num_classes=1000):\n\n        super(DenseNet, self).__init__()\n\n        # First convolution\n        self.features = nn.Sequential(OrderedDict([\n            ('conv0', nn.Conv2d(3, num_init_features, kernel_size=7, stride=2, padding=3, bias=False)),\n            ('norm0', nn.BatchNorm2d(num_init_features)),\n            ('relu0', nn.ReLU(inplace=True)),\n            ('pool0', nn.MaxPool2d(kernel_size=3, stride=2, padding=1)),\n        ]))\n\n        # Each denseblock\n        num_features = num_init_features\n        for i, num_layers in enumerate(block_config):\n            block = _DenseBlock(num_layers=num_layers, num_input_features=num_features,\n                                bn_size=bn_size, growth_rate=growth_rate, drop_rate=drop_rate)\n            self.features.add_module('denseblock%d' % (i + 1), block)\n            num_features = num_features + num_layers * growth_rate\n            if i != len(block_config) - 1:\n                trans = _Transition(num_input_features=num_features, num_output_features=num_features \/\/ 2)\n                self.features.add_module('transition%d' % (i + 1), trans)\n                num_features = num_features \/\/ 2\n\n        # Final batch norm\n        self.features.add_module('norm5', nn.BatchNorm2d(num_features))\n\n        # Linear layer\n        self.classifier = nn.Linear(num_features, num_classes)\n\n        # Official init from torch repo.\n        for m in self.modules():\n            if isinstance(m, nn.Conv2d):\n                nn.init.kaiming_normal_(m.weight)\n            elif isinstance(m, nn.BatchNorm2d):\n                nn.init.constant_(m.weight, 1)\n                nn.init.constant_(m.bias, 0)\n            elif isinstance(m, nn.Linear):\n                nn.init.constant_(m.bias, 0)\n\n    def forward(self, x):\n        features = self.features(x)\n        out = F.relu(features, inplace=True)\n        out = F.avg_pool2d(out, kernel_size=7, stride=1).view(features.size(0), -1)\n        out = self.classifier(out)\n        return out\n<\/code>\nI can\u2019t find the error. I used your code to create CustomNet and passing an instance of your DenseNet121 to it and it works without an error.\nCould you post a small executable code snippet to reproduce the error?\nThanks for the help. It turns out it\u2019s i didn\u2019t do tensor.to()\u2026\nGood you figured it out! Where was the missing to() call? Haven\u2019t found it so far.\nFor some reasons, it works when I add the to() call for the input data during feedforwarding\u2026a bit confused on why it works\nDid you forget to assign the tensor back?\n<code class=\"lang-python\">tensor = tensor.to('cuda')\n<\/code>\nyes exactly\u2026it was a stupid mistake"},{"x":"Hello, I\u2019m doing a project about semantic segmentation with 2 class using U-net , but my data was unbalanced, so I think maybe use focalloss is a good idea for my project, and I was use BCEloss, so changed the final layer from 1 channel to 2 channels, and use this code focalloss2d with link \"https:\/\/github.com\/doiken23\/pytorch_toolbox\/blob\/master\/focalloss2d.py\",but the loss was unchanged during trainning,  I\u2019m so confused ,I give the label 0 or 1 to the mask, I don\u2019t know where it is wrong. Thanks!\nhere is part of my Dataset\n<code class=\"lang-auto\">normalize = T.Normalize(mean=[0.4, 0.4, 0.4], std=[0.4, 0.4, 0.4])\n            self.transforms4imgs = T.Compose([\n                T.ToTensor(),\n                normalize\n            ])\n            self.transforms4label = T.Compose([\n                T.ToTensor(),\n            ])\n\ndata = cv2.imread(img_path)\nlabel = cv2.imread(label_path, 0)\n_, label = cv2.threshold(label, 100, 255, cv2.THRESH_BINARY)\nlabel = label \/ 255\nlabel = label[:, :, np.newaxis]\ndata = self.transforms4imgs(data)\nlabel = self.transforms4label(label)\n<\/code>","y":"Where did you modify the code? In your first post or the github repo?\nI\u2019m not sure about this line:\nlabel = label[:, :, np.newaxis]\n\nAre you trying to add a batch dimension to your target?\nIf so, the batch dimension should be at dim0, i.e. the first dimension.\nMake sure your label is of type long.\nHere is a small code example using F.cross_entropy:\n<code class=\"lang-python\">batch_size = 5\nnb_classes = 3\n\noutput = torch.randn(batch_size, nb_classes)\ntarget = torch.empty(batch_size, dtype=torch.long).random_(nb_classes)\n\nloss = F.cross_entropy(output, target)\n<\/code>","z":"If you just have two exclusive target classes and would like to use nn.BCELoss, you have to add a nn.Sigmoid() layer as your output layer into your model and also use a target between 0 and 1.\nOtherwise if your use case is a multi-label classification, i.e. both classes can be true at the same time for the same pixel, you could use two outputs for your model.\nThe focal loss implementation seems to use F.cross_entropy internally, so you should remove any non-linearities applied on your model output and pass the 2 channel output directly to your criterion.\nmany thanks! this driving me crazy for two days!! I have modified my code just now,  and is my label data right?\nWhere did you modify the code? In your first post or the github repo?\nI\u2019m not sure about this line:\nlabel = label[:, :, np.newaxis]\n\nAre you trying to add a batch dimension to your target?\nIf so, the batch dimension should be at dim0, i.e. the first dimension.\nMake sure your label is of type long.\nHere is a small code example using F.cross_entropy:\n<code class=\"lang-python\">batch_size = 5\nnb_classes = 3\n\noutput = torch.randn(batch_size, nb_classes)\ntarget = torch.empty(batch_size, dtype=torch.long).random_(nb_classes)\n\nloss = F.cross_entropy(output, target)\n<\/code>\nWhy the ReLU in the output does need to remove when using cross entropy? Thanks\nBecause when you using cross entropy, it contains log_softmax fuction.\nThank you!!! I know!\nSo we have to add ReLU to remove the negative value. But he said we have to remove the ReLU\noh, sorry, I misunderstood, while, I found a view that use leakyRelu(negative_slope = 0.3) is a good idea for sementation, because it persist the negative value, but I don\u2019t know why and it is right.\nGreat. So your code will be\n<code class=\"lang-auto\">self.lrelu = nn.LeakyReLU(negative_slope=0.3, inplace=True)\nx = self.conv(x)\nx = self.lrelu(self.bn(x))\n<\/code>"},{"x":"I am a little confused about the padding setting of the torch.nn.ConvTranspose2d.\nAs I see the document, it seems that the padding of the deconvolution is calculated by some settings of convolution.  padding(deconv) = K(conv) - 1 - padding(conv)\nAn Example:\nIf I want to upsample x3 using the deconvolution,  the setting of the convolution is  kernel = 3,  stride = 2,  padding = 0; According to the previous equation, the padding of the deconvolution should be 2;\nAnd by using the this  equation Hout=(Hin\u22121)\u2217stride[0]\u22122\u2217padding[0]+kernel_size[0]+output_padding[0]\n, we can get the setting  of deconvolution, kernel = 7,  stride = 3,  padding = 2;\nI don\u2019t know if my understanding is right.  Actually, I don\u2019t know if the first equation is required.  If not, the setting of  kernel = 5,  stride = 3,  padding = 1; also meets the second equation.\nAny help would be much appreciated.\nIt would be better if you could provide some related material.","y":"You are right. There are several setting for your ConvTranpose layer to achieve the same result.\nThe same goes for the opposite direction: you can get the same output shape using different settings for a Conv layer.\nThis tutorial with link \"http:\/\/deeplearning.net\/software\/theano\/tutorial\/conv_arithmetic.html#transposed-convolution-arithmetic\" and also these visualizations with link \"https:\/\/github.com\/vdumoulin\/conv_arithmetic\" might make things clearer.","z":"You are right. There are several setting for your ConvTranpose layer to achieve the same result.\nThe same goes for the opposite direction: you can get the same output shape using different settings for a Conv layer.\nThis tutorial with link \"http:\/\/deeplearning.net\/software\/theano\/tutorial\/conv_arithmetic.html#transposed-convolution-arithmetic\" and also these visualizations with link \"https:\/\/github.com\/vdumoulin\/conv_arithmetic\" might make things clearer.\nThis is what I want.\nThanks so much !!!\nIf this is of any help, I wrote a post on ConvTranspose1d which is extendable to 2d simply:\n\n\n\nMedium \u2013 27 Jul 18 with link \"https:\/\/medium.com\/.pdp\/how-pytorch-transposed-convs1d-work-a7adac63c4a5\"\n\n\n\nHow PyTorch Transposed Convs1D Work \u2013 Santi Pdp \u2013 Medium with link \"https:\/\/medium.com\/.pdp\/how-pytorch-transposed-convs1d-work-a7adac63c4a5\"\nWARNING: I\u2019ll be assuming you know what neural networks and convolutional neural networks are. Also, this post is written in PyTorch\u2026\nReading time: 9 min read\n\n\n\n\n\n\nIf anyone in the community sees a misunderstanding I\u2019ll be greateful about pointers "},{"x":"Hi!\nI know how to use ImageFolder to get my training batch from folders using this code\n<code class=\"lang-python\">transform = transforms.Compose([\n    transforms.Resize((224, 224), interpolation=3),\n    transforms.RandomHorizontalFlip(),\n    transforms.ToTensor()\n])\n\nimage_dataset = datasets.ImageFolder(os.path.join(data_dir, 'train'), transform)\ntrain_dataset = torch.utils.data.DataLoader(\n        image_datasets, batch_size=32,\n        shuffle=True, num_workers=16\n    )\n<\/code>\nBut I want to apply two transforms for the same image and get something like\n<code class=\"lang-python\">for img1, img2, label in train_dataset:\n<\/code>\nWhere img1 and img2 are same image with different size. An example below\n<code class=\"lang-python\">im = cv2.imread('img.jpg').reshape((1, 1, 3))\nim2 = cv2.resize(im, (448, 448))\nim = im.transpose((1, 2, 0)).reshape((1, 3, 224, 224))\nim2 = im2.transpose((1, 2, 0)).reshape((1, 3, 448, 448))\n<\/code>\nHow Can I do this?\nThank you","y":"For this you would have to create\/modify a data loader class.\nYou can have a look at this tutorial - Data Loading and Processing Tutorial with link \"https:\/\/pytorch.org\/tutorials\/beginner\/data_loading_tutorial.html\"\nI\u2019ve modified the code from the tutorial to reflect something like you have asked below\n<code class=\"lang-auto\">class ImageDataset(Dataset):\n\n    def __init__(self, csv_file, root_dir, transform=None):\n        \"\"\"\n        Args:\n            csv_file (string): Path to the csv file with annotations.\n            root_dir (string): Directory with all the images.\n            transform (callable, optional): Optional transform to be applied\n                on a sample.\n        \"\"\"\n        self.imgs_frame = pd.read_csv(csv_file)\n        self.root_dir = root_dir\n        self.transform = transform\n\n    def __len__(self):\n        return len(self.landmarks_frame)\n\n    def __getitem__(self, idx):\n        img_name = os.path.join(self.root_dir,\n                                self.imgs_frame.iloc[idx, 0])\n        im = cv2.imread('img.jpg').reshape((1, 1, 3))\n        im2 = cv2.resize(im, (448, 448))\n        im = im.transpose((1, 2, 0)).reshape((1, 3, 224, 224))\n        im2 = im2.transpose((1, 2, 0)).reshape((1, 3, 448, 448))\n        label = self.imgs_frame.iloc[idx, 1:].as_matrix()\n        if self.transform:\n            im = self.transform(im)\n            im2 = self.transform(im2)\n\n        return im1, im2, label\n<\/code>","z":"For this you would have to create\/modify a data loader class.\nYou can have a look at this tutorial - Data Loading and Processing Tutorial with link \"https:\/\/pytorch.org\/tutorials\/beginner\/data_loading_tutorial.html\"\nI\u2019ve modified the code from the tutorial to reflect something like you have asked below\n<code class=\"lang-auto\">class ImageDataset(Dataset):\n\n    def __init__(self, csv_file, root_dir, transform=None):\n        \"\"\"\n        Args:\n            csv_file (string): Path to the csv file with annotations.\n            root_dir (string): Directory with all the images.\n            transform (callable, optional): Optional transform to be applied\n                on a sample.\n        \"\"\"\n        self.imgs_frame = pd.read_csv(csv_file)\n        self.root_dir = root_dir\n        self.transform = transform\n\n    def __len__(self):\n        return len(self.landmarks_frame)\n\n    def __getitem__(self, idx):\n        img_name = os.path.join(self.root_dir,\n                                self.imgs_frame.iloc[idx, 0])\n        im = cv2.imread('img.jpg').reshape((1, 1, 3))\n        im2 = cv2.resize(im, (448, 448))\n        im = im.transpose((1, 2, 0)).reshape((1, 3, 224, 224))\n        im2 = im2.transpose((1, 2, 0)).reshape((1, 3, 448, 448))\n        label = self.imgs_frame.iloc[idx, 1:].as_matrix()\n        if self.transform:\n            im = self.transform(im)\n            im2 = self.transform(im2)\n\n        return im1, im2, label\n<\/code>\nThanks, Problem solved using your link and ImageFolder Implementation,\nI added one transform argument to my custom ImageFolder and return img1, img2, labels\nThanks\nCan you share the custom ImageFolder implementation please.\n\n<code class=\"lang-python\">import torch.utils.data as data\n\nfrom PIL import Image\nimport os\nimport os.path\n\nIMG_EXTENSIONS = [\n   '.jpg', '.JPG', '.jpeg', '.JPEG',\n   '.png', '.PNG', '.ppm', '.PPM', '.bmp', '.BMP',\n]\n\n\ndef is_image_file(filename):\n   return any(filename.endswith(extension) for extension in IMG_EXTENSIONS)\n\ndef find_classes(dir):\n   classes = os.listdir(dir)\n   classes.sort()\n   class_to_idx = {classes[i]: i for i in range(len(classes))}\n   return classes, class_to_idx\n\n\ndef make_dataset(dir, class_to_idx):\n   images = []\n   for target in os.listdir(dir):\n       d = os.path.join(dir, target)\n       if not os.path.isdir(d):\n           continue\n\n       for filename in os.listdir(d):\n           if is_image_file(filename):\n               path = '{0}\/{1}'.format(target, filename)\n               item = (path, class_to_idx[target])\n               images.append(item)\n\n   return images\n\n\ndef default_loader(path):\n   return Image.open(path).convert('RGB')\n\n\nclass ImageFolderLoader(data.Dataset):\n   def __init__(self, root, transform_1=None,\n                transform_2=None, target_transform=None,\n                loader=default_loader):\n       classes, class_to_idx = find_classes(root)\n       imgs = make_dataset(root, class_to_idx)\n\n       self.root = root\n       self.imgs = imgs\n       self.classes = classes\n       self.class_to_idx = class_to_idx\n       self.transform_1 = transform_1\n       self.transform_2 = transform_2\n       self.target_transform = target_transform\n       self.loader = loader\n\n   def __getitem__(self, index):\n       path, target = self.imgs[index]\n       img = self.loader(os.path.join(self.root, path))\n       if self.transform_1 is not None:\n           img1 = self.transform_1(img)\n       if self.transform_2 is not None:\n           img2 = self.transform_2(img)\n       if self.target_transform is not None:\n           target = self.target_transform(target)\n\n       return img1, img2, target\n\n   def __len__(self):\n       return len(self.imgs)\n<\/code>\nAnd to use it;\n<code class=\"lang-python\">data_transforms_1 = transforms.Compose([\n        transforms.Resize((224, 224), interpolation=3),\n        transforms.ToTensor(),\n        transforms.Normalize(mean=[0.485, 0.456, 0.406],\n                             std=[0.229, 0.224, 0.225])\n])\ndata_transforms_2 = transforms.Compose([\n        transforms.Resize((448, 448), interpolation=3),\n        transforms.ToTensor(),\n        transforms.Normalize(mean=[0.485, 0.456, 0.406],\n                             std=[0.229, 0.224, 0.225])\n])\nimage_datasets = ImageFolderLoader(\n        os.path.join(data_dir, 'train_folders'),\n        transform_1=data_transforms_1,\n        transform_2=data_transforms_2\n    )\n\ntrainloader = torch.utils.data.DataLoader(\n        image_datasets, batch_size=32,\n        shuffle=True, num_workers=16\n  )\n<\/code>\nInside your train function\n<code class=\"lang-python\">for i, data in enumerate(trainloader):\n          imgs1, imgs2, labels = data\n           ..........\n<\/code>\nHope it helped\nThanks for the prompt quick reply. It worked for a single datset. Im having two datsets in two different folders. I\u2019m trying to modify your code to do this. Still no luck though. Do you think whether this a good idea to modify your code to do that? Or is there a alternate way?\nYou can follow this example Simultaneously train on two dataset with link \"https:\/\/discuss.pytorch.org\/t\/train-simultaneously-on-two-datasets\/649\""},{"x":"<code class=\"lang-auto\">import torch\nfrom torch import nn\nfrom torch.autograd import Variable\n\ntorch.manual_seed(1)\n\nnet = nn.Sequential(\n    nn.Linear(100, 100),\n    nn.BatchNorm1d(100),\n    nn.Linear(100, 1)\n)\n\nim = Variable(torch.ones(10, 100))\nfor _ in range(10):\n    net.train(True)\n    print(net(im))\nfor _ in range(10):\n    net.train(False)\n    print(net(im))\n<\/code>\nthe first 10 prints output arrays with one value(-8.6313 * 1e-2), and the second 10 prints output arrays with another value (-0.1563 * 1e-2).\nIt is rather strange and confusing.\n\n\nthe first 10 output is the same, is it reasonable? I think the runing_mean and running_var have some initial value and then get updated at each step. so the output should be changing.\n\n\nthe second 10 output is the same, it is reasonable since it\u2019s in eval mode. what is confusing is that it is not the same as the last training output.  I think if we set the model to evaluation mode, it just freezes runing_mean and running_var, which should make the second 10 outputs the same as the last training output.\n\n\nAny ideas? I\u2019m almost driven crazy by this!","y":"On 1: in training mode, BatchNorm DOES NOT use running_mean and running_std to compute the output. It only updates running_mean and running_std values internally. Hence, what you see is exactly what\u2019s expected.\nOn 2: At test time, the output is not the same as the last training output. The output uses the previously updated running_mean and running_std to compute the output. The default momentum value for BatchNorm is 0.9, so after 10 iterations through, it should be roughly in the neighborhood, but not exactly the same. If you run the first for loop for 100 iterations, it\u2019ll probably be very close.\nThere\u2019s also the other problem that im, which is your input has a mean of 1 and a standard deviation of 0. That\u2019s not great to compute and running-update batch statistics.","z":"On 1: in training mode, BatchNorm DOES NOT use running_mean and running_std to compute the output. It only updates running_mean and running_std values internally. Hence, what you see is exactly what\u2019s expected.\nOn 2: At test time, the output is not the same as the last training output. The output uses the previously updated running_mean and running_std to compute the output. The default momentum value for BatchNorm is 0.9, so after 10 iterations through, it should be roughly in the neighborhood, but not exactly the same. If you run the first for loop for 100 iterations, it\u2019ll probably be very close.\nThere\u2019s also the other problem that im, which is your input has a mean of 1 and a standard deviation of 0. That\u2019s not great to compute and running-update batch statistics."},{"x":"Can someone give an idea on how to implement k-means clustering loss in pytorch?\n\nAlso I am using Pytorch nn.mse() loss. Is there a way to add L2 reguarization to this term. In short, if I want to use L2-Reg. loss.","y":"If you use triple backticks (```python) before and just the backtics (```) after your code, it will be well-formatted.\nIn Jupyter:\n<code class=\"lang-auto\">import torch\n\nclass KMeansClusteringLoss(torch.nn.Module):\n    def __init__(self):\n        super(KMeansClusteringLoss,self).__init__()\n\n    def forward(self, encode_output, centroids):\n        assert (encode_output.shape[1] == centroids.shape[1]),\"Dimensions Mismatch\"\n        n = encode_output.shape[0]\n        d = encode_output.shape[1]\n        k = centroids.shape[0]\n\n        z = encode_output.reshape(n,1,d)\n        z = z.repeat(1,k,1)\n\n        mu = centroids.reshape(1,k,d)\n        mu = mu.repeat(n,1,1)\n\n        dist = (z-mu).norm(2,dim=2).reshape((n,k))\n        loss = (dist.min(dim=1)[0]**2).mean() # note that you didn't have the square loss as the equation\n\n        return loss\n\ndef sq_loss_clusters(encode_output, centroids):\n    assert encode_output.size(1) == centroids.size(1), \"Dimension mismatch\"\n    return ((encode_output[:, None]-centroids[None])**2).sum(2).min(1)[0].mean()\nloss_fn = KMeansClusteringLoss()\npts = torch.randn(2000,10)\nmeans = torch.randn(10,10)\nl1 = loss_fn(pts, means)\nl2 = sq_loss_clusters(pts, means)\nassert (l1==l2).all()\n%timeit loss_fn(pts, means)\n%timeit sq_loss_clusters(pts, means)\n<\/code>\nhas 3.07ms for your version and 684\u00b5s for mine, so it\u2019s >4 faster.\nI think it\u2019s more clear what\u2019s going on, too, if you write it more concise (maybe add a comment linking to the equation and commenting that s_ik == 1 if the assignment of point i to cluster k is optimal given fixed centers, or spread it over several lines to be better able to follow - tastes vary and that is fine).\nNote that the equation you posted had the square distance, so I used that.\nBest regards\nThomas","z":"Could you explain, what s_ik is?\nWhich parameter do you want to use the L2 reg. on?\n\nFor the K-means loss, this is what I am doing. I just want to verify if its correct . Or is there any other clean way of doing this?\nimport torch\nclass KMeansClusteringLoss(torch.nn.Module):\ndef __init__(self):\n    super(KMeansClusteringLoss,self).__init__()\n\ndef forward(self, encode_output, centroids):\n    assert (encode_output.shape[1] == centroids.shape[1]),\"Dimensions Mismatch\"\n    n = encode_output.shape[0]\n    d = encode_output.shape[1]\n    k = centroids.shape[0]\n\n    z = encode_output.reshape(n,1,d)\n    z = z.repeat(1,k,1)\n\n    mu = centroids.reshape(1,k,d)\n    mu = mu.repeat(n,1,1)\n\n    dist = (z-mu).norm(2,dim=2).reshape((n,k))\n    loss = dist.min(dim=1)[0].mean()\n\n    return loss\ns_ik is bascially one-hot vector which is 1 if data point i belongs to cluster k.\nAnd for L2-reg. I simply want to implement Ridge Regression: Loss + \\lambda || w ||_2.\nwhere \\lambda would be a hyperparameter and Loss = nn.mse().\nI\u2019d probably not use repeat but let the broadcasting do it\u2019s thing. Also, personally I prefer unsqueeze or indexing (z = z[None, :, :]) over reshape to get the new dimensions.\nBest regards\nThomas\n Can you please explain a bit on how exactly should this be done? And is there some advantage of using broadcasting over repeat ?\nIf you use triple backticks (```python) before and just the backtics (```) after your code, it will be well-formatted.\nIn Jupyter:\n<code class=\"lang-auto\">import torch\n\nclass KMeansClusteringLoss(torch.nn.Module):\n    def __init__(self):\n        super(KMeansClusteringLoss,self).__init__()\n\n    def forward(self, encode_output, centroids):\n        assert (encode_output.shape[1] == centroids.shape[1]),\"Dimensions Mismatch\"\n        n = encode_output.shape[0]\n        d = encode_output.shape[1]\n        k = centroids.shape[0]\n\n        z = encode_output.reshape(n,1,d)\n        z = z.repeat(1,k,1)\n\n        mu = centroids.reshape(1,k,d)\n        mu = mu.repeat(n,1,1)\n\n        dist = (z-mu).norm(2,dim=2).reshape((n,k))\n        loss = (dist.min(dim=1)[0]**2).mean() # note that you didn't have the square loss as the equation\n\n        return loss\n\ndef sq_loss_clusters(encode_output, centroids):\n    assert encode_output.size(1) == centroids.size(1), \"Dimension mismatch\"\n    return ((encode_output[:, None]-centroids[None])**2).sum(2).min(1)[0].mean()\nloss_fn = KMeansClusteringLoss()\npts = torch.randn(2000,10)\nmeans = torch.randn(10,10)\nl1 = loss_fn(pts, means)\nl2 = sq_loss_clusters(pts, means)\nassert (l1==l2).all()\n%timeit loss_fn(pts, means)\n%timeit sq_loss_clusters(pts, means)\n<\/code>\nhas 3.07ms for your version and 684\u00b5s for mine, so it\u2019s >4 faster.\nI think it\u2019s more clear what\u2019s going on, too, if you write it more concise (maybe add a comment linking to the equation and commenting that s_ik == 1 if the assignment of point i to cluster k is optimal given fixed centers, or spread it over several lines to be better able to follow - tastes vary and that is fine).\nNote that the equation you posted had the square distance, so I used that.\nBest regards\nThomas\n Thank you so much. I will try this out.\nCould you also guide on how to use L2-regularization with Mean Squared Error loss?\nWell, either use the weight decay of the optimizers (which adds a multiple the derivative of |w|\u00b2 to the gradient) or sum |w|\u00b2 yourself (reg_loss = sum([p**2 for p in m.parameters()] or so should work).\nBest regards\nThomas\n\nWhat is the suggested way of doing this and why?\nI\u2019m not an expert on this, my rule of thumb would be to use built-in weight decay for efficiency if it works for you.\nBut really, I don\u2019t have any profound insights to offer here.\nBest regards\nThomas"},{"x":"Hi, Now i need to do a serial temporal feature extraction,and the input is a sequence shown as following.\n\u65b0\u5efa Microsoft Visio \u7ed8\u56fe (2).png981\u00d7465 6.66 KB with link \"https:\/\/discuss.pytorch.org\/uploads\/default\/original\/2X\/1\/1e0414251e1e153d7619bca894d813d6bb8df8e8.png\"\neach feature map is with the shape of (25,25,1) where 1 is the number of the channel. And we have T frames in a sequence,so the total shape of a sequence is (T,25,25,1). What i want to do is that process LSTM to each pixel in the feature map along the time axis and output a single map with shape of (25,25,1) as the yellow one shown. Using pseudo-code to describe the process.\n<code class=\"lang-python\">pixels_features = []\nfor each_row in range(25):\n  for each_col in range(25):\n    pixels_feature,(hn,cn) = LSTM(pixels[:,each_row, each_col,0], (h0,c0))\n    pixels_features.append(hn)\n<\/code>\nThe code can work in pytorch but it is very time-consuming for it is a serial computing. I wonder if there is a parallel solution for this problem? I don\u2019t know if i describe the problem well, please let me know if you have any question about my problem.\nThansk for any suggestion ","y":"This problem is inherently serial if you use LSTM\/GRU.\nThere is an alternative which is claimed to be much faster: SRU.\nTraining RNNs as Fast as CNNs (https:\/\/arxiv.org\/abs\/1709.02755)\nPyTorch code: https:\/\/github.com\/taolei87\/sru","z":"This problem is inherently serial if you use LSTM\/GRU.\nThere is an alternative which is claimed to be much faster: SRU.\nTraining RNNs as Fast as CNNs (https:\/\/arxiv.org\/abs\/1709.02755)\nPyTorch code: https:\/\/github.com\/taolei87\/sru\nThanks for your rapid reply. I think the forward process is possible to be parallel however it is hard  to be parallel in backward to update the params in LSTM. Is it right?\nI think so.\nTruncated backpropagation through time is used as a remedy.\nI do not know of a better solution.\nThanks for your suggestions, i will take SRU for a try:)"},{"x":"Sorry to bother you , I read the document and\n\ntorchvision.transforms.ToTensor : Converts a PIL Image or numpy.ndarray (H x W x C) in the range [0, 255] to a torch.FloatTensor of shape (C x H x W) in the range [0.0, 1.0].\n\nwell, my flow matrix various from negative number to positive number, I use cv2.calcOpticalFlowFarneback fuction to calculate it, how can I fix this?\nThanks for your reply!","y":"it\u2019s a difficult question in case of optical flow.\nFirst of all, depending on which net are you using normalization you need vary a lot. If you are using both displacement components you have to apply the same mapping for both, not to affect the vector. You could use just the magnitude.\nYou should check some paper about optical flow normalization. Btw you can try mapping the whole OF range into [0,1] or [-1,1]\n<code class=\"lang-auto\">def rescale(x,max_range,min_range):\n    max_val = np.max(x)\n    min_val = np.min(x)\n    return (max_range-min_range)\/(max_val-min_val)*(x-max_val)+max_range\n<\/code>\nConsider you have to do this operation to the whole batch. OF is a displacement map. This way you are linearly going from whatever interval to a bounded interval but you are lossing the scale information. If your batch is big enough max and min value will be approximately the same for each batch and this scale info won\u2019t be lost.\nThe best you can do is checking some paper about it.","z":"you can use torch.from_numpy() stament.\nhttps:\/\/pytorch.org\/docs\/stable\/torch.html#torch.from_numpy\nThis allows you to convert numpy arrays into tensors.\nMany thanks!  What\u2019 more, the flow matrix varied a lot ,so the mean value of channel doesn\u2019t convergence to a certain value, so how can I normalize this matrix?\nThanks!\nit\u2019s a difficult question in case of optical flow.\nFirst of all, depending on which net are you using normalization you need vary a lot. If you are using both displacement components you have to apply the same mapping for both, not to affect the vector. You could use just the magnitude.\nYou should check some paper about optical flow normalization. Btw you can try mapping the whole OF range into [0,1] or [-1,1]\n<code class=\"lang-auto\">def rescale(x,max_range,min_range):\n    max_val = np.max(x)\n    min_val = np.min(x)\n    return (max_range-min_range)\/(max_val-min_val)*(x-max_val)+max_range\n<\/code>\nConsider you have to do this operation to the whole batch. OF is a displacement map. This way you are linearly going from whatever interval to a bounded interval but you are lossing the scale information. If your batch is big enough max and min value will be approximately the same for each batch and this scale info won\u2019t be lost.\nThe best you can do is checking some paper about it.\nthank a lot!  Have a nice day!"},{"x":"Hi,\nWhen fine-tuning using the Inception V3 shipped with pytorch-examples, I encountered the RuntimeError like follows:\n<code class=\"lang-auto\">RuntimeError: Expected tensor for argument #1 'input' to have the same dimension as tensor for 'result'; but 4 does not equal 2 (while checking arguments for cudnn_convolution)\n<\/code>\nI inserted code:\n<code class=\"lang-auto\">if args.arch == 'inception_v3':\n        num_aux_in = model.module.AuxLogits.fc.in_features\n        print(num_aux_in)\n        model.module.AuxLogits.fc = nn.Linear(num_aux_in, NUM_CLASSES)\n        num_final_in = model.module.fc.in_features\n        print(num_final_in)\n        model.module.fc = nn.Linear(num_final_in, NUM_CLASSES)\n<\/code>\nI\u2019ve searched around and found some issues on github:\n\n#1 with link \"https:\/\/github.com\/pytorch\/pytorch\/issues\/4884\"\n#2 with link \"https:\/\/github.com\/pytorch\/examples\/issues\/362\"\n\nThey pointed this problem to Probably the size of the tensor you want to process becomes smaller than the kernel size at some point. However, I\u2019ve done similar task using vgg16, with the same routine and the same dataset\u2026\nAny suggestions?\nThanks in advance!\nP.S: The following is the full Traceback:\n<code class=\"lang-auto\">Traceback (most recent call last):\n  File \"main_ft.py\", line 387, in <module>\n    main()\n  File \"main_ft.py\", line 220, in main\n    train(train_loader, model, criterion, optimizer, epoch)\n  File \"main_ft.py\", line 258, in train\n    output = model(input)\n  File \"\/home\/x\/.local\/lib\/python3.6\/site-packages\/torch\/nn\/modules\/module.py\", line 491, in __call__\n    result = self.forward(*input, **kwargs)\n  File \"\/home\/x\/.local\/lib\/python3.6\/site-packages\/torch\/nn\/parallel\/data_parallel.py\", line 114, in forward\n    outputs = self.parallel_apply(replicas, inputs, kwargs)\n  File \"\/home\/x\/.local\/lib\/python3.6\/site-packages\/torch\/nn\/parallel\/data_parallel.py\", line 124, in parallel_apply\n    return parallel_apply(replicas, inputs, kwargs, self.device_ids[:len(replicas)])\n  File \"\/home\/x\/.local\/lib\/python3.6\/site-packages\/torch\/nn\/parallel\/parallel_apply.py\", line 65, in parallel_apply\n    raise output\n  File \"\/home\/x\/.local\/lib\/python3.6\/site-packages\/torch\/nn\/parallel\/parallel_apply.py\", line 41, in _worker\n    output = module(*input, **kwargs)\n  File \"\/home\/x\/.local\/lib\/python3.6\/site-packages\/torch\/nn\/modules\/module.py\", line 491, in __call__\n    result = self.forward(*input, **kwargs)\n  File \"\/home\/x\/.local\/lib\/python3.6\/site-packages\/torchvision\/models\/inception.py\", line 109, in forward\n    aux = self.AuxLogits(x)\n  File \"\/home\/x\/.local\/lib\/python3.6\/site-packages\/torch\/nn\/modules\/module.py\", line 491, in __call__\n    result = self.forward(*input, **kwargs)\n  File \"\/home\/x\/.local\/lib\/python3.6\/site-packages\/torchvision\/models\/inception.py\", line 308, in forward\n    x = self.conv1(x)\n  File \"\/home\/x\/.local\/lib\/python3.6\/site-packages\/torch\/nn\/modules\/module.py\", line 491, in __call__\n    result = self.forward(*input, **kwargs)\n  File \"\/home\/x\/.local\/lib\/python3.6\/site-packages\/torchvision\/models\/inception.py\", line 325, in forward\n    x = self.conv(x)\n  File \"\/home\/x\/.local\/lib\/python3.6\/site-packages\/torch\/nn\/modules\/module.py\", line 491, in __call__\n    result = self.forward(*input, **kwargs)\n  File \"\/home\/x\/.local\/lib\/python3.6\/site-packages\/torch\/nn\/modules\/conv.py\", line 301, in forward\n    self.padding, self.dilation, self.groups)\nRuntimeError: Expected tensor for argument #1 'input' to have the same dimension as tensor for 'result'; but 4 does not equal 2 (while checking arguments for cudnn_convolution)\n<\/code>","y":"The input size should be 299 instead of 224, and there are two losses in the network: a final fc and an intermediate aux_logits. One should notice these difference and adjust the crop_size in data_loader and the computation of metric. Cf. here with link \"https:\/\/github.com\/ahirner\/pytorch-retraining\/blob\/master\/retrain_benchmark_bees.ipynb\"","z":"The input size should be 299 instead of 224, and there are two losses in the network: a final fc and an intermediate aux_logits. One should notice these difference and adjust the crop_size in data_loader and the computation of metric. Cf. here with link \"https:\/\/github.com\/ahirner\/pytorch-retraining\/blob\/master\/retrain_benchmark_bees.ipynb\""},{"x":"Hello all,\ni\u2019m trying to freeze all parameters of my model. \u201cparam.required grad = False\u201d is very simple and powerful way that most of developer accept, but i failed to confirm the effect of that.\nFor the simplest test to check whether freezing is works, first i initailze model and assign param.requried grad as False. i run train function for the model, and visualize very few param from the model.\nI expect visualized params won\u2019t changed at all because i freeze the parameters, but the param keep changing when the loop is going on one batch to another. The belows are very psuedocode for my work, and what terminal shows.\nanybody has idea??\nThank you.\nmodel = init.model()\nfor param in model.parameters():\n    param.required_grad = False\n\noptimizer = torch.optim.RSMprop(model.parameters(), lr=args.lr, momentum=args.momentum,weight_decay = args.weight.decay)\n\nmodel.train()\nfor i (inputs, target, meta) in enumerate(train_loader):\n    outputs=model(inputs)\n    loss = criterion(inputs,target)\n    optimizer.zero_grad()\n    loss.backward()\n    optimizer.step()\n\n    ## visualize very few params\n    for param in model.parameters():\n        print(param[0])\n        break\n\nAt the terminal shows like\nProcessing |#######                         | (509\/2225) Data: 0.052407s | Batch: 1.401s | Total: 0:09:21 | ETA: 0:32:52 | Loss: 0.0322 | Acc:  0.0236Variable containing:\n(0 ,.,.) =\n1.00000e-02 *\n-5.1111  4.8789  7.2060  4.0981  1.5398 -1.9935  2.5382\n-6.8706 -1.9746  5.5094 -1.3251  0.4653  3.0574 -0.6753\n5.6140 -7.1054  6.3074  6.6501 -1.3995  6.1489  1.1402\n0.1733  3.8403  2.5650 -3.3059 -1.5468  6.1374 -2.1767\n8.3077 -0.2914 -7.0128  1.2817  7.1735 -6.6335 -1.9335\n-1.6674 -3.0738  2.7778 -3.6561 -0.6739  3.6340  3.9714\n7.2121 -0.4230  1.4884 -6.0607  2.8108  8.2189  3.3358\nProcessing |#######                         | (510\/2225) Data: 0.015237s | Batch: 1.283s | Total: 0:09:22 | ETA: 0:33:58 | Loss: 0.0321 | Acc:  0.0236Variable containing:\n(0 ,.,.) =\n1.00000e-02 *\n-5.1005  4.8870  7.2142  4.1049  1.5466 -1.9869  2.5436\n-6.8591 -1.9656  5.5182 -1.3171  0.4710  3.0624 -0.6712\n5.6252 -7.0960  6.3153  6.6564 -1.3954  6.1527  1.1435\n0.1828  3.8488  2.5727 -3.2997 -1.5428  6.1408 -2.1743\n8.3194 -0.2824 -7.0051  1.2889  7.1788 -6.6295 -1.9307\n-1.6547 -3.0628  2.7874 -3.6472 -0.6671  3.6397  3.9762\n7.2225 -0.4128  1.4972 -6.0499  2.8183  8.2252  3.3417","y":"Optimizer can\u2019t take parameters with requires_grad=False. It will throw an error. But in your case, you haven\u2019t really freezed the parameters. There is a typo in the code. It should be param.requires_grad=False not param.required_grad. Since your parameters still has requires_grad=True, it won\u2019t throw an error when creating an optimizer. Consequently, It will also optimize the parameters.\nI am surprised why it didn\u2019t throw an error (I checked, this doesn\u2019t throw error in 0.4.0 as well). Also, it\u2019s RMSprop not RSMprop.\n can you please have a look?","z":"btw, training code i have is operating correctly. i try to perform experiment based on the codes.\nwhich version of pytorch are you using?\nIn the version I am using (0.4.0), the optimizers do not accept the parameters with requires_grad=False. We have to manually filter out the params before passing it to the optimizer.\n\n\ngithub.com\/pytorch\/pytorch with link \"https:\/\/github.com\/pytorch\/pytorch\/issues\/679\"\n\n\n with link \"https:\/\/github.com\/alykhantejani\"\nIssue: Allow optimizers to skip nn.Parameters that have requires_grad=False with link \"https:\/\/github.com\/pytorch\/pytorch\/issues\/679\"\n\n\n\topened by alykhantejani with link \"https:\/\/github.com\/alykhantejani\"\n\ton 2017-02-02 with link \"https:\/\/github.com\/pytorch\/pytorch\/issues\/679\"\n\n\n\tclosed by apaszke with link \"https:\/\/github.com\/apaszke\"\n\ton 2017-02-03 with link \"https:\/\/github.com\/pytorch\/pytorch\/issues\/679\"\n\n\nI am trying to implement a gaussian blur as a convolution layer in a network, where the weights do not change....\n\n\n\n\n\n\n\nYou can either do this in your code:\noptimizer = torch.optim.RSMprop(filter(lambda p: p.requires_grad, model.parameters()), lr=args.lr, momentum=args.momentum, weight_decay = args.weight.decay)\nor update the pytorch version.\nOptimizer can\u2019t take parameters with requires_grad=False. It will throw an error. But in your case, you haven\u2019t really freezed the parameters. There is a typo in the code. It should be param.requires_grad=False not param.required_grad. Since your parameters still has requires_grad=True, it won\u2019t throw an error when creating an optimizer. Consequently, It will also optimize the parameters.\nI am surprised why it didn\u2019t throw an error (I checked, this doesn\u2019t throw error in 0.4.0 as well). Also, it\u2019s RMSprop not RSMprop.\n can you please have a look?\nSetting required_grad does not through an error as it just creates a new attribute for the Tensor.\nFor optimizer only accepting params that requires_grad, I guess that depend on the optimizer but I don\u2019t think there are any strong reason why it cannot. It might use a bit more memory than it should but for most simple optimizer that should be the worst thing that can happen.\nOh, thanks. Can you give us an example of optimizer taking freezed parameters? I couldn\u2019t think of any.\nIn the latest version at least, both optim.SGD and optim.Adam accept parameters with requires_grad=False. All parameters with requires_grad=False will have 0 gradients and so with plain SGD they won\u2019t change. Note though that if there is l2 regularization or some momentum terms already saved for these parameters, they will change! Having requires_grad=False only means from the optimizer point of view that the gradients will be 0. The parameters might still be changed.\ni\u2019m noticed that 0.2.0 is also generate error when put a model whose all parameters have \u201crequires_grad=False\u201d.\nand\nthe way you introduced(using \u201cfilter\u201d) really helps for me. Thank you very much.\nExactly. Thank you for finding a mistake i made.\nYeb. \u201crequired_grad\u201d doesn\u2019t make an error. OH MY GOD. You comments are really help to understand what it really mean \u201crequires_grade=False\u201d\nThank you"},{"x":"according to the documentation https:\/\/pytorch.org\/docs\/stable\/torchvision\/models.html , pretrained models are pretrained on ImageNet. But I\u2019d like to know exactly the year of the ImageNet dataset used.\nDoes it matter? Or imagenets are roughly the same?","y":"They are trained on ImageNet-12:\n\n\n\n\nUsing vision.models with the CIFAR dataset? with link \"https:\/\/discuss.pytorch.org\/t\/using-vision-models-with-the-cifar-dataset\/7278\/2?u=ptrblck\" vision with link \"\/c\/vision\"\n\n\n    The pre-trained models are all based on Imagenet-12. Most of the model definitions also need 224x224 as the minimum input size. For CIFAR, you are better off with other repos.\n  \n\n","z":"They are trained on ImageNet-12:\n\n\n\n\nUsing vision.models with the CIFAR dataset? with link \"https:\/\/discuss.pytorch.org\/t\/using-vision-models-with-the-cifar-dataset\/7278\/2?u=ptrblck\" vision with link \"\/c\/vision\"\n\n\n    The pre-trained models are all based on Imagenet-12. Most of the model definitions also need 224x224 as the minimum input size. For CIFAR, you are better off with other repos.\n  \n\n"},{"x":"Hi,\nFor a given input of size (batch, channels, width, height) I would like to apply a 2-strided convolution with a single fixed 2D-filter to each channel of each batch, resulting in an output of size (batch, channels, width\/2, height\/2).\nUsing the group parameter of nn.functional.conv2d I came up with this solution:\nI would like to apply the filter\n<code class=\"lang-auto\">fil = torch.tensor([\n    [0.5,  0.5],\n    [-0.5, -0.5]])\n<\/code>\nto my input\nX = torch.rand(32, 2048, 128, 128).\nTo this end, I add two dummy dimensions (out_channels and in_channels\/groups) to my filter and expand the 0th dimension of my filter tensor to be equal to the number of channels of my input (in this case 2048). I\u2019m keeping the 1st dimension unchanged since in_channels\/groups will be equal to 1 by using groups=in_channels in nn.functional.conv2d.\nfil_tensor = fil[None, None, :, :].expand(X.size(1), -1, -1, -1)\nThis works:\n<code class=\"lang-auto\">res = torch.nn.functional.conv2d(\n    X, fil_tensor, stride=2, groups=X.size(1))\n<\/code>\nbut I\u2019m worried about the step where I expanded my filter, basically creating 2048 copies of redundant information. Is there a better way to do this?\nThanks!","y":"I think it would be faster to reshape your input, so that your channels are stacked in the batch dimension.\n[batch_size, channels, h, w] would become [batch_size * channels, 1, h, w].\nThen you could use a conv layer with in_channels=1 and out_channels=1 and reshape the output again.\n<code class=\"lang-python\">batch_size = 10\nchannels = 3\nh, w = 24, 24\nx = torch.randn(batch_size, channels, h, w)\n\nconv = nn.Conv2d(1, 1, 4, 2, 1)\noutput = conv(x.view(-1, 1, h, w)).view(batch_size, channels, h\/\/2, w\/\/2)\nprint(output.shape)\n<\/code>","z":"I think it would be faster to reshape your input, so that your channels are stacked in the batch dimension.\n[batch_size, channels, h, w] would become [batch_size * channels, 1, h, w].\nThen you could use a conv layer with in_channels=1 and out_channels=1 and reshape the output again.\n<code class=\"lang-python\">batch_size = 10\nchannels = 3\nh, w = 24, 24\nx = torch.randn(batch_size, channels, h, w)\n\nconv = nn.Conv2d(1, 1, 4, 2, 1)\noutput = conv(x.view(-1, 1, h, w)).view(batch_size, channels, h\/\/2, w\/\/2)\nprint(output.shape)\n<\/code>\nThis is exactly what I was looking for! Appreciate it!\nJust for fun,\nI think you can also do it with:\n\nAverage pooling with kernel [1, 2] and stride [1, 2].\nFlip the sign of every other row\nSum every pair of rows.\n\nI don\u2019t think that\u2019s going to be more efficient than  's solution though \u2026\nQuite an interesting approach. Haven\u2019t thought about it and wanted to try it out.\nNot \u201coptimized\u201d code, but the error seems to show the results are equal (up to float precision):\n<code class=\"lang-python\">batch_size = 10\nchannels = 3\nh, w = 24, 24\nx = torch.randn(batch_size, channels, h, w)\n\n# View approach\nconv = nn.Conv2d(1, 1, 2, 2, bias=False)\nwith torch.no_grad():\n    conv.weight = nn.Parameter(torch.tensor([[[[0.5, 0.5],\n                                               [-0.5, -0.5]]]]))\noutput = conv(x.view(-1, 1, h, w)).view(batch_size, channels, h\/\/2, w\/\/2)\n\n# Pool approach\npool = nn.AvgPool2d((1, 2), (1, 2))\noutput_ = pool(x)\n\noutput_[:, :, 1::2, :] = output_[:, :, 1::2, :] * -1\noutput_ = torch.cat([output_[:, :, a:a+1, :] + output_[:, :, a+1:a+2, :] for a in range(0, h, 2)], dim=2)\n\nprint(torch.sum(output.abs() - output_.abs()))\n<\/code>\nWell advanced indexing is not my thing, but it works well indeed (might even be more efficient that the conv  :\n<code class=\"lang-auto\"># Pool approach without advanced indexing\npool = nn.AvgPool2d((1, 2), (1, 2))\noutput_2 = pool(x)\n\noutput_2 = output_2.view(batch_size, channels, h\/\/2, 2, w\/\/2)\noutput_2.select(3, 1).mul_(-1)\noutput_2 = output_2.sum(3)[0]\n<\/code>\nAwesome! Thanks for this approach. "},{"x":"def save_checkpoint(state, is_best, filename='checkpoint.pth.tar'):\n    torch.save(state, filename)\n    if is_best:\n        shutil.copyfile(filename, 'model_best.pth.tar')\n\n\nsave_checkpoint({\n                'epoch': epoch + 1,\n                'arch': args.arch,\n                'state_dict': model.state_dict(),\n                'best_prec1': best_prec1,\n                'optimizer': optimizer.state_dict()\n            }, is_best)\n\nI am saving my model like this. How can I load back the model so that I can use it in other places, like cnn visualization?\nThis is how I am loading the model now:\ntorch.load('model_best.pth.tar')\n\nBut when I do this, I get this error:\n\nAttributeError: \u2018dict\u2019 object has no attribute \u2018eval\u2019\n\nWhat am I missing here???","y":"You have to create a model instance and then load the saved weights as statdict:\n<code class=\"lang-auto\">model = MyModel() \nmodel.load_state_dict(torch.load('model_best.pth.tar')['state_dict'])\n<\/code>\nThe statedict itself is only a dict containing the tensor names and the corresponding weights. It has no information of the model\u2019s structure.","z":"You have to create a model instance and then load the saved weights as statdict:\n<code class=\"lang-auto\">model = MyModel() \nmodel.load_state_dict(torch.load('model_best.pth.tar')['state_dict'])\n<\/code>\nThe statedict itself is only a dict containing the tensor names and the corresponding weights. It has no information of the model\u2019s structure."},{"x":"In the event that a classification model is being trained on a large amount of data (~3,000,000 input images per epoch), what is the recommended approach for implementing checkpoint with link \"https:\/\/discuss.pytorch.org\/t\/torch-utils-checkpoint-checkpoint\/16827\/3\"-like functionality at the mini-batch level, instead of the epoch level (as shown here with link \"https:\/\/discuss.pytorch.org\/t\/saving-and-loading-a-model-in-pytorch\/2610\/3\")? Can anyone recommend a way to save the weights and gradients after every x mini-batches (instead of every x epochs)? Any code snippets, or MCVE\u2019s with link \"https:\/\/stackoverflow.com\/help\/mcve\" would be greatly appreciated. I am not running out of GPU memory. However, prior to code profiling I am experiencing hours worth of training time for a single epoch (in part due to the size of the training data). So this is the motivation for saving and restoring the training process at the mini-batch level prior to an entire epoch being completed.\nI have read the documentation for checkpoints with link \"https:\/\/pytorch.org\/docs\/stable\/checkpoint.html#torch-utils-checkpoint\" which seems to suggest that this is possible on the mini-batch level. I have also read the discussion on torch.utils.checkpoint.checkpoint with link \"https:\/\/discuss.pytorch.org\/t\/torch-utils-checkpoint-checkpoint\/16827\/3\" which seems to suggest that this is not the proper use of a checkpoint. Recommendations for the appropriate methodology (if available) are greatly appreciated! I have experimented a little bit with forward and backward hooks with link \"https:\/\/pytorch.org\/tutorials\/beginner\/former_torchies\/nn_tutorial.html#forward-and-backward-function-hooks\" and can see how this might be used to accomplish the functionality I desire, but I am wondering if there is a better alternative.\nCan I just deepcopy the: model.state_dict(), epoch, mini_batch_number, mini_batch_size, and optimizer.state_dict() after x mini-batch\u2019s and save and restore the checkpoint as normal? Or is there anything else to watch out for?\nI am relatively new to PyTorch and certainty new to the forums, so please let me know if this is the improper place to ask this question, or if this post can be improved. Constructive criticism is always appreciated!","y":"Neither the model nor the optimizer use the epoch or batch count.\nIt\u2019s up to you to store and load the model checkpoints.\nThe model and optimizer just use .load_state_dict() to restore their parameters.\nIf you can work with epoch + minibatch, it\u2019s fine. Alternatively you could somehow store the epoch as a fractal, i.e. for half the mini-batches in the epoch, just save 'epoch': 0.5.","z":"Hi,\nIf what you want is to save the current state of your model\/optim such that you can restart from there if needed, the checkpoint tool is not what you want. neither are hooks.\nYou simply want to implement the same thing that is done to save every x epochs and put it in your inner loop that iterates over mini-batches such that it runs every x mini-batches.\nThank you for your quick response. So the ImageNet Example on GitHub is misusing checkpoints? See:\n\n\nPyTorch Discussion on Saving and Restoring Training with link \"https:\/\/discuss.pytorch.org\/t\/saving-and-loading-a-model-in-pytorch\/2610\/3?u=campellcl\" and the related:\n\nImageNet PyTorch GitHub Example: Save Checkpoint of Best Performing Epoch with link \"https:\/\/github.com\/pytorch\/examples\/blob\/2fc0211d30b808f049ab7e7f4990858cf2ac471f\/imagenet\/main.py#L184\"\nImageNet PyTorch GitHub Example: Restoring Training Progress from Checkpoint with link \"https:\/\/github.com\/pytorch\/examples\/blob\/2fc0211d30b808f049ab7e7f4990858cf2ac471f\/imagenet\/main.py#L120\"\n\n\n\nWhat I am really trying to ask is, can I do what is shown here with link \"https:\/\/discuss.pytorch.org\/t\/saving-and-loading-a-model-in-pytorch\/2610\/3?u=campellcl\":\n<code class=\"lang-auto\"> save_checkpoint({\n            'epoch': epoch + 1,\n            'arch': args.arch,\n            'state_dict': model.state_dict(),\n            'best_prec1': best_prec1,\n            'optimizer' : optimizer.state_dict(),\n        }, is_best)\n<\/code>\nwith:\n<code class=\"lang-auto\">def save_checkpoint(state, is_best, filename='checkpoint.pth.tar'):\n    torch.save(state, filename)\n    if is_best:\n        shutil.copyfile(filename, 'model_best.pth.tar')\n<\/code>\nand resuming from the checkpoint:\n<code class=\"lang-auto\"> if args.resume:\n        if os.path.isfile(args.resume):\n            print(\"=> loading checkpoint '{}'\".format(args.resume))\n            checkpoint = torch.load(args.resume)\n            args.start_epoch = checkpoint['epoch']\n            best_prec1 = checkpoint['best_prec1']\n            model.load_state_dict(checkpoint['state_dict'])\n            optimizer.load_state_dict(checkpoint['optimizer'])\n            print(\"=> loaded checkpoint '{}' (epoch {})\"\n                  .format(args.resume, checkpoint['epoch']))\n        else:\n            print(\"=> no checkpoint found at '{}'\".format(args.resume))\n<\/code>\nBut with the mini-batch in addition to the epoch? If this isn\u2019t the correct use of a checkpoint, what method of serialization should I use to save and restore the state (gradients, weights, epoch_num, minibatch_num) during training?\nThis is the correct way to save and restore your model\u2019s parameters.\nThe name checkpoint can be a bit misleading, as it can refer to your model\u2019s state of parameters and the torch.utils.checkpoint (link with link \"https:\/\/pytorch.org\/docs\/stable\/checkpoint.html\"), which is used to trace memory for compute. This is, what  meant.\nOk, great. Thank you for clarifying! I didn\u2019t realize torch.utils.checkpoint was used for tracing memory. It is a bit of a misnomer then. I think the first definition is consistent with other machine learning frameworks (at least with TensorFlow with link \"https:\/\/www.tensorflow.org\/guide\/checkpoints\").\nSo is there a way to save and restore the mini-batch index along with the epoch index? I doubt it is as simple as:\n<code class=\"lang-auto\"> save_checkpoint({\n            'epoch': epoch + 1,\n            'mini_batch': minibatch + 1,\n            ...\n        }, is_best)\n<\/code>\nIs there functionality in the model or the optimizer to load a minibatch size and index alongside the epoch index?\nNeither the model nor the optimizer use the epoch or batch count.\nIt\u2019s up to you to store and load the model checkpoints.\nThe model and optimizer just use .load_state_dict() to restore their parameters.\nIf you can work with epoch + minibatch, it\u2019s fine. Alternatively you could somehow store the epoch as a fractal, i.e. for half the mini-batches in the epoch, just save 'epoch': 0.5.\nGood to know, thank you  and  for addressing my misconceptions. For others searching for this, my take away from this the discussion is as follows (please correct me if I\u2019m wrong):\n\nThere is currently no existing module in the PyTorch library for saving and restoring the model during training. The functionality is there, but It is up to the user to implement how to resume the training of a model.\nPyTorch does provide a way to save and restore the model\u2019s parameters through the load_state_dict() method with link \"https:\/\/pytorch.org\/docs\/stable\/nn.html#torch.nn.Module.load_state_dict\".\n\nThe same approach works for the optimizer\u2019s gradients, and parameters.\n\n\nAlthough serialization methods with link \"https:\/\/pytorch.org\/docs\/stable\/notes\/serialization.html#recommended-approach-for-saving-a-model\" do exist, they are intended for use with a trained model; or require additional independent logic to save and resume progress during training.\nAlthough there is a torch.utils.checkpoint with link \"https:\/\/pytorch.org\/docs\/stable\/checkpoint.html\" it is NOT intended for the same use case scenario as TensorFlow\u2019s similarly named checkpoints with link \"https:\/\/www.tensorflow.org\/guide\/checkpoints\", and is instead used to provide memory traces during computation.\n"},{"x":"Hi all,\nWhile I\u2019m trying to use the instance_norm function in the functional class with my own weight and bias, it raises some errors like below,\nTraceback (most recent call last):\nFile \u201clearn_train.py\u201d, line 320, in \npred_v = model([inputParaTensor_v, inputTensor_v])\nFile \u201c\/root\/anaconda3\/lib\/python3.6\/site-packages\/torch\/nn\/modules\/module.py\u201d, line 357, in call\nresult = self.forward(*input, **kwargs)\nFile \u201c\/root\/anaconda3\/lib\/python3.6\/site-packages\/torch\/nn\/parallel\/data_parallel.py\u201d, line 71, in forward\nreturn self.module(*inputs[0], **kwargs[0])\nFile \u201c\/root\/anaconda3\/lib\/python3.6\/site-packages\/torch\/nn\/modules\/module.py\u201d, line 357, in call\nresult = self.forward(*input, **kwargs)\nFile \u201clearn_train.py\u201d, line 90, in forward\nconv1_feat = self.relu(F.layer_norm(conv1_feat_norm))\nAttributeError: module \u2018torch.nn.functional\u2019 has no attribute \u2018layer_norm\u2019\nThe code snippet is very simple,\nconv1_feat_norm = F.conv2d(x_img, self.conv1_weight, bias=None, padding=1)\nconv1_feat = self.relu(F.layer_norm(conv1_feat_norm))\nDoes anyone have any idea why this happens?\nBest","y":"\nI think torch removed the interpolate layer inside the nn.function and created the equivalent methods for Upsampling and other in different modes.\n\nIt\u2019s opposite of what you said.\nI checked my PyTorch version. It is 4.0.0 py36_cuda8.0.61_cudnn7.1.2_1. So, it was my mistake, because as per the doc, UpSample() is deprecated in version > 4.0.0","z":"Hi,\nWhat is the version of pytorch you\u2019re using?\nIf you use the latest binary release which is 0.3.1, this function is not in the doc with link \"http:\/\/pytorch.org\/docs\/0.3.1\/search.html?q=layer_norm&amp;check_keywords=yes&amp;area=default\".\nThis function has been added in master and is only available if you compile from source at the moment.\nHi,\nThank you very much. This works for me very well!\nBest wishes\nHi, I am also getting a similar error after the announcement that nn.Upsample is DEPRECATED in favor of interpolate:\nMy ERROR:\n<code class=\"lang-auto\">AttributeError: module 'torch.nn.functional' has no attribute 'interpolate'\n<\/code>\nI have added this interpolate layer inside a nn.Sequential.\nAny insight into this.\nI think torch removed the interpolate layer inside the nn.function  and created the equivalent methods for Upsampling and other in different modes.\nhttps:\/\/pytorch.org\/docs\/stable\/nn.html?highlight=interpolate\nScreen Shot 2018-08-11 at 1.04.11 PM.png1390\u00d71106 88.4 KB with link \"https:\/\/discuss.pytorch.org\/uploads\/default\/original\/2X\/2\/267ddbbfd14a3db99712a0a435937bc92817fe57.png\"\n\nI think torch removed the interpolate layer inside the nn.function and created the equivalent methods for Upsampling and other in different modes.\n\nIt\u2019s opposite of what you said.\nI checked my PyTorch version. It is 4.0.0 py36_cuda8.0.61_cudnn7.1.2_1. So, it was my mistake, because as per the doc, UpSample() is deprecated in version > 4.0.0"},{"x":"I have a 300-D mean vector and a 300x300 covariance matrix and want to compute a Gaussian distribution of the same. The problem that I am facing, computing it manually is that the determinant is always computed as 0 as its a product of 300 weak numbers b\/w 0 and 1. Is there a way to avoid this \u2026 or a function to do the same?","y":"You could try working in log space (i.e. log(det(Sigma))), which typically results in much better stability when working with products of small numbers.\nIn the case of a diagonal covariance matrix, the log determinant is the sum of the log of each diagonal element.","z":"Hi,\nI don\u2019t know well but am just curious about if torch.distributions.MultivariateNormal with link \"https:\/\/pytorch.org\/docs\/stable\/distributions.html#torch.distributions.multivariate_normal.MultivariateNormal\" is enough.\nYou could try working in log space (i.e. log(det(Sigma))), which typically results in much better stability when working with products of small numbers.\nIn the case of a diagonal covariance matrix, the log determinant is the sum of the log of each diagonal element."},{"x":"I have a JSON file wherein I have defined a network (Consists of Conv and Dense layers)\nI want to create a Network dynamically based on this JSON file, thus I want my network to have layers according to file. Thus my layer can either have 4 layers or 10 layers.\nHow can I achieve this in PyTorch?\nFor example, my JSON file looks like this:\n{\n\u201cname\u201d: \u201cArch 1\u201d,\n\u201cbatch_size\u201d: 50,\n\u201clayers\u201d: [\n{\n\u201ctype\u201d: \u201cInput\u201d,\n\u201coutput_shape\u201d: [\n1,\n28,\n28\n]\n},\n{\n\u201ctype\u201d: \u201cConv2D\u201d,\n\u201cnum_filters\u201d: 32,\n\u201cfilter_size\u201d: [\n3,\n3\n],\n\u201cnon_linearity\u201d: \u201crectify\u201d,\n\u201cconv_mode\u201d: \u201csame\u201d\n},\n{\n\u201ctype\u201d: \u201cMaxPool2D\u201d,\n\u201cfilter_size\u201d: [\n2,\n2\n]\n},","y":"If this is exported from some other program, if you can export it in onnx or caffe format, my understanding is that pytorch has native importers for one or both of these types.\notherwise, if it\u2019s a proprietary format description, you can write code to convert your json format into a python object hierarchy (json.loads(json_string)), then iterate over it.\nyou can do something like:\nclass MyNetwork(nn.Module):\n    def __init__(self, net_string):\n        super().__init__()\n        self.module_list = nn.ModuleList()\n        for layer_def in json.loads(net_string):\n            layer = self._create_layer(layer_def)\n            self.module_list.append(layer)\n\n    def forward(self, x):\n        for layer in self.module_list:\n            x = layer(x)\n        return x\n\n(edit: ie, nn.ModuleList is quite useful for such dynamic layer lists)","z":"If this is exported from some other program, if you can export it in onnx or caffe format, my understanding is that pytorch has native importers for one or both of these types.\notherwise, if it\u2019s a proprietary format description, you can write code to convert your json format into a python object hierarchy (json.loads(json_string)), then iterate over it.\nyou can do something like:\nclass MyNetwork(nn.Module):\n    def __init__(self, net_string):\n        super().__init__()\n        self.module_list = nn.ModuleList()\n        for layer_def in json.loads(net_string):\n            layer = self._create_layer(layer_def)\n            self.module_list.append(layer)\n\n    def forward(self, x):\n        for layer in self.module_list:\n            x = layer(x)\n        return x\n\n(edit: ie, nn.ModuleList is quite useful for such dynamic layer lists)\nAnd what about the non linearities?\nCan you please explain with an example, as MaxPooling and Relu is applied in forward function on the example given in Official documentation\nI mean, conceptually, you can add any network module to a module list.  The only example I have that is opensource is unfortunately in C++, and not for pytorch, https:\/\/github.com\/hughperkins\/DeepCL\/blob\/master\/src\/netdef\/NetdefToNet.cpp#L104\nyou\u2019re going to make _create_layer have maybe an if statement like:\nif layer_def['type'] == 'relu':\n    return nn.ReLU()\nelif layer_def['type'] == 'conv':\n    return [stuff here to create a conv layer]\nelif ... etc ...\nI have one more doubt.\nSay I have a dense\/linear layer with say(1,1,3136) units.\nHow do I reshape this for Conv Layer (64,7,7) and feed this input to the Transpose Conv Layer?\nI am trying to create a Convolutional Autoencoder actually\nYou could write a custom layer which does nothing except reshaping and add this layer in front of your convolutional layer:\n<code class=\"lang-auto\">class ReshapeLayer(torch.nn.Module):\n    def __init__(batch_size, n_channels, height, width):\n        self.shape = [batch_size, n_channels, height, width] \n    def forward(self, x):\n        return x.view(*self.shape)\n<\/code>\nEDIT: if you don\u2019t know all of that dimension sizes you could also pass -1 for one dimension. With -1 the dimension is set in a way that it fits to the number of entries in the tensor and is calculated with respect to the other dimensions\nHow should I flatten the data before feeding to FC layer , i.e The other way round.\nWill this work? Is it necessary to create init method ?\nclass Flatten(nn.Module):\ndef forward(self, x):\nx = x.view(x.size()[0], -1)\nreturn x\nAn init method is necessary because torch.nn.Module is an abstract class with the init being an abstract method and subclasses of abstract classes should implement all abstract methods.\nDespite that fact the rest of your class should work the way you want it to"},{"x":"I am using Pytorch on a remote machine provided by university.\nI am trying to train my model using GPU, but unable to do so because of some issues with cuda.\nEven though cuda is available, I am unable to use it.\nAny help will be appreciated.\nAttached screenshot for referenceScreen Shot 2018-05-31 at 23.25.27.png1230\u00d7273 39.5 KB with link \"https:\/\/discuss.pytorch.org\/uploads\/default\/original\/2X\/9\/928124eb4219392a2795d7cc61e8fad38bbc608b.png\"\nAn this is the GPU specificationScreen Shot 2018-05-31 at 23.48.53.png925\u00d7417 37.6 KB with link \"https:\/\/discuss.pytorch.org\/uploads\/default\/original\/2X\/b\/b5e7217fd515a42e22af9f71ed3a30ef620a81e9.png\"","y":"It looks like python is utilizing almost all of your gpu memory. Is this a shared machine? Maybe someone else is using it.","z":"It looks like python is utilizing almost all of your gpu memory. Is this a shared machine? Maybe someone else is using it.\nYes, it worked on different machine. Thanks though"},{"x":"I am doing image classification where I have two input networks, of which the output nodes are concatenated in a fully connected layer. This is my code:\n<code class=\"lang-auto\">class resnet18_meteo(nn.Module):\n    def __init__(self, resnet18, meteo_NN, num_classes):\n        super(resnet18_meteo, self).__init__()\n        \n        # Respectively a torchvision resnet-18 and a 1-hidden layer NN\n        self.resnet_CNN = resnet18\n        self.meteo_net = meteo_NN\n        \n        # Sizes of the FC layers of both NN's\n        self.len_fc_resnet = self.resnet_CNN.fc.in_features\n        self.len_fc_meteo = self.meteo_net.fc_last.out_features\n        print(self.len_fc_meteo)\n        \n        # Remove FC layer from the resnet \n        self.modules=list(self.resnet_CNN.children())[:-1]\n        self.resnet18_convblocks= nn.Sequential(*self.modules)\n        \n        # Fully connected layer is now size resnet FC + meteo FC\n        self.fc = nn.Linear(self.len_fc_resnet + self.len_fc_meteo, num_classes)\n    \n    def forward(self, img_x, meteo_x):\n        \n        # Both should be flattened layers at end of networks\n        img_x = self.resnet18_convblocks(img_x)\n        meteo_x = self.meteo_net(meteo_x)\n        \n        # Flatten convolutional features\n        img_x_flattened = img_x.view(img_x.size(0), -1)\n        \n        # Concat the outputs of CNN and meteo-NN in fully connected layer\n        out = torch.cat([img_x_flattened, meteo_x], dim=1)\n        out = self.fc(out)\n        return out   \n<\/code>\nI use a resnet 18 structure to input the images. Besides, I have four additional non-image features that  I input to a small feedforward network. As can be seen, the outputs of these networks (size 512 + 32) are thus concatenated in the FC layer.\nNow I was wondering if I can give some kind of \u2018judgement weights\u2019 to these networks. For instance, I\u2019d like to have the resnet have 70% of the influence on the decision\/loss, and the non-image net have 30% of the influence.\nThe only way I can think of doing this is by totally disconnecting these networks from each other, calculate the losses for each data-point (for each network), and then weighting these losses. However, I would like to know if this type of thing could be incorporated into my current network structure.","y":"As you forward the composed features (in the variable out) through another fully connected layer you could simply weight them before using torch.cat(). This has however the disadvantage that you don\u2019t know how this will affect the parameters of the fully connected layer during training.\nEven if you have to disconnect the networks I would recommend to calculate separate losses.","z":"As you forward the composed features (in the variable out) through another fully connected layer you could simply weight them before using torch.cat(). This has however the disadvantage that you don\u2019t know how this will affect the parameters of the fully connected layer during training.\nEven if you have to disconnect the networks I would recommend to calculate separate losses."},{"x":"Hi everyone,\nI\u2019ve been trying to load a new dataset using DataLoader but I get the following error:\n<code class=\"lang-auto\">Traceback (most recent call last):\n  File \"C:\/Users\/User\/PycharmProjects\/STN_LSTM\/STN_LSTM.py\", line 98, in <module>\n    main()\n  File \"C:\/Users\/User\/PycharmProjects\/STN_LSTM\/STN_LSTM.py\", line 83, in main\n    for i_batch, sample_batched in enumerate(train_loader):\n  File \"C:\\Users\\User\\Anaconda3\\envs\\tensorflow\\lib\\site-packages\\torch\\utils\\data\\dataloader.py\", line 451, in __iter__\n    return _DataLoaderIter(self)\n  File \"C:\\Users\\User\\Anaconda3\\envs\\tensorflow\\lib\\site-packages\\torch\\utils\\data\\dataloader.py\", line 239, in __init__\n    w.start()\n  File \"C:\\Users\\User\\Anaconda3\\envs\\tensorflow\\lib\\multiprocessing\\process.py\", line 105, in start\n    self._popen = self._Popen(self)\n  File \"C:\\Users\\User\\Anaconda3\\envs\\tensorflow\\lib\\multiprocessing\\context.py\", line 223, in _Popen\n    return _default_context.get_context().Process._Popen(process_obj)\n  File \"C:\\Users\\User\\Anaconda3\\envs\\tensorflow\\lib\\multiprocessing\\context.py\", line 322, in _Popen\n    return Popen(process_obj)\n  File \"C:\\Users\\User\\Anaconda3\\envs\\tensorflow\\lib\\multiprocessing\\popen_spawn_win32.py\", line 65, in __init__\n    reduction.dump(process_obj, to_child)\n  File \"C:\\Users\\User\\Anaconda3\\envs\\tensorflow\\lib\\multiprocessing\\reduction.py\", line 60, in dump\n    ForkingPickler(file, protocol).dump(obj)\nTypeError: cannot serialize '_io.BufferedReader' object\n\n<\/code>\nI\u2019ve followed the data loading tutorial (https:\/\/pytorch.org\/tutorials\/beginner\/data_loading_tutorial.html) and have successfully created a dataset class inheriting from Dataset that overrides len and getitem so that they return the size of the dataset and an indexed sample respectively. I can also iterate and print samples from this dataset.\nHowever, once I run the code below that error pops up.\n<code class=\"lang-auto\">    train_loader = DataLoader(mnist_clutterred, batch_size=64,\n                            shuffle=True, num_workers=4)\n    print(len(train_loader))\n    for i_batch, sample_batched in enumerate(train_loader):\n        print(i_batch)\n<\/code>\nAny ideas on why I get that error?\nThanks in advance!","y":"I\u2019ve found the error: I am using the \u201ccpu\u201d (i.e. device = torch.device(\u201ccpu\u201d)) and at the same time I am defining workers.\nRemoving \u201cnum_workers=4\u201d lifted the error ","z":"I\u2019ve found the error: I am using the \u201ccpu\u201d (i.e. device = torch.device(\u201ccpu\u201d)) and at the same time I am defining workers.\nRemoving \u201cnum_workers=4\u201d lifted the error "},{"x":"I am new with Pytorch, and will be glad if someone will be able to help me understand the following (and correct me if I am wrong), regarding the meaning of the command x.view in Pytorch first tutorial:\nhttps:\/\/pytorch.org\/tutorials\/beginner\/blitz\/cifar10_tutorial.html\n(    def forward(self, x):\nx = self.pool(F.relu(self.conv1(x)))\nx = self.pool(F.relu(self.conv2(x)))\nx = x.view(-1, 16 * 5 * 5)\nx = F.relu(self.fc1(x))\nx = F.relu(self.fc2(x))\nx = self.fc3(x)\nreturn x)\n\n\nAs far as I understand, an input 256X256 image to a convolutional layer is inserted in its 2D form (i.e. - a 256X256 matrix, or a 256X256X3 in the case of a color image). Nevertheless, when we insert an image to a fully-connected linear layer, we need to first reshape the 2D image into a 1D vector. Is this why we use the command \u201cx = x.view(-1, 16 * 5 * 5)\u201d before inserting x into the fully connected layers?\n\n\nIf the input image x would be 3D (e.g. 256X256X256), would the syntax of the given above \u201cforward\u201d function remain the same?\n\n\nThanks a lot in advance","y":"Your explanation is right in general.\nJust some minor issues:\nIn PyTorch, images are represented as [channels, height, width], so a color image would be [3, 256, 256].\nDuring the training you will get batches of images, so your shape in the forward method will get an additional batch dimension at dim0: [batch_size, channels, height, width].\nThe .view() operation gives you a new view on the tensor without copying any data.\nThis means view is cheap to call and you don\u2019t think too much about potential performance issues.\nIn your example, x is reshaped into a tensor of shape 16*5*5 in dim1 and all remaining values in dim0. That\u2019s what the -1 stands for.\nIn the case of a 256 -channel image, the view operation would resume the same, since the spatial dimensions of the image are the same and the channels are defined by the number of kernels of conv2.","z":"Your explanation is right in general.\nJust some minor issues:\nIn PyTorch, images are represented as [channels, height, width], so a color image would be [3, 256, 256].\nDuring the training you will get batches of images, so your shape in the forward method will get an additional batch dimension at dim0: [batch_size, channels, height, width].\nThe .view() operation gives you a new view on the tensor without copying any data.\nThis means view is cheap to call and you don\u2019t think too much about potential performance issues.\nIn your example, x is reshaped into a tensor of shape 16*5*5 in dim1 and all remaining values in dim0. That\u2019s what the -1 stands for.\nIn the case of a 256 -channel image, the view operation would resume the same, since the spatial dimensions of the image are the same and the channels are defined by the number of kernels of conv2.\n\n\n\n ptrblck:\n\nou a new view on the tensor without copying any data.\nThis means view is cheap to call and you don\u2019t think too much about potential performance issues.\nIn your example, x is reshaped into a tensor of shape\n\n\nThanks a lot ptrbick!"},{"x":"I have been working on what I think should be a ridiculously simple problem.  Let\u2019s say I have a 2D matrix 5x5.  I would like to take each 3x3 patch, flatten the patch (to 9 elements), then put the flattened patch through a network that spits out a single number.  So the 5x5 matrix should become a 3x3 matrix.  Each individual number in this resulting 3x3 matrix came from running a 3x3 patch through a network.  I\u2019m also cool with a 5x5 matrix result if adding padding simplifies the solution.  The solutions I am coming up with create patches (using unfold) which I have to convert and then concat back together making sure the spatial relationship between the input and ouput matrix is preserved.  What am I missing here?","y":"You are not missing anything and your explanation seems fine.\nAs  said, it\u2019s comparable to a conv or pooling layer.\nHere is a small example for your use case:\n<code class=\"lang-python\">image = torch.randn(1, 1, 5, 5)\n\nkh, kw = 3, 3 # kernel size\ndh, dw = 1, 1 # stride\npatches = image.unfold(2, kh, dh).unfold(3, kw, dw)\npatches = patches.contiguous().view(-1, 9)\n\nlin = nn.Linear(9, 1)\noutput = lin(patches)\noutput.shape\noutput = output.view(1, 1, 3, 3)\n<\/code>","z":"What sort of network are you applying to the 3x3 patches?  What you\u2019re describing is effectively 2D convolution, but instead of a linear\/affine operator applied to the patch you\u2019re applying a network operator.\nI\u2019m flattening the 3x3 patches to 9 and then running it through a linear layer that creates a single value.\nIf each patch is using the same linear layer, then this is exactly a 2D convolution.\nAre you aware of any code\/blog\/whatever that shows an example of implementation then?\nYou are not missing anything and your explanation seems fine.\nAs  said, it\u2019s comparable to a conv or pooling layer.\nHere is a small example for your use case:\n<code class=\"lang-python\">image = torch.randn(1, 1, 5, 5)\n\nkh, kw = 3, 3 # kernel size\ndh, dw = 1, 1 # stride\npatches = image.unfold(2, kh, dh).unfold(3, kw, dw)\npatches = patches.contiguous().view(-1, 9)\n\nlin = nn.Linear(9, 1)\noutput = lin(patches)\noutput.shape\noutput = output.view(1, 1, 3, 3)\n<\/code>\nCool!  Definitely a lot simpler than the stuff I was coming up with.  Thanks!"},{"x":"In my dataloader I am providing dataset which is created by using torchdata.Dataset, I am using the following two lines\n<code class=\"lang-auto\">img = cv2.imread(path_img, 1)       \nimg = img.transpose(2, 0, 1)\n<\/code>\nIt is working normal for some iteration but then giving error like:\n<code class=\"lang-auto\">'Nonetype' object has no attribute 'transpose'\n<\/code>\nI am confused whether it is due to some of the images in the dataset or due to python.\nThanks in advance for help.","y":"Are you sure that all paths are correct??\nTry catch the error something like\n<code class=\"lang-auto\">try:\n    img = img.transpose(2, 0, 1)\ncatch:\n    print(path_img)\n    \n<\/code>\nThen manually check the path exists, it should help","z":"Are you sure that all paths are correct??\nTry catch the error something like\n<code class=\"lang-auto\">try:\n    img = img.transpose(2, 0, 1)\ncatch:\n    print(path_img)\n    \n<\/code>\nThen manually check the path exists, it should help\nThank you for the help. It is due to one image of my dataset. I have replaced it and problem is resolved."},{"x":"Hi,\nI\u2019m new to torch 0.4 and implement a Encoder-Decoder model for image segmentation.\nduring training to my lab server with 2 GPU cards only, I face the following problem say \u201cout of memory\u201d:\n\nimage.png1184\u00d7606 28.7 KB with link \"https:\/\/discuss.pytorch.org\/uploads\/default\/original\/2X\/d\/dadc088ad19835a33c6996a84f2c5e2f41c9b349.png\"\nmy input is 320*320 image and even I let batch_size = 1, it cannot finish even 1 epoch, I\u2019m not sure whether there is some commands to use multiple GPU card?\nAny suggestion is appreciated!\nThank you very much!\nBest,\nPeter","y":"Hi ptrblck,\nI solve the question posted here by using:\n \u2018s answer from a multiple GPUs\u2019 solution:\n<code class=\"lang-auto\">model = <specify model here>\nmodel = torch.nn.DataParallel(net, device_ids=range(torch.cuda.device_count()))\n<\/code>\nI notice you mentioned \u201cit splits the data\/batch onto different GPUs\u201d rather than model sharding\u2026 I feel puzzled on this statement. What\u2019s the advantage to split model class on different GPUs?  Does it mean it helps to distribute model training burden to multiple PCs (rather one PC only?)\nThank you very much!","z":"This is weird. To my understanding each iteration should be \u201cseparated\u201d. Your task is clearly not recurrent and your batch is clearly not increasing nor decreasing. So, why then you can finish a few iterations before getting out of memory error?\nMust there be some kind of memory leaking? Some codes sure help!\n\n\n\n Konpat_Ta_Preechakul:\n\nmemory leaking\n\n\nHi, Konapt, Thank you very much!\nI resize my input batch as (32 , 3,64,64) and 128 by 128 both works well.\nWhen I increase each image size to 224 by 224, 256 by 256\u2026 fails\u2026 it can hold ~20 epochs and tell my out of memory.\nI head the term \" memory leaking\" but could you please give me some suggestions on these? Thank you very much. I\u2019m not sure how to do it in pytorch and how to check it.\nBest,\nPeter\nActually, I need to see the code. Anyway, how did you get the \u201ctrain_loss\u201d, is it a kind of average? If you get the average by adding the \u201closs\u201d up something like:\n<code class=\"lang-auto\">loss = ((y_hat - y) ** 2).mean()\nsum_loss += loss\navg_loss = sum_loss \/ itr\nprint('train_loss', float(avg_loss))\n<\/code>\nMemory leaking comes from the sum_loss, since it will hold the graph for each iteration from the first iteration. Using .detach() should help in this case, but frankly you could just use float(loss) altogether.\nThank you Konapt,\nyou are a genius !!!\nI really use sum_loss += loss indeed!\nshould I use sum_loss += float(loss) to solve this problem??\nimage.png1696\u00d7270 15.7 KB with link \"https:\/\/discuss.pytorch.org\/uploads\/default\/original\/2X\/f\/fa26d33beaf4481a9bf995a73868c48f4ee92f35.png\"\nthank you !\nPeter\nYeah it\u2019ll work, go for it.\nAnd you got the the reason for it right (graph storing activations from old epochs which was not you wanted)!\nHi Naman,\nimage.png917\u00d7749 30.1 KB with link \"https:\/\/discuss.pytorch.org\/uploads\/default\/original\/2X\/9\/9672f1e43e87c66a670c4f4e16f528bc2b4685e2.png\"\nI try Konapat\u2019s suggestion and still face the \u201cout of memory\u201d at 20 epoch\u2026 any idea to help it? I\u2019m using pytorch 0.4 now.\nthank you very much!\nBest,\nPeter\nHi Konpat,\nThank you your advice very much!\nI try your suggestion and add float() to all loss.item during iteration and face the same \u201cout of memory error\u201d at 20 epoch.\nI\u2019m using torch 0.4 now and seems there is no .detach() to be used\u2026 I feel puzzled on it, any suggestion is appreciated.\nThank you very much!\nBest,\nPeter\nYou don\u2019t need to use float(loss.item()). .item() returns a standard python float and thus won\u2019t be tracking the computation graph.\nHow is your GPU memory usage in the first epochs? Are you already approx. on the limit?\nCould you observe the memory with nvidia-smi -l and see if the memory usage is increasing in the first epochs?\nI have no idea now , one last thing to look at is the train loader I think.\nHi ptrblck,\nThank you very much! I try your command and get the following information:\nimage.png800\u00d7453 13.4 KB with link \"https:\/\/discuss.pytorch.org\/uploads\/default\/original\/2X\/3\/32100fe12d4238b8dd315c1decd39edceaf40787.png\"\nimage.png1415\u00d7828 41.6 KB with link \"https:\/\/discuss.pytorch.org\/uploads\/default\/original\/2X\/8\/8a8c8f75591020b230b782d1a03bc1d999d38b66.png\"\nI notice that before the epoch to tell me out of memory, as shown in the blue line, GeForce GTX card seems to be almost doubled suddenly and it fails:\nimage.png810\u00d7602 19.6 KB with link \"https:\/\/discuss.pytorch.org\/uploads\/default\/original\/2X\/9\/9c388c65c3afea15d7ca0a19657cdacf5cfb08f1.png\"\nIs there any way in pytorch to use the another GPU card in sequence to satisfy it?\nOr should I need to reduces number CONV layer or number of channels in my model?\nThank you very much!\nBest,\nPeter\nHi Konpat,\nthank you very much anyway! I follow ptblck\u2019s advice to check nvidia\u2019s usage and find during 20th epoch, in one of up-sampling layers, when i do skip-connection operation to concatenate 2 layers from encoder and decoder layer like in U-Net, the memory required for GPU just doubled and it therefore fails:\nimage.png771\u00d733 1.96 KB with link \"https:\/\/discuss.pytorch.org\/uploads\/default\/original\/2X\/2\/21a743c5988d64661171d056d0a345398c29de80.png\"\nI may try to find a way to use another GPU card parallel during training or reduce my channels or CONV layers\u2019 number in my model.\nThank you very much!\nBest,\nPeter\nWhat doesn\u2019t really make sense, is that the torch.cat operation apparently needs more memory in the 20th epoch, whereas before that it seems to work.\nModel sharding would be an approach to split your model onto both GPUs without sacrificing the model capacity.\nSince I still think there might be an issue with your code, could you please post it?\nYou can add code by wrapping it in three `.\nHonestly, if your train loader is not provided directly by the library, I really want to see it.\nHi ptrblck,\nMy implementation to create the dataloader may be naive and not clean to you.\n1: To get data and label as numpy arrays: I use packages to read .dcm files and corresponding .nrrd files\n2: I convert the numpy array (optionally crop or not) from defalut type unit16 to int16 in order to be used in pytorch:\nbelow is one of the repetitive part for me to read .dcm file to get image array and convert a (320, 320) original image numpy array into (number of totoal image files,320,320):\n<code class=\"lang-auto\">\n''' > img_1 = [pydicom.dcmread(train_Prostate3T_img_path + '\/' + ID + '\/' + dcm_number).pixel_array for ID in Prostate3T_patient_ID for dcm_number in os.listdir(train_Prostate3T_img_path + '\/' + ID )]\n> Prostate3T_img = img_1[0][np.newaxis,...]\n> count = len(Prostate3T_img)\n> for i in range(1,len(img_1)):\n>     try:\n>         Prostate3T_img = np.vstack((Prostate3T_img, img_1[i][np.newaxis,...]))\n>     except:\n>         #print(\"mis-matched dimension at\", i, \"-th sample.\")\n>         #print(\"wrong shape:\", np.shape(img_1[i])) ## 18 wrong 256 x 256 shapes\n>         #print(\"prostate samples already counted:\", count)\n>         continue\n> print(\"original total samples should be counted:\", np.shape(img_1)[0])\n> print(\"Prostate3T image stacked shape: \", np.shape(Prostate3T_img))\n> print(\"===\"*3) '''\n> \n> Then I convert (1,320,320) per sample to (3,320,320) 3 channels using cv2.cvtColor:\n> ''' img_to_3_channels_Prostate3T = np.array([cv2.cvtColor(Prostate3T_img[i], cv2.COLOR_GRAY2RGB).T for i in range(len(Prostate3T_img))])\n ''''\n<\/code>\n3: convert them into torch tensor and use .utils.TensorDataset to put in pytorch dataset\n(the reason why I convert tensor type to .type(\u2018torch.FloatTensor\u2019) is that I found I can only use L1 Loss in this way an gives no error, my custom loss function is still under deleloped)\n<code class=\"lang-auto\">img_to_3_channels_Prostate3T_int16 = np.array(img_to_3_channels_Prostate3T, dtype=np.int16)\nimg_Prostate3T_tensor = torch.from_numpy(img_to_3_channels_Prostate3T_int16)\nimg_Prostate3T_tensor = img_Prostate3T_tensor.type('torch.FloatTensor')\nimg_Prostate3T_tensor = img_Prostate3T_tensor.to(device)\nProstate3T_dataset = utils.TensorDataset(img_Prostate3T_tensor, label_Prostate3T_tensor)\n<\/code>\n\nthen use utils.DataLoader to establish a Dataloader with SubsetRandomSampler.\n\n4.1 SubsetRandomSampler is used in this way:\ntrain_sampler = SubsetRandomSampler(np.arange(n_training_samples, dtype=np.int64))\n4.2. training set loader is used in this way:\ntrain_loader = torch.utils.data.DataLoader(Prostate3T_dataset, batch_size=batch_size, sampler=train_sampler)\nAny advice is appreciated!\nThank you very much!\nBest,\nPeter\nHi Konpat,\nAs I replied to ptrblck, I attached my codes and my explanation on the codes above for your reference.\nI just follow an online tutorial to get (320,320) numpy array firstly, and increase one dimension for each sample to (1,320,320) and convert them into 3 channels (3,320,320) and then concatenate all samples (# of total samples, 3, 320, 320).\nAnd then convert (# of total samples, 3, 320, 320) from dtype=unit 16 to int16 in order to be dealt in pytorch and convert is type from numpy array to torch tensor.\nAfter that, I use **utils.TensorDataset ** and SubsetRandomSampler to call\n\ntrain_loader = torch.utils.data.DataLoader(Prostate3T_dataset, batch_size=batch_size, sampler=train_sampler)\n\nAny suggestion is appreciated!\nThank you very much!\nBest,\nPeter\nThe data loading part looks OK. You could change some small details, e.g. use lazy loading, but this is unrelated to your OOM error.\nProbably we would have to see the other parts of your training procedure.\nHi ptrblck,\nThank you very much! I will modify and re-construct my code today and let you know the more reader-friendly code. (: ), hope you also enjoy tonight\u2019s worlds cup!)\nThank you!\nBest,\nPeter\nHi ptrblck,\nhope you enjoy these-2-day world cup matches.\nI remove repetitive codes and try to clean my code.\nFor training part, below is my code:\n\nsimilar to validation part as below:\n\nAny suggestion is appreciated!\nThank you very much!\nHi ptrblck,\nI solve the question posted here by using:\n \u2018s answer from a multiple GPUs\u2019 solution:\n<code class=\"lang-auto\">model = <specify model here>\nmodel = torch.nn.DataParallel(net, device_ids=range(torch.cuda.device_count()))\n<\/code>\nI notice you mentioned \u201cit splits the data\/batch onto different GPUs\u201d rather than model sharding\u2026 I feel puzzled on this statement. What\u2019s the advantage to split model class on different GPUs?  Does it mean it helps to distribute model training burden to multiple PCs (rather one PC only?)\nThank you very much!"},{"x":"I would get the error\nRuntimeError: Expected tensor for argument #1 'input' to have the same dimension as tensor for 'result'; but 4 does not equal 2 (while checking arguments for cudnn_convolution)\nIf I run the code below. Quick googling returns various issues that cause this. I assume Pytorch doesn\u2019t like the 9 in channels and 64 out channels?\n<code class=\"lang-auto\">import torch\nimport torch.nn as nn\n\nreal = torch.randn([32, 9, 32, 32])\n\nclass options():\n    def __init__(self):\n        self.ndf = 64\n\nopt = options()    \n\nnetd = nn.Sequential(\n            \n            nn.Conv2d(9,opt.ndf,4,2,1,bias=False),\n            nn.LeakyReLU(0.2,inplace=True),\n            \n            nn.Conv2d(opt.ndf,opt.ndf*2,4,2,1,bias=False),\n            nn.BatchNorm2d(opt.ndf*2),\n            nn.LeakyReLU(0.2,inplace=True),\n            \n            nn.Conv2d(opt.ndf*2,opt.ndf*4,4,2,1,bias=False),\n            nn.BatchNorm2d(opt.ndf*4),\n            nn.LeakyReLU(0.2,inplace=True),\n            \n            nn.Conv2d(opt.ndf*4,opt.ndf*8,4,2,1,bias=False),\n            nn.BatchNorm2d(opt.ndf*8),\n            nn.LeakyReLU(0.2,inplace=True),\n            \n            nn.Conv2d(opt.ndf*8,1,4,1,0,bias=False),\n            \n        )\n\nnetd.cuda()\n\ntest = netd(real)\n<\/code>","y":"Your input to the last conv layer is [batch_size, 512, 2, 2] which is too small for a kernel size of 4.\nChange the last layer to nn.Conv2d(opt.ndf*8, 1, 2, 1, 0, bias=False) and your model should run.","z":"Your input to the last conv layer is [batch_size, 512, 2, 2] which is too small for a kernel size of 4.\nChange the last layer to nn.Conv2d(opt.ndf*8, 1, 2, 1, 0, bias=False) and your model should run.\nOps thanks for the solution!\nEdited: Now during backprop of the output using that model I get\nExpected tensor for 'result' to have the same dimension as tensor for argument #1 'grad_output'; but 4 does not equal 1 (while checking arguments for cudnn_convolution_backward_input)\neg :\noutput = netd(input)\noutput.backward( one )\nYou can probably tell I am using WGAN\u2019s netd with no GP applied.\nEdited: I solved this by calling output = output.mean() to get its mean then backprop from there, I don\u2019t know if this is the best way but I am getting generations."},{"x":"Hello,\nI am trying to use a pre-trained resnet model to classify images, and I also want to get the next-to-last layer features.\nFrom this discussion with link \"https:\/\/discuss.pytorch.org\/t\/get-next-to-last-layer-on-pretrained-resnet\/13681\" I see that it\u2019s pretty easy to construct a new classifier that returns the next-to-last layer weights.\nCurrently my code looks like this:\n<code class=\"lang-auto\">class_model = resnet152(pretrained=True)\nclass_model.eval()\n\nlw_model = torch.nn.Sequential(*list(class_model.children())[:-1])\nlw_model.eval()\n\n#im_tensor is a normalized JPEG\nclass_out = class_model(im_tensor.unsqueeze(0))\nlw_out = lw_model(im_tensor.unsqueeze(0))\n\ntop5 = []\nfor idx in class_out[0].sort()[1][-5:]:\n    top5.append((idx.item(), labels[(str(idx.item()))][1]))\n\n_ = [print(c) for i,c in reversed(top5)]\n# print(lw_out.squeeze_())\n<\/code>\nIs there any way to get a classification and the next to last layer weights using just one model instead of running two? Perhaps toggle something in the pretrained resnet to have it emit the last weights in addition to the classifications?\nAny help would be greatly appreciated, thanks!","y":"Why don\u2019t you return two values from the forward function, the penultimate layer predictions as well as the final logits?\nsomething like\n<code class=\"lang-auto\">lw_model = torch.nn.Sequential(*list(class_model.children())[:-1])\nfinal_layer = list(class_model.children()))[-1]\n<\/code>\nand chain these two models to make the final prediction.","z":"Why don\u2019t you return two values from the forward function, the penultimate layer predictions as well as the final logits?\nsomething like\n<code class=\"lang-auto\">lw_model = torch.nn.Sequential(*list(class_model.children())[:-1])\nfinal_layer = list(class_model.children()))[-1]\n<\/code>\nand chain these two models to make the final prediction.\nThanks! It hadn\u2019t occurred to me that you could break up classifiers into chunks and compose them that way. Solved my issue."},{"x":"Sorry guys, I have a silly question.I am using python3.5 and pytorch 3.1, when I type\n<code class=\"lang-auto\">torch.Tensor(1000, 1000).mean()\n<\/code>\nI get the following result:\n<code class=\"lang-auto\">>>> import torch\n>>> torch.Tensor(1000, 1000).mean()\n0.0\n>>> torch.Tensor(1000, 1000).mean()\nnan\n>>> torch.Tensor(1000, 1000).mean()\nnan\n\n<\/code>\nnan stands for not a number, right?\nbut when I type this:\n<code class=\"lang-auto\">>>> torch.Tensor(1000, 1000)\n\n 0.0000e+00  4.3397e+02  0.0000e+00  ...   0.0000e+00  0.0000e+00  2.2500e+00\n 0.0000e+00  0.0000e+00  0.0000e+00  ...   0.0000e+00  1.0842e-19  1.9776e+00\n 0.0000e+00  0.0000e+00  0.0000e+00  ...   0.0000e+00  0.0000e+00  0.0000e+00\n                ...                   \u22f1                   ...                \n 0.0000e+00  0.0000e+00  0.0000e+00  ...   0.0000e+00  0.0000e+00  0.0000e+00\n 0.0000e+00  0.0000e+00  0.0000e+00  ...   0.0000e+00  0.0000e+00  0.0000e+00\n 0.0000e+00  0.0000e+00  0.0000e+00  ...   0.0000e+00  0.0000e+00  0.0000e+00\n[torch.FloatTensor of size 1000x1000]\n\n<\/code>\nThe tensor is filled with  numbers.\nSo, two questions:\n\n\nHow come 1000000 numbers\u2019 mean is not a number?\n\n\nAnd why sometimes I get nan, but sometimes I get numbers?Like this:\n\n\n<code class=\"lang-auto\">>>> torch.Tensor(1000, 1000).mean()\n-1.0074733384568384e+32\n<\/code>\nThanks in advance.","y":"The behavior of calling mean on an uninitialized tensor is undefined. I believe pytorch is interpreting the data as if it were valid numbers, which is why you get a result. However, there\u2019s no guarantees for the data that is going to be in the tensor, so that\u2019s why you get such unpredictable results. One possible explanation for why you get nan is one of the entries is interpreted by pytorch as nan, and the mean of a tensor with a nan element is always nan.","z":"That constructor doesn\u2019t do anything to the storage memory, it\u2019s all uninitialized. So the data is just garbage.\n\n\n\n royboy:\n\nThat constructor doesn\u2019t do anything to the storage memory, it\u2019s all uninitialized. So the data is just garbage.\n\n\nBut still, garbage have a value that can be  represented  by a number, right? If they have a number, how come pytorch knows it\u2019s garbage or a useful number? And why sometims I get a number for calculate the mean, and sometimes I get a nan?\nThe behavior of calling mean on an uninitialized tensor is undefined. I believe pytorch is interpreting the data as if it were valid numbers, which is why you get a result. However, there\u2019s no guarantees for the data that is going to be in the tensor, so that\u2019s why you get such unpredictable results. One possible explanation for why you get nan is one of the entries is interpreted by pytorch as nan, and the mean of a tensor with a nan element is always nan."},{"x":"Can someone please tell me what is the default distribution for a Tensor?\nif I write this code:\n<code class=\"lang-auto\">t = torch.Tensor(100, 100)\n<\/code>\nWhat is the distribution for t, normal distribution? Or uniform distribution? Thanks","y":"I am not sure it is stated explicitly in the doc.\nThe point is that unless you use a specific method to initialize it, it is uninitialized by default.","z":"Hi,\nBy default the content is uninitialized. So the tensor will be filled with whatever was in the memory before.\n\n\n\n albanD:\n\nSo the tensor will be filled with whatever was in the memory before.\n\n\nOk, Thank you, Could you please tell me where did you find this? I googled a lot, but found nothing.The document did not seem to tell us how the tensor was initialized.\nI am not sure it is stated explicitly in the doc.\nThe point is that unless you use a specific method to initialize it, it is uninitialized by default."},{"x":"Greetings I have some errors in the evaluation set in the sense that I sometimes achieve validation accuracies of 127%.\nI may be an error on cuda, or something?\nThanks in advance!\n<code class=\"lang-auto\">def evaluation(model, loader, epoch, mini_batch_size, sequence_size):\n    model.eval()\n    test_loss = 0\n    correct = 0\n    size_input = mini_batch_size * sequence_size\n    for (inputs, labels, agreement_score) in loader:\n        if(inputs.size(0) == size_input):\n            inputs = inputs.clone().reshape(mini_batch_size, sequence_size, inputs.size(1))\n            labels = labels.clone().squeeze().reshape(mini_batch_size*sequence_size)\n            agreement_score = agreement_score.clone().squeeze().reshape(mini_batch_size*sequence_size)\n        else:\n            (inputs, labels, agreement_score) = padd_incomplete_sequences(inputs, labels, agreement_score, mini_batch_size, sequence_size)\n            \n        \n        inputs, labels, agreement_score = Variable(inputs.cuda()), Variable(labels.cuda()), Variable(agreement_score.cuda())\n       \n        \n        output = model(inputs)\n        \n        loss = criterion(output, labels)\n        loss = loss.mean()\n        \n        #sum up batch loss\n        test_loss += loss.item()\n        pred = output.data.max(1, keepdim = True)[1]\n        correct += pred.eq(labels.data.view_as(pred)).cuda().sum()\n        \n    \n    \n    accuracy = 100. * correct \/ len(loader.dataset)\n    \n    validation_accuracy.append(accuracy)\n    validation_loss.append(loss.item())\n    validation_epochs.append(epoch)\n    \n    #test_loss \/= len(test_loader.dataset)\n    print('\\nTest set: loss: {:.4f}, Accuracy: {}\/{} ({:.0f}%)\\n'.format(\n    test_loss,\n    correct,\n    len(loader.dataset),\n    accuracy))\n<\/code>\nOutputs:\nTest set: loss: 1.6754, Accuracy: 329153\/258686 (127%)","y":"I think that might be the reason your Dataset length is smaller than the calculated predictions, since your Dataset returns the length of the data using its batch dimension.\nAssuming you have 240 samples, the DataLoader will return 10 batches a 24 samples.\nlen(loader.dataset) will also return 240.\nNow if you reshape these batches to 24*20000, your correct number of samples could be much higher, that the overall length.","z":"Could you print the shapes of output, pred and labels?\nSure sir,\nInputs shape : torch.Size([24, 20000, 100])\nOutput shape : torch.Size([480000, 6])\nLabels shape : torch.Size([480000])\nAgreement score shape: torch.Size([480000])\nCan it be a cuda error?\nI perform model training in cuda and I compute the loss and accuracy on cpu.\nIt looks like you are reshaping the batch dimension to mini_batch_size * seq_length, which might be the error.\nThe length of your Dataset will return the number of samples, while you multiply this number with the seq_length for the labels.\nIt is an implementation fault that I have written the code that way, but in this case mini_batch_size*sequence_length is the same as 24*20000.\nIn other words MINI_BATCH_SIZE == 24\nSEQUENCE_LENGTH == 20000\nThe reason why I\u2019m reshaping is for CrossEntropyLoss.\nThe model works well in general, but sometimes spits that high values.\nI think that might be the reason your Dataset length is smaller than the calculated predictions, since your Dataset returns the length of the data using its batch dimension.\nAssuming you have 240 samples, the DataLoader will return 10 batches a 24 samples.\nlen(loader.dataset) will also return 240.\nNow if you reshape these batches to 24*20000, your correct number of samples could be much higher, that the overall length.\nYou were right sir!\nI was padding the incomplete batches with zeroes and I was still computing the accuracy using the old dataset length.\nNow I made the following changes:\n<code class=\"lang-auto\">\n\ndef evaluation(model, loader, epoch, mini_batch_size, sequence_size):\n    model.eval()\n    test_loss = 0\n    correct = 0\n    padded_size = 0\n    size_input = mini_batch_size * sequence_size\n    for (inputs, labels, agreement_score) in loader:\n        if(inputs.size(0) == size_input):\n            inputs = inputs.clone().reshape(mini_batch_size, sequence_size, inputs.size(1))\n            labels = labels.clone().squeeze().reshape(mini_batch_size*sequence_size)\n            agreement_score = agreement_score.clone().squeeze().reshape(mini_batch_size*sequence_size)\n        else:\n            padded_size = size_input - inputs.size(0)\n            (inputs, labels, agreement_score) = padd_incomplete_sequences(inputs, labels, agreement_score, mini_batch_size, sequence_size)\n            \n        \n        inputs, labels, agreement_score = Variable(inputs.cuda()), Variable(labels.cuda()), Variable(agreement_score.cuda())\n       \n        \n        output = model(inputs)\n        \n        \n        loss = criterion(output, labels)\n        loss = loss.mean()\n        \n        #sum up batch loss\n        test_loss += loss.item()\n        pred = output.data.max(1, keepdim = True)[1]\n        correct += pred.eq(labels.data.view_as(pred)).cuda().sum()\n        \n    \n    \n    accuracy = 100. * correct \/ (len(loader.dataset) + padded_size)\n    \n    validation_accuracy.append(accuracy)\n    validation_loss.append(loss.item())\n    validation_epochs.append(epoch)\n    \n    #test_loss \/= len(test_loader.dataset)\n    print('\\nTest set: loss: {:.4f}, Accuracy: {}\/{} ({:.0f}%)\\n'.format(\n    test_loss,\n    correct,\n    (len(loader.dataset) + padded_size),\n    accuracy))\n<\/code>\nYou saved me sir, thanks again!"},{"x":"I was trying to find the reason why grouped convolution is implemented in PyTorch and whether it is based on a paper that shows the efficacy of this operation.\nAre there any benefits other than being able to distribute parameters across different devices?\nCan you please mention a few references that have discussed the reason why grouped convolution can be effective?","y":"There are some papers using grouped convolutions.\nDepthwise separable convolutions are used on the Xception paper with link \"https:\/\/arxiv.org\/abs\/1610.02357\" and here with link \"https:\/\/arxiv.org\/pdf\/1706.03059.pdf\". Both present grouped convolutions as an alternative to reduce the total number of network parameters and computational cost.\nThere are a bunch of other refs, but I cannot say I\u2019ve read many and thus will not cite without knowledge ","z":"There are some papers using grouped convolutions.\nDepthwise separable convolutions are used on the Xception paper with link \"https:\/\/arxiv.org\/abs\/1610.02357\" and here with link \"https:\/\/arxiv.org\/pdf\/1706.03059.pdf\". Both present grouped convolutions as an alternative to reduce the total number of network parameters and computational cost.\nThere are a bunch of other refs, but I cannot say I\u2019ve read many and thus will not cite without knowledge \nShuffleNet with link \"https:\/\/arxiv.org\/abs\/1707.01083\" also uses grouped convolutions, without going to the extreme of group size == 1. Again, the justification is the reduction in parameters and computations."},{"x":"I\u2019m going through the Pytorch Transfer Learning tutorial at: http:\/\/pytorch.org\/tutorials\/beginner\/transfer_learning_tutorial.html#visualize-a-few-images\nIn the data augmentation stage, there is the following step to normalize images:\ntransforms.Normalize([0.485, 0.456, 0.406], [0.229, 0.224, 0.225])\n\nI can understand why it\u2019s doing this but I can\u2019t find how the mean and std values get calculated? I tried to calculate the mean on the train data set and the mean values are:\narray([ 0.11727478,  0.04542569, -0.28624609], dtype=float32)","y":"All pre-trained models should be preprocessed in that same way with link \"https:\/\/discuss.pytorch.org\/t\/how-to-preprocess-input-for-pre-trained-networks\/683\/2\".\nThose values are mean, stddev for ImageNet, not for your specific dataset. You can also check the discussion here with link \"https:\/\/discuss.pytorch.org\/t\/confused-about-the-image-preprocessing-in-classification\/3965\".","z":"All pre-trained models should be preprocessed in that same way with link \"https:\/\/discuss.pytorch.org\/t\/how-to-preprocess-input-for-pre-trained-networks\/683\/2\".\nThose values are mean, stddev for ImageNet, not for your specific dataset. You can also check the discussion here with link \"https:\/\/discuss.pytorch.org\/t\/confused-about-the-image-preprocessing-in-classification\/3965\".\nThis makes sense now. It\u2019ll be much clearer if the author can add a line of comment in the tutorial to say why it\u2019s using those values."},{"x":"I have been following the ants and bees transfer learning tutorial from the official PyTorch Docs (http:\/\/pytorch.org\/tutorials\/beginner\/transfer_learning_tutorial.html). I am trying to finetune a VGG19 model by changing the final layer to predict one of two classes. I am able to modify the last fc layer using the following code.\nBut I get an error when executing the train_model function. The error is \u201csize mismatch at \/opt\/conda\/conda-bld\/pytorch_1513368888240\/work\/torch\/lib\/THC\/generic\/THCTensorMathBlas.cu:243\u201d. Any idea what the issue is ?\n<code class=\"lang-auto\">model_conv = torchvision.models.vgg19(pretrained=True)\nfor param in model_conv.parameters():\n    param.requires_grad = False\n\nmodel_conv = nn.Sequential(*list(model_conv.classifier.children())[:-1] +\n                     [nn.Linear(in_features=4096, out_features=2)])\nif use_gpu:\n    model_conv = model_conv.cuda()\n\ncriterion = nn.CrossEntropyLoss()\n\noptimizer_conv = optim.SGD(model_conv._modules['6'].parameters(), lr=0.001, momentum=0.9)\nexp_lr_scheduler = lr_scheduler.StepLR(optimizer_conv, step_size=7, gamma=0.1)\n\nmodel_conv = train_model(model_conv, criterion, optimizer_conv, exp_lr_scheduler, num_epochs=25)\n<\/code>","y":"so you just want to change the last layer of vgg19 ?\nyou can do like this.\n<code class=\"lang-auto\">old_classifier = list(model_conv.classifier.children()) # get the classifier part alone\nold_classifier.pop() # remove the last layer\nold_classifier.append(nn.Linear(4096,2)) # add a new layer\nmodel_conv.classifier = nn.Sequential(*old_classifier) # attach it to the original vgg model\n\n\n<\/code>","z":"Did you forget to flatten the output feature map of the final conv layer?\nI did not flatten the output feature map. Why is it required ? And how do you do that ? Sorry am a newbie with Deep learning and PyTorch!\nso you just want to change the last layer of vgg19 ?\nyou can do like this.\n<code class=\"lang-auto\">old_classifier = list(model_conv.classifier.children()) # get the classifier part alone\nold_classifier.pop() # remove the last layer\nold_classifier.append(nn.Linear(4096,2)) # add a new layer\nmodel_conv.classifier = nn.Sequential(*old_classifier) # attach it to the original vgg model\n\n\n<\/code>\nIt works now. Thanks a lot  !"},{"x":"I\u2019m trying to train my own dcgan for a project and I can\u2019t seem to get the generator to work right. I\u2019m prototyping things out on mnist to keep the problem simple while i experiment.\nI\u2019ve tried having a discriminator with an equal number of layers to the generator but in this case the discriminator quickly converges to 0.01 or lower and the generator just kind of generates random shapes that kinda start to look like numbers (if we are being generous) but don\u2019t fool the discriminator\nI\u2019ve tried lowering the number of conv layers in the discriminator. This actually seems to produce a convergence but its awful and the generated images are not numbers - the generator loss sits around 0.7.\nI\u2019ve tried labels smoothing the positive labels for the generator. this lowers the loss for the generator (which doesnt mean much) but the images still suck.\nI\u2019ve tried different loss functions (MSE, and BCELoss).\nI\u2019ve implemented dcgan (somewhat) succuessfully in tensorflow before but I\u2019m new to pytorch -\nQuestions:\nis there a way in pytorch for me to see if the discriminator can see the gradients of the generator maybe i screwed something up and that\u2019s why the discriminator can overfit so heavily?\nhow many epochs should i have to train dcgan to get decent results? to converge? I\u2019ve looked around and seen different numbers.\nwhat exactly should I be calling .cuda() on? I tried to read up on it but I\u2019m not getting a lot of clear answers. right now i\u2019m calling it on pretty much everything.\nhere\u2019s my training code:\nfor epoch in range(n_epochs):\nfor i, image_batch in enumerate(data_loader):\n    image_batch = image_batch[0]\n    # --- train discriminator ---\n    real_images = to_variable(image_batch)\n    outputs = discriminator(real_images)\n    pos_labels = to_variable(torch.ones(outputs.data.shape))\n    real_loss = loss_func(outputs, pos_labels)\n\n    # make fake images from generator and\n    # see how much probability in excess of\n    # 0 the discriminator gives each one to being\n    # real\n    noise = to_variable(torch.randn(batch_size, noise_dim))\n    fake_imagiies = generator(noise)\n    outputs = discriminator(fake_images)\n    neg_labels = to_variable(torch.zeros(outputs.data.shape))\n    fake_loss = loss_func(outputs, neg_labels)\n    \n    # backpropagate the loss from both\n    # real and fake images for discriminator\n    total_loss = real_loss + fake_loss\n    discriminator.zero_grad()\n    total_loss.backward()\n    d_optimizer.step()\n    \n    # --- train generator ---        \n    noise = to_variable(torch.randn(batch_size, noise_dim))\n    \n    # get the generator loss by seeing\n    # how close to being 1.0 (positive label)\n    # each fake image was throughthe discriminator\n    fake_images = generator(noise)\n    outputs = discriminator(fake_images)\n    gen_labels = to_variable(torch.ones(outputs.data.shape))\n    gen_loss= loss_func(outputs, gen_labels)\n    \n    # backpropagate and update \n    # generator\n    generator.zero_grad()\n    gen_loss.backward()\n    g_optimizer.step()   \n    \n    if not i % 250:\n        print('epoch {}'.format(epoch))\n        print('image {}'.format(i))\n        print('generator loss: {}, discriminator loss: {}'.format(gen_loss.data[0], total_loss.data[0]))      \n\nfake_img = generator(test_noise)\nimg = denorm_mnist(fake_img)\nresult = transforms.Compose([transforms.ToPILImage()])(fake_img.cpu().data[0])\nplt.figure()\nplt.imshow(result)\nplt.axis('off')\nplt.show()\n\nhere is my generator:\nclass Generator(nn.Module):\ndef __init__(self):\n    super(Generator, self).__init__()\n    self.main = nn.Sequential(\n        nn.ConvTranspose2d(noise_dim, \n                          g_filter_depth*8, \n                          kernel_size=4, \n                          stride=1, \n                          padding=0,\n                          bias=False),\n        nn.BatchNorm2d(g_filter_depth*8),\n        nn.LeakyReLU(0.2),\n        \n        nn.ConvTranspose2d(g_filter_depth*8, \n                          g_filter_depth*4, \n                          kernel_size=g_kernel_size, \n                          stride=2,\n                          padding=1,\n                          bias=False),\n        nn.BatchNorm2d(g_filter_depth*4),\n        nn.LeakyReLU(0.2),\n        \n        nn.ConvTranspose2d(g_filter_depth*4, \n                          g_filter_depth*2, \n                          kernel_size=g_kernel_size, \n                          stride=2, \n                          padding=1,\n                          bias=False),\n        nn.BatchNorm2d(g_filter_depth*2),\n        nn.LeakyReLU(0.2),\n        \n        nn.ConvTranspose2d(g_filter_depth*2, \n                          g_filter_depth, \n                          kernel_size=g_kernel_size, \n                          stride=2, \n                          padding=1,\n                          bias=False),\n        nn.BatchNorm2d(g_filter_depth),\n        nn.LeakyReLU(0.2),\n        \n        nn.ConvTranspose2d(g_filter_depth, \n                           d_filter_depth_in,\n                           kernel_size=g_kernel_size,\n                           stride=2,\n                           padding=1,\n                           bias=False),\n        nn.Tanh()\n    )\n    \ndef forward(self, inputs):\n    inputs = inputs.view(inputs.size(0), inputs.size(1), 1, 1)\n    output = self.main(inputs)\n    return output\n\nhere\u2019s my discriminator:\nclass Discriminator(nn.Module):\ndef __init__(self):\n    super(Discriminator, self).__init__()\n    self.main = nn.Sequential(\n        nn.Conv2d(in_channels=d_filter_depth_in, \n                  out_channels=d_filter_depth, \n                  kernel_size=4, \n                  stride=2,\n                  padding=1,\n                  bias=False),\n        nn.LeakyReLU(0.2),\n        \n        nn.Conv2d(in_channels=d_filter_depth, \n                  out_channels=d_filter_depth*2, \n                  kernel_size=4, \n                  stride=2,\n                  padding=1,\n                  bias=False),\n        nn.BatchNorm2d(d_filter_depth*2),\n        nn.LeakyReLU(0.2),\n        \n        nn.Conv2d(in_channels=d_filter_depth*2, \n                  out_channels=d_filter_depth*4, \n                  kernel_size=4, \n                  stride=2,\n                  padding=1,\n                  bias=False),\n        nn.BatchNorm2d(d_filter_depth*4),\n        nn.LeakyReLU(0.2),\n        \n        nn.Conv2d(in_channels=d_filter_depth*4, \n                  out_channels=d_filter_depth*8, \n                  kernel_size=4, \n                  stride=2,\n                  padding=1,\n                  bias=False),\n        nn.BatchNorm2d(d_filter_depth*8),\n        nn.LeakyReLU(0.2),\n        \n        nn.Conv2d(in_channels=d_filter_depth*8, \n                  out_channels=1, \n                  kernel_size=4, \n                  stride=1,\n                  padding=0,\n                  bias=False),\n        nn.Sigmoid()\n    )\ndef forward(self, inputs):\n    output = self.main(inputs)\n    return output\n\nany feedback. or pointing out any errors or rookie pytorch mistakes would be greatly appreciated.","y":"it\u2019s possible I may just have needed to tune my \u2018patience hyperparameter\u2019 \u2013 lol. after making sure everything was properly functioning it seems to be able to generate decent numbers past 5 epochs. or at least it seems to make progress against the generator and produce more believable numbers.","z":"it\u2019s possible I may just have needed to tune my \u2018patience hyperparameter\u2019 \u2013 lol. after making sure everything was properly functioning it seems to be able to generate decent numbers past 5 epochs. or at least it seems to make progress against the generator and produce more believable numbers.\nIn case anyone wants an example of some decent code (other than the pytorch implementation) here is the resulting GAN and blog post I put together. with link \"https:\/\/medium.com\/\/using-gans-to-create-monsters-for-your-game-c1a3ece2f0a0\""},{"x":"Right now, most of the examples just give out image, lables based upon the folder sturcture. How do i write a dataloader\/dataset, such that i can use it for a network that take input image and outputs image?","y":"You could create your own Dataset like this:\n<code class=\"lang-auto\">class MyDataset(Dataset):\n    def __init__(self, data_paths):\n        self.paths = data_paths \n\n    def __getitem(self, index):\n        image, target_image = load_images(self.paths[index]) # load as np.array or PIL.Image\n        # transform your images here\n        x = transformations.ToTensor()(image)\n        y = transformations.ToTensor()(target_image)\n\n        return x, y\n\n    def __len__(self):\n        return len(self.paths) \n<\/code>","z":"You could create your own Dataset like this:\n<code class=\"lang-auto\">class MyDataset(Dataset):\n    def __init__(self, data_paths):\n        self.paths = data_paths \n\n    def __getitem(self, index):\n        image, target_image = load_images(self.paths[index]) # load as np.array or PIL.Image\n        # transform your images here\n        x = transformations.ToTensor()(image)\n        y = transformations.ToTensor()(target_image)\n\n        return x, y\n\n    def __len__(self):\n        return len(self.paths) \n<\/code>\nThanks , I was able to implement a custom loader with the same idea on the lines of ImageFolder!\nVery nice!\nI was writing from my phone and just realized, that my code snippet has some typos (e.g. transformations should be transforms), sorry for that and apparently you fixed these issues anyway. \nYea, thats the benefit of using an advanced IDE like PyCharm "},{"x":"Hi,\nI am new to PyTorch, and I am enjoying it so much, thanks for this project!\nI have a question. Suppose I have an image of reduced size obtained through multiple layers of convolution and max-pooling. I need to down sample this image to the original size, and was wondering what are your recommendations for doing that?\nI did read the documentation and tried to use the max-unpooling layer in the decoder part of the network. However, it seems that it needs the indices produced by max-pooling in the encoding part. Do I need to save every intermediate index of the encoding part to use in the decoding part?\nBelow is an example code of what I am trying to achieve:\n<code class=\"lang-python\">class MyNet(Module):\n    def __init__(self, pastlen, futurelen, cspace):\n        super(MyNet, self).__init__()\n\n        # problem dimensions\n        P = pastlen\n        F = futurelen\n        C = 3 if cspace == \"RGB\" else 1\n\n        self.encoder = Sequential(\n            Conv2d(C, 64, kernel_size=3, padding=1),\n            BatchNorm2d(64), ReLU(),\n            MaxPool2d(2),\n            Conv2d(64, 128, kernel_size=5, stride=5),\n            BatchNorm2d(128), ReLU(),\n            MaxPool2d(3),\n            Flatten()\n        )\n\n        self.recurrence = Sequential(\n            RNN(input_size=5*3*128, hidden_size=20*20*C, num_layers=1)\n        )\n\n        self.decoder = Sequential(\n            ConvTranspose2d(F*C, F*C, kernel_size=3),\n            BatchNorm2d(F*C), ReLU(),\n            MaxUnpool2d(2) # doesn't work, asks for indices...\n        )\n<\/code>","y":"I suppose you would like to upsample your image to the original size. If so, you could use ConvTranspose2d or Resize from torchvision.\nThere are mixed results with both approaches. I think the original UNet implementation used ConvTranspose2d.","z":"Alternatively, I am down sampling the images with ConvTranspose2d itself, by choosing a stride of 2 every time. Please let me know if that is not a good idea.\nAt the end I will need to get the image with a specific size. What is a good operation for that?\nI suppose you would like to upsample your image to the original size. If so, you could use ConvTranspose2d or Resize from torchvision.\nThere are mixed results with both approaches. I think the original UNet implementation used ConvTranspose2d.\nI am using ConvTranpose2d, not getting very results at the moment, but I am still debugging things.\nAre there any utilities developed by the community to let\u2019s say visualize the intermediate layers of a PyTorch Sequential object? I am doing it manually sending summaries to TensorBoard, but if there is an effort somewhere already that I could reuse, that would be great."},{"x":"I am using pre-trained inceptionv3 model in my script. I normalize tensor with following mean and std:\n<code class=\"lang-auto\">mean=[0.485, 0.456, 0.406]\nstd=[0.229, 0.224, 0.225]\n<\/code>\nI would like to know how can I perform reverse operation. Currently, I perform this op in the following way:\n<code class=\"lang-auto\">#first-remove batch dimension\nimg[0] = img[0] * 0.229\nimg[1] = img[1] * 0.224 \nimg[2] = img[2] * 0.225 \nimg[0] += 0.485 \nimg[1] += 0.456 \nimg[2] += 0.406\n<\/code>\nIs there a better way to do this?","y":"Hi,\nyou can vectorize it: mt = torch.FloatTensor(mean).view(3,1,1), st similar allows you to do img*mt. This is by virtue of the broadcasting implemented in PyTorch. As PyTorch automatically prepends dimensions, it even works if img is a batch of images.\nBest regards\nThomas","z":"Hi,\nyou can vectorize it: mt = torch.FloatTensor(mean).view(3,1,1), st similar allows you to do img*mt. This is by virtue of the broadcasting implemented in PyTorch. As PyTorch automatically prepends dimensions, it even works if img is a batch of images.\nBest regards\nThomas\nHi,\nThanks for the reply. It works fine. In fact, I knew this and also tried it before posting my question here. I thought the problem is with unnormalization but it seems like the problem is something else."},{"x":"I am new to PyTorch and was setting up my notebook for image classification task using pre-trained inception_v3 model. I first load image using PIL and preprocess it as follows:\n<code class=\"lang-auto\">std=[0.229, 0.224, 0.225]\n\npreprocess = transforms.Compose([\n                transforms.Resize((299,299)),\n                transforms.ToTensor(),\n                transforms.Normalize(mean, std)\n            ])\nimage_tensor = preprocess(img)\nimage_tensor = image_tensor.unsqueeze(0)\n<\/code>\nHere is the problem. Now, I want to display this input image (image_tensor). My next cell looks like this:\n<code class=\"lang-auto\">x = image_tensor.squeeze(0) #remove batch dimension\n#unnormalize\nx.mul_(torch.FloatTensor(std).view(3,1,1)).add_(torch.FloatTensor(mean).view(3,1,1))\nx = x.numpy()\nx = np.transpose( x , (1,2,0))   # C X H X W  ==>   H X W X C\nx = np.clip(x, 0, 1)\nplt.imshow(x)\n<\/code>\nIf I run above cell it works fine and the image will be displayed. But If I run this cell again then image gets darker. If I run it once again then it gets even more darker. I have attached images. I am not getting why it\u2019s happening and how can I stop this. I want this cell to display original input image every time I run it. Any explanation will be helpful to me. Thanks!\n","y":"Yea my bad. So it looks like mul_ and add_ are \u2018inplace\u2019 operations which means that the underlying data will get modified. That\u2019s probably why this is happening. Use:\nhttp:\/\/pytorch.org\/docs\/master\/torch.html#torch.add\nhttp:\/\/pytorch.org\/docs\/master\/torch.html#torch.mul","z":"It looks like you are modifying your data. So every time you run the cell the modification you are doing is happening again.\nHi, I understand this. But not getting where is it happening. I don\u2019t change image_tensor in this cell. Do you know how can I stop this? Thanks!\nYea my bad. So it looks like mul_ and add_ are \u2018inplace\u2019 operations which means that the underlying data will get modified. That\u2019s probably why this is happening. Use:\nhttp:\/\/pytorch.org\/docs\/master\/torch.html#torch.add\nhttp:\/\/pytorch.org\/docs\/master\/torch.html#torch.mul\nI tried that. I also tried replacing that line with manual ops on each channel. It didn\u2019t work.\nNow, it works. I must have made some mistake.\nThanks!"},{"x":"Here is my code:\n<code class=\"lang-auto\">filters = torch.randn(8,4,3,3).cuda()\ninputs = torch.randn(1,4,5,5).cuda()\ntorch.nn.functional.conv2d(inputs, filters, padding=1)\n<\/code>\nError: TypeError: argument 0 is not a Variable.\nI don\u2019t want to use autograd.Variable to wrap my tensors and treat each computation as a node, even it\u2019s a very thin wrapper since I might loop each layer 1000 times. Why is a variable necessary for convolution operation?","y":"Hi,\nI think the main worry you have with using Variables everywere is the overhead it could imply compared to use pure Tensors directly? This has been looked into in details and in the current master branch, the overhead of using a Variable (with requires_grad=True or torch.no_grad()) is negligible. So you should not worry too much about the fact that you use Variables (or the merged version soon  ).\nI guess you want to look at the new Tensor as not only an n-d array, but an n-d array that can optionnaly keep track of operations (so that you can use it with an autograd engine).\nDid I miss another worry you had?","z":" \nIt seems there is no way in PyTorch to do convolution without using Variable class, which is a little unfortunate.  I\u2019m going to set the flag requires_grad=False. Is there any big overhead if I\u2019m going to loop a convolution and a deconvolution 1000 times to solve the coefficients for each lalyer and there are totally 10 layers? My understanding is that PyTorch will treat each operation as a node thus there will be 1000 nodes for each layer. Maybe I\u2019m wrong. I hope PyTorch doesn\u2019t treat this 10 layer network as a 10000 layers \u2026\nAfterall, I just need to loop the convolution and transpose convolution to solve an ISTA optimization problem for each layer. Only the convolution results matter and I will take care of the gradients myself for each layer.\nFurther, is that possible for Pytorch to provide a set of convolution and transpose convolution functions for purely tensors (not Variables) so that people working on inference based model don\u2019t have to worry about the autogradients and nodes, which are not very appropriate concepts in these models. This will be much more friendly to develop inference based unsupervised learning methods, E.g. Matthew Zeiler\u2019s adaptive deconvolutional networks. The convolution functions for tensors seems quick easy to provide. Or if the current PyTorch mechanism is already sufficient, I feel it deserves a slightly more detailed description on how to implement inference based layers without introducing big overhead.\nHi,\nFrom the next release onward, Tensors and Variables are going to be merged into a single object.\nIf you want to forward in a model (or just a convolution here) with minimal overhead when you will not use backward, then you should create your Variable with volatile=True.\n\nFrom the next release onward, Tensors and Variables are going to be merged into a single object.\n\nIs there any reason for this? When will the next release come out?\nThe reason is that the difference between Variable and Tensor creates a lot of misunderstanding and errors for new comers.\nI don\u2019t have the exact time for the release, but this change is mostly done in the master branch already.\nOh but sorry to be pestering you, but wouldn\u2019t that account for change of docs all the main tutorials on pytorch website ? I would like a brief overview of the change if that isn\u2019t too much to ask.\nYes that is going to be a major change and a lot of documentation will be heavily simplified (this will be done at the same time as the release itself).\nI am not actively working on it, so I just have a high level point of view:\nBreaking change are going to be kept at a minimum, so current code should keep working as is.\nIf you look at the current PR on the github repo, a lot of them are working on this change.\nThe main idea is that Variables are going to disappear from the python world. Everything is going to be a Tensor, or in nn a Parameter. And then the rest of the code is going to work as before, you won\u2019t need to wrap things in Variables anymore.\nFor more details, I would advice looking at current PR that are made and asking here again so that people that know the details better can answer.\nThanks for the comments!\nIf I create my variables by the following code, can I assume this is a clean tensor in the current version?\n<code class=\"lang-auto\">filters = Variable(torch.randn(8,4,3,3).cuda(), requires_grad=False, volatile=True)\nx = Variable(torch.randn(2,4,5,5).cuda(), requires_grad=False, volatile=True)\n<\/code>\nI would like to loops the following computation without creating an increasingly large subgraph:\n<code class=\"lang-auto\">for j in xrange(100000):\n    y = torch.nn.functional.conv2d(x, filters, padding=1)\n    x = torch.nn.functional.conv_transpose2d(y, filters, padding = 1)\n<\/code>\nI checked the GPU memory and it seems not increasing while the code is running and I guess it\u2019s doing what I want. But I just want to double check since in my code a lot of this kind operations will be used. Also, in an inference based model, there usually is not a \u2018forward\u2019 direction since these model follows an analysis-by-inference principle. I hope the \u2018forward\u2019 in the previous reply doesn\u2019t mean that the optimization is unrolled into many layers, I guess it won\u2019t in the above setting. But can anyone confirm this?\nIn Tensorflow this can be done by using Parameter to build one layer graph, then use python control flow to loop the computation though implementing a fully inference based model is still quite hard in Tensorflow. But since PyTorch build the graph dynamically, I\u2019m a little confused on how to achieve this safely.\nComment: Putting Variable under autograd package is apparently a choice with the pre-assumption that PyTorch will focus on error BP based models. This assumption makes PyTorch a little narrow minded. If PyTorch can provide clean convolutions for Tensors, all the people working on inference based vision models (RBM or general graphical models, adaptive deconvNet, sparse coding etc.) will probably benefit from it. Further it will make PyTorch much more general looking (as also a mathematical library) and yet this step is so easy to make.\n\n\n\n yubeic:\n\nIf I create my variables by the following code, can I assume this is a clean tensor in the current version?\n\n\nIt\u2019s not a clean tensor. It\u2019s a variable that doesn\u2019t track history. But it basically satisfies what you need.\nMoreover, since you are using the functional interface. If your input and filters don\u2019t require grad, then no history will be tracked anyways.\n\n\n\n yubeic:\n\nI hope the \u2018forward\u2019 in the previous reply doesn\u2019t mean that the optimization is unrolled into many layers, I guess it won\u2019t in the above setting. But can anyone confirm this?\n\n\nI\u2019m not sure what you mean by optimization being unrolled into many layers. Optimization (you probably mean model optimization) is not even applied here. There is no history, no gradients, only forward results.\n\n\n\n yubeic:\n\nBut since PyTorch build the graph dynamically, I\u2019m a little confused on how to achieve this safely.\n\n\nWhy does dynamic graph imply being not safe?\n\n\n\n yubeic:\n\nComment: Putting Variable under autograd package is apparently a choice with the pre-assumption that PyTorch will focus on error BP based models. This assumption makes PyTorch a little narrow minded. If PyTorch can provide clean convolutions for Tensors, all the people working on inference based vision models (RBM or general graphical models, adaptive deconvNet, sparse coding etc.) will probably benefit from it. Further it will make PyTorch much more general looking (as also a mathematical library) and yet this step is so easy to make.\n\n\nYou have some valid point, but it\u2019s not really that we are narrow minded. It was a design choice to separate things that tracks history for BP (which became Variable) and things that don\u2019t (which became Tensor). And the former is naturally in the .autograd namespace. And due to popularity reasons and code structure, things like conv layers are only directly supported on Variables. (you can also make them work directly on tensors with a bit of work.) Futhermore, this volatile=True option (you don\u2019t need requires_grad=False if you set volatile=True) already gives you very similar experience to directly working on tensors. Moreover, as  mentioned above, we have merged the two classes together. So I don\u2019t really get the reason for this complaint.\nThanks a lot for this very detailed explanation, Simon! \n\nI\u2019m not sure what you mean by optimization being unrolled into many layers. Optimization (you probably mean model optimization) is not even applied here. There is no history, no gradients, only forward results.\n\n\u2018Optimization\u2019 was a typo, I actually mean computation. For inference based models, e.g. in Adapt DeconvNet, the coefficients in each layer are computed by solving an optimization, which requires many iterations of convolution and transpose convolution. Then the coefficients are sent to the next layer. In BP based networks, the coefficients are solved by a single convolution, which is forward. My previous concern was that if I manually implement the inference optimization, which is similar to the for loop I provided earlier, the underlying PyTorch implementation might build some graph or other structures. Building this graph for an inference optimization is like unrolling the optimization into many forward layers. According to what you said, it seems in this case (requires_grad=False or volatile=True) PyTorch won\u2019t do anything heavy.\n\nWhy does dynamic graph imply being not safe?\n\nJust as the above explanation, by \u2018safe\u2019 I really mean there is no additional expensive behaviors beyond just the tensor computation.\n\nYou have some valid point, but it\u2019s not really that we are narrow minded. It was a design choice to separate things that tracks history for BP (which became Variable) and things that don\u2019t (which became Tensor). And the former is naturally in the .autograd namespace.\n\nSeparating Variable and Tensor was great! That simplifies a lot of code migration from CPU numpy code to GPU Tensor code. This was one of the reasons a few peers switched from Tensorflow to PyTorch. Another big reason is this forum is awesome!\n\nAnd due to popularity reasons and code structure, things like conv layers are only directly supported on Variables. (you can also make them work directly on tensors with a bit of work.)\n\nIt\u2019s why I said \u2018yet this step is so easy to make\u2019. Actually I was thinking about writing a set of convolution functionals for Tensors. If you can give me some hints or pointers, that will be great! When you use Variable under autograd package to build an inference based model, it generally makes people worry about the graph or other overhead. I actually discussed this with a few other users of PyTorch and Tensorflow. Our general consensus is that when you try to build an inference model, this design makes things much less straight forward. And overall, only Tensors and convolutions (for Tensors) are needed.\n\nFuthermore, this volatile=True option (you don\u2019t need requires_grad=False if you set volatile=True) already gives you very similar experience to directly working on tensors.\n\nOkay, then I will use this while waiting for the next version.\n\nMoreover, as  mentioned above, we have merged the two classes together.\n\nI didn\u2019t completely get this. Does that mean when they are merged, there will be a set of convolution function provided for clean tensors? If not, I would like to start to implement these functions though I might need some minimal instructions.\n\nSo I don\u2019t really get the reason for this complaint.\n\nMaybe narrow minded was a bit too strong. (We actually discussed what was the right word to express the feeling. It\u2019s really just a wish that PyTorch can become a better framework for theoretical modeling development.) We actually think PyTorch did a great job on the Tensor part. If a set of convolution functions are provided for clean Tensors, the framework is going to be much more perfect to support many inference based models. If my complaint was too strong, I apologize for that since PyTorch really did a good job.\nHi,\nI think the main worry you have with using Variables everywere is the overhead it could imply compared to use pure Tensors directly? This has been looked into in details and in the current master branch, the overhead of using a Variable (with requires_grad=True or torch.no_grad()) is negligible. So you should not worry too much about the fact that you use Variables (or the merged version soon  ).\nI guess you want to look at the new Tensor as not only an n-d array, but an n-d array that can optionnaly keep track of operations (so that you can use it with an autograd engine).\nDid I miss another worry you had?\n\nI think the main worry you have with using Variables everywere is the overhead it could imply compared to use pure Tensors directly? This has been looked into in details and in the current master branch, the overhead of using a Variable (with requires_grad=True or torch.no_grad()) is negligible.\n\nActually I was only worrying about the requires_grad=False case since the inference based models I\u2019m working on only needs tensors and convolutions. I just want to make sure that when I turn off the requires_grad option I can treat the variables as tensors and loop however many times without thinking about the additional structures. maybe \u201cPremature optimization is the root of all evil.\u201d \n\nDid I miss another worry you had?\n\nThanks a lot for all of these comments and explanations. Thanks to , too.  I think I have a much better idea and turning off the gradient is a solution for now. The new release of PyTorch sounds really exciting!\nHey,\nI know this thread is already solved, but I stumbled here and thought this page in the pytorch docs with link \"http:\/\/pytorch.org\/docs\/0.3.1\/notes\/autograd.html\" belonged here. I am actually quite surprised nobody posted this as a solution already!\nIt clearly explains the role of Variable and how you can disable the tracking of history through volatile=True.\nAlways wanted to be able to tell someone to RTFM. \nHave a great day!\n\nThanks for posting this link here. That page in the doc helps but doesn\u2019t solve my initial question automatically. Prorogation of the flags wasn\u2019t my concern since all of my variable will have the gradient flag off. My initial question was different from what\u2019s described in the doc and it\u2019s a common problem of using nearly all of the popular deep learning frameworks when you try to implement an inference based model since these frameworks were developed biased towards error BP based models (for an obvious reason after Alexnet). In Tensorflow we need to do other hacks to get around this.\nBut inference based models are still important given many key innovations in even error BP based deep learning models came from these models. These two kinds of models are deeply related beyond some superficial difference. My post was also to suggest building a better support for inference models. If you work on these models, I think you\u2019d already know what I\u2019m talking about. If not, Matthew Zeiler\u2019s adaptive deconvnet is one of my favorite papers on this track in case you are interested.\nThe current design of PyTorch is not bad at all. (Except it\u2019s a little confusing that using GPU convolution on Tensors needs a Variable under autograd module.) Turning off the gradient seems have solved the problem nicely and my early implementation went well. However, the new release sounds a lot more straight forward.\nHey,\nI guess I must have misinterpreted your question then.\nI thought you were looking for implementing convolution networks that worked without saving the computations for performing the BP algorithm. (aka executing them on tensors and not variables)\nThe documentation states that if you use volatile = True it won\u2019t save the computational graph needed for BP, which is basically the same as executing the convolution on tensors\u2026\nAnyway, thanks for the paper. It\u2019s always nice to read about adjacent fields of research, to broaden my own understanding of ML. "},{"x":"I recently installed pytorch and torchvision with command\n<code class=\"lang-auto\">conda install pytorch torchvision -c pytorch\n<\/code>\nbut I realized that some code was added to transforms (for example I saw RandomAffine here with link \"https:\/\/github.com\/pytorch\/vision\/tree\/master\/torchvision\/transforms\" ).\nI tried to do an upgrade of my torchvision module (which is 0.2.0) with conda upgrade torchvision -c pytorch but it says that the requirement is already satisfied. RandomAffine is not available though. What command should I execute to get this new (and very useful) transformation?\nThanks","y":"The transform you referred to is available at master, but not in a release yet. You can build from source (https:\/\/github.com\/pytorch\/vision). It should be very easy.","z":"The transform you referred to is available at master, but not in a release yet. You can build from source (https:\/\/github.com\/pytorch\/vision). It should be very easy.\nThanks  for the info."},{"x":"Hi, I\u2019m a new fan of Pytorch and I\u2019m trying to write some transforms on my dataset:\n<code class=\"lang-auto\">data_transforms = {\n    'train': transforms.Compose([\n        transforms.Resize(256),\n        transforms.RandomResizedCrop(224),\n        transforms.RandomHorizontalFlip(0.5),\n        ...\n<\/code>\nbut I get this error:\n<code class=\"lang-auto\">---------------------------------------------------------------------------\nTypeError                                 Traceback (most recent call last)\n<ipython-input-37-3baaf9faeff4> in <module>()\n      6         transforms.Resize(256),\n      7         transforms.RandomResizedCrop(224),\n----> 8         transforms.RandomHorizontalFlip(0.5),\n      9         transforms.RandomGrayscale(p=0.1),\n     10         transforms.ColorJitter(brightness=0.1,constrast=0.1,saturation=0.1,hue=0.1),\n\nTypeError: object() takes no parameters\n<\/code>\nDocumentation though says that RandomHorizontalFlip takes one argument, the probability of flipping. Where\u2019s my error?\n(pytorch 0.3.1, torchvision 0.2.0)\nEDIT: out of curiosity, I removed the problematic line and it throws an error now on the ColorJitter transform:\n<code class=\"lang-auto\">TypeError: __init__() got an unexpected keyword argument 'constrast'\n<\/code>\nLooks like a version problem but as I said before, upgrading does not change anything (\u201crequirement already statisfied\u201d). Any kind of hint would be greatly appreciated.","y":"The parameter was added to repo less than a month ago, and hasn\u2019t been included in a release yet: https:\/\/github.com\/pytorch\/vision\/commit\/59858699563964e95931fd9328d88b67a7704587#diff-549cc9d2872b8748db818d9a51e34708. You can remove the 0.5 for now.","z":"Change constrast to contrast.\nOMG, how embarrassing! \nI was so sure I had a problem with the version that I didn\u2019t notice my typo, thanks Shani.\nBut the error with the argument to RandomHorizontalFlip remains\u2026\nThe parameter was added to repo less than a month ago, and hasn\u2019t been included in a release yet: https:\/\/github.com\/pytorch\/vision\/commit\/59858699563964e95931fd9328d88b67a7704587#diff-549cc9d2872b8748db818d9a51e34708. You can remove the 0.5 for now.\nThanks  for the info!"},{"x":"Hi,\nI have a question regarding BatchNorm2d, below is the output when i call optimizer.step() to update parameters. I can see that the conv1.weight tensor size is 64 but it should have been 64x64x3x3 (Number of Input Channels = 64)?\nResolved now.","y":"There was some bug in the code.","z":"There was some bug in the code."},{"x":"Hi everybody,\nI have an image and want to calculate loss just for 1 pixel on it, therefore I use a mask array and masked the other value that I want to be shown in the loss . my code is just below, but I got an error and I couldn\u2019t fix it . just somebody can please help to know me whats wrong with my code?\n<code class=\"lang-auto\">fm = 64\nnet = nn.Sequential(\n    MaskedConv2d('A', 3,  fm, 3, 1, 1, bias=False), nn.BatchNorm2d(fm), nn.ReLU(True),\n    MaskedConv2d('A', fm, fm, 3, 1, 1, bias=False), nn.BatchNorm2d(fm), nn.ReLU(True),\n    MaskedConv2d('A', fm, fm, 3, 1, 1, bias=False), nn.BatchNorm2d(fm), nn.ReLU(True),\n    MaskedConv2d('A', fm, fm, 3, 1, 1, bias=False), nn.BatchNorm2d(fm), nn.ReLU(True),\n    MaskedConv2d('A', fm, fm, 3, 1, 1, bias=False), nn.BatchNorm2d(fm), nn.ReLU(True),\n    MaskedConv2d('A', fm, fm, 3, 1, 1, bias=False), nn.BatchNorm2d(fm), nn.ReLU(True),\n    MaskedConv2d('A', fm, fm, 3, 1, 1, bias=False), nn.BatchNorm2d(fm), nn.ReLU(True),\n    MaskedConv2d('A', fm, fm, 3, 1, 1, bias=False), nn.BatchNorm2d(fm), nn.ReLU(True),\n    nn.Conv2d(fm, 3, 1))\n\n\nsample = V(torch.zeros(bs, 3, picSize,picSize))\nresidual = V(torch.zeros(bs, 3, picSize, picSize))\nres_mask = V(torch.zeros_like(residual.data))\nfor batch_idx, (images, labels) in enumerate(dataloaders):\n    images = V(images, requires_grad = False)\n\n    for i in range(picSize):\n        for j in range(picSize):\n            optimizer.zero_grad()\n            out = net(sample)\n            residual[:,:,i,j] = (images[:,:,i,j] - out[:,:,i,j])**2\n            res_mask = V(torch.zeros_like(residual.data))\n            res_mask[:,:,i,j] = 1\n            residual = torch.mul(residual,res_mask)\n            loss = (residual.sum()\/(bs * 3))\n            pdb.set_trace()\n            loss.backward(retain_graph = True)\n            optimizer.step()\n            sample[:,:,i,j] = images[:,:,i,j]\n<\/code>\nError: one of the variables needed for gradient computation has been modified by an inplace operation\n","y":"I suspect that some_var[:,:,i,j] = stuff counts as an inplace operation.\nIf I have understood correctly what your code is doing then most of them are unnecessary, and the last one sample[:,:,i,j] = images[:,:,i,j] doesn\u2019t need gradients.\nWould this work?\nfor i in range(picSize):\n    for j in range(picSize):\n        optimizer.zero_grad()\n        out = net(sample)\n        residual = (images[:,:,i,j] - out[:,:,i,j])**2\n        loss = (residual.sum()\/(bs * 3))\n        pdb.set_trace()\n        loss.backward(retain_graph = True)\n        optimizer.step()\n        sample.data[:,:,i,j] = images[:,:,i,j]","z":"I suspect that some_var[:,:,i,j] = stuff counts as an inplace operation.\nIf I have understood correctly what your code is doing then most of them are unnecessary, and the last one sample[:,:,i,j] = images[:,:,i,j] doesn\u2019t need gradients.\nWould this work?\nfor i in range(picSize):\n    for j in range(picSize):\n        optimizer.zero_grad()\n        out = net(sample)\n        residual = (images[:,:,i,j] - out[:,:,i,j])**2\n        loss = (residual.sum()\/(bs * 3))\n        pdb.set_trace()\n        loss.backward(retain_graph = True)\n        optimizer.step()\n        sample.data[:,:,i,j] = images[:,:,i,j]\nTnx for your reply, I did those things because I want just a specific pixel of my output to be in the loss cost and not the whole of the output. for that I masked my output.\nIs there any option to my code do this work without any error?\nI believe my proposal does just that.\nresidual is calculated as the mse of a single pixel, and loss is calculated from the residual of that single pixel (I assume bs is the batch size), so when you do loss.backward() it will backpropagate through the calculations that lead to that pixel.\nYeah your right, I did the modification. and it worked very well. Tnx again for your reply "},{"x":"I have a few rectangle figures and I wanna reshape them to square, is there a way besides doing cropping? For example, if I have a figure with size 20050, and I wanna a 4040, then the desired way is to scale x axis to 20% and y to 80%. Thanks!!","y":"You can specify the desired size as a tuple in torchvision.transforms.Resize with link \"http:\/\/pytorch.org\/docs\/master\/torchvision\/transforms.html#torchvision.transforms.Resize\"","z":"You can specify the desired size as a tuple in torchvision.transforms.Resize with link \"http:\/\/pytorch.org\/docs\/master\/torchvision\/transforms.html#torchvision.transforms.Resize\""},{"x":"So I have a brunch of pics and I want to turn all of them to tensors. I am using transforms and PIL library to do so\n<code class=\"lang-auto\">loader = transforms.Compose([transforms.Resize([IMSIZE, IMSIZE]), transforms.ToTensor()])\nimage = Image.open(image_name)\nimage = loader(image).float()\n<\/code>\nThe expected result should be a 3d tensor with size 3*IMSIZE*IMSIZE, however, sometimes when dealing with png figures, it will give me 1*IMSIZE*IMSIZE or 4 *IMSIZE*IMSIZE.\nDoes someone have an idea what\u2019s going on here?\nFor example, this figure gives me 1*IMSIZE*IMSIZE tensor.\n","y":"Does using convert(\u2018RGB\u2019) help?\n<code class=\"lang-auto\">image = Image.open(image_name).convert('RGB')\n<\/code>","z":"Does using convert(\u2018RGB\u2019) help?\n<code class=\"lang-auto\">image = Image.open(image_name).convert('RGB')\n<\/code>"},{"x":"I\u2019m trying to implement a learnable bias layer which would add a constant value to every element of the input. Learnable bias layer exists in lasagne but I was unable to find such a layer in pytorch. I initialized bias as a parameter but the value is not changing. Is this the right way to do it? -\n class Net(nn.Module):\n     def __init__(self):\n       super(Net, self).__init__()\n       self.bias = nn.Parameter(torch.ones(1))\n       self.conv1 = nn.Conv2d(1, 1, kernel_size=1,  bias=False)\n\ndef forward(self,x):\n    x = self.conv1(x)\n    bias_matrix = Variable(torch.ones((x.size()))*self.bias.cpu().data[0]).cuda()\n    x =  x.add(bias_matrix)\n    print self.bias\n    return x","y":"Since you use the tensor inside your bias with self.bias.data, this will not work with the autograd system.\nIf you are using the master branch with automatic broadcasting, your can simply do:\n<code class=\"lang-auto\">def forward(self, x):\n    x = self.conv1(x)\n    out = x + self.bias\n    return out\n<\/code>\nIf not, you will need to do something like: out = x + self.bias.view(1,1,1,1).expand_as(x)","z":"Since you use the tensor inside your bias with self.bias.data, this will not work with the autograd system.\nIf you are using the master branch with automatic broadcasting, your can simply do:\n<code class=\"lang-auto\">def forward(self, x):\n    x = self.conv1(x)\n    out = x + self.bias\n    return out\n<\/code>\nIf not, you will need to do something like: out = x + self.bias.view(1,1,1,1).expand_as(x)\nDid that, but the value of bias remains the same. On a different note, this is the way in which I updated the parameters. Please tell me if there is an error.\nmodel = Net().cuda()\ncriterion = nn.MSELoss().cuda()\noptimizer = optim.SGD([{'params':[model.bias], 'lr':0.001}], lr=0.001)\ninput = Variable(torch.ones((1,1,2,2))).cuda()\ntarget = Variable(torch.zeros((1,1,2,2))).cuda()\nloss = criterion(model(input),target)\nprint loss\nloss.backward()\nYou need to call zero_grad() and step() on the optimizer to perform a step.\nSee the mnist example here: https:\/\/github.com\/pytorch\/examples\/blob\/master\/mnist\/main.py#L82-L86\nThanks for the quick reply. The bias value is changing now. Also, I defined the optimizer as optim.SGD([{\u2018params\u2019:[model.bias], \u2018lr\u2019:0.001}], lr=0.001) which will update bias value after each iteration. To update the weights of conv layer as well as the bias value, is optim.SGD(model.parameters(), lr=0.001) the right way to define the optimizer?\nYes, this is the right way to do it."},{"x":"I\u2019m trying to use LSTM on image data and quickly running out of device memory. My loop runs something like this:\n<code class=\"lang-auto\">x = torch.rand(20, 2, 3, 224, 738)  # T x B x CHW\n# init Variables h and c to proper size\ny = []\nfor xt in x:\n    xt = xt.cuda()\n    yt, (h,c) = lstm(xt, (h, c))\n    y.append(yt)\n<\/code>\nWhat I want to achieve is this: take a time slice from x (called xt above), transfer it to device, perform the LSTM operation on GPU, and then get the xt back to host, preserving the Variable connections.\nAny ideas?\nThanks ","y":"Posting my solution here for reference. The solution divides the data into \u2018groups\u2019 of M images. The user specifies M based on the size of their GPU and resolution of the images.\nThis solution loops through the N x T x C x H x W data by figuring out B x G x C x H x W batches based on the M value.\n<code class=\"lang-auto\"># data is N x T x C x H x W\n# target is N x T x d\nM = 64  # no. of images that can fit on the GPU \nN, T = data.size(0), data.size(1)\nG = min(T, M)  # no. of time slices that can fit on the GPU\nB = min(N, M\/G)  # batch size that can fit on the GPU\n\nif train:\n  data_var   = Variable(data, requires_grad=True)\n  target_var = Variable(target, requires_grad=False)\nelse:\n  data_var   = Variable(data, volatile=True)\n  target_var = Variable(target, volatile=True)\n\nloss_accum = 0 \nb_start = np.random.randint(N%B + 1)\nfor b in xrange(N\/B):\n  b_idx = b_start + torch.LongTensor(xrange(b*B, (b+1)*B))\n  xb = torch.index_select(data_var, dim=0, index=Variable(b_idx))\n  tb = torch.index_select(target_var, dim=0, index=Variable(b_idx).cuda())\n  model.reset_hidden_states(B)\n  g_start = np.random.randint(T%G + 1)\n  for g in xrange(T\/G):\n    g_idx = g_start + torch.LongTensor(xrange(g*G, (g+1)*G))\n    xg = torch.index_select(xb, dim=1, index=Variable(g_idx))\n    tg = torch.index_select(tb, dim=1, index=Variable(g_idx).cuda())\n    model.detach_hidden_states()\n    output = model(xg, cuda=cuda, async=True)\n\n    if criterion is not None:\n      loss = criterion(output, tg) \n      loss_accum += loss.data[0]\n\n      if train:\n        # SGD step\n        optim.learner.zero_grad()\n        loss.backward()\n        optim.learner.step()\n<\/code>\nwhere the model.reset_hidden_states() re-initializes them with random values from a normal distribution and \u2018repackages\u2019 them like in Help clarifying repackage_hidden in word_language_model with link \"https:\/\/discuss.pytorch.org\/t\/help-clarifying-repackage-hidden-in-word-language-model\/226\/2?u=samarth-robo\"","z":"Posting my solution here for reference. The solution divides the data into \u2018groups\u2019 of M images. The user specifies M based on the size of their GPU and resolution of the images.\nThis solution loops through the N x T x C x H x W data by figuring out B x G x C x H x W batches based on the M value.\n<code class=\"lang-auto\"># data is N x T x C x H x W\n# target is N x T x d\nM = 64  # no. of images that can fit on the GPU \nN, T = data.size(0), data.size(1)\nG = min(T, M)  # no. of time slices that can fit on the GPU\nB = min(N, M\/G)  # batch size that can fit on the GPU\n\nif train:\n  data_var   = Variable(data, requires_grad=True)\n  target_var = Variable(target, requires_grad=False)\nelse:\n  data_var   = Variable(data, volatile=True)\n  target_var = Variable(target, volatile=True)\n\nloss_accum = 0 \nb_start = np.random.randint(N%B + 1)\nfor b in xrange(N\/B):\n  b_idx = b_start + torch.LongTensor(xrange(b*B, (b+1)*B))\n  xb = torch.index_select(data_var, dim=0, index=Variable(b_idx))\n  tb = torch.index_select(target_var, dim=0, index=Variable(b_idx).cuda())\n  model.reset_hidden_states(B)\n  g_start = np.random.randint(T%G + 1)\n  for g in xrange(T\/G):\n    g_idx = g_start + torch.LongTensor(xrange(g*G, (g+1)*G))\n    xg = torch.index_select(xb, dim=1, index=Variable(g_idx))\n    tg = torch.index_select(tb, dim=1, index=Variable(g_idx).cuda())\n    model.detach_hidden_states()\n    output = model(xg, cuda=cuda, async=True)\n\n    if criterion is not None:\n      loss = criterion(output, tg) \n      loss_accum += loss.data[0]\n\n      if train:\n        # SGD step\n        optim.learner.zero_grad()\n        loss.backward()\n        optim.learner.step()\n<\/code>\nwhere the model.reset_hidden_states() re-initializes them with random values from a normal distribution and \u2018repackages\u2019 them like in Help clarifying repackage_hidden in word_language_model with link \"https:\/\/discuss.pytorch.org\/t\/help-clarifying-repackage-hidden-in-word-language-model\/226\/2?u=samarth-robo\""},{"x":"I apologize in advance if this is very trivial but I don\u2019t have a lot of experience in segmentation networks and pytorch.\nI am participating in ICLR Reproducibility Challenge 2018 and I am trying to reproduce the results in the submission \u201cAdversarial Learning For Semi-Supervised Semantic\u201d.\nI am confused about how to upsample the feature map produced by a network to match the size of the input image. This is how the paper explains the segmentation network.\n\u201cSegmentation network. We adopt the DeepLab-v2 (Chen et al., 2017) framework with ResNet- 101 (He et al., 2016) model pre-trained on the ImageNet dataset (Deng et al., 2009) as our segmenta- tion baseline network. However, we do not employ the multi-scale fusion proposed in Chen et al. (2017) due to the memory concern. Following the practice of recent work on semantic segmenta- tion (Chen et al., 2017; Yu &amp; Koltun, 2016), we remove the last classification layer and modify the stride of the last two convolution layers from 2 to 1, making the resolution of the output feature maps effectively 1\/8 times the input image size. To enlarge the receptive fields, we apply the dilated convolution (Yu &amp; Koltun, 2016) in conv4 and conv5 layers with a stride of 2 and 4, respectively. After the last layer, we employ the Atrous Spatial Pyramid Pooling (ASPP) proposed in Chen et al. (2017) as the final classifier. Finally, we apply an up-sampling layer along with the softmax output to match the size of the input image.\u201d\nI tried looking at the DeepLab-v2 version of Resnet101 and I couldn\u2019t understand which layer is con4 and conv5. So, instead,  I used the network defined here https:\/\/github.com\/speedinghzl\/Pytorch-Deeplab. While training on PascalVOC dataset, the input image is first randomly scaled and cropped to 321x321. For this size, the network produces an output map of size 41x41. I tried using torch.nn.Upsample(size=321x321) to map this 41x41 feature map to size 321x321 but it gives me an error that output size should be a multiple of input size ( which makes sense)\nMy question is, how can I upsample my segmentation network output feature map to the size of my input image?","y":"Hey, you want to go from 41x41 to 321x321? If you upsample 8 times with conv transpose 2d you get 328. Then a conv filter of size 8x8, you should get 321x321","z":"Hey, you want to go from 41x41 to 321x321? If you upsample 8 times with conv transpose 2d you get 328. Then a conv filter of size 8x8, you should get 321x321\nHey! Thanks for the suggestion. This would work for my training scheme where I know that my input is 321x321.\nBut in the inference phase, the input image size is not fixed and I want a cleaner way where I can upsample by simply specifying the target size.\nFor example, this is the behavior I want\n\nfeature_map = net(input) \/\/ The size of the feature map depends on the size of input\noutput = torch.nn.Upsample(size = input_size) \/\/ input_size is the size of input\n\nPlease write back if I didn\u2019t make something clear."},{"x":"When I load the CIFAR 100 dataset from torchvision.datasets, class label (target) from getitem function is an integer value. However, I also want to know the corresponding class name, e.g., beaver and dolphin. How to get the class label to class name mapping?\nThanks in advance.","y":"You can find meta in the original CIFAR 10\/100 python.tar.gz. Then unpickle it, you\u2019ll get a list of class names.","z":"You can find meta in the original CIFAR 10\/100 python.tar.gz. Then unpickle it, you\u2019ll get a list of class names."},{"x":"I search a lot from Google, and I am still confused how to get the features from Bottleneck.\nFor example, I use DenseNet Pre-trained Model with link \"https:\/\/github.com\/pytorch\/vision\/blob\/master\/torchvision\/models\/densenet.py\"\nAfter model.cuda(), I get as follow\n<code class=\"lang-auto\">DenseNet (\n  (features): Sequential (...)\n  (classifier): Linear (2208 -> 80)\n)\n<\/code>\nSo, how to get the features block\u2019s output?","y":"A way to do this is to create a new model that only uses DenseNet\u2019s features block:\nmodel = densenet121(pretrained=True)\n\nclass FeatureExtractor(nn.Module):\n    def __init__(self):\n        super(FeatureExtractor, self).__init__()\n        self.features = nn.Sequential(*list(model.features.children()))\n    def forward(self, x):\n        x = self.features(x)\n        return x\n\nmodel_features = FeatureExtractor()\n\nNow model_features  has the same architecture and parameters as the original pre-trained DenseNet, except that the fully-connected classifier on top has been removed.","z":"A way to do this is to create a new model that only uses DenseNet\u2019s features block:\nmodel = densenet121(pretrained=True)\n\nclass FeatureExtractor(nn.Module):\n    def __init__(self):\n        super(FeatureExtractor, self).__init__()\n        self.features = nn.Sequential(*list(model.features.children()))\n    def forward(self, x):\n        x = self.features(x)\n        return x\n\nmodel_features = FeatureExtractor()\n\nNow model_features  has the same architecture and parameters as the original pre-trained DenseNet, except that the fully-connected classifier on top has been removed.\n\n\n\n BartolomeD:\n\nmodel = densenet121(pretrained=True)\nclass FeatureExtractor(nn.Module):\ndef init(self):\nsuper(FeatureExtractor, self).init()\nself.features = nn.Sequential(*list(model.features.children()))\ndef forward(self, x):\nx = self.features(x)\nreturn x\nmodel_features = FeatureExtractor()\n\n\nThanks a lot! It works!"},{"x":"In order to apply Random Scaling and Cropping as a data preprocessing step in Semantic Segmentation, what interpolation should we use for labels?\nI am currently using Image.NEAREST from PIL but my labels get messed up after interpolation. (only displaying the labels for plane). This is an Image from PASCALVOC dataset.\n","y":"My Bad! Even the original ground truth label is distorted.","z":"My Bad! Even the original ground truth label is distorted."},{"x":"I wan to copy the discriminator model of DCGAN to Classification model.\n\n\n\nGitHub with link \"https:\/\/github.com\/martinarjovsky\/WassersteinGAN\"\n\n\n\nmartinarjovsky\/WassersteinGAN with link \"https:\/\/github.com\/martinarjovsky\/WassersteinGAN\"\nContribute to WassersteinGAN development by creating an account on GitHub.\n\n\n\n\n\n","y":"get the state_dict with model.state_dict(). It is a dictionary of parameters with names.\nNow filter the weights that you want to copy from the dictionary and you can call model.load_state_dict(my_modified_dict) on your newly constructed classification model.","z":"get the state_dict with model.state_dict(). It is a dictionary of parameters with names.\nNow filter the weights that you want to copy from the dictionary and you can call model.load_state_dict(my_modified_dict) on your newly constructed classification model."},{"x":"My Code is listed below:\ndef train(epoch):\n    model.train()\n    for batch_idx, (data, target) in enumerate(trainloader):\n        #print str(target.numpy())\n        h = data.shape[2]\n        w = data.shape[3]\n        c = 3\n        output = data.squeeze(0)\n        output = output.numpy()\n        output = np.resize(output,(h,w,c))\n        plt.imshow(output)\n        plt.show()\n        exit(0)\n\nActually, I want to get a output whose locations of each pixels are as same as my original input image, and however my final output is very different from my input image. So, how can i solve this problem?\nThank you for your help.","y":"Maybe you are looking for np.transpose(output, [1, 2 ,0])\u2026 But It seems that you are not fwding through the model, so I\u2019m not sure what you want to do with output.","z":"Maybe you are looking for np.transpose(output, [1, 2 ,0])\u2026 But It seems that you are not fwding through the model, so I\u2019m not sure what you want to do with output."},{"x":"I am using some of the functions provided by torch.nn.functional. The input to the function is a Tensor and I expect that the output is also a Tensor. But it seems that some of the functions will output torch.autograd.Variable, which seems strange.\nFor example, if the input x is a tensor, nn.functional.relu(x) with link \"http:\/\/pytorch.org\/docs\/master\/nn.html#torch.nn.functional.relu\" will return a Variable, while function like nn.functional.normalize(x) with link \"http:\/\/pytorch.org\/docs\/master\/nn.html#normalize\" will return a Tensor.\nWhy is there a distinction? What is the rationale behind this?","y":"Hi,\nnn.functional actually expects Variables as inputs.\nThe behavior when you input Tensors is not defined I think (the result will be correct, but the type of what you get as output will depend on the way it was implemented).","z":"Hi,\nnn.functional actually expects Variables as inputs.\nThe behavior when you input Tensors is not defined I think (the result will be correct, but the type of what you get as output will depend on the way it was implemented).\nThanks for clarification."},{"x":"Hi everyone,\nI am trying to load a 3D dataset using both the Dataset class and the DataLoader. My image data is an ndarray in int16 and loading it works using just the Dataset but breaks when using the DataLoader with the following error:\n\nRuntimeError: can\u2019t convert a given np.ndarray to a tensor - it has an invalid type. The only supported types are: double, float, int64, int32, and uint8.\n\nNow I am wondering why the Dataset accepts loading from a numpy int16 ndarray but the DataLoader does not.\nMy ToTensor class looks like this:\nclass ToTensor(object):\n    \"\"\"Convert ndarrays in sample to Tensors.\"\"\"\n\n    def __call__(self, sample):\n        image, label = sample['image'], sample['label']\n\n        # Expand with channel axis\n        # numpy image: H x W x Z\n        # torch image: C x H x W x Z\n        #print(image.dtype)\n        image = torch.from_numpy(image).unsqueeze(0)\n        label = torch.ByteTensor(label)\n\n        print(image.size(), label.size())\n        return {'image': image,\n                'label': label}\n\n\nppmi_dataset = PPMIDataset(excel_file=\u2018PPMI_main_table.xlsx\u2019,\nroot_dir=FOLDER, transform=ToTensor())\nprint(len(ppmi_dataset))\na = ppmi_dataset[0][\u2018image\u2019].numpy()\nprint(a.dtype)\nplt.imshow(a.squeeze()[:,:,45], cmap=\u2018gray\u2019)\nplt.show()\n\nThis works correctly and gives as an output (together with the image):\n\n606\ntorch.Size([1, 105, 127, 105]) torch.Size([1])\nint16\n\nUsing it with the DataLoader\ndataloader = DataLoader(ppmi_dataset, batch_size=2, shuffle=True)\n\nfor i, batch in enumerate(dataloader):\n    if i == 2:\n        plt.figure()\n        for idx in range(1):\n            plt.subplot(1, 2, 1 + idx)\n            plt.imshow(batch[idx]['image'].squeeze().numpy()[:,:,45])\n        plt.show()\n        break\n\nThis results in the error from above. Converting the ndarray to int32 in the ToTensor class works in both cases. Is this some intended feature that I don\u2019t see?","y":"What about ShortTensor?\nhttp:\/\/pytorch.org\/docs\/master\/tensors.html#torch.ShortTensor\nBtw: I found that the problem occurred because some of my labels were loaded as uint16 and some as int16. The former type is not supported and breaks the from_numpy() function.","z":"I was wrong. See  's post below.\nWhat about ShortTensor?\nhttp:\/\/pytorch.org\/docs\/master\/tensors.html#torch.ShortTensor\nBtw: I found that the problem occurred because some of my labels were loaded as uint16 and some as int16. The former type is not supported and breaks the from_numpy() function."},{"x":"What is the difference between torchvision.transforms.Resize() and torchvision.transforms.Scale()","y":"Both are the same. The recent release of torchvision deprecates Scale in favour of Resize.","z":"Use doc: http:\/\/pytorch.org\/docs\/master\/torchvision\/transforms.html#torchvision.transforms.Scale\nBoth are the same. The recent release of torchvision deprecates Scale in favour of Resize."},{"x":"The videos are labelled frame by frame. The frames are used to train a model.\nI tried to write a subclass of Dataset, which loaded all the labelling information for all the videos at initialization and __len__() returns the total frame number of all videos, while the __getitem__(idx) method compute which video contains the idx-th frame and load this video and extract the corresponding frame.\nThe fact that for every __getitem__() query a video file is loaded and searched for seems a great waste of time and resource. Not surprisingly, even if the batchsize is set to 4(image num, not video num) and num_workers for dataloader is set to 1, this code just don\u2019t work because of malloc failure.\nI used to extract all the frames in advance and use ImageFolder dataset. But it takes a lot of extra disk space and hours of time to extract the frames.\nSo is there any other solutions? thanks~","y":"I am also working on a project involving video files and I have observed the same issue as you. The way I resolved it was to disregard the idx value passed into getitem and instead do my own random indexing within the function. I randomly reload a different video 10% of the time, otherwise just keep sampling cases from it. This will greatly reduce the number of videos that need to be loaded as new video loads only happen in 10% of the sampling cases. (or less, if you like)\nPseudo code:\ndef __getitem__(self, index):\n\n    if random.random() > .9:\n        # sometimes reload a new video\n        self.loadNewVideo()\n\n    # select a scene, and then select a frame\n    sceneIdx = random.randrange(0, len(self.videoShots))\n    scene = self.videoShots[sceneIdx]\n    frameIndex = random.randrange(scene[0], scene[1] - 1) \n    frames, targets = self.buildTrainingCaseFromFrameIndex(frameIndex)\n    return frames, targets","z":"I am also working on a project involving video files and I have observed the same issue as you. The way I resolved it was to disregard the idx value passed into getitem and instead do my own random indexing within the function. I randomly reload a different video 10% of the time, otherwise just keep sampling cases from it. This will greatly reduce the number of videos that need to be loaded as new video loads only happen in 10% of the sampling cases. (or less, if you like)\nPseudo code:\ndef __getitem__(self, index):\n\n    if random.random() > .9:\n        # sometimes reload a new video\n        self.loadNewVideo()\n\n    # select a scene, and then select a frame\n    sceneIdx = random.randrange(0, len(self.videoShots))\n    scene = self.videoShots[sceneIdx]\n    frameIndex = random.randrange(scene[0], scene[1] - 1) \n    frames, targets = self.buildTrainingCaseFromFrameIndex(frameIndex)\n    return frames, targets\nYeah, this is a great method, thanks~"},{"x":"I installed pytorch and torchvision with anaconda.\nBut there is no Resize class\/module in torchvision.transforms.\nThe code in official github surely has these codes, but I failed to build pytorch from source.\nSo how can I use Resize?\nThansks","y":"You can use the old Scale transform. But if the size provided is just int, it will resize the minimum size of image to that number. If you want to resize the image\u2019s maximum length to size and keep aspect ratio. Just write your own resize, it is just a few line of code, see example code below for a reference,\n<code class=\"lang-auto\">\nclass Resize(object):\n    def __init__(self, size, interpolation=Image.BILINEAR):\n        self.size = size\n        self.interpolation = interpolation\n\n    def __call__(self, img):\n        old_size = img.size  # old_size[0] is in (width, height) format\n\n        ratio = float(self.size)\/max(old_size)\n        new_size = tuple([int(x * ratio) for x in old_size])\n\n        return img.resize(new_size, resample=self.interpolation)\n<\/code>","z":"download   related files with link \"https:\/\/github.com\/pytorch\/vision\/tree\/master\/torchvision\/transforms\"\nand replace your local files  (find them by locate torchvision\/transforms)\nYou can use the old Scale transform. But if the size provided is just int, it will resize the minimum size of image to that number. If you want to resize the image\u2019s maximum length to size and keep aspect ratio. Just write your own resize, it is just a few line of code, see example code below for a reference,\n<code class=\"lang-auto\">\nclass Resize(object):\n    def __init__(self, size, interpolation=Image.BILINEAR):\n        self.size = size\n        self.interpolation = interpolation\n\n    def __call__(self, img):\n        old_size = img.size  # old_size[0] is in (width, height) format\n\n        ratio = float(self.size)\/max(old_size)\n        new_size = tuple([int(x * ratio) for x in old_size])\n\n        return img.resize(new_size, resample=self.interpolation)\n<\/code>\nAs  said, download related files and don\u2019t forget to reboot your system to get things updated.\nSorry for such a long delay.\nlocate torchvision\/transforms prints nothing, and I guess just downloading the scripts without lib files may not work.\nI just found that, for python2 and torch0.2, there is no Resize and as  said, Scale is prefered, for python3 and torch0.3, Resize is prefered.\nJust wondering why there\u2019s no torch0.3 for python2 if using pip install.\nHave you read about this with link \"https:\/\/discuss.pytorch.org\/t\/updating-pytorch\/309\/12\"?\nIt works! updated to torch0.3.0post4, thanks\nCheers, always search through the forum before posting your question.\nDefinitely. Didn\u2019t realize that this is due to low version and asked a stupid qestion:blush:"},{"x":"This is in continuation of my previous post\n\n\n\n\nImagenet 10-crop testing example with link \"https:\/\/discuss.pytorch.org\/t\/imagenet-10-crop-testing-example\/11305\" vision with link \"\/c\/vision\"\n\n\n    I am trying to use 10-crop testing feature of Pytorch. However, i want to store the dataloader to a pickle file for efficiency.  Ten crop testing requires a lambda function and i get a traceback as follows \n\nAttributeError: Can't pickle local object 'main.<locals>.<lambda>' \n\nSo is there any other way apart from the one mentioned in original docs http:\/\/pytorch.org\/docs\/master\/torchvision\/transforms.html#torchvision.transforms.TenCrop \nFYI original tencrop transform is \n\ntransform = Compose([ \nT\u2026\n  \n\nI want to ask whats the proper way to do TenCrop testing for imagenet? I modified the val_loader in the imagenet example here with link \"https:\/\/github.com\/pytorch\/examples\/blob\/master\/imagenet\/main.py#L137\" as following\nval_loader = torch.utils.data.DataLoader(\n    datasets.ImageFolder(valdir, transforms.Compose([\n        transforms.TenCrop(224),\n        transforms.Lambda(lambda crops: torch.stack([transforms.ToTensor()(crop) for crop in crops])),\n        transforms.Lambda(lambda crops: torch.stack([normalize(crop) for crop in crops])),\n    ])),\n    batch_size=args.batch_size, shuffle=False,\n    num_workers=args.workers, pin_memory=True)\n\nI used rescaled images (256 x 256). So i modify the validate function in the above mentioned script as\n    target = target.cuda(async=True)\n    input_var = torch.autograd.Variable(input, volatile=True)\n    target_var = torch.autograd.Variable(target, volatile=True)\n\n    # compute output\n    bs, ncrops, c, h, w = input_var.size()\n    # compute output\n    temp_output = model(input_var.view(-1, c, h, w))\n    output = temp_output.view(bs, ncrops, -1).mean(1)\n    loss = criterion(output, target_var)\n\nI get the following exception\n\nValueError: Requested crop size (224, 224) is bigger than input size (220, 349)\n\nWhere am i going wrong? I am using tencrop transform as mentioned\nhere in docs. with link \"http:\/\/pytorch.org\/docs\/master\/torchvision\/transforms.html#torchvision.transforms.TenCrop\"","y":"Before transforms.TenCrop(224), you have to add transforms.Scale(256), some of your images are too small to be cropped to 224","z":"Before transforms.TenCrop(224), you have to add transforms.Scale(256), some of your images are too small to be cropped to 224\nYes, works like a charm now.  thanks \nI got confused with the following command\n\nfind . -name \u201c*.JPEG\u201d | xargs -I {} convert {} -resize \u201c256^>\u201d {}\n\ni thought it results in squared images."},{"x":"below is the screenshot\n\u56fe\u7247.png1492\u00d7780 78.3 KB with link \"https:\/\/discuss.pytorch.org\/uploads\/default\/original\/2X\/9\/9d71c224755472b9d96b3d97e24837daa39794c1.png\"","y":"I\u2019m assuming your intent is to programmatically assign a name for the submodule. Try one of the following:\n<code class=\"lang-auto\">class test(nn.Module):\n    def __init__(self):\n        super(test, self).__init__()\n        self.__setattr__('a' + '_1', nn.Conv2d(3, 3, kernel_size=3))\n        self.add_module('a' + '_2', nn.Conv2d(3, 3, kernel_size=3))\n        \n    def forward(self, x):\n        x = self.a_1(x)\n        x = self.a_2(x)\n        return x\n\nnet = test()\nnet\n<\/code>","z":"I\u2019m assuming your intent is to programmatically assign a name for the submodule. Try one of the following:\n<code class=\"lang-auto\">class test(nn.Module):\n    def __init__(self):\n        super(test, self).__init__()\n        self.__setattr__('a' + '_1', nn.Conv2d(3, 3, kernel_size=3))\n        self.add_module('a' + '_2', nn.Conv2d(3, 3, kernel_size=3))\n        \n    def forward(self, x):\n        x = self.a_1(x)\n        x = self.a_2(x)\n        return x\n\nnet = test()\nnet\n<\/code>"},{"x":"Hi,\nDoes anyone know how the provided VGG16_bn with link \"https:\/\/github.com\/pytorch\/vision\/blob\/2b2aa9c780f80865cf9f04dae114de7f92379416\/torchvision\/models\/vgg.py#L19\" model was trained? I want to train the VGG16 with batch normalization from scratch for some use.\nThanks!","y":"I guess the code is\n\n\n\nGitHub with link \"https:\/\/github.com\/pytorch\/examples\/tree\/master\/imagenet\"\n\n\n\npytorch\/examples with link \"https:\/\/github.com\/pytorch\/examples\/tree\/master\/imagenet\"\nA set of examples around pytorch in Vision, Text, Reinforcement Learning, etc.\n\n\n\n\n\n","z":"I guess the code is\n\n\n\nGitHub with link \"https:\/\/github.com\/pytorch\/examples\/tree\/master\/imagenet\"\n\n\n\npytorch\/examples with link \"https:\/\/github.com\/pytorch\/examples\/tree\/master\/imagenet\"\nA set of examples around pytorch in Vision, Text, Reinforcement Learning, etc.\n\n\n\n\n\nThanks for you reply  . It is my mistake. Actually I want to know the hyper-parameters such as the batch size, learning-rate, epochs, etc. The default parameters do not work.\nSorry but I can\u2019t find the origin code to reproduce vgg16-bn result.\nI could only find a related issue here( is responsible for vgg in torchvision )\n\n\ngithub.com\/pytorch\/vision with link \"https:\/\/github.com\/pytorch\/vision\/issues\/223#issuecomment-323171857\"\n\n\n with link \"https:\/\/github.com\/zrxzrx\"\nIssue: Is the VGG here about 2% lower in accuracy compared to the original caffe version? with link \"https:\/\/github.com\/pytorch\/vision\/issues\/223#issuecomment-323171857\"\n\n\n\topened by zrxzrx with link \"https:\/\/github.com\/zrxzrx\"\n\ton 2017-08-10 with link \"https:\/\/github.com\/pytorch\/vision\/issues\/223#issuecomment-323171857\"\n\n\n\tclosed by soumith with link \"https:\/\/github.com\/soumith\"\n\ton 2017-08-18 with link \"https:\/\/github.com\/pytorch\/vision\/issues\/223#issuecomment-323171857\"\n\n\nThe caffe version says the accuracy is about ~7% but in the documentation of pytorch, it says it is ~9%. Just...\n\n\n\n\n\n\n\n"},{"x":"Hi,\nContext\nI have a simple algorithm that distributes a number of tasks across a list of Process, then the results of the workers is sent back using a Queue. I was previously using numpy to do this kind of job.\nProblem\nTo be more consistent with my code, I decided to use only torch tensors, unfortunately I think transfering torch.Tensor over Queue is not possible, maybe because of Pickle or something. I get this kind of error when calling the get() method to retrieve the result from my Queue.\n<code class=\"lang-auto\">    worker_result = done_queue.get()\n  File \"\/home\/ganaye\/deps\/miniconda3\/lib\/python3.5\/multiprocessing\/queues.py\", line 113, in get\n    return ForkingPickler.loads(res)\n  File \"\/home\/ganaye\/deps\/miniconda3\/lib\/python3.5\/site-packages\/torch\/multiprocessing\/reductions.py\", line 70, in rebuild_storage_fd\n    fd = df.detach()\n  File \"\/home\/ganaye\/deps\/miniconda3\/lib\/python3.5\/multiprocessing\/resource_sharer.py\", line 57, in detach\n    with _resource_sharer.get_connection(self._id) as conn:\n  File \"\/home\/ganaye\/deps\/miniconda3\/lib\/python3.5\/multiprocessing\/resource_sharer.py\", line 87, in get_connection\n    c = Client(address, authkey=process.current_process().authkey)\n  File \"\/home\/ganaye\/deps\/miniconda3\/lib\/python3.5\/multiprocessing\/connection.py\", line 493, in Client\n    answer_challenge(c, authkey)\n  File \"\/home\/ganaye\/deps\/miniconda3\/lib\/python3.5\/multiprocessing\/connection.py\", line 732, in answer_challenge\n    message = connection.recv_bytes(256)         # reject large message\n  File \"\/home\/ganaye\/deps\/miniconda3\/lib\/python3.5\/multiprocessing\/connection.py\", line 216, in recv_bytes\n    buf = self._recv_bytes(maxlength)\n  File \"\/home\/ganaye\/deps\/miniconda3\/lib\/python3.5\/multiprocessing\/connection.py\", line 407, in _recv_bytes\n    buf = self._recv(4)\n  File \"\/home\/ganaye\/deps\/miniconda3\/lib\/python3.5\/multiprocessing\/connection.py\", line 379, in _recv\n    chunk = read(handle, remaining)\nConnectionResetError: [Errno 104] Connection reset by peer\n<\/code>\nConclusion\nAfter rapidly switching back to numpy and sending random numpy arrays, I am convinced the problem is from using torch tensor. One can reproduce the error with this tested code:\nCode\n<code class=\"lang-auto\">import multiprocessing as mp\nimport torch\n\ndef extractor_worker(done_queue):\n    done_queue.put(torch.Tensor(10,10))\n\nproducers = []\ndone_queue = mp.Queue()\nfor i in range(0, 1):\n    process = mp.Process(target=extractor_worker,\n                         args=(done_queue,))\n    process.start()\n    producers.append(process)\n\nresult_arrays = []\nnb_ended_workers = 0\nwhile nb_ended_workers != 1:\n    worker_result = done_queue.get()\n    if worker_result is None:\n        nb_ended_workers += 1\n    else:\n        result_arrays.append(worker_result)\n<\/code>\nSurprisingly it seems that writing is ok, but reading the object throws an error.\nIf any of you has a workaround to use torch tensor over Queue ! I can switch back to numpy if this is really impossible.\nThanks !","y":"Your background process needs to be alive when the main process reads the tensor.\nHere\u2019s a small modification to your example:\n<code class=\"lang-auto\">import multiprocessing as mp\nimport torch\n\ndone = mp.Event()\n\ndef extractor_worker(done_queue):\n    done_queue.put(torch.Tensor(10,10))\n    done_queue.put(None)\n    done.wait()\n\nproducers = []\ndone_queue = mp.Queue()\nfor i in range(0, 1):\n    process = mp.Process(target=extractor_worker,\n                         args=(done_queue,))\n    process.start()\n    producers.append(process)\n\nresult_arrays = []\nnb_ended_workers = 0\nwhile nb_ended_workers != 1:\n    worker_result = done_queue.get()\n    if worker_result is None:\n        nb_ended_workers += 1\n    else:\n        result_arrays.append(worker_result)\ndone.set()\n<\/code>","z":"Your background process needs to be alive when the main process reads the tensor.\nHere\u2019s a small modification to your example:\n<code class=\"lang-auto\">import multiprocessing as mp\nimport torch\n\ndone = mp.Event()\n\ndef extractor_worker(done_queue):\n    done_queue.put(torch.Tensor(10,10))\n    done_queue.put(None)\n    done.wait()\n\nproducers = []\ndone_queue = mp.Queue()\nfor i in range(0, 1):\n    process = mp.Process(target=extractor_worker,\n                         args=(done_queue,))\n    process.start()\n    producers.append(process)\n\nresult_arrays = []\nnb_ended_workers = 0\nwhile nb_ended_workers != 1:\n    worker_result = done_queue.get()\n    if worker_result is None:\n        nb_ended_workers += 1\n    else:\n        result_arrays.append(worker_result)\ndone.set()\n<\/code>\nThis is an unfortunate result of how Python pickling handles sending file descriptors. (We send tensors via shared memory instead of writing the values to the queue). The steps are roughly:\n\nBackground process sends token mp.Queue\nWhen the main process reads the token, it opens a unix socket to the background process\nThe background process sends the file descriptor via the unix socket\n\nThank you , this is an interesting implementation detail.\nFrom what you just said, I guess the previous solution (numpy) worked because the tensor is sent by value rather than using a file descriptor ? Otherwise I don\u2019t get it, I have used the numpy solution for months now.\ncan\u2019t wait to try this.\nDoes sending tensors run faster than sending (and having to convert to\/from) numpy arrays?\n my first thought was that it would be nearly the same cost, because they share the same storage. So I kept numpy as the main solution for storing an image array.\nHowever we have to keep in mind that the final object will be a torch tensor, so I compared what would be the difference of using numpy or torch, it turns out the first solution took on average 3.3 seconds to build a 512 batch and the second solution (torch) took less than 0.005 seconds. It was measured after few iterations to make sure my 6 workers were really efficient.\nThis speedup is the reason I decided to switch to torch tensor only. As I said, in the end you will only need a torch tensor, no value to use numpy just as a storage. I think the conversions numpy -> torch are the cause for this slowdown.\nOn the other hand I was pretty satisfied with the memory footprint of numpy, I hope it will stay constant with torch.\n I think a simpler solution is to replace mp.Queue() with mp.Manager().Queue()\nI haven\u2019t yet timed it against done = mp.Event() +  done.wait() + done.set() to see if the speed changes.\nFor my application, the number of transfers is quite limited, I will stay with the solution proposed. However with more consuming tasks like distributed computing, it might be useful to explore new solutions.\nI had a related question that I posted here - CUDA tensors on multiprocessing queue with link \"https:\/\/discuss.pytorch.org\/t\/cuda-tensors-on-multiprocessing-queue\/28626\".\nIn our application, we have a bunch of workers that are putting CUDA tensors onto a shared queue that is read by the main process. It seems that the workers need to keep the CUDA tensors in memory until the main process has read these tensors. There are two issues here - one, is that the main process actually runs fine without throwing any errors but the tensors read are often garbage values. It will be great if this would trigger an exception. Secondly, it does look like this constraint forces us to have some kind of communication also going backwards - from the main process to the worker processes. e.g. we could store the tensors in a temporary queue in the workers which would be cleared periodically based on information from the main process. Is this the best way to handle such a use case?\nFWIW Another solution is to use mp.JoinableQueue, using queue.task_done() on the consumer and queue.join() on the producer.\nHi , I\u2019m trying to use mp.JoinableQueue for one producer process, which keeps putting items to the queue. And another consumer process keeps getting item from the queue. For the two processes, does queue.join() and queue.task_done() only need to be set once? or multiple times for each get() and put() call? It would be great if you could give a simple example. Thank you.\nHi , it will be the latter:\nhttps:\/\/docs.python.org\/3.6\/library\/multiprocessing.html#multiprocessing.JoinableQueue.task_done\n\nFor each get() with link \"https:\/\/docs.python.org\/3.6\/library\/multiprocessing.html#multiprocessing.Queue.get\" used to fetch a task, a subsequent call to task_done() with link \"https:\/\/docs.python.org\/3.6\/library\/multiprocessing.html#multiprocessing.JoinableQueue.task_done\" tells the queue that the processing on the task is complete.\n\nThis is the toy example I had used to check it for myself:\n\n\ngist.github.com with link \"https:\/\/gist.github.com\/EricCousineau-TRI\/4cedd0847ffc5acdd992403beffbc7b1\"\n\n\nhttps:\/\/gist.github.com\/EricCousineau-TRI\/4cedd0847ffc5acdd992403beffbc7b1\ndoes_indeed_work.py\n<code class=\"Python\"># Using torch==1.4.0\n\nimport numpy as np\nimport torch\nimport torch.multiprocessing as mp\n\ntorch.set_grad_enabled(False)\n\ndef target(inputs, outputs):\n    x = inputs.get()<\/code>\nThis file has been truncated. show original with link \"https:\/\/gist.github.com\/EricCousineau-TRI\/4cedd0847ffc5acdd992403beffbc7b1\"\ndoes_not_work.py\n<code class=\"Python\"># Using torch==1.4.0\n\nimport numpy as np\nimport torch\nimport torch.multiprocessing as mp\n\ntorch.set_grad_enabled(False)\n\ndef target(inputs, outputs):\n    x = inputs.get()<\/code>\nThis file has been truncated. show original with link \"https:\/\/gist.github.com\/EricCousineau-TRI\/4cedd0847ffc5acdd992403beffbc7b1\"\n\n\n\n\n\n\n\n\nFeel free to modify it to test for multi-input multi-output queue processing."},{"x":"I am very new to coding + pytorch\nI have an input tensor torch.full((row,col), 2.0), called A\nand I want to have an output looks like [[2,2,2], [4,4,4], [6,6,6], \u2026], called B\nThe idea is to multiple A and B, pytorch will do broadcasting for me if I understand correctly.\nSo I tried:\nr, c = A.size()\nx = torch.arange( r ) + 1.0\nthe x has shape \u2018torch.Size([4])\u2019, it seems like not broadcastable, which means A*x reports error.\nI tried a little experiment, y = torch.randn(4,1). Then A*y is ok. So I think this is because x has shape [4] and y has shape [4,1]. I tried to change x\u2019s shape to [4,1] using x.view(4,1). But it didn\u2019t work. The shape of x is still [4]. Could anyone help me on this? Thank you a lot for your time!","y":"x.view(4, 1) doesn\u2019t change x but returns a tensor (view) of shape 4 x 1 if you want you can assign it to a new variable and use that or plug it into your operation directly.","z":"x.view(4, 1) doesn\u2019t change x but returns a tensor (view) of shape 4 x 1 if you want you can assign it to a new variable and use that or plug it into your operation directly.\nThe x vector should have c rows. Try this:\n<code class=\"lang-auto\">> >>> import torch\n> >>> row=4; col=7\n> >>> A=torch.full((row,col),2.0)\n> >>> r,c=A.size()\n> >>> r,c\n> (4, 7)\n> >>> x=torch.arange(c)+1.0\n> >>> x\n> tensor([1., 2., 3., 4., 5., 6., 7.])\n> >>> A*x\n> tensor([[ 2.,  4.,  6.,  8., 10., 12., 14.],\n>         [ 2.,  4.,  6.,  8., 10., 12., 14.],\n>         [ 2.,  4.,  6.,  8., 10., 12., 14.],\n>         [ 2.,  4.,  6.,  8., 10., 12., 14.]])\n<\/code>\nOr, in a more simple way: In A*x, the number of rows for the x vector, should be equal to the number columns of matrix A.\nThank you very much! It works exactly the way I want\nThank you very much!"},{"x":"Could someone kindly explain the logic behind the tensor reshape below? According to what kind of rule is the b.view(3*2, 2) tensor being re-organized?\n<code class=\"lang-auto\">import torch\u00ac                                                                                                    \n       a = torch.eye(2)\u00ac                                                                                                \n       b = a.repeat(1, 3)\u00ac\n<\/code>\n\ntensor([[1., 0., 1., 0., 1., 0.],\u00ac\n[0., 1., 0., 1., 0., 1.]])\u00ac\n\nb.view(3*2, 2)\u00ac\n\ntensor([[1., 0.],\u00ac\n[1., 0.],\u00ac\n[1., 0.],\u00ac\n[0., 1.],\u00ac\n[0., 1.],\u00ac\n[0., 1.]])\u00ac\n","y":"Hi,\nThe view operation will just change the view you look at the memory.\nA contiguous Tensor is stored in a row major way, meaning that the rows contain elements that are next to each other in memory.\nWhen you do view, you just read these with different size, still with rows containing elements that are next to each other in memory.\nIn general, view are often used to collapse and uncollapse dimensions.\nIn particular, if you want to work with a 1D Tensor in your case, you can do b.view(3*2*2). Then work with this 1D Tensor and finally do res.view(2, 6) to get back to the original shape.","z":"Hi,\nThe view operation will just change the view you look at the memory.\nA contiguous Tensor is stored in a row major way, meaning that the rows contain elements that are next to each other in memory.\nWhen you do view, you just read these with different size, still with rows containing elements that are next to each other in memory.\nIn general, view are often used to collapse and uncollapse dimensions.\nIn particular, if you want to work with a 1D Tensor in your case, you can do b.view(3*2*2). Then work with this 1D Tensor and finally do res.view(2, 6) to get back to the original shape."},{"x":"Hi,\nIf you try set the grad to false for a layer inside with grad enabled, does this override the with clause?\nFor example:\n<code class=\"lang-auto\">with torch.set_grad_enabled(mode = True):\n    self.model.lin2.requires_grad = False # Will this work?\n   for m in self.model.mlp_f: m.set_grad_enabled = False # Or this?\n<\/code>","y":"Keep in mind that set_grad_enabled is a state of the program (\u201cdo you want to keep track of gradients for outputs when the inputs require gradients\u201d) and applies to new tensors, while someparam.requires_grad_(False) (which is the suggested form to disable gradients of tensors and parameters) says \u201cthis thing, when used as input, doesn\u2019t require gradients\u201d.\nTo then decide whether a given operation\u2019s output requires gradients, the autograd engine checks if both gradient-mode is enabled and any inputs require gradients.\nAs such these are two distinct knobs you can operate independently, but they have a combined effect.\n\n\n\n eprox:\n\n<code class=\"lang-auto\">with torch.set_grad_enabled(mode = True):\n    self.model.lin2.requires_grad = False # Will this work?\n   for m in self.model.mlp_f: m.set_grad_enabled = False # Or this?\n<\/code>\n\n\nThis seems to try to work on Modules, which isn\u2019t a thing (and using requires_grad_ would tell you so, which is why it is preferred). If you wanted something like that, you should go for for p in self.model.mlp_f.parameters(): p.requires_grad_(False) which certainly works.\nBest regards\nThomas","z":"Keep in mind that set_grad_enabled is a state of the program (\u201cdo you want to keep track of gradients for outputs when the inputs require gradients\u201d) and applies to new tensors, while someparam.requires_grad_(False) (which is the suggested form to disable gradients of tensors and parameters) says \u201cthis thing, when used as input, doesn\u2019t require gradients\u201d.\nTo then decide whether a given operation\u2019s output requires gradients, the autograd engine checks if both gradient-mode is enabled and any inputs require gradients.\nAs such these are two distinct knobs you can operate independently, but they have a combined effect.\n\n\n\n eprox:\n\n<code class=\"lang-auto\">with torch.set_grad_enabled(mode = True):\n    self.model.lin2.requires_grad = False # Will this work?\n   for m in self.model.mlp_f: m.set_grad_enabled = False # Or this?\n<\/code>\n\n\nThis seems to try to work on Modules, which isn\u2019t a thing (and using requires_grad_ would tell you so, which is why it is preferred). If you wanted something like that, you should go for for p in self.model.mlp_f.parameters(): p.requires_grad_(False) which certainly works.\nBest regards\nThomas\nOk Thank you that is useful."},{"x":"For example,in a dataset which contains cats and dogs,\nIn C==1 case,the requirements is:\n    Just segmenting out cats,treating all others as background(of course target will just label cats as 1,set all other pixels to 0)\nIn this case,should the output channel be 1 or 2?\nIn my opinion,it can be either 1 or 2,if setting output channel to 1,just use BCELossWithLogits,and if set it to 2,we must one-hot encoding targets firstly,then use BCELossWithLogits,\nam I right about this first case?\nIn C>1 case,requirement is:\nSegmenting out cats And Dogs\nI this case should the output channel be 2 or 2+1(background),in other words,should be C or C+1(background) if C>1?\nIn my opinion,it must 3(C+1) that the background must be treated as a class,am I right about this second case?","y":"For a binary classification use case, you can use either one or two output channels.\nFor a single channel output, you could use nn.BCEWithLogitsLoss.\nIf you are using two output channels, you could treat your use case as a\n\nmulti-class classification (only one valid class per pixel) and use nn.CrossEntropyLoss\n\nor as a multi-label classfication (zero, one or more classes valid per pixel), in which case you would again use nn.BCEWithLogitsLoss\n\n\nThe background would also represent a class as answered in your other topic with link \"https:\/\/discuss.pytorch.org\/t\/when-using-crossentropyloss-for-segmentation-how-to-encode-labels-into-single-channel-target-image-if-value-c-is-a-must-be\/65862\/2\".","z":"For a binary classification use case, you can use either one or two output channels.\nFor a single channel output, you could use nn.BCEWithLogitsLoss.\nIf you are using two output channels, you could treat your use case as a\n\nmulti-class classification (only one valid class per pixel) and use nn.CrossEntropyLoss\n\nor as a multi-label classfication (zero, one or more classes valid per pixel), in which case you would again use nn.BCEWithLogitsLoss\n\n\nThe background would also represent a class as answered in your other topic with link \"https:\/\/discuss.pytorch.org\/t\/when-using-crossentropyloss-for-segmentation-how-to-encode-labels-into-single-channel-target-image-if-value-c-is-a-must-be\/65862\/2\".\n\n\n\n ptrblck:\n\nification (only one valid class per pixel) and use nn.CrossEntropyLoss\n\n\nI got a binary segmentation problem working but am now modifying to be multi-class, labels[0,1,2\u20268]. I changed my loss function to crossentropyloss (previously BCEwithlogicloss), however I am now seeing error:\nFile \u201c\/home\/user\/miniconda3\/envs\/3dunet\/lib\/python3.7\/site-packages\/torch\/nn\/functional.py\u201d, line 1848, in nll_loss\nout_size, target.size()))\nValueError: Expected target size (2, 80, 160, 160), got torch.Size([2, 1, 80, 160, 160])\nI am not sure where this extra dimension would be coming from, I also had to change the output of my network to 9 channels (C+1) as it was only 1 for the binary seg.\nAny ideas? Thanks,\nKyle\nBased on the expected target shape I assume you are dealing with outputs as volumes, where each voxel would be classifies into one of the 9 classes?\nIf that\u2019s correct, you would have to remove dim1 via target = target.squeeze(1).\nI don\u2019t know where it\u2019s coming from, but feel free to post the Dataset code (including shape information) so that we could have a look at it."},{"x":"I am slightly confused on using weighted BCEWithLogitsLoss. My input data shape is : 1 x 52 x 52 x 52 (3D Volume) and label for each volume is either 0 or 1 and i am using a batch size of 5. So, at each epoch, input is 5 x 1 x 52 x 52 x 52 and label is 1 x 5. The way i am calculating weights is:\n<code class=\"lang-auto\">weight_0 = count_of_lbl1 \/ (total_lbl_count)\nweight_1 = count_of_lbl0 \/ (total_lbl_count)\n<\/code>\nMy question is should i calculate weights for each batch or per dataset. Also, assuming my label per batch is [0, 0, 1, 0, 1] and weight_0 is 0.4 and weight_1 is 0.6, so does the weight tensor passed to nn.BCEWithLogitsLoss will be [0.4, 0.4, 0.6, 0.4, 0.6] ?","y":"Hello Anil!\n\n\n\n ayadav01:\n\nMy question is should i calculate weights for each batch or per dataset.\n\n\nI prefer calculating the weight based on the entire dataset,\nrather than on a per-batch basis, although it shouldn\u2019t matter\nif your batches are of reasonable size.  The point is that the\nweights aren\u2019t magic numbers that have to be just right.  They\nare approximate numbers that are used to (partially) account\nfor having a significantly unbalanced dataset.\n\n\nAlso, assuming my label per batch is [0, 0, 1, 0, 1] and weight_0 is 0.4 and weight_1 is 0.6, so does the weight tensor passed to nn.BCEWithLogitsLoss will be [0.4, 0.4, 0.6, 0.4, 0.6] ?\n\n\nThis would be reasonable, but the disadvantage is that you would\nhave to construct a new instance of BCEWithLogitsLoss for\nevery batch, because your weight tensor depends on the batch.\nI am assuming here that by \u201cweight tensor\u201d you mean the tensor\nyou pass into BCEWithLogitsLoss's constructor as its named\nweight argument, e.g.:\n<code class=\"lang-python\">criterion = torch.nn.BCEWithLogitsLoss (weight = my_weight_tensor)\nloss = criterion (preds, targets)\n<\/code>\nIt is more convenient to use the named pos_weight constructor\nargument:\n<code class=\"lang-auto\">criterion = torch.nn.BCEWithLogitsLoss (pos_weight = torch.FloatTensor ([count_of_lbl0 \/ count_of_lbl1]))\n<\/code>\nYou can now use the same criterion loss object, constructed\nonce, over and over again for each batch.\nI would also note that if your relative weights are, in fact, 0.4\nand 0.6, your dataset isn\u2019t really very unbalanced, and I probably\nwouldn\u2019t bother using weights in the loss function.\n\n\nMy input data shape is : 1 x 52 x 52 x 52 (3D Volume) \u2026 and i am using a batch size of 5. So, at each epoch, input is 5 x 1 x 52 x 52 x 52 and label is 1 x 5.\n\n\nAs an aside, your shapes look a little confused.  I assume that\n\u201c5 x 1 x 52 x 52 x 52\u201d is the shape of the input to your model\n(not your loss function).  The shape of your label is probably [5]\n(although it could be [5, 1]), but a shape of [1, 5] would be\nwrong.\nBest.\nK. Frank","z":"Hello Anil!\n\n\n\n ayadav01:\n\nMy question is should i calculate weights for each batch or per dataset.\n\n\nI prefer calculating the weight based on the entire dataset,\nrather than on a per-batch basis, although it shouldn\u2019t matter\nif your batches are of reasonable size.  The point is that the\nweights aren\u2019t magic numbers that have to be just right.  They\nare approximate numbers that are used to (partially) account\nfor having a significantly unbalanced dataset.\n\n\nAlso, assuming my label per batch is [0, 0, 1, 0, 1] and weight_0 is 0.4 and weight_1 is 0.6, so does the weight tensor passed to nn.BCEWithLogitsLoss will be [0.4, 0.4, 0.6, 0.4, 0.6] ?\n\n\nThis would be reasonable, but the disadvantage is that you would\nhave to construct a new instance of BCEWithLogitsLoss for\nevery batch, because your weight tensor depends on the batch.\nI am assuming here that by \u201cweight tensor\u201d you mean the tensor\nyou pass into BCEWithLogitsLoss's constructor as its named\nweight argument, e.g.:\n<code class=\"lang-python\">criterion = torch.nn.BCEWithLogitsLoss (weight = my_weight_tensor)\nloss = criterion (preds, targets)\n<\/code>\nIt is more convenient to use the named pos_weight constructor\nargument:\n<code class=\"lang-auto\">criterion = torch.nn.BCEWithLogitsLoss (pos_weight = torch.FloatTensor ([count_of_lbl0 \/ count_of_lbl1]))\n<\/code>\nYou can now use the same criterion loss object, constructed\nonce, over and over again for each batch.\nI would also note that if your relative weights are, in fact, 0.4\nand 0.6, your dataset isn\u2019t really very unbalanced, and I probably\nwouldn\u2019t bother using weights in the loss function.\n\n\nMy input data shape is : 1 x 52 x 52 x 52 (3D Volume) \u2026 and i am using a batch size of 5. So, at each epoch, input is 5 x 1 x 52 x 52 x 52 and label is 1 x 5.\n\n\nAs an aside, your shapes look a little confused.  I assume that\n\u201c5 x 1 x 52 x 52 x 52\u201d is the shape of the input to your model\n(not your loss function).  The shape of your label is probably [5]\n(although it could be [5, 1]), but a shape of [1, 5] would be\nwrong.\nBest.\nK. Frank\nI see what you mean (took me a while to figure out). so rather than passing the weight argument (which is the rescaling weight of each bach element) to the loss function we pass the pos_weight argument (which is weight of positive example). In what scenarios would you want to use one or the other ? Also, if i am calculating pos_weight argument for each batch, don\u2019t i still have to instantiate the loss function for each batch to pass the pos_weight argument. Will something like this work:\n<code class=\"lang-auto\">criterion = torch.nn.BCEWithLogitsLoss()\nfor img, label in trainloader:\n    # Assuming i calculate 'pos_weights' for each batch\n    weights = torch.FloatTensor ([count_of_lbl0 \/ count_of_lbl1])\n    criterion.pos_weight = weights\n    loss = criterion(output, label)\n<\/code>\nHi Anil!\n\n\n\n ayadav01:\n\nrather than passing the weight argument (which is the rescaling weight of each bach element) to the loss function we pass the pos_weight argument (which is weight of positive example).\n\n\nThat is correct.\n\n\nIn what scenarios would you want to use one or the other ?\n\n\nIf your sample weight only depends on the class of the sample,\nyou can use either.  I tend to think that pos_weight is a little more\nconvenient.\nIf your sample weight depends on something other than the sample\u2019s\nclass \u2013 for example, if you\u2019re \u201chard mining\u201d and want to weight \u201chard\u201d\nsamples more heavily \u2013 then you would have to use weight and\nprovide per-sample weights.\nAlso, if you are using the different, but related loss class, BCELoss\n(which you generally shouldn\u2019t be using), you will have to use weight\nbecause, for whatever reason, BCELoss doesn\u2019t have a pos_weight\nargument for its constructor.\n\n\nAlso, if i am calculating pos_weight argument for each batch, don\u2019t i still have to instantiate the loss function for each batch to pass the pos_weight argument.\n\n\nYes (although, as you note below, it does appear to be possible to\nmodify  the pos_weight property after BCEWithLogitsLoss has\nbeen constructed).  But, again, my preference is to use the same\npos_weight for the whole dataset, rather than calculate it for each\nbatch.\n\n\nWill something like this work:\n<code class=\"lang-auto\">    weights = torch.FloatTensor ([count_of_lbl0 \/ count_of_lbl1])\n    criterion.pos_weight = weights\n<\/code>\n\n\nI don\u2019t see this discussed in the documentation, but it does appear to\nwork for me (using pytorch 0.3.0, and weight rather than pos_weight).\nI would probably avoiding doing it this way because I\u2019m not sure that\nit\u2019s officially supported.\nBest.\nK. Frank\nSorry for being late.\nIn my case, 90% of my dataset\u2019s have negative class and 10% of positive class (both train and validation set, binary classification problem)\nHow woul I use BCEWithLogitsLoss or BCELoss with theirs paremeters weights and\/or pos_weight to handle my imbalanced dataset\u2019s?\nBest regards"},{"x":"Hi everyone,\nSuppose I have a tensor\nmytensor = torch.tensor([10, 20, 30, 40)]\nEach node has a different threshold. Lets say that min thresholds are:\nmythreshold = torch.tensor([2, 50, 25, 100])\nI wanted to do torch.clamp(mytensor, min = mythreshold ) and  get as output\ntorch.tensor([10, 50, 30, 100]).\nIs it possible?\nThanks!","y":"torch.where should work:\n<code class=\"lang-python\">mytensor = torch.tensor([10, 20, 30, 40])\nmythreshold = torch.tensor([2, 50, 25, 100])\n\ntorch.where(mytensor < mythreshold, mythreshold, mytensor)\n> tensor([10, 50, 30, 100])\n<\/code>","z":"torch.where should work:\n<code class=\"lang-python\">mytensor = torch.tensor([10, 20, 30, 40])\nmythreshold = torch.tensor([2, 50, 25, 100])\n\ntorch.where(mytensor < mythreshold, mythreshold, mytensor)\n> tensor([10, 50, 30, 100])\n<\/code>\nWorked like a charm. Thanks for your awesome work here at the forum \nHi Ptrblck,\nI want to use Border22 with the values just higher than 0, i tried x[x>value] or x[:,x>value], but does not work. I need to have values just higher than zero and then pass it to the backward.()\nthis ( Border22[Border22>0]) give me again 0 value too.\n<code class=\"lang-auto\">        Border=torch.ones(fake.shape)\n        Border[:,:,4-2:4+3,4-2:4+3]=0\n        Border22=torch.mul(Border,fake).view(-1)\n        Border22[Border22>0]\n        Ones=torch.ones(1)\n        loss2=criterion2()(Border22,Ones)\n       loss2.bachward.()<\/code>\nIt seems you are missing the assignment of the indexing operation:\n<code class=\"lang-python\">Border22 = Border22[Border22>0]\n<\/code>\nHi Ptrblck,\nI change my cod ein different ways to solve the error. The current code is as follow, but it gave me zero gradients. Would you please help me with that? L1 is 0.8\n<code class=\"lang-auto\">\n        Thresholddefalut=nn.Threshold(0.98,0)\n                                \n        bbb=fake.squeeze(1)\n        bbb1=MASKGaussy.squeeze(1)\n\n        zzz=Thresholddefalut(bbb)\n        zzz1=Thresholddefalut(bbb1)\n\n        L1=nn.L1Loss()(zzz,zzz1)\n\n        loss2=L1\n            \n        loss2.backward()\n        print(netG.l3[0].weight.grad)\n## -----------------\n\n        class Generator994(nn.Module):\n    def __init__(self,ngpu,nz,ngf):\n        super(Generator994, self).__init__()\n        self.ngpu=ngpu\n        self.nz=nz\n        self.ngf=ngf\n        self.l1= nn.Sequential(\n\n            nn.ConvTranspose2d(self.nz, self.ngf * 8, 3, 1, 0, bias=False),\n            nn.BatchNorm2d(self.ngf * 8),\n            nn.ReLU(True),)\n \n        self.l2=nn.Sequential(nn.ConvTranspose2d(self.ngf * 8, self.ngf * 4, 3, 1, 0, bias=False),\n            nn.BatchNorm2d(self.ngf * 4),\n            nn.ReLU(True),)\n\n        self.l3=nn.Sequential(nn.ConvTranspose2d( self.ngf * 4, self.ngf * 2, 3, 1, 0, bias=False),\n            nn.BatchNorm2d(self.ngf * 2),\n             nn.ReLU(True),)\n6\n        self.l4=nn.Sequential(nn.ConvTranspose2d( self.ngf*2, 1, 3, 1, 0, bias=False),nn.Sigmoid()\n\n        )\n\n    def forward(self, input):\n        out=self.l1(input)\n        out=self.l2(out)\n        out=self.l3(out)\n        out=self.l4(out)\n\n        return out\n<\/code>\nI would appreciate your response\nYour current code snippet uses undefined methods, so I cannot debug it.\nAs usual, could you post an executable code snippet, which would reproduce this issue?\nHi Ptrblck,\nI solved the problem of this code. Now I have another , never finish. I need to do binary thresholding for x>value =1 and x<value=0, but in the way that after thresholding I can pass it to the backward.().\nFor example  for the above code zzz and zzz1 be the binary maps and use in L1 loss. How define zzz and zzz1?\nBased on your description it seems you want to use a step function, which would have a zero gradient everywhere (except exactly as x==value), so it won\u2019t be useful for your loss and gradient calculation.\nIf this would work, you could directly optimize the model predictions without e.g. using sigmoid or softmax."},{"x":"Hi,\ni have the following code\n<code class=\"lang-auto\">import torch.nn as nn\n\nclass Net(nn.Module):\n    def __init__(self):\n        super(Net, self).__init__()\n        self.output_layer = nn.Linear(1, 1)\n\nnet = Net()\n# normal memory usage\nnet = net.cuda()\n# python consumes about 2gb of host memory\n\n# now waste time so we can look at the memory usage\nb = 23\nfor i in range(1000000000):\n    for j in range(1000000000):\n        b = b + i % 3434 + j % 3553\n        b = b % 2354\n\nprint(f\"aha, {b}\")\n<\/code>\nwhen running this with linux (kubuntu 19.10), python 3.7 and pytorch 1.3, host memory usage is about 2gb and the gpu memory usage increases by about 700mb.\nI understand that there is some sort of memory pool when using cuda (so 700mb gpu memory sounds ok). I can imagine that it might be necessary to have a copy of all gpu memory on the host. but the host memory is almost triple the gpu memory, which seems odd to me.\nwould be cool if somebody could explain that ","y":"From what I recall, there is a fixed overhead to the cuda runtime. And the an overhead that depends on the amount of cuda code there is in the whole codebase (not sure why). And we do have a large number of cuda kernels as we have pytorch\u2019s own ones, cudnn, magma etc.","z":"Hi,\nThis is most likely only the memory used by the cuda driver \n<code class=\"lang-python\">import torch\n# No memory\/process on the gpu + 180MB of cpu memory\ninput(\"Press enter to continue\")\n\n# Store a cpu Tensor\na = torch.rand(1)\n# Still nothing on the GPU, still 180MB on the cpu\ninput(\"Press enter to continue\")\n\n# Store a Tensor with one element on the gpu (the Tensor takes 4bytes)\na = torch.rand(1, device=\"cuda\")\n# Cuda driver is not initialized and uses 543MB on my gpu and almost 2GB - 180MB on the cpu\ninput(\"Press enter to continue\")<\/code>\nIt sounds very excessive to me, but i guess I\u2019ll just have to accept that.\nFrom what I recall, there is a fixed overhead to the cuda runtime. And the an overhead that depends on the amount of cuda code there is in the whole codebase (not sure why). And we do have a large number of cuda kernels as we have pytorch\u2019s own ones, cudnn, magma etc.\nok. well, i guess it\u2019s like loading a standard c\/c++ library. the whole code is loaded into memory, which is a lot in case of pytorch (as you said).\nand in case of cuda it might take more space than standard c\/c++ as it needs to load the device independent version and possibly a device specific cached version.\nstill, to me it seems like a big inefficiency (code could be loaded on demand etc). for me (research) that isn\u2019t an issue, but running inference on embedded devices can be impossible due to that.\ncheers for the answer \nedit:\nI also found this:\nhttps:\/\/devtalk.nvidia.com\/default\/topic\/1044191\/determine-memory-cuda-context-memory-usage\/?offset=5\nthey have a similar problem, although on a different scale (30mb vs 180mb allocation when only instantiating the kernel on different gpus). an nvidia engineer answered that the allocation depends on the gpu (number of SMs). so i guess in my case (rtx 2080) it\u2019s pretty large.\nUpdate (I hope it might help somebody):\ni started to write my own C++\/CUDA extension, along with a small C++ starter. I want to use that starter for easier debugging and profiling using cuda_gdb and friends. After making everything work with the cmake build system, I started up nsight to step into the code. Right after calling Tensor::cuda(), the debugger hanged. I thought that something is not working, but turns out that it just takes one minute and 40 seconds for that one step. After that stepping is back to normal. I guess that loading all these kernels inside cuda_gdb is super slow.\nSimilar things were also reported in the nvidia forums (thought that was 2009): https:\/\/forums.developer.nvidia.com\/t\/cuda-gdb-performance\/9175"},{"x":"I see in this post with link \"https:\/\/discuss.pytorch.org\/t\/vf-lstm-implementation\/35536\" where _VF.lstm is defined (presumably as LSTMImpl). Since it calls torch::lstm (this line with link \"https:\/\/github.com\/pytorch\/pytorch\/blob\/49777e67303f608987ec0948c7fd8f46f6d3ca83\/torch\/csrc\/api\/src\/nn\/modules\/rnn.cpp#L275\"), I\u2019m trying to track down torch::lstm. Any pointers would be much appreciated!","y":"Hi,\nThe torch::XXX binding is generated based on the info from here with link \"https:\/\/github.com\/pytorch\/pytorch\/blob\/4bead6438a4271f154980dc242199c2f1b897bc1\/aten\/src\/ATen\/native\/native_functions.yaml#L4269-L4273\" which then hits the native code here with link \"https:\/\/github.com\/pytorch\/pytorch\/blob\/4bead6438a4271f154980dc242199c2f1b897bc1\/aten\/src\/ATen\/native\/RNN.cpp#L1396-L1399\".\nHope this helps.","z":"Hi,\nThe torch::XXX binding is generated based on the info from here with link \"https:\/\/github.com\/pytorch\/pytorch\/blob\/4bead6438a4271f154980dc242199c2f1b897bc1\/aten\/src\/ATen\/native\/native_functions.yaml#L4269-L4273\" which then hits the native code here with link \"https:\/\/github.com\/pytorch\/pytorch\/blob\/4bead6438a4271f154980dc242199c2f1b897bc1\/aten\/src\/ATen\/native\/RNN.cpp#L1396-L1399\".\nHope this helps."},{"x":"What is the difference between pytorch geometric, pytorch 3D and Kaolin? Which one is best for what task?","y":"Dear ,\nI think is a good question but a bit broad. I will try to answer based on my knowledge ( I haven\u2019t used Kaolin yet)\n\n\nbut Pytorch geometric with link \"https:\/\/pytorch-geometric.readthedocs.io\/en\/latest\/\" and github with link \"https:\/\/github.com\/rusty1s\/pytorch_geometric\" has different methods implemented that you can see there and it is completely in Python (around 100 contributors),\n\n\nKaolin with link \"https:\/\/github.com\/NVIDIAGameWorks\/kaolin\" in C++ and Python (of course Pytorch) with only 13 contributors\n\n\nPytorch3D with link \"https:\/\/github.com\/facebookresearch\/pytorch3d\" with around 40 contributors\n\n\nThey all have different targets and applications, I would consider what is your goal and try to find people who are working on similar problems and ask what they are using and why.","z":"Dear ,\nI think is a good question but a bit broad. I will try to answer based on my knowledge ( I haven\u2019t used Kaolin yet)\n\n\nbut Pytorch geometric with link \"https:\/\/pytorch-geometric.readthedocs.io\/en\/latest\/\" and github with link \"https:\/\/github.com\/rusty1s\/pytorch_geometric\" has different methods implemented that you can see there and it is completely in Python (around 100 contributors),\n\n\nKaolin with link \"https:\/\/github.com\/NVIDIAGameWorks\/kaolin\" in C++ and Python (of course Pytorch) with only 13 contributors\n\n\nPytorch3D with link \"https:\/\/github.com\/facebookresearch\/pytorch3d\" with around 40 contributors\n\n\nThey all have different targets and applications, I would consider what is your goal and try to find people who are working on similar problems and ask what they are using and why."},{"x":"This is my train method for cross_entropy:\n<code class=\"lang-auto\">def train_crossentropy(train_iter, dev_iter, test_iter, model, args):\n    print('training...')\n    if args.cuda:\n        model.cuda()\n\n    optimizer = torch.optim.Adam(model.parameters(), lr=args.lr)\n\n    steps = 0\n    best_accuracy = 0\n    last_step = 0\n    model.train()\n    for epoch in range(1, args.epochs + 1):\n        for batch in train_iter:\n            feature, target = batch.text, batch.label\n\n            feature = feature.data.t()\n            target = target.data.sub(1)\n\n            if args.cuda:\n                feature, target = feature.cuda(), target.cuda()\n\n            optimizer.zero_grad()\n            logit = model(feature)\n\n            loss = F.cross_entropy(logit, target)\n            loss.backward()\n            optimizer.step()\n<\/code>\nThis is my train method for kl-loss:\n<code class=\"lang-auto\">def train_soft(train_iter, dev_iter, test_iter, model, args, temperature=1):\n    print('training with train_soft...')\n    if args.cuda:\n        model.cuda()\n\n    optimizer = torch.optim.Adam(model.parameters(), lr=args.lr)\n\n    steps = 0\n    best_accuracy = 0\n    last_step = 0\n    model.train()\n    for epoch in range(1, args.epochs + 1):\n        for batch in train_iter:\n            feature, target = batch.text, batch.label\n\n            feature = feature.data.t()\n            target = target.data.sub(1)\n            \n            ############# Soft-label changes start ######################\n            target_un = target.unsqueeze(0)\n            target_t = target_un.permute(1, 0)\n            \n            y_onehot = torch.FloatTensor(batch.batch_size, args.class_num)\n            y_onehot.zero_()\n            y_onehot.scatter_(1, target_t, 1)\n            \n            \n            if args.cuda:\n                feature, target, y_onehot = feature.cuda(), target.cuda(), y_onehot.cuda()\n\n            optimizer.zero_grad()\n            logit = model(feature)\n\n            logits_flat = logit.view(-1, logit.size(-1))\n            log_probs_flat = F.log_softmax(logits_flat \/ temperature, dim=1)\n            target_flat = F.softmax(y_onehot \/ temperature, dim=1)\n            \n            loss = F.kl_div(log_probs_flat, target_flat)\n            loss.backward()\n            \n            optimizer.step()\n\n<\/code>\nI\u2019m using the same target values in both cases. Just that in the KL version, the target values are converted into a one-hot format.\nI cross-entropy loss values (for a small batch) is of order 10, while kl-loss values (for the same batch) is of order 0.001.\nThis is leading to very slow gradient descent and hence worse performance (using kl-loss).\nIs this expected? I had an intuition that the loss values should be similar under this setup.","y":"I was able to exactly align the loss values for KL-divergence and cross-entropy. See the code below:\n<code class=\"lang-auto\">a = torch.randint(10, (2, 5))\nb = torch.Tensor([[0., 0., 0., 1., 0.], [0., 0., 1., 0., 0.]])\ntarget = torch.Tensor([3, 2]).type(torch.LongTensor)\n\nprint('a', a)\nprint('b', b)\n\na_log_soft = F.log_softmax(a.float(), dim=1)\nprint('a_log_soft', a_log_soft)\nb_soft = F.softmax(b, dim=1)\nprint('b_soft', b_soft)\n\nprint('kl_loss batchmean', F.kl_div(a_log_soft, b, reduction='batchmean'))\n\nprint('kl_loss_prob batchmean', F.kl_div(a_log_soft, b_soft, reduction='batchmean'))\n\nprint('cross_loss mean', F.cross_entropy(a.float(), target))\n<\/code>\nhere, kl_loss batchmean aligns perfectly with cross_loss mean. However, kl_loss_prob batchmean doesn\u2019t align with cross_loss mean. However, in a real scenario if we have our b input as raw logits, kl_loss batchmean is the one that should be used.\nAlso, make sure to use reduction='batchmean'.\nHence, in my original question all I need to do is change:\n<code class=\"lang-auto\">loss = F.kl_div(log_probs_flat, target_flat)\n<\/code>\nto\n<code class=\"lang-auto\">loss = F.kl_div(log_probs_flat, y_onehot, reduction='batchmean')\n<\/code>\nand this loss will exactly match with the cross-entropy loss.","z":"There are a couple of caveats in the notes coming with the KLDivLoss documentation with link \"https:\/\/pytorch.org\/docs\/stable\/generated\/torch.nn.KLDivLoss.html#torch.nn.KLDivLoss\".\nOnce you have these taken care of and use the correct input conventions, you should get the same loss and gradients for the things that ought to be mathematically equivalent (up to numerical error, of course).\nI was able to exactly align the loss values for KL-divergence and cross-entropy. See the code below:\n<code class=\"lang-auto\">a = torch.randint(10, (2, 5))\nb = torch.Tensor([[0., 0., 0., 1., 0.], [0., 0., 1., 0., 0.]])\ntarget = torch.Tensor([3, 2]).type(torch.LongTensor)\n\nprint('a', a)\nprint('b', b)\n\na_log_soft = F.log_softmax(a.float(), dim=1)\nprint('a_log_soft', a_log_soft)\nb_soft = F.softmax(b, dim=1)\nprint('b_soft', b_soft)\n\nprint('kl_loss batchmean', F.kl_div(a_log_soft, b, reduction='batchmean'))\n\nprint('kl_loss_prob batchmean', F.kl_div(a_log_soft, b_soft, reduction='batchmean'))\n\nprint('cross_loss mean', F.cross_entropy(a.float(), target))\n<\/code>\nhere, kl_loss batchmean aligns perfectly with cross_loss mean. However, kl_loss_prob batchmean doesn\u2019t align with cross_loss mean. However, in a real scenario if we have our b input as raw logits, kl_loss batchmean is the one that should be used.\nAlso, make sure to use reduction='batchmean'.\nHence, in my original question all I need to do is change:\n<code class=\"lang-auto\">loss = F.kl_div(log_probs_flat, target_flat)\n<\/code>\nto\n<code class=\"lang-auto\">loss = F.kl_div(log_probs_flat, y_onehot, reduction='batchmean')\n<\/code>\nand this loss will exactly match with the cross-entropy loss."},{"x":"I am trying to call the \\_\\_getitem__ function of my dataset once per batch due to the cost of each dataset query (on remote).\n<code class=\"lang-auto\">class Dataset(Dataset):\n\n    def __init__(self):\n       ...\n\n    def __len__(self):\n        ...\n\n    def __getitem__(self, batch_idx):  ------> here I get only one index\n        return self.wiki_df.loc[batch_idx]\n\n\nloader = DataLoader(\n                dataset=dataset,\n                batch_sampler=BatchSampler(\n                    SequentialSampler(dataset), batch_size=self.hparams.batch_size, drop_last=False),\n                num_workers=self.hparams.num_data_workers,\n            )\n<\/code>\nThis is the current implementation, which does not work.\nIs there a way to get the list of indices in the getitem function of the dataset","y":"Maybe this code snippet might be helpful:\n<code class=\"lang-python\">class MyDataset(Dataset):\n    def __init__(self):\n        self.data = torch.arange(100).view(100, 1).float()\n        \n    def __getitem__(self, index):\n        x = self.data[index]\n        return x\n    \n    def __len__(self):\n        return len(self.data)\n\ndataset = MyDataset()        \nsampler = torch.utils.data.sampler.BatchSampler(\n    torch.utils.data.sampler.RandomSampler(dataset),\n    batch_size=10,\n    drop_last=False)\n\nloader = DataLoader(\n    dataset,\n    sampler=sampler)\n\nfor data in loader:\n    print(data)\n<\/code>\nThe index inside __getitem__ will contain 10 random indices, which are used to create the batch in:\n<code class=\"lang-python\">x = self.data[index]\n<\/code>\nYou could replace this with your expensive data loading operation.\nLet me know, if that helps.","z":"You could disable automatic batching as described here with link \"https:\/\/pytorch.org\/docs\/stable\/data.html#disable-automatic-batching\" and use a BatchSampler.\nLet me know, if that works for you.\n\n\n\n ptrblck:\n\nYou could disable automatic batching as described here with link \"https:\/\/pytorch.org\/docs\/stable\/data.html#disable-automatic-batching\" and use a  BatchSampler .\nLet me know, if that works for you.\n\n\nWell conceptually yes, But practically I just can\u2019t get my hands around the documentation.\nIf I set both batchsampler and batchsize to none (in order to turn off automatic batching) how does the system knows my batchsize? how does the __getitem__ gets triggered?\nMaybe this code snippet might be helpful:\n<code class=\"lang-python\">class MyDataset(Dataset):\n    def __init__(self):\n        self.data = torch.arange(100).view(100, 1).float()\n        \n    def __getitem__(self, index):\n        x = self.data[index]\n        return x\n    \n    def __len__(self):\n        return len(self.data)\n\ndataset = MyDataset()        \nsampler = torch.utils.data.sampler.BatchSampler(\n    torch.utils.data.sampler.RandomSampler(dataset),\n    batch_size=10,\n    drop_last=False)\n\nloader = DataLoader(\n    dataset,\n    sampler=sampler)\n\nfor data in loader:\n    print(data)\n<\/code>\nThe index inside __getitem__ will contain 10 random indices, which are used to create the batch in:\n<code class=\"lang-python\">x = self.data[index]\n<\/code>\nYou could replace this with your expensive data loading operation.\nLet me know, if that helps.\n\n\n\n ptrblck:\n\n\nYes, it does:)\nSo not intuitive that batchsampler goes into the sampler parameter.\nmaybe it\u2019s just me\nCan we do the same thing with batch_sampler?\nCould you explain your question and use case a bit?\nIn my code snippet I\u2019m already using a BatchSampler."},{"x":"I\u2019m currently working on a regression model with input data of range(1, 26) and target data of range(1,  42) and I discovered that when I normalize the data to be within range(0, 1) and set learning rate to 0.001 the loss reduces by a huge factor after one epoch it\u2019s so unbelievable than when I don\u2019t normalize the training data (for eg: without normalization, loss can 5.0 and with normalization, loss can be 0.01 all after 1st epoch).\nSo this leaves me wondering could it be due to a relationship between the normalized input and learning rate or loss and learning rate tho, I don\u2019t think the loss should change whether normalized or not coz the target data too was normalized accordingly\nSo what could be the cause?","y":"\n\n\n Henry_Chibueze:\n\nI don\u2019t think the loss should change whether normalized or not coz the target data too was normalized accordingly\n\n\nI\u2019m not sure, if I understand this sentence correctly.\nAre you comparing these use cases:\n<code class=\"lang-python\"># case0\noutput = model(unnormalized_input)\nloss0 = criterion(output, unnormalized_target)\n\n# case1\noutput = model(normalized_input)\nloss1 = criterion(output, normalized_target)\n<\/code>\nIf so, then I would expect that loss1 would be potentially smaller than loss0, as the magnitude of the output and target is most likely smaller (assuming you are using e.g. nn.MSELoss).","z":"\n\n\n Henry_Chibueze:\n\nI don\u2019t think the loss should change whether normalized or not coz the target data too was normalized accordingly\n\n\nI\u2019m not sure, if I understand this sentence correctly.\nAre you comparing these use cases:\n<code class=\"lang-python\"># case0\noutput = model(unnormalized_input)\nloss0 = criterion(output, unnormalized_target)\n\n# case1\noutput = model(normalized_input)\nloss1 = criterion(output, normalized_target)\n<\/code>\nIf so, then I would expect that loss1 would be potentially smaller than loss0, as the magnitude of the output and target is most likely smaller (assuming you are using e.g. nn.MSELoss).\nOk I think I understand now\nI\u2019ll try using the smoothL1loss instead coz the MSEloss is leading to exploding and vanishing gradients coz of the way it penalize the algorithm based on losses greater than 1 and losses less than zero"},{"x":"Hi all,\nAssume that we have a pertained NN model like LeNet-5 which successfully predicts handwritten digits. In this case number of features is 784 (assuming 28x28 input images) and number of outputs is 10. Sum of the output values(probs) adds up to 1 and each output shows the probability of that class for the given input image.\nNow, assuming that I already have this model. And I can predict the class of any input image using feedforward model prediction.\nMy question is: For any test input image, I would like to calculate derivative of any model output w.r.t model input features. This way I will be calculating a 784x10 Jacobian matrix J.\nFor example J[0,0] is the derivative of output 0 w.r.t input feature 0 and J[2,3] is derivative of output 3 w.r.t input feature 2 and so on\u2026\nActually maybe I can calculate each element of this Jacobian via a messy code block but I wonder if there is any easy and elegant way of doing so.\nAny comment on how to calculate the J matrix?","y":"Hello \u00d6mer!\n\n\n\n jarvico:\n\nMy question is: For any test input image, I would like to calculate derivative of any model output w.r.t model input features. This way I will be calculating a 784x10 Jacobian matrix J.\n\n\nJust as you use pytorch\u2019s autograd to calculate the derivatives (gradient)\nof your loss function with respect to your model\u2019s parameters (and then\nuse those to update your model with gradient descent), you can use\nautograd to calculate the derivatives of a prediction for a single class\nwith respect to the input to your model.  This will be a single column of\nyour Jacobian matrix.\nYou can then loop over predictions \/ columns to build the full Jacobian.\nModels work on batches, even if you want to process a single image.\nSo, for a single image, you need a batch with batch size of one.\nLet\u2019s say you have an input tensor, with shape [1, 784].  (It could be\n[1, 28, 28], if that is what your model expects.)  You say your model\nhas ten classes, so you will have:\n<code class=\"lang-python\">preds = model (input)\n<\/code>\nwhere preds has shape [nBatch, nClass] = [1, 10].\nTell pytorch to track gradients with respect to your input, apply your\nmodel to input, and call .backward() on your preds[i], looping over\ni.  The .grad property of of your input tensor will be the ith column\nof the Jacobian.\n<code class=\"lang-python\">J = torch.zeros ((1, 784, 10))   # loop will fill in Jacobian\ninput.requires_grad = True\npreds = model (input)\nfor  i in range (10):\n    grd = torch.zeros ((1, 10))   # same shape as preds\n    grd[0, i] = 1    # column of Jacobian to compute\n    preds.backward (gradient = grd, retain_graph = True)\n    J[:,:,i] = input.grad   # fill in one column of Jacobian\n    input.grad.zero_()   # .backward() accumulates gradients, so reset to zero\n\n<\/code>\nYou could also try pytorch\u2019s experimental jacobian() with link \"https:\/\/pytorch.org\/docs\/stable\/autograd.html#torch.autograd.functional.jacobian\" function, which\nI think basically wraps the loop I outlined above, but with more bells\nand whistles (but I\u2019ve never used it).\nGood luck.\nK. Frank","z":"Hello \u00d6mer!\n\n\n\n jarvico:\n\nMy question is: For any test input image, I would like to calculate derivative of any model output w.r.t model input features. This way I will be calculating a 784x10 Jacobian matrix J.\n\n\nJust as you use pytorch\u2019s autograd to calculate the derivatives (gradient)\nof your loss function with respect to your model\u2019s parameters (and then\nuse those to update your model with gradient descent), you can use\nautograd to calculate the derivatives of a prediction for a single class\nwith respect to the input to your model.  This will be a single column of\nyour Jacobian matrix.\nYou can then loop over predictions \/ columns to build the full Jacobian.\nModels work on batches, even if you want to process a single image.\nSo, for a single image, you need a batch with batch size of one.\nLet\u2019s say you have an input tensor, with shape [1, 784].  (It could be\n[1, 28, 28], if that is what your model expects.)  You say your model\nhas ten classes, so you will have:\n<code class=\"lang-python\">preds = model (input)\n<\/code>\nwhere preds has shape [nBatch, nClass] = [1, 10].\nTell pytorch to track gradients with respect to your input, apply your\nmodel to input, and call .backward() on your preds[i], looping over\ni.  The .grad property of of your input tensor will be the ith column\nof the Jacobian.\n<code class=\"lang-python\">J = torch.zeros ((1, 784, 10))   # loop will fill in Jacobian\ninput.requires_grad = True\npreds = model (input)\nfor  i in range (10):\n    grd = torch.zeros ((1, 10))   # same shape as preds\n    grd[0, i] = 1    # column of Jacobian to compute\n    preds.backward (gradient = grd, retain_graph = True)\n    J[:,:,i] = input.grad   # fill in one column of Jacobian\n    input.grad.zero_()   # .backward() accumulates gradients, so reset to zero\n\n<\/code>\nYou could also try pytorch\u2019s experimental jacobian() with link \"https:\/\/pytorch.org\/docs\/stable\/autograd.html#torch.autograd.functional.jacobian\" function, which\nI think basically wraps the loop I outlined above, but with more bells\nand whistles (but I\u2019ve never used it).\nGood luck.\nK. Frank\nHi Frank,\nI just did a very minor change and it seems to work. Thanks for your help! Appreciated\u2026\nmy_input=test_data[num][0].view(1,1,28,28)\nJ = torch.zeros((784, 10))   # loop will fill in Jacobian\nJ = J.float()\nmy_input.requires_grad_()\npreds = model(my_input)\nprint(\"preds shape is: \",preds.shape)\nfor i in range (10):\ngrd = torch.zeros ((1, 10))   # same shape as preds\ngrd[0, i] = 1    # column of Jacobian to compute\npreds.backward(gradient = grd, retain_graph = True)\nJ[:,i] = my_input.grad.view(784).float()   # fill in one column of Jacobian\nmy_input.grad.zero_()   # .backward() accumulates gradients, so reset to zero\nprint(J.shape)\nprint(J)"},{"x":"Is there any easy way to reliably detect if a function is a JIT ScriptFunction?\nFor example:\n<code class=\"lang-auto\">.jit.script\ndef func1(x):\n    return x.sum()\n\ndef func2(x):\n    return x.sum()\n\ndef detect_jit_script(func):\n    pass # should return True for func1 and False for func2\n<\/code>\nI realize that func1 is compiled to a method (so it\u2019s no longer a function).\nHowever, using inspect.ismethod doesn\u2019t work because the input func can be a method of a class.\nAnd apparently the compiled func1 does not have any attributes other than __call__ which makes it harder to get and check the object.","y":"Is isinstance(func1, torch.jit.ScriptFunction) working for you?\nBest regards\nThomas","z":"Is isinstance(func1, torch.jit.ScriptFunction) working for you?\nBest regards\nThomas\nAh, yes, it actually works!\nI tried it before and it returns False, I didn\u2019t realize my code put func.__call__ into my function that\u2019s why it fails."},{"x":"Hi,\nI am trying to append some more data points to my batch but I can\u2019t get it to work.\nI have one tensor of [64, 40] and the other a [64, 10].\nWhenever I do torch.cat or torch.stack it gives me a dimension error.\nAnyone know how to append the latter [64, 10] to the [64, 40] tensor?","y":"hello\ntry this:\n<code class=\"lang-auto\">b = torch.rand(64, 10)\na = torch.rand(64, 40) \ntorch.cat([a, b], dim=1) # shape is torch.Size([64, 50])\n<\/code>\nyou can have more details in these posts:\nhow-to-concatenate-list-of-pytorch-tensors with link \"https:\/\/discuss.pytorch.org\/t\/how-to-concatenate-list-of-pytorch-tensors\/1350\/5\"\ntensor-stack-or-concatenate with link \"https:\/\/discuss.pytorch.org\/t\/tensor-stack-or-concatenate\/34331\"","z":"hello\ntry this:\n<code class=\"lang-auto\">b = torch.rand(64, 10)\na = torch.rand(64, 40) \ntorch.cat([a, b], dim=1) # shape is torch.Size([64, 50])\n<\/code>\nyou can have more details in these posts:\nhow-to-concatenate-list-of-pytorch-tensors with link \"https:\/\/discuss.pytorch.org\/t\/how-to-concatenate-list-of-pytorch-tensors\/1350\/5\"\ntensor-stack-or-concatenate with link \"https:\/\/discuss.pytorch.org\/t\/tensor-stack-or-concatenate\/34331\"\n\n\n\n pfloat:\n\ntorch.cat([a, b], dim=1)\n\n\nNice it worked. Thank you."},{"x":"I am trying to assign different weights to tensors in my batch when computing cross entropy loss.  The issue I am having is that these weights are not based on labels so I can\u2019t seem to give them to nn.CrossEntropyLoss() as outlined at https:\/\/pytorch.org\/docs\/stable\/nn.functional.html#cross-entropy.  For an example of what I am trying to do inside each batch:\n<code class=\"lang-auto\">criterion = CrossEntropyLoss_modified()\nfake_outputs = torch.rand((2,2)) #single batch\nlabels = torch.randint(0,2,(2,))\nweights = torch.tensor([.7,.3]) #these values will be changed for each batch and each item in the batch will have a weight assigned to it\ncriterion(fake_outputs, labels, weights) # should output a weighted loss\n<\/code>\nThe length of the weights will be the length of the batch, not the length.\nIs this doable?  Is there a better way to go about this?","y":"I would set the reduction to none: CrossEntropyLoss(reduction='none')\nThis returns you a num_batch size tensor. So you can then do something like this:\n(weights * criterion(output, labels)).mean()","z":"I would set the reduction to none: CrossEntropyLoss(reduction='none')\nThis returns you a num_batch size tensor. So you can then do something like this:\n(weights * criterion(output, labels)).mean()\nPerfect, just what I was looking for.  Thanks!"},{"x":"Hi,\nI\u2019m looking for a function to sort the value of a 2D tensor by a given order.\nFor example, A is a 2D tensor ([1, 0, 2, 2, 1], [0, 2, 1, 2, 0]), and I want to sort A following an order tensor B as ([3, 2, 4, 1, 0], [2, 1, 4, 3, 3]). So the sorted A should be ([2, 2, 1, 0, 1], [1, 2, 0, 2, 2]). There can be repeating elements in the same row of B and each row of A is sorted by the order in corresponding row of B.\nIs there any function I can directly apply to get the sorted A?\nAny suggestion is appreciated!","y":"Tensorized version can be implemented as follows:\n<code class=\"lang-auto\">def smart_sort(x, permutation):\n    d1, d2 = x.size()\n    ret = x[\n        torch.arange(d1).unsqueeze(1).repeat((1, d2)).flatten(),\n        permutation.flatten()\n    ].view(d1, d2)\n    return ret\n<\/code>\nAnd it sorts as you want:\n<code class=\"lang-auto\">a = torch.tensor([[1, 0, 2, 2, 1], [0, 2, 1, 2, 0]])\nb = torch.tensor([[3, 2, 4, 1, 0], [2, 1, 4, 3, 3]])\nsmart_sort(a, b)\n\noutput: tensor([[2, 2, 1, 0, 1],\n               [1, 2, 0, 2, 2]])\n<\/code>","z":"Hello,\nAs far as I know, there is no such function to get a 2D tensor sorted from another 2D tensor. However, with a simple for loop and indexing, it is possible to achieve the desired result:\n<code class=\"lang-auto\">    a = torch.tensor([[1, 0, 2, 2, 1], [0, 2, 1, 2, 0]], dtype=torch.int32)\n    # index tensor (b) must be of type long, byte or bool\n    b = torch.tensor([[3, 2, 4, 1, 0], [2, 1, 4, 3, 3]], dtype=torch.long)\n    for i in range(b.shape[0]):\n        # change the order of the ith tensor of a with the indexes of b[i]\n        c = a[i][b[i]]\n        print(c)\n    # tensor([2, 2, 1, 0, 1], dtype=torch.int32)\n    # tensor([1, 2, 0, 2, 2], dtype=torch.int32)\n<\/code>\nDon\u2019t know if this helps!\nTensorized version can be implemented as follows:\n<code class=\"lang-auto\">def smart_sort(x, permutation):\n    d1, d2 = x.size()\n    ret = x[\n        torch.arange(d1).unsqueeze(1).repeat((1, d2)).flatten(),\n        permutation.flatten()\n    ].view(d1, d2)\n    return ret\n<\/code>\nAnd it sorts as you want:\n<code class=\"lang-auto\">a = torch.tensor([[1, 0, 2, 2, 1], [0, 2, 1, 2, 0]])\nb = torch.tensor([[3, 2, 4, 1, 0], [2, 1, 4, 3, 3]])\nsmart_sort(a, b)\n\noutput: tensor([[2, 2, 1, 0, 1],\n               [1, 2, 0, 2, 2]])\n<\/code>\nThanks for the help!\nThank you for your advice!\nyou can try this,also\n<code class=\"lang-auto\">def sort_tensor(X, dim=-1):\n  d1, d2 = X.size()\n  ten, ind=torch.sort(X, dim=dim)\n  if dim == -1:\n    return X[torch.arange(d1).unsqueeze(1).repeat((1, d2)), ind]\n  elif dim == 0:\n    return X[ind, torch.arange(d2)]\n  else:\n    raise IndexError(\"wrong dim parameter\")\n<\/code>\noutput:\n<code class=\"lang-auto\">t = torch.tensor([[23, 45, 66, 100], \n                [90, 56, 8,132],\n                [34, 90, 88, 200],\n                [250, 15, 90, 5]])\n\nsort_tensor(t, dim=-1)\noutput:\ntensor([[ 23,  45,  66, 100],\n        [  8,  56,  90, 132],\n        [ 34,  88,  90, 200],\n        [  5,  15,  90, 250]])\n\nsort_tensor(t, dim=-0)\noutput:\ntensor([[ 23,  15,   8,   5],\n        [ 34,  45,  66, 100],\n        [ 90,  56,  88, 132],\n        [250,  90,  90, 200]])\n                  \n<\/code>"},{"x":"I tried to create a Transformer-based Seq2Seq generation using greedy search. Once I ran the code below, the outputs are exactly the same. That\u2019s good; nothing is wrong with this. However, if I run it again, the result is completely different.\nAnyone knows what might cause this?\n<code class=\"lang-auto\">\nfor _ in range(10):  # Do not forget this \"loop\" line.\n    \n    import math\n    import os\n    from collections import namedtuple\n    import random\n    import numpy as np\n    import torch \n    import torch.nn as nn\n    from tqdm.auto import tqdm\n    from transformers import BertTokenizer\n    from torch.utils.data import Dataset, DataLoader\n    from src.models.transformer import Transformer\n    \n    device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n    tokenizer = BertTokenizer.from_pretrained('bert-base-uncased', additional_special_tokens=['_eos', '_go'])\n    args = {\n        'max_src_len': 300,\n        'max_tgt_len': 50,\n        'batch_size': 64,\n        'vocab_size': len(tokenizer),\n        'hidden_size': 512,\n        'dropout': 0.2,\n        'num_layers': 4,\n        'num_heads': 8,\n        'sos_idx': tokenizer.encode('_go', add_special_tokens=False)[0],\n        'eos_idx': tokenizer.encode('_eos', add_special_tokens=False)[0],\n        'pad_idx': tokenizer.encode('[PAD]', add_special_tokens=False)[0],\n    }\n\n    args = namedtuple('args', args.keys())(*args.values())\n    \n    # Helper function\n    def convert_att_into_mask(mask):\n        return mask.bool().masked_fill(mask == 0, True).masked_fill(mask == 1, False)\n\n    # Transformer Generatior - Pretrained\n    gen_weight_path = '\/data\/vitou\/100DaysofCode\/conv_agent\/checkpoints\/pretrained\/transformer_generative.pt'\n    generator = Transformer(args).to(device)\n    generator.load_state_dict(torch.load(gen_weight_path))\n    generator.eval()\n\n    tokenizer_config = {\n        'add_special_tokens' : False, \n        'return_token_type_ids' : False, \n        'return_tensors' : 'pt',\n        'padding': True\n    }\n\n    with torch.no_grad():\n        ctx_text = \"hello. do you play any video games? _eos\"\n        src = tokenizer(ctx_text, **tokenizer_config)\n        src_input_ids, src_att_mask = [ x.to(device) for x in src.values() ]\n        src_input_ids = torch.transpose(src_input_ids, 0, 1).contiguous()\n        src_att_mask = convert_att_into_mask(src_att_mask)\n\n        res = generator.generate(src_input_ids, src_att_mask[:1])\n        print (\"[context]: \", ctx_text)\n        print (\"[greedy]:  \", tokenizer.decode(res))\n        print (\"-------------------------------------------\")\n<\/code>\nOutput #1\n<code class=\"lang-nohighlight\">[context]:  hello. do you play any video games? _eos\n[greedy]:   of the games. i don't really watch much tv. i don't watch much tv. you? _go people like to watch tv. _go of the time. _go people do. _go of the sports. _go of the actors\n------------------------------------------- \n[context]:  hello. do you play any video games? _eos\n[greedy]:   of the games. i don't really watch much tv. i don't watch much tv. you? _go people like to watch tv. _go of the time. _go people do. _go of the sports. _go of the actors\n-------------------------------------------\n[x8] more of this (exactly the same for each iteration)\n<\/code>\nOutput #2\n<code class=\"lang-nohighlight\">[context]:  hello. do you play any video games? _eos\n[greedy]:   i do, i love the simpsons! _eos\n------------------------------------------- \n[context]:  hello. do you play any video games? _eos\n[greedy]:   i do, i love the simpsons! _eos\n-------------------------------------------\n[x8] more of this (exactly the same for each iteration)\n<\/code>\nSo the output I got is either one of the two outputs above.","y":"Thank you  for trying to help. After going through it line-by-line, I have identified the problem. And as weird as it sounds, it has nothing to do with the model. It was this line.\n<code class=\"lang-auto\">tokenizer = BertTokenizer.from_pretrained('bert-base-uncased', additional_special_tokens=['_eos', '_go'])\n<\/code>\nFor some strange reason, the BertTokenizer randomly assigns different index to _eos and _go each time the code is run. I suspect they might use their own custom random function. That was probably why the seed we set does not affect this.\nSimply change the line to this, fix my problem.\n<code class=\"lang-auto\">tokenizer = BertTokenizer.from_pretrained('bert-base-uncased')\ntokenizer.add_tokens(['_eos', '_go'])\n<\/code>","z":"Is there any hidden randomness in Transformer in the generate function?\nIf it is something hard to spot (maybe something within a library function), to perfectly replicate something I fix random seeds of torch, numpy and random.\n<code class=\"lang-auto\">torch.manual_seed(0)\nnp.random.seed(0)\nrandom.seed(0)\n<\/code>\nI did that as well. But it has no luck.\nAlso, I put everything in a loop at Line 01.\nIt even loops through the import (LOL, I\u2019ve never done that before).\ngenerate function does greedy search, so I don\u2019t think it\u2019s the randomness over that.\nEven with the random seeds fixed, does it show different outputs? And do Output #1 and Output #2 alternate each time you run the snippet?\nYeah, apparently fixing the seed does not resolve the problem. And the outputs are only alternating between #1 or #2 each time I run the code.\nIt is also curious that those are the 2 outputs you cycle over. I\u2019m a little out of ideas.\nI would probably check the output after each step, help narrow it down more. Currently, it would seem it is something to do with generate, or some state in generator\nThis is the code in the generator. It uses argmax to choose the best token at each timestep.\nNow I\u2019m starting to doubt if there is something to do with PyTorch Transformer.\n<code class=\"lang-python\">def generate(self, src, src_key_padding_mask=None):\n        ''' src has dimension of LEN x 1 '''\n        src = self.embedding(src)\n        src = self.pos_encoder(src)\n        src = self.transformer_encoder(src, src_key_padding_mask=src_key_padding_mask)\n        \n        inputs = [self.args.sos_idx]\n        for i in range(self.args.max_tgt_len):\n            tgt = torch.LongTensor([inputs]).view(-1,1).to(device)\n            tgt_mask = self.get_mask(i+1).to(device)\n            tgt = self.embedding(tgt)\n            tgt = self.pos_encoder(tgt)\n            output = self.transformer_decoder(\n                tgt=tgt, \n                memory=src, \n                tgt_mask=tgt_mask,\n                memory_key_padding_mask = src_key_padding_mask )\n            \n            output = self.linear(output)\n            output = self.softmax(output)\n            output = output[-1] # the last timestep\n            \n            values, indices = output.max(dim=-1) \n            pred_token = indices.item()\n            inputs.append(pred_token)\n\n            # end when reach eos idx\n            if pred_token == self.args.eos_idx:\n                break\n<\/code>\nThank you  for trying to help. After going through it line-by-line, I have identified the problem. And as weird as it sounds, it has nothing to do with the model. It was this line.\n<code class=\"lang-auto\">tokenizer = BertTokenizer.from_pretrained('bert-base-uncased', additional_special_tokens=['_eos', '_go'])\n<\/code>\nFor some strange reason, the BertTokenizer randomly assigns different index to _eos and _go each time the code is run. I suspect they might use their own custom random function. That was probably why the seed we set does not affect this.\nSimply change the line to this, fix my problem.\n<code class=\"lang-auto\">tokenizer = BertTokenizer.from_pretrained('bert-base-uncased')\ntokenizer.add_tokens(['_eos', '_go'])\n<\/code>"},{"x":"I am trying to apply distributed training in my project, but got a strange problem. It seems all data were loaded to one gpu, please see the figure below.\nimage802\u00d7843 104 KB with link \"https:\/\/discuss.pytorch.org\/uploads\/default\/original\/3X\/4\/c\/4c5dee23d6eebab04d4d5dd0ea9706f1e4fd3c16.png\"\nBy the way, I am using pytorch 1.5, Ubuntu 18.04.","y":"Is the intention to let each process see 3 devices? If each process only works on one device, it\u2019s better to let it see only one device, so that it won\u2019t accidentally create states on other devices?","z":"Looks like all processes create some states on cuda:5. It\u2019s possible that some part of the code is using default device. One option is to use set_device with link \"https:\/\/pytorch.org\/docs\/stable\/cuda.html#torch.cuda.set_device\" to set the current device before creating any context\/data on the device. Another option is to set CUDA_VISIBLE_DEVICES to make sure that each process can only see one device.\nBTW, could you please add a \u201cdistributed\u201d tag for distributed-training related questions? Developers working on the distributed package actively monitors that tag.\nI did set CUDA_VISIBLE_DEVICES in my project, and it seems doesn\u2019t work. And I couldn\u2019t find a way to add tags afterwards.\nHow did you set CUDA_VISIBLE_DEVICES?  Did you set it in the very beginning of every spawned subprocess?\nOne way to verify is to add the following line right before where init_process_group is called in each subprocess.\n<code class=\"lang-auto\">print(os.environ['CUDA_VISIBLE_DEVICES'])\n<\/code>\nNot the very beginning, but before mp.spawn was called. And I did what you say, the result is here (mp.spawn is called in launch.launch). I think the correct result should be one single number at each time, but how to fix it?\nimage1852\u00d7899 239 KB with link \"https:\/\/discuss.pytorch.org\/uploads\/default\/original\/3X\/d\/5\/d5d6279b103da67cdfcd5209c2d5db419a23b05e.png\"\nIs the intention to let each process see 3 devices? If each process only works on one device, it\u2019s better to let it see only one device, so that it won\u2019t accidentally create states on other devices?\nMy problem has been fixed. Thanks a lot!"},{"x":"I\u2019m converting pytorch.tensor() object to numpy array like the below code.\ntensor_data.cpu().detach().numpy()\nBut it takes approximately 0.33 seconds. Is it normal?","y":"If your data is on the GPU, you would have to transfer the data to the RAM first via .cpu() and call numpy() on it.\nNote that (as  mentioned) the numpy() call should be really cheap, as the underlying data will be shared and no copy will be involved.\nSince CUDA operations are asynchronous, the .cpu() call will create a synchronization point, so that all currently executed and queued ops on the GPU have to finish, before the tensor will be pushed to the host.\nThe data transfer might be indeed not taking the majority of the time, but other CUDA operations in the background (e.g. your model\u2019s forward\/backward pass).","z":"Hi,\nThis is most likely the tensor_data.cpu() that is slow? Not the .detach().numpy() right?\nYes tensor_data.cpu() is slowing the operation. Do you know how I can convert numpy array without converting cpu() ?\nIf your data is on the GPU, you would have to transfer the data to the RAM first via .cpu() and call numpy() on it.\nNote that (as  mentioned) the numpy() call should be really cheap, as the underlying data will be shared and no copy will be involved.\nSince CUDA operations are asynchronous, the .cpu() call will create a synchronization point, so that all currently executed and queued ops on the GPU have to finish, before the tensor will be pushed to the host.\nThe data transfer might be indeed not taking the majority of the time, but other CUDA operations in the background (e.g. your model\u2019s forward\/backward pass).\nSo does this means that if .cpu() is waiting super long for synchronization, it basically says that my GPU is not strong enough to compute all the results I need to transfer to CPU? Am I understanding it correctly?\nIf you are not synchronizing the code, the next sync point will just accumulate the runtime.\n\n\n\n Hao_Hao_Tan:\n\nit basically says that my GPU is not strong enough to compute all the results I need to transfer to CPU?\n\n\nNo, the result will be calculated and pushed to the CPU once it\u2019s finished. Since the CUDA operation is executed asynchronously, the Python script executes the next line of code right after launching the CUDA kernel. Since the calculation on the GPU will take \u201csome\u201d time, the next line of code would wait, if it\u2019s a sync point."},{"x":"Hi,\ni want to convert a numpy array into a tensor with dtype double because it gives me the following error when calling:\n<code class=\"lang-auto\">wav_stacked = torch.tensor(wav_stacked)\nfor wave in wav_stacked:\n    torchaudio.transforms.MelSpectrogram(downsample_rate)(wave.double())\nTraceback (most recent call last):\n  File \"<stdin>\", line 1, in <module>\n  File \"\/usr\/local\/lib\/python3.8\/dist-packages\/torch\/nn\/modules\/module.py\", line 722, in _call_impl\n    result = self.forward(*input, **kwargs)\n  File \"\/home\/rudolpher68005\/.local\/lib\/python3.8\/site-packages\/torchaudio\/transforms.py\", line 427, in forward\n    mel_specgram = self.mel_scale(specgram)\n  File \"\/usr\/local\/lib\/python3.8\/dist-packages\/torch\/nn\/modules\/module.py\", line 722, in _call_impl\n    result = self.forward(*input, **kwargs)\n  File \"\/home\/rudolpher68005\/.local\/lib\/python3.8\/site-packages\/torchaudio\/transforms.py\", line 260, in forward\n    mel_specgram = torch.matmul(specgram.transpose(1, 2), self.fb).transpose(1, 2)\nRuntimeError: Expected object of scalar type Double but got scalar type Float for argument #3 'mat2' in call to _th_addmm_out\n<\/code>\nHow to convert the tensor so I can use it in the MelSpectogram function?","y":"The error was a little bit missleading. He expected a float not a double","z":"The error was a little bit missleading. He expected a float not a double"},{"x":"Hello! I wanted to ask if anyone knows of an implementation of the Natasha2 algorithm (introduced in this paper by Zeyuan Allen-Zhu: https:\/\/arxiv.org\/abs\/1708.08694)\nNatasha2 uses the Oja\u2019s algorithm to avoid saddle points and converge to a local minimum for non-convex functions.","y":"\n\n\nGitHub with link \"https:\/\/github.com\/ruoshiliu\/Natasha2\"\n\n\n\nruoshiliu\/Natasha2 with link \"https:\/\/github.com\/ruoshiliu\/Natasha2\"\nImplementation of Natasha1.5 and Natasha2 algorithm in PyTorch - ruoshiliu\/Natasha2\n\n\n\n\n\n","z":"\n\n\nGitHub with link \"https:\/\/github.com\/ruoshiliu\/Natasha2\"\n\n\n\nruoshiliu\/Natasha2 with link \"https:\/\/github.com\/ruoshiliu\/Natasha2\"\nImplementation of Natasha1.5 and Natasha2 algorithm in PyTorch - ruoshiliu\/Natasha2\n\n\n\n\n\n"},{"x":"I am implementing an attention-like mechanism autoregressively, which means I will need to accumulate a tensor containing all previous GRU hidden states and do a matrix multiplication at each timestep. To accumulate this tensor, I could use .cat(), but as memory is likely to be the bottleneck during training, this seems a bit sketch.\nMy current solution (does not work) was to pre-allocate a tensor with zeros, then just assign hidden states to it at each timestep, while simultaneously using it in a matrix multiplication operation. However, this returns an error that one of the variables needed for gradient computation has been modified by an inplace operation, as expected.\nMy question is, how would one accumulate a tensor in this way in a memory-efficient way? In the paper that introduced the technique I am using (SHA-RNN), this was praised as being computationally efficient due to the way you can accumulate this tensor and not repeat computations, but I am not seeing how this is possible at the moment.","y":"Update: The SHA-RNN codebase, which claimed to be very memory and computationally efficient, just uses torch.cat. I have been decieved.\n\n\ngithub.com with link \"https:\/\/github.com\/Smerity\/sha-rnn\/blob\/master\/model.py#L209\"\n\n\nSmerity\/sha-rnn\/blob\/master\/model.py#L209 with link \"https:\/\/github.com\/Smerity\/sha-rnn\/blob\/master\/model.py#L209\"\n<code class=\"lang-py\">\n    #x = x + z.sum(dim=-2)\n\n    h = h + x if self.residual else x.float()\n\nfocus, new_mem = None, []\nif self.attn is not None:\n    mh = self.lnmem(h)\n    h = self.lnmid(h)\n\n    if mem is not None:\n        bigh = torch.cat([mem, mh], dim=0)\n    else:\n        bigh = mh\n    new_mem = bigh[-len(pe):]\n\n    q, k = h, bigh\n\n    x, focus = checkpoint(self.attn, q, k, bigh, attn_mask)\n    #x, focus = tcheckpoint(self.attn, q, k, bigh, attn_mask)\n    x = self.drop(x)\n    h = x + h\n<\/code>\n\n\n\n\n\n","z":"Update: The SHA-RNN codebase, which claimed to be very memory and computationally efficient, just uses torch.cat. I have been decieved.\n\n\ngithub.com with link \"https:\/\/github.com\/Smerity\/sha-rnn\/blob\/master\/model.py#L209\"\n\n\nSmerity\/sha-rnn\/blob\/master\/model.py#L209 with link \"https:\/\/github.com\/Smerity\/sha-rnn\/blob\/master\/model.py#L209\"\n<code class=\"lang-py\">\n    #x = x + z.sum(dim=-2)\n\n    h = h + x if self.residual else x.float()\n\nfocus, new_mem = None, []\nif self.attn is not None:\n    mh = self.lnmem(h)\n    h = self.lnmid(h)\n\n    if mem is not None:\n        bigh = torch.cat([mem, mh], dim=0)\n    else:\n        bigh = mh\n    new_mem = bigh[-len(pe):]\n\n    q, k = h, bigh\n\n    x, focus = checkpoint(self.attn, q, k, bigh, attn_mask)\n    #x, focus = tcheckpoint(self.attn, q, k, bigh, attn_mask)\n    x = self.drop(x)\n    h = x + h\n<\/code>\n\n\n\n\n\n"},{"x":"Hi\nI have made a simple feed-forward network, and I\u2019m having problems with overfitting.\nI am trying to implement dropout, however, it doesn\u2019t do quite what I would expect, so I find it likely that maybe I\u2019m using it wrong \nI have tried 3, 4 and 5 layers, learningsrates from 0.00001 to 0.001. I have tried to place dropout at all layers and in only one, and with values 0.2 and 0.4.\nSo, is the following incorrect?\nclass Feedforward(torch.nn.Module):\ndef init(self, input_size, hidden_1_size, hidden_2_size, hidden_3_size, hidden_4_size):\nsuper(Feedforward, self).init()\nself.input_size = input_size\nself.hidden_1_size  = hidden_1_size\nself.hidden_2_size  = hidden_2_size\nself.hidden_3_size  = hidden_3_size\nself.hidden_4_size  = hidden_4_size\n#self.hidden_5_size  = hidden_5_size\n        self.layer_1 = torch.nn.Linear(self.input_size, self.hidden_1_size)\n        self.dropout1 = torch.nn.Dropout(0.2)\n        self.relu_1 = torch.nn.ReLU()     \n        \n        self.layer_2 = torch.nn.Linear(self.hidden_1_size, self.hidden_2_size)\n        self.dropout2 = torch.nn.Dropout(0.2)\n        self.relu_2 = torch.nn.ReLU()\n        \n        self.layer_3 = torch.nn.Linear(self.hidden_2_size, self.hidden_3_size)\n        self.dropout3 = torch.nn.Dropout(0.2)\n        self.relu_3 = torch.nn.ReLU()          \n        \n        self.layer_4 = torch.nn.Linear(self.hidden_3_size, self.hidden_4_size)\n        self.dropout4 = torch.nn.Dropout(0.2)\n        self.relu_4 = torch.nn.ReLU() #LeakyReLU()\n        \n        #self.layer_5 = torch.nn.Linear(self.hidden_4_size, self.hidden_5_size)\n        #self.dropout5 = torch.nn.Dropout(0.2)\n        #self.relu_5 = torch.nn.ReLU() #LeakyReLU()\n        \n        self.layer_6 = torch.nn.Linear(self.hidden_4_size,1) \n        \n    def forward(self, x):\n        hidden_1 = self.layer_1(x)\n        drop_1   = self.dropout1(hidden_1)\n        relu_1 = self.relu_1(drop_1)\n        \n        \n        hidden_2 = self.layer_2(relu_1)\n        drop_2   = self.dropout2(hidden_2)\n        relu_2 = self.relu_2(drop_2)\n        \n        \n        hidden_3 = self.layer_3(relu_2)\n        drop_3   = self.dropout3(hidden_3)\n        relu_3 = self.relu_3(drop_3)\n        \n        \n        hidden_4 = self.layer_4(relu_3)\n        drop_4   = self.dropout4(hidden_4)\n        relu_4   = self.relu_4(drop_4)\n        \n        #hidden_5 = self.layer_5(relu_4)\n        #drop_5   = self.dropout5(hidden_5)\n        #relu_5   = self.relu_5(drop_5)\n        \n        output = self.layer_6(relu_4)\n        \n        return output\n\nmodel = Feedforward(x_test.shape[1] ,300,300,300,100)\nlr=0.001 #0.00001\ncriterion = torch.nn.MSELoss()\noptimizer = torch.optim.Adam(model.parameters(), lr = lr)","y":"Yes, the code looks generally alright.\nIf your model isn\u2019t training properly, I would recommend to try to overfit a small data snippet (e.g. just 10 samples) and make sure your model is able to perfectly fit this set.\nPS: you can post code snippets by wrapping them into three backticks ```, which makes debugging easier ","z":"Yes, the code looks generally alright.\nIf your model isn\u2019t training properly, I would recommend to try to overfit a small data snippet (e.g. just 10 samples) and make sure your model is able to perfectly fit this set.\nPS: you can post code snippets by wrapping them into three backticks ```, which makes debugging easier "},{"x":"I have a snippet of code that uses gradient checkpoints from torch.utils.checkpoint to reduce GPU memory:\n    if use_checkpointing:\n        res2, res3, res4, res5 = checkpoint.checkpoint(self.resnet_backbone, data['data'])\n        fpn_p2, fpn_p3, fpn_p4, fpn_p5, fpn_p6 = checkpoint.checkpoint(self.fpn, res2, res3, res4, res5)\n    else:\n        res2, res3, res4, res5 = self.resnet_backbone(data['data'])\n        fpn_p2, fpn_p3, fpn_p4, fpn_p5, fpn_p6 = self.fpn(res2, res3, res4, res5)\n\nWhen I use gradient checkpointing, after training my model, the performance is worse. Does anyone know why this might be?","y":"Hi,\nThis most likely happens because the first part of your model doesn\u2019t get gradient because of some quirks of how checkpointing works.\nCan you try making data['data'] require gradients before giving it to the checkpoint? (you can ignore the computed gradient, just add a data['data'].requires_grad_() if it is a Tensor.","z":"Hi,\nThis most likely happens because the first part of your model doesn\u2019t get gradient because of some quirks of how checkpointing works.\nCan you try making data['data'] require gradients before giving it to the checkpoint? (you can ignore the computed gradient, just add a data['data'].requires_grad_() if it is a Tensor.\nWhen I do that I appear to get a new error:\n<code class=\"lang-auto\">[4]<stderr>:    loss.backward()\n[4]<stderr>:  File \"\/pythonhome_pypi\/lib\/python3.6\/site-packages\/torch\/tensor.py\", line 195, in backward\n[4]<stderr>:    torch.autograd.backward(self, gradient, retain_graph, create_graph)\n[4]<stderr>:  File \"\/pythonhome_pypi\/lib\/python3.6\/site-packages\/torch\/autograd\/__init__.py\", line 99, in backward\n[4]<stderr>:    allow_unreachable=True)  # allow_unreachable flag\n[4]<stderr>:  File \"\/pythonhome_pypi\/lib\/python3.6\/site-packages\/torch\/autograd\/function.py\", line 77, in apply\n[4]<stderr>:    return self._forward_cls.backward(self, *args)\n[4]<stderr>:  File \"\/pythonhome_pypi\/lib\/python3.6\/site-packages\/torch\/utils\/checkpoint.py\", line 99, in backward\n[4]<stderr>:    torch.autograd.backward(outputs, args)\n[4]<stderr>:  File \"\/pythonhome_pypi\/lib\/python3.6\/site-packages\/torch\/autograd\/__init__.py\", line 99, in backward\n[4]<stderr>:    allow_unreachable=True)  # allow_unreachable flag\n[4]<stderr>:RuntimeError: element 0 of tensors does not require grad and does not have a grad_fn\n<\/code>\nTo be specific I added .requires_grad_():\n<code class=\"lang-auto\">res2, res3, res4, res5 = checkpoint.checkpoint(self.resnet_backbone, data['data'].requires_grad_())\n<\/code>\nYour loss does not requires gradients anymore because you added an extra requires_grad_()? That looks weird. Did you change anything else by any chance?\nYeah that is all I added, I just double checked and this is the case D:\nCan you double check if the outputs of the checkpoint require gradient or not? In both cases.\nI tested all cases:\n<code class=\"lang-auto\">res2, res3, res4, res5 = checkpoint.checkpoint(self.resnet_backbone, data['data'])\nprint(res2.requires_grad, res3.requires_grad, res4.requires_grad, res5.requires_grad)\nres2, res3, res4, res5 = checkpoint.checkpoint(self.resnet_backbone, data['data'].requires_grad_())\nprint(res2.requires_grad, res3.requires_grad, res4.requires_grad, res5.requires_grad)\nres2, res3, res4, res5 = self.resnet_backbone(data['data'])\nprint(res2.requires_grad, res3.requires_grad, res4.requires_grad, res5.requires_grad)\n<\/code>\nThis outputs:\n<code class=\"lang-auto\">False False False False\nTrue True True True\nFalse True True True\n<\/code>\nIn the original resnet backbone architecture of my code I have frozen res2 layer, could this be the issue?\nEdit: it appears this is the culprit as I can train when I unfreeze the res2 layer. I suppose to fix this I can just .detach() it, ie.\n<code class=\"lang-auto\">res2, res3, res4, res5 = checkpoint.checkpoint(self.resnet_backbone, data['data'].requires_grad_())\nres2 = res2.detach()\n<\/code>\nOk,\nSo adding the requires grad on the data does fix the issue that the backbone would not be learning.\nFor the res2 freezing I\u2019m not sure what you mean here.\nThe checkpoint has this behavior that it make all outputs require gradient, because it does not know which elements will actually require it yet.\nNote that in the final computation during the backward, that gradient (should) will be discarded and not used, so the frozen part should remain frozen. Even though you don\u2019t see it in the forward pass.\nAdding the detach() after the checkpoint will work as well \nYeah so in my model, I have code that detaches the gradients for res2, ie:\n<code class=\"lang-auto\">        with torch.no_grad():\n            out = self.mod1(img)\n            out = self.mod2(self.pool2(out))\n            out = self.mod3(self.pool3(out)).detach()\n        res2 = out\n<\/code>\nAnd since checkpoint has the behaviour that it makes all outputs require a gradient, when it tries to do it here I think it gets conflicted. Since in the model definition a gradient is not required but then the flag requires_grad is true. Hence it outputs the error [4]<stderr>:RuntimeError: element 0 of tensors does not require grad and does not have a grad_fn. Or so I think is the case\u2026\nHo good catch !\nAnother quirk of the checkpoint module\u2026\nSo, here indeed, it might be simpler to remove this special code and detach() outside of the checkpoint.\nNote that in this code, the detach() is not needed as the no_grad() block already prevent any graph from being created.\nHey guys \nIt seems that I have a similar problem. My model is made up of a backbone CNN (namely ResNet50) and some individual layers at the end. I added checkpoint.checkpoint_sequential() for the resnet and checkpoint.checkpoint() for the individual layers at the end. The performance of my model drops heavily.\nSo I checked requires_grad on the outputs of the checkpoints as proposed in this post and in the first epoch everything was fine, but in the second epoch it turned to False. I tried to add\n requires_grad_ ()\nwhich resolved the problem for checkpoint.checkpoint().\nHowever, for checkpoint.sequential_checkpoint it still gives False in the second epoch. The weird thing is, that during evaluation, i.e. with torch.no_grad(), requires_grad on the output of the checkpoints resolved to Ture \nIf anyone has any idea, I would be pretty thankful \nEdit: changed the checkpointing inside resnet to checkpoint.checkpoint too and not requires_grad is set to false for both after the checkpointing after the first epoch\u2026"},{"x":"I am training distilBert model for text classification. I am using the amp package to train the mixed precision version. here is the code to train the model\n<code class=\"lang-auto\">     # define the model\n      model = BertMulticlassifier(....)\n\n    # Define optimizer and scheduler\n    optimizer = Config.OPTIMIZER(model.parameters(), lr=Config.LR, bias_correction=False)\n    # model, optimizer = amp.initialize(model, optimizer, opt_level=\"O2\")\n    scheduler = Config.SCHEDULER(optimizer=optimizer,\n                                 num_warmup_steps=400,\n                                 num_training_steps=len(train_data_loader) * Config.EPOCHS\n                                 )\n\n    model.compile(optimizer=optimizer, loss=loss_func, metrics=metric, scheduler=scheduler)\n    model.fit(train_loader=train_data_loader, val_loader=val_data_loader, epochs=Config.EPOCHS)\n   # save the model\n    torch.save(self.state_dict(), model_name)\n<\/code>\nLoad the model and run it on CPU:\n<code class=\"lang-auto\">model = BertMulticlassifier(....)\nmodel.load_state_dict(torch.load('model.bin', map_location='cpu'))\nmodel.eval()\nmodel.predict()\n<\/code>\nBy comparing the inference time between the distilbert and the mixed precision distilbert version, it was more or less the same, which does not make sense. I expect that the mixed precision version take less time.\nSo, I am wondering that the amp just used the mixed precision to make the train and then it saved the model with float32 or the problem is coming from the load function ? What is the right way to  load the model in the mixed precision version. Thanks to take a look ","y":"The native amp implementation is corresponding to opt_level=\"O1\" in apex.amp, as the usability is the most user-friendly and it should support all use cases. We are experimenting with other utilities to lower the memory usage, but for now we strongly recommend to stick to the native implementation for the best support.","z":"It seems you are using a higher-level API, which provides a fit and predict function, and which might use automatic mixed precision internally.\nWhich library are you using and how did you specify to use amp?\nThanks . Basically, I implemented my own class following the keras schema, using the transformers hugging face package\nhere is the internal part of the code in fit() function for one epoch\n<code class=\"lang-auto\">losses = []\npredictions = []\n for step, batch in  enumerate(data_loader):\n            input_ids = batch[\"input_ids\"].to(self.device)\n            attention_mask = batch[\"attention_mask\"].to(self.device)\n            targets = batch[\"targets\"].to(self.device)\n            outputs = self.__call__(\n                input_ids=input_ids,\n                attention_mask=attention_mask\n            )\n            loss = self.loss_func(outputs, targets)\n           ......\n           with amp.scale_loss(loss, self.optimizer) as scaled_loss:\n                     scaled_loss.backward()\n            nn.utils.clip_grad_norm_(self.parameters(), max_norm=1.0)\n            self.optimizer.step()\n            self.scheduler.step()\n            self.optimizer.zero_grad()\nreturn np.mean(predictions), np.mean(losses)\n<\/code>\nAlos pytorch provide an other way to quantize the model.  I have used this with bertdistill trained with mixed precision. The quantized model performs as good as the float32 version!\nSo my though is that the mixed precision is used just for the training phase and the model is saved all as 32 float??\nmodel = torch.quantization.quantize_dynamic(model, {torch.nn.Linear}, dtype=torch.qint8)\n\n\n\n Rafi:\n\nSo my though is that the mixed precision is used just for the training phase and the model is saved all as 32 float??\n\n\nYes, the parameters and buffers would be stored in float32. If you don\u2019t use autocast during inference, then you would execute the standard FP32 model.\nNote that mixed precision utilities are now usable in PyTorch directly without apex, so I would recommend to update the code to torch.cuda.amp with link \"https:\/\/pytorch.org\/docs\/stable\/amp.html\".\nThanks so much . I have changed my code using autocast for torch.cuda.amp. However I remark that before with\n<code class=\"lang-auto\">model, optimizer = amp.initialize(model, optimizer, opt_level=\"O2\")\n<\/code>\nI can use a batch_size of 16, however using the autocast, I could not use more than 8 !!! It seems some operations are not casted to float16 as in apex! I think the autocast implement the level 01 of apex\nThe native amp implementation is corresponding to opt_level=\"O1\" in apex.amp, as the usability is the most user-friendly and it should support all use cases. We are experimenting with other utilities to lower the memory usage, but for now we strongly recommend to stick to the native implementation for the best support."},{"x":"considering these two nets:\n<code class=\"lang-auto\">\nclass Net(nn.Module):\n    def __init__(self):\n        super(Net, self).__init__()\n        \n        self.conv1 = nn.Conv2d(3, 6, kernel_size=1, stride=1, bias=False)\n        self.conv2 = nn.Conv2d(6, 6, kernel_size=1, stride=1, bias=False)\n        self.conv3 = nn.Conv2d(6, 6, kernel_size=1, stride=1, bias=False)\n        \n    def forward(self,input):\n        input = F.relu(self.conv1(input))\n        input2 = input\n        \n        output = F.relu(self.conv2(input))\n        output2 = F.relu(self.conv3(input2))\n        \n        return output, output2\n<\/code>\n<code class=\"lang-auto\">        \nclass Net2(nn.Module):\n    def __init__(self):\n        super(Net2, self).__init__()\n        \n        self.conv1 = nn.Conv2d(3, 6, kernel_size=1, stride=1, bias=False)\n        self.conv2 = nn.Conv2d(6, 6, kernel_size=1, stride=1, bias=False)\n        self.conv3 = nn.Conv2d(6, 6, kernel_size=1, stride=1, bias=False)\n        \n    def forward(self,input):\n        input = F.relu(self.conv1(input))\n        input2 = input.clone()\n        \n        output = F.relu(self.conv2(input))\n        output2 = F.relu(self.conv3(input2))\n        \n        return output, output2\n<\/code>\nWhat is the difference between Net and Net2? is the Net model wrong?\nWhen I use clone in Net2, if i do back-propagation (via backward on the loss between output2 and target)\nwill weights in conv1 get updated as well or it just updates the conv3 weights?","y":"Both models work identically as seen here:\n<code class=\"lang-python\">model1 = Net()\nmodel2 = Net2()\nmodel2.load_state_dict(model1.state_dict())\n\nx = torch.randn(1, 3, 24, 24)\n\noutputs1 = model1(x)\noutputs2 = model2(x)\n\n# Compare outputs\nprint((outputs1[0] == outputs2[0]).all())\nprint((outputs1[1] == outputs2[1]).all())\n\n# Compare gradients\noutputs1[0].mean().backward(retain_graph=True)\noutputs1[1].mean().backward()\noutputs2[0].mean().backward(retain_graph=True)\noutputs2[1].mean().backward()\n\nfor p1, p2 in zip(model1.parameters(), model2.parameters()):\n    print((p1.grad == p2.grad).all())\n<\/code>","z":"Both models work identically as seen here:\n<code class=\"lang-python\">model1 = Net()\nmodel2 = Net2()\nmodel2.load_state_dict(model1.state_dict())\n\nx = torch.randn(1, 3, 24, 24)\n\noutputs1 = model1(x)\noutputs2 = model2(x)\n\n# Compare outputs\nprint((outputs1[0] == outputs2[0]).all())\nprint((outputs1[1] == outputs2[1]).all())\n\n# Compare gradients\noutputs1[0].mean().backward(retain_graph=True)\noutputs1[1].mean().backward()\noutputs2[0].mean().backward(retain_graph=True)\noutputs2[1].mean().backward()\n\nfor p1, p2 in zip(model1.parameters(), model2.parameters()):\n    print((p1.grad == p2.grad).all())\n<\/code>\nso to clarify, clone will copy the data to another memory but it has no interfere with gradient back propagation. in other words, when i use clone it will back propagate till the input unless i use detach \u2026\nYes. .clone() is recognized by Autograd and the new tensor will get the grad function as grad_fn=<CloneBackward>.\nWhy retain_graph=True is used when outputs1[0].mean().backward() is called? Will this accelerate training?\nAfter calling output1[0].mean().backward(), it frees the computation graph which still need in output1[1].mean().backward(). So it needs \u2018retain_graph\u2019.\nDoes this main that there is no difference between \u201cClone()\u201d and \u201c=\u201d in terms of the gradient copying?\nI\u2019m not sure I understand your question correctly.\nIf you are referring to the original question, neither input nor input2 were manipulated after the assignment, so that both model architectures yield the same results.\nI mean: when we save a differentiable variable into another with \u201c=\u201d, will this also move the gradients and all the computation graph to the newly defined variable? Or we should use \u201cclone()\u201d for explicitly doing this?\nThe new variable will reference the old one, so that both will see the value changes and gradients:\n<code class=\"lang-python\">lin = nn.Linear(1, 1)\nw = lin.weight\n\nlin(torch.randn(1, 1)).backward()\n\nprint(lin.weight.grad)\nprint(w.grad)\nprint(id(w)==id(lin.weight))\n<\/code>\n\n\n\n ptrblck:\n\nYes. .clone() is recognized by Autograd and the new tensor will get the grad function as grad_fn=<CloneBackward> .\n\n\nI never understood this, what is the point of recording .clone() as an operation? It\u2019s extremely unintuitive to me. When I see clone I expect something like deep copy and getting a fresh new version (copy) of the old tensor. Having copying as an operation in a forward pass (like using the identity) but not calling it the identity is extremely confusion.\nAm I correct or do I not understand how .clone() works?\nI don\u2019t understand what the use case for it is really.\nclone can be used e.g. on activations, which should be passed to multiple modules, where each module might manipulate the activation in-place.\nHere is a small example:\n<code class=\"lang-python\"># Setup\nmodule1 = nn.Sequential(\n    nn.ReLU(inplace=True),\n    nn.Linear(10, 1))\n\nmodule2 = nn.Sequential(\n    nn.Linear(10, 2))\n\n\ntorch.manual_seed(2809)\nact = torch.randn(1, 10)\nprint(act)\n\n# Wrong, since act will be modified inplace\nout1 = module1(act)\nprint(act)\nout2 = module2(act)\n\n# Right\ntorch.manual_seed(2809)\nact = torch.randn(1, 10)\nprint(act)\n\n# Wrong, since act will be modified inplace\nout1 = module1(act.clone())\nprint(act)\nout2 = module2(act)\n<\/code>\nIf you don\u2019t clone the activation, the first module would apply the relu on it and the call to module2 would get the wrong tensor.\nIf act was created by previous operations (layers), Autograd will properly calculate all gradients.\nLet me see if I can paraphrase to see if I got it.\n.clone() is useful to create a copy of the original variable that doesn\u2019t forget the history of ops so to allow gradient flow and avoid errors with inlace ops. The main error of in-place ops is overwriting data needed for the backward pass (or writing an in-place op to leaf node, in this case there would be no error message).\n\n\n\n pinocchio:\n\n.clone() is useful to create a copy of the original variable that doesn\u2019t forget the history of ops so to allow gradient flow\n\n\nYes, that is correct.\nWhile my example stressed out the inplace operation, clone() might of course be useful in other use cases and specific model architectures (the inplace ops were just the first use case that came to my mind).\n\n\n\n pinocchio:\n\nor writing an in-place op to leaf node, in this case there would be no error message\n\n\nIf the inplace op is not allowed, PyTorch should raise an error and should not silently fail.\nIf you encounter such a silent error, please let us know.\nThanks for the explanation! I was wondering if you could give an example of other use cases for cloning besides the inplace operation example. I was trying to think of other cases where it may be necessary\/beneficial but couldn\u2019t think of any.\nGenerally, clone is useful whenever you are dealing with references and would like to use the current value without any potential future changes.\nE.g. if you would like to compare values pulled from a state_dict, you would have to use clone() to create the reference values. Otherwise they would be updated in the next optimizer.step() call and your initially stored values would change as well.\nThat being said, in a \u201cstandard CNN training routine\u201d you probably wouldn\u2019t need to call it.\nThank you for your detailed explanation about .clone() method.\nI face an issue during training stacked of 2 unet module. the input to the network is a batch of single image and some geometric transformation on it. Then I apply inverse of augmentation to the images (F <sup>-1<\/sup>) and compute a loss #1 function. Also, I have loss #2 at the end of the network.\n(x + F(x)) --> Unet1 -->  loss#1(y+ F<sup>-1<\/sup>(y)) --> Unet2 --> loss#2(z+ F<sup>-1<\/sup>(z))\nIn general, I got this error:\n\u201cone of the variables needed for gradient computation has been modified by an inplace operation\u201d.\nI resolved this error by utilizing y.clone() and then apply inverse geometric transformation.\n(x + F(x)) --> Unet1 -->  loss#1(y+ F<sup>-1<\/sup>(y.clone())) --> Unet2 --> loss#2(z+ F<sup>-1<\/sup>(z.clone()))\nNow, it seems that it is working well, yet I am suspicious about it. Is it Ok or not? does the gradient flow back to the Unet1 and does the gradient from Unet2 flow back to Unet1?\nNote; I use pytorch functions for some geometric transformation, but for others not. I have converted them to numpy to do that.\nBased on the error message it seems that F^(-1) might apply some inplace operations on the input tensor, which replace values needed to calculate the gradients. A .clone() operation might solve this issue.\nNevertheless, you could check, if the desired gradients are calculated by calling backward() on intermediate outputs or just the final loss and checking the .grad attributes of all parameters, which should get gradients.\nIf some of these .grad attributes are returning a None value, Autograd didn\u2019t calculate any gradiets for them and your computation graph might have been broken.\n\n\n\n SEM:\n\nI have converted them to numpy to do that.\n\n\nThat sounds risky, as this would cut the computation graph, since Autograd isn\u2019t able to backpropagate through numpy operations. You would either need to stick to PyTorch methods or implement a custom autograd.Function as described here with link \"https:\/\/pytorch.org\/tutorials\/beginner\/examples_autograd\/two_layer_net_custom_function.html\".\nI think in spite of the fact A.clone() have resolved the issue, some of the gradients may be None. In fact, the network does not train properly and the loss#2 just ringing about a certain value. Maybe the computational graph is broken and network does not actually trained.\nI will try to stick to PyTorch for geometric transformations."},{"x":"I want to use a custom activation function that has a random component that gets applied to every neuron individually.\nIf I use the standard method and call the activation function on a layer, it applies the same value to every neuron in that layer.\nI am looking for the most efficient way to have the activation function affect every neuron individually and would appreciate any advise on the topic.\nThe only method I currently know about involves splitting the input tensor and applying the function each part and then concatenating the result, but I am hoping there is a more efficient way","y":"Hi,\nI\u2019m afraid the softplus function only accepts a single beta value. But you can just re-implement it\n<code class=\"lang-python\">batch_size = x.size(0)\n# Generate one beta per sample\nbeta = torch.empty(batch_size).uniform_(self.beta_lower, self.beta_higher)\nsoftplus = ((beta * x).exp() + 1).log() \/ beta\nres = x * torch.tanh(softplus)\n<\/code>\nWould that match your needs?","z":"Hi,\nThat will depend a lot on what your function is. Can you be more precise what it is? Or provide a code sample that shows the split\/concat version?\nThe function is going to be a modified mish with a random coefficient in the exponential.\nThe split concat version I was talking about was referencing this forum post: How to apply different activation fuctions to different neurons with link \"https:\/\/discuss.pytorch.org\/t\/how-to-apply-different-activation-fuctions-to-different-neurons\/13475\"\nIf you want a modified mish, then I would implement the function using just regular element-wise functions. And then you create a Tensor with the coefficient you want and can add\/multiply it at any place your need in the computation.\nI am sorry, I should have been more clear about the activation function\n<code class=\"lang-auto\">x *( torch.tanh(F.softplus(x, beta = random.uniform(self.beta_lower, self.beta_higher))))\n<\/code>\nThis is the function I want to use. And I want to know the best way to get a different beta for each neuron.\nThanks for your patient response!\nHi,\nI\u2019m afraid the softplus function only accepts a single beta value. But you can just re-implement it\n<code class=\"lang-python\">batch_size = x.size(0)\n# Generate one beta per sample\nbeta = torch.empty(batch_size).uniform_(self.beta_lower, self.beta_higher)\nsoftplus = ((beta * x).exp() + 1).log() \/ beta\nres = x * torch.tanh(softplus)\n<\/code>\nWould that match your needs?\nI think it just might!\nThanks a lot!\nI might be able to fix this fiddling around with the shapes, but I am currently getting this error:\n<code class=\"lang-auto\">The size of tensor a (64) must match the size of tensor b (500) at non-singleton dimension 1\n<\/code>\nWhere does it happen?\nNote that I assume in the code above the x is 1D. You might need to update that.\nFor example, if x is 2D:\n\nIf you want beta to be the same for all elements for a given sample, you can simply add a beta = beta.unsqueeze(-1) to add a new dimension of size 1 in beta and the broadcasting logic will take care to expand it.\nIf you want a different beta for every element of every sample, then just update the size given to torch.empty() to reflect the full size of x.\n\n\n\n\n albanD:\n\nbeta = beta.unsqueeze(-1)\n\n\nThe unsqueezing fixed it.\nThanks a lot!\nHi , Im also trying to apply a different activation function per neuron on a simple MLP. So can you please share how you managed to separate tensors from a linear layer to apply different  activation Fs. I want my model to have seven nodes and apply different activation functions per node (first layer). I tried the select_index and the other technique mentioned but i cant do it. Im getting errors like this \" ```\nThe size of tensor a (64) must match the size of tensor b (500) at non-singleton dimension 1\n<code class=\"lang-auto\">\"\nMaybe the problems sterms from the fact that i dont how a tensor representing the layer looks like and im simply indexing it in a similar way one would index a list. Also refer me to where i can find more info about this. Thanx . sorry for the long paragraph.<\/code>"},{"x":"I\u2019m a beginner in PyTorch but I\u2019ve made a data pipeline a couple of time. The way I know to split the data is, by taking indices and separating them into train and test.:\n<code class=\"lang-auto\">data_transforms = transforms.Compose([\n    transforms.Resize((50,50)),\n    transforms.RandomRotation(degrees=30),\n    transforms.RandomHorizontalFlip(),\n    transforms.ToTensor(),\n    transforms.Normalize(mean=[0.485, 0.456, 0.406],\n                        std=[0.229, 0.224, 0.225])\n    ])\n\ndata = ImageFolder('breast-histopathology\/',transform=data_transforms)\nvalid_size = 0.15\ntest_size = 0.15\n\nnum_train = len(data)\nindices = list(range(num_train))\nnp.random.shuffle(indices)\nvalid_split = int(np.floor((valid_size) * num_train))\ntest_split = int(np.floor((valid_size+test_size) * num_train))\nvalid_idx, test_idx, train_idx = indices[:valid_split], indices[valid_split:test_split], indices[test_split:]\n\nprint(len(valid_idx), len(test_idx), len(train_idx))\n\n# define samplers for obtaining training and validation batches\ntrain_sampler = SubsetRandomSampler(train_idx)\nvalid_sampler = SubsetRandomSampler(valid_idx)\ntest_sampler = SubsetRandomSampler(test_idx)\n\n\nloaders = {\n    'train': torch.utils.data.DataLoader(data, batch_size=128, sampler=train_sampler),\n    'test': torch.utils.data.DataLoader(data, batch_size=32, sampler=test_sampler),\n    'valid': torch.utils.data.DataLoader(data, batch_size=32, sampler=valid_sampler),\n}\n<\/code>\nHowever I don\u2019t know how to do it incase I want separate transforms. This method provides one data transform for the whole dataset. Is there a way to divide dataset and specify separate transforms for each subset(eg. augmented data for train and original for validation).\nP.S it can be done by making separate train and test folders using shutil or os but I was thinking if there\u2019s a method in pytorch for doing so.","y":"For best practice:\n\nI would advise to separate train\/val\/test earlier, possibly even at the file system level.\nI know nothing about your dataset, but you have checked that just splitting at a image level is the right thing to do (in medical imaging, you typically want to have all images of a given patient on the same side of the split and the like)?\nAfter the above two, it is natural to have separate datasets for train\/val\/test. At that point, having different transforms is easy. \n\n\nBest regards\nThomas","z":"For best practice:\n\nI would advise to separate train\/val\/test earlier, possibly even at the file system level.\nI know nothing about your dataset, but you have checked that just splitting at a image level is the right thing to do (in medical imaging, you typically want to have all images of a given patient on the same side of the split and the like)?\nAfter the above two, it is natural to have separate datasets for train\/val\/test. At that point, having different transforms is easy. \n\n\nBest regards\nThomas"},{"x":"Hi everyone, I am a bit of a newbie in PyTorch and I have a very basic issue that I am having trouble with.\nI have two matrices A and B, with different number of rows, but same number of columns.\nBasically, A and B are different collections of same-sized vectors.\nWhat I am trying to do is to subtract each vector in B from each vector in A.\nThis minimal example does exactly what I\u2019m trying to accomplish:\n<code class=\"lang-python\">a = torch.tensor([[1, 2, 3], [4, 5, 6], [7, 8, 9]])\nb = torch.tensor([[1, 1, 1], [2, 2, 2]])\n\nc = torch.stack([a-x for x in b])\n<\/code>\nbut of course I would like to get rid of that ugly \u201cfor\u201d cycle, which I guess is not good for performances.\nI think I need some sort of torch.diff(a, b, axis=1).\nDoes something like that exist?","y":"Broadcasting should work, but would trade memory for a potential speedup:\n<code class=\"lang-python\">c = a.unsqueeze(0) - b.unsqueeze(1)\n<\/code>","z":"Broadcasting should work, but would trade memory for a potential speedup:\n<code class=\"lang-python\">c = a.unsqueeze(0) - b.unsqueeze(1)\n<\/code>"},{"x":"i have four image dataset. full image, face image, face-mask image, landmarks image\nin develope vae, my goal is encode full image and reconstruct image is each face, face-mask, landmarks image\nbut when i load dataset using custom dataset and dataloader, each dataset shuffled but not corresponding image\nis any way to get same shuffled order for multi dataset?\n<code class=\"lang-auto\">class ImageDataset(Dataset):\n    def __init__(self, paths, is_aug=False):\n        super(ImageDataset, self).__init__()\n\n        # Length\n        self.length = len(paths)\n        # Image path\n        self.paths = paths\n        # Augment\n        self.is_aug = is_aug\n        self.transform = transforms.Compose([\n            transforms.ColorJitter(brightness=0.2, contrast=0.2, saturation=0.2, hue=0.1),\n            ImgAugTransform(),\n            lambda x: Image.fromarray(x),\n        ])\n        # Preprocess\n        self.output = transforms.Compose([\n            \n            transforms.ToTensor(),\n            ])\n\n    def __len__(self):\n        return self.length\n\n    def __getitem__(self, idx):\n        # Image\n        img = Image.open(self.paths[idx])\n        # Augment\n        if self.is_aug:\n            img = self.transform(img)\n        # Preprocess\n        img = self.output(img)\n\n        return img\n<\/code>\n<code class=\"lang-auto\">def get_celeba_loaders(batch_train, batch_test, path, total_size):\n    test_num = 128\n    images = glob.glob(os.path.join(\".\", \"ImageFolder\",path, \"*.jpg\"))\n    print(len(images))\n    datasets = {\n        \"train\": ImageDataset(images[test_num:total_size], True),\n        \"test\": ImageDataset(images[:test_num], False)\n    }\n    dataloaders = {\n        \"train\": DataLoader(datasets[\"train\"], batch_size=batch_train, shuffle=True),\n        \"test\": DataLoader(datasets[\"test\"], batch_size=batch_test, shuffle=False)\n    }\n\n    return dataloaders\n<\/code>\n<code class=\"lang-auto\">d1 = ud.get_celeba_loaders(args.batch_train, args.batch_test, 'Original', 100000)\nd2 = ud.get_celeba_loaders(args.batch_train, args.batch_test, 'face_part', 100000)\n\nfor i, x in enumerate(zip(d1['train'], d2['train'])):\n    origin = x[0].to(device)\n    rec_l = x[1].to(device)\n    imsave(origin, rec_l, os.path.join('.', f\"epoch\", f\"lmtrain.png\"), 8, 8)\n    break\n<\/code>\nthis is my code for dataset, it doesn\u2019t correspond after shuffled\nhow to get same order with shuffle","y":"If you are trying to sample data from multiple datasets, I would recommend to wrap all these unshuffled dataset in a custom Dataset and shuffle this \u201cwrapper\u201d dataset:\n<code class=\"lang-python\">class MyDataset(Dataset):\n    def __init__(self, datasetA, datasetB):\n        self.datasetA = datasetA\n        self.datasetB = datasetB\n        \n    def __getitem__(self, index):\n        xA = self.datasetA[index]\n        xB = self.datasetB[index]\n        return xA, xB\n    \n    def __len__(self):\n        return len(self.datasetA)\n    \ndatasetA = ...\ndatasetB = ...\ndataset = MyDataset(datasetA, datasetB)\nloader = DataLoader(dataset, batch_size=10, shuffle=True)\n<\/code>\nThis would make sure to shuffle the indices for MyDataset, which would apply the same index to each internal dataset.","z":"If you are trying to sample data from multiple datasets, I would recommend to wrap all these unshuffled dataset in a custom Dataset and shuffle this \u201cwrapper\u201d dataset:\n<code class=\"lang-python\">class MyDataset(Dataset):\n    def __init__(self, datasetA, datasetB):\n        self.datasetA = datasetA\n        self.datasetB = datasetB\n        \n    def __getitem__(self, index):\n        xA = self.datasetA[index]\n        xB = self.datasetB[index]\n        return xA, xB\n    \n    def __len__(self):\n        return len(self.datasetA)\n    \ndatasetA = ...\ndatasetB = ...\ndataset = MyDataset(datasetA, datasetB)\nloader = DataLoader(dataset, batch_size=10, shuffle=True)\n<\/code>\nThis would make sure to shuffle the indices for MyDataset, which would apply the same index to each internal dataset."},{"x":"i have a datalist[0:59]. i used k-folder cross validation, split the data:\nimage932\u00d7456 17.1 KB with link \"https:\/\/discuss.pytorch.org\/uploads\/default\/original\/3X\/2\/a\/2aaac8fbd19d6a3b51ac0c0af3b08e0629fbe526.png\" .\nBut when i try to return the first split for my data, i get error\nimage850\u00d7225 8.12 KB with link \"https:\/\/discuss.pytorch.org\/uploads\/default\/original\/3X\/1\/5\/15ce0f044085556be627bf13a2f6a2eb439903fd.png\" .\nHow can i get the data_list from my tensor","y":"solved by:  train_dataset =  [data_list[index] for index in train_idx]","z":"Of yhat types are data_list and train_indices? The first posted list(k) doesn\u2019t seem to be used in the second code snippet.\nPS: you can post code snippets by wrapping them into three backticks ```, which would make debugging easier. \nsolved by:  train_dataset =  [data_list[index] for index in train_idx]"},{"x":"I have two tensors, one is shaped (1,1,100) and another (1,1,400)\ninside forward() method of my model they get .cat() together, but for some reason it throws this error:\nSizes of tensors must match except in dimension 2. Got 400 and 100 (The offending index is 0)\nWhat is the reason???\nCode:\n<code class=\"lang-auto\">def forward(self, cond, hidden,state):\n    '''\n    Cond shape: (batch,1,100)\n\n    hidden and state shape: (batch,1,400)\n    '''\n    cond = torch.cat((cond,hidden),1) #Error is here\n    . . .\n<\/code>","y":"Fixed itself, change 1 to 2 in torch.cat()","z":"Fixed itself, change 1 to 2 in torch.cat()"},{"x":"My code is\u2026\n<code class=\"lang-auto\">batch_size = 16\n\ntransform = transforms.Compose([transforms.Resize((299,299))\n                                       ,transforms.ToTensor()\n                                       ,transforms.Normalize([0.485, 0.456, 0.406], [0.229, 0.224, 0.225])])\n\ndataset = ImageFolder('.data\/',transform=transform)\n\nkf = KFold(n_splits=5, shuffle=True)\n\nfor i, (train_index, test_index) in enumerate(kf.split(dataset)):\n\n    trainloader = torch.utils.data.DataLoader(train_index, batch_size=batch_size, shuffle=True, num_workers=0, pin_memory=False)\n    testloader = torch.utils.data.DataLoader(test_index, batch_size=batch_size, shuffle=True, num_workers=0, pin_memory=False)\n    \n    print('Fold : {}, train : {}, test : {}'.format(i+1, len(trainloader.dataset), len(testloader.dataset)))\n    \n    for batch_idx, (data, target) in enumerate(trainloader):\n        print('Train Batch idx : {}, data shape : {}, target shape : {}'.format(batch_idx, data.shape, target.shape))\n<\/code>\nerror occurred.\n<code class=\"lang-auto\">\nFold : 1, train : 579, test : 145\n\n---------------------------------------------------------------------------\nValueError                                Traceback (most recent call last)\n<ipython-input-6-0fe2bfb82b09> in <module>\n     16     print('Fold : {}, len train : {}, len test : {}'.format(i+1, len(trainloader.dataset), len(testloader.dataset)))\n     17 \n---> 18     for batch_idx, (data, target) in enumerate(trainloader):\n     19         print('Train Batch idx : {}, data shape : {}, target shape : {}'.format(batch_idx, data.shape, target.shape))\n\nValueError: too many values to unpack (expected 2)\n<\/code>\nI don\u2019t know how to handle train_index, test_index.\nCould anyone give me a help ?","y":"kf.split will return the train and test indices as far as I know.\nCurrently you are passing these indices to a DataLoader, which will just return a batch of indices.\nI think you should pass the train and test indices to a Subset with link \"https:\/\/pytorch.org\/docs\/stable\/data.html#torch.utils.data.Subset\" to create new Datasets and pass these to the DataLoaders.\nLet me know, if that works for you.","z":"kf.split will return the train and test indices as far as I know.\nCurrently you are passing these indices to a DataLoader, which will just return a batch of indices.\nI think you should pass the train and test indices to a Subset with link \"https:\/\/pytorch.org\/docs\/stable\/data.html#torch.utils.data.Subset\" to create new Datasets and pass these to the DataLoaders.\nLet me know, if that works for you.\nThe code I modified is as follows:\n<code class=\"lang-auto\">for i, (train_index, test_index) in enumerate(kf.split(dataset)):\n    \n    train = torch.utils.data.Subset(dataset, train_index)\n    test = torch.utils.data.Subset(dataset, test_index)\n\n    trainloader = torch.utils.data.DataLoader(train, batch_size=batch_size, shuffle=True, num_workers=0, pin_memory=False)\n    testloader = torch.utils.data.DataLoader(test, batch_size=batch_size, shuffle=True, num_workers=0, pin_memory=False)\n<\/code>\nThank you for reply. It\u2019s works well!\nLabel has mismatched with pic.\nI have \u20180\u2019, \u20181\u2019 in label, but output is always \u20180\u2019.\nI\u2019d did something miss in the code\u2026\nCould you help me one more time?\nIs your model only predicting class0?\nIf so, could you create a new thread and tag me in it?\nAlso, could you provide some information regarding your model, training, class distribution etc. in the new thread?\nI\u2019m not yet ran all of code.\nI ran just \u2018imshow\u2019\n<code class=\"lang-auto\">def imshow(inp, img_num):\n    inp = inp.numpy().transpose((1, 2, 0))\n    mean = np.array([0.485, 0.456, 0.406])\n    std = np.array([0.229, 0.224, 0.225])\n    inp = std * inp + mean\n    inp = np.clip(inp, 0, 1)\n    print('label : {}'.format(train[img_num][1]))\n    plt.imshow(inp)\n\nimshow(train[40][0], 40)\n<\/code>\nBut output shows mismatched label with pic\nThe index you are passing to your Dataset doesn\u2019t correspond to the target for the sample.\nI\u2019m not sure, if I understand your code correctly, but you can get the target using target = train[40][1].\nI get the target by\nprint('label : {}'.format(train[img_num][1]))\nbut as you mentioned, the target doesn\u2019t correspond to sample.\nhow could I modify the code that the target correspond to the sample?\nImageFolder will create the labels based on the passed folders, so the labels should also mismatch in the original Dataset before the splitting. Could you check that and see if some images might be stored in the wrong folder?\n<code class=\"lang-auto\">data\/\n                melanoma\/\n                    AM(1).jpg\n                    AM(2).jpg\n                    AM(3).jpg\n                    ...\n                benign\/\n                    BN(1).jpg\n                    BN(2).jpg\n                    BN(3).jpg\n                    ....\n<\/code>\nall images are stored in the right folder\u2026\nAnd\n<code class=\"lang-auto\">print(dataset.class_to_idx)\n<\/code>\n<code class=\"lang-auto\">{'benign': 0, 'melanoma': 1}\n<\/code>\nI need to think about what the problem is.\nIf your original Dataset is fine, the Subsets shouldn\u2019t be changed, since only the passed indices are called in this line of code with link \"https:\/\/github.com\/pytorch\/pytorch\/blob\/0d366e1bde42265d764a6a9aad27a38753156fb0\/torch\/utils\/data\/dataset.py#L103\".\nCould you check which index in your subset is giving the wrong class and then try to get the original index based on this information? Maybe this helps debugging which file seems to be wrong.\nAlternatively, you could write your own custom Dataset and return the image names along the data and target, so that debugging might be a bit easier.\nOh\u2026 your first reply was worked well\u2026\nI had print all classes index and that showed class \u20181\u2019  were gatherd end of index in Dataset.\nI\u2019m sorry for waste your time.\nThanks a lot of your detailed reply!\nHi , \nif we want to use StratifiedKFold for ImageFolder, then we have to pass the labels to split as well.\nso it will be something like this :  kf.split(dataset,y)\nbut for y, how can we assign labels from imagefolder?\nImageFolder contains the labels in its .target attribute with link \"https:\/\/github.com\/pytorch\/vision\/blob\/be8192e20d2529fa552bcfc099974da45365ffd6\/torchvision\/datasets\/folder.py#L122\"."},{"x":"Hi,\nI am trying to a do a calculation of the size occupied by the feature maps in a DNN model. For things like ReLU we can use nn.Functional or define ReLU as a layer in the init function. My question is in the nn.Functional case would we be considering the ReLU as a layer and using the inputs\/outputs from it to calculate feature map size. When I define it as a layer in init, it does make sense to do that. But I am not sure about that when using ReLU from nn.Functional considering its not really a layer that i have defined. Can someone please clarify?. Thanks","y":"Memory allocations are linked with operations on tensors, layers just wrap groups of these operations together. So, it is kinda hard to measure memory, as layer code may create intermediate tensors, some will be freed early, some kept for backpropagation. Check pytorch_memlab with link \"https:\/\/github.com\/Stonesjtu\/pytorch_memlab\" if you need a detailed profile.\nReLU allocates new memory for output by default, and it is usually not released, as next layer (linear\/conv) keeps it for gradient calculations.","z":"Memory allocations are linked with operations on tensors, layers just wrap groups of these operations together. So, it is kinda hard to measure memory, as layer code may create intermediate tensors, some will be freed early, some kept for backpropagation. Check pytorch_memlab with link \"https:\/\/github.com\/Stonesjtu\/pytorch_memlab\" if you need a detailed profile.\nReLU allocates new memory for output by default, and it is usually not released, as next layer (linear\/conv) keeps it for gradient calculations."},{"x":"Hello,\nI\u2019m training one model on two different sources of data.\nMy training loop is essentially\nfor \u2026\ntrain one batch from source A\ntrain one batch from source B\nNote that the code to train from the two sources isnt exactly the same, but the model is.\nWhen I initialize both DataLoaders with num_workers, I get the following execption:\nRuntimeError: Cannot re-initialize CUDA in forked subprocess. To use CUDA with multiprocessing, you must use the \u2018spawn\u2019 start method\nUsing the spawn method causes other side effects, so I would prefer to stay in this mode.\nIt there a way to get this to work?","y":"You can\u2019t use CUDA with fork. But you can set the ctx for just one data loader by specifying multiprocessing_context='spawn' when creating the data loader.","z":"The problem was different - one of the data loader uses cuda internally (because the data goes through a cleaning network before being returned). This was the problem. Is there a way to a void \u2018spawn\u2019 method in this case?\nYou can\u2019t use CUDA with fork. But you can set the ctx for just one data loader by specifying multiprocessing_context='spawn' when creating the data loader.\nThanks! Didn\u2019t know about the per-loader context flag. Much cleaner solution."},{"x":"Greetings, I am relatively new to pytorch (also not very familiar with manipulating tensors) and I am trying to implement a function with the same behavior as numpy.diff (https:\/\/docs.scipy.org\/doc\/numpy-1.15.0\/reference\/generated\/numpy.diff.html). I would like that this function works only with torch Tensors. After looking at the documentation, it doesn\u2019t seems like pytorch as a function that have the same behavior\u2026 Is there an equivalent function that I missed ? if None exist, how would your implement such function ?\nUltimately, I am trying to implement a loss function weighted by Mean Directional Accuracy (MDA)(https:\/\/en.wikipedia.org\/wiki\/Mean_Directional_Accuracy).\nFor example, the numpy version of it is as follows:\n<code class=\"lang-auto\">def diff(a, n=1, axis=-1):\n    if n == 0:\n        return a\n    if n < 0:\n        raise ValueError(\n            \"order must be non-negative but got \" + repr(n))\n\n    a = asanyarray(a)\n    nd = a.ndim\n    axis = normalize_axis_index(axis, nd)\n\n    slice1 = [slice(None)] * nd\n    slice2 = [slice(None)] * nd\n    slice1[axis] = slice(1, None)\n    slice2[axis] = slice(None, -1)\n    slice1 = tuple(slice1)\n    slice2 = tuple(slice2)\n\n    op = not_equal if a.dtype == np.bool_ else subtract\n    for _ in range(n):\n        a = op(a[slice1], a[slice2])\n\n    return a\n\n>>> x = np.array([1, 2, 4, 7, 0])\n>>> np.diff(x)\n#array([ 1,  2,  3, -7])\n<\/code>","y":"I think indexing should just work:\n<code class=\"lang-python\">x = torch.tensor([1, 2, 4, 7, 0])\nx_diff = x[1:] - x[:-1]\nprint(x_diff)\n> tensor([ 1,  2,  3, -7])\n<\/code>","z":"I think indexing should just work:\n<code class=\"lang-python\">x = torch.tensor([1, 2, 4, 7, 0])\nx_diff = x[1:] - x[:-1]\nprint(x_diff)\n> tensor([ 1,  2,  3, -7])\n<\/code>\nHow to get the same shape output?\nBelow a solution to keep the same shape when computing first differences across the last dimension by padding zeros at the front.\n<code class=\"lang-auto\">import torch.nn.functional as F\nt = torch.rand((10, 10, 10))\ndiff = t - F.pad(t, (1, 0))[:, :, :-1]\n<\/code>\nNote 1: you can also create a Conv layer with a 2 sized kernel with fixed weights [-1, 1], but that comes down to the same thing.\nNote 2: Don\u2019t know if this is optimal in terms of speed, I know it\u2019s faster than using a concatenation. Happy to learn when someone has a faster option \nHi Ptrblck\nSorry to take your time. I am using Matlab to find the coordinate so f specific value in 3 Diment by \"\n[XX,YY,ZZ]=ind2sub(size(SWITHresholedOutPut),find(SWITHresholedOutPut==1));\nI want to do same thing in pytorch, means find the coordinate of specific value in 3 dimension.\nWhat is the best option?\nWould this work for you?\n<code class=\"lang-python\">x = torch.arange(4*5*6).view(4, 5, 6)\nidx = (x == 82).nonzero()\n<\/code>\nHi Ptrblck\nI want to compute instantaneous frequency (IF) from hilbert transform using pytorch.\nCurrently, I am using Matlab for this task.\nMatlab script:\n<code class=\"lang-python\">[x,fs]=audioread('waveform');\n\nfor i = 1:length(sinc_filter)\n    C(i,:) = conv(x,sinc{i},'same');\n    HT(i,:)=(hilbert(C(i,:)));\n    instfreq (i,:)= abs(fs\/(2*pi)*diff(unwrap(angle(HT(i,:)))));\nend\n\n<\/code>\nI want to do the same thing in pytorch.\nI didn\u2019t find hilbert and unwrape function in pytorch.\nI tried using numpy but that\u2019s taking lot of time for computing instantaneous frequency for each mini-batch.\npython script using numpy:\n<code class=\"lang-auto\">#no. of sinc filters=21\n#filter_length=128\n#mini-batch size=8\n#self.filters size is [1,no. of sinc filters, filter_length]\n# Filter is sinc filters with size [batch_size, no. of sinc filter, audio samples(64000 samples)]. \n\nFilter=F.conv1d(waveforms, self.filters, stride=self.stride,\n                        padding=self.padding, dilation=self.dilation,\n                         bias=None, groups=1) \n\nself.Filter=Filter\n\nfs=16000\n\nFILTER_1=self.Filter.cpu().detach().numpy()  \n\ninst_freq=np.zeros((self.Filter.shape[0],self.Filter.shape[1],Filter.shape[2]-1))  # intialized inst_freq matrix\n\n# 1st for loop to compute IF for each filter and 2nd for loop is for each audio wave in mini-batch.\n# j range from 0 to 20.\n# i range from 0 to 7.\n\n\nfor j in range(self.Filter.shape[1]):\n    for i in range(self.Filter.shape[0]):\n        z= hilbert(FILTER_1[i,j,:])\n        inst_phase = np.unwrap(np.angle(z))\n        inst_freq[i,j,:] = np.diff(inst_phase)\/(2*np.pi)*fs \n                \n  \nMain_filt=torch.from_numpy(inst_freq).type(torch.float).to(self.device)\n\n<\/code>\nBut its taking lot of time for IF computation.\nIs there any way to do this things in pytorch.\nPlease let me know what is the best option to do this operations for IF computation in pytorch.\nyour suggestions would be really useful for me.\nThanks in advance.\nBased on the code it seems you would need to port the hilbert transformation and unwrap to PyTorch to avoid using scipy and numpy on the CPU.\nI had a quick look at the source code for hilbert with link \"https:\/\/github.com\/scipy\/scipy\/blob\/v1.5.2\/scipy\/signal\/signaltools.py#L2036-L2144\" and unwrap with link \"https:\/\/github.com\/numpy\/numpy\/blob\/v1.19.0\/numpy\/lib\/function_base.py#L1487-L1541\" and I think all needed operations are implemented in PyTorch, so that you could try to port these methods directly."},{"x":"If i have a tensor of shape (batch_size,channels,features), how do i separate it into a list of tensors of shape (batch_size,1,features) with length of channels?","y":"Answer found: use torch.split(tensor,1,channel_dim_index)|\nSo if tensor is of shape (3,100,250), use torch.split(tensor,1,1) to get a tuple of 100 tensor of shape (3,1,250)","z":"Answer found: use torch.split(tensor,1,channel_dim_index)|\nSo if tensor is of shape (3,100,250), use torch.split(tensor,1,1) to get a tuple of 100 tensor of shape (3,1,250)"},{"x":"Hi,\nI\u2019d like to log gradients obtained during training to a file to analyze\/replicate the training later.\nWhat\u2019s a convenient way of doing this in PyTorch ?","y":"If you use tensorboardX you could also log the grades to tensorboard to visualise:\n<code class=\"lang-auto\">from tensorboardX import SummaryWriter\nlogger = SummaryWriter(LOG_DIR)\n    \ndef log_gradients_in_model(model, logger, step):\n    for tag, value in model.named_parameters():\n        if value.grad is not None:\n            self.logger.add_histogram(tag + \"\/grad\", value.grad.cpu(), step)\n<\/code>","z":"<code class=\"lang-python\">grads = {n:p.grad.cpu() for n, p in model.named_parameters()}\n<\/code>\ngives you the grads of model's parameters. You can now store them away, either directly on disk (torch.save or, if you feel fancy, hdf5) or keep a list of them (when moving to cpu probably is a good idea, so I threw that in above) or so.\nBest regards\nThomas\nIf you use tensorboardX you could also log the grades to tensorboard to visualise:\n<code class=\"lang-auto\">from tensorboardX import SummaryWriter\nlogger = SummaryWriter(LOG_DIR)\n    \ndef log_gradients_in_model(model, logger, step):\n    for tag, value in model.named_parameters():\n        if value.grad is not None:\n            self.logger.add_histogram(tag + \"\/grad\", value.grad.cpu(), step)\n<\/code>"},{"x":"I have an imbalanced dataset with the items that I want to sample by which are not labels, but other features of the data.  I would like to keep one of the classes at 50% with the other classes (5) divided between the remaining 50% so 10% chance of being chosen per class.  The number of classes in the dataset (c) is:\n<code class=\"lang-auto\">Counter({'-1': 7557, '0': 3958, '2': 1306, '3': 1144, '4': 861, '1': 323})\n<\/code>\nwith the -1 the one that I want to sample with 50% probability.  I made a weighted random sampler to give me equal oversampling like this:\n<code class=\"lang-auto\">weight = {d : 1. \/ c[d] for d in c}\nsamples_weight = np.array([[weight[str(item[4])] for item in trainDS]])\nsampler = WeightedRandomSampler(sw, len(sw), replacement=True)\n<\/code>\nwhich seems to give me equal proportions of each one at approximately 20%.  I\u2019m having trouble understanding what my weight variable should be in order to get a 50\/10\/10\/10\/10\/10 split.  In the docs with link \"https:\/\/pytorch.org\/docs\/stable\/data.html#torch.utils.data.WeightedRandomSampler\" it just says that weights should be \u201ca sequence of weights, not necessary summing up to one\u201d, which isn\u2019t super helpful on what it actually should be.\nHow should I adjust it to get that split?","y":"Ok, I think that I figured it out with some inspiration from  .  If I take the balanced weightedrandomsampler weights and then multiply those by whatever proportions I want it works fine.  I don\u2019t know if there is a way to do it without \u201cbalancing\u201d them first.  For example:\n<code class=\"lang-auto\">numDataPoints = 1000\ndata_dim = 5\nbs = 100\n\n# Create dummy data with class imbalance 9 to 1\ndata = torch.FloatTensor(numDataPoints, data_dim)\ntarget = np.hstack((np.zeros(int(numDataPoints * 0.9), dtype=np.int32),\n                    np.ones(int(numDataPoints * 0.1), dtype=np.int32)))\n\nprint('target train 0\/1: {}\/{}'.format(len(np.where(target == 0)[0]), len(np.where(target == 1)[0])))\n\nclass_sample_count = np.array(\n    [len(np.where(target == t)[0]) for t in np.unique(target)])\nweight = 1. \/ class_sample_count\nweight = weight * [.3,.7]\nprint(weight)\n\nsamples_weight = np.array([weight[t] for t in target])\n\nsamples_weight = torch.from_numpy(samples_weight)\nsamples_weigth = samples_weight.double()\nsampler = WeightedRandomSampler(samples_weight, len(samples_weight))\n\ntarget = torch.from_numpy(target).long()\ntrain_dataset = torch.utils.data.TensorDataset(data, target)\n\ntrain_loader = DataLoader(\n    train_dataset, batch_size=bs, num_workers=1, sampler=sampler)\n\nfor i, (data, target) in enumerate(train_loader):\n  print(\"batch index {}, 0\/1: {}\/{}\".format(i, len(np.where(target.numpy() == 0)[0]), len(np.where(target.numpy() == 1)[0])))\n<\/code>\nThis outputs:\n<code class=\"lang-auto\">target train 0\/1: 900\/100\n[0.00033333 0.007     ]\nbatch index 0, 0\/1: 29\/71\nbatch index 1, 0\/1: 31\/69\nbatch index 2, 0\/1: 20\/80\nbatch index 3, 0\/1: 39\/61\nbatch index 4, 0\/1: 34\/66\nbatch index 5, 0\/1: 34\/66\nbatch index 6, 0\/1: 33\/67\nbatch index 7, 0\/1: 31\/69\nbatch index 8, 0\/1: 34\/66\nbatch index 9, 0\/1: 34\/66\n<\/code>\nI tried multiplying the desired class weight by 5 in my case (and implicitly multiplying the other classes by 1) and it seems to have the effect of making that class get chosen with a 50% probability.","z":"If I do:\nweight['-1'] *= 5\nIt seems to get to be approximately 50% but I\u2019m still not sure how to figure out what the scaling factor should be analytically.\nI would just assume that your weights should sum up to 1.0, that\u2019ll just make things easier. It\u2019s true that the weighted sampler doesn\u2019t need this requirement, but I\u2019m guessing it just converts the weights to probabilities behind the scene anyways.\nI would set your weight to this: torch.tensor([0.5, 0.1, 0.1, 0.1, 0.1, 0.1]). This should work just fine. Something like torch.tensor([50, 10, 10, 10, 10, 10]) is probably the same behind the scenes.\nOk, I\u2019ll try that out.  I assumed that it somehow was not using probabilities but what you are saying makes a lot of sense, essentially a sort of softmax.\nSetting weights to be weight = {'-1': 0.5, '0': 0.1, '3': 0.1, '1': 0.1, '2': 0.1, '4': 0.1} did not work.  The first class gets to be around 80% that way.\nAre you sure? This is the test I\u2019m performing:\n<code class=\"lang-auto\">\nfrom torch.utils.data import WeightedRandomSampler\n\nn = 100000\nw = [0.5, 0.1, 0.1, 0.1, 0.1, 0.1]\nl = list(WeightedRandomSampler(w, n))\n\nfor i in range(6):\n    print(f'{i} - {100.0 * l.count(i) \/ n}%')\n<\/code>\nThis is my output:\n<code class=\"lang-auto\">0 - 50.038%\n1 - 9.967%\n2 - 9.922%\n3 - 10.028%\n4 - 9.9%\n5 - 10.145%\n<\/code>\nIt might be the way you\u2019re computing the weights from that dictionary you have.\nHmm, yeah I\u2019ll take a look a look at how I\u2019m loading it and see if I can figure out where it\u2019s going wrong.\nI modified some code from  at https:\/\/discuss.pytorch.org\/t\/how-to-handle-imbalanced-classes\/11264  and when I just change it the weights to [.3 .7] just as an example it does not work unless I\u2019m misunderstanding you.  Here\u2019s what I did:\n<code class=\"lang-auto\">numDataPoints = 1000\ndata_dim = 5\nbs = 100\n\n# Create dummy data with class imbalance 9 to 1\ndata = torch.FloatTensor(numDataPoints, data_dim)\ntarget = np.hstack((np.zeros(int(numDataPoints * 0.9), dtype=np.int32),\n                    np.ones(int(numDataPoints * 0.1), dtype=np.int32)))\n\nprint('target train 0\/1: {}\/{}'.format(len(np.where(target == 0)[0]), len(np.where(target == 1)[0])))\n\nclass_sample_count = np.array(\n    [len(np.where(target == t)[0]) for t in np.unique(target)])\nweight = 1. \/ class_sample_count\nweight[0] = .3\nweight[1] = .7\nprint(weight)\n\nsamples_weight = np.array([weight[t] for t in target])\n\nsamples_weight = torch.from_numpy(samples_weight)\nsamples_weigth = samples_weight.double()\nsampler = WeightedRandomSampler(samples_weight, len(samples_weight))\n\ntarget = torch.from_numpy(target).long()\ntrain_dataset = torch.utils.data.TensorDataset(data, target)\n\ntrain_loader = DataLoader(\n    train_dataset, batch_size=bs, num_workers=1, sampler=sampler)\n\nfor i, (data, target) in enumerate(train_loader):\n  print(\"batch index {}, 0\/1: {}\/{}\".format(i, len(np.where(target.numpy() == 0)[0]), len(np.where(target.numpy() == 1)[0])))\n<\/code>\nand the output was:\n<code class=\"lang-auto\">target train 0\/1: 900\/100\n[0.3 0.7]\nbatch index 0, 0\/1: 82\/18\nbatch index 1, 0\/1: 72\/28\nbatch index 2, 0\/1: 71\/29\nbatch index 3, 0\/1: 74\/26\nbatch index 4, 0\/1: 82\/18\nbatch index 5, 0\/1: 74\/26\nbatch index 6, 0\/1: 81\/19\nbatch index 7, 0\/1: 83\/17\nbatch index 8, 0\/1: 82\/18\nbatch index 9, 0\/1: 78\/22\n<\/code>\nAre you sure your method will work if the dataset is imbalanced to start off with?\nOk, I think that I figured it out with some inspiration from  .  If I take the balanced weightedrandomsampler weights and then multiply those by whatever proportions I want it works fine.  I don\u2019t know if there is a way to do it without \u201cbalancing\u201d them first.  For example:\n<code class=\"lang-auto\">numDataPoints = 1000\ndata_dim = 5\nbs = 100\n\n# Create dummy data with class imbalance 9 to 1\ndata = torch.FloatTensor(numDataPoints, data_dim)\ntarget = np.hstack((np.zeros(int(numDataPoints * 0.9), dtype=np.int32),\n                    np.ones(int(numDataPoints * 0.1), dtype=np.int32)))\n\nprint('target train 0\/1: {}\/{}'.format(len(np.where(target == 0)[0]), len(np.where(target == 1)[0])))\n\nclass_sample_count = np.array(\n    [len(np.where(target == t)[0]) for t in np.unique(target)])\nweight = 1. \/ class_sample_count\nweight = weight * [.3,.7]\nprint(weight)\n\nsamples_weight = np.array([weight[t] for t in target])\n\nsamples_weight = torch.from_numpy(samples_weight)\nsamples_weigth = samples_weight.double()\nsampler = WeightedRandomSampler(samples_weight, len(samples_weight))\n\ntarget = torch.from_numpy(target).long()\ntrain_dataset = torch.utils.data.TensorDataset(data, target)\n\ntrain_loader = DataLoader(\n    train_dataset, batch_size=bs, num_workers=1, sampler=sampler)\n\nfor i, (data, target) in enumerate(train_loader):\n  print(\"batch index {}, 0\/1: {}\/{}\".format(i, len(np.where(target.numpy() == 0)[0]), len(np.where(target.numpy() == 1)[0])))\n<\/code>\nThis outputs:\n<code class=\"lang-auto\">target train 0\/1: 900\/100\n[0.00033333 0.007     ]\nbatch index 0, 0\/1: 29\/71\nbatch index 1, 0\/1: 31\/69\nbatch index 2, 0\/1: 20\/80\nbatch index 3, 0\/1: 39\/61\nbatch index 4, 0\/1: 34\/66\nbatch index 5, 0\/1: 34\/66\nbatch index 6, 0\/1: 33\/67\nbatch index 7, 0\/1: 31\/69\nbatch index 8, 0\/1: 34\/66\nbatch index 9, 0\/1: 34\/66\n<\/code>\nI tried multiplying the desired class weight by 5 in my case (and implicitly multiplying the other classes by 1) and it seems to have the effect of making that class get chosen with a 50% probability."},{"x":"Please, I am trying to train a LSTM but I don\u2019t know where it is failing. I have the error:\nModuleAttributeError: \u2018LSTM\u2019 object has no attribute \u2018hidden_size\u2019.\nFull code in Colab: https:\/\/colab.research.google.com\/drive\/1nHODrwukJZYMMnH-fkMTLlvA9o1ihv-r?usp=sharing\n``\nclass LSTM(nn.Module):\ndef init(self, input_size=1, hidden_size=100):\nsuper().init()\nself.hidden= None\n    self.lstm1 = nn.LSTM(\n        input_size=input_size,\n        hidden_size=hidden_size,\n        num_layers=1,\n        batch_first=True,\n        bidirectional=False,\n    )\n    self.lstm2 = nn.LSTM(\n        hidden_size,\n        hidden_size * 2,\n        num_layers=1,\n        batch_first=True,\n        bidirectional=False,\n    )\n    self.fc = nn.Linear(hidden_size * 2, 1)\n\ndef forward(self, x):\n    x, _ = self.lstm1(x)\n    x, _ = self.lstm2(x)\n\n    x = self.fc(x)\n\n    return x\n\n\u00b4\u00b4","y":"Hy, the error is generated at this line.\n<code class=\"lang-auto\">model.hidden = (torch.zeros(1,1,model.hidden_size).cuda(),\n                       torch.zeros(1,1,model.hidden_size).cuda())\n<\/code>\nRight then you should modify your LSTM class like this.\n<code class=\"lang-auto\">class LSTM(nn.Module):\n   def init(self, input_size=1, hidden_size=100):\n        super(LSTM,self).__init__()\n        self.hidden_size = hidden_size\n        self.hidden= None\n<\/code>\nShould do","z":"Hy, the error is generated at this line.\n<code class=\"lang-auto\">model.hidden = (torch.zeros(1,1,model.hidden_size).cuda(),\n                       torch.zeros(1,1,model.hidden_size).cuda())\n<\/code>\nRight then you should modify your LSTM class like this.\n<code class=\"lang-auto\">class LSTM(nn.Module):\n   def init(self, input_size=1, hidden_size=100):\n        super(LSTM,self).__init__()\n        self.hidden_size = hidden_size\n        self.hidden= None\n<\/code>\nShould do\nThanks a million  for your help. After changing it, the performance is very poor as you can see in this link:https:\/\/colab.research.google.com\/drive\/1Q7q0dGTylMYMji1E3M104r0149xc9dbh?usp=sharing\nI don\u2019t have clear yet the difference between hidden and hidden_size attributes of the model. I am new in this topic, sorry.\nCould you recommend me any good tutorial available on the web for this or to improve my knowledge on LSTMs for Time-Series?\nThanks.\nSure,\nYou can check my earlier post on this.\n\n\n\n\nCan LSTM run multivariate time series? with link \"https:\/\/discuss.pytorch.org\/t\/can-lstm-run-multivariate-time-series\/93779\/2\" windows with link \"\/c\/windows\/26\"\n\n\n    Yes you can use LSTM for time series data prediction. You can find alot of resources for that purpose. \nYou can check this github repo for research papers and link for data resources. \nhttps:\/\/github.com\/Alro10\/deep-learning-time-series \nIf you want to checkout for implementation you can also find that in below link. \nhttps:\/\/github.com\/zhangxu0307\/time_series_forecasting_pytorch\nhttps:\/\/github.com\/KurochkinAlexey\/ConvRNN\nhttps:\/\/stackoverflow.com\/questions\/56858924\/multivariate-input-lstm-in\u2026 with link \"https:\/\/stackoverflow.com\/questions\/56858924\/multivariate-input-lstm-in-pytorch\"\n\n\n\nFirst check these on PyTorch website.\nhttps:\/\/pytorch.org\/tutorials\/intermediate\/char_rnn_classification_tutorial.html\nhttps:\/\/pytorch.org\/tutorials\/beginner\/nlp\/sequence_models_tutorial.html\nThanks . The multivariate approach is just what I need. Great info. I will have a look on it today and will get back to you if I have more questions. Thanks again for your support.\nDear , thanks for the information that you provided to me. It is great. However, I still have some questions.\n1.- I still do not have clear the differences between hidden and hidden_size in the model.\nComplete code in: https:\/\/colab.research.google.com\/drive\/1Q7q0dGTylMYMji1E3M104r0149xc9dbh?usp=sharing\n<code class=\"lang-auto\">class LSTM(nn.Module):\n    def __init__(self, input_size=1, hidden_size=100):\n        super().__init__()\n        self.hidden= None\n        self.hidden_size=hidden_size\n\n        self.lstm1 = nn.LSTM(\n            input_size=input_size,\n            hidden_size=hidden_size,\n            num_layers=1,\n            batch_first=True,\n            bidirectional=False,\n        )\n        self.lstm2 = nn.LSTM(\n            hidden_size,\n            hidden_size * 2,\n            num_layers=1,\n            batch_first=True,\n            bidirectional=False,\n        )\n        self.fc = nn.Linear(hidden_size * 2, 1)\n\n    def forward(self, x):\n        x, _ = self.lstm1(x)\n        x, _ = self.lstm2(x)\n\n        x = self.fc(x)\n\n        return x\n<\/code>\n2.- Why is the c_n part of the model not used in the second LSTM? Like here\nhttps:\/\/colah.github.io\/posts\/2015-08-Understanding-LSTMs\/\nimage739\u00d7313 43 KB with link \"https:\/\/discuss.pytorch.org\/uploads\/default\/original\/3X\/1\/9\/1976c59fca638e2395778c3034e6f4d5207461af.png\"\nThanks in advance."},{"x":"Hello, I\u2019m new to Pytorch. I\u2019m tring to convert a code that use functions from scipy and numpy library in Pytorch in order to build a NN and execute it on the GPU.\nI have some convolution layers that perform the convolution between a gaussian filter and an image. Exploiting the separability of the gaussian filters I perform the convolution along the x-axis and then on the y-axis.\nThe code without PyTorch is:\n<code class=\"lang-auto\">from scipy.ndimage.filters import convolve\n\nimg_convolved = convolve(convolve(img, gy[:, None]), gx[None, :])\n# default padding mode is 'reflect'\n<\/code>\nwhere gx and gy are the 1D gaussian filters.\nMine \u201ctranslation\u201d to PyTorch is:\n<code class=\"lang-auto\">import torch\nimport torch.nn.functional as F\nimport numpy as np\n\nimg = np.random.rand(512, 512) # random image with shape\npad_w, pad_h = int(np.ceil(img.shape[0] \/ gx.size()[0])), int(np.ceil(img.shape[1] \/ gx.size()[0]))\n\nimg = torch.from_numpy(img).repeat(1, 1, 1, 1)\ngx, gy = torch.flip(gx, (0,)).type(torch.float32), \\\n         torch.flip(gy, (0,)).type(torch.float32)\ngx, gy = gx.repeat(1, 1, 1, 1), gy.repeat(1, 1, 1).unsqueeze(3)\nimg = F.pad(img, [0, 0, pad_w, pad_h], 'reflect')\n\nconvY = F.conv2d(img.float(), gy, stride=1)\nconvX = F.conv2d(convY, gx, stride=1)\n<\/code>\nTo notice that the dimension of the gaussian filters change based on the value of the sigma chosen (this will be an hyperparameter to optimize later). Some examples are 47,5,4,\u2026\nMy problems here are:\n\n\nThe final output of the convolution (convX) has the shape that depends on the filters shape. What I want is an image with the same shape of the original image (512,512). This is the output of the scipy convolve method, but not the one of the PyTorch convolution.\n\n\nWill I be able to run this on the GPU building a Convolutional layer?\n\n","y":"I simplified your code below:\n<code class=\"lang-python\">import torch\nimport torch.nn.functional as F\nimport numpy as np\n\nimg = torch.rand(1, 1, 512, 512)\n\nkx, ky = 5, 5\n\n# Goes from 1 channel to 1 channel\nweight_h = torch.rand(1, 1, ky, 1)\nweight_w = torch.rand(1, 1, 1, kx)\n\npadx = kx \/\/ 2\npady = ky \/\/ 2\n\nconvY = F.conv2d(img, weight_h, padding=(pady, 0))\nconvX = F.conv2d(convY, weight_w, padding=(0, padx))\n\nprint(img.size())\nprint(convY.size())\nprint(convX.size())\n<\/code>","z":"Hi,\nThe convolution will not ensure that the output is of the same size indeed.\nBut if you use stride=1, no dilation and your kernel has an odd size, you can set the padding to be floor(kern\/2) to make sure that the output will have the same size as the input.\nAnd yes these ops are all implemented on GPU so you will be able tto use it.\nHi, thank you for the answer.\nThe result of the first convolution has the desired shape, but the last one (along x-axis) change the dimension of the columns.\nAny ideas?\nThese are my new padding dimensions:\n<code class=\"lang-auto\">pad_w, pad_h = int(np.floor(gx.size()[0] \/ 2)), int(np.floor(gx.size()[0] \/ 2))\n<\/code>\nI simplified your code below:\n<code class=\"lang-python\">import torch\nimport torch.nn.functional as F\nimport numpy as np\n\nimg = torch.rand(1, 1, 512, 512)\n\nkx, ky = 5, 5\n\n# Goes from 1 channel to 1 channel\nweight_h = torch.rand(1, 1, ky, 1)\nweight_w = torch.rand(1, 1, 1, kx)\n\npadx = kx \/\/ 2\npady = ky \/\/ 2\n\nconvY = F.conv2d(img, weight_h, padding=(pady, 0))\nconvX = F.conv2d(convY, weight_w, padding=(0, padx))\n\nprint(img.size())\nprint(convY.size())\nprint(convX.size())\n<\/code>\nThank you very much!! You saved me a lot of time.\nBy the way I\u2019ve noticed that plotting the resulting image the last column and the first one are cut. I can\u2019t understand why.\nEdit: Ok I figured out the bug. It was in another part of the code."},{"x":"Good Afternoon,\nI am wondering whether this is a suitable way to approach a problem or if I should consider alternatives (that I am unaware of) as well.\nI have a relatively balanced classification problem involving 3 labels: 0, 1, and 2. I am interested, however, in paying a little more attention to getting the 0s right. My approach is, when using nn.CrossEntropyLoss, is to punish mistakes for the 0s more than 1 or 2 by:\n<code class=\"lang-auto\">weights = [2.0, 1.0, 1.0]\nclass_weights = torch.FloatTensor(weights).cuda()\ncriterion = nn.CrossEntropyLoss(weight=class_weights)\n<\/code>\nIs this is a reasonable strategy or are there other things that I might consider?\nThanks in advance for your time and consideration!","y":"Hi Andrew!\n\n\n\n localh:\n\n<code class=\"lang-auto\">weights = [2.0, 1.0, 1.0]\n<\/code>\n\n\nYes, this is a reasonable approach to more accurately classify\nclass-0 samples.\nThat is, adding these class weights to your loss function will train\nyour network to more often correctly label class-0 samples as\nclass-0 (fewer false negatives), but at the cost of more frequently\nincorrectly labelling class-1 and class-2 samples as class-0 (more\nfalse positives from the perspective of class-0).\n\n\nmore attention to getting the 0s right.\n\u2026\npunish mistakes for the 0s more than 1 or 2 by:\n\n\nJust to emphasize what I alluded to above, this depends on what\nyou mean by \u201cgetting the 0s right\u201d and \u201cpunish mistakes for the 0s\u201d.\nIf you consider mislabelling a class-1 as a class-0 (a false positive)\nas not \u201cgetting the 0s right\u201d and a \u201cmistake for the 0s,\u201d then this\nlikely won\u2019t accomplish your goal.  But if you\u2019re only counting\nmislabelling an actual class-0 (a false negative) as a mistake, this\nis the way to go.\nBest.\nK. Frank","z":"Hi Andrew!\n\n\n\n localh:\n\n<code class=\"lang-auto\">weights = [2.0, 1.0, 1.0]\n<\/code>\n\n\nYes, this is a reasonable approach to more accurately classify\nclass-0 samples.\nThat is, adding these class weights to your loss function will train\nyour network to more often correctly label class-0 samples as\nclass-0 (fewer false negatives), but at the cost of more frequently\nincorrectly labelling class-1 and class-2 samples as class-0 (more\nfalse positives from the perspective of class-0).\n\n\nmore attention to getting the 0s right.\n\u2026\npunish mistakes for the 0s more than 1 or 2 by:\n\n\nJust to emphasize what I alluded to above, this depends on what\nyou mean by \u201cgetting the 0s right\u201d and \u201cpunish mistakes for the 0s\u201d.\nIf you consider mislabelling a class-1 as a class-0 (a false positive)\nas not \u201cgetting the 0s right\u201d and a \u201cmistake for the 0s,\u201d then this\nlikely won\u2019t accomplish your goal.  But if you\u2019re only counting\nmislabelling an actual class-0 (a false negative) as a mistake, this\nis the way to go.\nBest.\nK. Frank\nThanks so much, this makes a lot of sense and this is precisely what I want:\n\nadding these class weights to your loss function will train\nyour network to more often correctly label class-0 samples as\nclass-0 (fewer false negatives)\n\nOut of curiosity, do you mind elaborating on this problem and what one might do?\n\nJust to emphasize what I alluded to above, this depends on what\nyou mean by \u201cgetting the 0s right\u201d and \u201cpunish mistakes for the 0s\u201d.\nIf you consider mislabelling a class-1 as a class-0 (a false positive)\nas not \u201cgetting the 0s right\u201d and a \u201cmistake for the 0s,\u201d then this\nlikely won\u2019t accomplish your goal.\n\nHi Andrew!\n\n\n\n localh:\n\nOut of curiosity, do you mind elaborating on this problem and what one might do?\n\nIf you consider mislabelling a class-1 as a class-0 (a false positive)\nas not \u201cgetting the 0s right\u201d and a \u201cmistake for the 0s,\u201d then this\nlikely won\u2019t accomplish your goal.\n\n\n\nYour network does what your loss function trains it to do.\nAs a general rule, if you train your network to do better on one thing,\nthen \u2013 all else being equal \u2013 it will be likely to do worse on something\nelse.\n(That doesn\u2019t mean you can\u2019t train your network to do better on\neverything, perhaps by training longer, or using a more apt loss\nfunction, or using a better optimization algorithm, or training with\nmore or better data, etc.)\nMy point is that using class weights that favor class-0, you\u2019re telling\nyour network training that you care less about getting classes 1 and\n2 right, so, in general, your network won\u2019t perform as well on classes\n1 and 2, including mislabelling them as class-0 \u2013 because that\u2019s what\nyou trained it to do.\nNow, if you don\u2019t really care about mixing up classes 1 and 2, but\nwant to get class-0 right from both a false negative and false positive\nperspective (that is, you\u2019re willing, e.g.,  to mislabel class-1 as class-2,\nbut you don\u2019t want to mislabel class-1 as class-0), you could, at the\nextreme, train a binary classifier that identifies class-0 vs. everything\nelse.  (Again, a trade-off: do better on both class-0 false positives and\nfalse negatives, at the cost of not distinguishing class 1 from class-2.)\nNow, some speculation, because I\u2019ve never actually tried this.  You\ncould add to your conventional three-class loss function (class-0 vs.\nclass-1 vs. class-2) that does distinguish between class-1 and class-2,\na two-class loss function (class-0 vs other-than-class-0).  This will\nhelp train your network to do better on both class-0 false negatives\nand false positives, while still somewhat distinguishing class-1 and\nclass-2, but at the cost of not distinguishing them as well.\nThis will also bias your network to do better on class-0, but in a\nway that\u2019s different than overweighting class-0 in your conventional\nthree-class loss function.  You\u2019d still be making a trade-off, just a\ndifferent one.  Which way to go depends on the details of what\u2019s\nmore important to you.\nBest.\nK. Frank\nWow! Thank you so much for the insightful and easy to understand post. Your comment below actually sparked a memory from something that I read not too long ago, a journal article I believe, which I think did just that. I am going to look for it and see if I can apply the same for fun\/learning.\n\nNow, some speculation, because I\u2019ve never actually tried this. You\ncould add to your conventional three-class loss function (class-0 vs.\nclass-1 vs. class-2) that does distinguish between class-1 and class-2,\na two-class loss function (class-0 vs other-than-class-0). This will\nhelp train your network to do better on both class-0 false negatives\nand false positives, while still somewhat distinguishing class-1 and\nclass-2, but at the cost of not distinguishing them as well.\n\nIf interested, I can post it here for you to skim \u2013 but otherwise, this has been quite the learning experience for me. Thank you again!\nEdit: I found it. It was a Stanford ML final project paper. The point is located on the end of page 3: https:\/\/github.com\/JRC1995\/BERT-Disaster-Classification-Capsule-Routing\/blob\/master\/Project_Report.pdf. I think its similar to your point."},{"x":"I have an input tensor like x = torch.rand(2,3,13,224,224) and after passing through this tensor from my model I get an output shape as torch.Size([2, 512, 13, 14, 14]). I then further want to reduce 14x14 to 1x1 so it looks like this torch.Size([2, 512, 13, 1, 1]) and then have a max of the 13 features to get an output tensor like torch.Size([2, 512, 1, 1, 1]).\nI am pretty new to PyTorch and I found way to do this,\ngap = torch.nn.AdaptiveAvgPool3d((None,1,1))\nx = model(torch.rand(2,3,13,224,224))\nx = gap(x)\nx = torch.max(x, 2, keepdim=True)[0]\nI want to know if this is the correct way to do this or not. I want to reduce all the 14x14 values to 1x1 using average pooling, my question is am i using AdaptiveAvgPool3d in a correct way or should I use AdaptiveAvgPool2d in some way for reducing 14x14?","y":"Your code looks alright.\nAlternatively, since your desired output shape after the average pooling operation is [batch_size, 512, 13, 1, 1], you could also directly apply the mean() operation on dim3 and dim4 in a similar way as you\u2019ve already done for torch.max (but this should yield the same output as your current approach).","z":"Your code looks alright.\nAlternatively, since your desired output shape after the average pooling operation is [batch_size, 512, 13, 1, 1], you could also directly apply the mean() operation on dim3 and dim4 in a similar way as you\u2019ve already done for torch.max (but this should yield the same output as your current approach)."},{"x":"The code-\n<code class=\"lang-auto\">def train_model(n_epoch, data):\n  Encoder.train()\n  Decoder.train()\n  best_acc1 = 0\n  iter = 0\n  for epoch in tqdm(range(n_epoch)):\n    for i, (images, labels) in tqdm(enumerate(data['train'])):\n\n      if torch.cuda.is_available():\n        images = images.cuda().float()\n        labels = labels.cuda()\n      else:\n        images = Variable(images)\n        labels = Variable(labels)\n      # print('works')\n      # Clear gradients w.r.t. parameters\n      optimizer.zero_grad()\n      # Forward pass to get output\/logits\n      features = Encoder(images)\n      features = features.unsqueeze_(1)\n      outputs = Decoder(features)\n        \n      # Calculate Loss: softmax --> cross entropy loss\n      loss = criterion(outputs, labels)\n        \n      # Getting gradients w.r.t. parameters\n      loss.backward()\n        \n      # Updating parameters\n      optimizer.step()\n      # print('Optimizer')  \n      iter += 1\n        \n      if iter % 500 == 0:\n        # print(\"iter\")\n        # Calculate Accuracy         \n        accuracy_v = data_accuracy(data, 'valid')\n\n        is_best = accuracy_v > best_acc1\n        best_acc1 = max(accuracy_v, best_acc1)\n        save_checkpoint({\n                'epoch': n_epoch,\n                'iter': iter,\n                'state_dict_encoder': Encoder.state_dict(),\n                'state_dict_decoder': Decoder.state_dict(),\n                'best_acc1': best_acc1,\n                'optimizer' : optimizer.state_dict(),\n            }, is_best)\n\n        # print(\"test\")\n        # accuracy_t = data_accuracy(data, 'train')\n        # Print Loss\n        print('Iteration: {}. Loss: {}. Accuracy {}'.format(iter, loss.item(), accuracy_v))\n<\/code>\nError message -\n<code class=\"lang-auto\">RuntimeError: CUDA error: device-side assert triggered\nException raised from launch_vectorized_kernel at \/pytorch\/aten\/src\/ATen\/native\/cuda\/CUDALoops.cuh:146 (most recent call first):\nframe #0: c10::Error::Error(c10::SourceLocation, std::string) + 0x42 (0x7fd6a31681e2 in \/usr\/local\/lib\/python3.6\/dist-packages\/torch\/lib\/libc10.so)\nframe #1: void at::native::gpu_kernel_impl<__nv_hdl_wrapper_t<false, false, __nv_dl_tag<void (*)(at::TensorIterator&amp;, c10::Scalar), &amp;at::native::add_kernel_cuda, 4u>, float (float, float), float> >(at::TensorIterator&amp;, __nv_hdl_wrapper_t<false, false, __nv_dl_tag<void (*)(at::TensorIterator&amp;, c10::Scalar), &amp;at::native::add_kernel_cuda, 4u>, float (float, float), float> const&amp;) + 0xe03 (0x7fd6a4f37933 in \/usr\/local\/lib\/python3.6\/dist-packages\/torch\/lib\/libtorch_cuda.so)\nframe #2: void at::native::gpu_kernel<__nv_hdl_wrapper_t<false, false, __nv_dl_tag<void (*)(at::TensorIterator&amp;, c10::Scalar), &amp;at::native::add_kernel_cuda, 4u>, float (float, float), float> >(at::TensorIterator&amp;, __nv_hdl_wrapper_t<false, false, __nv_dl_tag<void (*)(at::TensorIterator&amp;, c10::Scalar), &amp;at::native::add_kernel_cuda, 4u>, float (float, float), float> const&amp;) + 0x11b (0x7fd6a4f3934b in \/usr\/local\/lib\/python3.6\/dist-packages\/torch\/lib\/libtorch_cuda.so)\nframe #3: void at::native::gpu_kernel_with_scalars<__nv_hdl_wrapper_t<false, false, __nv_dl_tag<void (*)(at::TensorIterator&amp;, c10::Scalar), &amp;at::native::add_kernel_cuda, 4u>, float (float, float), float> >(at::TensorIterator&amp;, __nv_hdl_wrapper_t<false, false, __nv_dl_tag<void (*)(at::TensorIterator&amp;, c10::Scalar), &amp;at::native::add_kernel_cuda, 4u>, float (float, float), float> const&amp;) + 0xeb (0x7fd6a4f395bb in \/usr\/local\/lib\/python3.6\/dist-packages\/torch\/lib\/libtorch_cuda.so)\nframe #4: <unknown function> + 0x192a486 (0x7fd6a4efb486 in \/usr\/local\/lib\/python3.6\/dist-packages\/torch\/lib\/libtorch_cuda.so)\nframe #5: at::native::add_kernel_cuda(at::TensorIterator&amp;, c10::Scalar) + 0x1a (0x7fd6a4efc1fa in \/usr\/local\/lib\/python3.6\/dist-packages\/torch\/lib\/libtorch_cuda.so)\nframe #6: <unknown function> + 0xbce25e (0x7fd6dad8b25e in \/usr\/local\/lib\/python3.6\/dist-packages\/torch\/lib\/libtorch_cpu.so)\nframe #7: at::native::add_out(at::Tensor&amp;, at::Tensor const&amp;, at::Tensor const&amp;, c10::Scalar) + 0x71 (0x7fd6dad81b61 in \/usr\/local\/lib\/python3.6\/dist-packages\/torch\/lib\/libtorch_cpu.so)\nframe #8: <unknown function> + 0xf3b932 (0x7fd6a450c932 in \/usr\/local\/lib\/python3.6\/dist-packages\/torch\/lib\/libtorch_cuda.so)\nframe #9: <unknown function> + 0x2e9fad8 (0x7fd6dd05cad8 in \/usr\/local\/lib\/python3.6\/dist-packages\/torch\/lib\/libtorch_cpu.so)\nframe #10: <unknown function> + 0x3377258 (0x7fd6dd534258 in \/usr\/local\/lib\/python3.6\/dist-packages\/torch\/lib\/libtorch_cpu.so)\nframe #11: torch::autograd::AccumulateGrad::apply(std::vector<at::Tensor, std::allocator<at::Tensor> >&amp;&amp;) + 0x38a (0x7fd6dd535aaa in \/usr\/local\/lib\/python3.6\/dist-packages\/torch\/lib\/libtorch_cpu.so)\nframe #12: <unknown function> + 0x3375bb7 (0x7fd6dd532bb7 in \/usr\/local\/lib\/python3.6\/dist-packages\/torch\/lib\/libtorch_cpu.so)\nframe #13: torch::autograd::Engine::evaluate_function(std::shared_ptr<torch::autograd::GraphTask>&amp;, torch::autograd::Node*, torch::autograd::InputBuffer&amp;, std::shared_ptr<torch::autograd::ReadyQueue> const&amp;) + 0x1400 (0x7fd6dd52e400 in \/usr\/local\/lib\/python3.6\/dist-packages\/torch\/lib\/libtorch_cpu.so)\nframe #14: torch::autograd::Engine::thread_main(std::shared_ptr<torch::autograd::GraphTask> const&amp;) + 0x451 (0x7fd6dd52efa1 in \/usr\/local\/lib\/python3.6\/dist-packages\/torch\/lib\/libtorch_cpu.so)\nframe #15: torch::autograd::Engine::thread_init(int, std::shared_ptr<torch::autograd::ReadyQueue> const&amp;, bool) + 0x89 (0x7fd6dd527119 in \/usr\/local\/lib\/python3.6\/dist-packages\/torch\/lib\/libtorch_cpu.so)\nframe #16: torch::autograd::python::PythonEngine::thread_init(int, std::shared_ptr<torch::autograd::ReadyQueue> const&amp;, bool) + 0x4a (0x7fd6eacc734a in \/usr\/local\/lib\/python3.6\/dist-packages\/torch\/lib\/libtorch_python.so)\nframe #17: <unknown function> + 0xbd6df (0x7fd7076806df in \/usr\/lib\/x86_64-linux-gnu\/libstdc++.so.6)\nframe #18: <unknown function> + 0x76db (0x7fd7087626db in \/lib\/x86_64-linux-gnu\/libpthread.so.0)\nframe #19: clone + 0x3f (0x7fd708a9ba3f in \/lib\/x86_64-linux-gnu\/libc.so.6)\n<\/code>\nMy code runs fine till like 366 iteration. But after that this error is shown? Any idea what this is? I can\u2019t debug this cause the code seem to run fine untill a certain iteration.\n(I am running it in google colab)","y":"As  suggested, the error is raised by an out-of-bounds indexing.\nBased on the stack trace it seems that your model output contains logits (or log probabilities) for 2 classes, while the target uses a class index of 2, which would assume at least 3 classes.\nnn.CrossEntropyLoss and nn.NLLLoss expect a model output in the shape [batch_size, nb_classes] and a target in the shape [batch_size] containing class indices in the range [0, nb_classes-1].\nHere is a small code snippet to reproduce this error:\n<code class=\"lang-python\">batch_size = 2\nnb_classes = 2\noutput = torch.randn(batch_size, nb_classes, requires_grad=True)\ntarget = torch.randint(0, nb_classes, (batch_size,))\n\ncriterion = nn.CrossEntropyLoss()\nloss = criterion(output, target)\n\ntarget[0] = 2\nloss = criterion(output, target)\n> IndexError: Target 2 is out of bounds.\n<\/code>\nHow many classes are created in the ImageFolder and what is the shape of your model output?","z":"the device assert indicates that you are doing out-of-bounds indexing on one of your Tensors. For example\n<code class=\"lang-auto\">x = torch.randn(4); # x has indices 0, 1, 2, 3\nprint(x[4]) # device assert\n<\/code>\nI am loading the dataset using the default pytorch ImageFolder library. Can\u2019t seem to pinpoint any issues here with the tensors.\n<code class=\"lang-auto\">def load_data(train_path,valid_path, batch_size, n_iters):\n  train_dataset = datasets.ImageFolder(\n      root=train_path,\n      transform=transforms,\n  )\n  valid_dataset = datasets.ImageFolder(\n      root=valid_path,\n      transform=transforms,\n  )\n\n  epoch = n_iters\/(len(train_dataset)\/batch_size)\n  epoch = int(epoch)\n  print(\"Number of epoch {}\".format(epoch))\n  \n\n  train_loader = torch.utils.data.DataLoader(\n      train_dataset,\n      batch_size = batch_size,\n      shuffle = False\n  )\n\n  val_loader = torch.utils.data.DataLoader(\n      valid_dataset,\n      batch_size = batch_size,\n      shuffle = False\n  )\n\n  print('\\nTraining data info {}'.format(train_dataset))\n  print('\\nValid data info {}'.format(valid_dataset))\n\n  # print('\\n\\nTrain set length {}'.format(train_set))\n  # print('\\nValidation set length {}'.format(val_set))\n\n  print('\\n\\nTraining Batch loader size {}'.format(len(train_loader)))\n  print('\\nValidation Batch loader size {}'.format(len(val_loader)))\n\n  return epoch, train_loader, val_loader\n<\/code>\nimage1312\u00d7348 16.6 KB with link \"https:\/\/discuss.pytorch.org\/uploads\/default\/original\/3X\/c\/1\/c1ca75abc9567478183771c7f2ae8ecf489713f9.png\"\nAlso the error seems to be during calculating loss. The classes seem to fine according to the dataset.\n\nCould you rerun the code via:\n<code class=\"lang-python\">CUDA_LAUNCH_BLOCKING=1 python scripy.py args\n<\/code>\nand post the stack trace here?\nAlternatively, you could also run the script on the CPU, which should give you a better error message.\n<code class=\"lang-auto\">IndexError                                Traceback (most recent call last)\n<ipython-input-24-9ecd5aa7284f> in <module>()\n----> 1 train_model(n_epoch,data_load)\n\n4 frames\n<ipython-input-22-ad52cecfd4c0> in train_model(n_epoch, data)\n     22 \n     23       # Calculate Loss: softmax --> cross entropy loss\n---> 24       loss = criterion(outputs, labels)\n     25 \n     26       # Getting gradients w.r.t. parameters\n\n\/usr\/local\/lib\/python3.6\/dist-packages\/torch\/nn\/modules\/module.py in _call_impl(self, *input, **kwargs)\n    720             result = self._slow_forward(*input, **kwargs)\n    721         else:\n--> 722             result = self.forward(*input, **kwargs)\n    723         for hook in itertools.chain(\n    724                 _global_forward_hooks.values(),\n\n\/usr\/local\/lib\/python3.6\/dist-packages\/torch\/nn\/modules\/loss.py in forward(self, input, target)\n    946     def forward(self, input: Tensor, target: Tensor) -> Tensor:\n    947         return F.cross_entropy(input, target, weight=self.weight,\n--> 948                                ignore_index=self.ignore_index, reduction=self.reduction)\n    949 \n    950 \n\n\/usr\/local\/lib\/python3.6\/dist-packages\/torch\/nn\/functional.py in cross_entropy(input, target, weight, size_average, ignore_index, reduce, reduction)\n   2420     if size_average is not None or reduce is not None:\n   2421         reduction = _Reduction.legacy_get_string(size_average, reduce)\n-> 2422     return nll_loss(log_softmax(input, 1), target, weight, None, ignore_index, None, reduction)\n   2423 \n   2424 \n\n\/usr\/local\/lib\/python3.6\/dist-packages\/torch\/nn\/functional.py in nll_loss(input, target, weight, size_average, ignore_index, reduce, reduction)\n   2216                          .format(input.size(0), target.size(0)))\n   2217     if dim == 2:\n-> 2218         ret = torch._C._nn.nll_loss(input, target, weight, _Reduction.get_enum(reduction), ignore_index)\n   2219     elif dim == 4:\n   2220         ret = torch._C._nn.nll_loss2d(input, target, weight, _Reduction.get_enum(reduction), ignore_index)\n\nIndexError: Target 2 is out of bounds.\n<\/code>\nAfter running it on CPU.\nAlso the model worked when I ran it on a sample data by turning the data into tensors without using ImageFolder. Could I be doing something wrong with that?\nAs  suggested, the error is raised by an out-of-bounds indexing.\nBased on the stack trace it seems that your model output contains logits (or log probabilities) for 2 classes, while the target uses a class index of 2, which would assume at least 3 classes.\nnn.CrossEntropyLoss and nn.NLLLoss expect a model output in the shape [batch_size, nb_classes] and a target in the shape [batch_size] containing class indices in the range [0, nb_classes-1].\nHere is a small code snippet to reproduce this error:\n<code class=\"lang-python\">batch_size = 2\nnb_classes = 2\noutput = torch.randn(batch_size, nb_classes, requires_grad=True)\ntarget = torch.randint(0, nb_classes, (batch_size,))\n\ncriterion = nn.CrossEntropyLoss()\nloss = criterion(output, target)\n\ntarget[0] = 2\nloss = criterion(output, target)\n> IndexError: Target 2 is out of bounds.\n<\/code>\nHow many classes are created in the ImageFolder and what is the shape of your model output?\nSolved the issue. It seems my RNN was outputing for 2 classes when I had more than 2 classes.\nThank you for the detailed explanation."},{"x":"Hi guys, I\u2019m new here and I have a problem. I have a tensor with data mat and an empty tensor of the same shape. I also have a 1D index tensor \u2018ind\u2019, which has indices of slices I want to copy. What I want to do is to look at index of a slice I want and put it in the same spot in the empy tensor.\n<code class=\"lang-auto\">mat = torch.arange(0,16).reshape(4,2,2)\nempty = torch.zeros((4,2,2))\nind = torch.tensor([0,2])\n<\/code>\nThe idea is to do something like this ( which I know is wrong, I\u2019m new to this   ):\n<code class=\"lang-auto\">empty[ind] = mat[ind]\n<\/code>\nso here the empty matrix should have first and third slices, while second and fourth slices should\nremain empty ( or in this case filled with zeros ):\n<code class=\"lang-auto\">torch.tensor( [ [ [ 0, 1 ],\n                         [ 2, 3 ] ],\n                       [ [ 0 , 0 ],\n                         [ 0 , 0 ] ],\n                        [ [ 8, 9 ],\n                         [10, 11 ] ],\n                         [ [ 0 , 0 ],\n                         [ 0 , 0 ] ] ] )\n<\/code>\nHow do I solve this?\nThank you for your answers.","y":"Your code seems to be working fine, if you get rid of the type mismatch error by calling float() on mat \n<code class=\"lang-python\">mat = torch.arange(0,16).reshape(4,2,2).float()\nempty = torch.zeros((4,2,2))\nind = torch.tensor([0,2])\nempty[ind] = mat[ind]\nprint(empty)\n> tensor([[[ 0.,  1.],\n           [ 2.,  3.]],\n\n          [[ 0.,  0.],\n           [ 0.,  0.]],\n\n          [[ 8.,  9.],\n           [10., 11.]],\n\n          [[ 0.,  0.],\n           [ 0.,  0.]]])\n<\/code>","z":"Your code seems to be working fine, if you get rid of the type mismatch error by calling float() on mat \n<code class=\"lang-python\">mat = torch.arange(0,16).reshape(4,2,2).float()\nempty = torch.zeros((4,2,2))\nind = torch.tensor([0,2])\nempty[ind] = mat[ind]\nprint(empty)\n> tensor([[[ 0.,  1.],\n           [ 2.,  3.]],\n\n          [[ 0.,  0.],\n           [ 0.,  0.]],\n\n          [[ 8.,  9.],\n           [10., 11.]],\n\n          [[ 0.,  0.],\n           [ 0.,  0.]]])\n<\/code>\nThank you so much! I think I was being too vague, with calling tensor as an index. Now the code is working properly "},{"x":"cc  I have a question regarding pytorch tensor memory usage, it seems that what should be functionally similar designs consumes drastically different amount of CPU memory, I have not tried GPU memory yet.\nBelow are two implementations of replay buffer used in RL:\nImplementation 1, uses 4.094GiB memory, creates 20003 tensors in total\n<code class=\"lang-auto\">from time import sleep\nfrom copy import deepcopy\nimport gc\nimport torch as t\n\n\nif __name__ == \"__main__\":\n    buffer = []\n    state = t.randint(0, 255, [1, 8, 224, 224], dtype=t.uint8)\n    for i in range(10000):\n        old_state = state\n        state = t.randint(0, 255, [1, 8, 224, 224], dtype=t.uint8)\n        buffer.append(\n            {\"state\": {\"state\": old_state},\n             \"action\": {\"action\": t.zeros([1, 1])},\n             \"next_state\": {\"state\": state},\n             \"reward\": 0.0,\n             \"terminal\": False}\n        )\n    count = 0\n    for obj in gc.get_objects():\n        try:\n            if t.is_tensor(obj) or (\n                    hasattr(obj, 'data') and t.is_tensor(obj.data)):\n                count += 1\n        except:\n            continue\n    print(count)\n    sleep(20)\n\n<\/code>\nImplementation 2, uses 7.561GiB memory, creates 20003 tensors in total\n<code class=\"lang-auto\">from time import sleep\nfrom copy import deepcopy\nimport gc\nimport torch as t\n\n\nif __name__ == \"__main__\":\n    buffer = []\n    state = t.randint(0, 255, [1, 8, 224, 224], dtype=t.uint8)\n    for i in range(10000):\n        old_state = state\n        state = t.randint(0, 255, [1, 8, 224, 224], dtype=t.uint8)\n        transition = {\"state\": {\"state\": old_state},\n                      \"action\": {\"action\": t.zeros([1, 1])},\n                      \"next_state\": {\"state\": state},\n                      \"reward\": 0.0,\n                      \"terminal\": False}\n\n        # some processing\n        transition[\"state\"][\"state\"] = transition[\"state\"][\"state\"].detach()\n        transition[\"action\"][\"action\"] = transition[\"action\"][\"action\"].detach()\n        transition[\"next_state\"][\"state\"] = transition[\"next_state\"][\"state\"].detach()\n\n        if i > 0:\n            last_transition = buffer[i - 1]\n            if transition[\"state\"][\"state\"].equal(\n                    last_transition[\"next_state\"][\"state\"]):\n                # store a reference, instead of deep-copying\n                transition[\"state\"] = last_transition[\"next_state\"]\n            else:\n                transition[\"state\"] = deepcopy(transition[\"state\"])\n            transition[\"action\"] = deepcopy(transition[\"action\"])\n            transition[\"next_state\"] = deepcopy(transition[\"next_state\"])\n        else:\n            transition = deepcopy(transition)\n        buffer.append(transition)\n    count = 0\n    for obj in gc.get_objects():\n        try:\n            if t.is_tensor(obj) or (\n                    hasattr(obj, 'data') and t.is_tensor(obj.data)):\n                count += 1\n        except:\n            continue\n    print(count)\n    sleep(20)\n\n<\/code>\nI suppose that the difference is caused by deepcopying the storage used by tensors, however, it seems that pytorch will not release \/ reuse the storage, below code takes 9.504GiB memory and creates 30003 tensors:\n<code class=\"lang-auto\">    buffer = []\n    state = t.randint(0, 255, [1, 8, 224, 224], dtype=t.uint8)\n    for i in range(10000):\n        old_state = state\n        state = t.randint(0, 255, [1, 8, 224, 224], dtype=t.uint8)\n        transition = {\"state\": {\"state\": old_state},\n                      \"action\": {\"action\": t.zeros([1, 1])},\n                      \"next_state\": {\"state\": state},\n                      \"reward\": 0.0,\n                      \"terminal\": False}\n\n        # some processing\n        transition[\"state\"][\"state\"] = transition[\"state\"][\"state\"].detach()\n        transition[\"action\"][\"action\"] = transition[\"action\"][\"action\"].detach()\n        transition[\"next_state\"][\"state\"] = transition[\"next_state\"][\"state\"].detach()\n\n        if i > 0:\n            last_transition = buffer[i - 1]\n            if transition[\"state\"][\"state\"].equal(\n                    last_transition[\"next_state\"][\"state\"]):\n                # store a reference, instead of deep-copying\n                transition[\"state\"] = last_transition[\"next_state\"]\n            else:\n                transition[\"state\"] = deepcopy(transition[\"state\"])\n            transition[\"action\"] = deepcopy(transition[\"action\"])\n            transition[\"next_state\"] = deepcopy(transition[\"next_state\"])\n        else:\n            transition = deepcopy(transition)\n        buffer.append(transition)\n\n    x = [t.randint(0, 255, [1, 8, 224, 224], dtype=t.uint8) for _ in range(10000)]\n    count = 0\n    for obj in gc.get_objects():\n        try:\n            if t.is_tensor(obj) or (\n                    hasattr(obj, 'data') and t.is_tensor(obj.data)):\n                count += 1\n        except:\n            continue\n    print(count)\n    sleep(20)\n<\/code>\nI wonder is there any way to reduce memory usage if I would like to deepcopy the tensor, then discard the original tensor and keep the copy only, i.e. reduce the memory usage of implementation 2 to implementation 1.","y":"Hi,\nThe allocator I mention here is not from pytorch but from your libc (we have no control over it). These are classic CPU allocators. Famous alternatives include jemalloc or tmalloc. But I haven\u2019t tested them myself.","z":"   Any idea \/ explaination about this would be appreciated!\nHi,\nThe difference I see between 1 and 2 is that you create much more intermediary Tensors (when doing the deepcopy).\nSo I guess that the allocator you\u2019re using is not returning this memory to the operating system to speed up following allocations? Maybe you want to try a different one?\nThank you for your response!\nYes, since I would like the allocator to directly release the memory(the inefficient sollution), or efficiently reuse the allocated memory regions (I have not observed this behavior in Impl #3), so is there any other allocator implementations, and their documentation? I am not very familiar with this part in pytorch.\nBTW, the intermediary tensors are needed because I need to prevent users from unintentionally modify the stored tensor data.\nHi,\nThe allocator I mention here is not from pytorch but from your libc (we have no control over it). These are classic CPU allocators. Famous alternatives include jemalloc or tmalloc. But I haven\u2019t tested them myself.\nNow I see the problem, I will test with these allocators. Thank you!"},{"x":"I am designing a object detector. I want to ignore images in the dataset that do not have any annotations. Is there any way I can do that?","y":"Would it be possible to compute the invalid indices before and pass them as a list to your Dataset?\nIf so, you could just add an offset to an index vector using the invalid indices.\nE.g. if the invalid indices are at 2 and 4, the indices list could be:\n<code class=\"lang-python\">self.indices = [0, 1, 3, 5, ...]\nindex = self.indices[index]\n# use index as before in __getitem__\n<\/code>\nAlternatively, you could also pass these indices to a Subset, which will apply the same logic.\nIf you cannot compute these indices beforehand, you could return a pre-defined invalid value, check for it in the training loop, and skip this sample.","z":"It depends on your current workflow.\nBasically I would try to filter out all files without any annotations.\nE.g. if your annotations are stored in a .csv file, you could load it and just pass all files with a valid annotation to the Dataset.\nIf you could explain your current approach a bit (e.g. what kind of Dataset are you using currently), there might be better suited approaches.\nThis is the DataLoader:\n<code class=\"lang-auto\">class dataLoader(Dataset):\n    def __init__(self, path, root, transforms=None):\n        self.path = path\n        self.root = root \n        self.transforms = transforms\n        self.img_loc, self.data = get_file_path(self.path)\n        self.data_len = len(self.data)\n    \n    def __getitem__(self, index):\n        img_path = (self.img_loc[index])\n        img = Image.open(img_path).convert(\"RGB\")\n        image_id = torch.tensor([index])\n        \n        data = self.data[index]\n        xml = ET.parse(data).getroot()\n        \n        objects = [n.text for n in xml.findall('.\/\/object\/name')]\n        if not objects:\n            objects.append(('None'))\n        num_objs = len(objects)\n        \n        labels = []\n        for i in range(num_objs):\n            get_id = get_class_id(objects[i])\n            labels.append(get_id)\n        labels = torch.tensor(labels)\n\n        box = [[int(box.find('w').text) - 1,\n                int(box.find('h').text) - 1,\n                int(box.find('x').text) - 1,\n                int(box.find('y').text) - 1]\n                for box in xml.findall('.\/\/object\/bndbox')]\n\n        boxes = torch.as_tensor(box, dtype=torch.float32)\n        \n        if not box:\n            area = 0\n        else:\n            area = (boxes[:, 3] - boxes[:, 1]) * (boxes[:, 2] - boxes[:, 0])\n                 \n        iscrowd = torch.zeros((num_objs,), dtype=torch.int64)\n        \n        keep = (boxes[:, 3]>boxes[:, 1]) &amp; (boxes[:, 2]>boxes[:, 0])\n        print(all(keep))\n        boxes = boxes[keep]\n        labels = labels[keep]\n        area = area[keep]\n        iscrowd = iscrowd[keep]\n       \n        target = {}\n        target['labels'] = labels\n        target[\"image_id\"] = image_id\n        target['boxes'] = boxes\n        target[\"area\"] = area\n        target[\"iscrowd\"] = iscrowd\n                         \n        if self.transforms is not None:\n            img, target = self.transforms(img, target)\n        \n        return img, target, img_path\n                                 \n    def __len__(self):\n        return self.data_len\n<\/code>\nThe annotations are stored in a xml format. What i want to do is something like when the len(boxes) == 0, the current index should be skipped and move to the next index. I don\u2019t want to return any of the data of the index  when there len(boxes) == 0 as that indicates that there is no objects in the image.\nIs there a simple way to skip an index?\nWould it be possible to compute the invalid indices before and pass them as a list to your Dataset?\nIf so, you could just add an offset to an index vector using the invalid indices.\nE.g. if the invalid indices are at 2 and 4, the indices list could be:\n<code class=\"lang-python\">self.indices = [0, 1, 3, 5, ...]\nindex = self.indices[index]\n# use index as before in __getitem__\n<\/code>\nAlternatively, you could also pass these indices to a Subset, which will apply the same logic.\nIf you cannot compute these indices beforehand, you could return a pre-defined invalid value, check for it in the training loop, and skip this sample.\nThank you very much. I followed you suggestion and created a script to find all the valid samples in the dataset.\nHi there,\nI have the same problem with unlabeld pictures. Can you tell me how to create a script to find all the valid samples in the dataset?\nThanks for help.\nThis was a while ago, so I don\u2019t remember exactly what I did. The way I wrote my script was a for loop of all my data and kept only the data that had a valid annotations. E.g. whether the annotations contained a bounding box or a particular class. If you this doesn\u2019t help, I can have a look for the script I wrote and upload it for you."},{"x":"Dear forum members,\nOn my new Windows 10 AMD Ryzen system I wanted to go for a pure python (non-Anaconda) based pytorch install, since I didn\u2019t get numpy to work well with openblas on Anaconda.\nSo I created a fresh virtual environment, first installed numpy and then installed pytorch by running\n<code class=\"lang-auto\">pip3 install torch===1.3.0 torchvision===0.4.1 -f https:\/\/download.pytorch.org\/whl\/torch_stable.html\n<\/code>\nThe installation finished but when importing torch I ran into the error\n<code class=\"lang-auto\">from torch._C import *\nImportError: DLL load failed: The specified module could not be found\n<\/code>\nI tried installing the cpu-only version of torch which had no issues.\nI was finally able to fix the error on the GPU-version of pytorch by installing CUDA manually before installing pytorch. (again by running pip3 install torch===1.3.0 torchvision===0.4.1 -f https:\/\/download.pytorch.org\/whl\/torch_stable.html)\nThis confuses me as it was my understanding that pytorch brings it\u2019s own CUDA and cudNN which are independent from the system\u2019s CUDA installation.\nSo I would like to understand how installing CUDA could have fixed my problem?\nI would be grateful for some insight into this behaviour, as I would like to understand how my pytorch installation is configured.\nThank you,\nPaul","y":"Hi,\npytorch does not bring cuda. It does bring cudnn though.\nIn conda, we have cuda as a dependency because conda package can contain cuda. But it\u2019s not the case in pip, so you need to install the matching cuda version to be able to use cuda with the pip installed package.","z":"Hi,\npytorch does not bring cuda. It does bring cudnn though.\nIn conda, we have cuda as a dependency because conda package can contain cuda. But it\u2019s not the case in pip, so you need to install the matching cuda version to be able to use cuda with the pip installed package.\nThank you! I am new to pytorch.\nYesterday I buy a new PC,and  I did\u2019t install the CUDA manually, but  when I download the\npytorch cuda==10.1  with pip, I import it and test, it turns out the  cuda is available!  I don\u2019t konw why!\nI input the \u201cnvcc -V\u201d  in CMD but  it does\u2019t work.\nthen, I installed the tensorflow-gpu in another pip venv, but it  does\u2019t work\nI have no idea about  if I have the CUDA!\nI want to know how to confirm the CUDA is exist?\nthanks!\nDouble post from here with link \"https:\/\/discuss.pytorch.org\/t\/can-i-link-pytorch-to-already-installed-cuda\/36279\/6\" with an answer."},{"x":"I read the examples in the documentation and the explanation but i still can\u2019t understand what this function does.","y":"Hi,\nIt is setting particular values of a tensor at the provided indices. The value that you set can either be always the same or provided by another tensor wich is the same size as the indices tensor.\nFor example, if you want to do a one hot encoding of a 1D tensor of labels, you can start with a 2D tensor filled with zeros and then scatter 1s according to the labels of each entry.","z":"Hi,\nIt is setting particular values of a tensor at the provided indices. The value that you set can either be always the same or provided by another tensor wich is the same size as the indices tensor.\nFor example, if you want to do a one hot encoding of a 1D tensor of labels, you can start with a 2D tensor filled with zeros and then scatter 1s according to the labels of each entry.\nIn case anyone is looking at this after the documentation, here is an explanation for how they arrived at the first result:\n<code class=\"lang-auto\">>>> x = torch.rand(2, 5)\n>>> x\ntensor([[ 0.3992,  0.2908,  0.9044,  0.4850,  0.6004],\n        [ 0.5735,  0.9006,  0.6797,  0.4152,  0.1732]])\n>>> torch.zeros(3, 5).scatter_(0, torch.tensor([[0, 1, 2, 0, 0], [2, 0, 0, 1, 2]]), x)\ntensor([[ 0.3992,  0.9006,  0.6797,  0.4850,  0.6004],\n        [ 0.0000,  0.2908,  0.0000,  0.4152,  0.0000],\n        [ 0.5735,  0.0000,  0.9044,  0.0000,  0.1732]])\n\n<\/code>\nThe scatter says \u201csend the elements of x to the following indices in torch.zeros, according to ROW-WISE (dim 0)\u201d. In layman\u2019s terms, this is saying, for each element in the original\n[ 0.3992, 0.2908, 0.9044, 0.4850, 0.6004], [ 0.5735, 0.9006, 0.6797, 0.4152, 0.1732] tensor, we specify a row index (0,1 or 2) to send it to in the tensor we are scattering into.\nhttps:\/\/pytorch.org\/docs\/stable\/tensors.html#torch.Tensor.scatter_\nSorry for asking but could you elaborate more on this?\nI don\u2019t understand the index part!?\nUpdate :\nI guess I understood it thanks to God!\n0, 1, 2, 0 , 0 are the row indexes of our destination tensor .\nthis simply means, put the first element from source into the first row  of  the destination tensor (at the same exact index )\nso 0th element in x which is 0.3992 goes to row 0. and in index 0.\n1tst element in x which is 0.2908 goes to row 1 and in index 1\n2nd element in x which is 0.9044 will go to row 2 and in index 2\n3rd element in x which is 0.4850 will go to row 0 and in index 3\n4th element in x which is \u2019 0.6004 will go to row 0 and in index 4\nand this goes on.\nBasically, all elements in source will have their respective index, but they only are redirected to a specific row in the destination tensor!\nIs this right?\n: almost. You got it right, given dim=0  (i.e. row in this case). But if dim=1 (i.e. column), the source row position would be maintained, and the indices would be used to specify the column position in destination.\nEssentially, think of \u2018dim\u2019 with \u2018index\u2019, and keep everything else in output the same as in \u2018src\u2019.\nThe \u2018_\u2019 in scatter_ signifies that the operation has to happen in-place."},{"x":"I suppose that model.cuda() and model.to(device) are the same, but they actually gave me different running time.\nI have the follwoing:\n<code class=\"lang-auto\">device = torch.device(\"cuda\")\nmodel = model_name.from_pretrained(\".\/my_module\")  # load my saved model\ntokenizer = tokenizer_name.from_pretrained(\".\/my_module\")  # load tokenizer\nmodel.to(device)  # I think no assignment is needed since it's not a tensor\nmodel.eval()  # I run my model for testing\n<\/code>\nHowever, later testing process takes 2 min 19 sec, which is different from if I do model.cuda() instead of model.to(device), while the latter takes 1 min 08 sec. I know they both are fast, but I don\u2019t understand why their running times are quite different while the two ways of coding should be the same thing.\nI was wondering due to the data amount, time might fluctuate. I can understand that. But I just wanna make sure the the two ways of coding a the same.","y":"Hi,\nThey do the same thing yes: send each param to the GPU one after the other.\nAre you sure that you don\u2019t have something else on the machine that could be using either the GPU, the CPU or the disk and that would slow down your eval?","z":"Hi,\nThey do the same thing yes: send each param to the GPU one after the other.\nAre you sure that you don\u2019t have something else on the machine that could be using either the GPU, the CPU or the disk and that would slow down your eval?\nHi,\nYes, I didn\u2019t modify any line of code except changing the ways of utilizing GPU. If they actually do the same thing, then I guess it might due to the case that warm-up time varies.\nNote that if it is a shared machine, this kind of thing can also be cause by other users \nThat makes sense. Thank you!"},{"x":"Hi!\nI\u2019m trying to perform the validation aver the alexnet trained with the dataset \u2018Caltech101\u2019 and I tried different solutions but none these seems to work.\nThe problem is that I\u2019ve an index error like the following each time I run the validation part:\nIndexError: index 3457 is out of bounds for axis 0 with size 2892.\nHere my dataset class\u2019 implementation:\nhttps:\/\/github.com\/rubenIng93\/Homework2-Caltech101\/blob\/master\/caltech_dataset.py\n\nAny help would be appreciated!","y":"you have downsized  train_dataset in line 16\njust flip line 16 and line 17","z":"you have downsized  train_dataset in line 16\njust flip line 16 and line 17\nOh thanks a lot, I did not see that"},{"x":"I am writing a binary classification model that consists of audio files of 40 participants and classifies them according to whether they have a speech disorder or not. The audio files have been divided into 5 second segments and to avoid subject bias, I have split the training\/testing\/validation sets such that a subject only appears in one set (i.e. participant ID02 does not appear in both the training and testing sets). The following error appears when I attempt to enumerate over the DataLoader validLoader in the code below and I\u2019m not entirely sure why this error is occurring. Does anyone have any advice?\n<code class=\"lang-auto\">KeyError                                  Traceback (most recent call last)\n<ipython-input-69-55be99283cf7> in <module>()\n----> 1 for i, data in enumerate(valid_loader, 0):\n      2   images, labels = data\n      3   print(\"Batch\", i, \"size:\", len(images))\n\n3 frames\n\/usr\/local\/lib\/python3.6\/dist-packages\/torch\/utils\/data\/dataloader.py in __next__(self)\n    361 \n    362     def __next__(self):\n--> 363         data = self._next_data()\n    364         self._num_yielded += 1\n    365         if self._dataset_kind == _DatasetKind.Iterable and \\\n\n\/usr\/local\/lib\/python3.6\/dist-packages\/torch\/utils\/data\/dataloader.py in _next_data(self)\n    987             else:\n    988                 del self._task_info[idx]\n--> 989                 return self._process_data(data)\n    990 \n    991     def _try_put_index(self):\n\n\/usr\/local\/lib\/python3.6\/dist-packages\/torch\/utils\/data\/dataloader.py in _process_data(self, data)\n   1012         self._try_put_index()\n   1013         if isinstance(data, ExceptionWrapper):\n-> 1014             data.reraise()\n   1015         return data\n   1016 \n\n\/usr\/local\/lib\/python3.6\/dist-packages\/torch\/_utils.py in reraise(self)\n    393             # (https:\/\/bugs.python.org\/issue2651), so we work around it.\n    394             msg = KeyErrorMessage(msg)\n--> 395         raise self.exc_type(msg)\n\nKeyError: Caught KeyError in DataLoader worker process 0.\nOriginal Traceback (most recent call last):\n  File \"\/usr\/local\/lib\/python3.6\/dist-packages\/torch\/utils\/data\/_utils\/worker.py\", line 185, in _worker_loop\n    data = fetcher.fetch(index)\n  File \"\/usr\/local\/lib\/python3.6\/dist-packages\/torch\/utils\/data\/_utils\/fetch.py\", line 44, in fetch\n    data = [self.dataset[idx] for idx in possibly_batched_index]\n  File \"\/usr\/local\/lib\/python3.6\/dist-packages\/torch\/utils\/data\/_utils\/fetch.py\", line 44, in <listcomp>\n    data = [self.dataset[idx] for idx in possibly_batched_index]\n  File \"<ipython-input-44-245be0a1e978>\", line 19, in __getitem__\n    x = Image.open(self.df['path'][index])\n  File \"\/usr\/local\/lib\/python3.6\/dist-packages\/pandas\/core\/series.py\", line 871, in __getitem__\n    result = self.index.get_value(self, key)\n  File \"\/usr\/local\/lib\/python3.6\/dist-packages\/pandas\/core\/indexes\/base.py\", line 4405, in get_value\n    return self._engine.get_value(s, k, tz=getattr(series.dtype, \"tz\", None))\n  File \"pandas\/_libs\/index.pyx\", line 80, in pandas._libs.index.IndexEngine.get_value\n  File \"pandas\/_libs\/index.pyx\", line 90, in pandas._libs.index.IndexEngine.get_value\n  File \"pandas\/_libs\/index.pyx\", line 138, in pandas._libs.index.IndexEngine.get_loc\n  File \"pandas\/_libs\/hashtable_class_helper.pxi\", line 998, in pandas._libs.hashtable.Int64HashTable.get_item\n  File \"pandas\/_libs\/hashtable_class_helper.pxi\", line 1005, in pandas._libs.hashtable.Int64HashTable.get_item\nKeyError: 36\n<\/code>\nCan anyone advise why this is happening?\n<code class=\"lang-auto\">from google.colab import drive\ndrive.mount('\/content\/drive')\n\nimport torch\nimport torchvision\nimport torch.optim as optim\nimport torch.nn as nn\nimport torchvision.transforms as transforms\nfrom torchvision import utils\nfrom  torch.utils.data import Dataset\n\nfrom sklearn.metrics import confusion_matrix\nfrom skimage import io, transform, data\nfrom skimage.color import rgb2gray\n\nimport matplotlib.pyplot as plt\nfrom tqdm import tqdm\nfrom PIL import Image\nimport pandas as pd\nimport numpy as np\nimport csv\nimport os\nimport math\nimport cv2\n\nroot_dir = \"\/content\/drive\/My Drive\/Read_Text\/5_Second_Segments\/\"\nclass_names = [\n  \"Parkinsons_Disease\",\n  \"Healthy_Control\"\n]\n\ndef get_meta(root_dir, dirs):\n    \"\"\" Fetches the meta data for all the images and assigns labels.\n    \"\"\"\n    paths, classes = [], []\n    for i, dir_ in enumerate(dirs):\n        for entry in os.scandir(root_dir + dir_):\n            if (entry.is_file()):\n                paths.append(entry.path)\n                classes.append(i)\n                \n    return paths, classes\n\n\npaths, classes = get_meta(root_dir, class_names)\n\ndata = {\n    'path': paths,\n    'class': classes\n}\n\ndata_df = pd.DataFrame(data, columns=['path', 'class'])\ndata_df = data_df.sample(frac=1).reset_index(drop=True) # Shuffles the data\n\nfrom pandas import option_context\n\nprint(\"Found\", len(data_df), \"images.\")\n\nwith option_context('display.max_colwidth', 400):\n    display(data_df.head(100))\n\nclass Audio(Dataset):\n\n    def __init__(self, df, transform=None):\n        \"\"\"\n        Args:\n            image_dir (string): Directory with all the images\n            df (DataFrame object): Dataframe containing the images, paths and classes\n            transform (callable, optional): Optional transform to be applied\n                on a sample.\n        \"\"\"\n        self.df = df\n        self.transform = transform\n\n    def __len__(self):\n        return len(self.df)\n\n    def __getitem__(self, index):\n        # Load image from path and get label\n        x = Image.open(self.df['path'][index])\n        try:\n          x = x.convert('RGB') # To deal with some grayscale images in the data\n        except:\n          pass\n        y = torch.tensor(int(self.df['class'][index]))\n\n        if self.transform:\n            x = self.transform(x)\n\n        return x, y\n\ndef compute_img_mean_std(image_paths):\n    \"\"\"\n        Author: . Computing the mean and std of three channel on the whole dataset,\n        first we should normalize the image from 0-255 to 0-1\n    \"\"\"\n\n    img_h, img_w = 224, 224\n    imgs = []\n    means, stdevs = [], []\n\n    for i in tqdm(range(len(image_paths))):\n        img = cv2.imread(image_paths[i])\n        img = cv2.resize(img, (img_h, img_w))\n        imgs.append(img)\n\n    imgs = np.stack(imgs, axis=3)\n    print(imgs.shape)\n\n    imgs = imgs.astype(np.float32) \/ 255.\n\n    for i in range(3):\n        pixels = imgs[:, :, i, :].ravel()  # resize to one row\n        means.append(np.mean(pixels))\n        stdevs.append(np.std(pixels))\n\n    means.reverse()  # BGR --> RGB\n    stdevs.reverse()\n\n    print(\"normMean = {}\".format(means))\n    print(\"normStd = {}\".format(stdevs))\n    return means, stdevs\n\nnorm_mean, norm_std = compute_img_mean_std(paths)\n\ndata_transform = transforms.Compose([\n        transforms.Resize(256),\n        transforms.CenterCrop(256),\n        transforms.ToTensor(),\n        transforms.Normalize(norm_mean, norm_std),\n    ])\n\nunique_users = data_df['path'].str[-20:-16].unique()\ntrain_users, test_users = np.split(np.random.permutation(unique_users), [int(0.8*len(unique_users))])\ndf_train = data_df[data_df['path'].str[-20:-16].isin(train_users)]\ntest_data_df = data_df[data_df['path'].str[-20:-16].isin(test_users)]\n\ntrain_unique_users = df_train['path'].str[-20:-16].unique()\ntrain_users, validate_users = np.split(np.random.permutation(train_unique_users), [int(0.875*len(train_unique_users))])\ntrain_data_df = df_train[df_train['path'].str[-20:-16].isin(train_users)]\nvalid_data_df = df_train[df_train['path'].str[-20:-16].isin(validate_users)]\n\nins_dataset_train = Audio(\n    df=train_data_df,\n    transform=data_transform,\n)\n\nins_dataset_valid = Audio(\n    df=valid_data_df,\n    transform=data_transform,\n)\n\nins_dataset_test = Audio(\n    df=test_data_df,\n    transform=data_transform,\n)\n\ntrain_loader = torch.utils.data.DataLoader(\n    ins_dataset_train,\n    batch_size=8,\n    shuffle=True,\n    num_workers=2\n)\n\ntest_loader = torch.utils.data.DataLoader(\n    ins_dataset_test,\n    batch_size=16,\n    shuffle=True,\n    num_workers=2\n)\n\nvalid_loader = torch.utils.data.DataLoader(\n    ins_dataset_valid,\n    batch_size=16,\n    shuffle=True,\n    num_workers=2\n)\n\n\/\/(This is where the error is occurring.)\nfor i, data in enumerate(valid_loader, 0):\n  images, labels = data\n  print(\"Batch\", i, \"size:\", len(images))\n<\/code>","y":"Can you try replacing\n<code class=\"lang-auto\">train_data_df = df_train[df_train['path'].str[-20:-16].isin(train_users)]\nvalid_data_df = df_train[df_train['path'].str[-20:-16].isin(validate_users)]\n<\/code>\nwith\n<code class=\"lang-auto\">train_data_df = df_train[df_train['path'].str[-20:-16].isin(train_users)].reset_index(drop=True)\nvalid_data_df = df_train[df_train['path'].str[-20:-16].isin(validate_users)].reset_index(drop=True)\n<\/code>","z":"Can you try replacing\n<code class=\"lang-auto\">train_data_df = df_train[df_train['path'].str[-20:-16].isin(train_users)]\nvalid_data_df = df_train[df_train['path'].str[-20:-16].isin(validate_users)]\n<\/code>\nwith\n<code class=\"lang-auto\">train_data_df = df_train[df_train['path'].str[-20:-16].isin(train_users)].reset_index(drop=True)\nvalid_data_df = df_train[df_train['path'].str[-20:-16].isin(validate_users)].reset_index(drop=True)\n<\/code>\nThank you! That\u2019s solved my problem! "},{"x":"<code class=\"lang-auto\">    since = time.time()\n\n    best_model_wts = copy.deepcopy(model.state_dict())\n    best_acc = 0.0\n\n    for epoch in range(num_epochs):\n        print('Epoch {}\/{}'.format(epoch, num_epochs - 1))\n        print('-' * 10)\n\n        # Each epoch has a training and validation phase\n        for phase in ['train', 'val']:\n            if phase == 'train':\n                model.train()  # Set model to training mode\n            else:\n                model.eval()   # Set model to evaluate mode\n\n            running_loss = 0.0\n            running_corrects = 0\n\n            # Iterate over data.\n            for inputs, labels in dataloaders[phase]:\n                inputs = inputs.to(device)\n                labels = labels.to(device)\n\n                # zero the parameter gradients\n                optimizer.zero_grad()\n\n                # forward\n                # track history if only in train\n                with torch.set_grad_enabled(phase == 'train'):\n                    outputs = model(inputs)\n                    print(labels)\n                    # _, preds = torch.max(outputs, 1)\n                    loss = criterion(outputs, labels)\n\n                    # backward + optimize only if in training phase\n                    if phase == 'train':\n                        loss.backward()\n                        optimizer.step()\n\n                # statistics\n                running_loss += loss.item() * inputs.size(0)\n                running_corrects += torch.sum(preds == labels.data)\n            if phase == 'train':\n                scheduler.step()\n\n            epoch_loss = running_loss \/ dataset_sizes[phase]\n            epoch_acc = running_corrects.double() \/ dataset_sizes[phase]\n\n            print('{} Loss: {:.4f} Acc: {:.4f}'.format(\n                phase, epoch_loss, epoch_acc))\n\n            # deep copy the model\n            if phase == 'val' and epoch_acc > best_acc:\n                best_acc = epoch_acc\n                best_model_wts = copy.deepcopy(model.state_dict())\n\n        print()\n\n    time_elapsed = time.time() - since\n    print('Training complete in {:.0f}m {:.0f}s'.format(\n        time_elapsed \/\/ 60, time_elapsed % 60))\n    print('Best val Acc: {:4f}'.format(best_acc))\n\n    # load best model weights\n    model.load_state_dict(best_model_wts)\n    return model\n\n<\/code>\n<code class=\"lang-auto\">model_ft = models.resnet18(pretrained = True)\n\nnum_ftrs = model_ft.fc.in_features\n\nprint(num_ftrs)\n\n# Here the size of each output sample is set to 2.\n\n# Alternatively, it can be generalized to nn.Linear(num_ftrs, len(class_names))\n\nmodel_ft.fc = nn.Linear(num_ftrs,2)\n\nmodel_ft = model_ft.to(device)\n\ncriterion = nn.CrossEntropyLoss()\n\n# Observe that all parameters are being optimized\n\noptimizer_ft = optim.SGD(model_ft.parameters(), lr = 0.001, momentum = 0.9)\n\n#decay LR by a factor of 0.1 for every 7 epochs\n\nexp_lr_scheduler = lr_scheduler.StepLR(optimizer_ft, step_size=7, gamma = 0.1)\n<\/code>\nI am trying to train a classifier to classify sign language, my dataset has 28 classes but I am getting the following error, I am new to deep learning, can someone please gives a clue on how to solve it\n<code class=\"lang-auto\">   2220         ret = torch._C._nn.nll_loss2d(input, target, weight, _Reduction.get_enum(reduction), ignore_index)\n\nIndexError: Target 13 is out of bounds.<\/code>","y":"If seems you\u2019ve initialized the model to output only 2 classes instead of 28:\n<code class=\"lang-python\">model_ft.fc = nn.Linear(num_ftrs,2)\n<\/code>\nso you might want to change the out_features to 28 for the last linear layer. ","z":"If seems you\u2019ve initialized the model to output only 2 classes instead of 28:\n<code class=\"lang-python\">model_ft.fc = nn.Linear(num_ftrs,2)\n<\/code>\nso you might want to change the out_features to 28 for the last linear layer. \nThanks it worked "},{"x":"I have Pytorch 1.6 installed and according to this link https:\/\/pytorch.org\/docs\/stable\/mobile_optimizer.html, there is a mobile optimizer util. But, I cannot find it. I get the AttributeError: module 'torch.utils' has no attribute 'mobile_optimizer' when I try torch.utils.mobile_optimizer.optimize_for_mobile(traced_model)","y":"Can you try\n<code class=\"lang-auto\">import torch.utils.mobile_optimizer as mobile_optimizer\nmobile_optimizer.optimize_for_mobile(traced_model)\n<\/code>","z":"Can you try\n<code class=\"lang-auto\">import torch.utils.mobile_optimizer as mobile_optimizer\nmobile_optimizer.optimize_for_mobile(traced_model)\n<\/code>"},{"x":"I am training an unsupervised CNN encoder decoder network for dense output (optical flow). The network trains perfectly fine in version 1.4 and gives proper output. However, in Pytorch 1.6, with the same configuration, the trained network gives constant output (one value in the whole image)","y":"Well, first I thought the problem was in distributed training. So, I disabled that and ran on single GPU. Then I thought it was the network architecture. I tried other architectures but same issue.\nFinally, I concluded that this is not because of any of the above but because of the L2 regularization I was using with the Adam optimizer. I was using weight decay with 0.0005 as the weight but I think it was too large weight.","z":"Hi,\nIt is very hard to say without more information. There are almost ten thousand commits between 1.4 and 1.6 \nCan you explain what you did change when doing the upgrade?\nWhat have you tried to identify the difference?\nHow stable was the training in 1.4?\nWell, first I thought the problem was in distributed training. So, I disabled that and ran on single GPU. Then I thought it was the network architecture. I tried other architectures but same issue.\nFinally, I concluded that this is not because of any of the above but because of the L2 regularization I was using with the Adam optimizer. I was using weight decay with 0.0005 as the weight but I think it was too large weight."},{"x":"Hi folks,\nI was trying to re-run the CMM\/DM fine-tuning example by following the instructions from the repo \u2013 https:\/\/github.com\/pytorch\/fairseq\/blob\/master\/examples\/bart\/README.summarization.md#4-fine-tuning-on-cnn-dm-summarization-task\nWhen I run the command, (Point #4 in the link), it starts the training loop, prints the progress bar, but then fails with an OS Error.\n<code class=\"lang-auto\">epoch 001:   0%|                                                                                                                            | 0\/29399 [00:00<?, ?it\/s]2020-08-21 05:36:29 | INFO | fairseq.trainer | begin training epoch 1\nTraceback (most recent call last):\n  File \"\/projects\/anaconda3\/envs\/py36-fairseq\/lib\/python3.6\/multiprocessing\/queues.py\", line 234, in _feed\n    obj = _ForkingPickler.dumps(obj)\n  File \"\/projects\/anaconda3\/envs\/py36-fairseq\/lib\/python3.6\/multiprocessing\/reduction.py\", line 51, in dumps\n    cls(buf, protocol).dump(obj)\n  File \"\/projects\/anaconda3\/envs\/py36-fairseq\/lib\/python3.6\/site-packages\/torch\/multiprocessing\/reductions.py\", line 322, in reduce_storage\n    df = multiprocessing.reduction.DupFd(fd)\n  File \"\/projects\/anaconda3\/envs\/py36-fairseq\/lib\/python3.6\/multiprocessing\/reduction.py\", line 191, in DupFd\n    return resource_sharer.DupFd(fd)\n  File \"\/projects\/anaconda3\/envs\/py36-fairseq\/lib\/python3.6\/multiprocessing\/resource_sharer.py\", line 53, in __init__\n    self._id = _resource_sharer.register(send, close)\n  File \"\/projects\/anaconda3\/envs\/py36-fairseq\/lib\/python3.6\/multiprocessing\/resource_sharer.py\", line 77, in register\n    self._start()\n  File \"\/projects\/anaconda3\/envs\/py36-fairseq\/lib\/python3.6\/multiprocessing\/resource_sharer.py\", line 130, in _start\n    self._listener = Listener(authkey=process.current_process().authkey)\n  File \"\/projects\/anaconda3\/envs\/py36-fairseq\/lib\/python3.6\/multiprocessing\/connection.py\", line 438, in __init__\n    self._listener = SocketListener(address, family, backlog)\n  File \"\/projects\/anaconda3\/envs\/py36-fairseq\/lib\/python3.6\/multiprocessing\/connection.py\", line 576, in __init__\n    self._socket.bind(address)\nOSError: AF_UNIX path too long\n<\/code>\nCouldn\u2019t find anything related to this online (except that it\u2019s a Python error raised when the address path exceeds the UNIX limit).\nWas wondering if anyone has encountered something similar.\nThanks.","y":"Thanks for replying  .\nThere were some other issues with my hardware \u2013 it was running out of disk space.\nSolving that also solved this issue.","z":"Are you getting this error only when using the fairseq repository or also if you are using plain PyTorch code?\nIn the former case, I would recommend to create an issue in the fairseq repository, in the latter case, could you post an executable code snippet, so that we could reproduce this issue?\nThanks for replying  .\nThere were some other issues with my hardware \u2013 it was running out of disk space.\nSolving that also solved this issue."},{"x":"I recently know inside torch.manual_seed(seed), it calls torch.cuda.manual_seed_all(seed), which sets the seed for generating random numbers on all GPUs.\nThis brings me a question.\nIf I launch program A (use torch.manual_seed(3)) on cuda:0 and then program B (use torch.manual_seed(5)) on cuda:1, (Both are on the same machine), will program A end up in using seed 3 or 5 ?","y":"If both scripts are started independently (e.g. in other terminals), the seeds shouldn\u2019t have any influence, as the python programs won\u2019t interact with each other.","z":"If both scripts are started independently (e.g. in other terminals), the seeds shouldn\u2019t have any influence, as the python programs won\u2019t interact with each other."},{"x":"I understood that we need to move to gpu, but why do we need to move to GPU it\u2019s not the case with keras and all right?\nI am learning to to transfer learning from a tutorial and I found this.","y":"Keras might transfer the model parameters and data automatically for you behind the scenes, while you are responsible to do it in PyTorch.\nI guess it comes down to balance \u201cconvenience vs. flexibility\u201d and I personally like to manually specify which tensor is places in which device, which e.g. enables models sharing easily.","z":"Keras might transfer the model parameters and data automatically for you behind the scenes, while you are responsible to do it in PyTorch.\nI guess it comes down to balance \u201cconvenience vs. flexibility\u201d and I personally like to manually specify which tensor is places in which device, which e.g. enables models sharing easily."},{"x":"Hello,\nI\u2019m currently working on a NLP multi-class classification problem, where I have an unbalanced dataset.\nAfter some research, I found out that using WeightedRandomSampler, I could avoid the problem of always having the same biggest class being trained and predicted over and over again, with only sometimes other classes showing up.\n( this wrong training leads to Val Loss going up, Train Loss and a high Val Accuracy going Down)\nMy question is the following: Is it correct\/fair to apply the WeightedRandomSampler in the TEST and validation datasets? Or it shouldn`t make any difference at all?\nHere is how I do my dataset\n<code class=\"lang-auto\">from torch.utils.data import WeightedRandomSampler\n\n# create dataset from numpy\ntrain_dataset = TensorDataset(tensor_x_train,tensor_y_train) \nvalid_dataset = TensorDataset(tensor_x_valid,tensor_y_valid) \ntest_dataset = TensorDataset(tensor_x_test,tensor_y_test) \n\n# Calculates weights of each SAMPLE\nweights = 1. \/ torch.tensor(class_sample_count, dtype=torch.float)\nweights = weights.double()\n\n# Apply weights to each sample\nsample_weight_train = weights[tensor_y_train]\nsample_weight_test =weights[tensor_y_test]\nsample_weight_val =weights[tensor_y_valid]\n\n# create samplers\nsampler_train = WeightedRandomSampler(\n                              weights=sample_weight_train,\n                              num_samples=len(sample_weight_train),\n                              replacement=True)\nsampler_test = WeightedRandomSampler(\n                              weights=sample_weight_test,\n                              num_samples=len(sample_weight_test))\n\nsampler_val = WeightedRandomSampler(\n                              weights=sample_weight_val,\n                              num_samples=len(sample_weight_val))\n\n# create Dataloader\ntrain_dataloader = DataLoader(train_dataset,\n                              batch_size=BATCH_SIZE,\n                              sampler = sampler_train)\nvalid_dataloader = DataLoader(valid_dataset,\n                              batch_size=BATCH_SIZE,\n                             sampler = sampler_val) \ntest_dataloader = DataLoader(test_dataset,\n                             batch_size=BATCH_SIZE,\n                            sampler = sampler_test) \n<\/code>\nNow this code is how I measure the accuracy of my model in TEST set.\n<code class=\"lang-auto\">with torch.no_grad():\n      for x_test, y_test in test_dataloader:\n    \n        y_pred = model(x_test)\n\n        loss = criterion(y_pred, y_test)\n        acc = binary_accuracy(y_pred, y_test) # Will return a number from 0.0 to 1.1\n\n        epoch_loss += loss.item()\n        epoch_acc += acc.item()\n\nprint(\"LOSS: \", epoch_loss\/len(val_iter))\nprint(\"ACC: \",epoch_acc\/len(val_iter))\n<\/code>","y":"\n\n\n pinazawa123:\n\nMy question is the following: Is it correct\/fair to apply the WeightedRandomSampler in the TEST and validation datasets? Or it shouldn`t make any difference at all?\n\n\nI don\u2019t think applying a weighted sampling to the validation and test data would be a valid step.\nIn my opinion the validation dataset is a proxy of the test dataset and should give you a signal how well your model would perform an new unseen data and can be used for e.g. early stopping.\nOnce you\u2019ve finished training the model and think the performance is good enough you would run a final iteration on the test dataset to get the model performance on completely new real world data.\nIf your future task (e.g. when deploying the model) is also dealing with imbalanced data, which should be the case otherwise the test data wouldn\u2019t be imbalanced, I would recommend to leave these datasets as they are.\nThat being said, you might need to use another metric than the accuracy to measure the performance of your model due to the Accuracy Paradox with link \"https:\/\/en.wikipedia.org\/wiki\/Accuracy_paradox\".","z":" Could you please give me your opinion about this?\nThanks\n\n\n\n pinazawa123:\n\nMy question is the following: Is it correct\/fair to apply the WeightedRandomSampler in the TEST and validation datasets? Or it shouldn`t make any difference at all?\n\n\nI don\u2019t think applying a weighted sampling to the validation and test data would be a valid step.\nIn my opinion the validation dataset is a proxy of the test dataset and should give you a signal how well your model would perform an new unseen data and can be used for e.g. early stopping.\nOnce you\u2019ve finished training the model and think the performance is good enough you would run a final iteration on the test dataset to get the model performance on completely new real world data.\nIf your future task (e.g. when deploying the model) is also dealing with imbalanced data, which should be the case otherwise the test data wouldn\u2019t be imbalanced, I would recommend to leave these datasets as they are.\nThat being said, you might need to use another metric than the accuracy to measure the performance of your model due to the Accuracy Paradox with link \"https:\/\/en.wikipedia.org\/wiki\/Accuracy_paradox\".\nThanks! I will take a look at some other metrics too."},{"x":"Hi,\nTLDR; \u201cConv2d uses tensor cores, Conv3D doesn\u2019t when using apex AMP or FP16\u201d\nAccording to NVIDIA, cudnn 8 does support tensor core operations for 3D convolutions.\nTo be sure if tensor cores are really being used (HMMA instructions) I am checking this with the nvidia profiler with the sm__inst_executed_pipe_tensor_op_hmma.sum metrics activated (See https:\/\/developer.nvidia.com\/blog\/using-nsight-compute-nvprof-mixed-precision-deep-learning-models\/)\nI am really up to date by building pytorch from source:\nMy setup:\nPyTorch: 1.7.0a\ncudnn: 8.0.1\nCUDA: 10.2\nGPU: RTX 2080 Ti\nSimple test script:\n<code class=\"lang-auto\">import torch\nfrom torch import nn\nimport torch.nn.functional as F\nfrom time import time\nimport logging\nfrom apex import amp\n\nclass Net(nn.Module):\n    def __init__(self):\n        super(Net, self).__init__()\n        self.conv = nn.Conv3d(64, 64, 5, padding=2)\n        self.pool = nn.MaxPool3d(2, 2)\n        \n    def forward(self, x):\n        x = self.pool(F.relu(self.conv(x)))\n        return x\n\n\nif __name__ == '__main__':\n    net = Net().to(\"cuda\")\n    amp_net = amp.initialize(net, opt_level=\"O1\")\n\n    input_image = torch.rand((8, 64, 64, 64, 64), device=\"cuda\")\n    gpu_net = amp_net\n\n    gpu_net(input_image)\n<\/code>\nBehavior:\nIf I change the network to 2D input and 2D convolutoins, I can see that tensorcores are used in the metrics and that there is a speed-up.\nFor 3D they are not being used.\nEDIT: Instead of using AMP, I also tried converting the net and inputs to FP16, which also worked in the 2D case but not in 3D\nMaybe  can help?","y":"Solution:\nVerified that this is a bug inside cudnn 8.0.1 via official bug report.\nBuilding torch from source with cudnn 8.0.2 fixes the problem.","z":"Thanks for the code snippet. I\u2019ll try to grab a node tomorrow and profile it with the latest cudnn8 version, as the release candidate (8.0.1) might not pick the right kernels.\nThanks, sounds great. The profiler tells me that it\u2019s using the following kernel for convolution:\nvoid implicit convolveNd sgemm\nQuick update:\nI verified that TensorCores are used for this workload in the latest cudnn version on a V100.\nUnfortunately, I couldn\u2019t get an RTX2080Ti node yet and will try it tomorrow again, sorry. \nOk that\u2019s weird, can you let me know which exact versions of cudnn, torch, CUDA, you used?\nWas it the same test code?\nBtw: The final production code will run on a T4 card. I haven\u2019t tried to verify tensor-op use on that GPU though.\nIn the meantime, I tried to use cudnn from C++ directly to verify Tensor Cores work in general on this computer and setup.\nI was able to run the cudnnConvolutionForward cudnn method with 3D image and filter and the profiler confirms it\u2019s using the xmma new::gemm::kernel which makes extensive use of tensor cores as I can see from the metrics.\nSo in general it should work. Haven\u2019t managed from pytorch though\u2026\nI used internal CUDA and cudnn versions for this test.\nCould you post the complete model architecture (if possible) or at least some more Conv3d workloads so that I could profile them and make sure that TensorCores are used?\nHi,\nI cannot share the complete (and rather complex) model but I extended the minimal example from above a bit to make the effect more obvious (it\u2019s attached below).\nI am not sure about the padding though, I would assume the input shape dimensions need to be divisable by 8, hence I first use a normal conv on the 64x64 input then use padded conv.\nBut I also tried other combinations\u2026\nRunning the code in 2D mode I get:\n<code class=\"lang-auto\">2020-07-27 08:12:32,971 - Executing 2D Test\n2020-07-27 08:12:36,583 - FP32 duration: 2.420s\n2020-07-27 08:12:37,398 - FP16 duration: 0.814s\n2020-07-27 08:12:38,229 - AMP duration: 0.828s\n<\/code>\nWhen profiling, I set the rounds to 1 to make the report more readable (and because profiling is slow).\nThe profiler shows tensor core usage for FP16 and AMP part\nRunning in 3D mode:\n<code class=\"lang-auto\">2020-07-27 08:25:13,797 - Executing 3D Test\n2020-07-27 08:25:17,523 - FP32 duration: 2.203s\n2020-07-27 08:25:21,319 - FP16 duration: 3.790s\n2020-07-27 08:25:25,137 - AMP duration: 3.811s\n<\/code>\nFor 3D, the profiler doesn\u2019t show any tensor core usage.\nI used the nv-nsight-cu GUI profiler and configured it so that it calls the command line profiler with the following arguments: \/opt\/nvidia\/nsight-compute\/2019.5.0\/target\/linux-desktop-glibc_2_11_3-x64\/nv-nsight-cu-cli --export \"report\" --force-overwrite --target-processes all --kernel-regex-base function --launch-skip-before-match 0 --section LaunchStats --section Occupancy --section SpeedOfLight --sampling-interval auto --sampling-max-passes 5 --sampling-buffer-size 33554432 --profile-from-start 1 --cache-control all --clock-control base --apply-rules --metrics sm__inst_executed_pipe_tensor_op_hmma.sum \"conda\/envs\/pytorchcudnn8\/bin\/python\"  torch-try-tensorcores_minimal2d3d.py\nThe code:\n<code class=\"lang-auto\">import torch\nfrom torch import nn\nimport torch.nn.functional as F\nfrom time import time\nimport logging\nfrom apex import amp\n\n\nclass Net(nn.Module):\n    def __init__(self):\n        super(Net, self).__init__()\n        self.conv = nn_conv(64, 64, 5)\n        self.conv_padded = nn_conv(64, 64, 5, padding=2)\n        self.pool = nn_pool(2, 2)\n\n    def forward(self, x):\n        x = F.relu(self.conv(x))\n        for i in range(5):\n            x = F.relu(self.conv_padded(x))\n        return x\n\n\nif __name__ == \"__main__\":\n    logging.basicConfig(format=\"%(asctime)s - %(message)s\", level=logging.DEBUG)\n    # === CONFIGURATION ===\n    dimensionality = 2  # 2 for 2D, 3 for 3D\n\n    # === TEST CODE ===\n    if dimensionality == 2:\n        logging.info(f\"Executing 2D Test\")\n        rounds = 500\n        nn_conv = nn.Conv2d\n        nn_pool = nn.MaxPool2d\n        image_dims = (8, 64, 64, 64)\n    else:\n        logging.info(f\"Executing 3D Test\")\n        rounds = 2\n        nn_conv = nn.Conv3d\n        nn_pool = nn.MaxPool3d\n        image_dims = (8, 64, 64, 64, 64)\n\n    input_image = torch.rand(image_dims, device=\"cuda\")\n\n    # Run in FP32 mode\n    net = Net().to(\"cuda\").to(torch.float32)\n    start = time()\n    for i in range(rounds):\n        net(input_image)\n    torch.cuda.synchronize()\n    logging.info(f\"FP32 duration: {time() - start:.03f}s\")\n\n    # Run in FP16 mode\n    net = Net().to(\"cuda\").to(torch.float32)\n    fp16_net = net.to(torch.float16)\n    fp16_input = input_image.to(torch.float16)\n    start = time()\n    for i in range(rounds):\n        fp16_net(fp16_input)\n    torch.cuda.synchronize()\n    logging.info(f\"FP16 duration: {time() - start:.03f}s\")\n\n    # Run with AMP\n    net = Net().to(\"cuda\").to(torch.float32)\n    amp_net = amp.initialize(net, opt_level=\"O1\", verbosity=0)\n    start = time()\n    for i in range(rounds):\n        amp_net(input_image)\n    torch.cuda.synchronize()\n    logging.info(f\"AMP duration: {time() - start:.03f}s\")\n<\/code>\nIf you have been able to make it work for 3D, let me know how so I can try to reproduce it \nThat\u2019s helpful! Thanks for the code. I\u2019ll profile it and check, if TCs are used or not.\n: I debugged into pytorch source code to find the reason and I traced it down to the cudnn statement where the convolution descriptor is populated:\naten\/src\/ATen\/cudnn\/Descriptors.h:170\ncudnnSetConvolutionNdDescriptor(mut_desc(), dim, pad, stride, upscale, CUDNN_CROSS_CORRELATION, mathType);\n(https:\/\/github.com\/pytorch\/pytorch\/blob\/master\/aten\/src\/ATen\/cudnn\/Descriptors.h#L170)\nWhen changing the mode from CUDNN_CROSS_CORRELATION to CUDNN_CONVOLUTION, it will work and it will use tensor cores. I assume this is a bug inside cudnn, since both modes should work equally well, just like they do in 2D.\nI verified this effect using my completely separate C++ code which uses cudnn directly. Just by changing the convolution mode, I can see tensor cores turning off or on, keeping all other settings.\nEDIT: So when I used my custom pytorch version with CUDNN_CONVOLUTION, I still needed to modify the above example code slightly, because the convolution workspace was to large. But if you change the batch size from 8 to 1 it should work.\nSolution:\nVerified that this is a bug inside cudnn 8.0.1 via official bug report.\nBuilding torch from source with cudnn 8.0.2 fixes the problem."},{"x":"I have variable length data and want to pack it to batches with size of max sample len in batch repeating shorter samples.\nFor example like this\n[[0, 1, 2, 3, 4], [0, 1, 2]] => [[0, 1, 2, 3, 4], [0, 1, 2, 0, 1]]","y":"Oh sorry, I apparently missed the most important part of the question.\nI\u2019m not sure, if there is a function for this, but this code snippet should work:\n<code class=\"lang-python\">x = [torch.tensor([0, 1, 2, 3, 4]), torch.tensor([0, 1, 2])]\nmax_len = max([t.size(0) for t in x])\nres = [torch.cat((t, t[:max_len-t.size(0)])) for t in x]\n<\/code>","z":"You could use some rnn util functions:\n<code class=\"lang-python\">x = [torch.tensor([0, 1, 2, 3, 4]), torch.tensor([0, 1, 2])]\nx = torch.nn.utils.rnn.pack_sequence(x)\nout = torch.nn.utils.rnn.pad_packed_sequence(x, batch_first=True)\nprint(out)\n> (tensor([[0, 1, 2, 3, 4],\n        [0, 1, 2, 0, 0]]), tensor([5, 3]))\n<\/code>\nWhere the first return value will be the padded tensor, while the second will give you the lengths before padding.\nBut this is zero padding, not repeat padding.\nOh sorry, I apparently missed the most important part of the question.\nI\u2019m not sure, if there is a function for this, but this code snippet should work:\n<code class=\"lang-python\">x = [torch.tensor([0, 1, 2, 3, 4]), torch.tensor([0, 1, 2])]\nmax_len = max([t.size(0) for t in x])\nres = [torch.cat((t, t[:max_len-t.size(0)])) for t in x]\n<\/code>\nWith repeat padding my attention even worse since where is now non zero data in padding.\n with link \"https:\/\/i.imgur.com\/gZVF0K7.png\"\n\nShould you advise how to perform padding in query in multi head attention?\nI tried to make mask, but got nan in softmax.\nCould you explain your use case a bit more regarding the NaN output in softmax?\nWell, i wanted to mask attentions also by query axis.\nBut with default attn_mask setup it case nans.\nGoogle say, its because only -infs in axis.\nNow i edited source code of multi head attention forward like this:\nif attn_mask is not None:\n    attn_output_weights = attn_output_weights.view(bsz, num_heads, tgt_len, src_len)\n    attn_output_weights = attn_output_weights.masked_fill(attn_mask, 1e-9)\n    attn_output_weights = attn_output_weights.view(bsz * num_heads, tgt_len, src_len)\n\nattn_output_weights = softmax(\n    attn_output_weights, dim=-1)\n\nAnd made masks like this\ndef get_mask_from_lengths_3d(batch_size, lengths_query, lengths_key, nheads):\n    mask = torch.zeros(batch_size, lengths_key.max(),\n                    lengths_query.max()).cuda()\n\n    max_len = torch.max(lengths_key).item()\n    ids = torch.arange(0, max_len, out=torch.cuda.LongTensor(max_len))\n    mask[ids > lengths_key.unsqueeze(1) - 1] = 1\n\n    mask = mask.transpose(1, 2)\n\n    max_len = torch.max(lengths_query).item()\n    ids = torch.arange(0, max_len, out=torch.cuda.LongTensor(max_len))\n    mask[ids > lengths_query.unsqueeze(1) - 1] = 1\n\n    return mask.unsqueeze(1).repeat(1, nheads, 1, 1).bool()\n\n\ndef generate_square_subsequent_mask_3d(batch_size, lengths_query, nheads):\n    sz = lengths_query.max().item()\n    mask = torch.triu(torch.ones(sz, sz), 1).cuda(\n    ).unsqueeze(0).repeat(batch_size, 1, 1)\n\n    ids = torch.arange(0, sz, out=torch.cuda.LongTensor(sz))\n    mask[ids > lengths_query.unsqueeze(1) - 1] = 1\n\n    return mask.unsqueeze(1).repeat(1, nheads, 1, 1).bool()\n\nAlignment for one layer seems to be right\n  with link \"https:\/\/i.imgur.com\/rAKLJAa.png\"\n\nWith mask value float(-inf) it became nan immediately.\nHow should i zero pad 2d data?\nout = {}\nx = [torch.randn(10, 10), torch.randn(5, 5)]\nx = torch.nn.utils.rnn.pack_sequence(x, enforce_sorted=False)\nout = torch.nn.utils.rnn.pad_packed_sequence(x, batch_first=True)\n\n\nRuntimeError: The expanded size of the tensor (10) must match the existing size (5) at non-singleton dimension 1.  Target sizes: [5, 10].  Tensor sizes: [5, 5]\n\nIn your example dim1 should be equal, so you could pad the second tensor with F.pad:\n<code class=\"lang-python\">import torch.nn.functional as F\nF.pad(torch.randn(5, 5), (2, 3, 0, 0))\n<\/code>\nNote that I\u2019ve used a padding of 2 and 3 for the \u201cleft\u201d and \u201cright\u201d side of dim1, but you could of course also only pad on one side with 5 values or chose any other valid configuration.\n\n\n\n ptrblck:\n\nOh sorry, I apparently missed the most important part of the question.\nI\u2019m not sure, if there is a function for this, but this code snippet should work:\n<code class=\"lang-auto\">x = [torch.tensor([0, 1, 2, 3, 4]), torch.tensor([0, 1, 2])]\nmax_len = max([t.size(0) for t in x])\nres = [torch.cat((t, t[:max_len-t.size(0)])) for t in x]\n<\/code>\n\n\nI finally tried your snippet, but it do not work if one sample x2 longer then another"},{"x":"I have a problem where I need to index a Variable that requires gradient using a ByteTensor. However, this seems to be an in-place operation, and so PyTorch throws an error:\n<code class=\"lang-auto\">RuntimeError: a leaf Variable that requires grad has been used in an in-place operation.\n<\/code>\nThe following piece of code reproduces the error in a very simple way:\n<code class=\"lang-auto\">import torch\nfrom torch.autograd import Variable\n\nx = torch.LongTensor([0,1,0,1,1,0])\ny = Variable(torch.zeros(x.size()), requires_grad=True)\n\ny[x == 0] = Variable(torch.Tensor([-5]))\n\nprint('y',y)\n<\/code>\nClearly, the intended output would be:\n<code class=\"lang-auto\">y Variable containing:\n-5\n 0\n-5\n 0\n 0\n-5\n[torch.FloatTensor of size (6,)]\n<\/code>\nAs a workaround, we can use an explicit for-loop over the elements of x:\n<code class=\"lang-auto\">import torch\nfrom torch.autograd import Variable\n\nx = torch.LongTensor([0,1,0,1,1,0])\ny = Variable(torch.zeros(x.size()), requires_grad=True)\n\nfor i in range(x.size(0)):\n  if x[i] == 0:\n    y[i] = Variable(torch.Tensor([-5]))\n\nprint('y',y)\n<\/code>\nHere, y[i] = Variable(torch.Tensor([-5])) is not an in-place operation (don\u2019t know why) and therefore it works. However, this for-loop is very inefficient, taking a lot of time if x has a large dimension.\nAre there any alternatives? Thanks in advance.","y":"Hi,\nSince gradients are needed for y itself, your can\u2019t modify it inplace.\nWhat you can do is clone it before, that way the cloned version will not be a leaf Variable and can thus be changed inplace:\n<code class=\"lang-auto\">import torch\nfrom torch.autograd import Variable\n\nx = torch.LongTensor([0,1,0,1,1,0])\ny = Variable(torch.zeros(x.size()), requires_grad=True)\n\ntmp = y.clone()\ntmp[x == 0] = Variable(torch.Tensor([-5]))\n\nprint('y',y)\nprint('tmp',tmp)\n<\/code>","z":"Hi,\nSince gradients are needed for y itself, your can\u2019t modify it inplace.\nWhat you can do is clone it before, that way the cloned version will not be a leaf Variable and can thus be changed inplace:\n<code class=\"lang-auto\">import torch\nfrom torch.autograd import Variable\n\nx = torch.LongTensor([0,1,0,1,1,0])\ny = Variable(torch.zeros(x.size()), requires_grad=True)\n\ntmp = y.clone()\ntmp[x == 0] = Variable(torch.Tensor([-5]))\n\nprint('y',y)\nprint('tmp',tmp)\n<\/code>\nHi,\nThank you for your reply!\nThat\u2019s a pretty weird behavior\u2026 Just for curiosity, why can\u2019t the leaf variables be changed in-place?\nBecause a gradient update for a leaf Variable a is (with extra terms if you use fancy optimizer) a = a + lambda * a.grad. As you can see, you need the original value of a to be able to perform this update. This is why you cannot change this value inplace, otherwise, you couldn\u2019t do this operation anymore since you would have lost the original a.\nI thought PyTorch would not perform gradient updates on leaf Variables, but only on model Parameters. Isn\u2019t that right?\nParameter comes from the nn package and is just a convenient tool to define parameters in the context of neural networks.\nFrom the autograd point of view, they are Variable with requires_grad=True (hence they are leaf Variables).\nThe optim package works with any Variable that has requires_grad=True, so you can use it without using nn  (and thus without using Parameters), but it is less convenient for neural network usage.\nBut for example, you can use pytorch to get numerical value of any mathematical function (even though this is not it\u2019s original purpose). The sample below for example show you how to implement a function that computes numerical gradients for any 1D function:\n<code class=\"lang-auto\">import torch\nfrom torch import autograd\nfrom torch.autograd import Variable\n\ndef square(x):\n    return x ** 2\ndef cube(x):\n    return x ** 3\n\n# Implement derivative of fn at point y (fn has to be R -> R)\ndef derivative(fn, y):\n    y.requires_grad = True\n    out = fn(y)\n    return autograd.grad(out, y)[0]\n\n\nx = Variable(torch.Tensor([3]))\n\nprint(\"Evaluate square at 3\")\nprint(square(x))\nprint(\"Evaluate derivative of square at 3 (should be 2*3)\")\nprint(derivative(square, x))\nprint(\"Evaluate derivative of cube at 3 (should be 3*(3)**2\")\nprint(derivative(cube, x))\n<\/code>\nYeah, it makes total sense  Thank you!\nHi Aldan,\nJust curious. The same rule would apply also to non-leaf variables. Then how come it is okay to do in-place operation for non-leaf variables? Or, are we running into any danger for doing in-place operations for non-leaf variables, even if there was no error message? Thanks.\nHi,\nThe difference for non-leafs, is that you don\u2019t mind loosing the reference to the original value. Because changing it inplace means that you actually just want the new version.\nWe do extensive checks to make sure that if you don\u2019t get any error, the gradients are correct. Note that in some cases, you get an error even though we could compute the gradients in theory, but we prefer to be over-restrictive here.\nSo if you don\u2019t get any error, it works fine "},{"x":"I planned to use a model made of conv3d layers for video action classification but struck on how to feed the frames which were extracted from videos to the conv3d layers.","y":"Conv3D wants in the form of batch_size x channels x depth x height x width. Your number of frames is the depth. Your channels can be RGB or whatever information you have on your frames. You may need to use a library to process raw videos into this format.","z":"Conv3D wants in the form of batch_size x channels x depth x height x width. Your number of frames is the depth. Your channels can be RGB or whatever information you have on your frames. You may need to use a library to process raw videos into this format.\nTk u, can you suggest any libraries??"},{"x":"The code below calculates the MSE and MAE values but I have an issue where the values for MAE and MSE don\u2019t get stored at each epoch in store_MAE and store MSE. It seems to use the values of the last epoch only. Any idea what i need to do in the code to save the total values for each epoch i hope this make since\n<code class=\"lang-auto\">       # logging:\n        loss = torch.mean((preds - targets)**2)\n        count_error = torch.abs(preds - targets).mean()\n        mean_test_error += count_error\n        writer.add_scalar('test_loss', loss.item(), global_step=global_step)\n        writer.add_scalar('test_count_error', count_error.item(), global_step=global_step)\n        \n        global_step += 1\n        store_MAE = 0\n        store_MSE = 0\n        mean_test_error = mean_test_error \/ len(loader_test)\n        store_MAE += mean_test_error\n        mse = math.sqrt(loss \/ len(loader_test))\n        store_MSE +=mse\n        print(\"Test count error: %f\" % mean_test_error)\n        print(\"MSE: %f\" % mse)\n\n    if mean_test_error < best_test_error:\n        best_test_error = mean_test_error\n        torch.save({'state_dict':model.state_dict(),\n                    'optimizer_state_dict':optimizer.state_dict(),\n                    'globalStep':global_step,\n                    'train_paths':dataset_train.files,\n                    'test_paths':dataset_test.files},checkpoint_path)\n\nprint(\"MAE Total: %f\" % store_MAE)\nprint(\"MSE Total: %f\" % store_MSE)\nmodel_mae= store_MAE \/epoch\nmodel_mse= store_MSE \/epoch\nprint(\"Model MAE: %f\" % model_mae)\nprint(\"Model MSE: %f\" % model_mse)<\/code>","y":"It seems you are reinitializing store_MAE and store_MSE inside the logging loop?\nCould you move this code\n<code class=\"lang-python\">        store_MAE = 0\n        store_MSE = 0\n<\/code>\nbefore executing the loop?","z":"It seems you are reinitializing store_MAE and store_MSE inside the logging loop?\nCould you move this code\n<code class=\"lang-python\">        store_MAE = 0\n        store_MSE = 0\n<\/code>\nbefore executing the loop?\nMany Thanks for your help!"},{"x":"Hi all,\n2 questions from me regarding tensorboard:\n1)\nI previously got tensorboard to run but after killing it via the windows command prompt, am unable to restart it again. How can I restart it successfully without running into \u2019 localhost refused to connect.\/\n2)\nAlso, would like to delete history on tensorboard. I understand from some forums the directory has to be deleted + process killed. Not too sure how the former works, would appreciate any advice on this.\nWhere can I find the directory? Within the tensorboard package in windows explorer?\nThanks","y":"An update for anyone who might find themselves with the same problem:\nI decided to end my misery over tensorboard and have switched allegiance to visdom - it\u2019s been an absolute breeze.\nLinking a very useful github which helped me quickly set up visdom for the plotting of accuracy and loss graphs:\n\n\n\nGitHub with link \"https:\/\/github.com\/noagarcia\/visdom-tutorial\"\n\n\n\nnoagarcia\/visdom-tutorial with link \"https:\/\/github.com\/noagarcia\/visdom-tutorial\"\nA simple tutorial to use Visdom to plot your PyTorch training graphs - noagarcia\/visdom-tutorial\n\n\n\n\n\n","z":"An update for anyone who might find themselves with the same problem:\nI decided to end my misery over tensorboard and have switched allegiance to visdom - it\u2019s been an absolute breeze.\nLinking a very useful github which helped me quickly set up visdom for the plotting of accuracy and loss graphs:\n\n\n\nGitHub with link \"https:\/\/github.com\/noagarcia\/visdom-tutorial\"\n\n\n\nnoagarcia\/visdom-tutorial with link \"https:\/\/github.com\/noagarcia\/visdom-tutorial\"\nA simple tutorial to use Visdom to plot your PyTorch training graphs - noagarcia\/visdom-tutorial\n\n\n\n\n\n"},{"x":"Consider the following piece of code to fetch a data set for training from torchvision.datasets and to create a DataLoader for it.\n<code class=\"lang-auto\">import torch\nfrom torchvision import datasets, transforms\n\ntraining_set_mnist = datasets.MNIST('.\/mnist_data', train=True, download=True)\ntrain_loader_mnist = torch.utils.data.DataLoader(training_set_mnist, batch_size=128,\n                                                 shuffle=True)\n<\/code>\nAssume that several Python processes have access to the folder .\/mnist_data and execute the above piece of code simultaneously; in my case, each process is a different machine on a cluster and the data set is stored in an NFS location accessible by everyone. You may also assume that the data is already downloaded in this folder so download=True should have no effect. Moreover, each process may use a different seed, as set by torch.manual_seed().\nI would like to know whether this scenario is allowed in PyTorch. My main concern is whether the above code can change the data folders or files in .\/mnist_data such that if ran by multiple processes it can potentially lead to unexpected behavior or other issues. Also, given that shuffle=True I would expect that if 2 or more processes try to create the DataLoader each of them will get a different shuffling of the data assuming that the seeds are different. Is this true?","y":"Since you are only reading the files, there shouldn\u2019t be any interactions between the processes.\nYes, different seeds should result in a different shuffling.","z":"Since you are only reading the files, there shouldn\u2019t be any interactions between the processes.\nYes, different seeds should result in a different shuffling."},{"x":"So I downloaded a trained model\u2019s weights and loaded it using torch.load(). But I am unable to directly run inference on it since it says it is unable to reference the class of the model. Keep in mind I am not loading the state dict of the model but the whole model itself. Why do I still need to explicitly define the model class name? Just curious","y":"I think this is a limitation of the Python pickle module e.g. as described here with link \"https:\/\/wiki.python.org\/moin\/UsingPickle\" at the bottom of the page.","z":"I think this is a limitation of the Python pickle module e.g. as described here with link \"https:\/\/wiki.python.org\/moin\/UsingPickle\" at the bottom of the page."},{"x":"I\u2019ve trained a CNN model and I would like to run the trained model against new data. However, it seems that the trained model isn\u2019t predicting the count correctly as it done during the training. I have a feeling that the model is not using the PTH file. Could someone please advise what I am doing wrong, please?\n<code class=\"lang-auto\">mport argparse\nimport datetime\nimport glob\nimport os\nimport random\nimport shutil\nimport time\nfrom os.path import join\n\nimport numpy as np\nimport pandas as pd\nimport torch\nimport torch.nn as nn\nfrom torch.utils.data import DataLoader\nfrom torch.utils.tensorboard import SummaryWriter\nfrom torchvision.transforms import ToTensor\nfrom tqdm import tqdm\nimport torch.optim as optim\n\nfrom convnet3_eval import Convnet\nfrom dataset2_eval import CellsDataset\n\nparser = argparse.ArgumentParser('Predicting hits from pixels')\nparser.add_argument('name',type=str,help='Name of experiment')\nparser.add_argument('data_dir',type=str,help='Path to data directory containing images and gt.csv')\nparser.add_argument('--weight_decay',type=float,default=0.0,help='Weight decay coefficient (something like 10^-5)')\nparser.add_argument('--lr',type=float,default=0.0001,help='Learning rate')\nargs = parser.parse_args()\n\nmetadata = pd.read_csv(join(args.data_dir,'gt.csv'))\nmetadata.set_index('filename', inplace=True)\n\n\ndataset = CellsDataset(args.data_dir,transform=ToTensor(),return_filenames=True)\ndataset = DataLoader(dataset,num_workers=4,pin_memory=True)\nmodel_path = '\/base_model.pth'\n\nmodel = Convnet()\noptimizer = torch.optim.Adam(model.parameters(),lr=args.lr,weight_decay=args.weight_decay)\n\nfor images, paths in tqdm(dataset):\n\n    targets = torch.tensor([metadata['count'][os.path.split(path)[-1]] for path in paths]) # B\n    targets = targets.float()\n\n    # code to print training data to a csv file\n    filename=CellsDataset(args.data_dir,transform=ToTensor(),return_filenames=True)\n    output = model(images) # B x 1 x 9 x 9 (analogous to a heatmap)\n    preds = output.sum(dim=[1,2,3]) # predicted cell counts (vector of length B)\n    print(preds)\n    paths_test = np.array([paths])\n    names_preds = np.hstack(paths)\n    print(names_preds)                \n    df=pd.DataFrame({'Image_Name':names_preds, 'Target':targets.detach(), 'Prediction':preds.detach()})\n    print(df) \n    # save image name, targets, and predictions\n    df.to_csv(r'model.csv', index=False, mode='a')\n\n\nmodel.load_state_dict(torch.load(model_path))\nmodel.eval()<\/code>","y":"I am assuming this is your code for testing the model whose parameters are saved in base_model.pth.\nSo I am guessing you want to run this:\n\n\n\n alameer:\n\n<code class=\"lang-auto\">for images, paths in tqdm(dataset):\n\n    targets = torch.tensor([metadata['count'][os.path.split(path)[-1]] for path in paths]) # B\n    targets = targets.float()\n\n    # code to print training data to a csv file\n    filename=CellsDataset(args.data_dir,transform=ToTensor(),return_filenames=True)\n    output = model(images) # B x 1 x 9 x 9 (analogous to a heatmap)\n    preds = output.sum(dim=[1,2,3]) # predicted cell counts (vector of length B)\n    print(preds)\n    paths_test = np.array([paths])\n    names_preds = np.hstack(paths)\n    print(names_preds)                \n    df=pd.DataFrame({'Image_Name':names_preds, 'Target':targets.detach(), 'Prediction':preds.detach()})\n    print(df) \n    # save image name, targets, and predictions\n    df.to_csv(r'model.csv', index=False, mode='a')\n<\/code>\n\n\nWith the parameters saved in this:\n\n\n\n alameer:\n\nmodel_path = '\/base_model.pth'\n\n\nBut you load your model parameters at the very end and do nothing with it:\n\n\n\n alameer:\n\nmodel.load_state_dict(torch.load(model_path))\n\n\nTry putting these two lines:\n\n\n\n alameer:\n\n<code class=\"lang-auto\">model.load_state_dict(torch.load(model_path))\nmodel.eval()\n<\/code>\n\n\nAfter this line:\n\n\n\n alameer:\n\nmodel = Convnet()\n\n\nAlso. I think you know this, but this would mean you are trying to load trained parameters into Convnet, to do so the parameters saved in \/base_model.pth should have also been training on said Convnet","z":"I am assuming this is your code for testing the model whose parameters are saved in base_model.pth.\nSo I am guessing you want to run this:\n\n\n\n alameer:\n\n<code class=\"lang-auto\">for images, paths in tqdm(dataset):\n\n    targets = torch.tensor([metadata['count'][os.path.split(path)[-1]] for path in paths]) # B\n    targets = targets.float()\n\n    # code to print training data to a csv file\n    filename=CellsDataset(args.data_dir,transform=ToTensor(),return_filenames=True)\n    output = model(images) # B x 1 x 9 x 9 (analogous to a heatmap)\n    preds = output.sum(dim=[1,2,3]) # predicted cell counts (vector of length B)\n    print(preds)\n    paths_test = np.array([paths])\n    names_preds = np.hstack(paths)\n    print(names_preds)                \n    df=pd.DataFrame({'Image_Name':names_preds, 'Target':targets.detach(), 'Prediction':preds.detach()})\n    print(df) \n    # save image name, targets, and predictions\n    df.to_csv(r'model.csv', index=False, mode='a')\n<\/code>\n\n\nWith the parameters saved in this:\n\n\n\n alameer:\n\nmodel_path = '\/base_model.pth'\n\n\nBut you load your model parameters at the very end and do nothing with it:\n\n\n\n alameer:\n\nmodel.load_state_dict(torch.load(model_path))\n\n\nTry putting these two lines:\n\n\n\n alameer:\n\n<code class=\"lang-auto\">model.load_state_dict(torch.load(model_path))\nmodel.eval()\n<\/code>\n\n\nAfter this line:\n\n\n\n alameer:\n\nmodel = Convnet()\n\n\nAlso. I think you know this, but this would mean you are trying to load trained parameters into Convnet, to do so the parameters saved in \/base_model.pth should have also been training on said Convnet\nThis is the exact issue. Many Thanks for your help."},{"x":"Hi all,\nI\u2019m currently using alexnet pretrained network for my experiments. I need to preprocess the images performing normalization. I have images with values in the range [0,255]. I have divided by 255 and I have performed normalization using normalize = transforms.Normalize(mean=[0.485, 0.456, 0.406],std=[0.229, 0.224, 0.225]). However after the normalization the values are not in the range [0,1]. Should I compute for each image the mean and standard deviation and then subtracting the mean and divide by standard deviation?\nthanks for your help","y":"\n\n\n terro:\n\nHowever after the normalization the values are not in the range [0,1].\n\n\nThis is normal.\n\n\n\n terro:\n\nShould I compute for each image the mean and standard deviation and then subtracting the mean and divide by standard deviation?\n\n\nThis is exactly what torchvision.transforms.Normalize does.\nfor example one popular thing to do would be, using torchvision.transforms.ToTensor to make your image of range [0,255] to a tensor of range [0,1] and then use torchvision.transforms.Normalize with mean and std of 0.5 over all channels. Since (0 - 0.5) \/ 0.5 = -1 and (1 - 0.5) \/ 0.5 =1, this would normalize your images in a range of [-1,1].\nSince you are using a pretrained model you can also use the mean and std the data was normalized with when being trained. Since you are probably using alexnet pertained on imagenet, that would be the mean and std of the imagenet dataset.\nWhich is the one in your code: mean = [0.485, 0.456, 0.406], stds = [0.229, 0.224, 0.225]","z":"\n\n\n terro:\n\nHowever after the normalization the values are not in the range [0,1].\n\n\nThis is normal.\n\n\n\n terro:\n\nShould I compute for each image the mean and standard deviation and then subtracting the mean and divide by standard deviation?\n\n\nThis is exactly what torchvision.transforms.Normalize does.\nfor example one popular thing to do would be, using torchvision.transforms.ToTensor to make your image of range [0,255] to a tensor of range [0,1] and then use torchvision.transforms.Normalize with mean and std of 0.5 over all channels. Since (0 - 0.5) \/ 0.5 = -1 and (1 - 0.5) \/ 0.5 =1, this would normalize your images in a range of [-1,1].\nSince you are using a pretrained model you can also use the mean and std the data was normalized with when being trained. Since you are probably using alexnet pertained on imagenet, that would be the mean and std of the imagenet dataset.\nWhich is the one in your code: mean = [0.485, 0.456, 0.406], stds = [0.229, 0.224, 0.225]"},{"x":"Hello,\nI am trying to install Pytorch 1.6.0 via Pip package (pip install torch torchvision) on a computing grid in which I don\u2019t have permission to install any software by myself. The torch.cuda.is_available() returns False, but torch.version.cuda returns 10.2.\nHere is the output of nvcc --version :\n<code class=\"lang-auto\">nvcc: NVIDIA (R) Cuda compiler driver\nCopyright (c) 2005-2019 NVIDIA Corporation\nBuilt on Wed_Oct_23_19:24:38_PDT_2019\nCuda compilation tools, release 10.2, V10.2.89\n<\/code>\n, and this is the output of nvidia-smi:\n<code class=\"lang-auto\">+-----------------------------------------------------------------------------+\n| NVIDIA-SMI 410.78       Driver Version: 410.78       CUDA Version: 10.0     |\n|-------------------------------+----------------------+----------------------+\n| GPU  Name        Persistence-M| Bus-Id        Disp.A | Volatile Uncorr. ECC |\n| Fan  Temp  Perf  Pwr:Usage\/Cap|         Memory-Usage | GPU-Util  Compute M. |\n|===============================+======================+======================|\n|   0  Tesla P100-PCIE...  Off  | 00000000:3B:00.0 Off |                    0 |\n| N\/A   23C    P0    26W \/ 250W |      0MiB \/ 16280MiB |      0%      Default |\n+-------------------------------+----------------------+----------------------+\n                                                                               \n+-----------------------------------------------------------------------------+\n| Processes:                                                       GPU Memory |\n|  GPU       PID   Type   Process name                             Usage      |\n|=============================================================================|\n|  No running processes found                                                 |\n+-----------------------------------------------------------------------------+\n<\/code>\nHow can I fix the problem?","y":"torch.version.cuda indicates which CUDA runtime was shipped in the binaries (or used to build PyTorch from source). It does not tell you, if CUDA can be used as seen by the output of torch.cuda.is_available().\nFor CUDA10.2.89 you would need an NVIDIA driver >= 440.33 as seen here with link \"https:\/\/docs.nvidia.com\/deploy\/cuda-compatibility\/index.html#binary-compatibility__table-toolkit-driver\".\nYou could update the driver or, if that\u2019s not possible, use the binaries with CUDA9.2, which should work using your driver.","z":"I should add that other versions of CUDA are available on the computing grid, including CUDA 10.0 and CUDA 9.0, but unfortunately according to PyTorch installation none of these CUDA versions are appropriate to install Pytorch via Pip (currently PyTorch with CUDA 9.2, 10.1, and 10.2 can be installed via Pip). Is there any way to solve this problem?\ntorch.version.cuda indicates which CUDA runtime was shipped in the binaries (or used to build PyTorch from source). It does not tell you, if CUDA can be used as seen by the output of torch.cuda.is_available().\nFor CUDA10.2.89 you would need an NVIDIA driver >= 440.33 as seen here with link \"https:\/\/docs.nvidia.com\/deploy\/cuda-compatibility\/index.html#binary-compatibility__table-toolkit-driver\".\nYou could update the driver or, if that\u2019s not possible, use the binaries with CUDA9.2, which should work using your driver.\nThank you for the answer."},{"x":"Hello,\nI have defined a densenet architecture in PyTorch to use it on training data consisting of 15000 samples of 128x128 images. Here is the code:\n<code class=\"lang-auto\">class Dense_Block(nn.Module):\n    def __init__(self, in_channels):\n        super(Dense_Block, self).__init__()\n\n        self.relu = nn.ReLU(inplace = True)\n        self.bn = nn.BatchNorm2d(num_features = in_channels)\n\n        self.conv1 = nn.Conv2d(in_channels = in_channels, out_channels = 32, kernel_size = 3, stride = 1, padding = 1)\n        self.conv2 = nn.Conv2d(in_channels = 32, out_channels = 32, kernel_size = 3, stride = 1, padding = 1)\n        self.conv3 = nn.Conv2d(in_channels = 64, out_channels = 32, kernel_size = 3, stride = 1, padding = 1)\n        self.conv4 = nn.Conv2d(in_channels = 96, out_channels = 32, kernel_size = 3, stride = 1, padding = 1)\n        self.conv5 = nn.Conv2d(in_channels = 128, out_channels = 32, kernel_size = 3, stride = 1, padding = 1)\n\n    def forward(self, x):\n\n        bn = self.bn(x)\n        conv1 = self.relu(self.conv1(bn))\n\n        conv2 = self.relu(self.conv2(conv1))\n        c2_dense = self.relu(torch.cat([conv1, conv2], 1))\n\n        conv3 = self.relu(self.conv3(c2_dense))\n        c3_dense = self.relu(torch.cat([conv1, conv2, conv3], 1))\n\n        conv4 = self.relu(self.conv4(c3_dense))\n        c4_dense = self.relu(torch.cat([conv1, conv2, conv3, conv4], 1))\n\n        conv5 = self.relu(self.conv5(c4_dense))\n        c5_dense = self.relu(torch.cat([conv1, conv2, conv3, conv4, conv5], 1))\n\n        return c5_dense\n\nclass Transition_Layer(nn.Module):\n    def __init__(self, in_channels, out_channels):\n        super(Transition_Layer, self).__init__()\n\n        self.relu = nn.ReLU(inplace = True)\n        self.bn = nn.BatchNorm2d(num_features = out_channels)\n        self.conv = nn.Conv2d(in_channels = in_channels, out_channels = out_channels, kernel_size = 1, bias = False)\n        self.avg_pool = nn.AvgPool2d(kernel_size = 2, stride = 2, padding = 0)\n\n    def forward(self, x):\n\n        bn = self.bn(self.relu(self.conv(x)))\n        out = self.avg_pool(bn)\n\n        return out\n\nclass DenseNet(nn.Module):\n    def __init__(self, nr_classes):\n        super(DenseNet, self).__init__()\n\n        self.lowconv = nn.Conv2d(in_channels = 1, out_channels = 64, kernel_size = 7, padding = 3, bias = False)\n        self.relu = nn.ReLU()\n\n        # Make Dense Blocks\n        self.denseblock1 = self._make_dense_block(Dense_Block, 64)\n        self.denseblock2 = self._make_dense_block(Dense_Block, 128)\n        self.denseblock3 = self._make_dense_block(Dense_Block, 128)\n\n        # Make transition Layers\n        self.transitionLayer1 = self._make_transition_layer(Transition_Layer, in_channels = 160, out_channels = 128)\n        self.transitionLayer2 = self._make_transition_layer(Transition_Layer, in_channels = 160, out_channels = 128)\n        self.transitionLayer3 = self._make_transition_layer(Transition_Layer, in_channels = 160, out_channels = 64)\n\n        # Classifier\n        self.bn = nn.BatchNorm2d(num_features = 64)\n        self.pre_classifier = nn.Linear(64*16*16, 512)\n        self.classifier = nn.Linear(512, nr_classes)\n\n    def _make_dense_block(self, block, in_channels):\n        layers = []\n        layers.append(block(in_channels))\n        return nn.Sequential(*layers)\n\n    def _make_transition_layer(self, layer, in_channels, out_channels):\n        modules = []\n        modules.append(layer(in_channels, out_channels))\n        return nn.Sequential(*modules)\n\n    def forward(self, x):\n        out = self.relu(self.lowconv(x))\n\n        out = self.denseblock1(out)\n        out = self.transitionLayer1(out)\n\n        out = self.denseblock2(out)\n        out = self.transitionLayer2(out)\n\n        out = self.denseblock3(out)\n        out = self.transitionLayer3(out)\n\n        out = self.bn(out)\n#         print(out.shape)\n        out = out.reshape(-1, 64*16*16)\n\n        out = self.pre_classifier(out)\n        out = self.classifier(out)\n\n        return out\n<\/code>\nThen I define my DataSet class:\n<code class=\"lang-auto\">class MyDataset(Dataset):\n    def __init__(self, images, n, labels=None, transforms=None):\n        self.X = images\n        self.y = labels\n        self.n = n\n        self.transforms = transforms\n         \n    def __len__(self):\n        return (len(self.X))\n    \n    def __getitem__(self, i):\n        data = self.X.iloc[i, :]\n        data = np.asarray(data).astype(np.float).reshape(1,n,n)\n        \n        if self.transforms:\n            data = self.transforms(data).reshape(1,n,n)\n            \n        if self.y is not None:\n            y = self.y.iloc[i,:]\n            y = np.asarray(y).astype(np.float).reshape(128,) # for 128-vector of labels\n            return (data, y)\n        else:\n            return data\n<\/code>\nThen I create the instances of the train, dev, and test data:\n<code class=\"lang-auto\">train_data = MyDataset(train_images, n, train_labels, None)\ndev_data = MyDataset(dev_images, n, dev_labels, None)\ntest_data = MyDataset(test_images, n, test_labels, None)\n<\/code>\nThe shapes of  train_images ,  dev_images  and  test_images  are respectively  (15000, 16384) ,  (4000, 16384)  and  (1000, 16384) . So there are in total 20000 samples of 128x128 (=16384) images.\nThe shapes of  train_labels ,  dev_labels  and  test_labels  are respectively  (15000, 128) ,  (4000, 128)  and  (1000, 128) . So there are in total 20000 samples of 128 vectors.\nI define also a custom loss function:\n<code class=\"lang-auto\"># Huber\ndef Huber(yHat,y,delta=1.):\n    n_samples = yHat.size()[0]\n    n_points = yHat.size()[1]\n    preds = yHat\n    labels = y\n    size = yHat.size()[0]*yHat.size()[1]\n    diff = yHat - y\n    return torch.sum(torch.where(torch.abs(diff) < delta,.5*diff**2 , delta*(torch.abs(diff)-.5*delta**2))) \/ size\n<\/code>\nThen I create an instance of the model:\n<code class=\"lang-auto\">densenet = DenseNet(nr_classes=128).float().to('cuda:0')\n<\/code>\nThen I initialize parameters, create train- and dev-set dataloaders, and train the model using Adam optimizer and Huber loss-function:\n<code class=\"lang-auto\">def main():\n    nn.init.kaiming_uniform_(list(densenet.parameters())[0], nonlinearity = 'relu')\n    loader = DataLoader(train_data,batch_size=200,shuffle=False,num_workers=0)\n    loader_dev = DataLoader(dev_data,batch_size=10,shuffle=None,num_workers=0)\n    N_epochs = 10\n    for epoch in range(N_epochs):\n          optimizer = optim.Adam(densenet.parameters(), lr=.001, betas=(0.9, 0.999), eps=1e-08)\n          for batch in loader:\n                images = batch[0].float().to('cuda:0')\n                labels = batch[1].float().to('cuda:0')\n                preds = densenet(images)\n                loss = Loss(preds,labels).Huber()\n            \n                with torch.no_grad():\n                    loss_dev = 0\n                    for batch_dev in loader_dev:\n                        images_dev = batch_dev[0].float().to('cuda:0')\n                        labels_dev = batch_dev[1].float().to('cuda:0')\n                        preds_dev = densenet(images_dev)\n                        loss_ = Loss(preds_dev,labels_dev).Huber()\n                        loss_dev += loss_\n\n                 optimizer.zero_grad()\n                 loss.backward()\n                 optimizer.step()\n\nif __name__ == '__main__':\n    multiprocessing.freeze_support()\n    main()\n<\/code>\nI have two GPUs identified as cuda:0 (~ 24 Gb memory) and cuda:1 (~ 6 Gb memory)\nWith cuda:1 device I get the error message:\n<code class=\"lang-auto\">\n  File \"D:\\Jupiter_playground\\fashion_mnist_tidied.py\", line 1127, in <module>\n    main()\n\n  File \"D:\\Jupiter_playground\\fashion_mnist_tidied.py\", line 1074, in main\n    preds = network(images) # Pass Batch\n\n  File \"C:\\Users\\Admin\\.conda\\envs\\pytorch_env\\lib\\site-packages\\torch\\nn\\modules\\module.py\", line 722, in _call_impl\n    result = self.forward(*input, **kwargs)\n\n  File \"D:\\Jupiter_playground\\fashion_mnist_tidied.py\", line 752, in forward\n    out = self.denseblock1(out)\n\n  File \"C:\\Users\\Admin\\.conda\\envs\\pytorch_env\\lib\\site-packages\\torch\\nn\\modules\\module.py\", line 722, in _call_impl\n    result = self.forward(*input, **kwargs)\n\n  File \"C:\\Users\\Admin\\.conda\\envs\\pytorch_env\\lib\\site-packages\\torch\\nn\\modules\\container.py\", line 117, in forward\n    input = module(input)\n\n  File \"C:\\Users\\Admin\\.conda\\envs\\pytorch_env\\lib\\site-packages\\torch\\nn\\modules\\module.py\", line 722, in _call_impl\n    result = self.forward(*input, **kwargs)\n\n  File \"D:\\Jupiter_playground\\fashion_mnist_tidied.py\", line 691, in forward\n    c3_dense = self.relu(torch.cat([conv1, conv2, conv3], 1))\n\nRuntimeError: CUDA out of memory. Tried to allocate 1.17 GiB (GPU 1; 6.00 GiB total capacity; 4.34 GiB already allocated; 16.62 MiB free; 4.34 GiB reserved in total by PyTorch)\n<\/code>\nThen I tried to switch to cuda:0 device which has much more memory capacity, and the error in this case reads as:\n<code class=\"lang-auto\">\n  File \"D:\\Jupiter_playground\\fashion_mnist_tidied.py\", line 1127, in <module>\n    main()\n\n  File \"D:\\Jupiter_playground\\fashion_mnist_tidied.py\", line 1074, in main\n    preds = network(images) # Pass Batch\n\n  File \"C:\\Users\\Admin\\.conda\\envs\\pytorch_env\\lib\\site-packages\\torch\\nn\\modules\\module.py\", line 722, in _call_impl\n    result = self.forward(*input, **kwargs)\n\n  File \"D:\\Jupiter_playground\\fashion_mnist_tidied.py\", line 752, in forward\n    out = self.denseblock1(out)\n\n  File \"C:\\Users\\Admin\\.conda\\envs\\pytorch_env\\lib\\site-packages\\torch\\nn\\modules\\module.py\", line 722, in _call_impl\n    result = self.forward(*input, **kwargs)\n\n  File \"C:\\Users\\Admin\\.conda\\envs\\pytorch_env\\lib\\site-packages\\torch\\nn\\modules\\container.py\", line 117, in forward\n    input = module(input)\n\n  File \"C:\\Users\\Admin\\.conda\\envs\\pytorch_env\\lib\\site-packages\\torch\\nn\\modules\\module.py\", line 722, in _call_impl\n    result = self.forward(*input, **kwargs)\n\n  File \"D:\\Jupiter_playground\\fashion_mnist_tidied.py\", line 691, in forward\n    c3_dense = self.relu(torch.cat([conv1, conv2, conv3], 1))\n\nRuntimeError: CUDA out of memory. Tried to allocate 1.17 GiB (GPU 0; 24.00 GiB total capacity; 21.59 GiB already allocated; 372.94 MiB free; 21.69 GiB reserved in total by PyTorch)\n<\/code>\nWhy does PyTorch allocate almost all available memory?\nHowever, when I use train-set of 6 images and dev-set of 3 images (test-set of 1 image), training with cuda-devices works fine.","y":"Increasing the memory demand. I mean, start from a small batch size and increase it checking gpu usage with nvidia-smi. You need to compute the backward as well since it requires memory. When you see you can compute forward and backward reaching the memory limit thats the maximum batch siD you can use","z":"Because ur batch-size is 200 and densenet is demanding.  Why don\u2019t you use smaller BS and increase up the the gpu memory?\nI mean, according to the code your batch size is 200.\nDensenet is heavy (densily conected)\n\n\n\n JuanFMontesinos:\n\nincrease up the the gpu memory\n\n\nThanks for your reply. What do you mean by increasing the gpu memory? Buying a new one?\nIncreasing the memory demand. I mean, start from a small batch size and increase it checking gpu usage with nvidia-smi. You need to compute the backward as well since it requires memory. When you see you can compute forward and backward reaching the memory limit thats the maximum batch siD you can use\nThanks for the tip! I guess this is a good systematic approach "},{"x":"How could one do both per-class weighting (probably CrossEntropyLoss) -and- per-sample weighting while training in pytorch?\nThe use case is classification of individual sections of time series data (think 1000s of sections per recording). The classes are very imbalanced, but given the continuous nature of the signal, I cannot over or under sample. And, they cannot be analyzed in isolation, as information from surrounding sections is necessary for classification of each section.\nThe other problem is sometimes individual sections of the time series will be junk (think: pure noise, or no signal -which I can easily quantify during pre-processing). Therefore, although the network will try to classify that section, I want to give it a weight of zero, so that no error is propagated for the network being unable to classify an unclassifiable section.","y":"That sounds right!\nI\u2019m not sure, what S samples are in your example, but here is a small dummy code snippet showing, what I mean:\n<code class=\"lang-python\">batch_size = 10\nnb_classes = 2\n\nmodel = nn.Linear(10, nb_classes)\nweight = torch.empty(nb_classes).uniform_(0, 1)\ncriterion = nn.CrossEntropyLoss(weight=weight, reduction='none')\n\n# This would be returned from your DataLoader\nx = torch.randn(batch_size, 10)\ntarget = torch.empty(batch_size, dtype=torch.long).random_(nb_classes)\nsample_weight = torch.empty(batch_size).uniform_(0, 1)\n\noutput = model(x)\nloss = criterion(output, target)\nloss = loss * sample_weight\nloss.mean().backward()\n<\/code>\nDo you mean each batch has a different size or what exactly are your samples?\nCould you post a random tensor showing one sample batch?\nEDIT: Probably it\u2019s also a good idea to normalize the sample weights so that the range of the loss will approx. have the same range and won\u2019t depend on the current sample distribution in your batch.\n<code class=\"lang-python\">loss =(loss * sample_weight \/ sample_weight.sum()).sum()\n<\/code>\nI\u2019m not sure in what range your weights are, so maybe it\u2019s not necessary.","z":"For the class weighting I would indeed use the weight argument in the loss function, e.g. CrossEntropyLoss.\nI assume you could save a tensor with the sample weight during your preprocessing step.\nIf so, you could create your loss function using reduction='none', which would return the loss for each sample. Using this you could return your sample weights with your data and target for the current batch, multiply it with the loss, and finally calculate the average before calling backward().\n\n\n\n ptrblck:\n\nreduction\n\n\nAh, that sounds right. Let me repeat this back to make sure I\u2019m on the same page.\nI\u2019d have a network output 3D tensor of (R recordings, C classes, S samples). CrossEntropyLoss, with reduction=\u2018none\u2019 and a class_weight tensor of C classes, would return a 2D tensor of losses in (R recordings, S samples). Then I would multiply each R by the the unique sample_weight 1D tensor for that R. And finally average this before calling backward().\nDoes that sound correct? Btw, each recording has a different number of samples. Which, if I understand the benefits of the dynamic graph in pytorch, shouldn\u2019t matter.\nI\u2019m still wrapping my head around moving from keras to pytorch. Thank you.\nThat sounds right!\nI\u2019m not sure, what S samples are in your example, but here is a small dummy code snippet showing, what I mean:\n<code class=\"lang-python\">batch_size = 10\nnb_classes = 2\n\nmodel = nn.Linear(10, nb_classes)\nweight = torch.empty(nb_classes).uniform_(0, 1)\ncriterion = nn.CrossEntropyLoss(weight=weight, reduction='none')\n\n# This would be returned from your DataLoader\nx = torch.randn(batch_size, 10)\ntarget = torch.empty(batch_size, dtype=torch.long).random_(nb_classes)\nsample_weight = torch.empty(batch_size).uniform_(0, 1)\n\noutput = model(x)\nloss = criterion(output, target)\nloss = loss * sample_weight\nloss.mean().backward()\n<\/code>\nDo you mean each batch has a different size or what exactly are your samples?\nCould you post a random tensor showing one sample batch?\nEDIT: Probably it\u2019s also a good idea to normalize the sample weights so that the range of the loss will approx. have the same range and won\u2019t depend on the current sample distribution in your batch.\n<code class=\"lang-python\">loss =(loss * sample_weight \/ sample_weight.sum()).sum()\n<\/code>\nI\u2019m not sure in what range your weights are, so maybe it\u2019s not necessary.\n\n\n\n ptrblck:\n\nDo you mean each batch has a different size or what exactly are your samples?\nCould you post a random tensor showing one sample batch?\n\n\nWow, thank you. The code helps me understand better what you were saying about how to actually implement the operation at the end.\nLet\u2019s see\u2026 I\u2019m trying to think ahead to the final model, so bear with me. Also, I shouldn\u2019t have used the word sample the way I did. Hopefully I explain it better below.\nFor each of the N samples (each individual recording), there are D divisions (the number of chunks that the recording is divided into (given that each recording is a different length, this number will vary), and four 1D features (each feature is a different length, and enter the graph at a different location - at least, that\u2019s the plan). So, I see that as 4 input tensors for each sample\/recording: (D,w), (D,x), (D,y), (D,z).\nFor each sample\/recording, the output would be a 2D tensor after the softmax of (D, C).\nThen, I would have a 1D class-weight tensor of 1xC classes, and for each sample, a 1D division(sample)-weight tensor of 1xD divisions.\nExactly! The 1D division(sample)-weight tensor would be also returned from the DataLoader or do you need to calculate and load it from \u201csomewhere else\u201d?\nAs far as I understand the divisions vary based on some criteria of your recording.\nWould you want to process all divisions of a recording in a single batch or is a \u201cwindowed\u201d approach plausible?\nA sliding window approach can sometimes be a bit tricky so don\u2019t hesitate to ask here for some hints. \nThe division(sample)-weight will be calculated for every sample\/recording ahead of time, so I guess it can also be returned with DataLoader.\nThe division count varies just because each recording is a different length, with the largest being about 60% longer than the smallest. It\u2019s just the nature of the data.\nSo, in my previous models using keras, I did use a windowed approach. However, because tensorflow requires the complete tensor to exist ahead of time, this was a massive waste of memory, since almost all of the data is duplicated using a sliding window. And this massive waste is what led to keeping the windows smaller than I would\u2019ve liked; since the model could only see a small window at a time, it could never grasp the long-range trends and cycles that happen in the data. I\u2019m exploring a combination of spatial pyramid pooling (SPP) and the [TCN] architecture as a possible solution to the issue that the 1st feature for each division is very long (and in its unprocessed form, also varies in length (the sampling rate is different)), and I would also like the network to train on the entire recording at the same time, so it can \u201csee\u201d the long-range cycles.\nHopefully that all made sense.\n(https:\/\/github.com\/locuslab\/TCN\/tree\/master\/TCN)\nHi again \nI have been thinking about this answer and I\u2019m confused about a few situations where this solution could potentially be problematic.\nConsider a situation where a few samples have a weight of 0. That is, intuitively, those samples (or observations or entries) are meaningless.\nloss = loss * sample_weight could result in a loss of 0 for those particular samples. Hence during gradient computation, wouldn\u2019t the network technically be looking at a loss of 0 (perfect classification \/ regression) for samples which we weren\u2019t confident to begin with? Do you think backprop on a loss of 0 could result in network crashes?\nFurthermore, wouldn\u2019t sample_weight.requires_grad be true? Do you think it could create complications during backprop?\nI\u2019ve been doing something similar and my network almost always crashes mid training.\n, I cannot speak to if this -could- cause a crash, or if sample_weight -should- require_grad. However, I am confident, as I trained several hundred networks using this method, that it worked for me.\nHi , is it possible that some of your sample weights during training could have a value of 0?\n I\u2019m positive that at least 10% of my weights were 0.\n\n\n\n ptrblck:\n\nloss =(loss * sample_weight \/ sample_weight.sum()).sum()\n\n\n: How to multiple the sample weight size of 4 and loss size of 4x8x8. This is an example\n<code class=\"lang-auto\">import torch\nimport numpy as np\nimport torch.nn as nn\nnum_class =2\nb,h,w =4,8,8\ninput = torch.randn((b, 1, h, w), requires_grad=True)\ntarget = torch.empty((b, h, w), dtype=torch.long).random_(num_class)\npred = torch.rand((b, num_class, h, w), dtype=torch.float)\ncriterion = nn.CrossEntropyLoss(reduction='none')\nloss = criterion(pred, target)\nsample_weight = torch.empty(b).uniform_(0, 1)\n\nprint (sample_weight.shape, loss.shape)\nloss =(loss * sample_weight \/ sample_weight.sum()).sum()\nloss.mean().backward()\n<\/code>\nYou could unsqueeze the additional dimension and use broadcasting for the multiplication:\n<code class=\"lang-python\">sample_weight = sample_weight.view(-1, 1, 1)\nloss =(loss * sample_weight \/ sample_weight.sum()).sum()\n<\/code>\n\n\n\n ptrblck:\n\nsample_weight = sample_weight.view(-1, 1, 1) loss =(loss * sample_weight \/ sample_weight.sum()).sum()\n\n\nThanks. I used above code and it got error in backward()\n<code class=\"lang-auto\">---------------------------------------------------------------------------\nRuntimeError                              Traceback (most recent call last)\n<ipython-input-22-88aafe38e0f1> in <module>()\n     15 loss =(loss * sample_weight \/ sample_weight.sum()).sum()\n     16 print (sample_weight.shape, loss.shape)\n---> 17 loss.mean().backward()\n     18 \n     19 #loss_total = torch.mean(loss * weights)\n\n1 frames\n\/usr\/local\/lib\/python3.6\/dist-packages\/torch\/autograd\/__init__.py in backward(tensors, grad_tensors, retain_graph, create_graph, grad_variables)\n     91     Variable._execution_engine.run_backward(\n     92         tensors, grad_tensors, retain_graph, create_graph,\n---> 93         allow_unreachable=True)  # allow_unreachable flag\n     94 \n     95 \n\nRuntimeError: element 0 of tensors does not require grad and does not have a grad_fn\n<\/code>\nYour code snippet doesn\u2019t include a tensor with requires_grad=True in the loss calculation.\nThis would make the code work:\n<code class=\"lang-python\">pred = torch.rand((b, num_class, h, w), dtype=torch.float, requires_grad=True)\n<\/code>\n\n\n\n John1231983:\n\nloss =(loss * sample_weight \/ sample_weight.sum()).sum() loss.mean().backward()\n\n\nOMG, it worked. But I think\n<code class=\"lang-auto\">loss =(loss * sample_weight \/ sample_weight.sum()).sum()\nloss.mean().backward()\n<\/code>\nShould be\n<code class=\"lang-auto\">loss =torch.mean(loss * sample_weight)\nloss.backward()\n<\/code>\nBy removing .sum() in the last and use torch.mean(). Am I correct?\nTo verify it, I used same_weight is 1 then it should be same as reduction=\u2018mean\u2019\n<code class=\"lang-auto\">import torch\nimport numpy as np\nimport torch.nn as nn\nnum_class =2\nb,h,w =4,8,8\ninput = torch.randn((b, 1, h, w), requires_grad=True)\ntarget = torch.empty((b, h, w), dtype=torch.long).random_(num_class)\npred = torch.rand((b, num_class, h, w), dtype=torch.float, requires_grad=True)\ncriterion = nn.CrossEntropyLoss(reduction='none')\nloss = criterion(pred, target)\n\nsample_weight = torch.from_numpy(np.asarray([1, 1, 1, 1])).float()\nsample_weight = sample_weight.view(-1, 1, 1)\nloss1 =torch.mean((loss * sample_weight))\nprint (loss1)\n\nloss2 =(loss * sample_weight \/ sample_weight.sum()).sum()\nloss2 = loss2.mean()\nprint (loss2)\n\ncriterion = nn.CrossEntropyLoss()\nloss = criterion(pred, target)\nprint (loss)\n<\/code>\nOutput\n<code class=\"lang-auto\">tensor(0.7202, grad_fn=<MeanBackward0>)\ntensor(46.0946, grad_fn=<MeanBackward0>)\ntensor(0.7202, grad_fn=<NllLoss2DBackward>)\n<\/code>\nDoes this \u2018per-class per-sample weighing\u2019 approach improve performance on imbalanced dataset compared to static weighing?"},{"x":"I used to have a torch.autograd.Function that had self variables that could be modified both within the forward and backward calls, and outside the function and the values would carry over correctly.  Now I instead have created a new class that just stores these values.  I can pass them all into the forward function, save them with  ctx.save_for_backward, and use them in backward.  However it looks like if I change them outside the function (especially between forward and backward) the values seen in backward don\u2019t change and if I change them within backward the values outside backward don\u2019t change.  Is there a recommended method for something like this now that torch.autograd.Function are meant to be static?","y":"Hi,\nYou only want to save either inputs or outputs with save_for_backward. And they are saved in a slightly complex way.\nIf you just want to have a state to pass around, you can give a dictionnary to your forward:\n<code class=\"lang-auto\">class YourFn(Function):\n  \n    def forward(ctx, arg1, arg2, my_state):\n      # This assumes that my_state is NOT a Tensor\n      # If it is, you have to use ctx.save_for_backward()\n      # or you will see a memory leak\n      ctx.my_state = my_state\n      # compute the output\n      return output\n\n  \n  def backward(ctx, grad_output):\n    my_state = ctx.my_state\n    # compute grad1, grad2\n    return grad1, grad2, None\n\nfn_state = {}\noutput = YourFn.apply(arg1, arg2, fn_state)\n<\/code>","z":"Hi,\nYou only want to save either inputs or outputs with save_for_backward. And they are saved in a slightly complex way.\nIf you just want to have a state to pass around, you can give a dictionnary to your forward:\n<code class=\"lang-auto\">class YourFn(Function):\n  \n    def forward(ctx, arg1, arg2, my_state):\n      # This assumes that my_state is NOT a Tensor\n      # If it is, you have to use ctx.save_for_backward()\n      # or you will see a memory leak\n      ctx.my_state = my_state\n      # compute the output\n      return output\n\n  \n  def backward(ctx, grad_output):\n    my_state = ctx.my_state\n    # compute grad1, grad2\n    return grad1, grad2, None\n\nfn_state = {}\noutput = YourFn.apply(arg1, arg2, fn_state)\n<\/code>\nLooks like that works!  Any additional input for how to make a dictionary work in conjunction with dataParallel to use multiple GPUs?  I\u2019m now getting \u201cRuntimeError: The size of tensor a (8) must match the size of tensor b (16) at non-singleton dimension 0\u201d Which I\u2019m guessing is because some values are being split across GPUs and others are not.\nasked in a new thread here: Resetting Dataparallel after initialization, and using it in a dictionary with link \"https:\/\/discuss.pytorch.org\/t\/resetting-dataparallel-after-initialization-and-using-it-in-a-dictionary\/62660\"\nGoogling to try to solve my problem (Memory leaks from custom function with link \"https:\/\/discuss.pytorch.org\/t\/memory-leaks-from-custom-function\/93028\") I came across this, which is how i was doing it originally which was working haha. Did this answer not take memory leaks into account or has something changed since last year?  Or upon further research was this answer assuming the state was normal variables and not other tensors?\nYes, from the question I guess I assumed that the extra arguments were not Tensor. And so that wouldn\u2019t be a problem.\nLet me edit the answer to avoid any confusion in the future "},{"x":"I can successfully run  yolov5 python detect.py  to carry out the test.\nHowever, whenever I created another file, say,  myfile.py , and trying to packed it into a  class ,  device = select_device(opt.device)  will fail with this  ERROR  message:  munmap_chunk(): invalid pointer .\nPlease help\u2026","y":"Problem found\u2026\nIt looks my PyTorch is NOT successfully built against CUDA ? hmmmmm\u2026","z":"Problem found\u2026\nIt looks my PyTorch is NOT successfully built against CUDA ? hmmmmm\u2026"},{"x":"is there any way to solve the annotation of overlap, or increase the size of chart, or the range of X-axis\n<code class=\"lang-auto\">import matplotlib\nimport matplotlib.pyplot as plt\nimport numpy as np\n\nlabel_list=['PointNet',\n'PointNet++',\n'EdgeConv',\n'PointCNN',\n'Convpoint',\n'LightConvPoint',\n'SplineCNN',\n'RSConv']\n\ny1 = [90,\n90,\n85,\n90,\n95,\n95,\n87.5,\n75\n]\ny2= [89.2,\n90.7,\n91.7,\n92.2,\n92.5,\n92.5,\n92.65,\n93.6]\n\nx = np.arange(len(label_list))  # the label locations\nwidth = 0.4 # the width of the bars\n\nfig, ax = plt.subplots()\nrects1 = ax.bar(x - width\/2, y1, width, label='Church')\n\nrects2 = ax.bar(x + width\/2, y2, width, label='Modelnet40')\n\n# Add some text for labels, title and custom x-axis tick labels, etc.\nax.set_ylim(70,100)\nax.set_ylabel('Best achieved Test accuracy%')\nax.set_title('performance comparison modelnet40 and Churchdata')\n\nax.set_xticks(x)\n\nax.set_xticklabels(label_list,rotation='vertical')\n\n\n\n\nax.legend(loc='lower left')    \n\ndef autolabel(rects):\n    \"\"\"Attach a text label above each bar in *rects*, displaying its height.\"\"\"\n    for rect in rects:\n        height = rect.get_height()\n        ax.annotate('{}'.format(height),\n                    xy=(rect.get_x() + rect.get_width() \/ 2, height),\n                    xytext=(0, 3),  # 3 points vertical offset\n                    textcoords=\"offset points\",\n                    ha='center', va='bottom')\n\n\n\nautolabel(rects1)\nautolabel(rects2)\n\n\n\nplt.show()\n<\/code>","y":"This seems to be a matplotlib-specific question so I would recommend to post it on StackOverflow, where you would probably get a faster and better answer. ","z":"This seems to be a matplotlib-specific question so I would recommend to post it on StackOverflow, where you would probably get a faster and better answer. "},{"x":"I am trying to get the main diagonal from the multiplication of two large matrices. Here is my implementation:\n<code class=\"lang-auto\">def col_wise_mul(m1, m2):\n    result = torch.zeros(0)\n    for i in range(m1.shape[1]):\n        v1 = m1[:, i, :]\n        v2 = m2[:, i]\n        v = torch.matmul(v1, v2).unsqueeze(1)\n        result = torch.cat((result, v), dim=1)\n    return result\n<\/code>\nI know that I could multiply two matrices first and then get the diagonal like below:\n<code class=\"lang-auto\">x = torch.diagonal(torch.matmul(x_feature, label_feature), offset=0).transpose(0, 1)\n<\/code>\nAs the two matrices are very large, I am wondering is there any improvements that could done to my implementation (basically I want to avoid loop as this part will happen in the forward)?\nThanks!","y":"Hi,\nI think the best way to do this is to rewrite (assuming 2D matrices) matmul(m1, m2).diag() to (m1 * m2.t()).sum(dim=1).\nYou can handle the extra batch dimension if you have one by just changing the the dimension you sum over.","z":"Hi,\nI think the best way to do this is to rewrite (assuming 2D matrices) matmul(m1, m2).diag() to (m1 * m2.t()).sum(dim=1).\nYou can handle the extra batch dimension if you have one by just changing the the dimension you sum over.\nThanks! It really helps!"},{"x":"Hi,\nI was wondering what the equivalent function to tf.unsorted_segment_sum (https:\/\/www.tensorflow.org\/api_docs\/python\/tf\/unsorted_segment_sum) is. I can come up with two alternatives:\n\nUse (sparse) matrix multiplication. Suppose Y has shape (M,D) and X has shape (N, D), we can do\n\n<code class=\"lang-auto\">Y = I @ X\n<\/code>\nwhere I is a M by N sparse binary matrix. The issue with this implementation is the overhead of constructing I.\n\nUse scatter_add_, like\n\n<code class=\"lang-auto\">Y = torch.zeros(M, D).scatter_add_(dim=0, index=I, other=X)\n<\/code>\nwhere I is an index matrix of shape (N, D). The problem with this implementation is that scatter_add_ requires M >= N\nIs there any better solution? Thanks!","y":"Would scatter_add with dim=1 work?\n<code class=\"lang-python\">\nindex = torch.tensor([[0, 0, 1, 1, 0, 1],\n                      [1, 1, 0, 0, 1, 0]])\ndata = torch.tensor([[5., 1., 7., 2., 3., 4.],\n                     [5., 1., 7., 2., 3., 4.]])\n\ntorch.zeros(2, 2).scatter_add(1, index, data)\n> tensor([[  9.,  13.],\n          [ 13.,   9.]])\n<\/code>","z":"Would scatter_add with dim=1 work?\n<code class=\"lang-python\">\nindex = torch.tensor([[0, 0, 1, 1, 0, 1],\n                      [1, 1, 0, 0, 1, 0]])\ndata = torch.tensor([[5., 1., 7., 2., 3., 4.],\n                     [5., 1., 7., 2., 3., 4.]])\n\ntorch.zeros(2, 2).scatter_add(1, index, data)\n> tensor([[  9.,  13.],\n          [ 13.,   9.]])\n<\/code>\nThanks so much for the solution!\nThank you, this works. But from what I understand, this should require a lot of memory, since the source data array will be replicated x times if I x different selected sums are taken? For example, the original array is replicated thrice here:\n<code class=\"lang-auto\">index = torch.tensor([[0, 0, 1, 1, 0, 1],\n                      [1, 1, 0, 0, 1, 0],\n                      [0, 0, 1, 1, 1, 1],])\n\ndata = torch.tensor([[5., 1., 7., 2., 3., 4.],\n                     [5., 1., 7., 2., 3., 4.]])\n                     [5., 1., 7., 2., 3., 4.]])\n\ntorch.zeros(3, 3).scatter_add(1, index, data)\n> tensor([[ 9., 13.,  0.],\n          [13.,  9.,  0.],\n          [ 6., 16.,  0.]])\n<\/code>\nYou could use expand to change the metadata without triggering a copy:\n<code class=\"lang-python\">index = torch.tensor([[0, 0, 1, 1, 0, 1],\n                      [1, 1, 0, 0, 1, 0]])\ndata = torch.tensor([[5., 1., 7., 2., 3., 4.]])\n\ntorch.zeros(2, 2).scatter_add(1, index, data.expand(2, -1))\n<\/code>\nI have a similar question. My data is a 3x3 matrix with values (src), and the index contains column ids (index), indicating to which column each element of src belongs.\n<code class=\"lang-auto\">import torch\n\nsrc = torch.tensor([[1.0, 2.0, 3.0], [2.0, 0.0, 1.0], [1.0, 3.0, 4.0]], dtype=torch.int64)\n\nindex = torch.tensor([[0, 0, 1], [0, 0, 1], [0, 0, 1]])\n<\/code>\nThe result of the unsorted segment sum should be [9, 8, 0] (since there are 3 columns or segments). The sum over the first column (with id 0) is 9, the sum over the second column (with id 1) is 8, and the sum over the third column is 0 (since index does not contain any id 2). How can I achieve this with scatter_add?"},{"x":"I have a tensor of size (2,3), after running it through torch.unique, I get a tensor size of (2,3) for the unique but only get a tensor size of (1,3) for the counts when dim=1. I was expecting that I get (2,3)\nTest code:\n<code class=\"lang-auto\">x = torch.tensor([[2, 2, 1], [0, 1, 2]])\nu, c = torch.unique(x, dim=1, return_counts=True)\n<\/code>\nIs there a problem with the implementation or have I misunderstood the documentation.","y":"The returned counts tensor c should have the shape [3] (not [1, 3]), which is also what I get.\nThis is the expected shape, since the counts tensor will have the shape output.size(dim), if dim was specified. From the docs:\n\n\n\ncounts (Tensor): (optional) if return_counts is True, there will be an additional returned tensor (same shape as output or output.size(dim), if dim was specified) representing the number of ocurrences          for each unique value or tensor.\n\n\nNote, that you will get the unique \u201ccolumns\u201d in your example.\nThis code snippet might give you a better idea, as it contains duplicated columns:\n<code class=\"lang-python\">x = torch.tensor([[2, 2, 1], [0, 0, 2]])\nu, c = torch.unique(x, dim=1, return_counts=True)\nprint(u)\n> tensor([[1, 2],\n          [2, 0]])\nprint(c)\n> tensor([1, 2])\n<\/code>\nHere you can see, that two unique columns were found, where [[2], [0]] is duplicated and thus has a count of 2.","z":"The returned counts tensor c should have the shape [3] (not [1, 3]), which is also what I get.\nThis is the expected shape, since the counts tensor will have the shape output.size(dim), if dim was specified. From the docs:\n\n\n\ncounts (Tensor): (optional) if return_counts is True, there will be an additional returned tensor (same shape as output or output.size(dim), if dim was specified) representing the number of ocurrences          for each unique value or tensor.\n\n\nNote, that you will get the unique \u201ccolumns\u201d in your example.\nThis code snippet might give you a better idea, as it contains duplicated columns:\n<code class=\"lang-python\">x = torch.tensor([[2, 2, 1], [0, 0, 2]])\nu, c = torch.unique(x, dim=1, return_counts=True)\nprint(u)\n> tensor([[1, 2],\n          [2, 0]])\nprint(c)\n> tensor([1, 2])\n<\/code>\nHere you can see, that two unique columns were found, where [[2], [0]] is duplicated and thus has a count of 2.\nFor the following code:\n<code class=\"lang-python\">votes = tensor([[2, 2, 1],\n                [1, 1, 2]])\n\nuni, count = th.unique(votes, dim=1, return_counts=True)\n\n### output\nuni = tensor([[1, 2],\n              [2, 1]])\ncount = tensor([1, 2])\n<\/code>\nWhen I run it, count returns a shape of (2) instead of (2, 2) unlike uni. I understand your explanation above but how can one get the count for each row instead?\nTo get the count for each row, you would have to use dim=0 in the torch.unique call.\nHowever, neither dim=0 would return a count in the shape [2, 2], as you are then only counting unique rows.\nMakes sense\nHow can one get the count for each unique element in each row of the votes tensor? Resulting in a 2d array e.i. running unique on each row with count\n\n\n\n JosueCom:\n\nResulting in a 2d array e.i. running unique on each row with count\n\n\nThis wouldn\u2019t necessary result in a 2-dimensional tensor, since each row might contain a different number of unique elements and I think you would need to iterate the rows:\n<code class=\"lang-python\">x = torch.tensor([[0, 0, 0],\n                  [1, 1, 2]])\n\nfor x_ in x.split(1, 0):\n    print(torch.unique(x_, return_counts=True))\n\n> (tensor([0]), tensor([3]))\n  (tensor([1, 2]), tensor([2, 1]))\n<\/code>\nThanks. I think I will do that.\nI was trying to avoid doing it since it is running in the gpu but I guess it is fine.\nMy goal is to implement knn and this section of the code is meant to count the votes of the labels"},{"x":"Hello.\nI am facing an unexpected behaviour in my dataloading scheme. I have two dataloaders, a train_dl and a test_dl. The train_dl provides batches of data with the argument shuffle=True and the test_dl provide batches with the argument shuffle=False.\nI evaluate my test metrics each N epochs, i.e each N epochs I loop over test_dl dataset. I have realized that if the value of N changes, then the shuffled batches provided by the train_dl after evaluating the test metrics differ. In other words, suppose my training points are labelled: a,b,c,d,e,f and my batch size is 2. Then if I seed everything correctly I observe the following pattern always:\nEpoch 1:\nBatch 1: [a,b]\nBatch 2: [e,d]\nBatch 3: [c,f]\nEpoch 2:\nBatch 1: [a,f]\nBatch 2: [b,d]\nBatch 3: [e,c]\nHowever, if in between epoch 1 and epoch 2 I loop over my test_dl then the provided batches at epoch 2 differ or in other words it seems that even shuffle = False for the test_dl, the random state seems to change.\nIs this true?\nThanks","y":"Thanks for the code.\nI can reproduce this behavior using two simple DataLoaders:\n<code class=\"lang-python\">X_train = torch.arange(10).float().view(-1, 1)\ny_train = torch.arange(10).float().view(-1, 1) + 0.1\ntrain_dataset = dataset_class(X_train, y_train)\ntrain_loader = DataLoader(train_dataset, batch_size=2, shuffle=True)\n\nX_val = torch.arange(10, 20).float().view(-1, 1)\ny_val = torch.arange(10, 20).float().view(-1, 1) + 0.1\nval_dataset = dataset_class(X_val, y_val)\nval_loader = DataLoader(val_dataset, batch_size=2, shuffle=False)\n\n\nseed = 2809\nprint('Seeding with {}'.format(seed))\ntorch.manual_seed(seed)\nprint('Training loop only')\nfor epoch in range(2):\n    for i, (x, y) in enumerate(train_loader):\n        print('Iter{}, X_train: {}'.format(i, x))\n    print('='*10)\n\nseed = 2809\nprint('Seeding with {}'.format(seed))\ntorch.manual_seed(seed)\nprint('Training loop only')\nfor epoch in range(2):\n    for i, (x, y) in enumerate(train_loader):\n        print('Iter{}, X_train: {}'.format(i, x))\n    print('='*10)\n\nseed = 2809\nprint('Seeding with {}'.format(seed))\ntorch.manual_seed(seed)\nprint('Adding validation loop')\nfor epoch in range(2):\n    for i, (x, y) in enumerate(train_loader):\n        print('Iter{}, X_train: {}'.format(i, x))\n    for j, (x_v, y_v) in enumerate(val_loader):\n        print('ValIter{}, X_val: {}'.format(j, x_v))\n    print('='*10)\n<\/code>\nIf you execute the code (with your dataset_class definition), you\u2019ll see that the train_loader batches are not the same for the second epoch, if the val_loader was executed without shuffling.\nMy best guess is, that the _BaseDataLoaderIter calls into the PRNG in this line of code with link \"https:\/\/github.com\/pytorch\/pytorch\/blob\/bcab2d68486fbacf9c68674e6b953b407ac09b98\/torch\/utils\/data\/dataloader.py#L387\", which would be needed to seed each worker here with link \"https:\/\/github.com\/pytorch\/pytorch\/blob\/bcab2d68486fbacf9c68674e6b953b407ac09b98\/torch\/utils\/data\/dataloader.py#L764-L769\".\nA workaround would be to create a torch.Generator manually and pass it to your train_loader, so that PyTorch uses it for the _base_seed creation:\n<code class=\"lang-python\">gen = torch.Generator()\n\nX_train = torch.arange(10).float().view(-1, 1)\ny_train = torch.arange(10).float().view(-1, 1) + 0.1\ntrain_dataset = dataset_class(X_train, y_train)\ntrain_loader = DataLoader(train_dataset, batch_size=2, shuffle=True, generator=gen)\n\nX_val = torch.arange(10, 20).float().view(-1, 1)\ny_val = torch.arange(10, 20).float().view(-1, 1) + 0.1\nval_dataset = dataset_class(X_val, y_val)\nval_loader = DataLoader(val_dataset, batch_size=2, shuffle=False)\n\n\nseed = 2809\nprint('Seeding with {}'.format(seed))\n#torch.manual_seed(seed)\ngen.manual_seed(seed)\nprint('Training loop only')\nfor epoch in range(2):\n    for i, (x, y) in enumerate(train_loader):\n        print('Iter{}, X_train: {}'.format(i, x))\n    print('='*10)\n\nseed = 2809\nprint('Seeding with {}'.format(seed))\n#torch.manual_seed(seed)\ngen.manual_seed(seed)\nprint('Training loop only')\nfor epoch in range(2):\n    for i, (x, y) in enumerate(train_loader):\n        print('Iter{}, X_train: {}'.format(i, x))\n    print('='*10)\n\nseed = 2809\nprint('Seeding with {}'.format(seed))\n#torch.manual_seed(seed)\ngen.manual_seed(seed)\nprint('Adding validation loop')\nfor epoch in range(2):\n    for i, (x, y) in enumerate(train_loader):\n        print('Iter{}, X_train: {}'.format(i, x))\n    for j, (x_v, y_v) in enumerate(val_loader):\n        print('ValIter{}, X_val: {}'.format(j, x_v))\n    print('='*10)\n<\/code>","z":"Based on the description it seems that the test DataLoader is calling into the PRNG at one point. Since shuffle is set to False, the Dataset might still use it at one point. Could you post the Dataset implementation (for the testset in particular) as well as the test loop?\nThis is the dataset class:\n<code class=\"lang-auto\">\nclass dataset_class(torch.utils.data.Dataset):\n    ''' General dataset class\n        Args:\n                X: input -> torch.tensor\n                Y: targets -> torch.tensor\n    '''\n\n    def __init__(self,X: torch.tensor, Y: torch.tensor):\n        super(dataset_class, self).__init__()\n        assert X is not None , \"Invalid None type\"\n        assert Y is not None, \"Invalid None type\"\n        self.X = X\n        self.Y = Y\n\n    def __len__(self):\n        return self.X.size(0)\n\n    def __getitem__(self,idx):\n      \n        return self.X[idx],self.Y[idx]\n<\/code>\nwhere  self.X and self.Y are constant for all the runs, i.e its order is not shuffled unless the dataloader does it. This is the main loop:\n<code class=\"lang-auto\">for b,(x,y) in enumerate(train_loader):\n\n     x,y = x.to(cg.device),y.to(cg.device)\n                   \n      ## loss\n      loss = model.loss(x,y)\n      optimizer.zero_grad() # just in case\n      loss.backward()\n      optimizer.step()\n              \n      # Performance metrics                      \n      if ((ep+1)%self.validate_each) == 0:\n           model.performance_metrics(x,y,dataset='train')\n \n      with torch.no_grad():\n\n          if ((ep+1)%validate_each) == 0:\n                    for b,(x,y) in enumerate(test_loader):\n                        x,y = x.to(cg.device),y.to(cg.device)\n                        model.performance_metrics(x,y,dataset='test')\n<\/code>\nThanks for the code.\nI can reproduce this behavior using two simple DataLoaders:\n<code class=\"lang-python\">X_train = torch.arange(10).float().view(-1, 1)\ny_train = torch.arange(10).float().view(-1, 1) + 0.1\ntrain_dataset = dataset_class(X_train, y_train)\ntrain_loader = DataLoader(train_dataset, batch_size=2, shuffle=True)\n\nX_val = torch.arange(10, 20).float().view(-1, 1)\ny_val = torch.arange(10, 20).float().view(-1, 1) + 0.1\nval_dataset = dataset_class(X_val, y_val)\nval_loader = DataLoader(val_dataset, batch_size=2, shuffle=False)\n\n\nseed = 2809\nprint('Seeding with {}'.format(seed))\ntorch.manual_seed(seed)\nprint('Training loop only')\nfor epoch in range(2):\n    for i, (x, y) in enumerate(train_loader):\n        print('Iter{}, X_train: {}'.format(i, x))\n    print('='*10)\n\nseed = 2809\nprint('Seeding with {}'.format(seed))\ntorch.manual_seed(seed)\nprint('Training loop only')\nfor epoch in range(2):\n    for i, (x, y) in enumerate(train_loader):\n        print('Iter{}, X_train: {}'.format(i, x))\n    print('='*10)\n\nseed = 2809\nprint('Seeding with {}'.format(seed))\ntorch.manual_seed(seed)\nprint('Adding validation loop')\nfor epoch in range(2):\n    for i, (x, y) in enumerate(train_loader):\n        print('Iter{}, X_train: {}'.format(i, x))\n    for j, (x_v, y_v) in enumerate(val_loader):\n        print('ValIter{}, X_val: {}'.format(j, x_v))\n    print('='*10)\n<\/code>\nIf you execute the code (with your dataset_class definition), you\u2019ll see that the train_loader batches are not the same for the second epoch, if the val_loader was executed without shuffling.\nMy best guess is, that the _BaseDataLoaderIter calls into the PRNG in this line of code with link \"https:\/\/github.com\/pytorch\/pytorch\/blob\/bcab2d68486fbacf9c68674e6b953b407ac09b98\/torch\/utils\/data\/dataloader.py#L387\", which would be needed to seed each worker here with link \"https:\/\/github.com\/pytorch\/pytorch\/blob\/bcab2d68486fbacf9c68674e6b953b407ac09b98\/torch\/utils\/data\/dataloader.py#L764-L769\".\nA workaround would be to create a torch.Generator manually and pass it to your train_loader, so that PyTorch uses it for the _base_seed creation:\n<code class=\"lang-python\">gen = torch.Generator()\n\nX_train = torch.arange(10).float().view(-1, 1)\ny_train = torch.arange(10).float().view(-1, 1) + 0.1\ntrain_dataset = dataset_class(X_train, y_train)\ntrain_loader = DataLoader(train_dataset, batch_size=2, shuffle=True, generator=gen)\n\nX_val = torch.arange(10, 20).float().view(-1, 1)\ny_val = torch.arange(10, 20).float().view(-1, 1) + 0.1\nval_dataset = dataset_class(X_val, y_val)\nval_loader = DataLoader(val_dataset, batch_size=2, shuffle=False)\n\n\nseed = 2809\nprint('Seeding with {}'.format(seed))\n#torch.manual_seed(seed)\ngen.manual_seed(seed)\nprint('Training loop only')\nfor epoch in range(2):\n    for i, (x, y) in enumerate(train_loader):\n        print('Iter{}, X_train: {}'.format(i, x))\n    print('='*10)\n\nseed = 2809\nprint('Seeding with {}'.format(seed))\n#torch.manual_seed(seed)\ngen.manual_seed(seed)\nprint('Training loop only')\nfor epoch in range(2):\n    for i, (x, y) in enumerate(train_loader):\n        print('Iter{}, X_train: {}'.format(i, x))\n    print('='*10)\n\nseed = 2809\nprint('Seeding with {}'.format(seed))\n#torch.manual_seed(seed)\ngen.manual_seed(seed)\nprint('Adding validation loop')\nfor epoch in range(2):\n    for i, (x, y) in enumerate(train_loader):\n        print('Iter{}, X_train: {}'.format(i, x))\n    for j, (x_v, y_v) in enumerate(val_loader):\n        print('ValIter{}, X_val: {}'.format(j, x_v))\n    print('='*10)\n<\/code>\nOkei thank you. Normally I dont see a difference in performance due to how batches are shuffled. But for my particular problem I am experimenting  very different results just by setting validate_each to different numbers (different numbers implies different number loops over the test dataset hence different order).\nThanks for your solution"},{"x":"The question I\u2019m about to ask is probably not PyTorch-specific, but I encountered it in context of PyTorch DataLoader.\nHow do you properly add random perturbations when data is loaded and augmented by several processes?\nLet me show on a simple example that this is not a trivial question. I have two files:\n\n\naugmentations.py:\n\n<code class=\"lang-auto\">import numpy as np\nimport os\n\nclass RandomAugmentation:\n    def __call__(self, obj):\n        perturbation = np.random.randint(10)\n        print(os.getpid(), perturbation)\n        return obj\n<\/code>\n\nmain.py\n\n<code class=\"lang-auto\">import numpy as np\nfrom time import sleep\n\nfrom torch.utils.data import Dataset, DataLoader\nfrom torchvision.datasets import ImageFolder\nfrom torchvision.transforms import Compose, ToTensor, Resize\n\nfrom augmentations import RandomAugmentation\n\nPATH = ...\n\ntransform = Compose([\n    RandomAugmentation(),\n    Resize((16, 16)),\n    ToTensor()\n])\nds = ImageFolder(PATH, transform=transform)\ndl = DataLoader(ds, batch_size=2, num_workers=3)\n\nfor epoch_nr in range(2):\n    for batch in dl:\n        break\n    sleep(1)\n    print('-' * 80)\n<\/code>\nIn the main.py we generate batches of data like we would in a regular image recognition task. RandomAugmentation prints output of RNG that would be used for data augmentation in a real task.\nOn my machine the output from running main.py was:\n<code class=\"lang-auto\">20909 6\n20909 8\n20908 6\n20910 6\n20908 8\n20910 8\n20909 7\n20909 6\n20908 7\n20908 6\n20910 7\n20908 0\n20910 6\n20908 1\n--------------------------------------------------------------------------------\n20952 6\n20953 6\n20952 8\n20952 7\n20953 8\n20952 6\n20954 6\n20952 0\n20952 1\n20953 7\n20954 8\n20953 6\n20954 7\n20954 6\n--------------------------------------------------------------------------------\n<\/code>\nClearly, i-th perturbation added by one worker is exactly the same as i-th perturbation added by any other worker (in this example 6, 8, 7, 6, and so on). What\u2019s even worse, in the next epoch data loader is kind of reset, and generates exactly the same (perturbated) data as before, what completely ruins the entire idea of data augmentation.\nFor the time being I designed such a workaround:\naugmentations.py\n<code class=\"lang-auto\">import numpy as np\nfrom time import time\nimport os\n\nRNG_PID = None\nRNG = None\n\n\ndef get_rng():\n    global RNG\n    global RNG_PID\n    if os.getpid() != RNG_PID:\n        RNG = np.random.RandomState([int(time()), os.getpid()])\n        RNG_PID = os.getpid()\n        print(\"Initialize RNG\", int(time()), os.getpid())\n    return RNG\n\nclass RandomAugmentation:\n    def __call__(self, obj):\n        rng = get_rng()\n        perturbation = rng.randint(10)\n        print(os.getpid(), perturbation)\n        return obj\n<\/code>\nHowever, it doesn\u2019t seem like a very elegant solution to me.\nIt would be great to hear how you deal with this matter ","y":"It\u2019s a known issue, since you are using other libraries (numpy in your case) to generate random numbers.\nEach worker will duplicate numpy\u2019s PRNG, so that you\u2019ll see the same numbers.\nIt\u2019s described in the FAQ with link \"https:\/\/pytorch.org\/docs\/stable\/notes\/faq.html#my-data-loader-workers-return-identical-random-numbers\" a bit better.\nYou could sample random numbers using torch.randint or use worker_init_fn to set the seed for each numpy process.","z":"It\u2019s a known issue, since you are using other libraries (numpy in your case) to generate random numbers.\nEach worker will duplicate numpy\u2019s PRNG, so that you\u2019ll see the same numbers.\nIt\u2019s described in the FAQ with link \"https:\/\/pytorch.org\/docs\/stable\/notes\/faq.html#my-data-loader-workers-return-identical-random-numbers\" a bit better.\nYou could sample random numbers using torch.randint or use worker_init_fn to set the seed for each numpy process.\nI think it would be good to clarify this in the official tutorial https:\/\/pytorch.org\/tutorials\/beginner\/data_loading_tutorial.html since it also uses numpy\u2019s random module.\nThat might be indeed a good idea. Would you be interested in implementing it? \nOk I just submitted PR https:\/\/github.com\/pytorch\/tutorials\/pull\/1121 with the changes. I just added a note after the Transforms example clarifying the potential issue."},{"x":"Hi when i try to run my train code this error pop up and im not sure how to fix it this is my code\n<code class=\"lang-auto\">##########################################################################################################\n#try to train\nif __name__ == '__main__':\n\n    #set_trace()\n    args = edict({\n        'operation' : 'train',\n        'feature_file' : None,\n         'result_sample_path' : None,\n         'gpu' : 'GPU',\n         'path_image_query' : None,\n         'query_label' : 'Query label',\n         'dataset' : None,\n         'specific_dataset_folder_name' : 'lfw',\n         'img_extension' : 'jpg',\n         'preprocessing_method' : 'sphereface',\n         'model_name' : 'mobiface',\n         'batch_size' : 3,\n         'image_query':'\/content\/drive\/My Drive\/recfaces13\/recfaces\/datasets\/LFW',\n         'train':True,\n         'device':'cuda'\n})\n    print(args)\n\n    # selecting the size of the crop based on the network\n    if args.model_name == 'mobilefacenet' or args.model_name == 'sphereface':\n        crop_size = (96, 112)\n    elif args.model_name == 'mobiface' or args.model_name == 'shufflefacenet':\n        crop_size = (112, 112)\n    elif args.model_name == 'openface':\n        crop_size = (96, 96)\n    elif args.model_name == 'facenet':\n        crop_size = (160, 160)\n    else:\n        raise NotImplementedError(\"Model \" + args.model_name + \" not implemented\")\n\n    if args.dataset is not None:\n        # process whole dataset\n        assert args.specific_dataset_folder_name is not None, 'To process a dataset, ' \\\n                                                              'the flag --specific_dataset_folder_name is required.'\n        process_dataset(args.operation, args.model_name, args.batch_size,\n                        args.dataset, args.specific_dataset_folder_name,\n                        args.img_extension, args.preprocessing_method, crop_size,\n                        args.result_sample_path, args.feature_file)\n    #elif args.image_query is not None:\n        # process unique image\n      #  dataset = ImageDataLoader(args.image_query, args.preprocessing_method,\n      #                            crop_size, args.operation == 'extract_features')\n      #  dataloader = torch.utils.data.DataLoader(dataset, batch_size=1, shuffle=False, num_workers=2, drop_last=False)\n      #  features = None\n    elif args.operation == 'train':\n      ##########set_trace()\n\n      net = load_net('mobilefacenet', 'gpu')\n      net = net.cuda()\n      model_name=args.model_name\n      \n      dataset = LFW(args.image_query,args.specific_dataset_folder_name, args.img_extension, args.preprocessing_method, crop_size)\n      dataloader = torch.utils.data.DataLoader(dataset, batch_size=1, shuffle=False, num_workers=2, drop_last=False)\n\n      \n    #  data_counter_per_class = np.zeros((len(dataloader)))\n    #  for i in range(len(dataloader)):\n     #   path = os.path.join('image_query', dataloader[i])\n    #    files = get_files_from_folder(path)\n    #    data_counter_per_class[i] = len(files)\n    #    test_counter = np.round(data_counter_per_class * (1 - train_ratio))\n      #dataloader1=dataloader.split(',')\n      #train,test=train_test_split(dataloader,test_size=0.2)\n      #trainloader = torch.utils.data.DataLoader(dataset, batch_size=4, shuffle=True, num_workers=2, drop_last=False)\n     # testloader = torch.utils.data.DataLoader(dataset, batch_size=4, shuffle=False, num_workers=2, drop_last=False) \/\/create path\/\/\n     #create array of data path split that data path and \n      features = None\n\n      if args.feature_file is not None and os.path.isfile(args.feature_file):\n            features = scipy.io.loadmat(args.feature_file)      \n      epoch = 2\n      criterion = nn.CrossEntropyLoss()\n      optimizer = optim.SGD(net.parameters(), lr=0.001, momentum=0.9)\n      train_loss = list()\n      #set_trace()\n      \n      for i, data in enumerate(dataloader):\n        \n        inpss, labs = data\n        for inps in inpss:\n          #set_trace()\n          inps, labs = inps.cuda(args['device']), labs.cuda(args['device'])\n          inps.squeeze_(0)\n          labs.squeeze_(0)\n          inps = Variable(inps).cuda(args['device'])\n          labs = Variable(labs).cuda(args['device'])\n          optimizer.zero_grad()\n          outs = net(inps)\n          soft_outs = F.softmax(outs, dim=1)\n          prds = soft_outs.data.max(1)[1]\n          loss = criterion(outs, labs)\n          loss.backward()\n          optimizer.step()\n          prds = prds.squeeze_(1).squeeze_(0).cpu().numpy()\n          inps_np = inps.detach().squeeze(0).cpu().numpy()\n          labs_np = labs.detach().squeeze(0).cpu().numpy()\n          train_loss.append(loss.data.item())\n          print('[epoch %d], [iter %d \/ %d], [train loss %.5f]' % (epoch, i + 1, len(train_loader), np.asarray(train_loss).mean()))\n\n<\/code>\nand this is my dataload\n<code class=\"lang-auto\">class LFW(object):\n    def __init__(self, root, specific_folder, img_extension, preprocessing_method=None, crop_size=(96, 112)):\n        \"\"\"\n        Dataloader of the LFW dataset.\n\n        root: path to the dataset to be used.\n        specific_folder: specific folder inside the same dataset.\n        img_extension: extension of the dataset images.\n        preprocessing_method: string with the name of the preprocessing method.\n        crop_size: retrieval network specific crop size.\n        \"\"\"\n\n        self.preprocessing_method = preprocessing_method\n        self.crop_size = crop_size\n        self.imgl_list = []\n        self.classes = []\n        self.people = []\n        self.model_align = None\n\n        # read the file with the names and the number of images of each people in the dataset\n        with open(os.path.join(root, 'people.txt')) as f:\n            people = f.read().splitlines()[1:]\n\n        # get only the people that have more than 20 images\n        for p in people:\n            p = p.split('\\t')\n            if len(p) > 1:\n                if int(p[1]) >= 20:\n                    for num_img in range(1, int(p[1]) + 1):\n                        self.imgl_list.append(os.path.join(root, specific_folder, p[0], p[0] + '_' +\n                                                           '{:04}'.format(num_img) + '.' + img_extension))\n                        self.classes.append(p[0])\n                        self.people.append(p[0])\n\n        le = preprocessing.LabelEncoder()\n        self.classes = le.fit_transform(self.classes)\n\n        print(len(self.imgl_list), len(self.classes), len(self.people))\n\n    def __getitem__(self, index):\n        imgl = imageio.imread(self.imgl_list[index])\n        cl = self.classes[index]\n\n        # if image is grayscale, transform into rgb by repeating the image 3 times\n        if len(imgl.shape) == 2:\n            imgl = np.stack([imgl] * 3, 2)\n\n        imgl, bb = preprocess(imgl, self.preprocessing_method, crop_size=self.crop_size,\n                              is_processing_dataset=True, return_only_largest_bb=True, execute_default=True)\n\n        # append image with its reverse\n        imglist = [imgl, imgl[:, ::-1, :]]\n\n        # normalization\n        for i in range(len(imglist)):\n            imglist[i] = (imglist[i] - 127.5) \/ 128.0\n            imglist[i] = imglist[i].transpose(2, 0, 1)\n        imgs = [torch.from_numpy(i).float() for i in imglist]\n\n        return imgs, cl, imgl, bb, self.imgl_list[index], self.people[index]\n\n    def __len__(self):\n        return len(self.imgl_list)\n<\/code>","y":"I had to do a lot of changes to make it start training\u2026\n<code class=\"lang-auto\">      for i, data in enumerate(dataloader):\n        \n        inps, labs = data\n        inps, labs = inps.cuda(args['device']), labs.cuda(args['device'])\n\n        inps = Variable(inps).cuda(args['device'])\n        labs = Variable(labs).cuda(args['device'])\n        optimizer.zero_grad()\n        outs = net(inps.permute(0, 3, 1, 2).float())\n        soft_outs = F.softmax(outs, dim=1)\n        prds = soft_outs.data.max(1)[1]\n        loss = criterion(outs, labs)\n        loss.backward()\n        optimizer.step()\n        prds = prds.cpu().numpy()\n        inps_np = inps.detach().cpu().numpy()\n        labs_np = labs.detach().cpu().numpy()\n        train_loss.append(loss.data.item\n                        ())\n        print('[epoch %d], [iter %d \/ %d], [train loss %.5f]' % (epoch, i + 1, len(dataloader), np.asarray(train_loss).mean()))\n<\/code>","z":"There must be batch dimension as well, so your input should have size [1, 3, 112, 112]. For this you can change net(inps) to net(inps[None, ...])\u2026\nHi can you explain what do you mean by that?\nhow to chnge the size ? i really have no idea can you help\nCan you change\nouts = net(inps)\nto\n<code class=\"lang-auto\">outs = net(inps[None, ...])\n<\/code>\n\n\n\n user_123454321:\n\nnet(inps[None, \u2026])\n\n\nhi thank  i change it and now the error become\nGiven groups=1, weight of size [64, 3, 3, 3], expected input[1, 112, 112, 3] to have 3 channels, but got 112 channels instead\ndo you know how would i able to fix this? sorry just im not quite good with python\nsi just wonder what it mean by 4dimension?\nHey can you send all your code in a file maybe? That is if it is not confidential.\nyes that will be fine\ni work on google colab\nthis is my link to file\nhttps:\/\/colab.research.google.com\/drive\/1kw_LGQIPsosuxnlZuLy585LnhYp3idEO?usp=sharing\nand this is the whole file\nhttps:\/\/drive.google.com\/drive\/folders\/1EhNbxNx707irJUGH_3DgS2kzFUMrBUmc?usp=sharing\nim doing my master and this is my dissertation project i cant move on at all and start write it error keep pop up and i have no idea what went wrong\nI had to do a lot of changes to make it start training\u2026\n<code class=\"lang-auto\">      for i, data in enumerate(dataloader):\n        \n        inps, labs = data\n        inps, labs = inps.cuda(args['device']), labs.cuda(args['device'])\n\n        inps = Variable(inps).cuda(args['device'])\n        labs = Variable(labs).cuda(args['device'])\n        optimizer.zero_grad()\n        outs = net(inps.permute(0, 3, 1, 2).float())\n        soft_outs = F.softmax(outs, dim=1)\n        prds = soft_outs.data.max(1)[1]\n        loss = criterion(outs, labs)\n        loss.backward()\n        optimizer.step()\n        prds = prds.cpu().numpy()\n        inps_np = inps.detach().cpu().numpy()\n        labs_np = labs.detach().cpu().numpy()\n        train_loss.append(loss.data.item\n                        ())\n        print('[epoch %d], [iter %d \/ %d], [train loss %.5f]' % (epoch, i + 1, len(dataloader), np.asarray(train_loss).mean()))\n<\/code>\nso am i correct replace this in to training part\nthank you so much how can i repay you. you really help me so much. Just wonder my dataloader part is ok am i correct\njust wonder can you explain to me how you fix the 122 channel?\nAh\u2026the 112  are not channels but height (and width) of the image. There are only 3 channels in the image ( red, green and blue). Pytorch expects channels to be the 1st dimension (0th dimension is batch dimension), 2nd dimension to be height and 3rd to be width\u2026So we need to transpose the image to get channels first\u2026\nam i correct this line of code that the defined thos RGB\nouts = net(inps.permute(0, 3, 1, 2).float())\njust quick question to test this module can you give some suggestion how would i able to save this result so i can use it in testing stage? just right now im not sure how would i use it for testing\nYou can save a model in pytorch using the torch.save function like this\n<code class=\"lang-auto\">torch.save(net.state_dict(), 'my_model.pth')\n<\/code>\njust quick question when test my model do i actually need to save my module or not nessesery? im pretty new to this idea so im not quite sure do i need to save it or not really"},{"x":"I load my pretrained pytorch but i cant predict a single image\nPart of Code:\nfrom torchvision import transforms\ntransform = transforms.Compose([            #[1]\ntransforms.Resize(256),                    #[2]\ntransforms.ToTensor(),                     #[4]\ntransforms.Normalize(                      #[5]\nmean=[0.485, 0.456, 0.406],                #[6]\nstd=[0.229, 0.224, 0.225]                  #[7]\n)])\nfrom PIL import Image\nimg = Image.open(\"\/content\/gdrive\/My Drive\/DCN(data)\/Valid\/Covid\/ryct.2020200034.fig2.jpeg\")\nimg_t = transform(img)\nbatch_t = torch.unsqueeze(img_t, 3)\nbatch_t.size()\nmodel.eval()\nout = model(batch_t)\nprint(out.shape)\nout\nERROR:\nRuntimeError                              Traceback (most recent call last)\n in ()\n1 model.eval()\n----> 2 out = model(batch_t)\n3 print(out.shape)\n4 out\n6 frames\n\/usr\/local\/lib\/python3.6\/dist-packages\/torch\/nn\/modules\/conv.py in conv2d_forward(self, input, weight)\n340                             _pair(0), self.dilation, self.groups)\n341         return F.conv2d(input, weight, self.bias, self.stride,\n\u2013> 342                         self.padding, self.dilation, self.groups)\n343\n344     def forward(self, input):\nRuntimeError: Given groups=1, weight of size 8 3 3 3, expected input[3, 256, 276, 1] to have 3 channels, but got 256 channels instead","y":"Hi there,\nplease wrap your code with preformatted text ,\nI think , your problem is given in your input i.e [3,256,256,1] ,\nto clear, Pytorch uses [batch, channel, height, width] , so you need to change shape in that way>\nHere i found detail about this, https:\/\/discuss.pytorch.org\/t\/dimensions-of-an-input-image\/19439\/2?u=ptrblk with link \"https:\/\/discuss.pytorch.org\/t\/dimensions-of-an-input-image\/19439\/2\".\nHope this works","z":"Hi there,\nplease wrap your code with preformatted text ,\nI think , your problem is given in your input i.e [3,256,256,1] ,\nto clear, Pytorch uses [batch, channel, height, width] , so you need to change shape in that way>\nHere i found detail about this, https:\/\/discuss.pytorch.org\/t\/dimensions-of-an-input-image\/19439\/2?u=ptrblk with link \"https:\/\/discuss.pytorch.org\/t\/dimensions-of-an-input-image\/19439\/2\".\nHope this works"},{"x":"I am getting the above warning message when running my code. I have looked at this post with link \"https:\/\/github.com\/eriklindernoren\/PyTorch-GAN\/issues\/46\" to fix the issue, but that didn\u2019t help\n<code class=\"lang-auto\">import torch\nimport torch.nn as nn\n\nclass Conv2d(nn.Module):\n    def __init__(self, in_channels, out_channels, kernel_size, stride=1, NL='relu', same_padding=False, bn=False, dilation=1):\n        super(Conv2d, self).__init__()\n        padding = int((kernel_size - 1) \/\/ 2) if same_padding else 0\n        self.conv = []\n        if dilation==1:\n            self.conv = nn.Conv2d(in_channels, out_channels, kernel_size, stride, padding=padding, dilation=dilation)\n        else:\n            self.conv = nn.Conv2d(in_channels, out_channels, kernel_size, stride, padding=dilation, dilation=dilation)\n        self.bn = nn.BatchNorm2d(out_channels, eps=0.001, momentum=0, affine=True) if bn else None\n        if NL == 'relu' :\n            self.relu = nn.ReLU(inplace=True) \n        elif NL == 'prelu':\n            self.relu = nn.PReLU() \n        else:\n            self.relu = None\n\n    def forward(self, x):\n        x = self.conv(x)\n        if self.bn is not None:\n            x = self.bn(x)\n        if self.relu is not None:\n            x = self.relu(x)\n        return x\n\nclass FC(nn.Module):\n    def __init__(self, in_features, out_features, NL='relu'):\n        super(FC, self).__init__()\n        self.fc = nn.Linear(in_features, out_features)\n        if NL == 'relu' :\n            self.relu = nn.ReLU(inplace=True) \n        elif NL == 'prelu':\n            self.relu = nn.PReLU() \n        else:\n            self.relu = None\n\n    def forward(self, x):\n        x = self.fc(x)\n        if self.relu is not None:\n            x = self.relu(x)\n        return x\n\n\nclass convDU(nn.Module):\n\n    def __init__(self,\n        in_out_channels=2048,\n        kernel_size=(9,1)\n        ):\n        super(convDU, self).__init__()\n        self.conv = nn.Sequential(\n            nn.Conv2d(in_out_channels, in_out_channels, kernel_size, stride=1, padding=((kernel_size[0]-1)\/\/2,(kernel_size[1]-1)\/\/2)),\n            nn.ReLU(inplace=True)\n            )\n\n    def forward(self, fea):\n        n, c, h, w = fea.size()\n\n        fea_stack = []\n        for i in range(h):\n            i_fea = fea.select(2, i).resize(n,c,1,w)\n            if i == 0:\n                fea_stack.append(i_fea)\n                continue\n            fea_stack.append(self.conv(fea_stack[i-1])+i_fea)\n            # pdb.set_trace()\n            # fea[:,i,:,:] = self.conv(fea[:,i-1,:,:].expand(n,1,h,w))+fea[:,i,:,:].expand(n,1,h,w)\n\n\n        for i in range(h):\n            pos = h-i-1\n            if pos == h-1:\n                continue\n            fea_stack[pos] = self.conv(fea_stack[pos+1])+fea_stack[pos]\n        # pdb.set_trace()\n        fea = torch.cat(fea_stack, 2)\n        return fea\n\nclass convLR(nn.Module):\n\n    def __init__(self,\n        in_out_channels=2048,\n        kernel_size=(1,9)\n        ):\n        super(convLR, self).__init__()\n        self.conv = nn.Sequential(\n            nn.Conv2d(in_out_channels, in_out_channels, kernel_size, stride=1, padding=((kernel_size[0]-1)\/\/2,(kernel_size[1]-1)\/\/2)),\n            nn.ReLU(inplace=True)\n            )\n\n    def forward(self, fea):\n        n, c, h, w = fea.size()\n\n        fea_stack = []\n        for i in range(w):\n            i_fea = fea.select(3, i).resize(n,c,h,1)\n            if i == 0:\n                fea_stack.append(i_fea)\n                continue\n            fea_stack.append(self.conv(fea_stack[i-1])+i_fea)\n\n        for i in range(w):\n            pos = w-i-1\n            if pos == w-1:\n                continue\n            fea_stack[pos] = self.conv(fea_stack[pos+1])+fea_stack[pos]\n\n\n        fea = torch.cat(fea_stack, 3)\n        return fea\n<\/code>","y":"Hi,\nThis warning appears because you\u2019re using upsample somewhere.\nYou should find where and replace it with interpolate if you want to get rid of the warning.","z":"Hi,\nThis warning appears because you\u2019re using upsample somewhere.\nYou should find where and replace it with interpolate if you want to get rid of the warning.\n\n\n\n albanD:\n\ninterpolate\n\n\nThanks very much for your help "},{"x":"Hi!\nI have encountered this error:\n<code class=\"lang-auto\">'numpy.ndarray' object has no attribute 'contiguous'\n<\/code>\nin this code:\n<code class=\"lang-auto\">a=numpy.array([[1,2,3,4],[5,6,7,8],[1,2,3,4],[5,6,7,8]])\ndct.dct_2d(a)\n<\/code>\nthat the dct func contains:\n<code class=\"lang-auto\">x_shape = x.shape\nN = x_shape[-1]\nx = x.contiguous().view(-1, N)\n<\/code>\nCould you mind helping me solve this?","y":"contiguous is a torch method, it has to be a torch tensor not a numpy array","z":"contiguous is a torch method, it has to be a torch tensor not a numpy array"},{"x":"Hi\nI would like to calculate the MSE and MAE for a PyTorch model. How do I go about doing this? Are there functions already available to do this, please? Many thanks in advance","y":"Yes, they are already implemented in Pytorch. Use nn.MSELoss and nn.L1Loss respectively.","z":"Yes, they are already implemented in Pytorch. Use nn.MSELoss and nn.L1Loss respectively.\nThanks a lot "},{"x":"I am currently learning how to use PyTorch to build a neural network. I have learned keras before and I would like to do the same thing in PyTorch like \u2018model.fit\u2019 and plotting a graph containing both training loss and validation loss.\nIn order to know whether the model is underfitting or not, I have to plot a graph to compare the training loss and validation loss.\nHowever, I cannot compute the right validation loss. I know that gradients should only be updated during training and it should not be updated during testing\/validation. With no change in gradients, does it mean the loss will not change? Sorry, my concept is not clear enough. But I think not, loss should be computed by comparing expected output and prediction using loss function.\nIn my code, 80 datasets are used for training and 20 datasets are used for validation. In my code, the neural network is prediction this formula: y =2X^3 + 7X^2 - 8*X + 120 It is easy to compute so I use this for learning how to build neural network through PyTorch.\nHere is my code:\n<code class=\"lang-auto\">import torch\nimport torch.nn as nn    #neural network model\nimport pandas as pd\nimport numpy as np\nimport matplotlib.pyplot as plt\nimport torch.nn.functional as F\nfrom torch.autograd import Variable\nfrom sklearn.preprocessing import MinMaxScaler\n\n#Load datasets\ndataset = pd.read_csv('test_100.csv')\n\nX = dataset.iloc[:, :-1].values\nY = dataset.iloc[:, -1:].values\n\nX_scaler = MinMaxScaler()\nY_scaler = MinMaxScaler()\nprint(X_scaler.fit(X))\nprint(Y_scaler.fit(Y))\nX = X_scaler.transform(X)\nY = Y_scaler.transform(Y)\n\nx_temp_train = X[:79]\ny_temp_train = Y[:79]\nx_temp_test = X[80:]\ny_temp_test = Y[80:]\n\nX_train = torch.FloatTensor(x_temp_train)\nY_train = torch.FloatTensor(y_temp_train)\nX_test = torch.FloatTensor(x_temp_test)\nY_test = torch.FloatTensor(y_temp_test)\n\nD_in = 1 # D_in is input features\nH = 24 # H is hidden dimension\nD_out = 1 # D_out is output features.\n\n#Define a Artifical Neural Network model\nclass Net(nn.Module):\n#------------------Two Layers------------------------------\n    def __init__(self, D_in, H, D_out):\n        super(Net, self).__init__()\n\n        self.linear1 = nn.Linear(D_in, H)  \n        self.linear2 = nn.Linear(H, D_out)\n        \n    def forward(self, x):\n        h_relu = self.linear1(x).clamp(min=0)\n        prediction = self.linear2(h_relu)\n        return prediction\nmodel = Net(D_in, H, D_out)\nprint(model)\n\n#Define a Loss function and optimizer\ncriterion = torch.nn.MSELoss()\noptimizer = torch.optim.SGD(model.parameters(), lr=0.2) #2e-7\n\n#Training\ninputs = Variable(X_train)\noutputs = Variable(Y_train)\ninputs_val = Variable(X_test)\noutputs_val = Variable(Y_test)\nloss_values = []\nval_values = []\nepoch = []\nfor phase in ['train', 'val']:\n    if phase == 'train':\n        model.train()  # Set model to training mode\n    else:\n        optimizer.zero_grad() #zero the parameter gradients\n        model.eval()   # Set model to evaluate mode\n    for i in range(50):      #epoch=50\n        if phase == 'train':\n            model.train()\n            prediction = model(inputs)\n            loss = criterion(prediction, outputs) \n            print('train loss')\n            print(loss)\n            loss_values.append(loss.detach())\n            optimizer.zero_grad() #zero the parameter gradients\n            epoch.append(i)\n            loss.backward()       #compute gradients(dloss\/dx)\n            optimizer.step()      #updates the parameters\n        elif phase == 'val':\n            model.eval()\n            prediction_val = model(inputs_val)\n            loss_val = criterion(prediction_val, outputs_val) \n            print('validation loss')\n            print(loss)\n            val_values.append(loss_val.detach())\n            optimizer.zero_grad() #zero the parameter gradients\n          \nplt.plot(epoch,loss_values)\nplt.plot(epoch, val_values)\nplt.title('model loss')\nplt.ylabel('loss')\nplt.xlabel('epoch')\nplt.legend(['train','validation'], loc='upper left')\nplt.show()\n<\/code>\nHere is the result:\n<code class=\"lang-auto\">train loss\ntensor(0.9788, grad_fn=<MseLossBackward>)\ntensor(2.0834, grad_fn=<MseLossBackward>)\ntensor(3.2902, grad_fn=<MseLossBackward>)\ntensor(0.8851, grad_fn=<MseLossBackward>)\ntensor(0.0832, grad_fn=<MseLossBackward>)\ntensor(0.0402, grad_fn=<MseLossBackward>)\ntensor(0.0323, grad_fn=<MseLossBackward>)\ntensor(0.0263, grad_fn=<MseLossBackward>)\ntensor(0.0217, grad_fn=<MseLossBackward>)\ntensor(0.0181, grad_fn=<MseLossBackward>)\ntensor(0.0153, grad_fn=<MseLossBackward>)\ntensor(0.0132, grad_fn=<MseLossBackward>)\ntensor(0.0116, grad_fn=<MseLossBackward>)\ntensor(0.0103, grad_fn=<MseLossBackward>)\ntensor(0.0094, grad_fn=<MseLossBackward>)\ntensor(0.0087, grad_fn=<MseLossBackward>)\ntensor(0.0081, grad_fn=<MseLossBackward>)\ntensor(0.0077, grad_fn=<MseLossBackward>)\ntensor(0.0074, grad_fn=<MseLossBackward>)\ntensor(0.0072, grad_fn=<MseLossBackward>)\ntensor(0.0070, grad_fn=<MseLossBackward>)\ntensor(0.0068, grad_fn=<MseLossBackward>)\ntensor(0.0067, grad_fn=<MseLossBackward>)\ntensor(0.0067, grad_fn=<MseLossBackward>)\ntensor(0.0066, grad_fn=<MseLossBackward>)\ntensor(0.0065, grad_fn=<MseLossBackward>)\ntensor(0.0065, grad_fn=<MseLossBackward>)\ntensor(0.0065, grad_fn=<MseLossBackward>)\ntensor(0.0064, grad_fn=<MseLossBackward>)\ntensor(0.0064, grad_fn=<MseLossBackward>)\ntensor(0.0064, grad_fn=<MseLossBackward>)\ntensor(0.0064, grad_fn=<MseLossBackward>)\ntensor(0.0063, grad_fn=<MseLossBackward>)\ntensor(0.0063, grad_fn=<MseLossBackward>)\ntensor(0.0063, grad_fn=<MseLossBackward>)\ntensor(0.0063, grad_fn=<MseLossBackward>)\ntensor(0.0063, grad_fn=<MseLossBackward>)\ntensor(0.0062, grad_fn=<MseLossBackward>)\ntensor(0.0062, grad_fn=<MseLossBackward>)\ntensor(0.0062, grad_fn=<MseLossBackward>)\ntensor(0.0062, grad_fn=<MseLossBackward>)\ntensor(0.0062, grad_fn=<MseLossBackward>)\ntensor(0.0062, grad_fn=<MseLossBackward>)\ntensor(0.0061, grad_fn=<MseLossBackward>)\ntensor(0.0061, grad_fn=<MseLossBackward>)\ntensor(0.0061, grad_fn=<MseLossBackward>)\ntensor(0.0061, grad_fn=<MseLossBackward>)\ntensor(0.0061, grad_fn=<MseLossBackward>)\ntensor(0.0061, grad_fn=<MseLossBackward>)\ntensor(0.0061, grad_fn=<MseLossBackward>)\n\nvalidation loss\ntensor(0.0061, grad_fn=<MseLossBackward>)\ntensor(0.0061, grad_fn=<MseLossBackward>)\ntensor(0.0061, grad_fn=<MseLossBackward>)\ntensor(0.0061, grad_fn=<MseLossBackward>)\ntensor(0.0061, grad_fn=<MseLossBackward>)\ntensor(0.0061, grad_fn=<MseLossBackward>)\ntensor(0.0061, grad_fn=<MseLossBackward>)\ntensor(0.0061, grad_fn=<MseLossBackward>)\ntensor(0.0061, grad_fn=<MseLossBackward>)\ntensor(0.0061, grad_fn=<MseLossBackward>)\ntensor(0.0061, grad_fn=<MseLossBackward>)\ntensor(0.0061, grad_fn=<MseLossBackward>)\ntensor(0.0061, grad_fn=<MseLossBackward>)\ntensor(0.0061, grad_fn=<MseLossBackward>)\ntensor(0.0061, grad_fn=<MseLossBackward>)\ntensor(0.0061, grad_fn=<MseLossBackward>)\ntensor(0.0061, grad_fn=<MseLossBackward>)\ntensor(0.0061, grad_fn=<MseLossBackward>)\ntensor(0.0061, grad_fn=<MseLossBackward>)\ntensor(0.0061, grad_fn=<MseLossBackward>)\ntensor(0.0061, grad_fn=<MseLossBackward>)\ntensor(0.0061, grad_fn=<MseLossBackward>)\ntensor(0.0061, grad_fn=<MseLossBackward>)\ntensor(0.0061, grad_fn=<MseLossBackward>)\ntensor(0.0061, grad_fn=<MseLossBackward>)\ntensor(0.0061, grad_fn=<MseLossBackward>)\ntensor(0.0061, grad_fn=<MseLossBackward>)\ntensor(0.0061, grad_fn=<MseLossBackward>)\ntensor(0.0061, grad_fn=<MseLossBackward>)\ntensor(0.0061, grad_fn=<MseLossBackward>)\ntensor(0.0061, grad_fn=<MseLossBackward>)\ntensor(0.0061, grad_fn=<MseLossBackward>)\ntensor(0.0061, grad_fn=<MseLossBackward>)\ntensor(0.0061, grad_fn=<MseLossBackward>)\ntensor(0.0061, grad_fn=<MseLossBackward>)\ntensor(0.0061, grad_fn=<MseLossBackward>)\ntensor(0.0061, grad_fn=<MseLossBackward>)\ntensor(0.0061, grad_fn=<MseLossBackward>)\ntensor(0.0061, grad_fn=<MseLossBackward>)\ntensor(0.0061, grad_fn=<MseLossBackward>)\ntensor(0.0061, grad_fn=<MseLossBackward>)\ntensor(0.0061, grad_fn=<MseLossBackward>)\ntensor(0.0061, grad_fn=<MseLossBackward>)\ntensor(0.0061, grad_fn=<MseLossBackward>)\ntensor(0.0061, grad_fn=<MseLossBackward>)\ntensor(0.0061, grad_fn=<MseLossBackward>)\ntensor(0.0061, grad_fn=<MseLossBackward>)\ntensor(0.0061, grad_fn=<MseLossBackward>)\ntensor(0.0061, grad_fn=<MseLossBackward>)\ntensor(0.0061, grad_fn=<MseLossBackward>)\n<\/code>\nI don\u2019t know why validation loss is always the same. It will be a flat line in the graph. It is not what I want.\nThanks for your time!","y":"Thanks for response!\nThe problem is solved by modifying the procedure of training and validation.\nOriginal:\n1.all epochs training\n\nall epochs validation\n\nCorrect procedure:\n\n\nepoch 1 training\n\n\nepoch 1 validation\n\n\nepoch 2 training\n\n\nepoch 2 validation\n\n\n\u2026\n\u2026","z":"Based on the output it seems your model training has reached a plateau, so you would need to play around with some hyperparameters, such as the learning rate etc.\nIs the training loss decreasing in the following epochs or is it also approx. constant?\nAlso, Variables are deprecated since PyTorch 0.4, so you can use tensors now. \nThanks for response!\nThe problem is solved by modifying the procedure of training and validation.\nOriginal:\n1.all epochs training\n\nall epochs validation\n\nCorrect procedure:\n\n\nepoch 1 training\n\n\nepoch 1 validation\n\n\nepoch 2 training\n\n\nepoch 2 validation\n\n\n\u2026\n\u2026"},{"x":"Hi everyone,\nI have a dataset with 885 images and I have to perform data augmentation generating 3000 training examples for each image by random translation and random rotation. However since the dataset would increase too much and I cannot store all the images on the disk. I know that I can perform transform \u2018on the fly\u2019 but I need to create the augment the dataset and then train the complete dataset.\nIs there a way to augment data and save augmented \u2018dataLoader\u2019 in separate files, use another process to load the saved \u2018dataloaders\u2019 and train the network with all the examples?","y":"Thanks for the information.\nIf I understand the use case correctly, you would have 884*3000 images in each epoch, where each of the original 884 images will be randomly transformed 3000 times.\nIn that case, my previous proposal should work and this code snippet would show, what I meant:\n<code class=\"lang-python\">class MyDataset(Dataset):\n    def __init__(self, data, length):\n        self.data = data\n        self.data_len = len(self.data)\n        self.len = length\n        \n    def __getitem__(self, index):\n        data_idx = index % self.data_len\n        print('index {}, data_idx {}'.format(index, data_idx))\n        x = self.data[data_idx]\n        return x\n    \n    def __len__(self):\n        return self.len\n\n\ndata = torch.randn(10, 1)\nlength = 30        \ndataset = MyDataset(data, length)\nloader = DataLoader(dataset, batch_size=2)\n\nfor x in loader:\n    print(x.shape)\n<\/code>\nBasically, you would artificially increase the number of samples by passing the length directly to the dataset and inside the __getitem__ method you could use the modulo operation to repeatedly sample from the data.\nLet me know, if this would work for you.","z":"After applying the transformation on your data (without training the model), you could save each sample via torch.save. Later in your training script, you could create a custom Dataset, as explained here with link \"https:\/\/pytorch.org\/tutorials\/beginner\/data_loading_tutorial.html\", and load each augmented sample instead of the image.\nLet me know, if you get stuck.\nI have tried to do the torch.save but the problem is that creating 3000 transformed sample for each image and store them would occupy too much memory on disk.\nI misunderstood the question then, sorry.\nCould you explain, how this workflow would look like:\n\nI know that I can perform transform \u2018on the fly\u2019 but I need to create the augment the dataset and then train the complete dataset.\nIs there a way to augment data and save augmented \u2018dataLoader\u2019 in separate files, use another process to load the saved \u2018dataloaders\u2019 and train the network with all the examples?\n\nUsually you would lazily load the data in the Dataset.__getitem__ method and apply the transformation.\nI don\u2019t really understand the point of storing the DataLoader (or Dataset).\nIf you want to increase the number of returned samples, you could multiply the number of sample by 3000 in Dataset.__len__.\nThank you so much for your help. The dataset contains 884 images. For each image in the dataset I need to generate 3000 new images rotated and translated randomly with related bounding boxes without storing them for training. But I do not know how generate exactly 3000 transformed image for each image and consequently how these transformations are reflected in the number of samples.\nThanks for the information.\nIf I understand the use case correctly, you would have 884*3000 images in each epoch, where each of the original 884 images will be randomly transformed 3000 times.\nIn that case, my previous proposal should work and this code snippet would show, what I meant:\n<code class=\"lang-python\">class MyDataset(Dataset):\n    def __init__(self, data, length):\n        self.data = data\n        self.data_len = len(self.data)\n        self.len = length\n        \n    def __getitem__(self, index):\n        data_idx = index % self.data_len\n        print('index {}, data_idx {}'.format(index, data_idx))\n        x = self.data[data_idx]\n        return x\n    \n    def __len__(self):\n        return self.len\n\n\ndata = torch.randn(10, 1)\nlength = 30        \ndataset = MyDataset(data, length)\nloader = DataLoader(dataset, batch_size=2)\n\nfor x in loader:\n    print(x.shape)\n<\/code>\nBasically, you would artificially increase the number of samples by passing the length directly to the dataset and inside the __getitem__ method you could use the modulo operation to repeatedly sample from the data.\nLet me know, if this would work for you.\nThank you so much. It works for me. I have a question if you could help me more. Following the tutorial https:\/\/pytorch.org\/tutorials\/beginner\/data_loading_tutorial.html I apply the transformation in Dataset.__getitem__ method. I have a doubt. For each epoch the dataset contains always the same transformed samples? or for each epoch the samples in the dataset are transformed randomly and therefore the dataset is different for each epoch?\nThe data will be transformed randomly for each call and thus also for each epoch as long as you don\u2019t manually seed the code and force the random number generator to apply the same \u201crandom\u201d transformations."},{"x":"<code class=\"lang-auto\">class FaceLandmarksDataset(Dataset):\n    \"\"\"Face Landmarks dataset.\"\"\"\n\n    def __init__(self, csv_file, root_dir, transform=None):\n\n        self.landmarks_frame = pd.read_csv(csv_file)\n        self.root_dir = root_dir\n        self.transform = transform\n\n    def __len__(self):\n        return len(self.landmarks_frame)\n\n    def __getitem__(self, idx):\n        if torch.is_tensor(idx):\n            idx = idx.tolist()\n\n        img_name = os.path.join(self.root_dir,\n                                self.landmarks_frame.iloc[idx, 0])\n        image = io.imread(img_name)\n        landmarks = self.landmarks_frame.iloc[idx, 1:]\n        landmarks = np.array([landmarks])\n        landmarks = landmarks.astype('float').reshape(-1, 2)\n        sample = {'image': image, 'landmarks': landmarks}\n\n        if self.transform:\n            sample = self.transform(sample)\n\n        return sample\nclass CsvDataset(Dataset):\n\n    def __init__(self, path, csv_file, transform=None):\n\n        self.root = os.getcwd()\n        for i in path:  # deal with system difference\n            self.root = os.path.join(self.root, i)\n        self.csv = pd.read_csv(os.path.join(self.root, csv_file))\n        self.transform = transform\n\n    def __len__(self):\n        return len(self.csv)\n\n    def __getitem__(self, idx):\n        # if torch.is_tentor(idx):\n        #     idx = idx.tolist()\n\n        img_path = os.path.join(self.root, self.csv.iloc[idx, 0])\n        img = io.imread(img_path)\n        label = self.csv.iloc[idx, 1:]\n        label = np.array([label])\n        label = label.astype('float').reshape(-1, 2)\n        sample = {'image':img, 'label':label}\n        if self.transform:\n            sample = self.transform(sample)\n\n        return sample\n<\/code>\nI tried to write a custom dataset according the tutorial below.\n\nhttps:\/\/pytorch.org\/tutorials\/beginner\/data_loading_tutorial.html\n\nAnd I got this error:\n<code class=\"lang-auto\">Traceback (most recent call last):\n  File \"C:\/Users\/Nathan\/pytorch\/hierarchy\/CIFAR10-tutorial.py\", line 102, in <module>\n    sample = face_dataset[i]\n  File \"C:\/Users\/Nathan\/pytorch\/hierarchy\/CIFAR10-tutorial.py\", line 82, in __getitem__\n    if torch.is_tentor(idx):\nAttributeError: module 'torch' has no attribute 'is_tentor'\n<\/code>\nuntil I comment the if statement\nFaceLandmarksDataset works well with the if statement but my CsvDataset did not!\n","y":"You have a small typo in the line of code.\ntorch.is_tensor should work, while you are writing is_tentor ","z":"You have a small typo in the line of code.\ntorch.is_tensor should work, while you are writing is_tentor \nThanks! I will increase my font size. \nHi . I have a possibly unrelated question regarding this tutorial\u2019s code.\nWhy is the following code snippet included in the __getitem__ function:\n<code class=\"lang-python\">...\n    if torch.is_tensor(idx):\n        idx = idx.tolist()\n...\n<\/code>\nIn my understanding, the method can only read one sample, as only one image file can only be read by io.imread(). Giving multiple indices should raise an error. The tolist() method here seems a bit confusing. Could you give some comments on that? Thanks for your reply in advance.\nThe pandas.DataFrame doesn\u2019t seem to accept tensors as indices.\nI guess the tolist() operation might not really be necessary, but instead a Python literal might work.\nYou could check it by using idx = idx.item() and rerun the script."},{"x":"Hi i try to test my model and this error pop up i use the similar method when i train the data and there is no error can anyone check what went wrong?\nthis is when i train\n<code class=\"lang-auto\">def train(dataloader,net): \n      net = load_net(net, 'gpu')\n      net = net.cuda()   \n      epoch = 2\n      criterion = nn.CrossEntropyLoss()\n      optimizer = optim.SGD(net.parameters(), lr=0.001, momentum=0.9)\n      train_loss = list()\n      #set_trace()\n      for i, data in enumerate(dataloader):\n        \n        inps, labs = data\n        inps, labs = inps.cuda(args['device']), labs.cuda(args['device'])\n\n        inps = Variable(inps).cuda(args['device'])\n        labs = Variable(labs).cuda(args['device'])\n        optimizer.zero_grad()\n        outs = net(inps.permute(0, 3, 1, 2).float())\n        soft_outs = F.softmax(outs, dim=1)\n        prds = soft_outs.data.max(1)[1]\n        loss = criterion(outs, labs)\n        loss.backward()\n        optimizer.step()\n        prds = prds.cpu().numpy()\n        inps_np = inps.detach().cpu().numpy()\n        labs_np = labs.detach().cpu().numpy()\n        train_loss.append(loss.data.item\n                        ())\n        print('[epoch %d], [iter %d \/ %d], [train loss %.5f]' % (epoch, i + 1, len(dataloader), np.asarray(train_loss).mean()))\n<\/code>\n<code class=\"lang-auto\">x=train(dataloadertrain,net='mobiface')\n<\/code>\nThis is for test\n<code class=\"lang-auto\">def test(test_loader, net):\n  correct = 0\n  total = 0\n  since_test = time.time()\n  # Iterating over batches.\n  for i, data in enumerate(test_loader):\n    # Obtaining images, labels and paths for batch.\n\n  \n    inps, labs = data\n    inps, labs = inps.cuda(args['device']), labs.cuda(args['device'])\n    #inps.squeeze_(0) # create a mini-batch as expected by the model\n    #labs.squeeze_(0)\n\n    inps = Variable(inps).cuda(args['device'])\n    labs = Variable(labs).cuda(args['device'])\n    # # Forwarding.\n    outs = net(inps.permute(0, 3, 1, 2).float())\n    \n    for j in range(len(outs['out'])):\n      diff = outs['out'][j].argmax(0) - labs[j]\n      diff = diff.int().cpu().numpy()\n      unique, counts = np.unique(diff,return_counts=True)\n      print(unique, counts)\n      total += np.sum(counts)\n      correct += np.sum(counts[unique == 0])\n      print(total,correct)\n  \n  time_elapsed = time.time() - since_test ##Edit\n  print('Training complete in {:.0f}m {:.0f}s'.format(time_elapsed \/\/ 60, time_elapsed % 60)) # Edit\n  print(f\"accuracy is {(correct\/total)*100}%\")\n\n<\/code>\n<code class=\"lang-auto\">test(dataloadertest, x)\n<\/code>\nI found this method online and it look fine to another person but when i try to impliment it this error pop up\n<code class=\"lang-auto\"><ipython-input-142-ce0ef120179a> in <module>()\n----> 1 test(dataloadertest, x)\n\n<ipython-input-141-4000f8300d82> in test(test_loader, net)\n     16     labs = Variable(labs).cuda(args['device'])\n     17     # # Forwarding.\n---> 18     outs = net(inps.permute(0, 3, 1, 2).float())\n     19 \n     20     for j in range(len(outs['out'])):\n\nTypeError: 'NoneType' object is not callable\n<\/code>","y":"It should look like this:\n<code class=\"lang-python\">.no_grad()\ndef test(data_loader, net):\n    net.eval()\n    # the rest of the code\n<\/code>\nPlease check the docs for any further questions.","z":"You are passing x that is None to test as the second argument. The train function doesn\u2019t have an explicit return statement so it returns None. Just return the model in train to remove the error. Additionally, you can decorate the test function with torch.no_grad() to speed up the inference and call net.eval() to set the model in evaluation mode.\nso am i correct return net for training and\ncan you explain what is torch.no_grad() and net.eval()\nWithout torch.no_grad() any operation that involves the tensors with requires_grad=True is recorded by autograd and this process consumes memory. Plus, it can make your code less error-prone (it can save you from occasions when you call backward in the eval phase by accident).\nCalling net.eval() is mandatory on the model if the model contains layers\/modules with different behaviour between the train and eval phase (dropout, batchnorm, etc.).\njust i never heard of this 2 technique before where do it put in in a code?\nIt should look like this:\n<code class=\"lang-python\">.no_grad()\ndef test(data_loader, net):\n    net.eval()\n    # the rest of the code\n<\/code>\nPlease check the docs for any further questions.\nthank you for the help if i got more problem can i ask?\nHi just one last question do you have good reinforcement i can fine to develop my own convolution layer just i need to develop my own convolution layer but im not sure where to start nd what is the basic principle to develop my own one\nPlease open a new  thread for that."},{"x":"<code class=\"lang-auto\">import torch\nimport torch.nn as nn\n\ndevice = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n<\/code>\nThis gives me an error Module 'torch' has no 'device' member i\u2019m using version 1.6\n<code class=\"lang-auto\">>>> print(torch.__version__)\n1.6.0\n<\/code>\nIt was working in Google Colab, also conda is available\n<code class=\"lang-auto\">if torch.cuda.is_available(): print(True)\nTrue\n<\/code>\nIf i do like this net = net.to(\"cuda:0\") i get no output at all and computer starts freezing\nEDIT\nI encounter this problem only in visual studio code\u2026\nhttps:\/\/github.com\/kanedaaaa\/mnist-pytorch\/blob\/master\/models\/cnn.py here is the code, very simple cnn model","y":"Found a solution\ndevice = 'cuda' if torch.cuda.is_available() else 'cpu'\nMy pc froze a bit but finally it gave my desired output.\nidk what happened and why torch.device isnt working tho\u2026","z":"Hi,\nOpening a brand new colab notebook and adding\n<code class=\"lang-auto\">import torch\nprint(torch.__version__)\nprint(torch.device)\n<\/code>\nprints just fine.\nDid you override some things in the torch library by any chance?\nUh, no, I\u2019m working in vsc, vsc gives me that error.\nIt is working in jupyter. I mentioned colab because thought maybe torch.device was changed to something else in new update.\nEDIT\nI dont think so that i messed up something, since jupyter is not giving me any errors and + its fresh new install\nI don\u2019t think any change happened no.\nCan you give more info about your platform, python and how you installed pytorch?\nFound a solution\ndevice = 'cuda' if torch.cuda.is_available() else 'cpu'\nMy pc froze a bit but finally it gave my desired output.\nidk what happened and why torch.device isnt working tho\u2026"},{"x":"My understanding was that having GPU\u2019s with poor half-precision would be a big obstacle for running AMP.  For example, compare the following GPU\u2019s:\nNVIDIA 1080Ti\nSingle Precision GFLOPS.    10609\nHalf Precision GFLOPS         166\nTensor Cores                          0\nCompare this with an RTX GPU like the 2080 Ti\nRTX 2080Ti\nSingle Precision GFLOPS.    11750\nHalf Precision GFLOPS         23500\nTensor Cores                          544\nIn AMP\/Mixed Precision many of the operations are taking place at half-precision, this gives you incredible memory savings and speed.\nI tested this with some models tonight on my 1080Ti\u2019s (4 using DataParallel), and I was blown away.  Epochs complete 30% faster, and I could almost double my batch size.  So I don\u2019t understand why it wasn\u2019t dogged slow since the half-precision GFLOPS on a 1080Ti are abysmal.  Is AMP doing some sort of trickery, such as using 16bit and stuff two numbers into a 32bit register or something?","y":"It\u2019s possible your network is mostly bandwidth bound rather than compute bound, in which case the 1\/2X memory traffic for fp16 ops would allow a speedup.  It\u2019s also probable that many Pytorch kernels compute internally in fp32 even if the input\/output is fp16, so the lack of raw fp16 throughput isn\u2019t a problem.  It\u2019s also possible the same is true for cuda library calls (gemms\/convolutions):  they may use fp32 compute internally for fp16 input\/output, so the compute throughput is not worse than fp32 and the required bandwidth is reduced.","z":"It\u2019s possible your network is mostly bandwidth bound rather than compute bound, in which case the 1\/2X memory traffic for fp16 ops would allow a speedup.  It\u2019s also probable that many Pytorch kernels compute internally in fp32 even if the input\/output is fp16, so the lack of raw fp16 throughput isn\u2019t a problem.  It\u2019s also possible the same is true for cuda library calls (gemms\/convolutions):  they may use fp32 compute internally for fp16 input\/output, so the compute throughput is not worse than fp32 and the required bandwidth is reduced."},{"x":"In CUDA\/Apex AMP, you set the optimization level:\nmodel, optimizer = amp.initialize(model, optimizer, opt_level=\"O1\")\nIn the examples I read on PyTorch\u2019s website, I don\u2019t see anything analogous to this.  How is this accomplished?","y":"Native AMP is similar to the recommended O1 level and doesn\u2019t use any other opt_levels.","z":"Native AMP is similar to the recommended O1 level and doesn\u2019t use any other opt_levels.\nThanks, it would seem 01 is the way to go most of the time, so I don\u2019t think I\u2019m losing anything.\nLet us know, if you encounter any issues. \nI just have one issue I have seen, which I posted here:\n\n\n\n\nTorch.cuda.amp and Accuracy with link \"https:\/\/discuss.pytorch.org\/t\/torch-cuda-amp-and-accuracy\/92746\"\n\n\n    I converted my training loop to use AMP, and I notice my accuracy numbers are now all 0.  What needs to be changed to work with AMP? z was calculated within a with_autocast(): clause so I thought it should be fine, but apparently it is not. \n        print(\"Training..............................\\r\", end='') \n        train_iter = iter(train_loader)\n        next_batch = train_iter.next()\n        next_batch = [_.cuda(non_blocking=True) for _ in next_batch ]\n        for idx in range(len(train_loader)\u2026\n  \n\n"},{"x":"I would like to visualize the number of image data for each class of the cifar10 dataset on a graph.I have tried implementing this on pytorch but I couldn\u2019t get any satisfying results. Can anyone please help me to plot the CIFAR10 Dataset classes frequency .\nHere is an exemple of an implementation i have tried but i keep getting an error when comparing labels with the classes indexes\n<code class=\"lang-auto\">def classes_freq():\n    for images, labels in trainloader_classes:\n          #print(labels.shape)\n      \n          images = images.to(device)\n          labels = labels.to(device)\n          count = [0]*10\n          for k in range(len(trainloader_classes)):\n            for i,e in enumerate(classes):\n             if (labels[k].item() == i):\n                count[i]+=1\n    return count\n\nclasses_freq()\n\n\nimport matplotlib.pyplot as plt\n\ndef plot(y_units):\n\n  x_units = ['plane', 'car', 'bird', 'cat',\n           'deer', 'dog', 'frog', 'horse', 'ship', 'truck']\n\n  plt.bar(x_units, y_units,width=0.8, color=['red','blue'])\n\n  plt.xlabel('x - axis')\n  plt.ylabel('y - axis')\n  plt.title('classes frequency')\n\n  plt.show()\n\n#plot([10,20,23,50,60,70,80,90,10,45])\n<\/code>\nError:\n\nIndexError: index 1 is out of bounds for dimension 0 with size 1\n","y":"You can read about the CIFAR10 class frequencies on the offical website with link \"https:\/\/www.cs.toronto.edu\/~kriz\/cifar.html\" (they are already well knowned).\nIt\u2019s much better to iterate over torch.utils.data.Dataset directly instead of DataLoader (you don\u2019t need for your task batching, parallelization and other utilities that this class provides).\nThe entire error\u2019s stack trace would be helpful for us to find the error so I\u2019ll give you my, much more performant solution:\n<code class=\"lang-python\">train_dataset = torchvision.datasets.CIFAR10(root='.\/data', train=True,\n                                             download=True, transform=transform)\n\nlabels = torch.tensor([instance[1] for instance in train_dataset])\nclass_freq = labels.bincount()\n\nclasses = ['plane', 'car', 'bird', 'cat','deer', 'dog', \n           'frog', 'horse', 'ship', 'truck']\n\nindexes = np.arange(len(classes))\nwidth = 0.3\nplt.bar(indexes, class_freq.numpy(), align='edge', width=width)\nplt.xticks(indexes + width * 0.5, classes)\nplt.show()\n<\/code>\nIt\u2019s handy to use torch.bincount to count the frequencies in one step.","z":"You can read about the CIFAR10 class frequencies on the offical website with link \"https:\/\/www.cs.toronto.edu\/~kriz\/cifar.html\" (they are already well knowned).\nIt\u2019s much better to iterate over torch.utils.data.Dataset directly instead of DataLoader (you don\u2019t need for your task batching, parallelization and other utilities that this class provides).\nThe entire error\u2019s stack trace would be helpful for us to find the error so I\u2019ll give you my, much more performant solution:\n<code class=\"lang-python\">train_dataset = torchvision.datasets.CIFAR10(root='.\/data', train=True,\n                                             download=True, transform=transform)\n\nlabels = torch.tensor([instance[1] for instance in train_dataset])\nclass_freq = labels.bincount()\n\nclasses = ['plane', 'car', 'bird', 'cat','deer', 'dog', \n           'frog', 'horse', 'ship', 'truck']\n\nindexes = np.arange(len(classes))\nwidth = 0.3\nplt.bar(indexes, class_freq.numpy(), align='edge', width=width)\nplt.xticks(indexes + width * 0.5, classes)\nplt.show()\n<\/code>\nIt\u2019s handy to use torch.bincount to count the frequencies in one step.\n\n\n\n mariosasko:\n\n<code class=\"lang-auto\">labels = torch.tensor([instance[1] for instance in train_dataset])\nclass_freq = labels.bincount()\n\nclasses = ['plane', 'car', 'bird', 'cat','deer', 'dog', \n           'frog', 'horse', 'ship', 'truck']\n\nindexes = np.arange(len(classes))\nwidth = 0.3\nplt.bar(indexes, class_freq.numpy(), align='edge', width=width)\nplt.xticks(indexes + width * 0.5, classes)\nplt.show()\n<\/code>\n\n\nA clean and really much more better solution ! Thank you .\nBut I really want to understand why the error occurred,and why exactly the index k is out of bounds? Is it because the first loop does not pass to the next image data in the trainloader_classes?\nOh I see it.\nIt seems that this line is the issue: if (labels[k].item() == i):. You are trying to index with 1 a tensor (labels) with shape [1] (it would work fine if the index was 0).\nyes with the index 0 it work , I guess because this loop for k in range(len(trainloader_classes))is iterating over one image while it\u2019s suppose to pass to the next label in the next iteration"},{"x":"I\u2019m trying to calculate the similarity between two activation matrix of two different models following the Teacher Guided Architecture Search paper. My question is, does python has a native implementation of pdist similar to Scipy.spatial.distance.pdist? And a native application for Pearson correlation?","y":"I assume you meant PyTorch, not Python. For pdist see torch.nn.functional.pdist and for Pearson correlation check out this issue with link \"https:\/\/github.com\/pytorch\/pytorch\/issues\/1254\".","z":"I assume you meant PyTorch, not Python. For pdist see torch.nn.functional.pdist and for Pearson correlation check out this issue with link \"https:\/\/github.com\/pytorch\/pytorch\/issues\/1254\".\nThank you!\ndo you know what metric pytorch for pdist? for example in Scipy.spatial.distance.pdist, you can select between cosine, euclidean, \u2026etc\nCheck out the docs with link \"https:\/\/pytorch.org\/docs\/stable\/nn.functional.html#pdist\" for available metrics (in that regard pdist in PyTorch is very limited compared to the scipy\u2019s counterpart)."},{"x":"I\u2019m following this example on doc with link \"https:\/\/pytorch.org\/docs\/stable\/tensors.html#torch.Tensor.repeat\"\n<code class=\"lang-auto\">In [42]: x = torch.tensor([1,2,3])\n\n\nIn [45]: x.repeat(4,2) \nOut[45]: tensor([[1, 2, 3, 1, 2, 3],\n        [1, 2, 3, 1, 2, 3],\n        [1, 2, 3, 1, 2, 3],\n        [1, 2, 3, 1, 2, 3]])\n\nIn [46]: x.repeat(4,2).shape \nOut[46]: torch.Size([4, 6])\n<\/code>\nSo far, so good.\nBut why does repeating just 1 time on 3rd dimension expand 3rd dim to 3 (not 1)?\n<code class=\"lang-auto\">[On the doc]\n\n>>> x.repeat(4, 2, 1).size()\ntorch.Size([4, 2, 3])\n<\/code>\nDouble checking.\n<code class=\"lang-auto\">In [43]: x.repeat(4,2,1)\nOut[43]:\ntensor([[[1, 2, 3],\n         [1, 2, 3]],\n\n        [[1, 2, 3],\n         [1, 2, 3]],\n\n        [[1, 2, 3],\n         [1, 2, 3]],\n\n        [[1, 2, 3],\n         [1, 2, 3]]])\n<\/code>\nWhy does it behave this way?","y":"maybe implementation of torch.repeat is here,\n\n\ngithub.com with link \"https:\/\/github.com\/zdevito\/ATen\/blob\/2448a0d8b547e8c787bb948a23009c375c46272c\/aten\/src\/ATen\/native\/TensorShape.cpp\"\n\n\nzdevito\/ATen\/blob\/2448a0d8b547e8c787bb948a23009c375c46272c\/aten\/src\/ATen\/native\/TensorShape.cpp with link \"https:\/\/github.com\/zdevito\/ATen\/blob\/2448a0d8b547e8c787bb948a23009c375c46272c\/aten\/src\/ATen\/native\/TensorShape.cpp\"\n<code class=\"lang-cpp\">#include <TH\/THTensor.hpp>\n#include <algorithm>\n#include <vector>\n#include <ATen\/ATen.h>\n#include <ATen\/ExpandUtils.h>\n#include <ATen\/InferSize.h>\n#include <ATen\/NativeFunctions.h>\n#include <ATen\/WrapDimUtils.h>\n#include <c10\/util\/Exception.h>\n#include <c10\/util\/Optional.h>\n#include <ATen\/native\/Resize.h>\n#include <ATen\/SparseTensorUtils.h>\n#include <ATen\/quantized\/QTensorImpl.h>\n#include <algorithm>\n#include <vector>\n#include <ATen\/NamedTensorUtils.h>\n#include <ATen\/core\/EnableNamedTensor.h>\n\nnamespace at {\nnamespace native {\n<\/code>\n\n  This file has been truncated. show original with link \"https:\/\/github.com\/zdevito\/ATen\/blob\/2448a0d8b547e8c787bb948a23009c375c46272c\/aten\/src\/ATen\/native\/TensorShape.cpp\"\n\n\n\n\n\nwhere they say, add new leading dimensions to the tensor if the number of target dimensions is larger than the number of source dimensions, that is this,\nScreenshot (652)1208\u00d7565 27.8 KB with link \"https:\/\/discuss.pytorch.org\/uploads\/default\/original\/3X\/7\/0\/7090b99e78818c65789299f42c8923cbf1e5432b.png\"","z":"maybe, it consider unsqueezed shape, for example,\n<code class=\"lang-auto\">x = torch.tensor([1, 2, 3])\nx.shape\n<\/code>\ngive\n<code class=\"lang-auto\">torch.Size([3])\n<\/code>\nwhen we do,\n<code class=\"lang-auto\">x.repeat(4,2)\n<\/code>\nthen,\nit consider x shape to be\n<code class=\"lang-auto\">torch.size([1, 3])\n<\/code>\nand repeat this tensor 4 times along the 1st dim, 2 times along the 2nd dim, resulting in output shape to be,\n<code class=\"lang-auto\">torch.size([4, 6])\n<\/code>\nwhen we do,\n<code class=\"lang-auto\">x.repeat(4, 2, 1)\n<\/code>\nthen, the unsqueezed shape of x be,\n<code class=\"lang-auto\">torch.size([1, 1, 3])\n<\/code>\nand repeat this tensor 4 times along the 1st dim, 2 times along the 2nd dim, 1 time along the 3rd dim, resulting in output shape to be,\n<code class=\"lang-auto\">torch.size([4, 2, 3])\n<\/code>\nThank you. Yes that\u2019s my guess too. how should we confirm?\nmaybe implementation of torch.repeat is here,\n\n\ngithub.com with link \"https:\/\/github.com\/zdevito\/ATen\/blob\/2448a0d8b547e8c787bb948a23009c375c46272c\/aten\/src\/ATen\/native\/TensorShape.cpp\"\n\n\nzdevito\/ATen\/blob\/2448a0d8b547e8c787bb948a23009c375c46272c\/aten\/src\/ATen\/native\/TensorShape.cpp with link \"https:\/\/github.com\/zdevito\/ATen\/blob\/2448a0d8b547e8c787bb948a23009c375c46272c\/aten\/src\/ATen\/native\/TensorShape.cpp\"\n<code class=\"lang-cpp\">#include <TH\/THTensor.hpp>\n#include <algorithm>\n#include <vector>\n#include <ATen\/ATen.h>\n#include <ATen\/ExpandUtils.h>\n#include <ATen\/InferSize.h>\n#include <ATen\/NativeFunctions.h>\n#include <ATen\/WrapDimUtils.h>\n#include <c10\/util\/Exception.h>\n#include <c10\/util\/Optional.h>\n#include <ATen\/native\/Resize.h>\n#include <ATen\/SparseTensorUtils.h>\n#include <ATen\/quantized\/QTensorImpl.h>\n#include <algorithm>\n#include <vector>\n#include <ATen\/NamedTensorUtils.h>\n#include <ATen\/core\/EnableNamedTensor.h>\n\nnamespace at {\nnamespace native {\n<\/code>\n\n  This file has been truncated. show original with link \"https:\/\/github.com\/zdevito\/ATen\/blob\/2448a0d8b547e8c787bb948a23009c375c46272c\/aten\/src\/ATen\/native\/TensorShape.cpp\"\n\n\n\n\n\nwhere they say, add new leading dimensions to the tensor if the number of target dimensions is larger than the number of source dimensions, that is this,\nScreenshot (652)1208\u00d7565 27.8 KB with link \"https:\/\/discuss.pytorch.org\/uploads\/default\/original\/3X\/7\/0\/7090b99e78818c65789299f42c8923cbf1e5432b.png\""},{"x":"I found that the dimensions of the data set processed by \u201cDataLoader\u201d cannot be \u201cCrossEntropyLoss\u201d is used.How can I modify to make this xor feedforward network work properly?\nI also modified the data set according to \u201chttps:\/\/discuss.pytorch.org\/t\/1d-segmentation-multi-target-not-supported\/63584\u201d, but it still doesn\u2019t work.\n<code class=\"lang-python\">dataset(xor_dataset ) to list:\n[(tensor([0., 0.]), tensor([0])), (tensor([0., 1.]), tensor([1])), (tensor([1., 0.]), tensor([1])), (tensor([1., 1.]), tensor([0]))] \n\ndatasetloader(train_loader) to list result:\n[[tensor([[0., 0.]]), tensor([[0]])], [tensor([[0., 1.]]), tensor([[1]])], [tensor([[1., 0.]]), tensor([[1]])], [tensor([[1., 1.]]), tensor([[0]])]]\n<\/code>\nThe following code output error:\nRuntimeError: 1D target tensor d, multi-target not supported.\n<code class=\"lang-python\">import torch\nimport torch.nn as nn\nfrom torch.utils.data import IterableDataset, DataLoader, TensorDataset\n\nfrom ignite.engine import Events, create_supervised_trainer, create_supervised_evaluator\nfrom ignite.metrics import Accuracy, Loss \n\nclass Net(nn.Module):\n    def __init__(self):\n        super(Net, self).__init__()\n        self.fc1 = nn.Linear(2,3,True)\n        self.fc2 = nn.Linear(3,1,True)\n\n    def forward(self, x):\n        x= torch.sigmoid(self.fc1(x))\n        x = self.fc2(x)\n        return x \n\nclass XorDataset(IterableDataset):\n    def __init__(self):\n        super(XorDataset).__init__()\n        self.inputs = torch.Tensor([\n            [0,0],\n            [0,1],\n            [1,0],\n            [1,1]\n        ])\n        self.targets = torch.tensor([0,1,1,0], dtype=torch.int64).view(-1,1)\n\n    def __iter__(self):\n        result = (i for i in zip(self.inputs, self.targets))\n        return result\n\ndef get_data_loaders():\n    xor_dataset = XorDataset()\n    train_loader = val_looader = xor_dataloader = DataLoader(xor_dataset)\n    return train_loader, val_looader\n\n\nif __name__ == \"__main__\":\n    net = Net()\n    train_loader, val_loader = get_data_loaders()\n    optimizer = torch.optim.SGD(net.parameters(), lr=0.01)\n    criterion = nn.CrossEntropyLoss()\n\n    trainer = create_supervised_trainer(net, optimizer, criterion)\n    \n    def thresholded_output_transform(output):\n        y_pred, y = output\n        y_pred = torch.round(y_pred)\n        return y_pred, y\n\n    val_metrics = {\n        \"accuracy\": Accuracy(thresholded_output_transform),\n        \"nll\": Loss(criterion)\n    }\n    \n    evaluator = create_supervised_evaluator(net, metrics=val_metrics)\n\n    .on(Events.ITERATION_COMPLETED(every=5000))\n    def log_training_loss(trainer):\n        print(\"Epoch[{}] Loss: {:.4f}\".format(trainer.state.epoch, trainer.state.output))\n\n\n    trainer.run(train_loader, max_epochs=20000)\n<\/code>","y":"I found the problem:\n\nI misunderstood the three sentences of Shape in <a>https:\/\/pytorch.org\/docs\/stable\/generated\/torch.nn.CrossEntropyLoss.html?highlight=crossentropyloss#torch.nn.CrossEntropyLoss<\/a>. (doc too crude).\nThe shape of output in my code is 2. So the following code should be changed like this:\n\n<code class=\"lang-python\">     def __init__(self):\n         super(Net, self).__init__()\n         self.fc1 = nn.Linear(2,3,True)\n         self.fc2 = nn.Linear(3,1,True)\n<\/code>\ncorrect:\n<code class=\"lang-python\">         self.linear1 = nn.Linear(2, 3)\n         self.linear2 = nn.Linear(3, 2)\n<\/code>\n\nData set output format. I am too rigid and always want to change it in pytorch.\n\nIt took me two weeks to finally figure it out, and I found a good tutorial: https:\/\/d2l.ai\/chapter_linear-networks\/softmax-regression-concise.html.\nFinal code:\n<code class=\"lang-python\">import torch\nimport torch.nn as nn\nfrom torch.utils.data import IterableDataset, DataLoader, TensorDataset\n\nimport math \n\nfrom ignite.engine import Events, create_supervised_trainer, create_supervised_evaluator\nfrom ignite.metrics import Accuracy, Loss \n\nfrom ignite.contrib.handlers.visdom_logger import *\n\nclass Net(nn.Module):\n    def __init__(self):\n        super(Net, self).__init__()\n        self.linear1 = nn.Linear(2, 3)\n        self.linear2 = nn.Linear(3, 2)\n        self.relu = nn.ReLU()\n\n    def forward(self, x):\n        y1 = self.linear1(x)\n        y3 = self.relu(y1)\n        y = self.linear2(y3)\n        return y\n\nclass XorDataset(IterableDataset):\n    def __init__(self):\n        super(XorDataset).__init__()\n        self.x = [torch.tensor(i, dtype=torch.float) for i in [[0,0],[0,1],[1,0],[1,1]]]\n        self.y = [torch.tensor(i, dtype=torch.long) for i in [0,1,1,0]]\n\n    def __iter__(self):\n        worker_info = torch.utils.data.get_worker_info()\n        if worker_info is None:\n            iter_x = self.x\n            iter_y = self.y\n        else:\n            pre_worker = int(math.ceil(len(self.x) \/ float(worker_info.num_workers)))\n            worker_id = worker_info.id\n            start = 0 + worker_id * pre_worker\n            end = min(start + pre_worker, len(self.x))\n            iter_x = self.x[start:end]\n            iter_y = self.y[start:end]\n        result = (i for i in zip(iter_x,iter_y))\n        return result\n\ndef get_data_loaders():\n    xorDataset = XorDataset()\n    xorDataLoader = DataLoader(xorDataset,num_workers=1)\n    train_iter = test_iter = xorDataLoader\n    return train_iter, test_iter\n\n\nif __name__ == \"__main__\":\n    train_loader, val_loader = get_data_loaders()\n    net = Net()\n    optimizer = torch.optim.SGD(net.parameters(), lr=0.01)\n    criterion = nn.CrossEntropyLoss()\n\n    trainer = create_supervised_trainer(net, optimizer, criterion)\n    \n    def thresholded_output_transform(output):\n        y_pred, y = output\n        y_pred = torch.round(y_pred)\n        return y_pred, y\n\n    val_metrics = {\n        \"accuracy\": Accuracy(thresholded_output_transform),\n        \"nll\": Loss(criterion)\n    }\n    \n    evaluator = create_supervised_evaluator(net, metrics=val_metrics)\n\n    .on(Events.ITERATION_COMPLETED(every=5000))\n    def log_training_loss(trainer):\n        print(\"Epoch[{}] Loss: {:.2f}\".format(trainer.state.epoch, trainer.state.output))\n\n    # .on(Events.EPOCH_COMPLETED)\n    # def log_training_results(trainer):\n    #     evaluator.run(train_loader)\n        # metrics = evaluator.state.metrics\n        # print(\"Training Results - Epoch: {}  Avg accuracy: {:.2f} Avg loss: {:.2f}\"\n        #   .format(trainer.state.epoch, metrics[\"accuracy\"], metrics[\"nll\"]))\n\n    # .on(Events.EPOCH_COMPLETED)\n    # def log_validation_results(trainer):\n    #     evaluator.run(val_loader)\n        # metrics = evaluator.state.metrics\n        # print(\"Validation Results - Epoch: {}  Avg accuracy: {:.2f} Avg loss: {:.2f}\"\n        #   .format(trainer.state.epoch, metrics[\"accuracy\"], metrics[\"nll\"]))\n\n    # vd_logger = VisdomLogger()\n    # vd_logger.attach_output_handler(\n    #     evaluator,\n    #     event_name=Events.EPOCH_COMPLETED,\n    #     tag=\"training\",\n    #     metric_names=[\"nll\", \"accuracy\"],\n    #     global_step_transform=global_step_from_engine(trainer),\n    # )\n    \n\n    trainer.run(train_loader, max_epochs=20000)\n\n\n<\/code>","z":"I guess the target tensor has two dimensions, while only the batch dimension is expected.\nThe two dimensions might be introduces by the view(-1, 1) op in:\n<code class=\"lang-python\">self.targets = torch.tensor([0,1,1,0], dtype=torch.int64).view(-1,1)\n<\/code>\nYou could either remove dim1 via:\n<code class=\"lang-python\">loss = criterion(output, target.squeeze(1))\n<\/code>\nor try to use view(-1) instead (the view might not be necessary at all).\nI found the problem:\n\nI misunderstood the three sentences of Shape in <a>https:\/\/pytorch.org\/docs\/stable\/generated\/torch.nn.CrossEntropyLoss.html?highlight=crossentropyloss#torch.nn.CrossEntropyLoss<\/a>. (doc too crude).\nThe shape of output in my code is 2. So the following code should be changed like this:\n\n<code class=\"lang-python\">     def __init__(self):\n         super(Net, self).__init__()\n         self.fc1 = nn.Linear(2,3,True)\n         self.fc2 = nn.Linear(3,1,True)\n<\/code>\ncorrect:\n<code class=\"lang-python\">         self.linear1 = nn.Linear(2, 3)\n         self.linear2 = nn.Linear(3, 2)\n<\/code>\n\nData set output format. I am too rigid and always want to change it in pytorch.\n\nIt took me two weeks to finally figure it out, and I found a good tutorial: https:\/\/d2l.ai\/chapter_linear-networks\/softmax-regression-concise.html.\nFinal code:\n<code class=\"lang-python\">import torch\nimport torch.nn as nn\nfrom torch.utils.data import IterableDataset, DataLoader, TensorDataset\n\nimport math \n\nfrom ignite.engine import Events, create_supervised_trainer, create_supervised_evaluator\nfrom ignite.metrics import Accuracy, Loss \n\nfrom ignite.contrib.handlers.visdom_logger import *\n\nclass Net(nn.Module):\n    def __init__(self):\n        super(Net, self).__init__()\n        self.linear1 = nn.Linear(2, 3)\n        self.linear2 = nn.Linear(3, 2)\n        self.relu = nn.ReLU()\n\n    def forward(self, x):\n        y1 = self.linear1(x)\n        y3 = self.relu(y1)\n        y = self.linear2(y3)\n        return y\n\nclass XorDataset(IterableDataset):\n    def __init__(self):\n        super(XorDataset).__init__()\n        self.x = [torch.tensor(i, dtype=torch.float) for i in [[0,0],[0,1],[1,0],[1,1]]]\n        self.y = [torch.tensor(i, dtype=torch.long) for i in [0,1,1,0]]\n\n    def __iter__(self):\n        worker_info = torch.utils.data.get_worker_info()\n        if worker_info is None:\n            iter_x = self.x\n            iter_y = self.y\n        else:\n            pre_worker = int(math.ceil(len(self.x) \/ float(worker_info.num_workers)))\n            worker_id = worker_info.id\n            start = 0 + worker_id * pre_worker\n            end = min(start + pre_worker, len(self.x))\n            iter_x = self.x[start:end]\n            iter_y = self.y[start:end]\n        result = (i for i in zip(iter_x,iter_y))\n        return result\n\ndef get_data_loaders():\n    xorDataset = XorDataset()\n    xorDataLoader = DataLoader(xorDataset,num_workers=1)\n    train_iter = test_iter = xorDataLoader\n    return train_iter, test_iter\n\n\nif __name__ == \"__main__\":\n    train_loader, val_loader = get_data_loaders()\n    net = Net()\n    optimizer = torch.optim.SGD(net.parameters(), lr=0.01)\n    criterion = nn.CrossEntropyLoss()\n\n    trainer = create_supervised_trainer(net, optimizer, criterion)\n    \n    def thresholded_output_transform(output):\n        y_pred, y = output\n        y_pred = torch.round(y_pred)\n        return y_pred, y\n\n    val_metrics = {\n        \"accuracy\": Accuracy(thresholded_output_transform),\n        \"nll\": Loss(criterion)\n    }\n    \n    evaluator = create_supervised_evaluator(net, metrics=val_metrics)\n\n    .on(Events.ITERATION_COMPLETED(every=5000))\n    def log_training_loss(trainer):\n        print(\"Epoch[{}] Loss: {:.2f}\".format(trainer.state.epoch, trainer.state.output))\n\n    # .on(Events.EPOCH_COMPLETED)\n    # def log_training_results(trainer):\n    #     evaluator.run(train_loader)\n        # metrics = evaluator.state.metrics\n        # print(\"Training Results - Epoch: {}  Avg accuracy: {:.2f} Avg loss: {:.2f}\"\n        #   .format(trainer.state.epoch, metrics[\"accuracy\"], metrics[\"nll\"]))\n\n    # .on(Events.EPOCH_COMPLETED)\n    # def log_validation_results(trainer):\n    #     evaluator.run(val_loader)\n        # metrics = evaluator.state.metrics\n        # print(\"Validation Results - Epoch: {}  Avg accuracy: {:.2f} Avg loss: {:.2f}\"\n        #   .format(trainer.state.epoch, metrics[\"accuracy\"], metrics[\"nll\"]))\n\n    # vd_logger = VisdomLogger()\n    # vd_logger.attach_output_handler(\n    #     evaluator,\n    #     event_name=Events.EPOCH_COMPLETED,\n    #     tag=\"training\",\n    #     metric_names=[\"nll\", \"accuracy\"],\n    #     global_step_transform=global_step_from_engine(trainer),\n    # )\n    \n\n    trainer.run(train_loader, max_epochs=20000)\n\n\n<\/code>"},{"x":"The following snippet of code which converts an image into a tensor, results in access violation exception when run in debug mode only.\n<code class=\"lang-cpp\">auto ToTensor(cv::Mat img, bool show_output = false, bool unsqueeze = false, int unsqueeze_dim = 0)\n{\n    try\n    {\n        if (show_output)\n            std::cout << \"image shape: \" << img.size() << \" channels: \" << img.channels() << \" depth: \" << img.depth() << \" type: \" << img.type() << std::endl;\n        at::Tensor tensor_image = torch::from_blob(img.data, { img.rows, img.cols, img.channels() }, at::kByte);\n\n        if (unsqueeze)\n        {\n            tensor_image.unsqueeze_(unsqueeze_dim);\n            std::cout << \"tensors new shape: \" << tensor_image.sizes() << std::endl;\n        }\n\n        if (show_output)\n        {\n            std::cout << tensor_image.slice(2, 0, 1) << std::endl;\n        }\n        \n        if (show_output)\n            std::cout << \"tenor shape: \" << tensor_image.sizes() << std::endl;\n        \n        return tensor_image;\n    }\n    catch (const c10::Error&amp; e)\n    {\n        std::cout << \"error occured: \" << e.msg() << e.what() << std::endl;\n        return torch::tensor({ 0.229, 0.224, 0.225 });\n    }\n}\nAnd here is  the full exception message :  \n<\/code>\nexception message :\n<code class=\"lang-auto\">Exception thrown at 0x00007FF70568DB3A in test.exe: 0xC0000005: Access violation writing location 0x0000000000000003.\n<\/code>\nWhats wrong here?","y":"OK, it was caused becasue of the release libs were in the PATH and it would read the libs and crash.\nIt\u2019d be great to have different names for debug and release codes so the headaches regarding these would be minimized.","z":"OK, it was caused becasue of the release libs were in the PATH and it would read the libs and crash.\nIt\u2019d be great to have different names for debug and release codes so the headaches regarding these would be minimized."},{"x":"I tried to use a C++ DLL that calls a python module in my C++ frontend which uses torchlib  and upon trying to import the mentioned module, it produces this error :\n\n<code class=\"lang-auto\">The procedure entry point mkldnn_stream_destroy could not be located in \nthe dynamic link library ..\\torch\\lib\\caffe_2_detectron_ops.dll\n<\/code>\nThe issue doesnt exist, when I call the dll in a standalone project (that does not use torchlib).\nwhat is the problem here?","y":"OK. it seems it was caused becasue of a mistmatched versions between the installed torch (1.5.1) and using the latest libtorch (nightly build).\nI had previously reverted back to 1.5.1 and forgot that the libtorch was from the latest nightly build. so this was casued.\nremoving and reinstalling both of them got rid of this issue (","z":" Can you kindly give me a hand in  this?\nI\u2019m really confused about the casue of this error and how to solve it\nOK. it seems it was caused becasue of a mistmatched versions between the installed torch (1.5.1) and using the latest libtorch (nightly build).\nI had previously reverted back to 1.5.1 and forgot that the libtorch was from the latest nightly build. so this was casued.\nremoving and reinstalling both of them got rid of this issue ("},{"x":"I have seen lots of ways to measure time in PyTorch. But what is the most proper way to do it now (both for cpu and cuda)?\nShould I clear the memory cache if I use timeit?\nAnd is it possible to get accurate results if I\u2019m computing on a cluster? And is it a way to make this results reproducible?\nAnd what is better: timeit or profiler?","y":"There are many things you can do CPU-only benchmarking: I\u2019ve used timeit as well as profilers.\nCUDA is asynchronous so you will need some tools to measure time. CUDA events are good for this if you\u2019re timing \u201cadd\u201d on two cuda tensors, you should sandwich the call between CUDA events:\n<code class=\"lang-auto\">start = torch.cuda.Event(enable_timing=True)\nend = torch.cuda.Event(enable_timing=True)\n\nstart.record()\nz = x + y\nend.record()\n\n# Waits for everything to finish running\ntorch.cuda.synchronize()\n\nprint(start.elapsed_time(end))\n\n<\/code>\nThe pytorch autograd profiler is a good way to get timing information as well: https:\/\/pytorch.org\/docs\/stable\/autograd.html?highlight=autograd%20profiler#torch.autograd.profiler.profile. It uses the cuda event api under the hood and is easy to use:\n<code class=\"lang-auto\">with torch.autograd.profiler.profile(use_cuda=True) as prof:\n   \/\/ do something\nprint(prof)\n<\/code>\nIt\u2019ll tell you the CPU and CUDA timings of your functions.","z":"There are many things you can do CPU-only benchmarking: I\u2019ve used timeit as well as profilers.\nCUDA is asynchronous so you will need some tools to measure time. CUDA events are good for this if you\u2019re timing \u201cadd\u201d on two cuda tensors, you should sandwich the call between CUDA events:\n<code class=\"lang-auto\">start = torch.cuda.Event(enable_timing=True)\nend = torch.cuda.Event(enable_timing=True)\n\nstart.record()\nz = x + y\nend.record()\n\n# Waits for everything to finish running\ntorch.cuda.synchronize()\n\nprint(start.elapsed_time(end))\n\n<\/code>\nThe pytorch autograd profiler is a good way to get timing information as well: https:\/\/pytorch.org\/docs\/stable\/autograd.html?highlight=autograd%20profiler#torch.autograd.profiler.profile. It uses the cuda event api under the hood and is easy to use:\n<code class=\"lang-auto\">with torch.autograd.profiler.profile(use_cuda=True) as prof:\n   \/\/ do something\nprint(prof)\n<\/code>\nIt\u2019ll tell you the CPU and CUDA timings of your functions.\nThank you!\nBut does profiler perform synchronization during time measurement?\nThank you for the examples!\nHow important is to perform some dry runs before time measurements?\nOne is usually enough, the main reason for a dry-run is to put your CPU and GPU on maximum performance state. This is especially useful for laptops as laptops CPU are all on powersaving by default.\nCPU and GPU are very quick to switch to the maximum performance test so just doing a 3000x3000 matrix multiplication before the actual benchmark should be enough and takes a couple seconds at most.\nCaveat: on some CPUs, AVX2 workload will downcloak the CPU frequency (and AVX512 is worse)\nLet me apologize for digging this topic, but there is one thing I want to make sure I understand correctly. Let\u2019s say I have a training loop that performs both CPU bound and GPU bound operations in a sequence. It:\n\nLoads a batch of data from the disc\nConverts the data to PyTorch Tensors on a selected device\nDo the necessary calculations(forward, loss, backward)\n\nThe script runs slower then I expected it to run and I wanted to time the respective blocks to see what is the contribution of each of this major actions on the time of processing one batch. My first approach was to try with the python\u2019s time module, but it fooled me and I ended up optimizing a function that should take just a fraction of the total execution time for a whole day, and then when I used the method described in  answer, it turned out that this small function really is meaningless when it comes to the total execution time. So, my question is now, is the above described method device agnostic?  Can I also use the cuda events to time the parts of code that are fully CPU bound, like loading the data from disk? Will the time printed by the below snippet give me an accurate result?\n<code class=\"lang-auto\">start = torch.cuda.Event(enable_timing=True)\nend = torch.cuda.Event(enable_timing=True)\n\nstart.record()\nsome CPU bound operations, i.e. loading data...\nend.record()\n\n# Waits for everything to finish running\ntorch.cuda.synchronize()\n\nprint(start.elapsed_time(end))\n<\/code>\nI would be grateful for clearing this out for me, I just want to avoid misunderstanding.\nShort answer it will work.\nLong answer.\nCuda Events are something which will be marked when cuda starts running some code.\nThe cpu will just dispatch it async to the GPU.\nSo when cpu hits start.record() it send it to the GPU and GPU records the time when it starts executing.\nNow whatever the code be after this, if CPU bound CPU will execute it or if GPU bound CPU will queue it in the cuda stream and keep moving. Now when CPU hits end.record() it again queues it in stream and when GPU executes it will record the time.\nThe magic comes from torch.cuda.synchronize() which is a synchronous call, and makes sure that all cuda commands dispatched before are done. In case of CPU bound code there are only two calls start.record() and end.record() so they will be done.\nI am a little confused by the operations ordering:\nShouldn\u2019t it be:\n<code class=\"lang-auto\">start.record()\nz = x + y\n\n# Waits for everything to finish running\ntorch.cuda.synchronize()\nend.record()\n<\/code>\nI have actually tried before and after and it gives different results, which I did not expect\u2026\nCan someone explains why there is a difference ?\n so which one is the right way? Thanks.\nHi,\nWe have a question that how to use the API torch.autograd.profiler.profile to compute the total time ?\nThanks.\n \nShort answer: You want synchronize() after end.record() as in the original post above.\nLong answer:\nThe way this works is the function call and the events are added to a CUDA stream object, in order. Each kernel and event in a stream is executed in order, synchronously. The only asynchronous part (for a single stream) is with respect to the CPU. When you call torch.cuda.synchronize() it will wait for the GPU to complete all events in the (default) stream, before returning control to the CPU. So you want everything, including your end event, before the synchronization call. You can read more about it in a nice NVIDIA blog post here with link \"https:\/\/devblogs.nvidia.com\/how-implement-performance-metrics-cuda-cc\/\".\nDoes this answer apply when running on multiple GPUs ?\nIf I want to measure the time for model inference on multiple GPUs (4 Tesla), will CUDA events help measure the overall GPU execution time ?\nI\u2019ve tried on colab but find\nt0 = time.time()\noutputs = net(x)\ntorch.cuda.current_stream().synchronize()\nt1 = time.time()\ngives a more accurate measurement\u2026"},{"x":"Hi,\nI got two dict self.theta &amp; self.beta and a fp value eta, when I tried to modify it as below:\n<code class=\"lang-auto\">self.theta[0]=self.theta[0]-eta*self.beta[0]\n<\/code>\nIt returned TypeError: can't multiply sequence by non-int of type 'float'.\nIf I put it into a for loop of the dict, it worked, but I cannot separately manipulate with a single key such as [0] above, have to finish the modification in the for loop in this case.\n<code class=\"lang-auto\">for key in self.theta.items(): \n      self.theta[key[0]]=self.theta[key[0]]-eta*self.dE_dTheta[key[0]]\n<\/code>\nI wonder is it possible to directly modify the self.theta[0] without getting the error? The numpy array seems like a workaround but I need to change the code a lot so I am thinking about solely torch methods.","y":"Thank you for your reply, you are totally right and I solve it with some help from stackoverflow. Specifically, I messed up the index of self.theta which makes me do eta * [] so it raised the error.\nSorry for the \u2018unrelated\u2019 post.\n(https:\/\/stackoverflow.com\/questions\/63302668\/why-does-items-work-for-the-multiplication-between-sequence-and-float#)","z":"This doesn\u2019t seem to be a PyTorch-specific issue, but seems to be raised by Python directly.\nCould you post the content of both dicts and explain the use case a bit?\nThank you for your reply, you are totally right and I solve it with some help from stackoverflow. Specifically, I messed up the index of self.theta which makes me do eta * [] so it raised the error.\nSorry for the \u2018unrelated\u2019 post.\n(https:\/\/stackoverflow.com\/questions\/63302668\/why-does-items-work-for-the-multiplication-between-sequence-and-float#)"},{"x":"I have a for loop in which the first iteration is substantially faster than subsequent iterations. This discrepancy only appears when I use a GPU. On a CPU every iteration takes more or less the same time. I\u2019m wondering what causes this and whether there\u2019s anything I can do to prevent the slow down in later iterations.\nThe code is here. https:\/\/colab.research.google.com\/drive\/1TC6khI8T44KcYCrdAkHpldw_ejRG_eO2?usp=sharing\nApologies if this is a common question or there\u2019s a simple explanation, but I haven\u2019t been able to find any answers.","y":"CUDA operations are executed asynchronously, so you would have to synchronize the code before starting and stopping the timer via torch.cuda.synchronize(). Otherwise you\u2019ll only profile the Python overhead as well as the kernel launch in the first iterations until your script encounters a blocking operation.","z":"CUDA operations are executed asynchronously, so you would have to synchronize the code before starting and stopping the timer via torch.cuda.synchronize(). Otherwise you\u2019ll only profile the Python overhead as well as the kernel launch in the first iterations until your script encounters a blocking operation.\nAh! Thanks, things make sense after this."},{"x":"Here- is the directory(+= folder contents)\nD:\/Data\/\u2026\/Train\/\nPX1_0000- T2W.nii.gz + seg.nii.gz\nPX1_0001- T2W.nii.gz+ seg.nii.gz\n.\n.\n\u2026 PX1_0200- T2W.nii.gz+ seg.nii\nSame follows for 200 patients.\ndef get_list_of_files(base_dir):\nlist_of_lists = []\nfor phase_type in [\u2018train\u2019]:\ncurrent_directory = join(base_dir, phase_type)\npatients = subfolders(current_directory, join=True)\nfor p in patients:\npatient_directory = join(current_directory, p)\nt2_file = join(patient_directory, p + \u201cT2W.nii.gz\u201d)\nseg_file = join(patient_directory, p + \u201cseg.nii.gz\u201d)\nthis_case = [t2_file, seg_file]\nassert all((isfile(i) for i in this_case)), \"some file is missing for patient %s; make sure the following \"\n\u201cfiles are there: %s\u201d % (p, str(this_case))\nlist_of_lists.append(this_case)\nprint(\u201cFound %d patients\u201d % len(list_of_lists))\nreturn list_of_lists\nCAN YOU PLEASE HELP ME IN GETTING?","y":"Code snippets can be added by wrapping them into three backticks ```, which makes debugging easier.\nAre you seeing an error or unexpected results when running the script?\nThis questions doesn\u2019t seem to be PyTorch-specific, so you might also get a better and faster answer e.g. on StackOverflow.","z":"Proper formatting might be great to review problems for others.\nPlease explain problem briefly.\nHi. Thanks!\nMy directory structure is like the above\u2026 follows for 200 folders.\nI want to read in the contents of the file, but it\u2019s not working. Where\u2019s it going wrong?\nCode snippets can be added by wrapping them into three backticks ```, which makes debugging easier.\nAre you seeing an error or unexpected results when running the script?\nThis questions doesn\u2019t seem to be PyTorch-specific, so you might also get a better and faster answer e.g. on StackOverflow."},{"x":"Hi there!\nI am sort of a newbie with tensor manipulation, I have looked around for this, but I can\u2019t seem to get anywhere, and I am looking for something using torch tensors that would allow me to do the following:\n<code class=\"lang-auto\">results = []\nfor xs in x:\n    r = []\n    for xss in xs:\n        r.append(sift(xss))\n    results.append(r)\n<\/code>\nWhere x is a tensor, I am just going through its batch_size and feature_channels dimensions in these loops and applying a SIFT layer for each image patch group.\nI would also like to be able to run this as fast as possible\nThanks in advance!","y":"You can try to extend the custom layer to process batches (all the layers in pytorch can process batches and most of the operations like matrix multiplication too)\nand then process everything as a (b*c,h,w) tensor without envolving loops","z":"You can just do that as it is written, however, since xss comes from xs which comes from x you can run the whole xs in one run.\nSorry, how would I do it as it is written? This is implemented with python lists which are really slow. I want to make this as fast as possible to reduce training time.\nIt probably should also be noted that this code is from the forward method of a costum layer, while running the model on a gpu.\nYep but I mean, you are running python. Thus, you use python lists.\nThe way of avoiding lists is trying not to make use of loops and writting it as pytorch operations.\nIf you need to speed up anything and to avoid python lists the way to go is making a c++ implementation.\nhttps:\/\/pytorch.org\/docs\/master\/cpp_index.html\nBut in general, it is \u201cstrange\u201d to use lists in a forward. Because for almost all the official layers you can usually proccess everything at once with bit of rewriting.\nThe only case in which you must need to use for loops is for very customized operators or for dinamically generated networks (when you create N layers based on  a parameter and then you cannot manually call them all)\nAnother tool you is torchscript https:\/\/pytorch.org\/docs\/master\/jit.html\nBut if you want the versatility and readability of lists, then you pay the price.\nBasically what I need to do is I have a tensor of shape (b, c, h, w)\nI have to process each of the (h, w) in the tensor separately with a custom layer that only takes bidimensional input, so I just iterated over the b and c dimensions and appended each result.\nIsn\u2019t there any other way to do this with a pytorch function?\nYou can try to extend the custom layer to process batches (all the layers in pytorch can process batches and most of the operations like matrix multiplication too)\nand then process everything as a (b*c,h,w) tensor without envolving loops\nThat is what I am trying to do, but by just wrapping it around another costum layer that already feeds it what is can process.\nI guess I will have to really change the inner structure of the costum layer, then.\nas general advice I would recommend you to make all your custom layers to be able to process batches because that\u2019s the standard way of working of pytorch.\nThanks for the advice. Properly noted, but unfortunately the costum layer isn\u2019t mine, so I will probably just have to deal with it.\nThank you so much for the help anyway!"},{"x":"I am training the following model to classify is something is in one state or another. What I\u2019m struggling with is when using smaller batch sizes, my models validation score sits at 50%, so no better than guessing. If I increase the batch size to even just 4, I start getting 85+% accuracy. I\u2019m using BCELoss.\n<code class=\"lang-auto\">import torch.nn as nn\n\nclass Classifier(nn.Module):\n    def __init__(self):\n        super(Classifier, self).__init__()\n        \n        self.conv_layers = nn.Sequential(\n            nn.Conv1d(3, 6, 5),\n            nn.ReLU(),\n            nn.Conv1d(6,16,5),\n            nn.ReLU(),\n            nn.Conv1d(16,32,5),\n            nn.ReLU(),\n        )\n        self.linear_layers = nn.Sequential(\n            nn.Linear(32*3,16),\n            nn.ReLU(),\n            nn.Linear(16,2),\n            nn.Softmax(dim=0)\n        )\n        \n        \n    def forward(self, x):\n        x = self.conv_layers(x)\n        x = x.view(-1, 32*3)\n        return self.linear_layers(x)\n<\/code>\nAccuracy is being calculated as follows:\n<code class=\"lang-auto\">running_acc += torch.eq(torch.argmax(lb, axis=1), torch.argmax(out, axis=1)).sum()\n...\nacc = running_acc \/ total_ds_length\n<\/code>\nSince the label is one hot encoded.\nMy model is in eval() mode.","y":"I was actually wrong to use BCELoss.\nCrossEntropyLoss better suited my application and now provides an accuracy increase of 4% as well over both the validation and test sets with all kinds of batch sizes.\nA lesson in understanding the right approach.","z":"I was actually wrong to use BCELoss.\nCrossEntropyLoss better suited my application and now provides an accuracy increase of 4% as well over both the validation and test sets with all kinds of batch sizes.\nA lesson in understanding the right approach."},{"x":"Hi. I guess there are three way to use activation function in a custom module:\n\nUse nn.Softmax() in the initializer in the custom model.\nUse nn.softmax() in the forward function in the custom model.\nUse nn.functional.softmax() in the forward function in the custom model.\n\nWhat are the differences among them and what is the proper way to use an activation function in my custom model?","y":"\n\n\n I_H_Yoo:\n\n\nFor softmax, and if you aren\u2019t building a Sequential , I\u2019d use the functional interface.\n\nWhy would you do that? What\u2019s the beneficial of using functional interface instead of Module?\n\n\nI think about modules as holding state (e.g. weights, as you mention) and using that and the inputs I pass to produce the output. So for softmax, using the functional interface expresses how I think about it (i.e. as a function of the inputs and no weights etc.).\nBut don\u2019t make it a science, just go with what you prefer.","z":"nn.Softmax is an nn.Module, which can be initialized e.g. in the __init__ method of your model and used in the forward.\ntorch.softmax() (I assume nn.softmax is a typo, as this function is undefined) and nn.functional.softmax are equal and I would recommend to stick to nn.functional.softmax, since it\u2019s documented.  gives a better answer here with link \"https:\/\/discuss.pytorch.org\/t\/why-there-isnt-a-method-named-torch-softmax\/90554\/2\". \n nails it with his comment.\nWe also do have a short discussion in the Deep Learning with PyTorch book with link \"https:\/\/pytorch.org\/deep-learning-with-pytorch\" in chapter 8, similar to the brief mention in slide 54 for a course I taught earlier this month with link \"https:\/\/github.com\/t-vi\/acdl2020\/raw\/master\/pytorch_introduction_slides.pdf\" - copied here for your convenience:\nSome thoughts on functional vs. Module\n\nIf you write for re-use, the functional \/ Module split of PyTorch has turned out\nto be a good idea.\nUse functional for stuff without state (unless you have a quick and dirty Sequential).\nNever re-use modules (define one torch.nn.ReLU and use it 5 times). It\u2019s a trap!\nWhen doing analysis or quantization (when ReLU becomes stateful due to quantization params), this will break.\n\nThe latter two seem to cover your case here, but 2. is more a matter of personal preference, while 3. is really about writing better (as in less risky, more clear) code.\nItem 1. ist more is more for when you write a library or something you expect to be re-used a lot.\nBest regards\nThomas\nThank you for your answer but I\u2019m still not sure what are the differences of them.\nSo can I say that functional is for re-use? What do \u2018stuff without state\u2019 and \u2018quantization\u2019 mean? Which should I use between functional and Module?\n\n\n\n I_H_Yoo:\n\nSo can I say that functional is for re-use?\n\n\nThat is more \u201cif you implement something new, provide both because you don\u2019t know which is more convenient to your users\u201d.\n\n\n\n I_H_Yoo:\n\nWhat do \u2018stuff without state\u2019 and \u2018quantization\u2019 mean?\n\n\nThat\u2019s another slide, there.\nOr, try the \u201cwhat is nn\u201d tutorial with link \"https:\/\/pytorch.org\/tutorials\/beginner\/nn_tutorial.html\".\nQuantization has its own set of tutorials with link \"https:\/\/pytorch.org\/tutorials\/advanced\/static_quantization_tutorial.html\".\n\n\nWhich should I use between functional and Module?\n\n\nFor softmax, and if you aren\u2019t building a  Sequential, I\u2019d use the functional interface. But there is a modicum of taste in that.\nBest regards\nThomas\nThank you for your kind answer.I understand the \u2018state\u2019. I guess it refers a weight in a  neural network right? Quantization is something I\u2019ve never seen so I need to see youre reference. However, I\u2019m little bit confuse becuase I think there is no conclusion for some kind of a threshold that using functional or Module. Maybe that\u2019s why I\u2019m not an native english speaker.\nI apologise for the inconvenience but if I\u2019m not asking you this question, I will never know. Please excuse me for bothering you. And I want to quote what you mentioned just like you did to mine, but I don\u2019t know how to do it so I just write them down.\n\nThat is more \u201cif you implement something new, provide both because you don\u2019t know which is more convenient to your users\u201d\n\nI don\u2019t want to make an activation function. I just want to know proper way to use them (softmax, ReLU, whatever it is). However, I guess there is no clear standard for it. The functional are not saved in the stae of a neural network so I should avoid it if I want to save them as state_dict right?\n\nFor softmax, and if you aren\u2019t building a Sequential , I\u2019d use the functional interface.\n\nWhy would you do that? What\u2019s the beneficial of using functional interface instead of Module?\nOnce again, I apologise for the inconvenience.\n\n\n\n I_H_Yoo:\n\n\nFor softmax, and if you aren\u2019t building a Sequential , I\u2019d use the functional interface.\n\nWhy would you do that? What\u2019s the beneficial of using functional interface instead of Module?\n\n\nI think about modules as holding state (e.g. weights, as you mention) and using that and the inputs I pass to produce the output. So for softmax, using the functional interface expresses how I think about it (i.e. as a function of the inputs and no weights etc.).\nBut don\u2019t make it a science, just go with what you prefer."},{"x":"I have a large dataset (I have split it into 2000 files, each with an hour of 256hz time series data, too large for memory), and my model takes 30 seconds of this data as input. I want to sample these 30 second windows randomly from the data. I was worried that using a map style dataset would be too slow considering it would have to load a whole file in order to get a  30 second window from it, which turned out to be true. I made a custom batch sampler and a custom dataset, where getitem calls the pandas function read_csv with skiprows and nrows parameters to try and only load what I need, but it is still incredibly slow due to the read_csv call on every sample to a file with ~900k rows, even with batching and multiple workers.\nFirst, if I just saved a standard dataset with predefined windows rather than taking random intervals from the data and used a pytorch random sampler on it, would this be an efficient solution? I don\u2019t really know how the stock dataset classes work or if they would be efficient with randomly sampling a dataset of this size. (I would be fine with this if the sampling was well-distributed, worked quickly, and didn\u2019t require storing the data many times over)\nI\u2019m open to any suggestions about how to approach this. Thanks.","y":"I would preprocess csv files to numpy files. Maybe with numpy.memmap, if no pre-loading method is used.","z":"I would preprocess csv files to numpy files. Maybe with numpy.memmap, if no pre-loading method is used."},{"x":"Hello,\nI\u2019m training an incremental classification model (ResNet-32) on the CIFAR-100 dataset.\nUsing cross-entropy loss, I can get the final 43.2% accuracy. I fixed random seed for each random module (eg Python built-in random module, np.random, torch, etc.), so I can get the same accuracy each time if I trained the model many times.\nHowever, if I put some other torch.tensor on GPU, it causes the accuracy drop (42.6%). And indeed this torch.tensor is not used in training.\n(torch.tensor example, self.class_sizes = torch.tensor([20, 20, 500, 500,.....]).view(-1, 1).to(self._device), and self.class_sizes doesn\u2019t be used in training process. I just put this variable there.)\nDoes anyone have any ideas or encountered similar situations? Thanks!\nconda environment\n<code class=\"lang-auto\">pytorch=1.3.0=py3.7_cuda10.0.130_cudnn7.6.3_0\ntorchvision=0.2.2\ncudatoolkit=10.0.130=0\n<\/code>\n0809 Update:\nAnother similar situation\nIf I have a Python class (BigModel below), which includes the training\/test procedure.\n<code class=\"lang-auto\">class AuxModel(nn.Module):\n    def __init__(self):\n        super(AuxModel, self).__init__()\n\n        self.align_FC = nn.Linear(64, 1)\n\n        for m in self.modules():\n            if isinstance(m, nn.Linear):\n                nn.init.kaiming_normal_(m.weight, mode='fan_out', nonlinearity='relu')\n                if m.bias is not None:\n                    m.bias.data.zero_()\n\n    def forward(self, x):\n        return self.align_FC(x)\n\nclass BigModel():\n    def __init__(self, args):\n           ....\n           self._network = ResNet32(...)\n\n           self.SP = AuxModel()\n\n    def train(self, ...):\n            ....\n\n    def test(self, ...):\n            ....\n<\/code>\nIf I train the model (ResNet32), and totally not used self.SP, I will get lower accuracy (74.2%).\nBut if I directly comment out the #self.SP, I can get higher accuracy (76.6%). Another thing I can know is that I do not use the self.SP in my code, because I directly commented out that, I did not get any error message.","y":"Since AuxModel initialized modules randomly, the pseudorandom number generator with link \"https:\/\/en.wikipedia.org\/wiki\/Pseudorandom_number_generator\" will be called and will thus change the random values of all following operations. The accuracy decrease seems to be a symptom of the changed seed and you should see the same effect, if you remove self.SP and rerun the code with different seeds.","z":"Since AuxModel initialized modules randomly, the pseudorandom number generator with link \"https:\/\/en.wikipedia.org\/wiki\/Pseudorandom_number_generator\" will be called and will thus change the random values of all following operations. The accuracy decrease seems to be a symptom of the changed seed and you should see the same effect, if you remove self.SP and rerun the code with different seeds.\nMany thanks for your reply!\nI will try to use different seeds and self.SP architectures to do some experiments, and report the results later.\nNot mentioned above, different self.SP architectures cause different final accuracy drops! I think this observation also results from the symptom that you had mentioned, which mainly related to pseudorandom number generator)"},{"x":"I have two models both run using the same parameters (Epoch=50, Batch size=16) If I compare both models one seem to run for 965 steps, and the other runs for 395. I would expect them both to run similar steps. Any idea why would this be the case. I have attached a copy of the graph?Screenshot 2020-08-07 at 11.21.03831\u00d7586 35.3 KB with link \"https:\/\/discuss.pytorch.org\/uploads\/default\/original\/3X\/e\/e\/eee5470e860ad1a008441d947ca93898e099369d.png\" Is the model clever enough to stop as training value isn\u2019t changing any more?","y":"The runtime difference might come from:\n\ndifferent datasets and thus a different number of samples\ndifferent print logic in your code\ndifferent training (including early stopping)\n\u2026\n\nWithout seeing code I can just speculate. ","z":"The runtime difference might come from:\n\ndifferent datasets and thus a different number of samples\ndifferent print logic in your code\ndifferent training (including early stopping)\n\u2026\n\nWithout seeing code I can just speculate. \n Many Thanks once more for your help. The points you mentioned help to pin point the issue. Thanks a lot."},{"x":"I\u2019m running the data loader below which applies a filter to a microscopy image prior to training. In order to count the red and green. This code filters the red cells. Since I have applied this to the code I keep on getting the error message above. I have tried increasing the memory allocation to the maximum allowance possible but that didn\u2019t help. Is there a way I could modify the filter so it isn\u2019t causing this issue, please? Many thanks in advance\n<code class=\"lang-auto\">import os\n\nimport numpy as np\nimport torch\nfrom PIL import Image\nfrom torch.utils.data import Dataset\nfrom torchvision import transforms, utils\n#from torchvision.transforms import Grayscalei\nimport pandas as pd\nimport pdb\nimport cv2\n\nclass CellsDataset(Dataset):\n    # a very simple dataset\n\n    def __init__(self, root_dir, transform=None, return_filenames=False):\n        self.root = root_dir\n        self.transform = transform\n        self.return_filenames = return_filenames\n        self.files = [os.path.join(self.root,filename) for filename in os.listdir(self.root)]\n        self.files = [path for path in self.files\n                      if os.path.isfile(path) and os.path.splitext(path)[1]=='.png']\n\n    def __len__(self):\n        return len(self.files)\n\n    def __getitem__(self, idx):\n        path = self.files[idx]        \n        image = cv2.imread(path)\n        sample = image.copy()\n        # set blue and green channels to 0\n        sample[:, :, 0] = 0\n        sample[:, :, 1] = 0\n\nchannel.\n        if self.transform:\n            sample = self.transform(sample)\n\n        if self.return_filenames:\n            return sample, path\n        else:<\/code>","y":"Soo the point is that original vgg was using that size. Network is \u201cused to see\u201d objects whose sizes are contained in a 112x112 image.\nThere is something called receptive field (rather than boring you with a shitty explanation I will link to a blog https:\/\/towardsdatascience.com\/understand-local-receptive-fields-in-convolutional-neural-networks-f26d700be16c) soo if the input image is too large and the network is not deep enough, this may harm the results.\nSoo imagine that you are a neuron and you have to describe what you see. If you are too closed to an object you will be able to provide fine details but without a \u201cpanoramic\u201d context. However if you are too far, you will provide a good overall description but coarse details.\nIn that way it is said that first layers learn basic features (gradients) meanwhile deeper layers learn more abstract features.\nSoo to address this problem several techniques were developed like pooling or dilated convolution","z":"If you set num_workes=0 (this is turning off multiprocessing) do you get any error?\n I haven\u2019t tried doing this. I will give this a try and let you know. Thanks for your reply.\n I have still got the same error message after changing the num_workes=0. Anything else to try?\nHi,\nIn theory if you set num_workers=0, dataloader should run on the main thread. Therefore it\u2019s a bit strange  to get a worker error. Typical errors are lack of memory (which raises a MemoryError). Is there no extra info? (line in which the error is produced etcetera\u2026)  are you running the code in a server which may limit your resources? A killed signal seems like if the process were closed by the user\/superuser out of the python pipe itself\nYes. The code is running a server. This is the error message i am getting now:\n<code class=\"lang-auto\">slurmstepd: error: Detected 1 oom-kill event(s) in step 9708662.batch cgroup. Some of your processes may have been killed by the cgroup out-of-memory handler.\n<\/code>\nIs there an alternative way I could change this code so it doesn\u2019t copy the image each time. As this is likely causing the issue of running out of memory\n<code class=\"lang-auto\">    def __getitem__(self, idx):\n        path = self.files[idx]        \n        image = cv2.imread(path)\n        sample = image.copy()\n        # set blue and green channels to 0\n        sample[:, :, 0] = 0\n        sample[:, :, 1] = 0\n<\/code>\n Just curious to know why would you need to copy the image? Can the filter put directly on image?\nVery good question. The reason i am copying the image because the example I am working from is copying the image and i thought i need to copy it before i filter it. If the code works without copying the image i think this would help. I will give this ago. Many Thanks for pointing this out.\nSoo slurm is the queue manager. I think the error happens because you are using more memory than the memory you requested. Try to request more memory (indeed not using the .copy() will be useful) or to reduce the batch size.\nThanks for your reply! I have tried a smaller batch size and removed the copy() from the code but the i keep getting the same error message. Is there anything else you might suggest. I am running this code using a different CNN model it works okay but when i use the VGG model i get this error message.\nSoo it seems you are running out of memory in the GPU (are u using gpus?) can you try to use a gpu with more memory? or try to reduce even more the batch size.\nI\u2019m not using GPUs. I think i need to re-size my image when using the VGG model as below. If i do so, i get the following error message. Are you able to advise what i need to change please?\n<code class=\"lang-auto\">RuntimeError: Calculated padded input size per channel: (1 x 1). Kernel size: (3 x 3). Kernel size can't be greater than actual input size\n<\/code>\nThe code for my convnet file:\n<code class=\"lang-auto\">import torch\nimport torch.nn as nn\n\nclass Convnet(nn.Module):\n    \"\"\"\n    A custom convnet for convolving 1080x1080 fluorescence micrograph images.\n    \"\"\"\n\n    def __init__(self):\n        super(Convnet,self).__init__()\n        self.main = nn.Sequential(\n\n            nn.Conv2d(3,16,10,stride=2,padding=0),\n            nn.LeakyReLU(0.2,inplace=False),\n            nn.Conv2d(16,32,7,stride=2,padding=0),\n            nn.LeakyReLU(0.2,inplace=False),\n            nn.Conv2d(32,64,3,stride=3,padding=0),\n            nn.LeakyReLU(0.2,inplace=False),\n            nn.Conv2d(64,96,3,stride=2,padding=0),\n            nn.LeakyReLU(0.2,inplace=False),\n            nn.Conv2d(96,128,3,stride=2,padding=0),\n            nn.LeakyReLU(0.2,inplace=False),\n            nn.Conv2d(128,192,3,stride=2,padding=0),\n            nn.LeakyReLU(0.2,inplace=False),\n            nn.Conv2d(192,256,3,stride=2,padding=0),\n            nn.LeakyReLU(0.2,inplace=False),\n            nn.Conv2d(256,256,3,stride=2,padding=0),\n            nn.LeakyReLU(0.2,inplace=False),\n            nn.Conv2d(256,1,1,stride=1,padding=0)\n        )\n\n    def forward(self, x):\n        # x: B x 2 x 1024 x 1024\n        return self.main(x) # B x 1 x 9 x 9\n<\/code>\nThanks a lot\nIt seems that you have reduced the size too much.\nIn short, from one of the convolutions onwards your feature map is already smaller than 3x3 (the kernel size). In fact 1x1 (so it\u2019s not even 2d but a vector)\nUse a bigger input image or delete some convolutions\nBy default VGG images are 112x112 if i\u2019m not wrong (so that is the optimal size for the pretrained wiegths)\nMany Thanks for your help. I have increased the image size to 1024 x 1024 has resolved the issue. If the optimal size is 112x112, and i am using 1024x1024. How would you anticipate this to affect my training model? Thanks\nSoo the point is that original vgg was using that size. Network is \u201cused to see\u201d objects whose sizes are contained in a 112x112 image.\nThere is something called receptive field (rather than boring you with a shitty explanation I will link to a blog https:\/\/towardsdatascience.com\/understand-local-receptive-fields-in-convolutional-neural-networks-f26d700be16c) soo if the input image is too large and the network is not deep enough, this may harm the results.\nSoo imagine that you are a neuron and you have to describe what you see. If you are too closed to an object you will be able to provide fine details but without a \u201cpanoramic\u201d context. However if you are too far, you will provide a good overall description but coarse details.\nIn that way it is said that first layers learn basic features (gradients) meanwhile deeper layers learn more abstract features.\nSoo to address this problem several techniques were developed like pooling or dilated convolution\n I hope you could help me again, please? I\u2019m still having an issue with running the model - It keeps running out of memory.  Even after reducing the size of the image. Each step during the training takes a long time to complete. I think the issue is with the snippet of code below, where I  read the image then split by color. If i run the model without the the image slpit it works okay, but i need to spit the image by channel to do the counting. I have tried changing the batch size to 8 from 16. The only different in this model is also the change to grayscale which i don\u2019t seem to be able to becuase the image is in PIL format. Can you suggest anything please?\n<code class=\"lang-auto\"> def __getitem__(self, idx):\n        path = self.files[idx]\n\n        img = imread(filename=path)\n        sample = resize(src=img, dsize=(1024, 1024))\n        #sample = functional.to_grayscale(sample, num_output_channels=3)\n        #image = cv2.imread(path)\n\n        sample[:, :, 0] = 0\n        sample[:, :, 1] = 0\n<\/code>\nHi,\nI don\u2019t know at all how does it internally replace the values.\nYou can try\n<code class=\"lang-auto\">        sample[:, :, 0].zero_()\n        sample[:, :, 1].zero_()\n<\/code>\n(Assuming that sample is a torch tensor)\nThat\u2019s in-place replacement which should consume no extra memory.\nIt\u2019s normal a single iteration takes a long time since you are running it on CPU.\nYou can try to use google colab\nsample is no a numpy array as i had to change it to split the image. Is there a quick method to change it back please?\n<code class=\"lang-auto\">    sample[:, :, 0].zero_()\nAttributeError: 'numpy.ndarray' object has no attribute 'zero_'\n\n<\/code>\nThanks\nI really have no idea sorry.\nI don\u2019t really know the details about numpy\u2019s operators.\nMany Thanks for all of your help. I have changed the image size to 256 x 256, and batch size to 16 which helped in getting to resolve the memory issue. Thanks"},{"x":"Hi,\nI tried to build from source of pytorch on gtx-1070ti.\npython setup.py build develop\nbut found no inference speed up, the results seem to be opposite to this post (https:\/\/medium.com\/repro-repo\/build-pytorch-from-source-on-ubuntu-18-04-1c5556ca8fbf)\nany idea?","y":"In that case you want to make sure you have the latest cuda\/cudnn installed.\nFor distributed, the gloo backend should be already properly provided.\nFor mixed precision I think basic cuda libs handle that well.\nFor linear algebra. You want a good blas\/lapack lib on cpu like mkl or openblas. And for gpu you will need magma.\nYou can find some instructions here: https:\/\/github.com\/pytorch\/pytorch\/blob\/master\/CONTRIBUTING.md","z":"Hi,\nThe binary comes with a set of bundled libraries (mkl, magma, etc) that are very important for speed.\nIf you compile from source, you will want to make sure you have the relevant ones for your workload installed locally so that they can be used during compilation.\nAlso in some cases, build of openBLAS, magma, etc tailored to your machine will be slightly faster.\nBut if you don\u2019t have any blas library (or use the system default one) you might see lower performance compared to the binary code that has an off the shelf optimized blas for example.\npytorch has many third party libraries,  but i  am not sure which one is the key library to speed up the performance.\nDo you have any suggestion that how to enable these libraries when compiling?\nHere are my cmake output, which library I have to enable in order to speed up the inference?\n\u2013 ******** Summary ********\n\u2013 General:\n\u2013   CMake version         : 3.15.4\n\u2013   CMake command         : \/home\/acer\/cmake-3.15.4-Linux-x86_64\/bin\/cmake\n\u2013   System                : Linux\n\u2013   C++ compiler          : \/usr\/bin\/c++\n\u2013   C++ compiler id       : GNU\n\u2013   C++ compiler version  : 7.5.0\n\u2013   BLAS                  : MKL\n\u2013   CXX flags             :  -fvisibility-inlines-hidden -DUSE_PTHREADPOOL -fopenmp -DNDEBUG -DUSE_FBGEMM -DUSE_QNNPACK -DUSE_PYTORCH_QNNPACK -DUSE_XNNPACK -DUSE_VULKAN_WRAPPER -O2 -fPIC -Wno-narrowing -Wall -Wextra -Werror=return-type -Wno-missing-field-initializers -Wno-type-limits -Wno-array-bounds -Wno-unknown-pragmas -Wno-sign-compare -Wno-unused-parameter -Wno-unused-variable -Wno-unused-function -Wno-unused-result -Wno-unused-local-typedefs -Wno-strict-overflow -Wno-strict-aliasing -Wno-error=deprecated-declarations -Wno-stringop-overflow -Wno-psabi -Wno-error=pedantic -Wno-error=redundant-decls -Wno-error=old-style-cast -fdiagnostics-color=always -faligned-new -Wno-unused-but-set-variable -Wno-maybe-uninitialized -fno-math-errno -fno-trapping-math -Werror=format -Wno-stringop-overflow\n\u2013   Build type            : Release\n\u2013   Compile definitions   : ONNX_ML=1;ONNXIFI_ENABLE_EXT=1;ONNX_NAMESPACE=onnx_torch;HAVE_MMAP=1;_FILE_OFFSET_BITS=64;HAVE_SHM_OPEN=1;HAVE_SHM_UNLINK=1;HAVE_MALLOC_USABLE_SIZE=1;USE_EXTERNAL_MZCRC;MINIZ_DISABLE_ZIP_READER_CRC32_CHECKS\n\u2013   CMAKE_PREFIX_PATH     : \/home\/acer\/.pyenv\/versions\/pytorch_build\/lib\/python3.7\/site-packages;\/usr\/local\/cuda\n\u2013   CMAKE_INSTALL_PREFIX  : \/home\/acer\/nfs-share\/pytorch\/torch\n\u2013   TORCH_VERSION         : 1.7.0\n\u2013   CAFFE2_VERSION        : 1.7.0\n\u2013   BUILD_CAFFE2_MOBILE   : OFF\n\u2013   USE_STATIC_DISPATCH   : OFF\n\u2013   BUILD_BINARY          : OFF\n\u2013   BUILD_CUSTOM_PROTOBUF : ON\n\u2013     Link local protobuf : ON\n\u2013   BUILD_DOCS            : OFF\n\u2013   BUILD_PYTHON          : True\n\u2013     Python version      : 3.7.4\n\u2013     Python executable   : \/home\/acer\/.pyenv\/versions\/pytorch_build\/bin\/python\n\u2013     Pythonlibs version  : 3.7.4\n\u2013     Python library      : \/home\/acer\/.pyenv\/versions\/3.7.4\/lib\/libpython3.7m.so.1.0\n\u2013     Python includes     : \/home\/acer\/.pyenv\/versions\/3.7.4\/include\/python3.7m\n\u2013     Python site-packages: lib\/python3.7\/site-packages\n\u2013   BUILD_CAFFE2_OPS      : ON\n\u2013   BUILD_SHARED_LIBS     : ON\n\u2013   BUILD_TEST            : True\n\u2013   BUILD_JNI             : OFF\n\u2013   INTERN_BUILD_MOBILE   :\n\u2013   CLANG_CODE_COVERAGE   : OFF\n\u2013   USE_ASAN              : OFF\n\u2013   USE_CUDA              : ON\n\u2013     CUDA static link    : OFF\n\u2013     USE_CUDNN           : ON\n\u2013     CUDA version        : 10.2\n\u2013     cuDNN version       : 7.6.5\n\u2013     CUDA root directory : \/usr\/local\/cuda\n\u2013     CUDA library        : \/usr\/local\/cuda\/lib64\/stubs\/libcuda.so\n\u2013     cudart library      : \/usr\/local\/cuda\/lib64\/libcudart.so\n\u2013     cublas library      : \/usr\/lib\/x86_64-linux-gnu\/libcublas.so\n\u2013     cufft library       : \/usr\/local\/cuda\/lib64\/libcufft.so\n\u2013     curand library      : \/usr\/local\/cuda\/lib64\/libcurand.so\n\u2013     cuDNN library       : \/usr\/lib\/x86_64-linux-gnu\/libcudnn.so\n\u2013     nvrtc               : \/usr\/local\/cuda\/lib64\/libnvrtc.so\n\u2013     CUDA include path   : \/usr\/local\/cuda\/include\n\u2013     NVCC executable     : \/usr\/local\/cuda\/bin\/nvcc\n\u2013     NVCC flags          : -DONNX_NAMESPACE=onnx_torch;-gencode;arch=compute_61,code=sm_61;-Xcudafe;\u2013diag_suppress=cc_clobber_ignored;-Xcudafe;\u2013diag_suppress=integer_sign_change;-Xcudafe;\u2013diag_suppress=useless_using_declaration;-Xcudafe;\u2013diag_suppress=set_but_not_used;-Xcudafe;\u2013diag_suppress=field_without_dll_interface;-Xcudafe;\u2013diag_suppress=base_class_has_different_dll_interface;-Xcudafe;\u2013diag_suppress=dll_interface_conflict_none_assumed;-Xcudafe;\u2013diag_suppress=dll_interface_conflict_dllexport_assumed;-Xcudafe;\u2013diag_suppress=implicit_return_from_non_void_function;-Xcudafe;\u2013diag_suppress=unsigned_compare_with_zero;-Xcudafe;\u2013diag_suppress=declared_but_not_referenced;-Xcudafe;\u2013diag_suppress=bad_friend_decl;-std=c++14;-Xcompiler;-fPIC;\u2013expt-relaxed-constexpr;\u2013expt-extended-lambda;-Wno-deprecated-gpu-targets;\u2013expt-extended-lambda;-gencode;arch=compute_61,code=sm_61;-Xcompiler;-fPIC;-DCUDA_HAS_FP16=1;-D__CUDA_NO_HALF_OPERATORS__;-D__CUDA_NO_HALF_CONVERSIONS__;-D__CUDA_NO_HALF2_OPERATORS__\n\u2013     CUDA host compiler  : \/usr\/bin\/cc\n\u2013     NVCC --device-c     : OFF\n\u2013     USE_TENSORRT        : OFF\n\u2013   USE_ROCM              : OFF\n\u2013   USE_EIGEN_FOR_BLAS    : ON\n\u2013   USE_FBGEMM            : ON\n\u2013     USE_FAKELOWP          : OFF\n\u2013   USE_FFMPEG            : OFF\n\u2013   USE_GFLAGS            : OFF\n\u2013   USE_GLOG              : OFF\n\u2013   USE_LEVELDB           : OFF\n\u2013   USE_LITE_PROTO        : OFF\n\u2013   USE_LMDB              : OFF\n\u2013   USE_METAL             : OFF\n\u2013   USE_MKL               : OFF\n\u2013   USE_MKLDNN            : ON\n\u2013   USE_MKLDNN_CBLAS      : OFF\n\u2013   USE_NCCL              : ON\n\u2013     USE_SYSTEM_NCCL     : OFF\n\u2013   USE_NNPACK            : ON\n\u2013   USE_NUMPY             : ON\n\u2013   USE_OBSERVERS         : ON\n\u2013   USE_OPENCL            : OFF\n\u2013   USE_OPENCV            : OFF\n\u2013   USE_OPENMP            : ON\n\u2013   USE_TBB               : OFF\n\u2013   USE_VULKAN            : OFF\n\u2013   USE_PROF              : OFF\n\u2013   USE_QNNPACK           : ON\n\u2013   USE_PYTORCH_QNNPACK   : ON\n\u2013   USE_REDIS             : OFF\n\u2013   USE_ROCKSDB           : OFF\n\u2013   USE_ZMQ               : OFF\n\u2013   USE_DISTRIBUTED       : ON\n\u2013     USE_MPI             : ON\n\u2013     USE_GLOO            : ON\n\u2013     USE_TENSORPIPE      : ON\n\n\n\n be7f984b2f2e66ba7969:\n\ni am not sure which one is the key library to speed up the performance.\n\n\nIt all depend what is your workload \nDo you use cpu or gpu? Do you use distributed? Do you use mixed precision? Do you use linear algebra functions?\nI use nvidia gpu and distributed, also mixed precision. and of course linear algebra functions.\nIn that case you want to make sure you have the latest cuda\/cudnn installed.\nFor distributed, the gloo backend should be already properly provided.\nFor mixed precision I think basic cuda libs handle that well.\nFor linear algebra. You want a good blas\/lapack lib on cpu like mkl or openblas. And for gpu you will need magma.\nYou can find some instructions here: https:\/\/github.com\/pytorch\/pytorch\/blob\/master\/CONTRIBUTING.md"},{"x":"I\u2019m still fairly new to PyTorch, and grappling with this problem for my own understanding:   I am trying to insert a new untrained classifier block at the end of densenet 121, replacing the existing one and defining it using nn.Module instead of nn.Sequential.\nIt works fine if I do something along these lines:\nclassifier = nn.Sequential(\n                      nn.Linear(1024, 500),\n                      nn.ReLU(),\n                      nn.Linear(500, 1),\n                      nn.Sigmoid())\n\nI\u2019d like to try and define a nn.Module from scratch instead.  In this case I was trying something like:\nclass New_Classifier(nn.Module):\n    def __init__(self):\n        super(New_Classifier, self).__init__()\n        self.fc1 = nn.Linear(feature_length,hidden_1)\n        self.fc2 = nn.Linear(hidden_1, hidden_2)\n        self.fc3 = nn.Linear(hidden_2, 1)   # should output the final score\n        # dropout layer\n        self.dropout = nn.Dropout(0.2)\n\ndef forward(self, x):\n    x = F.relu(self.fc1(x)) # fully connected layer 1\n    x = self.dropout(x)\n    x = F.relu(self.fc2(x)) # fully connected layer 2\n    x = torch.sigmoid(self.fc3(x))   # Return a value between 0 and 1\n    return x\n\nmodel.classifier = New_Classifier\nI realise this isn\u2019t very elegant or necessary, I just wanted to understand how one would go about this if the new classifier wasn\u2019t nicely definable with nn.Sequential or similar.  I get the following error:\nTypeError: cannot assign \u2018main.New_Classifier\u2019 as child module \u2018classifier\u2019 (torch.nn.Module or None expected)\nAny suggestions what caused this error?  I suspect the \u2018child module\u2019 is a clue.   Or if I\u2019m trying to do something that just can\u2019t be done?","y":"Replace the line model.classifier = New_Classifier with model.classifier = New_Classifier() (just append () at the end) so you actually create an instance of the module.","z":"Replace the line model.classifier = New_Classifier with model.classifier = New_Classifier() (just append () at the end) so you actually create an instance of the module.\nThanks very much Mariosasko, that was awesome.  I threw in the () and it worked flawlessly!  Now I\u2019m a little embarrassed I missed that!"},{"x":"Hi all! Recently, I started implementing checkpoint saves on my code, as it has become difficult running everything in one go. I save everything as prescribed on the PyTorch tutorials, however when I resume training, the network behaves as if the checkpoint hasn\u2019t been loaded properly. You can see this behaviour in the figure below. I\u2019ll also include relevant extracts from my code, in case anybody can spot if I am doing something silly. Thank you!\nScreenshot 2020-07-29 at 11.44.311617\u00d7382 24.7 KB with link \"https:\/\/discuss.pytorch.org\/uploads\/default\/original\/3X\/2\/7\/2789d2fbcb123257951071779b573b987090151b.png\"\n<code class=\"lang-auto\">\n\nclass Solver():    \n\n    def __init__(self ... ):\n\n        self.model = model\n        self.optimizer = optimizer(model.parameters(), **optimizer_arguments)\n        ....\n        self.model_name = model_name\n        ....\n        self.learning_rate_scheduler = lr_scheduler.StepLR(self.optimizer,\n                                                           step_size=learning_rate_scheduler_step_size,\n                                                           gamma=learning_rate_scheduler_gamma)\n        self.start_epoch = 1\n        self.start_iteration = 1\n        ....\n\n        if use_last_checkpoint:\n            self.load_checkpoint()\n\n    def train(self, train_loader, validation_loader):\n\n        model, optimizer, learning_rate_scheduler = self.model, self.optimizer, self.learning_rate_scheduler\n\n        if torch.cuda.is_available():\n            torch.cuda.empty_cache()  # clear memory\n            model.cuda(self.device)  # Moving the model to GPU\n\n        iteration = self.start_iteration\n\n        for epoch in range(self.start_epoch, self.number_epochs+1):\n            ...... \n                if phase == 'train':\n                    model.train()\n                else:\n                    model.eval()\n               .....\n                with torch.no_grad():\n                    ....\n                    if phase == 'validation':\n                        early_stop, save_checkpoint = self.EarlyStopping(\n                            np.mean(losses))\n                        self.early_stop = early_stop\n                        if save_checkpoint == True:\n                            validation_loss = np.mean(losses)\n                            checkpoint_name = os.path.join(\n                                self.experiment_directory_path, self.checkpoint_directory, 'checkpoint_epoch_' + str(epoch) + '.' + checkpoint_extension)\n                            self.save_checkpoint(state={'epoch': epoch + 1,\n                                                        'start_iteration': iteration + 1,\n                                                        'arch': self.model_name,\n                                                        'state_dict': model.state_dict(),\n                                                        'optimizer': optimizer.state_dict(),\n                                                        'scheduler': learning_rate_scheduler.state_dict()\n                                                        },\n                                                 filename=checkpoint_name\n                                                 )\n                ......\n                if phase == 'train':\n                    learning_rate_scheduler.step()\n    ......\n\n    def save_checkpoint(self, state, filename):\n\n        torch.save(state, filename)\n\n    def load_checkpoint(self, epoch=None):\n\n        ....\n            checkpoint_file_path = os.path.join(.....)\n            self._checkpoint_reader(checkpoint_file_path)\n\n    def _checkpoint_reader(self, checkpoint_file_path):\n       ....\n        checkpoint = torch.load(checkpoint_file_path)\n        self.start_epoch = checkpoint['epoch']\n        self.start_iteration = checkpoint['start_iteration']\n        self.model.load_state_dict = checkpoint['state_dict']\n        self.optimizer.load_state_dict = checkpoint['optimizer']\n\n        for state in self.optimizer.state.values():\n            for key, value in state.items():\n                if torch.is_tensor(value):\n                    state[key] = value.to(self.device)\n\n        self.learning_rate_scheduler.load_state_dict = checkpoint['scheduler']\n\n<\/code>","y":"load_state_dict is a method, which should get the state_dict as its input.\nCurrently you are reassigning the state_dict to the function:\n<code class=\"lang-python\">self.model.load_state_dict = checkpoint['state_dict']\nself.optimizer.load_state_dict = checkpoint['optimizer']\n<\/code>\nReplace these lines of code with:\n<code class=\"lang-python\">self.model.load_state_dict(checkpoint['state_dict'])\nself.optimizer.load_state_dict(checkpoint['optimizer'])\n<\/code>\nand rerun the code.","z":"A close-up view from the training performance, without the outliers. As you can see, even after the first peak, the values don\u2019t get close to the point where they were at the stop of training, as if something hasn\u2019t loaded properly. However, when printing values and state_dicts while going through the code step-by-step, all seems to be there.\nScreenshot 2020-07-29 at 12.18.581229\u00d7402 59.1 KB with link \"https:\/\/discuss.pytorch.org\/uploads\/default\/original\/3X\/6\/4\/64943436c737f034ff8a9efebafa353cf5078335.png\"\nload_state_dict is a method, which should get the state_dict as its input.\nCurrently you are reassigning the state_dict to the function:\n<code class=\"lang-python\">self.model.load_state_dict = checkpoint['state_dict']\nself.optimizer.load_state_dict = checkpoint['optimizer']\n<\/code>\nReplace these lines of code with:\n<code class=\"lang-python\">self.model.load_state_dict(checkpoint['state_dict'])\nself.optimizer.load_state_dict(checkpoint['optimizer'])\n<\/code>\nand rerun the code.\nThank you ! I\u2019m rerunning now and will let you know of the outcome!"},{"x":"I was recently trying to train a resnet on ImageNet with consistent images inputs across runs, yet still with data augmentation, such as cropping, flipping rotating, etc.\nI run into a problem with the fact, that there is no way of consistently getting the same random crops.\nHere is a minimal example I created:\n<code class=\"lang-auto\">import torch\nfrom torchvision import transforms\n\n\ntorch.random.manual_seed(1)\n\nx = torch.rand((3, 10, 10))\n\ntf_crop = transforms.Compose([\n    transforms.ToPILImage(),\n    transforms.RandomResizedCrop(4),\n    transforms.ToTensor(),\n])\ntf_flip = transforms.Compose([\n    transforms.ToPILImage(),\n    transforms.RandomHorizontalFlip(),\n    transforms.ToTensor(),\n])\ntf_rot = transforms.Compose([\n    transforms.ToPILImage(),\n    transforms.RandomRotation(45),\n    transforms.ToTensor(),\n])\n\n# Consistent x among the calls\nprint(x[:2, :2, :2])\n\n# RandomRotation, RandomResizedCrop and Random HorizontalFlip changes stuff\n# even if seed stays the same\nfor idx in range(2):\n    torch.random.manual_seed(1)\n    print(f'Crop {idx + 1}')\n    print(tf_crop(x)[:2, :2, :2].numpy())\nfor idx in range(2):\n    torch.random.manual_seed(1)\n    print(f'Flip {idx + 1}')\n    print(tf_flip(x)[:2, :2, :2].numpy())\nfor idx in range(2):\n    torch.random.manual_seed(1)\n    print(f'Rotation {idx + 1}')\n    print(tf_rot(x)[:2, :2, :2].numpy())\n<\/code>\nEach iteration of each of the for loops produces different results. However, at least in my understanding, across iterations, the results should be the same since the seed is set to the same number.\nI would appreciate it if someone could point in direction on how to achieve consistency across different iterations of for loops (NOT across different for loops).","y":"Some torchvision transformation use the Python random library to sample the numbers.\nAdd random.seed(1) to the loop and you should get the same results.","z":"Some torchvision transformation use the Python random library to sample the numbers.\nAdd random.seed(1) to the loop and you should get the same results.\nThank you, that worked flawlessly!\nIn PyTorch 1.6, you need to use torch.manual_seed(5) and random.seed(5) same time. Please see the issue in: https:\/\/github.com\/pytorch\/pytorch\/issues\/42331"},{"x":"I am trying to do multiclass-classification using simple ANN .\nDont know why my loss is not decreasing it remains constant\nCODE-\n<code class=\"lang-auto\">class NET(nn.Module):\ndef __init__(self):\n    super().__init__()\n    self.model=nn.Sequential(\n        nn.Linear(6,512),\n        nn.ReLU(),\n        nn.Linear(512,1024),\n        nn.ReLU(),\n        nn.Linear(1024,17),\n        nn.Softmax()\n    )\ndef forward(self ,x):\n    return self.model(x)\n\nnet=NET().to(device)\n\nopt=optim.Adam(net.parameters() , lr=0.01)\nLoss_fn=nn.L1Loss()\n\n\n%%time\nepochs=10\nloss_arr=[]\n\nfor epoch in range(epochs):\n     opt.zero_grad()\n\n     outputs=net(train_data)\n     loss=Loss_fn(outputs ,train_out )\n     loss.backward()\n\n    opt.step()\n    loss_arr.append(loss.item())\n    print(\"Epochs : %d\/%d ,loss :%f\" % (epoch ,epochs ,loss))\n<\/code>\nI want Mean absolute error that\u2019s why i am using nn.L1Loss()\nOUTPUT -\n<code class=\"lang-auto\">Epochs : 0\/10 ,loss :10.401250\nEpochs : 1\/10 ,loss :10.401250\nEpochs : 2\/10 ,loss :10.401250\nEpochs : 3\/10 ,loss :10.401250\nEpochs : 4\/10 ,loss :10.401250\nEpochs : 5\/10 ,loss :10.401250\nEpochs : 6\/10 ,loss :10.401250\nEpochs : 7\/10 ,loss :10.401250\nEpochs : 8\/10 ,loss :10.401250\nEpochs : 9\/10 ,loss :10.401250\nWall time: 1.62 s\n<\/code>","y":"Again, I would recommend to play around with hyperparameters, such as the learning rate.\nSince the loss changes and no dropout (or other \u201crandom\u201d operations are used), it would mean that the parameters get some updates but the overall training isn\u2019t able to reduce the loss.\nStart by using a lower learning rate (e.g. 1e-3) and try to overfit the current data sample.","z":"The loss changes for random input data using your code snippet:\n<code class=\"lang-python\">train_data = torch.randn(64, 6)\ntrain_out = torch.empty(64, 17).uniform_(0, 1)\n<\/code>\nso I would recommend to play around with some hyperparameters, such as the learning rate.\nThat being said, I\u2019m not familiar with your use case, but a softmax output in L1Loss doesn\u2019t seem to be the usual use case.\nIf you are dealing with a multi class classification use case, I would recommend to try out nn.CrossEntropyLoss and pass the raw logits to it.\nThanks !! removing softmax layer worked\nI want to Trouble you again .\nSorry for this ,Plz help me\nI am facing same issue with another model.I am building simple digit classifier using ANN\ncode=\n<code class=\"lang-auto\">class NET(nn.Module):\n    def __init__(self):\n        super().__init__()\n        self.model=nn.Sequential(\n            nn.Linear(784, 128),\n            nn.ReLU(),\n            nn.Linear(128, 256),\n            nn.ReLU(),\n            nn.Linear(256, 512),\n            nn.ReLU(),\n            nn.Linear(512, 10),\n        \n        )\n    def forward(self,x):\n            return self.model(x)\n\n\nloss_fn=nn.CrossEntropyLoss()\nopt=optim.Adam(net.parameters() , lr=0.01)\n\nloss_epoch=[]\nepochs=5\nfor i in range(epochs):\n        \n        opt.zero_grad()\n        output=net(x_train)\n        loss=loss_fn(output, y_train)\n        loss.backward()\n        opt.step()\n        loss_epoch.append(loss.item())\n        \n        print(\"Epochs: {}\/{} , Loss:{}\".format(i,epochs,loss))\n<\/code>\nOUTPUT=\n<code class=\"lang-auto\"> Epochs: 0\/5, Loss:2.301159143447876\n\n Epochs: 1\/5, Loss:2.301161289215088\n\n Epochs: 2\/5, Loss:2.3011562824249268\n\n Epochs: 3\/5, Loss:2.3011701107025146\n\n Epochs: 4\/5, Loss:2.3011698722839355\n<\/code>\ndata- Mnist Digit data\nAgain, I would recommend to play around with hyperparameters, such as the learning rate.\nSince the loss changes and no dropout (or other \u201crandom\u201d operations are used), it would mean that the parameters get some updates but the overall training isn\u2019t able to reduce the loss.\nStart by using a lower learning rate (e.g. 1e-3) and try to overfit the current data sample.\nThank you very much .It was due to high LR.\nLoss starts decreasing when LR is 1e-7"},{"x":"cud-error741\u00d7199 23 KB with link \"https:\/\/discuss.pytorch.org\/uploads\/default\/original\/3X\/9\/8\/987f2ecba96217e7457f33bcf86c5621704efc57.png\"\nI got this error while running a simple line of code. Most of the time when I get this error I just switch off my gpu and run it on cpu , and always I get what is wrong with my code. I did the same with this but It ran fine with cpu.","y":"You could try to set this environment argument via os.environ['CUDA_LAUNCH_BLOCKING'] = \"1\".\nThis would have to be added before any other imports in your script to work properly.","z":"Could you rerun the code with CUDA_LAUNCH_BLOCKING=1 python script.py args and post the complete stack trace here, please?\nActually I was running this on google colab. So How can I run this command there?\nAnyway I switched off my gpu and restarted it after 1 day and was not getting any error\u2026but then too I want to know the reason\nYou could try to set this environment argument via os.environ['CUDA_LAUNCH_BLOCKING'] = \"1\".\nThis would have to be added before any other imports in your script to work properly."},{"x":"Hi,\nI want to implement a dice loss for multi-class segmentation, my solution requires to encode the target tensor with one-hot encoding because I am working on a multi label problem. If you have a better solution than this, please feel free to share it.\nThis loss function needs to be differentiable in order to do backprop. I am not sure how to encode the target while keeping autograd working. I am currently having this error :\nRuntimeError: one of the variables needed for gradient computation has been modified by an inplace operation\ncode based on  work. https:\/\/github.com\/pytorch\/pytorch\/issues\/1249\n<code class=\"lang-auto\">def dice_loss(output, target):\n    \"\"\"\n    input is a torch variable of size BatchxnclassesxHxW representing log probabilities for each class\n    target is a 1-hot representation of the groundtruth, shoud have same size as the input\n    \"\"\"\n    encoded_target = Variable(output.data.clone())\n    encoded_target[...] = 0\n    encoded_target.scatter_(1,\n                            target.view(target.size(0), 1,\n                                        target.size(1), target.size(2)),\n                            1)\n\n    assert output.size() == encoded_target.size(), \"Input sizes must be equal.\"\n    assert output.dim() == 4, \"Input must be a 4D Tensor.\"\n\n    num = output * encoded_target  # b,c,h,w--p*g\n    num = torch.sum(num, dim=3)  # b,c,h\n    num = torch.sum(num, dim=2)\n\n    den1 = output * output  # p^2\n    den1 = torch.sum(den1, dim=3)  # b,c,h\n    den1 = torch.sum(den1, dim=2)\n\n    den2 = encoded_target * encoded_target  # g^2\n    den2 = torch.sum(den2, dim=3)  # b,c,h\n    den2 = torch.sum(den2, dim=2)  # b,c\n\n    dice = (2 * num \/ (den1 + den2))\n\n    dice_total = -1 * torch.sum(dice) \/ dice.size(0)\n    return dice_total\n<\/code>\nIf you think of a solution that does not requires one-hot encoding to evaluate the dice similarity of a multi-class problem, I am also interested !\nThanks","y":"Finally got something to work :\n<code class=\"lang-auto\">def dice_loss(output, target, weights=None, ignore_index=None):\n    \"\"\"\n    output : NxCxHxW Variable\n    target :  NxHxW LongTensor\n    weights : C FloatTensor\n    ignore_index : int index to ignore from loss\n    \"\"\"\n    eps = 0.0001\n\n    output = output.exp()\n    encoded_target = output.detach() * 0\n    if ignore_index is not None:\n        mask = target == ignore_index\n        target = target.clone()\n        target[mask] = 0\n        encoded_target.scatter_(1, target.unsqueeze(1), 1)\n        mask = mask.unsqueeze(1).expand_as(encoded_target)\n        encoded_target[mask] = 0\n    else:\n        encoded_target.scatter_(1, target.unsqueeze(1), 1)\n\n    if weights is None:\n        weights = 1\n\n    intersection = output * encoded_target\n    numerator = 2 * intersection.sum(0).sum(1).sum(1)\n    denominator = output + encoded_target\n\n    if ignore_index is not None:\n        denominator[mask] = 0\n    denominator = denominator.sum(0).sum(1).sum(1) + eps\n    loss_per_channel = weights * (1 - (numerator \/ denominator))\n\n    return loss_per_channel.sum() \/ output.size(1)\n<\/code>\nIt\u2019s a combination of code seen on github, tested on a 2D semantic segmentation problem.","z":"the inplace operation error comes from building encoded_target.\nencoded_target is not differentiable anyways, so first build it and then wrap it in a Variable, like this:\n<code class=\"lang-auto\"> encoded_target = output.data.clone()\nencoded_target[...] = 0\nencoded_target.scatter_(1, target.unsqueeze(1), 1)\nencoded_target = Variable(encoded_target)\n<\/code>\nYou are right, I don\u2019t even need it to be differentiable. Here is a new solution, however I would like to expand the original problem with a new feature : ignore_index\n<code class=\"lang-auto\">def dice_loss(output, target, weights=1):\n    encoded_target = output.data.clone().zero_()\n    encoded_target.scatter_(1, target.unsqueeze(1), 1)\n    encoded_target = Variable(encoded_target)\n\n    assert output.size() == encoded_target.size(), \"Input sizes must be equal.\"\n    assert output.dim() == 4, \"Input must be a 4D Tensor.\"\n\n    num = (output * encoded_target).sum(dim=3).sum(dim=2)\n    den1 = output.pow(2).sum(dim=3).sum(dim=2)\n    den2 = encoded_target.pow(2).sum(dim=3).sum(dim=2)\n\n    dice = (2 * num \/ (den1 + den2)) * weights\n    return dice.sum() \/ dice.size(0)\n<\/code>\nIn semantic segmentation we generally have a label that we want to ignore from the loss, this requirement is already specified by the ignore_index parameter of NLLLoss.\nI would like to implement the same for this dice loss, I already thought of two solutions but I don\u2019t like them :\n\nthe worst : re-encode all the labels so that the ignore_index is a valid new label, which implies to modify my classifier layer. This is really ugly for a lot of reasons.\ninside the loss function, remap ignore_label to a new label, expand the output to match the correct size, and finally ignore this label in the end. I don\u2019t really like this solution neither, it involves copying+modifying the targets and expanding the channel dimension of the output tensor (I think).\n\nIf you have already faced this kind of problem, I would like to have your point of view on this.\nThanks !\nHere is my solution to the ignore_index feature, not sure this is 100% correct. I added some comment so you can understand the logic behind. This is simple masking of the tensors.\n<code class=\"lang-auto\">def dice_loss(output, target, weights=1, ignore_index=None):\n    encoded_target = output.data.clone().zero_()\n    if ignore_index is not None:\n        # mask of invalid label\n        mask = target == -1\n        # clone target to not affect the original variable ?\n        filtered_target = target.clone()\n        # replace invalid label with whatever legal index value\n        filtered_target[mask] = 0\n        # one hot encoding\n        encoded_target.scatter_(1, filtered_target.unsqueeze(1), 1)\n        # expand the mask for the encoded target array\n        mask = mask.unsqueeze(1).expand(output.data.size())\n        # apply 0 to masked pixels\n        encoded_target[mask] = 0\n    else:\n        encoded_target.scatter_(1, target.unsqueeze(1), 1)\n    encoded_target = Variable(encoded_target)\n\n    assert output.size() == encoded_target.size(), \"Input sizes must be equal.\"\n    assert output.dim() == 4, \"Input must be a 4D Tensor.\"\n\n    num = (output * encoded_target).sum(dim=3).sum(dim=2)\n    den1 = output.pow(2)\n    den2 = encoded_target.pow(2)\n    if ignore_index is not None:\n        # exclude masked values from den1\n        den1[mask] = 0\n\n    dice = 2 * (num \/ (den1 + den2).sum(dim=3).sum(dim=2)) * weights\n    return -dice.sum() \/ dice.size(0)\n<\/code>\nFinally got something to work :\n<code class=\"lang-auto\">def dice_loss(output, target, weights=None, ignore_index=None):\n    \"\"\"\n    output : NxCxHxW Variable\n    target :  NxHxW LongTensor\n    weights : C FloatTensor\n    ignore_index : int index to ignore from loss\n    \"\"\"\n    eps = 0.0001\n\n    output = output.exp()\n    encoded_target = output.detach() * 0\n    if ignore_index is not None:\n        mask = target == ignore_index\n        target = target.clone()\n        target[mask] = 0\n        encoded_target.scatter_(1, target.unsqueeze(1), 1)\n        mask = mask.unsqueeze(1).expand_as(encoded_target)\n        encoded_target[mask] = 0\n    else:\n        encoded_target.scatter_(1, target.unsqueeze(1), 1)\n\n    if weights is None:\n        weights = 1\n\n    intersection = output * encoded_target\n    numerator = 2 * intersection.sum(0).sum(1).sum(1)\n    denominator = output + encoded_target\n\n    if ignore_index is not None:\n        denominator[mask] = 0\n    denominator = denominator.sum(0).sum(1).sum(1) + eps\n    loss_per_channel = weights * (1 - (numerator \/ denominator))\n\n    return loss_per_channel.sum() \/ output.size(1)\n<\/code>\nIt\u2019s a combination of code seen on github, tested on a 2D semantic segmentation problem.\nHi,\nWhy are you using loss_per_channel instead of finding the total loss of all channels?  Are you getting multi loss. For example, please explain, a prediction of [10,3,5,5] with ground truth [10,1,5,5] will work?\nBest\nHi,\nIf your example means you have 11 labels, using this loss will average 11 dice losses, one for each channel. I chose to have the loss per channel in case I need to weight the loss of each channels. This function will return the global dice loss, not the loss per channels.\nHi trypag,\nThanks a lot for support. I have developed this code for dice similarity measure following your code.\n\ndef dice_loss(self,output, target, weights=None, ignore_index=None):\n    # output : NxCxHxW Variable of float tensor\n    # target :  NxHxW long tensor\n    # weights : C float tensor\n    # ignore_index : int value to ignore from loss\n    smooth = 1.\n    loss = 0.\n\n    output = output.exp()   # computes the exponential of each element ie. for 0 it finds 10\n    encoded_target = output.data.clone().zero_() # make output size array and initialize with zeros\n    #ignore_index=1\n\n    if ignore_index is not None:\n        mask = target == ignore_index\n        target = target.clone()\n        target[mask] = 0\n        encoded_target.scatter_(1, target.unsqueeze(1), 1)\n        mask = mask.unsqueeze(1).expand_as(encoded_target)\n        encoded_target[mask] = 0\n    else:\n      unseq=target.long()  # here\n      unseq=unseq.data   # here\n      encoded_target.scatter_(1, unseq, 1)\n     \n\n    encoded_target = Variable(encoded_target)\n\n    if weights is None:\n        weights = Variable(torch.ones(output.size(1)).type_as(output.data))\n\n    intersection = output * encoded_target\n    numerator = 2 * intersection.sum(3).sum(2).sum(0) + smooth\n    denominator = (output + encoded_target).sum(3).sum(2).sum(0) + smooth\n    loss_per_channel = weights * (1 - (numerator \/ denominator)) # weights may be directly multiplied\n\n    return loss_per_channel.sum() \/ output.size(1)\n\n\nThe code seems to working fine, here are two things to consider. 1) what is the purpose of getting exponential of target variable and 2) i had to change few lines as shown in bold as without them i was getting errors. Can you have a look on this code. Thirdly as given in \u201cGeneralized Dice overlap as a deep learning loss function for highly unbalanced segmentation\u201d  paper, the weights should be multiplied as (1-weights*(numerator\/denominator)) and it is also given here https:\/\/cmiclab.cs.ucl.ac.uk\/CMIC\/NiftyNet\/blob\/dev\/niftynet\/layer\/loss_segmentation.py in generalized dice loss function.\nBest\nOutput is not the target variable, it\u2019s the output of my model, the feature vectors.\nI am using the exponential because the output of my model is log(softmax), so as to obtain the softmax I use the exponential of log(softmax). The original formulation is written with the softmax, I just had to adapt to my model.\nI don\u2019t see any code in bold, I noticed you changed encoded_target = output.data.clone().zero_(), the original was encoded_target = output.detach() * 0, it should have worked if output is a Variable type.\nHi , I have a question for the line of \" numerator = 2 * intersection.sum(0).sum(1).sum(1)\". Here you made the summation over the batch first, that doesn\u2019t seem right to me. The dice loss should be calculated over every example and then summing them together. Am I correct?\nthe summation over batch first in the calculation of numerator and denominator make the codes compute some approximation of dice, but not exactly dice loss."},{"x":"Hi,\nThere is something with PyTorch data augmentation that I would like to understand. I used the following code to create a training data loader:\nrgb_mean = (0.4914, 0.4822, 0.4465)\nrgb_std = (0.2023, 0.1994, 0.2010)\n\ntransform_train = transforms.Compose([\n    transforms.RandomCrop(32, padding=4),\n    transforms.RandomHorizontalFlip(),\n    transforms.ToTensor(),\n    transforms.Normalize(rgb_mean, rgb_std),\n])\n\nkwargs = {'num_workers': 2, 'pin_memory': True} if args.cuda else {}\ntrain_loader = torch.utils.data.DataLoader(datasets.CIFAR10(root=data_root, train=True, transform=transform_train,\n    transforms.Normalize(rgb_mean, rgb_std),\n]), download=False), batch_size=128, **kwargs)\n\nprint('len(train_loader)=',len(train_loader))\n\nI guess that data augmentation was used with two transformations: random crop and random horizontal flip. Thus, I would expect the obtained total number of training samples to be 3 times the size of the training set of Cifar-10, i.e. 3*50000 = 150000. However, the output of the above code is:\nlen(train_loader) = 391\nwhich means there are approximately 391*128 ~= 50000 samples. Where are the augmented data?\nThank you in advance for your help!","y":"Hello,\nin any epoch the dataloader will apply a fresh set of random operations \u201con the fly\u201d. So instead of showing the exact same items at every epoch, you are showing a variant that has been changed in a different way. So after three epochs, you would have seen three random variants of each item in a dataset.\nThat said: I don\u2019t think your counting method works for estimating the number of samples in the augmented set: The flip will double the number of pictures, but the crop has many potential outcomes. Also you would need to multiply the relative increases. (One might also question whether the augmented samples fully count, but that is a different discussion.)\nBest regards\nThomas","z":"Hello,\nin any epoch the dataloader will apply a fresh set of random operations \u201con the fly\u201d. So instead of showing the exact same items at every epoch, you are showing a variant that has been changed in a different way. So after three epochs, you would have seen three random variants of each item in a dataset.\nThat said: I don\u2019t think your counting method works for estimating the number of samples in the augmented set: The flip will double the number of pictures, but the crop has many potential outcomes. Also you would need to multiply the relative increases. (One might also question whether the augmented samples fully count, but that is a different discussion.)\nBest regards\nThomas\nThanks, Thomas! That totally makes sense. So the augmentation happens inside of this line:\nfor (data, target) in dataloader:\n\nVery cool.\nThe idea is that, according to your words,  , every epoch the data is augmetated. I\u2019m still puzzled with this, does this mean that data was transformed in every epoch? So, how to do make 3x times data in every specific epoch? To make 50000x3=150000 data in one certain epoch?\n it just means that the data is changed on the fly. The epoch size does not change, you just get randomly transformed samples every epoch. So the concept of a static dataset becomes a bit more dynamic.\nYeah, I use the some transorms to do this, such as torchvision.transforms.RandomCrop,\nbut how to double or many times the dataset? such as five_crop to 5x bigger the dataset?\nI\u2019m so confused.  .\nWell, you can see it this way: The new \u201cONE epoch\u201d is in fact FIVE consecutive old \u201cone epoch\u201d stacked together.\nCan you give an experiment on that? It\u2019s just hard to get this.\nLet n denote the size of the original dataset. In ordinary augmentation (i.e. precomputed and static, not on the fly), if we have 5 transformations then the size of the augmented data is 5n, which means at each epoch the number of iterations is also 5n. Now, if we augment the data on the fly (with random transformations) using PyTorch, then each epoch has the same number of iterations n. If we concatenate 5 epochs cons\u00e9cutive to create a large epoch (or call it whatever you want), then the total number of iterations in this large epoch is 5n. Thus it is roughly equivalent to static augmentation. (Note that this large epoch is a valid epoch because there\u2019s no duplicate in the 5n iteration since the transformations are random.)\nIt\u2019s important to understand that the data_loader only goes through the files and indexes files and their target labels. The actual loading of the images happens when the get_item() function is called, which is basically when you enumerate the DataLoader. This is usually the line in the program which looks like this -\n for data in train_loader:\n\nIt is at this time that the transformations are randomly done.\nFor clarification:\nIf you have random transformations, after each epoch, you will receive a new set of randomly transformed samples (e.g. rotated random num of deg).\nIn this case, it\u2019s enough to multiply the number of epochs to get more samples.\nBut you may want to concat all of generated samples if your transformations\nperforms determined operations. (e.g. add padding)\nThen just use ConcatDataset like here: Concatenation while using Data Loader with link \"https:\/\/discuss.pytorch.org\/t\/concatenation-while-using-data-loader\/10772\"\nIn my opinion, the first approach is slightly better to avoid overfitting.\nAfter all, you can create a transformation that will be randomly applied or not.\nHi,\nIs there any way to implement data augmentation only for some specific CIFAR-10 classes?\nYou can create your own Dataset and internally load CIFAR10.\nIn the __getitem__ method you could check what the current target is and apply the transformation based on this condition.\nCould you give me a example?\nSure! You could start with the following code:\n<code class=\"lang-auto\">class MyDataset(Dataset):\n    def __init__(self, train=True, transforms=None):\n        self.cifar10 = datasets.CIFAR10(root='YOUR_PATH',\n                                        train=train,\n                                        download=False)\n        self.transforms = transforms\n\n    def __getitem__(self, index):\n        x, y = self.cifar10[index]\n        \n        if self.transforms:\n            print('Chosing transform ', y)\n            x = self.transforms[y](x) # Chose class transform based on y\n        \n        return x, y\n\n    def __len__(self):\n        return len(self.cifar10)\n\n\nclass_transforms = []\nfor _ in range(10):\n    transform = transforms.Compose([\n        transforms.ColorJitter(\n            brightness=0.1*torch.randn(1),\n            contrast=0.1*torch.randn(1),\n            saturation=0.1*torch.randn(1),\n            hue=0.1*torch.randn(1)),\n        transforms.ToTensor()\n    ])\n    class_transforms.append(transform)\n\n\ndataset = MyDataset(transforms=class_transforms)\nloader = DataLoader(dataset, batch_size=5, shuffle=True)\ndata, target = iter(loader).next()\n> ('Chosing transform ', 9)\n('Chosing transform ', 2)\n('Chosing transform ', 5)\n('Chosing transform ', 0)\n('Chosing transform ', 2)\nprint(target)\n> tensor([ 9,  2,  5,  0,  2])\n<\/code>\nI have just applied random transformations to each class. Your class_transforms will probably look a bit more complicated. \nThanks,  \nHi,\nDoes that means  will get different randomly transformed samples every epoch, and unlikely to get same transformed sample twice?\nMaybe imgaug is the best choice.\nYes, I think so. I think the more transform options we use, the more likely we get different samples at every iteration.\nThere is a new movement right now in data augmentation I found it used in most of the super-resolution problem. for example these lines in \u201cResidual Dense Network CVPR 2018\u201d\n\nwe randomly extract 16 LR RGB patches with the size of 32 \u00d7 32 as inputs. We randomly augment\nthe patches by flipping horizontally or vertically and rotating 90\u25e6 . 1,000 iterations of back-propagation constitute an epoch\n\nwhich I guess either they are doing nested-looping or they are augmenting the dataset offline before training.\nThey said training on DIV2K they have 1000 iteration per epoch for this patche size while you should have 50. it means 20 times larger training per epoch."},{"x":"I have a 2D tensor and I have the following problem:\n<code class=\"lang-auto\">a=tensor([[1296, 1295, 1292,    4, 1311,    4, 1293,    2],\n        [1297, 1295, 1292, 1404, 1294,    4, 1293,    2]]\n<\/code>\nI need to mask all values greater than 1292, also I want to mask values in sorted order by incrementing values. What I want is:\n<code class=\"lang-auto\">tensor([[3, 2, 1292,    4, 5,    4, 1,    2],\n        [4, 3, 1292, 5, 2,    4, 1,    2]]\n<\/code>\nHow can I do this?","y":"<code class=\"lang-python\">def func(x):\n    _, indices = torch.topk(x, k=x.size(1), dim=1)\n    _, indices = torch.sort(x, dim=1)\n    mask = x <= 1292\n    num_greater = mask.sum(dim=1, keepdim=True)\n    \n    _, s_indices = torch.sort(indices, dim=1)\n    s_indices -= num_greater - 1\n    \n    return torch.where(mask, x, s_indices)\n\nx = torch.tensor([[1296, 1295, 1292, 4, 1311, 4, 1293, 2],\n                  [1297, 1295, 1292, 1401, 1294, 4, 1293, 2]], dtype=torch.int64)\nfunc(x)\n\n# tensor([[   3,    2, 1292,    4,    4,    4,    1,    2],\n#         [   4,    3, 1292,    5,    2,    4,    1,    2]])\n<\/code>","z":"<code class=\"lang-python\">def func(x):\n    _, indices = torch.topk(x, k=x.size(1), dim=1)\n    _, indices = torch.sort(x, dim=1)\n    mask = x <= 1292\n    num_greater = mask.sum(dim=1, keepdim=True)\n    \n    _, s_indices = torch.sort(indices, dim=1)\n    s_indices -= num_greater - 1\n    \n    return torch.where(mask, x, s_indices)\n\nx = torch.tensor([[1296, 1295, 1292, 4, 1311, 4, 1293, 2],\n                  [1297, 1295, 1292, 1401, 1294, 4, 1293, 2]], dtype=torch.int64)\nfunc(x)\n\n# tensor([[   3,    2, 1292,    4,    4,    4,    1,    2],\n#         [   4,    3, 1292,    5,    2,    4,    1,    2]])\n<\/code>\nHow to do the same thing when values are repeated on a dimension?\n<code class=\"lang-auto\">a=tensor([[1296, 1295, 1292,    4, 1311,    4, 1293,    2],\n        [1297, 1295, 1292, 1404, 1293,    4, 1293,    2]]\n<\/code>\na should become:\n<code class=\"lang-auto\">a = tensor([[3, 2, 1292,    4, 5,    4, 1,    2],\n        [3, 2, 1292, 4, 1,    4, 1,    2]]\n<\/code>\nI don\u2019t think that can be done with the above code, without introducing more complexity. You can also write a for loop that does the same in brute force manner."},{"x":"I tried parallelizing my training to multiple GPUs using DataParallel on two GTX1080 GPUs.\nThe training hangs after the start and I cannot even kill the docker container this is running in.\nI can execute the same code on a single GPU without any problems.\nI already tried the solutions described here with link \"https:\/\/github.com\/pytorch\/pytorch\/issues\/24081#issuecomment-569437435\" and here with link \"https:\/\/github.com\/pytorch\/pytorch\/issues\/1637#issuecomment-338268158\".\nBoth didn\u2019t help.\nWhen I run p2pBandwidthLatencyTest, I get the following output:\n<code class=\"lang-auto\">[P2P (Peer-to-Peer) GPU Bandwidth Latency Test]\nDevice: 0, GeForce GTX 1080, pciBusID: 3, pciDeviceID: 0, pciDomainID:0\nDevice: 1, GeForce GTX 1080, pciBusID: 41, pciDeviceID: 0, pciDomainID:0\nDevice=0 CAN Access Peer Device=1\nDevice=1 CAN Access Peer Device=0\n\n***NOTE: In case a device doesn't have P2P access to other one, it falls back to normal memcopy procedure.\nSo you can see lesser Bandwidth (GB\/s) and unstable Latency (us) in those cases.\n\nP2P Connectivity Matrix\n     D\\D     0     1\n     0\t     1     1\n     1\t     1     1\nUnidirectional P2P=Disabled Bandwidth Matrix (GB\/s)\n   D\\D     0      1\n     0  44.99   4.63\n     1   4.73  49.25\nUnidirectional P2P=Enabled Bandwidth (P2P Writes) Matrix (GB\/s)\n   D\\D     0      1\n     0 176.67   0.46\n     1   0.66 255.81\nBidirectional P2P=Disabled Bandwidth Matrix (GB\/s)\n   D\\D     0      1\n     0 257.41   5.53\n     1   5.40 256.42\nBidirectional P2P=Enabled Bandwidth Matrix (GB\/s)\n   D\\D     0      1\n     0 255.38   1.12\n     1   1.28 254.56\nP2P=Disabled Latency Matrix (us)\n   GPU     0      1\n     0   1.20  10.46\n     1  10.36   1.24\n\n   CPU     0      1\n     0   7.17  13.92\n     1  13.99   7.51\nP2P=Enabled Latency (P2P Writes) Matrix (us)\n<\/code>\n(the test hangs after that)","y":"The driver version is 440.82 and CUDA version 10.2.\nI think the drivers are the problem, since I had the same problem on any machine that uses these drivers (some of them have V100s instead of 1080s).\nHowever, I was able to get multi-GPU training running with the same driver version on another machine when I installed the driver via PPA instead of installing from NVIDIA directly (as it was done on the other machines).\nThanks for your suggestion to look at the drivers.\nI\u2019ll report if reinstalling on the other machines doesn\u2019t solve the problem, but for now I consider this problem solved.","z":"Could you reinstall the CUDA driver and NCCL, if you are running the test on bare metal?\nAlso, which driver are you using?\nThe driver version is 440.82 and CUDA version 10.2.\nI think the drivers are the problem, since I had the same problem on any machine that uses these drivers (some of them have V100s instead of 1080s).\nHowever, I was able to get multi-GPU training running with the same driver version on another machine when I installed the driver via PPA instead of installing from NVIDIA directly (as it was done on the other machines).\nThanks for your suggestion to look at the drivers.\nI\u2019ll report if reinstalling on the other machines doesn\u2019t solve the problem, but for now I consider this problem solved.\nWe had a similar problem when running inference, with some changes to libtorch that led to an unexpected P2P memory access from GPU0 to GPU1. If you disable IOMMU in the kernel setting the hang goes away. We are on 440.59. The problem started after we upgraded our driver. You would also see things like DMAR:[fault reason 05] PTE Write access is not set DMA write in the kernel log."},{"x":"The model is just a resnet50, finetuned on a 1.6m images dataset.\nThe first 10k steps just took about 106m, which the 10k steps from 30k to 40k took about 171m.\nBefore around 15k steps, the gpu util is quite stable and nearly reach 100%. Afterwards, the gpu util begins to jitter rapidly and seems periodically from 0~100%, but pcie bandwidth util is quite low(occasionally reach 60%) and the loading subprocesses seem to work fine: the cpu utilization is low and the disk reading speed is low (checked with iotop) which I guess is a sign that every loading subprocess has already loaded sufficient data. The gpu temperature is not high either(69C top) and gpu on the highest performace level.\nWhy does this happen? Is there any solutions?\nps. the image files reside in a ssd. the batch size is 64. The nivida-smi daemon mode is on. Once I stopped after the first epoch, and then loaded the trained model and resumed training, the speeds of the two epoch were quite same, as opposed to training continuously.","y":"Using torch.cuda.empty_cache() once in a while solved this problem.","z":"Using torch.cuda.empty_cache() once in a while solved this problem.\nThat\u2019s quite interesting. You shouldn\u2019t need to do that to solve the issue. Are you using 0.3 or some commit after 0.3?\n\n0.3.0.post4\nAnd here\u2019s a duration plot every 100 batches.\u706b\u72d0\u622a\u56fe_2017-12-23T10-59-54.263Z.png824\u00d7368 29.9 KB with link \"https:\/\/discuss.pytorch.org\/uploads\/default\/original\/2X\/7\/72f5cd13206be3d74b142765fb7b30da108771ba.png\"\nSorry about the step overlap. The linearly growing part corresponds to code without torch.cuda.empty_cache(), and the lower stable part corresponds to code with this magic command.\nAnd as I stated before, even without this command, the gpu memory utilization is quite stable during the whole training process. So I\u2019m quite confused too.\nThis might suggest something weird in our code. Do you have a script that I can try to reproduce the issue? Thanks!\nI am using pytorch version 1.1.0 and facing the same issue where the training time per step increases within the epoch. I tried removing all the lists\/other data objects that could result in storing variables from previous iterations but that did not help. Any suggestions? Also, I am using multiple GPUs to train my model and using nn.DataParallel package\nIs this issue solved here with link \"https:\/\/discuss.pytorch.org\/t\/slow-training-with-nn-dataparallel\/91377\/3\" or is this unrelated?\nThis is resolved in the reference you provided, thanks."},{"x":"When I create a PyTorch model, how do I print the number of trainable parameters? They have such features in Keras but I don\u2019t know how to do it in PyTorch.","y":"I like this solution!\nTo add my 50 cents, I would use numel() instad of np.prod() and compress the expression in one line:\n<code class=\"lang-auto\">def count_parameters(model):\n    return sum(p.numel() for p in model.parameters() if p.requires_grad)\n<\/code>","z":"for parameter in model.parameters():\n    print(parameter)\n\n\n\n sinhasam:\n\nmodel.parameters()\n\n\nhow does one make sure model.parameters() is not empty? It seems to be empty for me.\nCan I see what you are trying to do? The parameters should not be empty unless you have something like:\nclass Model(nn.Module):\n    def __init__(self):\n        super(model, self).__init__()\nmodel = Model()\n\nThe above model has no parameters.\nyou can see it here: How does one make sure that the custom NN has parameters? with link \"https:\/\/discuss.pytorch.org\/t\/how-does-one-make-sure-that-the-custom-nn-has-parameters\/5948\"\n<code class=\"lang-auto\">class NN(torch.nn.Module):\n    def __init__(self, D_layers,act,w_inits,b_inits,bias=True):\n        super(type(self), self).__init__()\n        # actiaction func\n        self.act = act\n        #create linear layers\n        self.linear_layers = [None]\n        for d in range(1,len(D_layers)):\n            linear_layer = torch.nn.Linear(D_layers[d-1], D_layers[d],bias=bias)\n            self.linear_layers.append(linear_layer)\n<\/code>\nI posted my response on your original question!\ndef get_n_params(model):\n    pp=0\n    for p in list(model.parameters()):\n        nn=1\n        for s in list(p.size()):\n            nn = nn*s\n        pp += nn\n    return pp\nTo compute the number of trainable parameters:\nmodel_parameters = filter(lambda p: p.requires_grad, model.parameters())\nparams = sum([np.prod(p.size()) for p in model_parameters])\nI like this solution!\nTo add my 50 cents, I would use numel() instad of np.prod() and compress the expression in one line:\n<code class=\"lang-auto\">def count_parameters(model):\n    return sum(p.numel() for p in model.parameters() if p.requires_grad)\n<\/code>\n\n\n\n baldassarre.fe:\n\ndef count_parameters(model):\nreturn sum(p.numel() for p in model.parameters() if p.requires_grad)\n\n\nProvided the models are similar in keras and pytorch, the number of trainable parameters returned are different in pytorch and keras.\nimport torch\nimport torchvision\nfrom torch import nn\nfrom torchvision import models\na= models.resnet50(pretrained=False)\na.fc = nn.Linear(512,2)\ncount = count_parameters(a)\nprint (count)\n23509058\nNow in keras\nimport keras.applications.resnet50 as resnet\nmodel =resnet.ResNet50(include_top=True, weights=None, input_tensor=None, input_shape=None, pooling=None, classes=2)\nprint model.summary()\nTotal params: 23,591,810\nTrainable params: 23,538,690\nNon-trainable params: 53,120\nAny reasons why this difference in numbers pop up?\nHi Alex, well spotted. I never did this comparison.\nOne easy check it to compare the layers one by one, (Linear, Conv2d, BatchNorm etc.), and see if there\u2019s any difference in the number of params.\nHowever, I don\u2019t think there will be any difference, provided that you pay attention to the sneaky default parameters.\nAfter that, you can patiently compare the graphs layer by layer and see if you spot any difference. Maybe it\u2019s a matter of omitted\/shared biases in some of the layers.\nBtw, the first test is also a good check for the count_parameters() function, let us now if you discover some unexpected behavior \n\n\n\n sinhasam:\n\nfor parameter in model.parameters(): print(parameter)\n\n\nHave you checked if they are the bias weights?\nI guess this counts shared parameters multiple times, doesn\u2019t it?\n<code class=\"lang-auto\">import torch\nfrom models.modelparts import count_parameters\nclass tstModel(torch.nn.Module):\n    def __init__(self):\n        super().__init__()\n        self.p = torch.nn.Parameter(\n            torch.randn(1, 1, 1, requires_grad=True)\n                .expand(-1, 5, -1)\n        )\nprint(count_parameters(tstModel()))\n<\/code>\nprints 5\nIf I understand correctly, expand just creates tensor with 5 views to the same parameter, so the right answer should be 1.\nBut I don\u2019t know how to fix that.\ndid anyone figure out a solution for shared parameters?\nSo I get that by default, Conv2d includes the bias. But I\u2019m unclear as to why they (the biases) are being included in \u2018requires_grad\u2019.\nIn [1]: conv_3 = nn.Conv2d(512, 256, kernel_size=3, bias=True)\nIn [2]: sum(p.numel() for p in conv_3.parameters())\nOut[2]: 1179904\nIn [3]: sum(p.numel() for p in conv_3.parameters() if p.requires_grad)\nOut[3]: 1179904\nThe bias is a trainable parameter, which requires gradients and is optimized in the same way as the weight parameter.\nDo you have a use case, where the bias is fixed to a specific value?\nAh sorry. It was a conceptual error on my part. I had confused the idea of bias being a constant value with a weight with bias being a constant value.\nThanks for the clarification.\njust out of curiosity, is there a np.prod for pytorch?\nWell, there\u2019s torch.prod with link \"https:\/\/pytorch.org\/docs\/stable\/torch.html?highlight=prod#torch.prod\", but unlike numpy it accepts only tensors and does not accept tuples, lists, etc.\nYou may find this useful: https:\/\/pypi.org\/project\/pytorch-model-summary\/"},{"x":"In pytorch, to update the model, should I use optimizer.step or model.step ? My question also valid for zero_grad() method?\nHere is a example snippet:\n<code class=\"lang-auto\">import torch\nimport torch nn\nclass SomeNeuralNet(nn.Module):\n    def __init__(self,hs,es,dropout):\n          SomeNeuralNet(ClaimRecognizer, self).__init__()\n          # Some initialization here\n    def forward(x):\n          # forward propagation here\n\nmodel = SomeNeuralNet(es,hs,dp)\noptimizer = optim.Adam(model.parameters())\nloss_function = nn.NLLLoss()\nfor epoch in N:\n   for x in data:\n       # Which one I should call ? optimizer.zero_grad() or model.zero_grad() or both ?       \n       model.zero_grad()\n       optimizer.zero_grad()\n      logp = model(x)\n      loss = loss_function(logp,gold_outs)\n      loss.backward()\n       # Which one I should call ? Optimizer.step() or model.step() or both ?\n       optimizer.step()\n       model.step()\n<\/code>","y":"nn.Module doesn\u2019t have a step method, so you should call optimizer.step().\nThe model itself doesn\u2019t know anything about the optimization of its parameters.\nIn case of calling zero_grad it depends on your use case.\nIf you pass all parameters to the optimizer, both calls will be identical and clear all gradients.\nIn case you pass only certain parameters to the optimizer, optimizer.zero_grad() will only clear those, while model.zero_grad() will clear all parameters.","z":"nn.Module doesn\u2019t have a step method, so you should call optimizer.step().\nThe model itself doesn\u2019t know anything about the optimization of its parameters.\nIn case of calling zero_grad it depends on your use case.\nIf you pass all parameters to the optimizer, both calls will be identical and clear all gradients.\nIn case you pass only certain parameters to the optimizer, optimizer.zero_grad() will only clear those, while model.zero_grad() will clear all parameters.\nThank you for the explanation.\nDoes the optimizer step() method change\/update the optimizer? In other words, should I be saving\/loading the optimizer as well as the model during the training process?\nYes, it might change some internal states, such as running estimates of the gradients if these are used, so that you should save the optimizer.stat_dict() additionally to the model.state_dict() to restore the training later.\nIn case the optimizer doesn\u2019t use these estimates, saving the state_dict() wouldn\u2019t change anything and I would recommend to use it anyway."},{"x":"I want to give an input to the model and predict the class label.How can I write my code?Please answer.Thanks in advance!","y":"Try using pred_out, pred_index = torch.max(pred, 1)\nAlso, try wrapping your inference code with the wrapper: with torch.no_grad():","z":"You can use the batch size as 1 if you want to do single sample using pytorch test data loader.\nIf not you can create tensor of same size and shape as u r input and just pass it as model(input)\nThanks a lot for answering! I get :\npredict is: tensor([[0.1559, 0.4340, 0.3785, 0.0226, 0.0032, 0.0519, 0.1933]],\ndevice=\u2018cuda:0\u2019, grad_fn=)\nWhen tensor is converted to numpy:  predict is: [[-0.2556236   0.03545919 -0.38335416  0.3578361  -0.42080054  0.3241588\n0.24587154]] .I want to get class label,and how can I predict class label?Thanks in advance!\nTry using pred_out, pred_index = torch.max(pred, 1)\nAlso, try wrapping your inference code with the wrapper: with torch.no_grad():\nThanks a lot! pred_out is: tensor([0.6067])\npred_index is: tensor([1])\nThe outputs are not same when I run again and again.Why this happens?Please answer how to get same result when it is run again and again.Thanks in advance!\nSure.\nFor getting deterministic results, you need to set the seed.\nRefer to this post: Reproducibility with all the bells and whistles with link \"https:\/\/discuss.pytorch.org\/t\/reproducibility-with-all-the-bells-and-whistles\/81097\"\ncan you explain why:\n<code class=\"lang-auto\">torch.max(output.data, 1)\n<\/code>\nworks?\nI was reading the docs (https:\/\/pytorch.org\/docs\/stable\/generated\/torch.max.html) and it\u2019s still not entirely clear to me. This is the sentence that doesn\u2019t make sense to me:\n<code class=\"lang-auto\">Returns a namedtuple (values, indices) where values is the maximum value of each row of the input tensor in the given dimension dim.\n<\/code>\nI don\u2019t understand what the max value of a row mean given a specific slice of a tensor. Rows only exist in 2D tensors (matrices), at least to me. Can you clarify?"},{"x":"I am trying to average the output of the nn.MSELoss() over the following batch before firing batch_loss.backward().\n<code class=\"lang-auto\">[tensor(0.0031, device='cuda:0', grad_fn=<MseLossBackward>),\n tensor(0.0010, device='cuda:0', grad_fn=<MseLossBackward>),\n tensor(0.0015, device='cuda:0', grad_fn=<MseLossBackward>),\n tensor(0.0027, device='cuda:0', grad_fn=<MseLossBackward>),\n tensor(0.0042, device='cuda:0', grad_fn=<MseLossBackward>),\n tensor(0.0045, device='cuda:0', grad_fn=<MseLossBackward>),\n tensor(0.0044, device='cuda:0', grad_fn=<MseLossBackward>),\n tensor(0.0005, device='cuda:0', grad_fn=<MseLossBackward>),\n tensor(0.0034, device='cuda:0', grad_fn=<MseLossBackward>),\n tensor(0.0023, device='cuda:0', grad_fn=<MseLossBackward>)]\n<\/code>\nHow can I do this whilst retaining the grad_fn information? As if I use torch.mean(torch.tensor()), I lose this information.\nThanks!","y":"sum(output).backward(). It will work if output is a list.","z":"sum(output).backward(). It will work if output is a list."},{"x":"Is there anything similar to NumPy random.randint to generate random integers between a certain range?","y":"this should work:\n<code class=\"lang-auto\">import torch\n\nbatch_size = 16\ny = torch.randint(low=0,high=Dout,size=(batch_size,))\n<\/code>\ndocs: https:\/\/pytorch.org\/docs\/stable\/generated\/torch.randint.html#torch.randint","z":"torch.rand with link \"http:\/\/pytorch.org\/docs\/0.3.1\/torch.html?highlight=torch%20random#torch.rand\" outputs a tensor fill out with random numbers within [0,1). You can use that and convert it to the range [l,r) using a formula like l + torch.rand() * (r - l) and then converting them to integers as usual.\nYou could also directly set the range using:\ntorch.LongTensor(10).random_(0, 10)\n\nwhere numbers define the lower and upper bound, respectively.\nPyTorch also supports randint: https:\/\/pytorch.org\/docs\/stable\/torch.html#torch.randint\n<code class=\"lang-auto\">torch.randint(3, 5, (3,))\ntensor([ 4.,  3.,  4.])\n<\/code>\nthis should work:\n<code class=\"lang-auto\">import torch\n\nbatch_size = 16\ny = torch.randint(low=0,high=Dout,size=(batch_size,))\n<\/code>\ndocs: https:\/\/pytorch.org\/docs\/stable\/generated\/torch.randint.html#torch.randint"},{"x":"I am aware the code is (How to predict only one test sample in pytorch model? with link \"https:\/\/discuss.pytorch.org\/t\/how-to-predict-only-one-test-sample-in-pytorch-model\/81407\/7\"):\n<code class=\"lang-auto\">pred = torch.max(output.data, 1)\n<\/code>\nas seen on the cifar10 beginner tutorial. But I\u2019d like to understand why that\u2019s correct.\nI was reading the docs (https:\/\/pytorch.org\/docs\/stable\/generated\/torch.max.html) and it\u2019s still not entirely clear to me. This is the sentence that doesn\u2019t make sense to me:\n<code class=\"lang-auto\">Returns a namedtuple (values, indices) where values is the maximum value of each row of the input tensor in the given dimension dim.\n<\/code>\nI don\u2019t understand what the max value of a row mean given a specific slice of a tensor. Rows only exist in 2D tensors (matrices), at least to me. Can you clarify?\nNote that I do understand what the dim is suppoe to do (see: https:\/\/towardsdatascience.com\/understanding-dimensions-in-pytorch-6edf9972d3be)\n\nref\/motivation, understand this clearly: Calculating accuracy of the current minibatch? with link \"https:\/\/discuss.pytorch.org\/t\/calculating-accuracy-of-the-current-minibatch\/4308\"","y":"the main thing is that you have to reduce\/collapse the dimension where the classification raw value\/logit is with a max and then select it with a .indices. Usually this is dimensions 1 since dim 0 has the batch size e.g. [batch_size,D_classification] where the raw data might of size [batch_size,C,H,W]\nA synthetic example with raw data in 1D as follows:\n<code class=\"lang-auto\">import torch\nimport torch.nn as nn\n\n# data dimension [batch-size, D]\nD, Dout = 1, 5\nbatch_size = 16\nx = torch.randn(batch_size, D)\ny = torch.randint(low=0,high=Dout,size=(batch_size,))\n\nmdl = nn.Linear(D, Dout)\nlogits = mdl(x)\nprint(f'y.size() = {y.size()}')\n# removes the 1th dimension with a max, which is the classification layer\n# which means it returns the most likely label. Also, note you need to choose .indices since you want to return the\n# position of where the most likely label is (not it's raw logit value)\npred = logits.max(1).indices\nprint(pred)\n\nprint('--- preds vs truth ---')\nprint(f'predictions = {pred}')\nprint(f'y = {y}')\n\nacc = (pred == y).sum().item() \/ pred.size(0)\nprint(acc)\n<\/code>\noutput:\n<code class=\"lang-auto\">\ny.size() = torch.Size([16])\ntensor([3, 1, 1, 3, 4, 1, 4, 3, 1, 1, 4, 4, 4, 4, 3, 1])\n--- preds vs truth ---\npredictions = tensor([3, 1, 1, 3, 4, 1, 4, 3, 1, 1, 4, 4, 4, 4, 3, 1])\ny = tensor([3, 3, 3, 0, 3, 4, 0, 1, 1, 2, 1, 4, 4, 2, 0, 0])\n0.25\n<\/code>","z":"Hi,\n\n\n\n Brando_Miranda:\n\nRows only exist in 2D tensors (matrices), at least to me\n\n\nPyTorch modules such as Conv or Linear, only accept batched data, so if you have a single image you still have to create batches of size 1. So, not matter what type of data you are working with, you at least have 2D tensor for input and obviously, your model in the simplest case will generate a prob\/logit for each sample in each batch where in case of a single instance in a batch, it would be [1, number_of_classes]. Now, taking max will give you value of max and index of max for each row which corresponds to each sample in each batch.\nBests\nthe main thing is that you have to reduce\/collapse the dimension where the classification raw value\/logit is with a max and then select it with a .indices. Usually this is dimensions 1 since dim 0 has the batch size e.g. [batch_size,D_classification] where the raw data might of size [batch_size,C,H,W]\nA synthetic example with raw data in 1D as follows:\n<code class=\"lang-auto\">import torch\nimport torch.nn as nn\n\n# data dimension [batch-size, D]\nD, Dout = 1, 5\nbatch_size = 16\nx = torch.randn(batch_size, D)\ny = torch.randint(low=0,high=Dout,size=(batch_size,))\n\nmdl = nn.Linear(D, Dout)\nlogits = mdl(x)\nprint(f'y.size() = {y.size()}')\n# removes the 1th dimension with a max, which is the classification layer\n# which means it returns the most likely label. Also, note you need to choose .indices since you want to return the\n# position of where the most likely label is (not it's raw logit value)\npred = logits.max(1).indices\nprint(pred)\n\nprint('--- preds vs truth ---')\nprint(f'predictions = {pred}')\nprint(f'y = {y}')\n\nacc = (pred == y).sum().item() \/ pred.size(0)\nprint(acc)\n<\/code>\noutput:\n<code class=\"lang-auto\">\ny.size() = torch.Size([16])\ntensor([3, 1, 1, 3, 4, 1, 4, 3, 1, 1, 4, 4, 4, 4, 3, 1])\n--- preds vs truth ---\npredictions = tensor([3, 1, 1, 3, 4, 1, 4, 3, 1, 1, 4, 4, 4, 4, 3, 1])\ny = tensor([3, 3, 3, 0, 3, 4, 0, 1, 1, 2, 1, 4, 4, 2, 0, 0])\n0.25\n<\/code>\nTo understand reductions see this and related links:\n<code class=\"lang-auto\">\n# https:\/\/towardsdatascience.com\/understanding-dimensions-in-pytorch-6edf9972d3be\n# dimension\n\"\"\"\nDimension reduction. It collapses\/reduces a specific dimension by selecting an element from that dimension to be\nreduced.\nConsider x is 3D tensor. x.sum(1) converts x into a tensor that is 2D using an element from D1 elements in\nthe 1th dimension. Thus:\nx.sum(1) = x[i,k] = op(x[i,:,k]) = op(x[i,0,k],...,x[i,D1,k])\nthe key is to realize that we need 3 indices to select a single element. So if we use only 2 (because we are collapsing)\nthen we have D1 number of elements possible left that those two indices might indicate. So from only 2 indices we get a\nset that we need to specify how to select. This is where the op we are using is used for and selects from this set.\nIn theory if we want to collapse many indices we need to indicate how we are going to allow indexing from a smaller set\nof indices (using the remaining set that we'd usually need).\n\"\"\"\n\nimport torch\n\nx = torch.tensor([\n     [1, 2, 3],\n     [4, 5, 6]\n   ])\n\nprint(f'x.size() = {x.size()}')\n\n# sum the 0th dimension (rows). So we get a bunch of colums that have the rows added together.\nx0 = x.sum(0)\nprint(x0)\n\n# sum the 1th dimension (columns)\nx1 = x.sum(1)\nprint(x1)\n\nx_1 = x.sum(-1)\nprint(x_1)\n\nx0 = x.max(0)\nprint(x0.values)\n\ny = torch.tensor([[\n         [ 1,  2,  3,  4],\n         [ 5,  6,  7,  8],\n         [ 9, 10, 11, 12]],\n\n        [[13, 14, 15, 16],\n         [17, 18, 19, 20],\n         [21, 22, 23, 24]]])\n\nprint(y)\n\n# into the screen [1, 13]\nprint(y[:,0,0])\n# columns [1, 5, 9]\nprint(y[0,:,0])\n# rows [1, 2, 3, 4]\nprint(y[0,0,:])\n\n# for each remaining index, select the largest value in the \"screen\" dimension\ny0 = y.max(0)\nprint(y0.values)\n<\/code>"},{"x":"How can I calculate the acuracy for  the current mini batch in my training? My trainining code is just:\nfor epoch in range(args.epochs):\n  for i, (images, captions, lengths) in enumerate(train_loader):\n    targets = pack_padded_sequence(captions, lengths, batch_first=True)[0]\n    features = encoder(images)\n    outputs = decoder(features, captions, lengths)\n    loss = criterion(outputs, targets)\n\nhow can I get how many are correct from that code?\nI tried several ways but cant get it to work:\ncorrect = (targets.eq(outputs)).sum()\nIm sure there should be a generic way to do this. if criterion can calculate the loss without knowing the shapes, I think I should be able to calculate accuracy as well? Any idea on how to do this?\nFile \u201ctrain.py with link \"http:\/\/train.py\"\u201d, line 182, in \nmain(args)\nFile \u201ctrain.py with link \"http:\/\/train.py\"\u201d, line 139, in main\ncorrect = (ttargets.eq(toutputs)).sum()\nFile \u201c\/home\/joy\/anaconda3\/lib\/python3.6\/site-packages\/torch\/autograd\/variable.py\u201d, line 710, in eq\nreturn Eq()(self, other)\nFile \u201c\/home\/joy\/anaconda3\/lib\/python3.6\/site-packages\/torch\/autograd\/_functions\/compare.py\u201d, line 14, in forward\nmask = getattr(tensor1, self.fn_name)(other)\nTypeError: eq received an invalid combination of arguments - got (torch.cuda.FloatTensor), but expected one of:\n\n(int value)\ndidn\u2019t match because some of the arguments have invalid types: (torch.cuda.FloatTensor)\n(torch.cuda.LongTensor other)\ndidn\u2019t match because some of the arguments have invalid types: (torch.cuda.FloatTensor)\n","y":"I think the simplest answer is the one from the cifar10 tutorial with link \"https:\/\/pytorch.org\/tutorials\/beginner\/blitz\/cifar10_tutorial.html\":\n<code class=\"lang-auto\">total = 0\nwith torch.no_grad():\n    net.eval()\n    for data in testloader:\n        images, labels = data\n        outputs = net(images)\n        _, predicted = torch.max(outputs.data, 1)\n        total += labels.size(0)\n        correct += (predicted == labels).sum().item()\n\nprint('Accuracy of the network on the 10000 test images: %d %%' % (\n    100 * correct \/ total))\n<\/code>\nso:\n<code class=\"lang-auto\">acc = (true == pred).sum().item()\n<\/code>\nIf you have a counter don\u2019t forget to eventually divide by the size of the data-set or analogous values.\nI\u2019ve used:\n<code class=\"lang-auto\">N = data.size(0) # since usually it's size (batch_size, D1, D2, ...)\ncorrect += (1\/N) * correct\n<\/code>\n\nSelf contained code:\n<code class=\"lang-auto\"># testing accuracy function\n# https:\/\/discuss.pytorch.org\/t\/calculating-accuracy-of-the-current-minibatch\/4308\/11\n# https:\/\/stackoverflow.com\/questions\/51503851\/calculate-the-accuracy-every-epoch-in-pytorch\n\nimport torch\nimport torch.nn as nn\n\nD = 1\ntrue = torch.tensor([0,1,0,1,1]).reshape(5,1)\nprint(f'true.size() = {true.size()}')\n\nbatch_size = true.size(0)\nprint(f'batch_size = {batch_size}')\nx = torch.randn(batch_size,D)\nprint(f'x = {x}')\nprint(f'x.size() = {x.size()}')\n\nmdl = nn.Linear(D,1)\nlogit = mdl(x)\n_, pred = torch.max(logit.data, 1)\n\nprint(f'logit = {logit}')\n\nprint(f'pred = {pred}')\nprint(f'true = {true}')\n\nacc = (true == pred).sum().item()\nprint(f'acc = {acc}')\n<\/code>\n\nAlso, I find this code to be good reference:\n<code class=\"lang-auto\">def calc_accuracy(mdl, X, Y):\n    # reduce\/collapse the classification dimension according to max op\n    # resulting in most likely label\n    max_vals, max_indices = mdl(X).max(1)\n    # assumes the first dimension is batch size\n    n = max_indices.size(0)  # index 0 for extracting the # of elements\n    # calulate acc (note .item() to do float division)\n    acc = (max_indices == Y).sum().item() \/ n\n    return acc\n<\/code>\n\nExplaining pred = mdl(x).max(1)see this How does one get the predicted classification label from a pytorch model? with link \"https:\/\/discuss.pytorch.org\/t\/how-does-one-get-the-predicted-classification-label-from-a-pytorch-model\/91649\"\nthe main thing is that you have to reduce\/collapse the dimension where the classification raw value\/logit is with a max and then select it with a .indices. Usually this is dimensions 1 since dim 0 has the batch size e.g. [batch_size,D_classification] where the raw data might of size [batch_size,C,H,W]\nA synthetic example with raw data in 1D as follows:\n<code class=\"lang-auto\">import torch\nimport torch.nn as nn\n\n# data dimension [batch-size, D]\nD, Dout = 1, 5\nbatch_size = 16\nx = torch.randn(batch_size, D)\ny = torch.randint(low=0,high=Dout,size=(batch_size,))\n\nmdl = nn.Linear(D, Dout)\nlogits = mdl(x)\nprint(f'y.size() = {y.size()}')\n# removes the 1th dimension with a max, which is the classification layer\n# which means it returns the most likely label. Also, note you need to choose .indices since you want to return the\n# position of where the most likely label is (not it's raw logit value)\npred = logits.max(1).indices\nprint(pred)\n\nprint('--- preds vs truth ---')\nprint(f'predictions = {pred}')\nprint(f'y = {y}')\n\nacc = (pred == y).sum().item() \/ pred.size(0)\nprint(acc)\n<\/code>\noutput:\n<code class=\"lang-auto\">\ny.size() = torch.Size([16])\ntensor([3, 1, 1, 3, 4, 1, 4, 3, 1, 1, 4, 4, 4, 4, 3, 1])\n--- preds vs truth ---\npredictions = tensor([3, 1, 1, 3, 4, 1, 4, 3, 1, 1, 4, 4, 4, 4, 3, 1])\ny = tensor([3, 3, 3, 0, 3, 4, 0, 1, 1, 2, 1, 4, 4, 2, 0, 0])\n0.25\n<\/code>\n\nreference:\n\nCalculating accuracy of the current minibatch? with link \"https:\/\/discuss.pytorch.org\/t\/calculating-accuracy-of-the-current-minibatch\/4308\/5\"\nHow does one get the predicted classification label from a pytorch model? with link \"https:\/\/discuss.pytorch.org\/t\/how-does-one-get-the-predicted-classification-label-from-a-pytorch-model\/91649\/3\"\nSO: https:\/\/stackoverflow.com\/questions\/51503851\/calculate-the-accuracy-every-epoch-in-pytorch\/63253642#63253642\n\n","z":"<code class=\"lang-auto\">correct = (ttargets.eq(toutputs.long())).sum()\n<\/code>\nThanks ,that fixed the Long issue. I get another issue after that, the tensor sizes do not match.\nttargets.size() is\n[torch.cuda.LongTensor of size 11 (GPU 0)]\nand toutputs.size() is:\n[torch.cuda.FloatTensor of size 11x13 (GPU 0)]\nMy question is how can I calculate accuracy generically for any tensor like how  \" loss = criterion(outputs, targets)\" can calculate the loss without knowing the details? I looked through the code for criterion (CrossEntropyLoss) and I am confused on how it is calculating loss.\n<code class=\"lang-auto\">max_index = ttoutputs.max(dim = 1)[1]\n(max_index == ttargets).sum()\n<\/code>\nthe detail of max function can be found from docs of torch.max\nDoes the following not work:\n<code class=\"lang-auto\">def calc_accuracy(mdl,X,Y):\n    # TODO: why can't we call .data.numpy() for train_acc as a whole?\n    max_vals, max_indices = torch.max(mdl(X),1)\n    train_acc = (max_indices == Y).sum().data.numpy()\/max_indices.size()[0]\n    return train_acc\n<\/code>\n??\nseems this works too:\n<code class=\"lang-auto\">def calc_accuracy(mdl,X,Y):\n    max_vals, max_indices = torch.max(mdl(X),1)\n    train_acc = (max_indices == Y).sum().item()\/max_indices.size()[0]\n    return train_acc\n<\/code>\nif you want the output to be a tensor:\n<code class=\"lang-auto\">def calc_accuracy(mdl,X,Y):\n    \"\"\"Calculates model accuracy\n    \n    Arguments:\n        mdl {nn.model} -- nn model\n        X {torch.Tensor} -- input data\n        Y {torch.Tensor} -- labels\/target values\n    \n    Returns:\n        [torch.Tensor] -- accuracy\n    \"\"\"\n    max_vals, max_indices = torch.max(mdl(X),1)\n    n = max_indices.size(0) #index 0 for extracting the # of elements\n    train_acc = (max_indices == Y).sum(dtype=torch.float32)\/n\n    return train_acc\n<\/code>\ni use this usually for classification:\n<code class=\"lang-auto\">def accuracy(true,pred):\n    acc = (true.argmax(-1) == pred.argmax(-1)).float().detach().numpy()\n    return float(100 * acc.sum() \/ len(acc))\n<\/code>\nwhich true and pred are both a torch tensor\nI use the following snippet when having preds, and labels and mask:\nacc=(((pred==labels)*mask).sum(dim=1))\/mask.sum(dim=1)\ndo you have a small snippet to test this that is self contained?\nCan you explain the negative index?\nI know this is basic but it\u2019s not fresh in my head. What is the mask suppose to stand for when they are predictions? is it 0-1 values? What is it suppose to mask off? Unwanted examples\u2026?\n\n\ncan you comment on the true.argmax(-1)? What does the -1 do? ----- hopefully answer will be here someday: Argmax with PyTorch with link \"https:\/\/discuss.pytorch.org\/t\/argmax-with-pytorch\/1528\/10\"\nI think the simplest answer is the one from the cifar10 tutorial with link \"https:\/\/pytorch.org\/tutorials\/beginner\/blitz\/cifar10_tutorial.html\":\n<code class=\"lang-auto\">total = 0\nwith torch.no_grad():\n    net.eval()\n    for data in testloader:\n        images, labels = data\n        outputs = net(images)\n        _, predicted = torch.max(outputs.data, 1)\n        total += labels.size(0)\n        correct += (predicted == labels).sum().item()\n\nprint('Accuracy of the network on the 10000 test images: %d %%' % (\n    100 * correct \/ total))\n<\/code>\nso:\n<code class=\"lang-auto\">acc = (true == pred).sum().item()\n<\/code>\nIf you have a counter don\u2019t forget to eventually divide by the size of the data-set or analogous values.\nI\u2019ve used:\n<code class=\"lang-auto\">N = data.size(0) # since usually it's size (batch_size, D1, D2, ...)\ncorrect += (1\/N) * correct\n<\/code>\n\nSelf contained code:\n<code class=\"lang-auto\"># testing accuracy function\n# https:\/\/discuss.pytorch.org\/t\/calculating-accuracy-of-the-current-minibatch\/4308\/11\n# https:\/\/stackoverflow.com\/questions\/51503851\/calculate-the-accuracy-every-epoch-in-pytorch\n\nimport torch\nimport torch.nn as nn\n\nD = 1\ntrue = torch.tensor([0,1,0,1,1]).reshape(5,1)\nprint(f'true.size() = {true.size()}')\n\nbatch_size = true.size(0)\nprint(f'batch_size = {batch_size}')\nx = torch.randn(batch_size,D)\nprint(f'x = {x}')\nprint(f'x.size() = {x.size()}')\n\nmdl = nn.Linear(D,1)\nlogit = mdl(x)\n_, pred = torch.max(logit.data, 1)\n\nprint(f'logit = {logit}')\n\nprint(f'pred = {pred}')\nprint(f'true = {true}')\n\nacc = (true == pred).sum().item()\nprint(f'acc = {acc}')\n<\/code>\n\nAlso, I find this code to be good reference:\n<code class=\"lang-auto\">def calc_accuracy(mdl, X, Y):\n    # reduce\/collapse the classification dimension according to max op\n    # resulting in most likely label\n    max_vals, max_indices = mdl(X).max(1)\n    # assumes the first dimension is batch size\n    n = max_indices.size(0)  # index 0 for extracting the # of elements\n    # calulate acc (note .item() to do float division)\n    acc = (max_indices == Y).sum().item() \/ n\n    return acc\n<\/code>\n\nExplaining pred = mdl(x).max(1)see this How does one get the predicted classification label from a pytorch model? with link \"https:\/\/discuss.pytorch.org\/t\/how-does-one-get-the-predicted-classification-label-from-a-pytorch-model\/91649\"\nthe main thing is that you have to reduce\/collapse the dimension where the classification raw value\/logit is with a max and then select it with a .indices. Usually this is dimensions 1 since dim 0 has the batch size e.g. [batch_size,D_classification] where the raw data might of size [batch_size,C,H,W]\nA synthetic example with raw data in 1D as follows:\n<code class=\"lang-auto\">import torch\nimport torch.nn as nn\n\n# data dimension [batch-size, D]\nD, Dout = 1, 5\nbatch_size = 16\nx = torch.randn(batch_size, D)\ny = torch.randint(low=0,high=Dout,size=(batch_size,))\n\nmdl = nn.Linear(D, Dout)\nlogits = mdl(x)\nprint(f'y.size() = {y.size()}')\n# removes the 1th dimension with a max, which is the classification layer\n# which means it returns the most likely label. Also, note you need to choose .indices since you want to return the\n# position of where the most likely label is (not it's raw logit value)\npred = logits.max(1).indices\nprint(pred)\n\nprint('--- preds vs truth ---')\nprint(f'predictions = {pred}')\nprint(f'y = {y}')\n\nacc = (pred == y).sum().item() \/ pred.size(0)\nprint(acc)\n<\/code>\noutput:\n<code class=\"lang-auto\">\ny.size() = torch.Size([16])\ntensor([3, 1, 1, 3, 4, 1, 4, 3, 1, 1, 4, 4, 4, 4, 3, 1])\n--- preds vs truth ---\npredictions = tensor([3, 1, 1, 3, 4, 1, 4, 3, 1, 1, 4, 4, 4, 4, 3, 1])\ny = tensor([3, 3, 3, 0, 3, 4, 0, 1, 1, 2, 1, 4, 4, 2, 0, 0])\n0.25\n<\/code>\n\nreference:\n\nCalculating accuracy of the current minibatch? with link \"https:\/\/discuss.pytorch.org\/t\/calculating-accuracy-of-the-current-minibatch\/4308\/5\"\nHow does one get the predicted classification label from a pytorch model? with link \"https:\/\/discuss.pytorch.org\/t\/how-does-one-get-the-predicted-classification-label-from-a-pytorch-model\/91649\/3\"\nSO: https:\/\/stackoverflow.com\/questions\/51503851\/calculate-the-accuracy-every-epoch-in-pytorch\/63253642#63253642\n\n"},{"x":"Hello!\nI try to implement the classification of images with bayesian CNN using dropout,  when I started the program I noticed that the test accuracy exceeds 100 which is not logical, I don\u2019t see what the problem is I don\u2019t know if it\u2019s because of convolution and pooling layer parameters or what, Any idea, please?\n<code class=\"lang-auto\">import torch\nimport torchvision\nimport torchvision.transforms as transforms\nbatch_size = 4\n\ntrain_transform = transforms.Compose(\n    [\n#   transforms.RandomCrop(32, padding=4),\n#   transforms.RandomHorizontalFlip(),\n     transforms.ToTensor(),\n     transforms.Normalize((0.5, 0.5, 0.5), (0.5, 0.5, 0.5))])\n\ntest_transform = transforms.Compose(\n    [transforms.ToTensor(),\n     transforms.Normalize((0.5, 0.5, 0.5), (0.5, 0.5, 0.5))])\n\ntrainset = torchvision.datasets.CIFAR10(root='.\/data1', train=True,\n                                        download=True, transform=train_transform)\ntrainloader = torch.utils.data.DataLoader(trainset, batch_size=batch_size,\n                                          shuffle=True)\n\ntestset = torchvision.datasets.CIFAR10(root='.\/data1', train=False,\n                                       download=True, transform=test_transform)\ntestloader = torch.utils.data.DataLoader(testset, batch_size=batch_size,shuffle=False)\n\nclasses = ('plane', 'car', 'bird', 'cat',\n           'deer', 'dog', 'frog', 'horse', 'ship', 'truck')\nprint('train set size: {}'.format(len(trainset)))\nlog_freq = len(trainset)\/\/batch_size\nprint('log freq: {}'.format(log_freq))\nprint('test set size: {}'.format(len(testset)))\n\nimport matplotlib.pyplot as plt\nimport numpy as np\n\ndef imshow(img):\n    img = img \/ 2 + 0.5     # unnormalize\n    npimg = img.numpy()\n    plt.imshow(np.transpose(npimg, (1, 2, 0)))\n    plt.show()\n\ndataiter = iter(trainloader)\nimages, labels = dataiter.next()\nimshow(torchvision.utils.make_grid(images[:4]))\n\nn_batches = len(dataiter)\n\nimport torch.nn as nn\nimport torch.nn.functional as F\n\nclass Net_MCDO(nn.Module):\n    \n   def __init__(self):\n        super(Net, self).__init__()\n        self.conv1 = nn.Conv2d(3, 6, 5)\n        self.pool = nn.MaxPool2d(2, 2)\n        self.conv2 = nn.Conv2d(6, 16, 5)\n        self.fc1 = nn.Linear(16 * 5 * 5, 120)\n        self.fc2 = nn.Linear(120, 84)\n        self.fc3 = nn.Linear(84, 10)\n        self.dropout = nn.Dropout(p=0.3)\n\n    def forward(self, x):\n        x = self.pool(F.relu(self.dropout(self.conv1(x))))  # recommended to add the relu\n        x = self.pool(F.relu(self.dropout(self.conv2(x))))  # recommended to add the relu\n        x = x.view(-1, 16 * 5 * 5)\n        x = F.relu(self.fc1(x))\n        x = F.relu(self.fc2(self.dropout(x)))\n        x = self.fc3(self.dropout(x)) # no activation function needed for the last layer\n        return x\n\n\nmcdo=Net_MCDO()\n\nimport torch.optim as optim\nfrom torch.autograd import Variable\nCE = nn.CrossEntropyLoss()\nlearning_rate=0.001\noptimizer=optim.SGD(mcdo.parameters(), lr=learning_rate, momentum=0.9)\nepoch_num = 30\ntrain_accuracies=np.zeros(epoch_num)\ntest_accuracies=np.zeros(epoch_num)\n\n\nfor epoch in range(epoch_num):\n    average_loss = 0.0\n    total=0\n    success=0\n    for i, data in enumerate(trainloader, 0):\n        inputs, labels = data\n        inputs, labels = Variable(inputs), Variable(labels)\n        optimizer.zero_grad()\n        outputs = mcdo.train()(inputs)\n        loss=CE(outputs, labels)\n        loss.backward()\n        optimizer.step()\n        average_loss += loss.item()\n        _,predicted = torch.max(outputs.data, 1)\n        total += labels.size(0)\n        success += (predicted==labels.data).sum()\n    #endfor\n    train_accuracy = 100.0*success\/total\n    succes=0\n    total=0\n    for (inputs, labels) in testloader:\n        inputs, labels = Variable(inputs), Variable(labels)\n        outputs = mcdo.eval()(inputs)\n        _,predicted = torch.max(outputs.data, 1)\n        total += labels.size(0)\n        success += (predicted==labels.data).sum()\n    #endfor\ntest_accuracy = 100.0*success\/total\n    print(u\"epoch{}, average_loss{}, train_accuracy{}, test_accuracy{}\".format(\n          epoch,\n          average_loss\/n_batches,\n          train_accuracy,\n          100*success\/total\n          ))\n    #save\n    train_accuracies[epoch] = train_accuracy\n    test_accuracies[epoch] = 100.0*success\/total\n#end for\n\nplt.plot(np.arange(1, epoch_num+1), train_accuracies)\nplt.plot(np.arange(1, epoch_num+1), test_accuracies)\nplt.show()\n<\/code>","y":"Does the problem persist in both training and testing accuracy?\nI do notice you have made a typo in\n\n\n\n MERAH_Samia:\n\n<code class=\"lang-auto\">succes=0\ntotal=0\nfor (inputs, labels) in testloader:\n<\/code>\n\n\nThat might be the case?","z":"I would most likely guess you are using the variable total in the wrong way. It seems total is to represent the total shape of the label tensor, whereas labels.size(0) it would most likely get you the batch size. Try  total+=labels.nelement() as it would return the total number of elements in the tensor. Does that solve it?\nNooo the problem persists :(((( Thank you\nDoes the problem persist in both training and testing accuracy?\nI do notice you have made a typo in\n\n\n\n MERAH_Samia:\n\n<code class=\"lang-auto\">succes=0\ntotal=0\nfor (inputs, labels) in testloader:\n<\/code>\n\n\nThat might be the case?\nYou have initialized success with  a single s. i.e. the variable name is \u201csucces\u201d, so \u201csuccess\u201d isnt initialized to 0 when you want it that way i guess. Fixing it will help you i think.\nPlease let me know if it worked.\nYeaaaaaaaa, it worked, Thank you so muuuuch\nThe problem was in the variable name  \u201csucces\u201d\nThank youu soo muuuuch"},{"x":"I had to convert my  CLDNN model from Keras to Torch. for work reasons.\nIn order to restore the performance of the model on Keras, the model structure, hy-params,the initialization of weight and bias are still done addcording to my CLDNN model.\nThe current result is that the classifcation result of torch is 3% less than the result on keras. The performance degardation is fatal in terms of my work.\nKeras saves it\u2019s weighted as doubles while PyTorch saves its weights as floats and there is a small amount of truncation error with link \"https:\/\/gereshes.com\/2019\/06\/24\/how-to-transfer-a-simple-keras-model-to-pytorch-the-hard-way\/\"\nAnd my question how to set  save my model\u2019s weights as double rather float?","y":"\n\n\n\nHow can i convert model's parameter data type to Double? with link \"https:\/\/discuss.pytorch.org\/t\/how-can-i-convert-models-parameter-data-type-to-double\/886\"\n\n\n    Default data type of parameters is Float. \nI want to convert it to Double. \nHow can i do this?\n  \n\nHi Gavan,\nCan you verify if this discussion addresses the same issue as yours? In that case I think it has been resolved earlier by ","z":"\n\n\n\nHow can i convert model's parameter data type to Double? with link \"https:\/\/discuss.pytorch.org\/t\/how-can-i-convert-models-parameter-data-type-to-double\/886\"\n\n\n    Default data type of parameters is Float. \nI want to convert it to Double. \nHow can i do this?\n  \n\nHi Gavan,\nCan you verify if this discussion addresses the same issue as yours? In that case I think it has been resolved earlier by \nThank you! I tried the method in the link and it works.\nHowever, the performance of the model has not improve yet. \nIs it possible that you are overlooking some small error? Like I was not calling model.eval() couple days back, and It was giving around 5% more error. Please verify.\nI checked all of my model, including the code of the traning and validation process.\nAnd there is nothing wrong with that. Thank you."},{"x":"Is there a way to re-permute the indices for SubsetRandomSampler at each epoch without re-initialising the DataLoader at each epoch? I am loading a large dataset so do not want to reload it at every epoch.\nmy dataloader function is:\n<code class=\"lang-auto\">def train_dataloader(args):\n    train_dataset_file = \"{0}\/train_data_{1}.hdf5\".format(args.locations[\"train_test_datadir\"],args.region)\n    \ntrain_dataset = data_io.ConcatDataset(\"train\",args.nlevs, train_dataset_file, args.locations['normaliser_loc'], args.batch_size, xvars=args.xvars,\n             yvars=args.yvars, yvars2=args.yvars2, samples_frac=args.samples_fraction, data_frac=args.data_fraction, no_norm=args.no_norm)\n    indices = list(range(train_dataset.__len__()))\n    train_sampler = torch.utils.data.SubsetRandomSampler(indices)\n    train_loader = torch.utils.data.DataLoader(train_dataset, batch_sampler=None, batch_size=None, sampler=train_sampler, shuffle=False)\n    return train_loader\n<\/code>\nAnd I call it as follows:\n<code class=\"lang-auto\">def train_loop(model, loss_function, optimizer, scheduler, args):\n  \n    training_loss = []\n    train_ldr = train_dataloader(args)\n    validation_loss = []\n    test_ldr = test_dataloader(args)\n    \n    for epoch in range(1, args.epochs + 1):\n        ## Training\n        train_loss = 0\n        for batch_idx, batch in enumerate(train_ldr):\n            # Sets the model into training mode\n            # print(batch_idx)\n            model.train()\n<\/code>","y":"Do you want to shuffle the passed indices in the SubsetRandomSampler or would you like to shuffle the global indices again and create new subsets?\nIn the former case the indices would already be reshuffled in each epoch as seen here:\n<code class=\"lang-python\">class MyDataset(Dataset):\n    def __init__(self):\n        pass\n    \n    def __getitem__(self, index):\n        print(index)\n        return torch.randn(1)\n    \n    def __len__(self):\n        return 10\n\n\nindices = torch.arange(10)\nsampler = torch.utils.data.sampler.SubsetRandomSampler(indices)\ndataset = MyDataset()\nloader = DataLoader(dataset, sampler=sampler)\n\nfor epoch in range(2):\n    print(epoch)\n    for _ in loader:\n        pass\n<\/code>","z":"Looking further at this it seems the change here\n\n\ngithub.com\/pytorch\/pytorch with link \"https:\/\/github.com\/pytorch\/pytorch\/commit\/0b90b9cdd36de3131e7b3868c71acc48170f3f1a\"\n\n\n\n\n\n\n\n\nAllow shuffle when auto-batching disabled in DataLoader (#39865) with link \"https:\/\/github.com\/pytorch\/pytorch\/commit\/0b90b9cdd36de3131e7b3868c71acc48170f3f1a\"\n\n\n\n        committed 10:13PM - 11 Jun 20 UTC\n\n\nydaiming with link \"https:\/\/github.com\/ydaiming\"\n\n\n+24\n-11 with link \"https:\/\/github.com\/pytorch\/pytorch\/commit\/0b90b9cdd36de3131e7b3868c71acc48170f3f1a\"\n\n\n\n\n\nSummary:\nFix https:\/\/github.com\/pytorch\/pytorch\/issues\/35761\ncc SsnL\nNote: closed the other PR for this new branch.\nPull Request resolved: https:\/\/github.com\/pytorch\/pytorch\/pull\/39865\nDifferential Revision: D22003612\nPulled By: ezyang\nfbshipit-source-id: 26aecd1b298fe99d3924f4c8157cd6cae2561c7c\n\n\n\n\n\n\nwill allow me to use shuffle=True with batch_size=None and I will not need to use SubsetRandomSampler. I am just wondering if anyone can think of a way I can do this in the mean time? Thanks.\nDo you want to shuffle the passed indices in the SubsetRandomSampler or would you like to shuffle the global indices again and create new subsets?\nIn the former case the indices would already be reshuffled in each epoch as seen here:\n<code class=\"lang-python\">class MyDataset(Dataset):\n    def __init__(self):\n        pass\n    \n    def __getitem__(self, index):\n        print(index)\n        return torch.randn(1)\n    \n    def __len__(self):\n        return 10\n\n\nindices = torch.arange(10)\nsampler = torch.utils.data.sampler.SubsetRandomSampler(indices)\ndataset = MyDataset()\nloader = DataLoader(dataset, sampler=sampler)\n\nfor epoch in range(2):\n    print(epoch)\n    for _ in loader:\n        pass\n<\/code>\nThank you! I misunderstood how SubsetRandomSampler works. I thought it was keeping the same order of indices as it was initialised with and shuffle was required in the DataLoader."},{"x":"I am using torchvision.transforms to transforms a dataset. I am wondering what other methods available to do similar data transforms:\n<code class=\"lang-auto\">transforms = Compose([RandomChoice([functional.hflip,functional.vflip])])\ndataset_train.duplicate_and_transform(transforms) \n<\/code>","y":"The torchvision.transforms docs with link \"https:\/\/pytorch.org\/docs\/stable\/torchvision\/transforms.html\" give you an good overview for all available transformations.","z":"The torchvision.transforms docs with link \"https:\/\/pytorch.org\/docs\/stable\/torchvision\/transforms.html\" give you an good overview for all available transformations.\n Thanks a lot."},{"x":"According to the documentation with link \"https:\/\/pytorch.org\/docs\/stable\/nn.html#linear-layers\" for torch.nn, the default initialization uses a uniform distribution bounded by 1\/sqrt(in_features), but this code with link \"https:\/\/github.com\/pytorch\/pytorch\/blob\/3b1c3996e1c82ca8f43af9efa196b33e36efee37\/torch\/nn\/modules\/linear.py#L79\" appears to show the default initialization as Kaiming uniform. Am I correct in thinking these are not the same thing? And if so, perhaps the documentation can be updated?\nDoes anyone know the motivation for this choice of default? In particular, the code appears to define a negative slope for the subsequent nonlinearity of sqrt(5), whereas the default negative slope for a leaky relu in pytorch is 0.01. Also, does anyone know how this negative slope is actually incorporated into the initialization?","y":"Hi,\nLet me explain it step by step.\n\n\nHere is kaiming_uniform_ with link \"https:\/\/pytorch.org\/docs\/stable\/nn.init.html#torch.nn.init.kaiming_uniform_\".\nimage764\u00d7150 15.6 KB with link \"https:\/\/discuss.pytorch.org\/uploads\/default\/original\/3X\/2\/e\/2e734d7ffa1228e1394abcb8aa3b4db688b6bcc8.png\"\nimage731\u00d7425 8.69 KB with link \"https:\/\/discuss.pytorch.org\/uploads\/default\/original\/3X\/a\/5\/a5811a62ab07cb15bb9419fa1f5c19d59e0ac408.png\"\nWhere negative_slope=sqrt(5) so the gain=sqrt(2\/6)=1\/sqrt(3) for kaiming.\nIf we replace this in bound formula, we get bound = [1\/sqrt(3) ] * [sqrt(3\/ fan_in)] which with a little simplification, it will be bound = 1\/sqrt(fan_in) which can be represented by bound^2 = 1 \/ fan_in.\n\n\nIn linear implementation with link \"https:\/\/pytorch.org\/docs\/stable\/nn.html#torch.nn.Linear\" code you referenced:\n\nimage710\u00d7170 15.9 KB with link \"https:\/\/discuss.pytorch.org\/uploads\/default\/original\/3X\/4\/a\/4ad2915ff2923e2096f20eb067b59d51495d8bb5.png\"\n\n\nSo what we have here is that k= 1\/in_feautres which in case of kaming it can be represented k=1\/fan_in. Also, we want a boundary of [-sqrt(k), sqrt(k)] where k = bound^2= 1 \/ fan_in from step 1.\nFor simplcity, just replace sqrt(5) in gain formula then optain bound in kaiming_uniform_ and replace the bound as k in linear.\nEdit: Add some related posts\n\nhttps:\/\/github.com\/pytorch\/pytorch\/issues\/15314\nWhy the default negative_slope for kaiming_uniform initialization of Convolution and Linear layers is \u221a5? with link \"https:\/\/discuss.pytorch.org\/t\/why-the-default-negative-slope-for-kaiming-uniform-initialization-of-convolution-and-linear-layers-is-5\/29290\"\n\nBests","z":"Hi,\nLet me explain it step by step.\n\n\nHere is kaiming_uniform_ with link \"https:\/\/pytorch.org\/docs\/stable\/nn.init.html#torch.nn.init.kaiming_uniform_\".\nimage764\u00d7150 15.6 KB with link \"https:\/\/discuss.pytorch.org\/uploads\/default\/original\/3X\/2\/e\/2e734d7ffa1228e1394abcb8aa3b4db688b6bcc8.png\"\nimage731\u00d7425 8.69 KB with link \"https:\/\/discuss.pytorch.org\/uploads\/default\/original\/3X\/a\/5\/a5811a62ab07cb15bb9419fa1f5c19d59e0ac408.png\"\nWhere negative_slope=sqrt(5) so the gain=sqrt(2\/6)=1\/sqrt(3) for kaiming.\nIf we replace this in bound formula, we get bound = [1\/sqrt(3) ] * [sqrt(3\/ fan_in)] which with a little simplification, it will be bound = 1\/sqrt(fan_in) which can be represented by bound^2 = 1 \/ fan_in.\n\n\nIn linear implementation with link \"https:\/\/pytorch.org\/docs\/stable\/nn.html#torch.nn.Linear\" code you referenced:\n\nimage710\u00d7170 15.9 KB with link \"https:\/\/discuss.pytorch.org\/uploads\/default\/original\/3X\/4\/a\/4ad2915ff2923e2096f20eb067b59d51495d8bb5.png\"\n\n\nSo what we have here is that k= 1\/in_feautres which in case of kaming it can be represented k=1\/fan_in. Also, we want a boundary of [-sqrt(k), sqrt(k)] where k = bound^2= 1 \/ fan_in from step 1.\nFor simplcity, just replace sqrt(5) in gain formula then optain bound in kaiming_uniform_ and replace the bound as k in linear.\nEdit: Add some related posts\n\nhttps:\/\/github.com\/pytorch\/pytorch\/issues\/15314\nWhy the default negative_slope for kaiming_uniform initialization of Convolution and Linear layers is \u221a5? with link \"https:\/\/discuss.pytorch.org\/t\/why-the-default-negative-slope-for-kaiming-uniform-initialization-of-convolution-and-linear-layers-is-5\/29290\"\n\nBests\nThanks so much for this very thorough explanation. So, if I understand correctly, this achieves what is described in the documentation (parameters drawn from uniform distribution bounded by 1\/sqrt(in_features)), but in a kind of circuitous way. Although this approach uses the init.kaiming_uniform_ function, it is not actually Kaiming initialization (in the scenario where the subsequent nonlinearity is a ReLU). To get Kaiming initialization for a ReLU layer, one would need to re-initialize the weights using init.kaiming_uniform_ with nonlinearity set to \u2018relu\u2019. init Is that correct?\nYou are welcome.\nI think so, actually if you read the paper that introduces kaiming or he, its main advantage is that defines uniform distribution in the way that enables deep NNs with many layers to converge faster, so it is kind of super class of basic uniform as it can be obtained from kaiming_uniform. So, that is why I think if you define a small model using this approach, it won\u2019t hurt the model\u2019s performance and maybe because of this logic, it is the default init method(simplicity purposes without loosing performance).\nBut as you mentioned, logically gain for different activation functions are different and need to be incorporated.\nSo what happened is that there were initializations in Torch7.\nThese have later been expressed as calls to kaiming_uniform.\nBut so there is not as much relation to the ideas of He\u2019s paper as the use of the function might suggest.\nAt some point people thought changing the init with link \"https:\/\/github.com\/pytorch\/pytorch\/issues\/18182\" were a good idea. But it seems that that\u2019s something everyone wants to have done but noone wants to actually do.\nBest regards\nThomas\nHi,\nAs i understand from this, linear layers are initialized by kaiming initialization. This initialization is supposed to give output tensors of mean 0 and std 1.\nHowever, I noticed some contradicting stuff\u2013\n<code class=\"lang-auto\">>>> k=nn.Linear(512,512)\n>>> k1=torch.rand(512)\n>>> out=k(k1)\n>>> print(out.std(),out.mean())\ntensor(0.3505, grad_fn=<StdBackward0>) tensor(0.0181, grad_fn=<MeanBackward0>)\n<\/code>\nNow, when i force kaiming initialization on the layer by \u2013\n<code class=\"lang-auto\"> >>> nn.init.kaiming_uniform_(k.weight,mode='fan_in')\nParameter containing:\ntensor([[-0.0134,  0.0247, -0.0817,  ..., -0.0807, -0.0330, -0.0881],\n        [-0.0641,  0.0447,  0.0645,  ...,  0.1048, -0.0307,  0.0989],\n        [ 0.0745, -0.0076,  0.0161,  ...,  0.0252,  0.0285, -0.0527],\n        ...,\n        [ 0.0190, -0.0529, -0.0549,  ..., -0.0369,  0.0331,  0.0136],\n        [ 0.0347,  0.0516,  0.0108,  ..., -0.0772,  0.0027,  0.0584],\n        [-0.0236,  0.0565,  0.0082,  ...,  0.0717, -0.0619, -0.0772]],\n       requires_grad=True)\n>>> out=k(k1)\n>>> print(out.std(),out.mean())\ntensor(0.8420, grad_fn=<StdBackward0>) tensor(-0.0287, grad_fn=<MeanBackward0>)\n<\/code>\nI get the tensors nearing the actual mean and std values. What am i missing here?\nThanks \n\n\n\n Hmrishav_Bandyopadhy:\n\nWhat am i missing here?\n\n\nSo the thing we discussed above is that while the default init is expressed as kaiming init times some gain factor, it is not kaiming init, as the gain factor is \u201cbogus\u201d w.r.t. kaiming init and the activation function and only serves to reproduce ancient behaviour.\nHappily,  started work on it."},{"x":"Hello.\nI saw the code\n<code class=\"lang-python\">cudnn.fastest = True\n<\/code>\nin the repo. https:\/\/github.com\/zylo117\/Yet-Another-EfficientDet-Pytorch.\nWhat is the meaning of cudnn.fastest?\nThank you for your answer in advance.","y":"It shouldn\u2019t have any effect, as torch.backends.cudnn.fastest is not implemented.\nIn your IDE or here with link \"https:\/\/github.com\/pytorch\/pytorch\/blob\/b7bda236d18815052378c88081f64935427d7716\/torch\/backends\/cudnn\/__init__.py#L86-L93\" you can see, that benchmark, deterministic, and enabled are known flags.\nbenchmark = True will use cudnnFind to profile all available Kernels and select the fastest one.\nThe repository seems to only set this new argument without using it in the code.","z":"It just signals the Pytorch to use the fastest implementation available for operations such as Convolution etc. when enabled, they usually consume more memory (that is cudnn.benchmark and cudnn.fastest)\nIt shouldn\u2019t have any effect, as torch.backends.cudnn.fastest is not implemented.\nIn your IDE or here with link \"https:\/\/github.com\/pytorch\/pytorch\/blob\/b7bda236d18815052378c88081f64935427d7716\/torch\/backends\/cudnn\/__init__.py#L86-L93\" you can see, that benchmark, deterministic, and enabled are known flags.\nbenchmark = True will use cudnnFind to profile all available Kernels and select the fastest one.\nThe repository seems to only set this new argument without using it in the code.\nThank you so much .\nSo it means that torch.backends.cudnn.fastest can be deleted in my coding set. right?\nYou should be able to delete it, as I cannot find any usage of it.\nSince the PyTorch backends doesn\u2019t use this flag, I searched for any usage in the repository, but couldn\u2019t find it either.\nI\u2019m not familiar with the repository, but I couldn\u2019t find any \u201ccustom PyTorch\u201d installation, which would use this flag.\nThank you so much \nAnyway, let me know, if you see any difference after removing the flag.\nAlso,  seems to have seen it before, so it would be interesting where it was used before.\nActually I guess I mistook torch with Pytorch and though this is available here as well (as cudnn.benchmark is!) . This was the case for torch back in the day if I\u2019m not mistaken.\nSince Pytorch released I have only used cudnn.benchmark myself and never used the cudnn.fastest as I remember benchmark handled everything by itself . I have never seen cudnn.fastes in any official examples and chances are those repos that do use cudnn.fastes, do it as a habit comming from torch!"},{"x":"Hi all,\nIs it possible to concat two tensors have different dimensions?\nfor example: if A of shape = [16, 512] and B of shape = [16, 32, 2048]\nHow they could combined to be of shape [16, 544, 2048] ?\nAny help\/suggestion, please?","y":"I\u2019m not sure, how you would like to fill dim2 in your first tensor, but if you just want to repeat the values, this code would work:\n<code class=\"lang-python\">a = torch.randn(16, 512)\nb = torch.randn(16, 32, 2048)\na = a.unsqueeze(2).expand(-1, -1, 2048)\nc = torch.cat((a, b), dim=1)\nprint(c.shape)\n> torch.Size([16, 544, 2048])\n<\/code>","z":"I\u2019m not sure, how you would like to fill dim2 in your first tensor, but if you just want to repeat the values, this code would work:\n<code class=\"lang-python\">a = torch.randn(16, 512)\nb = torch.randn(16, 32, 2048)\na = a.unsqueeze(2).expand(-1, -1, 2048)\nc = torch.cat((a, b), dim=1)\nprint(c.shape)\n> torch.Size([16, 544, 2048])\n<\/code>\nHi Ptrblck,\nI hope you are well. Sorry I need to concatenate two tensors x and y with the size of 64x100x9x9. 64 is batch size, 100 is the number of channel and 9x9x is the width and height.\nI want to have concatenated result in 64x100x18x18 form\nI used torch.cat[x,y],dim??)\nYou won\u2019t be able to create the output as [64, 100, 18, 18], since this would double the number of all available elements.\nHowever, you could create a tensor in the shape [64, 100, 18, 9] or [64, 100, 9, 18] via:\n<code class=\"lang-python\">x = torch.randn(64, 100, 9, 9)\ny = torch.randn(64, 100, 9, 9)\n\nprint(torch.cat((x, y), dim=2).shape)\nprint(torch.cat((x, y), dim=3).shape)\n<\/code>"},{"x":"Hi guys,\nWhen I use Dataloader to load data with num_worker>0, I got the following error:\n<code class=\"lang-auto\">Exception in thread Thread-1:\nTraceback (most recent call last):\n  File \"\/home\/Maro\/anaconda3\/envs\/pytorch\/lib\/python3.5\/threading.py\", line 914, in _bootstrap_inner\n    self.run()\n  File \"\/home\/Maro\/anaconda3\/envs\/pytorch\/lib\/python3.5\/threading.py\", line 862, in run\n    self._target(*self._args, **self._kwargs)\n  File \"\/home\/Maro\/anaconda3\/envs\/pytorch\/lib\/python3.5\/multiprocessing\/resource_sharer.py\", line 139, in _serve\n    signal.pthread_sigmask(signal.SIG_BLOCK, range(1, signal.NSIG))\n  File \"\/home\/Maro\/anaconda3\/envs\/pytorch\/lib\/python3.5\/signal.py\", line 60, in pthread_sigmask\n    sigs_set = _signal.pthread_sigmask(how, mask)\nValueError: signal number 32 out of range\n\n<\/code>\nAnd I must set num_workers to 0, but it cost too much time in loading data in each iteration.\nIs there any good way to solve this problem? Any advice will be highly appreciated.\nThanks in advance.","y":"I\u2019ve never seen this error on my machine, but searching for it it looks like Python 3.7 could fix this issue.\nWould it be possible for you to create a new conda environment and try the code with Python 3.7?\nIf not, could you post a code snippet so that we can reproduce this error?","z":"Does the error related to the os configuration?\nI have run the same code on two server with the same os (Ubuntu16.04.1) but different gpu. And set num_workers=12 in DataLoader for both. One can run normally but another raise the ValueError mentioned above.\nI\u2019ve never seen this error on my machine, but searching for it it looks like Python 3.7 could fix this issue.\nWould it be possible for you to create a new conda environment and try the code with Python 3.7?\nIf not, could you post a code snippet so that we can reproduce this error?\nOh,  thank you very much, your help was greatly appreciated!\nIt resolved by updating Python3.5->Python3.7.https:\/\/bugs.python.org\/issue33329\nAnother question, though it does not relate to this topic:\nIf I have two cards ( card0 and card1 ) and train my network just on card1, when I set pin_memory=True in DataLoader it will occupy a portion of card0\u2019s memory ( it seems to be the default  ), is there any way to assign memory consumption to card1?\nThanks a lot.\nPS: In another machine it works fine with Python3.5, it is so strange and I dont know what makes the mistake.\nI assume the little memory will be allocated on GPU0.\nIf that\u2019s the case, you can run your script using:\n<code class=\"lang-auto\">CUDA_VISIBLE_DEVICES=1 python script.py args\n<\/code>\nto hide GPU0 in your script. GPU1 will then be remapped to GPU0, so that you would have to use 'cuda:0'or just 'cuda' in your script.\nWhich PyTorch version are you using btw? I thought this issue was solved recently.\nI haven\u2019t seen this issue before, but apparently it\u2019s related to some multiprocessing functions in Python.\nThanks a lot.\nI will have a try on it.\nMy PyTorch version is 1.0.1.post2, and on another machine is 1.0.0 which worked fine before.\nYes  , it is        a   python  bug  , due  to  this  isseu    https:\/\/bugs.python.org\/issue33329 ,  it got fixed in 3.6.6\nI get this error when running python 3.7.7"},{"x":"First I use a pytorch pretrained Resnet, then I use these codes to get the hidden feature.\n<code class=\"lang-auto\">feat_out = []\n\ndef hook_fn_forward(module, input, output):\n    feat_out.append(output)\n    print(output)\n\nmodules = model.named_children()\nfor name, module in modules:\n    module.register_forward_hook(hook_fn_forward)\n\npred = model(x)\n<\/code>\nBut when I run these codes the first time, len(feat_out) gives me 10, and print in hook function prints 10 lines. If I run these codes again, len(feat_out) gives me 20, and print in hook function prints 20 lines. Every time I run, the length of output in hook function increase by 1. The output is output in this time plus all past output. Only if I reinitialize the model and run these codes, the past output history will be removed.\nHow can I clear the output every time I run the model?\nI use these codes in colab with link \"https:\/\/colab.research.google.com\/drive\/1pzVPBkZiHCzu7MiIcEm_gLcMqXmbUpLY?usp=sharing\" to reproduce this problem in minimum length (5 lines to load data, 2 lines to initialize model, 8 lines for this problem).","y":"Also, from one of your past answer, I found that I didn\u2019t remove the hooks, which essencially caused the problems.\n<code class=\"lang-auto\">total_feat_out = []\n\ndef hook_fn_forward(module, input, output):\n    total_feat_out.append(output) \n    print(output.shape)\n\nmodules = model_ft.named_children()\nhandles = {}\nfor name, module in modules:\n    handles[name] = module.register_forward_hook(hook_fn_forward)\n\nmodel_ft.eval() \nwith torch.no_grad():\n    pred = model_ft(dat)\n    for k, v in handles.items():\n        handles[k].remove()\n``<\/code>","z":"Since you are initializing the feat_out list as a global object, this behavior is expected.\nYou could either reinitialize it after the first forward pass or use a dict instead, which can be easily used to replace the outputs.\nAlso note, if you are not running the forward pass in a with torch.no_grad() block, the output tensors will stay attached to the computation graph, which will prevent PyTorch from deleting this graph after the backward call. If you don\u2019t need these output tensors for a gradient calculation you could alternatively store the detached version via feat_out.append(output.detach()).\nThese are my full code, in case you don\u2019t want to go to Colab.\n<code class=\"lang-auto\">!wget https:\/\/download.pytorch.org\/tutorial\/hymenoptera_data.zip\n!unzip -q hymenoptera_data.zip\n\ndata_transforms = transforms.Compose([\n                             transforms.RandomResizedCrop(224),\n                             transforms.ToTensor()])\ndata_dir = 'hymenoptera_data\/\/train'\nimage_datasets = datasets.ImageFolder(os.path.join(data_dir), transform=data_transforms)\ndataloaders = torch.utils.data.DataLoader(image_datasets, batch_size=4, num_workers=4)\n\nmodel_ft = models.resnet18(pretrained=True)\nmodel_ft.fc = nn.Linear(model_ft.fc.in_features, 2) #  size of each output is set to 2\n\ndat, lab = next(iter(dataloaders))\n\n\ntotal_feat_out = []\n\ndef hook_fn_forward(module, input, output):\n    total_feat_out.append(output) \n    print(output.shape)\n\nmodules = model_ft.named_children()\nfor name, module in modules:\n    module.register_forward_hook(hook_fn_forward)\n\nmodel_ft.eval() \nwith torch.no_grad():\n    pred = model_ft(dat)\n<\/code>\nThank you for your attention. This problem also make output in hook function increase, every time it runs, it increases by 1. So the  print  in hook function prints 10 lines more.\nYou are still appending to the global list, so the list will grow in each forward pass.\nTo store only the activation of the last forward pass you would need to reinitialize the list or clear it in any other way:\n<code class=\"lang-python\">with torch.no_grad():\n    out = model_ft(torch.randn(1, 3, 224, 224))\n    print(len(total_feat_out)) # prints 10\n    total_feat_out = [] # reinitialize list to delete old activations\n<\/code>\nThank you very much. I finally get what I want!!\nI avoid this problem by using  dict  as you told me to replace the outputs.\n<code class=\"lang-auto\">model_ft = models.resnet18(pretrained=True)\n\nfeature_out = {}\nlayers_name = list(model_ft._modules.keys())\nlayers = list(model_ft._modules.values())\n\n\ndef hook_fn_forward(module, input, output):\n    layer = layers_name[np.argwhere([module == m for m in layers])[0, 0]]\n    total_feat_out[layer] = output\n    \n\nmodules = model_ft.named_children()\nfor name, module in modules:\n    module.register_forward_hook(hook_fn_forward)\n\nmodel_ft.eval() \nwith torch.no_grad():\n    pred = model_ft(dat)\n<\/code>\nAlso, from one of your past answer, I found that I didn\u2019t remove the hooks, which essencially caused the problems.\n<code class=\"lang-auto\">total_feat_out = []\n\ndef hook_fn_forward(module, input, output):\n    total_feat_out.append(output) \n    print(output.shape)\n\nmodules = model_ft.named_children()\nhandles = {}\nfor name, module in modules:\n    handles[name] = module.register_forward_hook(hook_fn_forward)\n\nmodel_ft.eval() \nwith torch.no_grad():\n    pred = model_ft(dat)\n    for k, v in handles.items():\n        handles[k].remove()\n``<\/code>"},{"x":"Hi,I realize that I have no knowledge of how gradient values we get after calling backward.\nfor example :\n<code class=\"lang-auto\">x = torch.randn((4,5),requires_grad = True)\nz = x.mean()\nz.backward()\n#print(x.grad)\n<\/code>\nAssume I have the values of x\n<code class=\"lang-auto\">>>> x\ntensor([[-0.3571,  0.1481,  0.1713, -1.2597, -0.7667],\n        [-0.1553, -0.9620,  0.0103,  3.3494,  0.2220],\n        [ 2.1131, -0.2404,  0.4820,  0.3816,  1.9752],\n        [ 1.7232, -0.5064, -0.8151,  0.3720,  0.1470]], requires_grad=True)\n>>> \n<\/code>\nAnd the result of x.grad returns\n<code class=\"lang-auto\">tensor([[1.1500, 1.1500, 1.1500, 1.1500, 1.1500],\n        [1.1500, 1.1500, 1.1500, 1.1500, 1.1500],\n        [1.1500, 1.1500, 1.1500, 1.1500, 1.1500],\n        [1.1500, 1.1500, 1.1500, 1.1500, 1.1500]])\n\n<\/code>\nAnd if I change  z = x.mean()  to z = x.sum()  , x.grad becomes\n<code class=\"lang-auto\">tensor([[2.1500, 2.1500, 2.1500, 2.1500, 2.1500],\n                [2.1500, 2.1500, 2.1500, 2.1500, 2.1500],\n                [2.1500, 2.1500, 2.1500, 2.1500, 2.1500],\n                [2.1500, 2.1500, 2.1500, 2.1500, 2.1500]])\n<\/code>\nI would like to know how do we compute and get the values both 1.1500 and 2.1500\nThanks in advance !","y":"Okay, I can help you a bit there. Gradient is a \u201cfancy\u201d name for derivative (differential calculus). so lets say you have a tensor (2*2) defined as:\nW11 W12\nW21 W22\nWhen you take torch.mean(), the returned variable say W = (W11 + W21 + W12 + W22)\/4.0\nAnd hence the gradient matrix shall look like,\nd(W)\/d(W11) d(W)\/d(W12)\nd(W)\/d(W21) d(W)\/d(W22)\nWhich is;\n.25 .25\n.25 .25\nI think you can go through the PRML book by chris bishop and gain further insights.\nhope that helped.","z":"Hi Two!\n\n\n\n two_Two:\n\n<code class=\"lang-auto\">x = torch.randn((4,5),requires_grad = True)\nz = x.mean()\nz.backward()\n#print(x.grad)\n<\/code>\n\u2026\nAnd the result of x.grad returns\n<code class=\"lang-auto\">tensor([[1.1500, 1.1500, 1.1500, 1.1500, 1.1500],\n        [1.1500, 1.1500, 1.1500, 1.1500, 1.1500],\n        [1.1500, 1.1500, 1.1500, 1.1500, 1.1500],\n        [1.1500, 1.1500, 1.1500, 1.1500, 1.1500]])\n<\/code>\n\u2026\nI would like to know how do we compute and get the values both 1.1500 and 2.1500\n\n\nThe gradients you report are obviously wrong.  I cannot reproduce this\nwith a pytorch-version-0.3.0 test script:\n<code class=\"lang-python\">import torch\ntorch.__version__\n\ntorch.manual_seed (2020)\n\nw1 = torch.randn (4,5)\nw2 = w1.clone()\n\nx1 = torch.autograd.Variable (w1, requires_grad = True)\nx1\nz1 = x1.mean()\nz1.backward()\nx1.grad\n\nx2 = torch.autograd.Variable (w2, requires_grad = True)\nx2\nz2 = x2.sum()\nz2.backward()\nx2.grad\n<\/code>\nHere is the output showing the correct gradients:\n<code class=\"lang-nohighlight\">>>> import torch\n>>> torch.__version__\n'0.3.0b0+591e73e'\n>>>\n>>> torch.manual_seed (2020)\n<torch._C.Generator object at 0x000001E2AF8B6630>\n>>>\n>>> w1 = torch.randn (4,5)\n>>> w2 = w1.clone()\n>>>\n>>> x1 = torch.autograd.Variable (w1, requires_grad = True)\n>>> x1\nVariable containing:\n 1.2372 -0.9604  1.5415 -0.4079  0.8806\n 0.0529  0.0751  0.4777 -0.6759 -2.1489\n-1.1463 -0.2720  1.0066 -0.0416 -1.2853\n-0.4948 -1.2964 -1.2502 -0.7693  1.6856\n[torch.FloatTensor of size 4x5]\n\n>>> z1 = x1.mean()\n>>> z1.backward()\n>>> x1.grad\nVariable containing:\n1.00000e-02 *\n  5.0000  5.0000  5.0000  5.0000  5.0000\n  5.0000  5.0000  5.0000  5.0000  5.0000\n  5.0000  5.0000  5.0000  5.0000  5.0000\n  5.0000  5.0000  5.0000  5.0000  5.0000\n[torch.FloatTensor of size 4x5]\n\n>>>\n>>> x2 = torch.autograd.Variable (w2, requires_grad = True)\n>>> x2\nVariable containing:\n 1.2372 -0.9604  1.5415 -0.4079  0.8806\n 0.0529  0.0751  0.4777 -0.6759 -2.1489\n-1.1463 -0.2720  1.0066 -0.0416 -1.2853\n-0.4948 -1.2964 -1.2502 -0.7693  1.6856\n[torch.FloatTensor of size 4x5]\n\n>>> z2 = x2.sum()\n>>> z2.backward()\n>>> x2.grad\nVariable containing:\n 1  1  1  1  1\n 1  1  1  1  1\n 1  1  1  1  1\n 1  1  1  1  1\n[torch.FloatTensor of size 4x5]\n<\/code>\nCould you post a complete, runnable script that reproduces your issue\nand let us know what version of pytorch you are running?\nBest.\nK. Frank\nIs the question \u201cHow are the gradients computed numerically?\u201d ?\nDear Frank \nOh,that\u2019s so terrible,I would like to re-post this again and sorry about wasting your time\u2026\n<code class=\"lang-auto\">>>> import torch\n>>> torch.__version__\n'1.5.1+cu101'\n>>> x = torch.tensor([[-0.3571,  0.1481,  0.1713, -1.2597, -0.7667],\n...         [-0.1553, -0.9620,  0.0103,  3.3494,  0.2220],\n...         [ 2.1131, -0.2404,  0.4820,  0.3816,  1.9752],\n...         [ 1.7232, -0.5064, -0.8151,  0.3720,  0.1470]], requires_grad=True)\n>>> z = x.mean()\n>>> z.backward()\n>>> x.grad\ntensor([[0.0500, 0.0500, 0.0500, 0.0500, 0.0500],\n       [0.0500, 0.0500, 0.0500, 0.0500, 0.0500],\n       [0.0500, 0.0500, 0.0500, 0.0500, 0.0500],\n       [0.0500, 0.0500, 0.0500, 0.0500, 0.0500]])\n\n<\/code>\nAnd another one would be :\n<code class=\"lang-auto\">>>> import torch \n>>> torch.__version__\n'1.5.1+cu101'\n>>> x = torch.tensor([[-0.3571,  0.1481,  0.1713, -1.2597, -0.7667],\n...         [-0.1553, -0.9620,  0.0103,  3.3494,  0.2220],\n...         [ 2.1131, -0.2404,  0.4820,  0.3816,  1.9752],\n...         [ 1.7232, -0.5064, -0.8151,  0.3720,  0.1470]], requires_grad=True)\n>>> z = x.sum()\n>>> z.backward()\n>>> x.grad\ntensor([[1., 1., 1., 1., 1.],\n        [1., 1., 1., 1., 1.],\n        [1., 1., 1., 1., 1.],\n        [1., 1., 1., 1., 1.]])\n<\/code>\nAnd once again,Thanks in advance ! !\nYeah,It could be haha,I was struggling how to give this post a great title,and it looks like I fail\u2026\nOkay, I can help you a bit there. Gradient is a \u201cfancy\u201d name for derivative (differential calculus). so lets say you have a tensor (2*2) defined as:\nW11 W12\nW21 W22\nWhen you take torch.mean(), the returned variable say W = (W11 + W21 + W12 + W22)\/4.0\nAnd hence the gradient matrix shall look like,\nd(W)\/d(W11) d(W)\/d(W12)\nd(W)\/d(W21) d(W)\/d(W22)\nWhich is;\n.25 .25\n.25 .25\nI think you can go through the PRML book by chris bishop and gain further insights.\nhope that helped.\nSure ,why could not I figure this out \u2026\nthinks of every element in tensor is a independent variables.and do basic calculus thing\ny=x1+x2\ndy \/ dx1 =1\nif y=(x1+x2)\/2\ndy\/dx1 = 0.5\nAnd thank you very much!"},{"x":"Hi~\nFor sigmoid, there\u2019s torch.nn.Sigmoid with link \"https:\/\/pytorch.org\/docs\/stable\/nn.html?highlight=sigmoid#torch.nn.Sigmoid\" to generate a instance of Sigmoid class, and torch.nn.functional.sigmoid with link \"https:\/\/pytorch.org\/docs\/stable\/nn.functional.html?highlight=sigmoid#torch.nn.functional.sigmoid\" function. I noticed another torch.sigmoid with link \"https:\/\/pytorch.org\/docs\/stable\/torch.html?highlight=sigmoid#torch.sigmoid\" function and some related topics (link1 with link \"https:\/\/discuss.pytorch.org\/t\/torch-nn-sigmoid-vs-torch-sigmoid\/57691\", link2 with link \"https:\/\/discuss.pytorch.org\/t\/is-there-any-different-between-torch-sigmoid-and-torch-nn-functional-sigmoid\/995\").\nHowever, for softmax function for example, there\u2019s only torch.nn.Softmax with link \"https:\/\/pytorch.org\/docs\/stable\/nn.html?highlight=softmax#torch.nn.Softmax\" and torch.nn.functional.softmax with link \"https:\/\/pytorch.org\/docs\/stable\/nn.functional.html?highlight=softmax#torch.nn.functional.softmax\", but no torch.softmax. I am confused and want to know what is the thinkings behind designing these. And are there other functions designed like this?\nThanks!","y":"So the idea is to put more deep-learning-oriented functions in torch.nn.functional and keep general-purpose functions in under torch directly. softmax was deemed to fall into the former, sigmoid in the latter category.\nWhile there is torch.softmax, this is by accident (which is why it is not documented) rather than as a design (previous version of PyTorch didn\u2019t have the fancy torch._C._nn module to put the C++-implementations of torch.nn.functional-functions). I would advise to only use the documented variants to stay out of trouble should someone start to clean up.\nBest regards\nThomas","z":"So the idea is to put more deep-learning-oriented functions in torch.nn.functional and keep general-purpose functions in under torch directly. softmax was deemed to fall into the former, sigmoid in the latter category.\nWhile there is torch.softmax, this is by accident (which is why it is not documented) rather than as a design (previous version of PyTorch didn\u2019t have the fancy torch._C._nn module to put the C++-implementations of torch.nn.functional-functions). I would advise to only use the documented variants to stay out of trouble should someone start to clean up.\nBest regards\nThomas\nThank you very much!"},{"x":"When I use a long tensor to index embedding, it loses its grad_fn:\n<code class=\"lang-auto\">embed_option = nn.Embedding(4, 10)\nvec = embed_option(torch.LongTensor([[0,1,2]]))\n<\/code>\nvec.requires_grad is Flase.\nWhen I tried in main scope, everything worked fine. However, if I define this in a nn.Module class, it loses its gradient function. Any idea how this could happen?","y":"fk me. I wrapped it in with torch.no_grad() in caller function.","z":"fk me. I wrapped it in with torch.no_grad() in caller function."},{"x":"Can I use \u2018torch.cuda.amp.autocast\u2019 with \u2018torch.einsum\u2019 ? Will this work ?","y":"since einsum is a fairly dynamic op, i think amp.autocast might keep it in the \u201csafe\u201d region and do the computation in fp32.\nIt should be quick to try and find out.","z":"since einsum is a fairly dynamic op, i think amp.autocast might keep it in the \u201csafe\u201d region and do the computation in fp32.\nIt should be quick to try and find out."},{"x":"I have input vectors with different size: [1,0,1], [1,0,1,1,1,1]\u2026\nIs there a way to add additional NN layer that Pads vectors on the right side with 2?\n<code class=\"lang-auto\">[1,0,1] -> [1,0,1,2,2,2,2,2] \n[1,0,1,1,1,1] ->  [1,0,1,1,1,1,2,2]\n<\/code>\n<code class=\"lang-auto\">MAX_STRING_SIZE = 8\nmodel = torch.nn.Sequential(\n    torch.nn.Linear(MAX_STRING_SIZE, 1),\n    torch.nn.Sigmoid()\n)\n<\/code>","y":"torch.nn.functional. pad ( input ,  pad ,  mode=\u2018constant\u2019 ,  value=2 )\nyou can also try\n<code class=\"lang-auto\">tensor1 = torch.Tensor([1,0,2])\nmax_seq_len = 7\ntensor2 = torch.ones(max_seq_len)*2\ntensor1 = torch.cat((tensor1,tensor2))[:max_seq_len]\n<\/code>\nchange the max_seq_len to get padding till different sizes","z":"torch.nn.functional. pad ( input ,  pad ,  mode=\u2018constant\u2019 ,  value=2 )\nyou can also try\n<code class=\"lang-auto\">tensor1 = torch.Tensor([1,0,2])\nmax_seq_len = 7\ntensor2 = torch.ones(max_seq_len)*2\ntensor1 = torch.cat((tensor1,tensor2))[:max_seq_len]\n<\/code>\nchange the max_seq_len to get padding till different sizes\nHow to use it with batches?\ntensor1 = torch.Tensor([[1,0,2],[1,0,2,1,0,2]])"},{"x":"I am trying to adapt the tutorial on a DenseNet from https:\/\/goosemi.wordpress.com\/2018\/05\/30\/first-blog-post\/ to the Fashion MNIST dataset which can be downloaded from https:\/\/github.com\/zalandoresearch\/fashion-mnist\nThe formats of MNIST images of hand-written digits and fashion items seem to be the same, i.e. these are 28x28 greyscale images. However, something does not match up, as the code from the tutorial throws an error. Does anyone has a clue where the problem(s) in the code is(are)?\nBelow is the code. First, I import the necessary modules\/classes.\n<code class=\"lang-auto\">import torch\nimport torch.nn as nn\nimport torchvision\nimport torchvision.transforms as transforms\nfrom torch.utils.data import DataLoader\nimport torch.optim as optim\nimport torch.nn.functional as F\n<\/code>\nThen I import the dataset (and apply normalization).\n<code class=\"lang-auto\">mean = 0.2860347330570221\nstd = 0.3530242443084717\n\n# Normalised train set\ntrain_set_normal = torchvision.datasets.FashionMNIST(\n    root='.\/data'\n    ,train=True\n    ,download=True\n    ,transform=transforms.Compose([\n          transforms.ToTensor()\n        , transforms.Normalize(mean, std)\n    ])\n)\n<\/code>\nThen there I define the DenseNet architecture as it is defined in the tutorial.\n<code class=\"lang-auto\">class Dense_Block(nn.Module):\n    def __init__(self, in_channels):\n        super(Dense_Block, self).__init__()\n\n        self.relu = nn.ReLU(inplace = True)\n        self.bn = nn.BatchNorm2d(num_features = in_channels)\n\n        self.conv1 = nn.Conv2d(in_channels = in_channels, out_channels = 32, kernel_size = 3, stride = 1, padding = 1)\n        self.conv2 = nn.Conv2d(in_channels = 32, out_channels = 32, kernel_size = 3, stride = 1, padding = 1)\n        self.conv3 = nn.Conv2d(in_channels = 64, out_channels = 32, kernel_size = 3, stride = 1, padding = 1)\n        self.conv4 = nn.Conv2d(in_channels = 96, out_channels = 32, kernel_size = 3, stride = 1, padding = 1)\n        self.conv5 = nn.Conv2d(in_channels = 128, out_channels = 32, kernel_size = 3, stride = 1, padding = 1)\n\n    def forward(self, x):\n\n        bn = self.bn(x)\n        conv1 = self.relu(self.conv1(bn))\n\n        conv2 = self.relu(self.conv2(conv1))\n        c2_dense = self.relu(torch.cat([conv1, conv2], 1))\n\n        conv3 = self.relu(self.conv3(c2_dense))\n        c3_dense = self.relu(torch.cat([conv1, conv2, conv3], 1))\n\n        conv4 = self.relu(self.conv4(c3_dense))\n        c4_dense = self.relu(torch.cat([conv1, conv2, conv3, conv4], 1))\n\n        conv5 = self.relu(self.conv5(c4_dense))\n        c5_dense = self.relu(torch.cat([conv1, conv2, conv3, conv4, conv5], 1))\n\n        return c5_dense\n\nclass Transition_Layer(nn.Module):\n    def __init__(self, in_channels, out_channels):\n        super(Transition_Layer, self).__init__()\n\n        self.relu = nn.ReLU(inplace = True)\n        self.bn = nn.BatchNorm2d(num_features = out_channels)\n        self.conv = nn.Conv2d(in_channels = in_channels, out_channels = out_channels, kernel_size = 1, bias = False)\n        self.avg_pool = nn.AvgPool2d(kernel_size = 2, stride = 2, padding = 0)\n\n    def forward(self, x):\n\n        bn = self.bn(self.relu(self.conv(x)))\n        out = self.avg_pool(bn)\n\n        return out\n\nclass DenseNet(nn.Module):\n    def __init__(self, nr_classes):\n        super(DenseNet, self).__init__()\n\n        self.lowconv = nn.Conv2d(in_channels = 3, out_channels = 64, kernel_size = 7, padding = 3, bias = False)\n        self.relu = nn.ReLU()\n\n        # Make Dense Blocks\n        self.denseblock1 = self._make_dense_block(Dense_Block, 64)\n        self.denseblock2 = self._make_dense_block(Dense_Block, 128)\n        self.denseblock3 = self._make_dense_block(Dense_Block, 128)\n\n        # Make transition Layers\n        self.transitionLayer1 = self._make_transition_layer(Transition_Layer, in_channels = 160, out_channels = 128)\n        self.transitionLayer2 = self._make_transition_layer(Transition_Layer, in_channels = 160, out_channels = 128)\n        self.transitionLayer3 = self._make_transition_layer(Transition_Layer, in_channels = 160, out_channels = 64)\n\n        # Classifier\n        self.bn = nn.BatchNorm2d(num_features = 64)\n        self.pre_classifier = nn.Linear(64*4*4, 512)\n        self.classifier = nn.Linear(512, nr_classes)\n\n    def _make_dense_block(self, block, in_channels):\n        layers = []\n        layers.append(block(in_channels))\n        return nn.Sequential(*layers)\n\n    def _make_transition_layer(self, layer, in_channels, out_channels):\n        modules = []\n        modules.append(layer(in_channels, out_channels))\n        return nn.Sequential(*modules)\n\n    def forward(self, x):\n        out = self.relu(self.lowconv(x))\n\n        out = self.denseblock1(out)\n        out = self.transitionLayer1(out)\n\n        out = self.denseblock2(out)\n        out = self.transitionLayer2(out)\n\n        out = self.denseblock3(out)\n        out = self.transitionLayer3(out)\n\n        out = self.bn(out)\n        out = out.view(-1, 64*4*4)\n\n        out = self.pre_classifier(out)\n        out = self.classifier(out)\n\n        return out\n<\/code>\nFinally, I am trying to train the network.\n<code class=\"lang-auto\">dense_network = DenseNet(nr_classes=10)\n\nloader = DataLoader(train_set_normal,batch_size=10,shuffle=True,num_workers=0)\noptimizer = optim.Adam(dense_network.parameters(), lr=0.01)\n\nN_epochs = 10\nfor epoch in range(N_epochs):\n    \n    total_loss = 0\n    total_correct = 0\n    \n    for batch in loader: # Get Batch\n\n        images, labels = batch\n\n        preds = dense_network(images) # Pass Batch\n\n        loss = F.cross_entropy(preds, labels)           \n\n        optimizer.zero_grad()\n        loss.backward() # Calculate Gradients\n        optimizer.step() # Update Weights\n        total_loss += loss.item()\n        total_correct += get_num_correct(preds, labels)\n        \n    print(\n    \"epoch\", epoch, \n    \"total_correct:\", total_correct, \n    \"loss:\", total_loss\n    )\n<\/code>\nAs a result, I see the following error-message.\n<code class=\"lang-auto\">RuntimeError                              Traceback (most recent call last)\n<ipython-input-4-84df8dc8fb2f> in <module>\n     14         images, labels = batch\n     15 \n---> 16         preds = dense_network(images) # Pass Batch\n     17 \n     18         loss = F.cross_entropy(preds, labels)\n\n~\\anaconda3\\lib\\site-packages\\torch\\nn\\modules\\module.py in __call__(self, *input, **kwargs)\n    548             result = self._slow_forward(*input, **kwargs)\n    549         else:\n--> 550             result = self.forward(*input, **kwargs)\n    551         for hook in self._forward_hooks.values():\n    552             hook_result = hook(self, input, result)\n\n<ipython-input-3-417f3f93841b> in forward(self, x)\n     80 \n     81     def forward(self, x):\n---> 82         out = self.relu(self.lowconv(x))\n     83 \n     84         out = self.denseblock1(out)\n\n~\\anaconda3\\lib\\site-packages\\torch\\nn\\modules\\module.py in __call__(self, *input, **kwargs)\n    548             result = self._slow_forward(*input, **kwargs)\n    549         else:\n--> 550             result = self.forward(*input, **kwargs)\n    551         for hook in self._forward_hooks.values():\n    552             hook_result = hook(self, input, result)\n\n~\\anaconda3\\lib\\site-packages\\torch\\nn\\modules\\conv.py in forward(self, input)\n    351 \n    352     def forward(self, input):\n--> 353         return self._conv_forward(input, self.weight)\n    354 \n    355 class Conv3d(_ConvNd):\n\n~\\anaconda3\\lib\\site-packages\\torch\\nn\\modules\\conv.py in _conv_forward(self, input, weight)\n    348                             _pair(0), self.dilation, self.groups)\n    349         return F.conv2d(input, weight, self.bias, self.stride,\n--> 350                         self.padding, self.dilation, self.groups)\n    351 \n    352     def forward(self, input):\n\nRuntimeError: Given groups=1, weight of size [64, 3, 7, 7], expected input[10, 1, 28, 28] to have 3 channels, but got 1 channels instead\n<\/code>","y":"\n\n\n Capo_Mestre:\n\nself.pre_classifier = nn.Linear(64*4*4, 512)\n\n\nIf you trace the stacktrace, it ends up in this line which as you might guess size is different from value in .view.\nYou can find these type of issues easily by tracing the stacktrace from last line to first. Then error message usually is common in many different posts, so you can the idea of error.","z":"Hi,\n\n\n\n Capo_Mestre:\n\ngreyscale images\n\n\nGrayscale images have 1 channel but in the code for self.lowconv, you set in_channel=3.\n\n\n\n Capo_Mestre:\n\nself.lowconv = nn.Conv2d(in_channels = 3, out_channels = 64, kernel_size = 7, padding = 3, bias = False)\n\n\nThe error says:\n\n\n\n Capo_Mestre:\n\n<code class=\"lang-auto\">RuntimeError: Given groups=1, weight of size [64, 3, 7, 7], expected input[10, 1, 28, 28] to have 3 channels, but got 1 channels instead\n<\/code>\n\n\nWhich means you have a batch of 10 images of size [1, 28, 28], but you are trying to use 64 filters of size [3, 7, 7] which cannot match the channel size in input.\nBests\nThanks a lot for your reply! I have corrected in_channels = 3 to in_channels = 1 and there is another error:\n<code class=\"lang-auto\">---------------------------------------------------------------------------\nRuntimeError                              Traceback (most recent call last)\n<ipython-input-6-84df8dc8fb2f> in <module>\n     14         images, labels = batch\n     15 \n---> 16         preds = dense_network(images) # Pass Batch\n     17 \n     18         loss = F.cross_entropy(preds, labels)\n\n~\\anaconda3\\lib\\site-packages\\torch\\nn\\modules\\module.py in __call__(self, *input, **kwargs)\n    548             result = self._slow_forward(*input, **kwargs)\n    549         else:\n--> 550             result = self.forward(*input, **kwargs)\n    551         for hook in self._forward_hooks.values():\n    552             hook_result = hook(self, input, result)\n\n<ipython-input-5-d099d05e804c> in forward(self, x)\n     92 \n     93         out = self.bn(out)\n---> 94         out = out.view(-1, 64*4*4)\n     95 \n     96         out = self.pre_classifier(out)\n\nRuntimeError: shape '[-1, 1024]' is invalid for input of size 5760\n<\/code>\n\n\n\n Capo_Mestre:\n\nout = self.bn(out)\n\n\nIt means, out in this line has a shape that is different from [64, 4, 4] that is why you getting size mismatch error.\nTry to put a print(out) right after the aforementioned line and then use the sizes for out.view(-1, <printed_size>).\nThanks   a lot for looking into the code.\nI have added the line print(out.shape) right after the line out = self.bn(out) and got the following output in front of the error:\n<code class=\"lang-auto\">torch.Size([10, 64, 3, 3])\n<\/code>\nThen I changed from out = out.view(-1, 64*4*4) to out = out.view(-1, 64*3*3) and the error message now reads as\n<code class=\"lang-auto\">---------------------------------------------------------------------------\nRuntimeError                              Traceback (most recent call last)\n<ipython-input-23-84df8dc8fb2f> in <module>\n     14         images, labels = batch\n     15 \n---> 16         preds = dense_network(images) # Pass Batch\n     17 \n     18         loss = F.cross_entropy(preds, labels)\n\n~\\anaconda3\\lib\\site-packages\\torch\\nn\\modules\\module.py in __call__(self, *input, **kwargs)\n    548             result = self._slow_forward(*input, **kwargs)\n    549         else:\n--> 550             result = self.forward(*input, **kwargs)\n    551         for hook in self._forward_hooks.values():\n    552             hook_result = hook(self, input, result)\n\n<ipython-input-22-b8522ba5d926> in forward(self, x)\n     95         out = out.reshape(-1, 64*3*3)\n     96 \n---> 97         out = self.pre_classifier(out)\n     98         out = self.classifier(out)\n     99 \n\n~\\anaconda3\\lib\\site-packages\\torch\\nn\\modules\\module.py in __call__(self, *input, **kwargs)\n    548             result = self._slow_forward(*input, **kwargs)\n    549         else:\n--> 550             result = self.forward(*input, **kwargs)\n    551         for hook in self._forward_hooks.values():\n    552             hook_result = hook(self, input, result)\n\n~\\anaconda3\\lib\\site-packages\\torch\\nn\\modules\\linear.py in forward(self, input)\n     85 \n     86     def forward(self, input):\n---> 87         return F.linear(input, self.weight, self.bias)\n     88 \n     89     def extra_repr(self):\n\n~\\anaconda3\\lib\\site-packages\\torch\\nn\\functional.py in linear(input, weight, bias)\n   1608     if input.dim() == 2 and bias is not None:\n   1609         # fused op is marginally faster\n-> 1610         ret = torch.addmm(bias, input, weight.t())\n   1611     else:\n   1612         output = input.matmul(weight.t())\n\nRuntimeError: size mismatch, m1: [10 x 576], m2: [1024 x 512] at C:\\w\\b\\windows\\pytorch\\aten\\src\\TH\/generic\/THTensorMath.cpp:41\n<\/code>\n\n\n\n Capo_Mestre:\n\nself.pre_classifier = nn.Linear(64*4*4, 512)\n\n\nIf you trace the stacktrace, it ends up in this line which as you might guess size is different from value in .view.\nYou can find these type of issues easily by tracing the stacktrace from last line to first. Then error message usually is common in many different posts, so you can the idea of error.\nOh, missed that one! Thanks heaps for pointing out to this! I have corrected the line from self.pre_classifier = nn.Linear(64*4*4, 512) to self.pre_classifier = nn.Linear(64*3*3, 512) and the script started the training process! It is rather slow though, but it works so far. Thanks again!"},{"x":"Consider the following snippet:\n<code class=\"lang-python\">from time import time\nimport torch\n\nfor run in range(5):\n    start = time()\n    torch.empty(1, device=\"cuda\")\n    end = time()\n    print(f\"{run}: {end - start:.2g} seconds\")\n<\/code>\nOn my machine this results in the following output:\n<code class=\"lang-auto\">0: 1.6 seconds\n1: 3e-05 seconds\n2: 7.9e-06 seconds\n3: 7.4e-06 seconds\n4: 7.2e-06 seconds\n<\/code>\nIt is pretty clear that the first time a tensor a moved to the GPU some startup or initialization is happening. Is it possible to manually do this upfront?\n\nI\u2019m asking this since I have multiple tests in my test suite that require CUDA. Depending on which test is run first, it indirectly performs the startup is therefore slow. If have a flag to skip slow tests, but the test in itself is not slow.","y":"You are right. I only see the smaller ~3MB allocation, but not the creation of the CUDA context.\nIn that case you might need to create a dummy tensor instead in the init method. ","z":"The first CUDA call initializes the CUDA context, which is this slow.\nYou might want to add an init method, which calls into torch.cuda.init() or any random CUDA tensor creation.\nAlso note, that you are not timing the tensor creation in this current code snippet, since you are not synchronizing the code. The 1e-6 times might thus profile the Python overhead from the kernel launch.\nfacpalm. Sorry, I\u2019m not sure how I missed torch.cuda.init() in the docs.\nAbout the timing: this was just my quick and dirty method to show the difference between the initial startup and the following runs. I do not use this to actually profile my code.\n Are you sure torch.cuda.init() is the way to go here? I\u2019ve added it before the loop and this has a negligible effect on the first run.\nYou are right. I only see the smaller ~3MB allocation, but not the creation of the CUDA context.\nIn that case you might need to create a dummy tensor instead in the init method. "},{"x":"<code class=\"lang-auto\">class ConditionLayer(nn.Module):\n    def __init__(self, layer_template, layer_name):\n        super(ConditionLayer, self).__init__()\n        self.layer_template = layer_template\n        self.layer_name = layer_name\n        self.set_params(*self.layer_template)\n\n    def set_params(self, cout, cin=None, height= None, width = None):\n        self.cout = cout\n        self.cin = cin\n        if height is not None:\n            self.height  = height\n            self.width   = width\n            self.spatial = height * width\n        else:\n            self.spatial = None\n\n        self.M_o = nn.Parameter(torch.eye(self.cout)).cuda()\n\n        if self.cin is not None:\n            self.M_i = nn.Parameter(torch.eye(self.cin)).cuda()\n\n        if self.spatial is not None:\n            self.M_f = nn.Parameter(torch.eye(self.spatial)).cuda()\n\n\n    def forward(self,x):\n        x_ =x\n        if self.spatial is not None:\n            x = x.permute(2,3,0,1)\n            x = x.view(-1,self.cout,self.cin)\n            x = x.view(-1,self.cout*self.cin)\n            x = torch.matmul(self.M_f,x)\n            x = x.view(self.spatial,self.cout,self.cin)\n            x = x.view(self.height, self.width, self.cout, self.cin)\n            x = x.permute(2,3,0,1)\n\n        if self.cin is not None:\n            x = x.transpose(1,0).contiguous()\n            x = x.view(self.cin,-1)\n            x = torch.matmul(self.M_i,x)\n            if self.spatial is not None:\n                x = x.view(self.cin, self.cout, self.height, self.width)\n            else:\n                x = x.view(self.cin,self.cout)\n            x = x.transpose(1,0).contiguous()\n\n        x = x.view(self.cout, -1)\n        x = torch.matmul(self.M_o,x)\n        x = x.view(*x_.shape)\n        return x\n\n\n\nclass Preconditioner(nn.Module):\n    def __init__(self, model_template, optimizer=None, step_size=0.1,\n                 learn_step_size=False, per_param_step_size=False,\n                 scheduler=None,device=None):\n\n        super(Preconditioner, self).__init__()\n        self.model_template = model_template\n        self.init_parameters()\n\n    def init_parameters(self):\n        self.condition_layer_obj = {}\n        for key in self.model_template.keys():\n            self.condition_layer_obj[key] = ConditionLayer(self.model_template[key], key)\n\n    def forward(self,grad):\n\n        preconditioned_grad = OrderedDict()\n        for layer_name in grad.keys():\n            preconditioned_grad[layer_name] = self.condition_layer_obj[layer_name](grad[layer_name])\n\n        return preconditioned_grad\n\n<\/code>\nThe above snippet creates a class Preconditioner (which inherits nn.Module), in the fucntion init_parameters, we create a dictionary, which stores objects of the class ConditionalLayer, where in we create trainable parameters self.M_o, self.M_i, self.M_f, but Preconditioner.named_parameters() returns an empty iterator, shouldnt it return the trainable params ?","y":"Python lists and dicts won\u2019t properly register submodules and parameters, so you should use nn.ModuleList, nn.ModuleDict, nn.ParameterList, or nn.ParameterDict` instead.","z":"Any help would be much appreciated, thanks!\nPython lists and dicts won\u2019t properly register submodules and parameters, so you should use nn.ModuleList, nn.ModuleDict, nn.ParameterList, or nn.ParameterDict` instead."},{"x":"In the forward method my my model, I come across a list of tensors. All of those tensors have requires_grad=True. However, when I try to obtain a tensor from this list using torch.FloatTensor(myTensorList), the requires_grad of the resulting tensor is False, which is breaking the graph for computing gradients. Will setting the requires_grad=True when casting my list to a FloatTensor will keep my graph concrete? Is it safe to do this? If not, what else can I do?","y":"Hi,\nIt is because FloatTensor creates a brand new Tensor from raw values.\nYou can use torch.cat(myTensorList) or torch.stack(myTensorList) to do this.","z":"Hi,\nIt is because FloatTensor creates a brand new Tensor from raw values.\nYou can use torch.cat(myTensorList) or torch.stack(myTensorList) to do this."},{"x":"I\u2019ve trained a model using PyTorch and saved a state dict file. I have loaded the pre-trained model using the code below. I am getting an error message regarding invalid syntax. Do i need to replace **kwargs with something else?  Many Thanks in advance\n<code class=\"lang-auto\">  File \"load_model_delete.py\", line 63\n    model=VGG((*args, **kwargs))\n                       ^\nSyntaxError: invalid syntax\n\n<\/code>\nI am following instruction available at this site: https:\/\/pytorch.org\/tutorials\/beginner\/saving_loading_models.html#saving-loading-model-across-devices\nMany Thanks\n<code class=\"lang-auto\">import argparse\nimport datetime\nimport glob\nimport os\nimport random\nimport shutil\nimport time\nfrom os.path import join\n\nimport numpy as np\nimport pandas as pd\nimport torch\nimport torch.nn as nn\nfrom torch.utils.data import DataLoader\nfrom torch.utils.tensorboard import SummaryWriter\nfrom torchvision.transforms import ToTensor\nfrom tqdm import tqdm\nimport torch.optim as optim\n\nfrom convnet3 import Convnet\nfrom dataset2 import CellsDataset\n\nfrom convnet3 import Convnet\nfrom VGG import VGG\nfrom dataset2 import CellsDataset\nfrom torchvision import models\nfrom Conv import Conv2d\n\nparser = argparse.ArgumentParser('Predicting hits from pixels')\nparser.add_argument('name',type=str,help='Name of experiment')\nparser.add_argument('data_dir',type=str,help='Path to data directory containing images and gt.csv')\nparser.add_argument('--weight_decay',type=float,default=0.0,help='Weight decay coefficient (something like 10^-5)')\nparser.add_argument('--lr',type=float,default=0.0001,help='Learning rate')\nargs = parser.parse_args()\n\nmetadata = pd.read_csv(join(args.data_dir,'gt.csv'))\nmetadata.set_index('filename', inplace=True)\n\n# create datasets:\n\ndataset = CellsDataset(args.data_dir,transform=ToTensor(),return_filenames=True)\ndataset = DataLoader(dataset,num_workers=4,pin_memory=True)\nmodel_path = '\/Users\/nubstech\/Documents\/GitHub\/CellCountingDirectCount\/VGG_model_V1\/checkpoints\/checkpoint.pth'\n\nclass VGG(nn.Module):\n    def __init__(self, pretrained=True):\n        super(VGG, self).__init__()\n        vgg = models.vgg16(pretrained=True)\n        # if pretrained:\n        vgg.load_state_dict(torch.load(model_path))\n        features = list(vgg.features.children())\n        self.features4 = nn.Sequential(*features[0:23])\n\n\n        self.de_pred = nn.Sequential(Conv2d(512, 128, 1, same_padding=True, NL='relu'),\n                                    Conv2d(128, 1, 1, same_padding=True, NL='relu'))\n\n\n    def forward(self, x):\n        x = self.features4(x)       \n        x = self.de_pred(x)\n\n        return x\n\nmodel=VGG()\n#model.load_state_dict(torch.load(model_path),strict=False)\nmodel.eval()        \n\n#optimizer = torch.optim.Adam(model.parameters(),lr=args.lr,weight_decay=args.weight_decay)\n\nfor images, paths in tqdm(dataset):\n\n    targets = torch.tensor([metadata['count'][os.path.split(path)[-1]] for path in paths]) # B\n    targets = targets.float()\n\n    # code to print training data to a csv file\n    #filename=CellsDataset(args.data_dir,transform=ToTensor(),return_filenames=True)\n    output = model(images) # B x 1 x 9 x 9 (analogous to a heatmap)\n    preds = output.sum(dim=[1,2,3]) # predicted cell counts (vector of length B)\n    print(preds)\n    paths_test = np.array([paths])\n    names_preds = np.hstack(paths)\n    print(names_preds)                \n    df=pd.DataFrame({'Image_Name':names_preds, 'Target':targets.detach(), 'Prediction':preds.detach()})\n    print(df) \n    # save image name, targets, and predictions\n    df.to_csv(r'model.csv', index=False, mode='a')\n\n<\/code>","y":"\n\n\n alameer:\n\nvgg = models.vgg16(pretrained=pretrained)\n\n\nYou have this line under your init method. It doesn\u2019t know where to get vgg16 from.  If you don\u2019t need this (since it looks like you already have VGG defined in the class?) you can just comment it out. Otherwise if you do need it, you need to have this line: import torchvision.models as models to get access to the vgg16 class.","z":"\n\n\n alameer:\n\nmodel=VGG((*args, **kwargs))\n\n\nThis should be VGG(*args, **kwargs). I\u2019m noticing that you don\u2019t have kwargs defined however. Args should be a list or tuple of values. Kwargs should be a dictionary.\nHere\u2019s an example of how to use the * operator:\n<code class=\"lang-auto\">args = (a, b, c)\nkwargs = {'x': 1, 'y': 1}\n\n# These two lines are equivalent \nModel(*args, **kwargs)\nModel(a, b, c, x=1, y=1)\n<\/code>\nMany Thanks! How do decide which values to use please? Sorry, if this is really stupid question.\n Is there an example i could learn from please?\nNo worries! There\u2019s no stupid questions.\nIf you already trained a model and you want to load its weight, then you need to initialize the model object using the exact same parameters.\nIn your example, I can see in your def __init__(self, pretrained=True): that there\u2019s no positional arguments (we don\u2019t count self) and you have one keyword argument. Since your pretrained doesn\u2019t do anything yet (you have the code commented out), using either VGG(pretrained=True) or VGG(pretrained=False) will work. Really the most important thing is that the same exact layers are being constructed and initialized.\nThanks! Sorry i am still confused about how to go about getting this to work. I have taken the line of code model=VGG((*args, **kwargs)) but now got an error message NameError: name 'models is not defined. Many Thanks for your patient.\n\n\n\n alameer:\n\nvgg = models.vgg16(pretrained=pretrained)\n\n\nYou have this line under your init method. It doesn\u2019t know where to get vgg16 from.  If you don\u2019t need this (since it looks like you already have VGG defined in the class?) you can just comment it out. Otherwise if you do need it, you need to have this line: import torchvision.models as models to get access to the vgg16 class.\nThanks very mcuh!\nI am getting the following error:\n<code class=\"lang-auto\">RuntimeError: Error(s) in loading state_dict for VGG:\n\tMissing key(s) in state_dict: \"features.0.weight\", \"features.0.bias\", \"features.2.weight\", \"features.2.bias\", \"features.5.weight\", \"features.5.bias\", \"features.7.weight\", \"features.7.bias\", \"features.10.weight\", \"features.10.bias\", \"features.12.weight\", \"features.12.bias\", \"features.14.weight\", \"features.14.bias\", \"features.17.weight\", \"features.17.bias\", \"features.19.weight\", \"features.19.bias\", \"features.21.weight\", \"features.21.bias\", \"features.24.weight\", \"features.24.bias\", \"features.26.weight\", \"features.26.bias\", \"features.28.weight\", \"features.28.bias\", \"classifier.0.weight\", \"classifier.0.bias\", \"classifier.3.weight\", \"classifier.3.bias\", \"classifier.6.weight\", \"classifier.6.bias\". \n\tUnexpected key(s) in state_dict: \"state_dict\", \"optimizer_state_dict\", \"globalStep\", \"train_paths\", \"test_paths\". \n<\/code>\nIf i add strict= false to the load statement the code work but the prediction don\u2019t looks right to me. Any idea why would this be the case please?\nI have updated the code in the snippet t show the latest code that i am working on."},{"x":"Hi,\nthis should be a quick one, but I wasn\u2019t able to figure it out myself.\nWhen I use a pre-defined module in PyTorch, I can typically access its weights fairly easily.\nHowever, how do I access them if I wrapped the module in nn.Sequential() first?\nPlease see toy example below.\nclass My_Model_1(nn.Module):\n    def __init__(self,D_in,D_out):\n        super(My_Model_1, self).__init__()\n        self.layer = nn.Linear(D_in,D_out)\n    def forward(self,x):\n        out = self.layer(x)\n        return out\n\nclass My_Model_2(nn.Module):\n    def __init__(self,D_in,D_out):\n        super(My_Model_2, self).__init__()\n        self.layer = nn.Sequential(nn.Linear(D_in,D_out))\n    def forward(self,x):\n        out = self.layer(x)\n        return out\n\nmodel_1 = My_Model_1(10,10)\nprint(model_1.layer.weight)\nmodel_2 = My_Model_2(10,10)\n# How do I print the weights now?\n# model_2.layer.0.weight doesn't work.\n\nMany thanks.\nAny help much appreciated.","y":"<code class=\"lang-auto\">model_2.layer[0].weight\n<\/code>","z":"<code class=\"lang-auto\">model_2.layer[0].weight\n<\/code>\nHi! Many thanks, this is what I was looking for. Was trying the wrong braces.\nWhen I am doing this, the error i am getting is \"The model has no attribute \u2018layer\u2019 \".\n\nlayer was defined in __init__:\nself.layer = nn.Sequential(nn.Linear(D_in,D_out))\n\nYou have to use the variable name defined in your model.\nHi,\nIs there any way in Pytorch to get access to the layers of a model and weights in each layer without typing the layer name. Something like model.layers in keras which is discussed in the following:\n\n\nstackoverflow.com with link \"https:\/\/stackoverflow.com\/questions\/50151157\/keras-how-to-get-layer-index-when-already-know-layer-name\"\n\n\n with link \"https:\/\/stackoverflow.com\/users\/9734424\/nguy%e1%bb%85n-c%c3%b4ng-minh\"\n\nKeras: How to get layer index when already know layer name? with link \"https:\/\/stackoverflow.com\/questions\/50151157\/keras-how-to-get-layer-index-when-already-know-layer-name\"\n\n\npython, keras\n\n\n  asked by\n  \n  Nguy\u1ec5n C&amp;#244;ng Minh with link \"https:\/\/stackoverflow.com\/users\/9734424\/nguy%e1%bb%85n-c%c3%b4ng-minh\"\n  on 09:10AM - 03 May 18 with link \"https:\/\/stackoverflow.com\/questions\/50151157\/keras-how-to-get-layer-index-when-already-know-layer-name\"\n\n\n\n\n\n\n\nWhen I using this method with model.eval() I getting different values of weights for the same example.\nIs that mean that my model don\u2019t work correctly ?\nHi ptrblck\nHappy to find you here.\nI am building 2 CNN layers with 3 FC layers and using drop out two times.\nMy neural network is defined as follow: Do you see any thing wrong in that? I appreciate your feedback.\nimport torch\nimport torchvision\nimport torchvision.transforms as transforms\nfrom torch.utils.data import TensorDataset, DataLoader\nimport torch.optim as optim\nimport torch.nn as nn\nfrom torch.utils.data.dataset import random_split\nfrom torch.nn import functional as F\nimport matplotlib.pyplot as plt\nfrom torch.autograd import Variable\nclass ConvNetRedo1(nn.Module):\ndef init(self,numf1,numf2,fz1,fz2,nn2,nn3): # numf1( nnumber of filters first layer)numf2(nnumber of filters first layer)),fz1 kernel size(),fz2,nn2,nn3\nsuper(ConvNetRedo1, self).init()\nself.numf1=numf1\nself.numf2=numf2\nself.fz1=fz1\nself.fz2=fz2\nself.nn2=nn2\nself.nn3=nn3\nself.layer1 = nn.Sequential(nn.Conv3d(1, self.numf1, kernel_size=self.fz1, stride=1, padding=2),nn.ReLU(),nn.MaxPool3d(kernel_size=2, stride=2))\nself.layer2 = nn.Sequential(nn.Conv3d(self.numf1,self.numf2, kernel_size=self.fz2, stride=1, padding=2),nn.ReLU(),nn.MaxPool3d(kernel_size=2, stride=2))\nself.fc1 = nn.Linear(3072, self.nn2) ##3027\nself.fc2 = nn.Linear( self.nn2, self.nn3) # FULLY CONNECTED LAYERS\nself.fc3 = nn.Linear( self.nn3, 1) # FULLY CONNECTED LAYERS\nself.relu = nn.ReLU()                          # Non-Linear ReLU Layer: max(0,x)\nself.sigmoid = nn.Sigmoid()\nself.drop_out1 = nn.Dropout(0.5)\nself.drop_out2 = nn.Dropout(0.5)\nself.Relu=nn.LeakyReLU(0.1, inplace=True)\ndef forward(self, x):\n\n\n    x=x.unsqueeze(1).float()\n    out = self.layer1(x)\n    \n    out = self.layer2(out)\n    \n    out = out.view(out.size(0), -1)\n    \n    out = self.fc1(out)\n    out=self.drop_out1(out)\n    out=self.relu(out)\n\n    out = self.fc2(out)\n    out=self.drop_out2(out)\n    out=self.relu(out)\n    \n    out = self.fc3(out)\n    out = self.sigmoid(out)\n    return out\nI\u2019m not sure, if the number of features are correctly defined without knowing the input shape, but skimming through the model definition, I cannot find any obvious mistakes.\nAre you seeing any issues with the model?\nHi\nMany thanks for your reply.\nThe size of the features are good. I just want to know the way of designing, is  dropout used in the good step? Is it better to use after Relu?\nIt doesn\u2019t matter if dropout is applied before or after the relu. I cannot see any obvious mistakes.\nMany thanks for your reply. Sorry I am transferring my model and data and labels to the GPU.I am not sure if I should transfer criterion and optimizer to the GPU or not?\nI used them in this way\ncriterion = nn.BCELoss()\noptimizer = torch.optim.Adam(model.parameters(), lr=.03)\nI appreciate your help\nDo u know any books or links which is usable?\nS\nYou could transfer the criterion to the GPU just to avoid possible issues, but it shouldn\u2019t be necessary for nn.BCELoss.\nOne minor advice: I would remove the last sigmoid in your model and use nn.BCEWithLogitsLoss instead, as it will be numerically more stable.\nCheck out \u201cDeep Learning with PyTorch\u201d by , , and , which can be downloaded for free on the official website with link \"https:\/\/pytorch.org\/deep-learning-with-pytorch\".\n(It\u2019s not the full book if I\u2019m not mistaken, as it\u2019s still work in progress  )\nMeans finished the model to\nout = self.fc3(out)\nand use nn.BCEWithLogitsLoss . it has the in-built  sigmoid in it?\n<code class=\"lang-auto\">def forward(self, x):\n\n\n    x=x.unsqueeze(1).float()\n    out = self.layer1(x)\n    \n    out = self.layer2(out)\n    \n    out = out.view(out.size(0), -1)\n    \n    out = self.fc1(out)\n    out=self.drop_out1(out)\n    out=self.relu(out)\n\n    out = self.fc2(out)\n    out=self.drop_out2(out)\n    out=self.relu(out)\n    \n    out = self.fc3(out)\n\n    return out\n\nand \n\ncriterion=nn.BCEWithLogitsLoss()    indeed the input of the this loss function should be the out put from linear layer ?<\/code>\nYes, that\u2019s the correct usage.\nDear ptrblck\nI read now the book that you suggest, I am really confused in some cases during training I saw (optimizer.zero_grad()) is used before getting output and applying model and in this book ( lantiga)  it is after getting the output?!\nwhich one is correct?\nt_p = model(t_un, *params)\nloss = loss_fn(t_p, t_c)\noptimizer.zero_grad()\nloss.backward()\noptimizer.step( )\nIt depends on your coding style, but it should be called before loss.backward() in case you don\u2019t want explicitly to accumulate the gradients (which is a valid use case, but not the usual work flow).\nI personally try to add it right at the beginning of the training loop, since I think I remember it better, but I still forget it from time to time. \nI really appreciate your help \nyou are really helping me\nDear ptrblck\nI used the thing that we discussed but the outputs are more than 1  !!"},{"x":"I have been using pytorch for quite a while, but now, one of the projects I am working on, requires me to use opencv and no PIL \nTypically, I do this in pytorch:\n<code class=\"lang-auto\">        self.data_transforms = torchvision.transforms.Compose([\n            torchvision.transforms.Resize((224,224)),\\\n            torchvision.transforms.ToTensor(),\n            torchvision.transforms.Normalize([0.485, 0.456, 0.406], [0.229, 0.224, 0.225])\n        ])\n<\/code>\nId like to replicate the same with openCV with minimal effort (because the above snippet is present in quite a few of my dataloader classes).\nWhat is the most efficient way to go about this?","y":"This repository with link \"https:\/\/github.com\/jbohnslav\/opencv_transforms\" seems to have implemented (some) torchvision.transforms in OpenCV.","z":"This repository with link \"https:\/\/github.com\/jbohnslav\/opencv_transforms\" seems to have implemented (some) torchvision.transforms in OpenCV."},{"x":"I want to make the value of a specific index equal to 0. but very very slow\u2026\n<code class=\"lang-auto\">mask = [1,2,3,4]\n\ninput = torch.zeros(100,64,32,32)\n\nfor m in mask:\n   for i in range(100):\n      input[i][m] = 0\n<\/code>\nHow do I optimize my code?","y":"This code should work:\n<code class=\"lang-python\">x = torch.zeros(100, 64, 32, 32)\nmask = torch.tensor([1,2,3,4])\nx[:, mask] = 1\n<\/code>\nNote that I changed the assignment to a 1, otherwise you would reassign a 0 to the tensor containing all zeros. ","z":"This code should work:\n<code class=\"lang-python\">x = torch.zeros(100, 64, 32, 32)\nmask = torch.tensor([1,2,3,4])\nx[:, mask] = 1\n<\/code>\nNote that I changed the assignment to a 1, otherwise you would reassign a 0 to the tensor containing all zeros. \nGood JOB!! Many Thanks!!!\nHow to do this in the last dimension of the tensor?"},{"x":"Hi. i\u2019m running on this error to solve it from train code, but haven\u2019t yet.\nI\u2019d really appreciate if someone could help me figure this out.\n\nfrom nvidia-smi below >>>>\n\u00b1----------------------------------------------------------------------------+\n| NVIDIA-SMI 450.51.05    Driver Version: 450.51.05    CUDA Version: 11.0     |\n|-------------------------------\u00b1---------------------\u00b1---------------------+\n| GPU  Name        Persistence-M| Bus-Id        Disp.A | Volatile Uncorr. ECC |\n| Fan  Temp  Perf  Pwr:Usage\/Cap|         Memory-Usage | GPU-Util  Compute M. |\n|                               |                      |               MIG M. |\n|===============================+======================+======================|\n|   0  GeForce RTX 208\u2026  On   | 00000000:01:00.0  On |                  N\/A |\n|  0%   42C    P8    19W \/ 300W |    413MiB \/ 11018MiB |      1%      Default |\n|                               |                      |                  N\/A |\n\u00b1------------------------------\u00b1---------------------\u00b1---------------------+\n| Processes:                                                                  |\n|  GPU   GI   CI        PID   Type   Process name                  GPU Memory |\n|        ID   ID                                                   Usage      |\n|=============================================================================|\n|    0   N\/A  N\/A      1282      G   \/usr\/lib\/xorg\/Xorg                 18MiB |\n|    0   N\/A  N\/A      1418      G   \/usr\/bin\/gnome-shell               57MiB |\n|    0   N\/A  N\/A      1637      G   \/usr\/lib\/xorg\/Xorg                181MiB |\n|    0   N\/A  N\/A      1752      G   \/usr\/bin\/gnome-shell              143MiB |\n|    0   N\/A  N\/A      3014      G   \u2026ommunity\/202\/jbr\/bin\/java        8MiB |\n\u00b1----------------------------------------------------------------------------+\n\nCUDA 11.0 was installed with NVIDIA-driver(450.51.05) automatically.\nso i installed CUDA 10.0 and changed instead of CUDA 11.0\nbut i don\u2019t know why nvidia-smi shows CUDA 11.0\n(~\/.profile)\nexport PATH=\/usr\/local\/cuda-10.0\/bin:$PATH\nexport LD_LIBRARY_PATH=\/usr\/local\/cuda-10.0\/lib64:$LD_LIBRARY_PATH\n\nEnvironments>>>\nGeForce RTX 2080Ti\nCUDA Version 10.0.130\n#define CUDNN_MAJOR 7\n#define CUDNN_MINOR 6\n#define CUDNN_PATCHLEVEL 2\nAnaconda3 \/ Python3.6 \/\nconda install pytorch torchvision cudatoolkit=10.1 -c pytorch\n\n\nHere is the error log >>>>\n\nTHCudaCheck FAIL file=\/opt\/conda\/conda-bld\/pytorch_1591914742272\/work\/aten\/src\/THC\/THCGeneral.cpp line=47 error=100 : no CUDA-capable device is detected\nTraceback (most recent call last):\nFile \u201ctrain_pose.py\u201d, line 256, in \nmodel = construct_model(args)\nFile \u201ctrain_pose.py\u201d, line 48, in construct_model\nmodel = torch.nn.DataParallel(model, device_ids=args.gpu).cuda()\nFile \u201c\/home\/jj\/anaconda3\/envs\/pytorch\/lib\/python3.6\/site-packages\/torch\/nn\/modules\/module.py\u201d, line 307, in cuda\nreturn self._apply(lambda t: t.cuda(device))\nFile \u201c\/home\/jj\/anaconda3\/envs\/pytorch\/lib\/python3.6\/site-packages\/torch\/nn\/modules\/module.py\u201d, line 203, in _apply\nmodule._apply(fn)\nFile \u201c\/home\/jj\/anaconda3\/envs\/pytorch\/lib\/python3.6\/site-packages\/torch\/nn\/modules\/module.py\u201d, line 203, in _apply\nmodule._apply(fn)\nFile \u201c\/home\/jj\/anaconda3\/envs\/pytorch\/lib\/python3.6\/site-packages\/torch\/nn\/modules\/module.py\u201d, line 203, in _apply\nmodule._apply(fn)\nFile \u201c\/home\/jj\/anaconda3\/envs\/pytorch\/lib\/python3.6\/site-packages\/torch\/nn\/modules\/module.py\u201d, line 225, in _apply\nparam_applied = fn(param)\nFile \u201c\/home\/jj\/anaconda3\/envs\/pytorch\/lib\/python3.6\/site-packages\/torch\/nn\/modules\/module.py\u201d, line 307, in \nreturn self._apply(lambda t: t.cuda(device))\nFile \u201c\/home\/jj\/anaconda3\/envs\/pytorch\/lib\/python3.6\/site-packages\/torch\/cuda\/init.py\u201d, line 154, in _lazy_init\ntorch._C._cuda_init()\nRuntimeError: cuda runtime error (100) : no CUDA-capable device is detected at \/opt\/conda\/conda-bld\/pytorch_1591914742272\/work\/aten\/src\/THC\/THCGeneral.cpp:47\n\nthanks.\n","y":"Hi. i resolved the problem through reinstalling Pytorch version(1.5.1 > 1.0.0).\nthanks.","z":"Hi. i resolved the problem through reinstalling Pytorch version(1.5.1 > 1.0.0).\nthanks."},{"x":"torch.manual_seed seems not honored in  torch.randint.\nI ran the following program multiple times in torch 1.5.1 and got different output each time.\ntorch.manual_seed=123\ntorch.randint(3, 5, (3,))\ntorch.manual_seed(123) call used to work before torch 1.4, but with torch 1.5.1 I got the following error. So I changed the call to be torch.manual_seed=123, not sure if that is the problem.\n\nTypeError                                 Traceback (most recent call last)\n in \n----> 1 torch.manual_seed(123)\nTypeError: \u2018int\u2019 object is not callable","y":"Can you restart the notebook? This is most likely because above you did torch.manual_seed = 123.\nDoing import foo multiple times in python does not import it again, it just returns the same module.","z":"Hi,\ntorch.manual_seed is a function. You should call it with the new seed you want to use doc with link \"https:\/\/pytorch.org\/docs\/stable\/torch.html#torch.manual_seed\".\nWhen you do torch.manual_seed=123 you actually override the function and change it to 123. This is why you get the error you report. Don\u2019t do this assignment and just call the function after a new import of pytorch.\nimage951\u00d7551 18.9 KB with link \"https:\/\/discuss.pytorch.org\/uploads\/default\/original\/3X\/9\/0\/90739d19d87f5b16fb34d9525d5d0a1bda2847f1.png\"\nAs you can see, with torch 1.5.1 the function torch.manual_seed cannot take in an int somehow.\nCan you restart the notebook? This is most likely because above you did torch.manual_seed = 123.\nDoing import foo multiple times in python does not import it again, it just returns the same module.\nYou are right! Many thanks!"},{"x":"Hi. I am trying to load a model with:\n<code class=\"lang-auto\">import torch\n\nimport pyautogui as mouse\nimport cv2 \n\n\nfrom ScreenRecorder import Record,IniRecord,Frame\n\n\ndef start(model):\n    sc_ini = Frame()\n    monitor = sc_ini.get()\n    sc = IniRecord(monitor,1.6)\n    while True:\n        frame = sc.getFrame()\n        cv2.imshow('frame',frame)\n        output_xy,output_click = model.forward(frame)\n        #print(output_xy,output_click)\n        if cv2.waitKey(1) &amp; 0xFF == ord('q'):\n            print(\"break\")            \n            break\n        \n\nmodel = torch.load_state_dict(torch.load('Model\/model_save'))\n<\/code>\nBut it says\n<code class=\"lang-auto\">\n  File \"D:\/Nextcloud\/Python\/Gamebot\/Bot.py\", line 31, in <module>\n    model = torch.load_state_dict(torch.load('Model\/model_save'))\n\nAttributeError: module 'torch' has no attribute 'load_state_dict'\n<\/code>","y":"You have to call it on your model:\nmodel.load_state_dict(torch.load(...))","z":"You have to call it on your model:\nmodel.load_state_dict(torch.load(...))\nIs there another way to load it? because else i have to import the whole model (network) in my file.\nAnd it still complains\n<code class=\"lang-auto\">.    output_xy,output_click = model(frame)\n\nTypeError: 'NoneType' object is not callable\n<\/code>\nDid you try to assign the model? (model = model.load_state_dict())\nJust skip it and load the state_dict directly:\n<code class=\"lang-auto\">model = MyModel()\nmodel.load_state_dict(torch.load(...))\n<\/code>\nIf i test the model. Do i have to use the batch size as input as well or can i put single images in as well?\nYou can use single images, but have to make sure the tensor contains the batch dimension:\n<code class=\"lang-python\">image = ... # shape = [channels, height, width]\nimage = image.unsqueeze(0) # shape = [1, channels, height, width]\noutput = model(image)\n<\/code>\nHmm is it just not enough training or why does it always ouputs 0 and never tries 1 ?\nI don\u2019t know what kind of model and use case you are working on, but it might be worth starting a new thread if you encounter any errors to keep this topic clean.\nHi ptrblck\nI saved my trained Nets on GPU and now wants to use them on CPU.\nI read your comments but still have same problem as (AttributeError: \u2018list\u2019 object has no attribute \u2018load_state_dict\u2019\nMy code is:\n            checkpoint = torch.load(Path1,map_location=torch.device('cpu'))\n\n<h1>model.load_state_dict(torch.load(Path1,map_location=torch.device(\u2018cpu\u2019))[\u2018model_state_dict\u2019])<\/h1>\n            model.load_state_dict(torch.load(Path1)['model_state_dict'])\n            optimizer.load_state_dict(torch.load(Path1,map_location=torch.device('cpu'))['optimizer_state_dict'])\nHow did you create your model instance?\nBased on the error message it seems it\u2019s a list instead of a subclass of nn.Module.\nHi , Many thnaks fo ryour reply. muy model is \nimport torch\nimport torchvision\nimport torchvision.transforms as transforms\nfrom torch.utils.data import TensorDataset, DataLoader\nimport torch.optim as optim\nimport torch.nn as nn\nfrom torch.utils.data.dataset import random_split\nfrom torch.nn import functional as F\nimport matplotlib.pyplot as plt\nfrom torch.autograd import Variable\nclass ConvNet(nn.Module):\ndef init(self,numf1,numf2,fz1,fz2,nn2,nn3):\nsuper(ConvNet, self).init()\nself.numf1=numf1\nself.numf2=numf2\nself.fz1=fz1\nself.fz2=fz2\nself.nn2=nn2\nself.nn3=nn3\nself.layer1 = nn.Sequential(nn.Conv3d(1, self.numf1, kernel_size=self.fz1, stride=1, padding=2),nn.ReLU(),nn.MaxPool3d(kernel_size=2, stride=2))\nself.layer2 = nn.Sequential(nn.Conv3d(32,self.numf2, kernel_size=self.fz2, stride=1, padding=2),nn.ReLU(),nn.MaxPool3d(kernel_size=2, stride=2))\nself.fc1 = nn.Linear(3072, self.nn2) ##3027\nself.drop_out = nn.Dropout(0.3)\nprint(\u201cDropout\u201d)\nself.fc2 = nn.Linear( self.nn2, self.nn3) # FULLY CONNECTED LAYERS\nself.fc3 = nn.Linear( self.nn3, 1) # FULLY CONNECTED LAYERS\n    self.sigmoid = nn.Sigmoid()\n    \ndef forward(self, x):\n\n<h1>print(\u201chere: {}\u201d.format(x.shape))<\/h1>\n   # print(type(x))\n\n<h1>x=np.expand_dims(x,axis=0)<\/h1>\n<h1>print(\u201cxthen\u201d,x.shape)<\/h1>\n<h1>x=x.astype(int)<\/h1>\n<h1>x=torch.from_numpy(x)<\/h1>\n    x=x.unsqueeze(1).float()\n\n<h1>print(type(x))<\/h1>\n    #print(x.shape)\n    out = self.layer1(x)\n    \n   # print(out.shape)\n  #  print(out)\n    out = self.layer2(out)\n\n<h1>print(out.shape)<\/h1>\n    out = out.view(out.size(0), -1)\n\n<h1>print(out.shape)<\/h1>\n    out = self.fc1(out)\n    out = self.fc2(out)\n    out = self.fc3(out)\n    out = self.sigmoid(out)\n\n<h1>print(\u201coutsize\u201d,out.shape)<\/h1>\n    return out\nand I saved it in this way:\nPath=root_dir1+\u2019\/Fold_\u2019+str(FoldNum)+\u2018NumDarw=\u2019+str(NumDraw)+\u2018Iteration\u2019+str(Iteration)+\".pth\"\ncheckpoint = {\u2018model_state_dict\u2019: model.state_dict(),\u2018optimizer_state_dict\u2019: optimizer.state_dict()}\n# print(checkpoint)\ntorch.save(checkpoint,Path) ## save for each 10 iteration\none time I run my code when I saved the trained Nets on CPU. but now I am loading the trained Nets from GPU to use on CPU  and get this error\nThanks for the code. Could you post the code you are using to restore the model?\n        model=[]\n        optimizer=[]\n        TargetWholev2=[]\n        for Iteration1 in range(9):\n            Path1=root_dir2+'\/Fold_'+str(FoldNum)+'NumDarw='+str(NumDraw)+'Iteration'+str(Iteration1+1)+'.pth'\n\n            \n            checkpoint = torch.load(Path1,map_location=torch.device('cpu'))\n\n            model.load_state_dict(torch.load(Path1,map_location=torch.device('cpu'))['model_state_dict'])\n\n\n            optimizer.load_state_dict(torch.load(Path1,map_location=torch.device('cpu'))['optimizer_state_dict'])\nYou are currently initializing model and optimizer as an empty Python list, which creates this error.\nInitialize both as you have done in your training script, i.e.:\n<code class=\"lang-python\">model = ConvNet(...)\noptimizer = optim.SGD(model.parameters(), ...)\n\n# Now load the state_dicts\nmodel.load_state_dict(...)\noptimizer.load_state_dict(...)\n<\/code>\nI used this but give me this error:(Error(s) in loading state_dict for ConvNet:\nsize mismatch for layer1.0.weight: copying a param with shape torch.Size([32, 1, 3, 3, 3]) from checkpoint, the shape in current model is torch.Size([2, 1, 3, 3, 3]).\nsize mismatch for layer1.0.bias: copying a param with shape torch.Size([32]) from checkpoint, the shape in current model is torch.Size([2]).)\nmodel = ConvNet(2,64,3,3,300,20)\noptimizer =torch.optim.Adam(model.parameters(), lr=LR)\nTargetWholev2=[]\nfor Iteration1 in range(9):\nPath1=root_dir2+\u2019\/Fold_\u2019+str(FoldNum)+\u2018NumDarw=\u2019+str(NumDraw)+\u2018Iteration\u2019+str(Iteration1+1)+\u2019.pth\u2019\n<h1>checkpoint = torch.load(Path1)<\/h1>\n            checkpoint = torch.load(Path1,map_location=torch.device('cpu'))\n\n            model.load_state_dict(torch.load(Path1,map_location=torch.device('cpu'))['model_state_dict'])\n\n\n            optimizer.load_state_dict(torch.load(Path1,map_location=torch.device('cpu'))['optimizer_state_dict'])\nThe error points to different shapes of your parameters, which means that you\u2019ve initialized the model in a different way.\nCould you check, how you\u2019ve initialized the ConvNet before saving the state_dict and use the same arguments?\nyes you are right 2 is 32 i missed the 3 \nI really appreciate your time \nexcuse me, the time of training, despite the CNN was shallow was 4 days for just one draw. How I can speed up the GPU? The number of workers in data loader are important for speed?"},{"x":"Hello,\nto save memory I am trying to train a resnet18 model in half precision. Therefore, I converted my inputs and my model. I also changed the BachNorm2d layers back to normal floating tensors with the following lines:\n<code class=\"lang-auto\">resnet18.half()\nfor layer in resnet18.modules():\n    if isinstance(layer, nn.BatchNorm2d):\n        layer.float()\n<\/code>\nNevertheless, I receive nan values as loss after the second batch is trained. Since, everything works find without half precision and the chance of instabilities during applying the BatchNorm2d, I was wondering if I am missing something or if there is another way?","y":"Calling model.half() might work, but can easily create overflows and thus NaNs.\nWe recommend to use the automatic mixed-precision training with link \"https:\/\/pytorch.org\/docs\/master\/amp.html\", which takes care of these issues and also stabilizes the training using loss scaling.\nYou can use it in the nightly binaries or by building PyTorch from source.","z":"Calling model.half() might work, but can easily create overflows and thus NaNs.\nWe recommend to use the automatic mixed-precision training with link \"https:\/\/pytorch.org\/docs\/master\/amp.html\", which takes care of these issues and also stabilizes the training using loss scaling.\nYou can use it in the nightly binaries or by building PyTorch from source."},{"x":"Hi everyone,\nThis is a general doubt I have about the relationship between the activation function and the data.\nImagine our data (images), after normalization, is centered at 0 and take values between -1 and 1. It means our network will try to output images which values will be also between -1 and 1.\nSo, for example, if we use ReLU as activation function after each layer, we are removing negative values during training, so the network will never be able to correctly predict our targets.\nIs that right?\nIf I use ReLU, should I normalize my data between 0 and 1 insted of between -1 and 1?\nPD: I ask this because I saw cases where they normalize between -1 and 1 and they use ReLU successfully.\nThanks in advance ","y":"Hi Deep!\n\n\n\n dllacer:\n\nif we use ReLU as activation function after each layer, we are removing negative values during training, so the network will never be able to correctly predict our targets.\n\n\nNo, this way of looking at it isn\u2019t correct.\nThe reason is that the weights in your layers can be negative and\nthe biases are not constrained.  So a negative input value can be\nmultiplied by a negative weight or have a positive bias added to it,\nand therefore become positive, before it gets to the ReLU activation.\nBest.\nK. Frank","z":"Hi Deep!\n\n\n\n dllacer:\n\nif we use ReLU as activation function after each layer, we are removing negative values during training, so the network will never be able to correctly predict our targets.\n\n\nNo, this way of looking at it isn\u2019t correct.\nThe reason is that the weights in your layers can be negative and\nthe biases are not constrained.  So a negative input value can be\nmultiplied by a negative weight or have a positive bias added to it,\nand therefore become positive, before it gets to the ReLU activation.\nBest.\nK. Frank"},{"x":"I have a dataset of around 200 images. However, the format of my image is not standard because it has 6 dimensions.\nOne image would look like this (before being cast to tensor and being stack in a 3d Tensor):\n[img0, img1, img2, img3, img4, img5]. (They are all grayscale PIL Image )\nHowever my dataset is quite small and I want to do upsampling.  I wanted to use torchvisions.transforms to do that. My problem is that I need to perform the exact same list of transformation (like RandomHorizontalFlip, Random Rotation, etc\u2026) for all the dimension of my image and but I do not want to do the same transformation for every image. Is there a way to do so?","y":"You could use the functional API of torchvision.transforms as given in this example with link \"https:\/\/discuss.pytorch.org\/t\/torchvision-transfors-how-to-perform-identical-transform-on-both-image-and-target\/10606\/7\".\nThis would make sure to apply the same \u201crandom\u201d transformation on all image slices.\nLet me know, if this would work for you.","z":"You could use the functional API of torchvision.transforms as given in this example with link \"https:\/\/discuss.pytorch.org\/t\/torchvision-transfors-how-to-perform-identical-transform-on-both-image-and-target\/10606\/7\".\nThis would make sure to apply the same \u201crandom\u201d transformation on all image slices.\nLet me know, if this would work for you.\nYes I found your example. It worked thank you very much."},{"x":"I am currently using a hybrid model using pytorch by hybrid I mean Autoencoder and CNN and here is my model :\nconvnet(\n(fc_encoder): Sequential(\n(0): Linear(in_features=9950, out_features=200, bias=True)\n)\n(decoder): Sequential(\n(0): Linear(in_features=200, out_features=9950, bias=True)\n)\n(con1): Conv1d(1, 2, kernel_size=(5,), stride=(1,))\n(max1): MaxPool1d(kernel_size=4, stride=4, padding=0, dilation=1, ceil_mode=False)\n(con2): Conv1d(2, 4, kernel_size=(5,), stride=(1,))\n(max2): MaxPool1d(kernel_size=4, stride=4, padding=0, dilation=1, ceil_mode=False)\n(l1): Linear(in_features=2480, out_features=1024, bias=True)\n(drop): Dropout(p=0.5, inplace=False)\n(l2): Linear(in_features=1024, out_features=512, bias=True)\n(l3): Linear(in_features=512, out_features=128, bias=True)\n(l4): Linear(in_features=128, out_features=32, bias=True)\n(l5): Linear(in_features=32, out_features=1, bias=True)\n)\nI have some doubts regarding:\n\n\nI am using one loss function to evaluate my model which is BCELoss. I want to know if hybrid model is implemented do I need to use 2 loss functions? What will happen if I evaluate my model just on one loss function?\n\n\nThe second question is if I am changing my learning rate I have a big difference in the ROC curve why this is happening?\n\n\nAny clarification needed for the shape of the input I can provide.\nThanks in advance ","y":"If I\u2019m not mistaken, you are passing the inputs first to the Autoencoder and pass this output to the CNN.\nThe code for this looks generally alright and you wouldn\u2019t need to use two losses, if your complete model only returns a single output.\nNote that the last linear layers are used without any non-linearly, which could be reduced to a single linear transformation, so you might want to add some activation function between them.\n\n\nAs described, it depends on your use case, but the current model definition would not need two separate loss functions. You could of course use the Autoencoder output to calculate an auxiliary loss, but that\u2019s up to you.\n\n\nThe learning rate is a hyperparameter and it is expected that your model performance suffers from a bad learning rate value. E.g. if you set it to an unreasonably high values such as 1000 your model would most likely not converge.\n\n\nPS: you can post code snippets by wrapping them into three backticks ```, which makes debugging easier. ","z":"This is my class to define the model:\nclass convnet(nn.Module):\ndef init(self,num_inputs=9950):\nsuper(convnet,self).init()\nself.num_inputs=num_inputs\nself.fc_encoder= nn.Sequential(\nnn.Linear(num_inputs,200)\n)\nself.decoder = nn.Sequential(\nnn.Linear(200,num_inputs)\n)\nself.con1=nn.Conv1d(1, 2, 5)\nself.max1=nn.MaxPool1d(4)\nself.con2=nn.Conv1d(2, 4, 5)\nself.max2=nn.MaxPool1d(4)\nself.l1=nn.Linear(2480,1024)\nself.drop=nn.Dropout(p= 0.5)\nself.l2=nn.Linear(1024,512)\nself.l3=nn.Linear(512,128)\nself.l4=nn.Linear(128,32)\nself.l5=nn.Linear(32,1)\ndef forward(self, x):\nx = self.fc_encoder(x)\nx = self.decoder(x)\nx=x.unsqueeze(1)\nx=self.con1(x)\nx=self.max1(x)\nx=self.con2(x)\nx=self.max2(x)\nx = x.view(x.size(0), -1)\nx=self.l1(x)\nx=self.drop(x)\nx=self.l2(x)\nx=self.l3(x)\nx=self.l4(x)\nx=self.l5(x)\nreturn torch.sigmoid(x)\nm = convnet()\nm.to(device)\nIf I\u2019m not mistaken, you are passing the inputs first to the Autoencoder and pass this output to the CNN.\nThe code for this looks generally alright and you wouldn\u2019t need to use two losses, if your complete model only returns a single output.\nNote that the last linear layers are used without any non-linearly, which could be reduced to a single linear transformation, so you might want to add some activation function between them.\n\n\nAs described, it depends on your use case, but the current model definition would not need two separate loss functions. You could of course use the Autoencoder output to calculate an auxiliary loss, but that\u2019s up to you.\n\n\nThe learning rate is a hyperparameter and it is expected that your model performance suffers from a bad learning rate value. E.g. if you set it to an unreasonably high values such as 1000 your model would most likely not converge.\n\n\nPS: you can post code snippets by wrapping them into three backticks ```, which makes debugging easier. \nThank you so much for your reply. Reading your answer has really helped me to understand the concepts. I really appreciate your help \nYes you are correct first I am passing it to auto encoder and then to CNN. I will add a activation function to my model thank you for the suggestion \nThe reason I got confused in the loss function is because I am getting very high accuracy which seems too good to be true. I am getting accuracy of 0.99% by using learning rate 0.0003 and the ROC as attached below with your experience is it normal to get such high accuracy? Or I am doing something wrong?\nI have data set of size 1035 split is 90\/10 for training and testing respectively.\nThanks in advance \n\nYou could get such high accuracy values and it of course depends on the data set and used model.\nUsually you would split the dataset into three parts:\n\ntraining data, which is only used during the training to update the parameters\nvalidation data, which is often used after each full training epoch, to apply early stopping and for general hyperparameter tuning\ntest data, which is never touched during training\/validation, but one single time, if you end the training and would like to test the final model.\n\nIn the most careful approach, you should not modify the model or any hyperparameter anymore, as you would be leaking the test error information into the training, after calculating the test error.\nThank you so much for sharing your knowledge really helped me a lot.\n Is it normal to have the ROC curve like the image attached above? Usually, whenever I read papers or see over the GitHub they are moreover curvy.\nCan you please give your thoughts on that?\nI really appreciate your response \nOften these curves have more points in the plane and are thus \u201ccurvier\u201d.\nI guess that your predictions might have very specific clusters, such that changing the threshold for the positive\/negative predictions doesn\u2019t create a lot of different FPR\/TPR points.\nOkay thank you so much for the clarification "},{"x":"Hi there,\nI want to train a NN using the output of another already trained NN.  I think I am not able to do it right. new to pytorch. kindly help.\nIn the code below, the already trained NN is called \u201cmodel\u201d (hence, usd in eval() mode), and the network to be trained is \u201cmodelFCNN\u201d (a fully connected NN).\nI have conctenated the output from already trained NN (called \"model \" in the code below),\nwith another data coming in from data loader, but the trainning is very bizzare and I think the netowrk is not learning at all , although it shows loss in traning.\nperhas, I am not back propagating the error in an appropriate manner.\nAny , and all help is welcome. I thank you very much.\nCode is below:\nFirst the hyperparameters are declared and FCNN network class is defined.\n<code class=\"lang-auto\">HP1 = {\n    #'NUM_HIDDEN_NODES'    : 10 ,\n    #'NUM_EXAMPLES'        : 5 ,\n    'TRAIN_SPLIT'         : .8 ,\n    'MINI_BATCH_SIZE'     : 500,#10*int((2\/3)*(u_array_train.shape[1]+1)\/4) ,\n    'NUM_EPOCHS'          : 2000,\n    'LEARNING_RATE'       : 5e-2 ,\n    'LEARNING_RATE_DECAY' : 500 ,\n    'WEIGHT_DECAY'        : 5e-4 ,\n    'NUM_MOMENTUM'        : 0.9 ,\n    'NUM_PATIENCE'        : 50 ,\n    'SEED'                : 2018\n}\ndef to_np(x):\n    return x.data.cpu().numpy()\n\ndef to_var(x):\n    #if torch.cuda.is_available():\n        #x = x.cuda(async=async)\n    return Variable(x)\n\n# creat FCNN network and define all other parameters\n\n#build neural network\nclass NetworkFCNN(nn.Module):\n    def __init__(self,D_in,D_out):\n        super().__init__()\n        \n        # Inputs to hidden layer linear transformation\n        self.lin1 = nn.Linear(D_in, 100)\n        self.lin2=nn.Linear(100,100)\n        self.lin3=nn.Linear(100,100)\n        # Output layer,\n        self.output = nn.Linear(100, D_out)\n        \n        # Define sigmoid activation and softmax output \n        #self.tanh = F.tanh()\n        \n        \n    def forward(self, x):# this is where the data flows in the network, respecting \n                         #sequence of layers in forward method is very important.\n        # Pass the input tensor through each of our operations\n        \n        x = self.lin1(x)\n        x = F.tanh(x)\n        \n        x = self.lin2(x)\n        x = F.tanh(x)\n        \n        x = self.lin3(x)\n        x = F.tanh(x)\n        \n        x = self.output(x)\n        y = F.tanh(x)\n        \n        return y\n\ncriterion1 = torch.nn.MSELoss(size_average=False)\noptimizer1 = torch.optim.Adam(model.parameters(), \n                            lr=HP1['LEARNING_RATE']) \n                            #momentum=HP['NUM_MOMENTUM'], \n                            #weight_decay=HP['WEIGHT_DECAY'], \n                            #)\nscheduler = torch.optim.lr_scheduler.LambdaLR(optimizer1, lr_lambda=[epoch_smooth_decay])  # select the LR schedule\n\n<\/code>\nthen, the data loaders are defined\n<code class=\"lang-auto\">train_set_ref=torch.utils.data.TensorDataset(torch.FloatTensor(R_k_train),torch.FloatTensor(R_k_plus_1_train))\ntrain_loader_ref=torch.utils.data.DataLoader(train_set_ref,batch_size=HP1['MINI_BATCH_SIZE'],\n                                         shuffle=False,pin_memory=True,num_workers=0)\n\n<\/code>\nthen, the network training is done. please not that\nout put  from an already trained NN called \"model \" is to be concatenated with output of  FCNN (that is to be trained) .\nThe error is calculated between data from dataloader and output of the already trained NN \u201cmodel\u201d.\nThis error is backpropagated to train FCNN.\nI think  error is not being propagated correctly, or perhaps I should use detach(). please help.\n<code class=\"lang-auto\">\nmodelFCNN = NetworkFCNN(R_k_plus_1_train.shape[1], 1)\n\n#weight intizalization routine (Xavier if batch normalisation is there int he netowrk, or else uniform)\nglorot_weight_zero_bias(model=modelFCNN)\n\n\nmodelFCNN.train()\nmodel.eval()\n\nref_train_losses = []\nref_valid_losses = []\nref_valid_score = []\nref_epochs=[]\n\n#start = time.time()\n\n#epoch_iter = tqdm(range(1, HP['NUM_EPOCHS'] + 1))\nepoch_iter =range(1, HP1['NUM_EPOCHS'] + 1)\n\n#for epoch in range(1, HP['NUM_EPOCHS'] + 1):\n\nfor epoch in epoch_iter:   \n    #epoch_iter.set_description('Epoch')\n    \n    ref_epochs.append(epoch)\n    \n    #training over all batch data\n    batch_idx, tloss_avg_ref,vloss_avg_ref = 0, 0,0\n    for batch_idx, (data_rk, data_rk_plus1) in enumerate(train_loader_ref):\n        #print (batch_idx)\n        y_pred_ref = modelFCNN(to_var(data_rk_plus1)) # predict y based on x\n        \n        #print(\"y pred ref is\")\n        #print(y_pred_ref)\n        \n        # concatenate the tensor with the state vector along columns\n        #sysnn_input = np.concatenate( (to_np(data_rk),to_np(y_pred_ref) ),axis=1)\n        sysnn_input = torch.cat( (data_rk,y_pred_ref) ,dim=1)\n        \n        #print (\"print concantenatd input tensor is\")\n        #print(sysnn_input)\n        \n        # now feed in this input to the trained sys NN model\n        output_sys=model(to_var(sysnn_input))\n        \n        #print (\"output of sys NN is \")\n        #print(output_sys)\n        \n        #print (\"data_rk_plus1 is \")\n        #print(data_rk_plus1)\n        \n        #compute the loss \n        loss_FCNN = criterion(output_sys, to_var(data_rk_plus1)) # compute loss  \n        #print (\"loss is\")\n        #print(loss_FCNN)\n\n        optimizer1.zero_grad() # clear gradients\n        loss_FCNN.backward() # compute gradients\n        optimizer1.step() # apply gradients\n\n        tloss_avg_ref += loss_FCNN.item()\n\n    tloss_avg_ref \/= batch_idx+1\n    ref_train_losses.append(tloss_avg_ref)\n        \n    print(\" Epoch : %s , Train loss: %s \" %(epoch,tloss_avg_ref))<\/code>","y":"Thanks for the code.\nUsing your code and just passing some random values into the models, I can get valid gradients in both models:\n<code class=\"lang-python\">model = Network(2, 2)\nmodelFCNN = NetworkFCNN(2, 1)\ncriterion1 = torch.nn.MSELoss()\n    \ndata = torch.randn(1, 2)\ny_pred_ref = modelFCNN(data)\n\n# concatenate the tensor with the state vector along columns\nsysnn_input = torch.cat((torch.rand_like(y_pred_ref), y_pred_ref), dim=1)\n\n# now feed in this input to the trained sys NN model\noutput_sys=model(sysnn_input)\n\n#compute the loss \nloss_FCNN = criterion1(output_sys, data)\n\nloss_FCNN.backward() # compute gradients\n\n# check gradients\nfor name, param in model.named_parameters():\n    print(name, param.grad.abs().sum())\n\nfor name, param in modelFCNN.named_parameters():\n    print(name, param.grad.abs().sum())\n<\/code>\nUnfortunately, your code isn\u2019t executable in the current form, so could you compare my code snippet to your training?","z":"\n\n\n Mayank_Jha:\n\nI have conctenated the output from already trained NN (called \"model \" in the code below),\nwith another data coming in from data loader,\n\n\nThat\u2019s not the case in the posted code snippet.\nYou are directly passing the output (output_sys) of the trained model to the criterion and use this loss to calculate the gradients.\nIn the code snippet the output of modelFCNN, which is supposed to be trained, is concatenated with the data from the DataLoader.\nIs this an error in the code or in the description?\nThank you for the message. Yes, I made error in the description.  I feed in one of the  data from loader into FCNN (to be trained) network . Then ,  concatenate its output with another data from data loader, and feed the concatenated one to the  \u201cmodel\u201d (which is trained). The output from \u201cmodel\u201d is used to calculate error . This error is to be backpropagated through (FCNN).\nAgain, thanks a lot for your precious time here.\nThanks for the update.\nIn that case the code should be working find.\nYou could disable the gradient calculation for model by setting the .requires_grad attribute of all parameters to False, but this shouldn\u2019t change the overall work flow.\nTo make sure the modelFCNN is getting valid gradients, you could check them after the backward() call via:\n<code class=\"lang-python\">for name, param in modelFCNN.named_parameters():\n    print(name, param.grad)\n<\/code>\nIf you are seeing any None gradients, this would mean that this particular parameter doesn\u2019t get valid gradients and there might be a bug in the code.\nthank you.\nIndeed, when I apply the code proposed above by you, I see none in all parameters.  cannot see yet, where shud be a bug.\nScreenshot_2020-07-19 dynamical_sysId_concatenate_FCNN3 - Jupyter Notebook704\u00d7188 3.61 KB with link \"https:\/\/discuss.pytorch.org\/uploads\/default\/original\/3X\/c\/7\/c717500092766edaedc95bf658bfee9b67f093a3.png\"\nthe way I declared my optimizers adn criterion is this :\nplease let me know if I make a mistake here?\n<code class=\"lang-auto\">criterion1 = torch.nn.MSELoss(size_average=False)\noptimizer1 = torch.optim.Adam(modelFCNN.parameters(), \n                            lr=HP1['LEARNING_RATE']) \n                            #momentum=HP['NUM_MOMENTUM'], \n                            #weight_decay=HP['WEIGHT_DECAY'], \n                            #)\nscheduler = torch.optim.lr_scheduler.LambdaLR(optimizer1, lr_lambda=[epoch_smooth_decay])  # select the<\/code>\nfinally, the code for training looks like this. but the loss does not move at all and all parameters show NONe. i have absolutly no idea , what wrong m i doing.\n<code class=\"lang-auto\">modelFCNN = NetworkFCNN(R_k_plus_1_train.shape[1], 1)\n\n#weight intizalization routine (Xavier if batch normalisation is there int he netowrk, or else uniform)\nglorot_weight_zero_bias(model=modelFCNN)\n\ncriterion1 = torch.nn.MSELoss()\noptimizer1 = torch.optim.Adam(modelFCNN.parameters(), \n                            lr=HP1['LEARNING_RATE']) \n                            #momentum=HP['NUM_MOMENTUM'], \n                            #weight_decay=HP['WEIGHT_DECAY'], \n                            #)\n#scheduler1 = torch.optim.lr_scheduler.LambdaLR(optimizer1, lr_lambda=[epoch_smooth_decay])  \n# select the LR schedule\n\n\nmodelFCNN.train()\nmodel.eval()\n\nref_train_losses = []\nref_valid_losses = []\nref_valid_score = []\nref_epochs=[]\n\n#start = time.time()\n\n#epoch_iter = tqdm(range(1, HP['NUM_EPOCHS'] + 1))\nepoch_iter =range(1, HP1['NUM_EPOCHS'] + 1)\n\n#for epoch in range(1, HP['NUM_EPOCHS'] + 1):\n\nfor epoch in epoch_iter:   \n    #epoch_iter.set_description('Epoch')\n    \n    ref_epochs.append(epoch)\n    \n    #training over all batch data\n    batch_idx_ref, tloss_avg_ref,vloss_avg_ref = 0, 0,0\n    for batch_idx_ref, (data_rk, data_rk_plus1) in enumerate(train_loader_ref):\n        #print (batch_idx)\n        y_pred_ref = modelFCNN(to_var(data_rk_plus1)) # predict y based on x\n        \n        \n\n        \n        # concatenate the tensor with the state vector along columns\n        #sysnn_input = np.concatenate( (to_np(data_rk),to_np(y_pred_ref) ),axis=1)\n        sysnn_input = torch.cat( (data_rk,y_pred_ref) ,dim=1)\n\n        \n        # now feed in this input to the trained sys NN model\n        output_sys=model(to_var(sysnn_input))\n\n        \n        #compute the loss \n        loss_FCNN = criterion1(output_sys, to_var(data_rk_plus1)) # compute loss  \n\n\n        optimizer1.zero_grad() # clear gradients\n        loss_FCNN.backward() # compute gradients\n        optimizer1.step() # apply gradients\n\n        tloss_avg_ref += loss_FCNN.item()\n\n    tloss_avg_ref \/= batch_idx+1\n    ref_train_losses.append(tloss_avg_ref)\n        \n    print(\" Epoch : %s , Train loss: %s \" %(epoch,tloss_avg_ref))<\/code>\nI assume you have checked the .grad attributes after the first backward call?\nIf so, could you post an executable minimal code snippet, so that we could debug it?\nhi, thank you very much .\nhere are the essential code with essential functions.\n\ndata and training of \u201cmodel\u201d (no debugging required)\n\n<code class=\"lang-auto\">\nimport matplotlib.pyplot as plt\n#from IPython.display import display, Markdown, Latex\n\nimport numpy as np\nimport math, random\nimport pandas as pd\nfrom pandas import DataFrame\n\nfrom tqdm import tqdm\n\nimport torch\nimport torch.nn as nn\nimport torch.nn.functional as F\nfrom torch.autograd import Variable\nimport torch.nn.init as init\nimport torch.utils.data\nimport torch.optim \nfrom torch.optim import lr_scheduler\n\nfrom sklearn.preprocessing import MinMaxScaler\nfrom sklearn.preprocessing import LabelEncoder\nfrom sklearn.metrics import r2_score\n\nfrom scipy.signal import gausspulse\n%matplotlib notebook\nimport numpy as np\nimport matplotlib.pyplot as plt\n\n#%matplotlib inline\n#import mpld3\n#mpld3.enable_notebook()\n\nJ = 0.001;b = 0.1;K = 0.01;R = 1;L = 0.5;\n#A = [-b\/J   K\/J , \n     #-K\/L   -R\/L];\n#B = [0 1\/L];\n#C = [1   0];D = 0;\n\n# define the continuous-time system matrices\n\nA=np.matrix([[-b\/J, K\/J],[- K\/L, -R\/L]])\nB=np.matrix([[0],[1\/L]])\nC=np.matrix([[1, 0]])\namp=10\n\n#define an initial state for simulation\n#x0=np.random.rand(2,1)\nx0=np.array([[0],[0]])\n\n#define the number of time-samples used for the simulation \n#and the sampling time for the discretization\n\ntime_tot=50\nsampling=0.1\ntime=int(time_tot \/sampling)\n\nmin_u=1\nmax_u=10\nstep_u=1\n\n\n##########define an input sequence for the simulation\n#input voltage\n\ntt= np.arange(0, time, 1)\n\nfreq_sin=0.05\nfreq_sin_val=0.05\n\n\nin_seq=1*np.sin((freq_sin)*tt)\n#in_seq=amp*np.ones(time)\n\n#input_seq=np.random.rand(time,1)\n#in_seq=amp*np.random.rand(time,1)\n#in_seq=amp*np.ones(time)\n\n#print(in_seq)\n%matplotlib notebook\nplt.plot(in_seq)\nplt.title(\"in seq chosen for system simulation\")\nplt.show()\n\n0#####calculate the discrete system using Euler mathod\n# the following function simulates the state-space model using the backward Euler method\n# the input parameters are:\n#    -- A,B,C              - continuous time system matrices \n#    -- initial_state      - the initial state of the system \n#    -- time_steps         - the total number of simulation time steps \n#    -- sampling_perios    - the sampling period for the backward Euler discretization \n# this function returns the state sequence and the output sequence\n# they are stored in the vectors Xd and Yd respectively\n\ndef simulate(A,B,C,initial_state,input_sequence, time_steps,sampling_period):\n    from numpy.linalg import inv\n    \n    I=np.identity(A.shape[0]) # this is an identity matrix\n    Ad=inv(I-sampling_period*A)\n    Bd=Ad*sampling_period*B\n    \n    Xd=np.zeros(shape=(A.shape[0],time_steps+1))\n    Yd=np.zeros(shape=(C.shape[0],time_steps+1))\n    \n    for i in range(0,time_steps):\n       if i==0:\n           Xd[:,[i]]=initial_state\n           Yd[:,[i]]=C*initial_state\n           x=Ad*initial_state+Bd*input_sequence[i]\n       else:\n           Xd[:,[i]]=x\n           Yd[:,[i]]=C*x\n           x=Ad*x+Bd*input_sequence[i]\n    Xd[:,[-1]]=x\n    Yd[:,[-1]]=C*x\n    return Xd, Yd\n    \nstate,output=simulate(A,B,C,x0,in_seq, time ,sampling)    \nprint(state.shape)\n%matplotlib notebook\nplt.figure(2)\nplt.title(\"state trajectory with chosen input\")\nplt.plot(state[1])\nplt.show()\n#XXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXX   Training data XXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXX\n\n######################## creat stacked input profiles #####################\ninput_mag_train=np.arange(min_u,max_u,step_u)\nu_len_train=input_mag_train.shape[0]\nprint(\"u_len_train is:\" ), print(u_len_train)\n\ntt= np.arange(0, time, 1)\n\n\nu_train=[]\nfor i in input_mag_train:\n    #u1_train=i*np.ones(time)\n    u1_train=i*np.sin((freq_sin)*tt)\n    #print(u1_train)\n    u_train.append(u1_train)\nu_array_train=np.array(u_train)\n####################################################################\nprint(\"u train shape is:\"),print(u_array_train.shape)\nprint(\"u train is:\"),print(u_array_train)\n\n\n\n\n############################creat statcked state data ################################\n#state_train=[]\n#output=[]\nss_train=[]\n\nfor j in range(0,u_array_train.shape[0]):\n    \n    #choose the input profile\n    #in_seq=amp*np.ones(time)\n    input_seq_train=u_array_train[j,:]\n    \n    #intial condtions\n    x0=np.array([[0],[0]])\n    x0_train=x0\n    \n    state_train,output=simulate(A,B,C,x0_train,input_seq_train, time ,sampling)\n    #state_train.append(state_train)\n    #output=output.append(output)\n    #print(\"state train is: \"),print(state_train)\n    state_train.tolist()\n    #print(\"ss is:\"),print(ss)\n    ss_train.append(state_train)\n    \n    #print (\"appedde list is:\", ss )\n\n#convert the list into array\nss_train_array=np.array(ss_train)\n#print('ss array train is  dim:(u_len, no of states, time length+1)): '),print(ss_train_array)\n\nm1=ss_train_array[0,0,:].reshape(-1,1)\nm2=ss_train_array[0,1,:].reshape(-1,1)\nm12=np.concatenate((m1,m2),axis=1)\n#print(\"m12 at begining :\"),print(m12)\nfor j in range(1,u_array_train.shape[0]):\n    a1=ss_train_array[j,0,:].reshape(-1,1)\n    a2=ss_train_array[j,1,:].reshape(-1,1)\n    a12=np.concatenate((a1,a2),axis=1)\n    m12=np.concatenate((m12,a12),axis=0)\n    #print(\"a12 is :\"),print(a12)\nprint(\"m12 is :\"),print(m12)\n# m12 is the state vector cocatenated and arranged as desired \n# fiirst colum is x1, coulomn 2 is x2, and column 3 is input trajectory\n\n#m12 is the x_var with column 1 and 2. \n# the data must be transformed here (standardised)\n#the scaler is defined and fit (one and one time only) to the train set\n#this same scaler should be fit on all future datasets including the test and validation \n\n#defining the scaler\nscaler=MinMaxScaler(feature_range=(-1,1))\n# fit and transform the X-train set\nm12=scaler.fit_transform(m12)\n\n\n#cocatenate zeros in fron of u vector \nzeros_to_add=np.zeros(shape=(int((max_u-min_u)\/step_u),1 ))\num=np.concatenate((zeros_to_add,u_array_train),axis=1)\n# cretae the colum vector of control inputs.\ncc=um.reshape(-1,1)\n\n\nprint (\"u array train is\", u_array_train)\n#print (\"u array train transformed is\",scaler.transform(u_array_train))\n\n\n# concatenate with the state vector\nX_train=np.concatenate((m12,cc),axis=1)\n\n\n\n# now, use X-train to creat Y-train or target for the NN\n\n#create Y-train from here\n# tot hat end, first remove the first row of X-train\ntest= np.delete(X_train, 0, axis=0)\n#then remove the 3rd column of X_train\nY_train1= np.delete(test, 2, axis=1)\n#fnally to assure that train and test set have equal length, remove the last row of X-train\nX_train1= np.delete(X_train, (X_train.shape[0]-1), axis=0)\n\nprint(\"X_train1 shape is :\"), print(X_train1.shape)\nprint(\"Y_train1 shape is :\"), print(Y_train1.shape)\n\nprint(\"X_train1:\"), print(X_train1)\nprint(\"Y_train1 :\"), print(Y_train1)\n\n%matplotlib notebook\nplt.figure()\nplt.title(\"training state trajectory with chosen input\")\nplt.plot(X_train1)\nplt.show()\n\n\n\n#hyper parameters for NN network\n#setting the hyperparameters\nHP = {\n    #'NUM_HIDDEN_NODES'    : 10 ,\n    #'NUM_EXAMPLES'        : 5 ,\n    'TRAIN_SPLIT'         : .8 ,\n    'MINI_BATCH_SIZE'     : 500,#10*int((2\/3)*(u_array_train.shape[1]+1)\/4) ,\n    'NUM_EPOCHS'          : 300,\n    'LEARNING_RATE'       : 5e-3 ,\n    'LEARNING_RATE_DECAY' : 500 ,\n    'WEIGHT_DECAY'        : 5e-4 ,\n    'NUM_MOMENTUM'        : 0.9 ,\n    'NUM_PATIENCE'        : 50 ,\n    'SEED'                : 2018\n}\nnp.random.seed(HP['SEED'])\ntorch.manual_seed(HP['SEED'])\n\n#some essential fucntions that are used throughout\ndef to_np(x):\n    return x.data.cpu().numpy()\n\ndef to_var(x, async=False):\n    if torch.cuda.is_available():\n        x = x.cuda(async=async)\n    return Variable(x)\n\n####### XXXXXXXXXXXXXXXXXX  Test data  XXXXXXXXXXXXXXXXXXXXXXXXXXXXX\n######################## creat stacked input profiles #####################\nfreq_sin_test=0.05\n\n\nmin_u_test=1\nmax_u_test=2\nstep_u_test=1\n\ninput_mag_test=np.arange(min_u_test,max_u_test,step_u_test)\nu_len_test=input_mag_test.shape[0]\n\n\n\n#input_mag_test= np.arange(0, time, sampling)\n#u_len_test=input_mag_test.shape[0]\nu_test=[]\n#time axis for sinus wave creat\ntt= np.arange(0, time, 1)\n\nfor i in input_mag_test:\n    #u1_test=i*(np.cos((freq_sin_test)*tt)*np.sin((freq_sin_test)*tt))\n    #u1_test=i*np.ones(time)\n    #u1_test=i*np.sin((0.1)*input_mag_test)\n    #u1_test=i*np.random.rand(time,1)\n    u1_test=i*np.sin((freq_sin_test)*tt)\n    u_test.append(u1_test)\n\nu_array_test=np.array(u_test)\n#print('uarray is'),print(u_array_test)\n####################################################################\n\n\n#######creat statcked state data ################################\n#state_train=[]\n#output=[]\nss_test=[]\n\nfor j in range(0,u_array_test.shape[0]):\n    \n    #choose the input profile\n    #in_seq=amp*np.ones(time)\n    input_seq_test=u_array_test[j,:]\n    \n    #intial condtions\n    x0=np.array([[0],[0]])\n    x0_test=x0\n    \n    state_test,output=simulate(A,B,C,x0_test,input_seq_test, time ,sampling)\n    #state_train.append(state_train)\n    #output=output.append(output)\n    #print(\"state train is: \"),print(state_train)\n    state_test.tolist()\n    #print(\"ss is:\"),print(ss)\n    ss_test.append(state_test)\n    \n    #print (\"appedde list is:\", ss )\n\n#convert the list into array\nss_test_array=np.array(ss_test)\n#print('array ssnp is  dim:(u_len, no of states, time length+1)): '),print(ss_test_array)\n\n\n#### creating the cocatenated arrays for vlaidation data\n\nm1=ss_test_array[0,0,:].reshape(-1,1)\nm2=ss_test_array[0,1,:].reshape(-1,1)\nm12t=np.concatenate((m1,m2),axis=1)\n#print(\"m12 at begining :\"),print(m12)\n\nfor j in range(1,u_array_test.shape[0]):\n    a1=ss_test_array[j,0,:].reshape(-1,1)\n    a2=ss_test_array[j,1,:].reshape(-1,1)\n    a12=np.concatenate((a1,a2),axis=1)\n    m12t=np.concatenate((m12t,a12),axis=0)\n    #print(\"a12 is :\"),print(a12)\n    #print(\"m12 is :\"),print(m12)\n# m12 is the state vector cocatenated and arranged as desired \n# fiirst colum is x1, coulomn 2 is x2, and column 3 is input trajectory\n\n#m12t is the x_var with column 1 and 2. \n# the data must be transformed here (standardised)\n#the scaler is defined and fit (one and one time only) to the train set\n#this same scaler should be fit on all future datasets including the test and validation \n\n\n# fit and transform the X-val set\nm12t=scaler.transform(m12t)\n\n\n\n\n\n#cocatenate zeros in fron of u vector\nzeros_to_add_test=np.zeros(shape=(int((max_u_test-min_u_test)\/step_u_test),1 ))\num=np.concatenate((zeros_to_add_test,u_array_test),axis=1)\n# cretae the colum vector of control inputs.\ncc=um.reshape(-1,1)\n\n# concatenate with the state vector\nX_test=np.concatenate((m12t,cc),axis=1)\n\n#create Y-train from here\n# tot hat end, first remove the first row of X-train\ntest= np.delete(X_test, 0, axis=0)\n#then remove the 3rd column of X_train\nY_test1= np.delete(test, 2, axis=1)\n#fnally to assure that train and test set have equal length, remove the last row of X-train\nX_test1= np.delete(X_test, (X_test.shape[0]-1), axis=0)\n\nprint(\"X_test1 shape is :\"), print(X_test1.shape)\nprint(\"Y_test1 shape is :\"), print(Y_test1.shape)\n       \n\n%matplotlib notebook\nplt.figure()\nplt.title(\"test X trajectory with chosen input\")\nplt.plot(X_test1)\nplt.show()\n    \nplt.figure()\nplt.title(\"test Y trajectory with chosen input\")\nplt.plot(Y_test1)\nplt.show()\n    \n    \n#print (\"ssnp_val was:\"), print(ss_val_array)        \n#print(\"X_test is:\"),print(X_test1)\n#print(\"Y test is :\"),print(Y_test1) \n#print(X_test1.shape),print(Y_test1.shape)\n\n\n\n# finally construct the test oader for pytorch\n#test_x, test_y=x_data[test_idx],y_data[test_idx]\ntest_set=torch.utils.data.TensorDataset(torch.FloatTensor(X_test1),torch.FloatTensor(Y_test1))\ntest_loader=torch.utils.data.DataLoader(test_set,batch_size=HP['MINI_BATCH_SIZE'],\n                                        shuffle=False,\n                                        pin_memory=True, num_workers=0)\n\n\n# creat training set compising of both x and y\ntrain_set=torch.utils.data.TensorDataset(torch.FloatTensor(X_train1),torch.FloatTensor(Y_train1))\ntrain_loader=torch.utils.data.DataLoader(train_set,batch_size=HP['MINI_BATCH_SIZE'],\n                                         shuffle=True,pin_memory=True,num_workers=0)\n\n\n\n\n#build neural network strucutre\nclass Network(nn.Module):\n    def __init__(self,D_in,D_out):\n        super().__init__()\n        \n        # Inputs to hidden layer linear transformation\n        self.lin1 = nn.Linear(D_in, 100)\n        self.lin2=nn.Linear(100,100)\n        self.lin3=nn.Linear(100,100)\n        # Output layer,\n        self.output = nn.Linear(100, D_out)\n        \n        # Define sigmoid activation and softmax output \n        #self.tanh = F.tanh()\n        \n        \n    def forward(self, x):# this is where the data flows in the network, respecting \n                         #sequence of layers in forward method is very important.\n        # Pass the input tensor through each of our operations\n        \n        x = self.lin1(x)\n        x = F.tanh(x)\n        \n        x = self.lin2(x)\n        x = F.tanh(x)\n        \n        x = self.lin3(x)\n        x = F.tanh(x)\n        \n        x = self.output(x)\n        y = F.tanh(x)\n        \n        return y\n\nmodel = Network(X_train1.shape[1], Y_train1.shape[1])\n\ncriterion = torch.nn.MSELoss(size_average=False)\noptimizer = torch.optim.SGD(model.parameters(), \n                            lr=HP['LEARNING_RATE']) \n                            #momentum=HP['NUM_MOMENTUM'], \n                            #weight_decay=HP['WEIGHT_DECAY'], \n                            #)\nmodel.train()\n\ntrain_losses = []\nvalid_losses = []\nvalid_score = []\nepochs=[]\n\n#start = time.time()\n\n#epoch_iter = tqdm(range(1, HP['NUM_EPOCHS'] + 1))\nepoch_iter =range(1, HP['NUM_EPOCHS'] + 1)\n\n#for epoch in range(1, HP['NUM_EPOCHS'] + 1):\n\nfor epoch in epoch_iter:   \n    #epoch_iter.set_description('Epoch')\n    \n    epochs.append(epoch)\n    \n    #training over all batch data\n    batch_idx, tloss_avg,vloss_avg = 0, 0,0\n    for batch_idx, (data, target) in enumerate(train_loader):\n        y_pred = model(to_var(data)) # predict y based on x\n        \n        #print(\"y pred at epoch number %s is: %s\" %(epoch,y_pred)),#, print(y_pred)\n        #print(\"target is:\"), print(to_var(target))\n        loss = criterion(y_pred, to_var(target)) # compute loss\n\n        optimizer.zero_grad() # clear gradients\n        loss.backward() # compute gradients\n        optimizer.step() # apply gradients\n\n        tloss_avg += loss.item()\n\n    tloss_avg \/= batch_idx+1\n    train_losses.append(tloss_avg)\n    print(\" Epoch : %d , Train loss: %d \" %(epoch,tloss_avg))   \n    \n    \n#####################   test the trained model   ########################\nmodel.eval()\ny_pred = []\nfor data, target in test_loader:\n    \n    output = model(to_var(data))\n    to_np(output)\n    #print (\"another\")\n    y_pred.append(to_np(output))\n\ny_pred = np.array(y_pred)\n\n\n# intialize and put all the componets of y_pred in a \n# single 2 column array\n\n# intializing\nY_pred1=y_pred[0]\n\nfor i in range (1,y_pred.shape[0]):\n    Y_pred1=np.concatenate((Y_pred1,y_pred[i]),axis=0)\nprint(\"Y_pred1 shape is :\",Y_pred1.shape)\n\n\n####### plotting the figure ###########\n%matplotlib notebook\nplt.figure(figsize=(7,6))\nplt.plot(Y_test1[:,0],label='Test speed', color='r')\nplt.plot(Y_pred1[:,0],label='Predicted speed', color='k')\n\nplt.plot(Y_pred1[:,1],label='Predicted current', color='g')\nplt.plot(Y_test1[:,1],label='Test current', color='b')\n\nplt.title(\"Prediction and True Values\")\nplt.xlabel(\"Indexes\");\nplt.ylabel(\"Amplitude\");\nplt.grid(True)\nplt.legend(loc='upper right');\n\nfrom numpy import linalg as LA\nerror=Y_pred1-Y_test1\n#print(error)\n# this is the measure of the prediction performance in percents\nerror_percentage=LA.norm(error,2)\/LA.norm(Y_test1,2)*100\nprint(\"Error percentage is\",error_percentage)\n<\/code>\nthen, data generation for training of \u201cmodelFCNN\u201d\n<code class=\"lang-auto\">################## training FCNN using trained NN \"model\" ############################\n################ this section describes data preparation, \n################ should be skipped for debugging        #######################\n\n\n\n%matplotlib notebook\nimport numpy as np\nimport matplotlib.pyplot as plt\n\n\n#first prepare the u_ref to generate r_k+1 and r_k\n\nmin_u2_train   = 1\nmax_u2_train   = 3\nstep_u2_train  = 1\n\nref_mag_train   =  np.arange(min_u2_train,max_u2_train,step_u2_train)\nref_u_len       =  ref_mag_train.shape[0]\nprint(ref_u_len)\n\nref_tot_time = 60\nref_time     =  int(ref_tot_time \/0.1)# sampling)\n\nref_tt       =  np.arange(0, ref_time, 1)\nref_freq_sin =  0.03\n\nref_u_train  = []\n\nfor i in ref_mag_train:\n    #u1_train=i*np.ones(ref_tt)\n    ref_u1_train=i*np.sin((ref_freq_sin)*ref_tt)\n    #print(u1_train)\n    ref_u_train.append(ref_u1_train)\nref_u_array_train=np.array(ref_u_train)\n####################################################################\n#print(\"ref_u_train is %s\" %(ref_u_array_train))\n\nprint(\"ref_time is %s\" %(ref_time))\n#\n\n###########################creat statcked state data ################################\nref_ss_train=[] #ref ss is the r_k  and r_k+1 will be created from it later. \n\nfor j in range(0,ref_u_array_train.shape[0]):\n    \n    #choose the input profile\n    #in_seq=amp*np.ones(time)\n    input_seq_train=ref_u_array_train[j,:]\n    \n    #intial condtions\n    x0=np.array([[0],[0]])\n    x0_train=x0\n    \n    state_train,output=simulate(A,B,C,x0_train,input_seq_train, ref_time ,sampling)\n    #state_train.append(state_train)\n    #output=output.append(output)\n    #print(\"state train is: \"),print(state_train)\n    state_train.tolist()\n    #print(\"ss is:\"),print(ss)\n    ref_ss_train.append(state_train)\n    \n    #print (\"appedde list is:\", ss )\n\n#convert the list into array\nref_ss_train_array=np.array(ref_ss_train)\n#print('array ssnp is  dim:(u_len, no of states, time length+1)): '),print(ss_train_array)\n\n#print (\"ref_ss_train_array is %s\" \n#      %(ref_ss_train_array))\n\n##################### DATA FORMATTING for ref_X-train (r_k) and ref_Y-train (r_k+1)##########################\n#here, the -previously cre\nm1=ref_ss_train_array[0,0,:].reshape(-1,1)\nm2=ref_ss_train_array[0,1,:].reshape(-1,1)\nm12r=np.concatenate((m1,m2),axis=1)\n#print(\"m12r at begining :\"),print(m12r)\nfor j in range(1,ref_u_array_train.shape[0]):\n    a1=ref_ss_train_array[j,0,:].reshape(-1,1)\n    a2=ref_ss_train_array[j,1,:].reshape(-1,1)\n    a12=np.concatenate((a1,a2),axis=1)\n    m12r=np.concatenate((m12r,a12),axis=0)\n    #print(\"a12 is :\"),print(a12)\n\n# fit and transform the X-train set\nR_k=scaler.transform(m12r)\n\n\n#create Y-train from here\n# tot that end, first remove the first row of X-train\n\nR_k_plus_1_train     =    np.delete(R_k, 0, axis=0)\n\n#print (\"R_k_plus1_train is %s\" %(R_k_plus_1_train))\n\n#fnally to assure that train and test set have equal length, remove the last row of R_k\nR_k_train= np.delete(R_k, (R_k.shape[0]-1), axis=0)\n\n#print (\"R_k train is %s\" %(R_k_train))\nprint(\"R_k_train shape is :\"), print(R_k_train.shape)\nprint(\"R_k_plus1_train shape is :\"), print(R_k_plus_1_train.shape)\n\n\n%matplotlib notebook\nplt.figure(103)\nplt.plot(R_k_train,label='Rk train train', color='r')\nplt.plot(R_k_plus_1_train,label='rk +1 train', color='b')\n#plt.plot(X_test1[:,2],label='command test', color='k')\nplt.title(\"rk and rk+1 train data\")\nplt.xlabel(\"time indexes\");\nplt.ylabel(\"amplitude\");\nplt.grid(True)\nplt.legend(loc='upper right');\nplt.show()\n\n<\/code>\nthen, hyper parameters and sructure of to be trained model FCNN\n<code class=\"lang-auto\">\n####################### declaring NN hyper parameters and NN strucutre\nHP1 = {\n    #'NUM_HIDDEN_NODES'    : 10 ,\n    #'NUM_EXAMPLES'        : 5 ,\n    'TRAIN_SPLIT'         : .8 ,\n    'MINI_BATCH_SIZE'     : 500,#10*int((2\/3)*(u_array_train.shape[1]+1)\/4) ,\n    'NUM_EPOCHS'          : 100,\n    'LEARNING_RATE'       : 5e-2 ,\n    'LEARNING_RATE_DECAY' : 500 ,\n    'WEIGHT_DECAY'        : 5e-4 ,\n    'NUM_MOMENTUM'        : 0.9 ,\n    'NUM_PATIENCE'        : 50 ,\n    'SEED'                : 2018\n}\n\n\n# creat FCNN network and define all other parameters\n\n#build neural network\nclass NetworkFCNN(nn.Module):\n    def __init__(self,D_in,D_out):\n        super().__init__()\n        \n        # Inputs to hidden layer linear transformation\n        self.lin1 = nn.Linear(D_in, 100)\n        self.lin2=nn.Linear(100,100)\n        self.lin3=nn.Linear(100,100)\n        # Output layer,\n        self.output = nn.Linear(100, D_out)\n        \n        # Define sigmoid activation and softmax output \n        #self.tanh = F.tanh()\n        \n        \n    def forward(self, x):# this is where the data flows in the network, respecting \n                         #sequence of layers in forward method is very important.\n        # Pass the input tensor through each of our operations\n        \n        x = self.lin1(x)\n        x = F.relu(x)\n        \n        x = self.lin2(x)\n        x = F.relu(x)\n        \n        x = self.lin3(x)\n        x = F.relu(x)\n        \n        x = self.output(x)\n        y = F.tanh(x)\n        \n        return y\n\n# creat training set compising of both x and y\ntrain_set_ref=torch.utils.data.TensorDataset(torch.FloatTensor(R_k_train),torch.FloatTensor(R_k_plus_1_train))\ntrain_loader_ref=torch.utils.data.DataLoader(train_set_ref,batch_size=HP1['MINI_BATCH_SIZE'],\n                                         shuffle=True,pin_memory=True,num_workers=0)\n\n\n################ finally training the FCNN using the \n#################output from already trained \"model\" #########################\nmodelFCNN = NetworkFCNN(R_k_plus_1_train.shape[1], 1)\n\n\n\ncriterion1 = torch.nn.MSELoss()\noptimizer1 = torch.optim.Adam(modelFCNN.parameters(), \n                            lr=HP1['LEARNING_RATE']) \n                            #momentum=HP['NUM_MOMENTUM'], \n                            #weight_decay=HP['WEIGHT_DECAY'], \n                            #)\n#scheduler1 = torch.optim.lr_scheduler.LambdaLR(optimizer1, lr_lambda=[epoch_smooth_decay])  \n# select the LR schedule\n\n\n\n\n<\/code>\nfinally, the part that needs debugging, taining of FCNN using already trained \u201cmodel\u201d\n<code class=\"lang-auto\">modelFCNN.train()\nmodel.eval()\n\nref_train_losses = []\nref_valid_losses = []\nref_valid_score = []\nref_epochs=[]\n\n#start = time.time()\n\n#epoch_iter = tqdm(range(1, HP['NUM_EPOCHS'] + 1))\nepoch_iter =range(1, HP1['NUM_EPOCHS'] + 1)\n\n#for epoch in range(1, HP['NUM_EPOCHS'] + 1):\n\nfor epoch in epoch_iter:   \n    #epoch_iter.set_description('Epoch')\n    \n    ref_epochs.append(epoch)\n    \n    #training over all batch data\n    batch_idx_ref, tloss_avg_ref,vloss_avg_ref = 0, 0,0\n    for batch_idx_ref, (data_rk, data_rk_plus1) in enumerate(train_loader_ref):\n        #print (batch_idx)\n        y_pred_ref = modelFCNN(to_var(data_rk_plus1)) # predict y based on x\n        \n        \n\n        \n        # concatenate the tensor with the state vector along columns\n        #sysnn_input = np.concatenate( (to_np(data_rk),to_np(y_pred_ref) ),axis=1)\n        sysnn_input = torch.cat( (data_rk,y_pred_ref) ,dim=1)\n\n        \n        # now feed in this input to the trained sys NN model\n        output_sys=model(to_var(sysnn_input))\n\n        \n        #compute the loss \n        loss_FCNN = criterion1(output_sys, to_var(data_rk_plus1)) # compute loss  \n\n\n        optimizer1.zero_grad() # clear gradients\n        loss_FCNN.backward() # compute gradients\n        optimizer1.step() # apply gradients\n\n        tloss_avg_ref += loss_FCNN.item()\n\n    tloss_avg_ref \/= batch_idx+1\n    ref_train_losses.append(tloss_avg_ref)\n        \n    print(\" Epoch : %s , Train loss: %s \" %(epoch,tloss_avg_ref))\n<\/code>\nmorever, i have NOT explicitely put\n<code class=\"lang-auto\">requires_grad=True\n<\/code>\nwith any input, I assume it is so by default, when passing as Variable?\nThanks for the code.\nUsing your code and just passing some random values into the models, I can get valid gradients in both models:\n<code class=\"lang-python\">model = Network(2, 2)\nmodelFCNN = NetworkFCNN(2, 1)\ncriterion1 = torch.nn.MSELoss()\n    \ndata = torch.randn(1, 2)\ny_pred_ref = modelFCNN(data)\n\n# concatenate the tensor with the state vector along columns\nsysnn_input = torch.cat((torch.rand_like(y_pred_ref), y_pred_ref), dim=1)\n\n# now feed in this input to the trained sys NN model\noutput_sys=model(sysnn_input)\n\n#compute the loss \nloss_FCNN = criterion1(output_sys, data)\n\nloss_FCNN.backward() # compute gradients\n\n# check gradients\nfor name, param in model.named_parameters():\n    print(name, param.grad.abs().sum())\n\nfor name, param in modelFCNN.named_parameters():\n    print(name, param.grad.abs().sum())\n<\/code>\nUnfortunately, your code isn\u2019t executable in the current form, so could you compare my code snippet to your training?\nhi, thank you for you snippet code. In fact I removed Variable (x) from each line and I get gradients as well as it is training well.\nthank you again, very much for having spent time on my code and for your precious help\nregards and thanks."},{"x":"I am doing some transfer learning, and set a function inside my NN class as so:\n<code class=\"lang-auto\">    def freeze(self):    \n        for name, child in self.arch.named_children():\n            if name != \"_fc\":\n                for param in child.parameters():\n                    param.requires_grad = False\n        for name, param in self.arch.named_parameters():\n            if(\"bn\" in name):\n                param.requires_grad = True\n<\/code>\nProbably a bit convoluted I know, but I was basically just wanting to freeze everything except all BatchNorms and my classifier.\nI instantiate my model like so:\n<code class=\"lang-auto\">   arch = EfficientNet.from_pretrained(param['arch'])   \n    model = Net(arch=arch, n_meta_features=len(meta_features))  # New model for each fold \n    model.freeze()\n    model = nn.DataParallel(model)\n    model = model.to(device)\n<\/code>\nI think I must be doing something wrong however, because I would have suspected my optimizer to throw an error, as i was calling it like so:\n<code class=\"lang-auto\">    optim = torch.optim.AdamW(model.parameters(), lr=param['lr'], amsgrad=True) \n<\/code>\ninstead of like so:\n<code class=\"lang-auto\">    optim = torch.optim.AdamW(filter(lambda p: p.requires_grad, net.parameters()), lr=param['lr'], amsgrad=True) \n\n<\/code>\nas it\u2019s my understanding that the optimizer will throw an error if it encounters frozen parameters.  I also noticed that my training time (about 1min 47sec per epoch) is essentially unchanged as well, and I would have suspected it to train faster.\nIs there something I am doing wrong?","y":"Assuming that you set all the requires_grad flags before training at all, there is very little difference in using filter.\nIn the optimizer code, it iterates over each parameter and if the gradient is None, it continues. If requires_grad=False, the parameter\u2019s gradient will never be updated thus it will always remains None. For this example, I think filter adds very little performance boost.","z":"Hi,\nI believe that the optimizer assigns None to all those places were you have set requires_grad=False. As a result, the gradients of all those parameters are not updating. An optimizer works with frozen params even when you do optim = torch.optim.AdamW(model.parameters(), lr=param['lr'], amsgrad=True),\nas essentially, the params have just requires_grad set as False. My suggestion is unfreeze the params and you will see a difference in training time \nFor more info, check out this with link \"https:\/\/discuss.pytorch.org\/t\/trying-to-understand-optimizer-and-relation-to-requires-grad\/7994\"\nIf the optimizer is smart enough so that it looks at the requires_grad, why do people pass lambda functions into the optimizer, why not just pass in model.parameters()?\nAs for my training time, because I was setting all the BatchNorm to be unfrozen, and those are scattered literally all throughout the model, backdrop still had to be done, even though it wasn\u2019t updating the parameters, since everything with backpropogation is chained\/dependent.  I tried to just freeze the classifier and saw a difference in speed, but since the actual updates to the weights is super quick, its the actual calculations that take all the time, that is why I wasn\u2019t seeing a speed increase.\nSo I guess I am still unclear though.  Is there any difference or benefit to passing in model.parameters() vs passing in filter(lambda p: p.requires_grad, model.parameters()) when you have parts of your model frozen?\nAssuming that you set all the requires_grad flags before training at all, there is very little difference in using filter.\nIn the optimizer code, it iterates over each parameter and if the gradient is None, it continues. If requires_grad=False, the parameter\u2019s gradient will never be updated thus it will always remains None. For this example, I think filter adds very little performance boost.\nThank you, it makes sense.  Perhaps the behavior of at least some optimizers was different in versions past?  I had read that the optimizer would throw an error if it encountered frozen parameters, that is why I was concerned that perhaps my parameters were not set right.\nsame question in\n\n\n\n\nParameters with requires_grad = False are updated during training with link \"https:\/\/discuss.pytorch.org\/t\/parameters-with-requires-grad-false-are-updated-during-training\/90096\" autograd with link \"\/c\/autograd\/7\"\n\n\n    Hello. I\u2019am trying to freeze front layers during training. \nBefore starting optimization, the optimizer is constructed by \noptimizer = torch.optim.SGD(net.parameters(), lr, ...)\n\nThen, during training, i changed the front layers\u2019 requires_grad=False. \nSpecifically, \nfor epoch in range(total_epoch):\n  if epoch == freeze_epoch:\n    net.conv1.weight.requires_grad = False\n  train_one_epoch() #update network in an epoch\n\n\nHowever, I found that the weights of the front layers are still updated. \nI als\u2026\n  \n\n\nmomentum, weight decay affect updating params with requires_grad=False\nThanks, so it seems the lambda function has merits."},{"x":"I am wondering what is the right way to use a sampler like WeightedRandomSampler for imbalanced classification problems.\nSpecifically, I am unclear as to whether I only use sampling during:\n\nTraining\nTraining + Validation\nTraining + Validation + Testing (whereby each gets its own sampler to capture the distribution in its respective data set)\n\nI\u2019ve poked through several threads and noticed people using it for training only and training + validation.\nThanks for your time and help!","y":"I think the common use case would be to use it during the training only.\nThe validation dataset should act as a good proxy for the final model performance on the unseen test data. So I would try to keep the data distribution of the validation and test datasets as close as possible.\nSince your test dataset is also imbalanced, using weighted sampling on it might not give you a proper signal how your model would perform on real world data (which should also be imbalanced).\nHowever, that\u2019s my biased opinion so let\u2019s wait for some other opinions. ","z":"I think the common use case would be to use it during the training only.\nThe validation dataset should act as a good proxy for the final model performance on the unseen test data. So I would try to keep the data distribution of the validation and test datasets as close as possible.\nSince your test dataset is also imbalanced, using weighted sampling on it might not give you a proper signal how your model would perform on real world data (which should also be imbalanced).\nHowever, that\u2019s my biased opinion so let\u2019s wait for some other opinions. \nThank you for always responding. This makes a lot of sense!"},{"x":"I want to train from scratch the AlexNet model:\n<code class=\"lang-auto\">model = models.alexnet(pretrained=False)\nnum_features = model.classifier[6].in_features\nfeatures = list(model.classifier.children())[:-1] # Remove last layer\nfeatures.extend([nn.Linear(num_features, 4)]) # Add our layer with 4 outputs\nmodel.classifier = nn.Sequential(*features) # Replace the model classifier\n\nfor param in model.features.parameters():\n    param.require_grad = True\n<\/code>\nI want to save the parameters\/hyperparmeters (weights, bias, model values\/structure etc.).\nThis is the training function that I am using:\n<code class=\"lang-auto\">def train(model, dataloaders, criterion1, criterion2, optimizer, num_epochs):\n    since = time.time()\n    train_loss = []\n    val_acc_history = []\n    best_model_wts = copy.deepcopy(model.state_dict())\n    best_acc = 0.0\n    for epoch in range(num_epochs):\n        print('Epoch {}\/{}'.format(epoch + 1, num_epochs))\n        print('-' * 10)\n        for phase in ['train', 'validation']:\n            if phase == 'train':\n                model.train() \n            else:\n                model.eval()  \n            running_loss = 0.0\n            running_corrects = 0 \n            for images, target in dataloaders[phase]:\n                images = images.to(device)\n                labels = target['label'].to(device)\n                norm = target['norm_box'].to(device)\n                \n                optimizer.zero_grad()\n                with torch.set_grad_enabled(phase == 'train'):\n                    out_bbox = model(images)\n                    loss_bbox = criterion2(out_bbox, norm)\n                    loss = loss_bbox\n\n                    if phase == 'train':\n                        loss.backward()\n                        optimizer.step()\n\n                running_loss += loss.item() * images.size(0)\n                running_corrects += IoU(norm, out_bbox, 0.3, batch_size)\n\n            epoch_loss = running_loss \/ len(dataloaders[phase].dataset)\n            if phase == 'train':    \n                train_loss.append(epoch_loss)\n            epoch_acc = running_corrects \/ len(dataloaders[phase].dataset)\n\n            print('{} Loss: {:.4f} Acc: {:.4f}'.format(phase, epoch_loss, epoch_acc))\n            \n            if phase == 'validation' and epoch_acc > best_acc:\n                best_acc = epoch_acc\n                best_model_wts = copy.deepcopy(model.state_dict())\n            if phase == 'validation':\n                val_acc_history.append(epoch_acc)\n\n        print()\n    time_elapsed = time.time() - since\n    print('Training complete in {:.0f}m {:.0f}s'.format(time_elapsed \/\/ 60, time_elapsed % 60))\n    print('Best val Acc: {:4f}'.format(best_acc))\n    torch.save(best_model_wts, 'weights\/best_wts.pth')\n    return model, train_loss, val_acc_history\n<\/code>\nWhen I load the state_dict:\n<code class=\"lang-auto\">wts = model.load_state_dict(torch.load('weights\/best_wts.pth))\n<\/code>\nI get the message:\n<code class=\"lang-auto\">IncompatibleKeys(missing_keys=[], unexpected_keys=[])\n<\/code>\nAnd when trying to look through the state_dict:\n<code class=\"lang-auto\">print(list(wts.keys()))\n<\/code>\nI get the error:\n<code class=\"lang-auto\">AttributeError: 'IncompatibleKeys' object has no attribute 'keys'\n<\/code>\nAny advice would be greatly appreciated.","y":"Your current code will only save the model.state_dict, i.e. all parameters of your model.\nThe model structure will not be saved, so you should always keep the model definition close to the checkpoint.\nAlso, if you would like to use this checkpoint for finetuning, I would recommend to store the optimizer.state_dict(), since some optimizers have internal parameters (e.g. Adam), which would be otherwise reset to their default values.\nHave a look at the ImageNet example with link \"https:\/\/github.com\/pytorch\/examples\/blob\/1de2ff9338bacaaffa123d03ce53d7522d5dcc2e\/imagenet\/main.py#L249\" to see, how the checkpoint is created.","z":"This message just states, that no incompatible keys were found, so you are good to go.\nAs this message might be confusing, we have a discussion here with link \"https:\/\/github.com\/pytorch\/pytorch\/issues\/20128\" to disable it if no incompatible keys were found.\nThanks for the reply.\nCould you please also confirm if this method does indeed save the parameters\/hyperparameters (weights, bias, model values\/structure etc.) of the model?\nI hope that makes sense.\nYour current code will only save the model.state_dict, i.e. all parameters of your model.\nThe model structure will not be saved, so you should always keep the model definition close to the checkpoint.\nAlso, if you would like to use this checkpoint for finetuning, I would recommend to store the optimizer.state_dict(), since some optimizers have internal parameters (e.g. Adam), which would be otherwise reset to their default values.\nHave a look at the ImageNet example with link \"https:\/\/github.com\/pytorch\/examples\/blob\/1de2ff9338bacaaffa123d03ce53d7522d5dcc2e\/imagenet\/main.py#L249\" to see, how the checkpoint is created.\nThank you very much for the explanation and the link.\nis there a reason why the standard way doesn\u2019t work for you:\n\n\nstackoverflow.com with link \"https:\/\/stackoverflow.com\/questions\/42703500\/best-way-to-save-a-trained-model-in-pytorch\"\n\n\n with link \"https:\/\/stackoverflow.com\/users\/5352399\/wasi-ahmad\"\n\nBest way to save a trained model in PyTorch? with link \"https:\/\/stackoverflow.com\/questions\/42703500\/best-way-to-save-a-trained-model-in-pytorch\"\n\n\npython, serialization, deep-learning, pytorch\n\n\n  asked by\n  \n  Wasi Ahmad with link \"https:\/\/stackoverflow.com\/users\/5352399\/wasi-ahmad\"\n  on 07:06PM - 09 Mar 17 UTC with link \"https:\/\/stackoverflow.com\/questions\/42703500\/best-way-to-save-a-trained-model-in-pytorch\"\n\n\n\n\n\n\n\nsee answer above:\nref: https:\/\/github.com\/pytorch\/pytorch\/blob\/761d6799beb3afa03657a71776412a2171ee7533\/docs\/source\/notes\/serialization.rst\n\n<h1>Recommended approach for saving a model<\/h1>\nThere are two main approaches for serializing and restoring a model.\nThe first (recommended) saves and loads only the model parameters:\ntorch.save(the_model.state_dict(), PATH)\n\nThen later:\nthe_model = TheModelClass(*args, **kwargs)\nthe_model.load_state_dict(torch.load(PATH))\n\nThe second saves and loads the entire model:\ntorch.save(the_model, PATH)\n\nThen later:\nthe_model = torch.load(PATH)\n\nHowever in this case, the serialized data is bound to the specific classes\nand the exact directory structure used, so it can break in various ways when\nused in other projects, or after some serious refactors."},{"x":"Regarding ReduceLROnPlateau():\n<code class=\"lang-auto\">optim = torch.optim.AdamW(model.parameters(), lr=param['lr'], amsgrad=True) \nscheduler = ReduceLROnPlateau(optimizer=optim, mode='max', patience=1, verbose=True, factor=0.5)\nval_roc = torch.tensor(15.0, device=device)\nscheduler.step(val_roc)\nscheduler.get_last_lr()\n<\/code>\n<code class=\"lang-auto\">---------------------------------------------------------------------------\nAttributeError                            Traceback (most recent call last)\n<ipython-input-39-f8112ad93830> in <module>\n----> 1 scheduler.get_last_lr()\n\nAttributeError: 'ReduceLROnPlateau' object has no attribute 'get_last_lr'\n<\/code>\nInside def step() we do:\n        self._last_lr = [group['lr'] for group in self.optimizer.param_groups]\nYet there is no way to pull that out.\nClass class _LRScheduler(object): has\n<code class=\"lang-auto\">    def get_last_lr(self):\n        \"\"\" Return last computed learning rate by current scheduler.\n        \"\"\"\n        return self._last_lr\n<\/code>\nBut inside of ReduceLROnPlateau()'s init we do not have:\n        super(ReduceLROnPlateau, self).__init__(optimizer, last_epoch)\nIm just trying to track the LR as my model builds, and encountered this.  I think someone forgot to super\n_LRScheduler?","y":"You could create a feature request for get_last_lr for this scheduler and meanwhile use the 'lr' attribute of the parameter group of the optimizer directly:\n<code class=\"lang-python\">print(optimizer.param_groups[0]['lr'])\n<\/code>","z":"You could create a feature request for get_last_lr for this scheduler and meanwhile use the 'lr' attribute of the parameter group of the optimizer directly:\n<code class=\"lang-python\">print(optimizer.param_groups[0]['lr'])\n<\/code>"},{"x":"I want to calculate the differences between adjacent elements of a tensor X in 1 D.\ne.g if my tensor has a dimension of BxC where B is batch size and C is a channel.\nfunction diff(X) should outputt [X(2)-X(1) X(3)-X(2) ... X(c)-X(c-1)].\nHow can I do it in pytorch?","y":"This code should work to compute the difference in the C dimension:\n<code class=\"lang-python\">B, C = 10, 5\nx = torch.arange(B * C).view(B, C)\nresult = x[:, 1:]  - x[:, :-1]\n<\/code>","z":"This code should work to compute the difference in the C dimension:\n<code class=\"lang-python\">B, C = 10, 5\nx = torch.arange(B * C).view(B, C)\nresult = x[:, 1:]  - x[:, :-1]\n<\/code>"},{"x":"Hi, I have custom dataset that loads data from a subfolder root_x that contains 625 training images and a subfolder root_y that contains 625 matrices. My issue is that when I call the function len() after the loading, the returned value is 67 instead of 625.  Can someone explains me why and what I should change to get the correct size when I call len().\n#custom class\nclass MappingDataset(Dataset):\ndef __init__(self, root_x, root_y, transform=None):\n    self.root_x = root_x\n    self.root_y = root_y\n   \ndef __len__(self):\n    return len(self.root_x)\n\ndef __getitem__(self, idx):\n    img_name = self.root_x + str(idx) + '.png'\n    image = io.imread(img_name)\n    wpmap_name = self.root_y + str(idx) + '.txt'\n    wpmap = np.loadtxt(wpmap_name, np.int)\n    sample = {'image': image, 'wpmap': wpmap} \n    return sample\n\n#main\ntraining_set = MappingDataset(root_x, root_y)\nprint(len(training_set) = 67 #instead of 625\nI note that I can iterate over the training_set when I explicitly give the range of the for loop.\nfor i in range(1, 626):\nsample = training_set[i]\nprint(i, sample[\u2018image\u2019].shape, sample[\u2018wpmap\u2019].shape)","y":"I corrected myself. That was the issue.\ndef len(self):\nreturn len(os.listdir(self.root_x))","z":"I corrected myself. That was the issue.\ndef len(self):\nreturn len(os.listdir(self.root_x))"},{"x":"I am kind of new to PyTorch and training on GPU. When I define a model (a network) myself, I can move all tensor I define in the model to cuda using xx.to(device). However, if I want to use the model defined by others, for example, cloning from others\u2019 github repo, I cannot modify the model.\nIn this case, if I just move the network to cuda, it won\u2019t work. I wonder how I can move all tesnors they define in the model to cuda? ","y":"Hmm I\u2019m afraid there is not.\nOnce again I doubt that if the code is properly done it can fall in issues like that. I imagine that original authors also used a gpu. Therefore it should be somehow adapted to a gpu allocation.\nAnyway if you plan to use that code, reformating to be adapted to cpu\/gpu\/multi-gpu is not a loss of time.\nIn the end, you are creating a new tensor there (which is the only case in which you have to choose allocation device)","z":"Pytorch works with a recursive calling.\nA nn.Module  instance (aka, neural network\/layer) is usually composed by other nn.Module instances.\nonce you have defined the main one (let\u2019s call it model) you can move all the instances and subinstances (layers) inside that by doing model=model.cuda()\nnn.Modules are composed of 2 main parts.\nThe init method in which you define the layers (which are also nn.Modules as I aforementioned) and the forward, which defines how those layers are used.\nFollowing good practices, code shouldn\u2019t hardcode allocation (whether cuda or cpu) in neither init or forward.\nThat means that, once you have a model, you can choose whether to process it using the cpu or cuda.\nFor the former case you have to do nothing (as tensors are defined as cpu tensors by default). For the latter you just need to allocate the model and inputs in cuda:\nPseudocode\n<code class=\"lang-auto\">model = model.cuda()\nfor batch in dataloader:\n   output = model(batch.cuda())\n<\/code>\nHi, thanks for the reply!\nI try to do that and the same error still occurs.\n\u201cexpected device cpu but got device cuda:0\u201d.\nI think the problem is that when they define some tensors in the main model or subinstances, they do not allocate the device. I wonder whether there is a way to move all those tensors to cuda.\nI once met the same problem and what I did that time is to allocate all tensors defined in the model and subinstances to the same device as the input. However, this time the model is larger and I may not want to modfiy their code.(I am tryin to directly import their model this time)\nI read some posts and think maybe the problem is that those tensors are not defined as parameters or buffers. For example, maybe they are just defined to do some computation in forward function. Does that make sense to you? And is there a way to solve this problem?\nThe best way I can find is to use register_buffer. However, it seems that it has to be used when you defined the model.\nhmmm in theory when you register a tensor inside a nn.Module it does internally calls register_buffer.\nif you use a debugger (for example in pycharm, spyder or ipdb) to stop the execution in the exception. You will be able to see which tensor is at cpu.\nBTW are you sure it\u2019s not about the input tensors?\nThat error can be thrown whether because the model is allocated at cuda but the input is not or the other way around.\nWithout seeing the code it\u2019s difficult to provide more clue.\nThe only exception in which calling model.cuda() would map tensors to gpu is if any parameter isn\u2019t properly registered as a nn.parameter\/buffer.\nThis happens, for example, if you create an ordinary list\/tuple\/dictionary of tensors\/layers. The recursive allocator only explores nn.Modules.\nThat is why pytorch provide their own list-like object and dict-like object.\nHi, I try to debug it and find where the problem occurs. I copy a small part of the code to explain the situation. In the function _permute, they define a tensor using\n<code class=\"lang-python\">logabsdet = torch.zeros(batch_size)\n<\/code>\nAfter I modify the code to\n<code class=\"lang-python\">logabsdet = torch.zeros(batch_size, device = inputs.device)\n<\/code>\nThe code works. Is there anyway to solve this problem without modifying the model since in real case I am using a much larger model and it may not be very convenient to modify all of them in this way. \n<code class=\"lang-python\">class Permutation(Transform):\n    \"\"\"Permutes inputs on a given dimension using a given permutation.\"\"\"\n\n    def __init__(self, permutation, dim=1):\n        if permutation.ndimension() != 1:\n            raise ValueError(\"Permutation must be a 1D tensor.\")\n        if not is_positive_int(dim):\n            raise ValueError(\"dim must be a positive integer.\")\n\n        super().__init__()\n        self._dim = dim\n        self.register_buffer(\"_permutation\", permutation)\n\n    \n    def _inverse_permutation(self):\n        return torch.argsort(self._permutation)\n\n    \n    def _permute(inputs, permutation, dim):\n        if dim >= inputs.ndimension():\n            raise ValueError(\"No dimension {} in inputs.\".format(dim))\n        if inputs.shape[dim] != len(permutation):\n            raise ValueError(\n                \"Dimension {} in inputs must be of size {}.\".format(\n                    dim, len(permutation)\n                )\n            )\n        batch_size = inputs.shape[0]\n        outputs = torch.index_select(inputs, dim, permutation)\n        logabsdet = torch.zeros(batch_size)\n        return outputs, logabsdet\n\n    def forward(self, inputs, context=None):\n        return self._permute(inputs, self._permutation, self._dim)\n\n    def inverse(self, inputs, context=None):\n        return self._permute(inputs, self._inverse_permutation, self._dim)\n<\/code>\nHmm\u2026 I do find one way to do this by setting the default tensor type to cuda. But this does not seem like a decent way\u2026\nHmm I\u2019m afraid there is not.\nOnce again I doubt that if the code is properly done it can fall in issues like that. I imagine that original authors also used a gpu. Therefore it should be somehow adapted to a gpu allocation.\nAnyway if you plan to use that code, reformating to be adapted to cpu\/gpu\/multi-gpu is not a loss of time.\nIn the end, you are creating a new tensor there (which is the only case in which you have to choose allocation device)\nHi, I also once assumed they should be adapted to a gpu allocation. Maybe it\u2019s because I  miss something. Anyway, the code is working now. In future, modifying their code myself or asking the author directly might be a good idea.\nThanks for your help!! "},{"x":"I need to implement a custom loss function and looking for best practices on doing so. In the PyTorch documentation, it is clear that the predefined loss functions are implemented as classes. My question is, should I also define my custom loss function as a class or a normal Python function would do? I also would like to know why PyTorch implements losses as classes. Thanks in advance!","y":"It\u2019s not really necessary.\nIt\u2019s all about how pytorch is structured. Classes calls in the enf functionals which are functions with no kwargs but only args whose signature is c++.\nSo they just follow that convention.\nAnother reason is that some class may have weighting schemes and if they are defined as classes (nn.Modules) they can be allocated on cuda easily and recursively.","z":"It\u2019s not really necessary.\nIt\u2019s all about how pytorch is structured. Classes calls in the enf functionals which are functions with no kwargs but only args whose signature is c++.\nSo they just follow that convention.\nAnother reason is that some class may have weighting schemes and if they are defined as classes (nn.Modules) they can be allocated on cuda easily and recursively."},{"x":"When I use meter.AverageValueMeter() times wrong,\nMy code looks like this:\n<code class=\"lang-auto\">    loss_meter = meter.AverageValueMeter()\n    previous_loss = 1e10\n    # train\n    for epoch in range(opt.max_epoch):\n        \n        loss_meter.reset()\n        # confusion_matrix.reset()\n\n        for ii,(data,label) in tqdm(enumerate(train_dataloader),total=len(train_data)):\n\n            # train model \n            input = Variable(data)\n            input = input.float()\n            target = Variable(label)\n            if opt.use_gpu:\n                input = input.cuda()\n                target = target.cuda()\n\n            optimizer.zero_grad()\n            score = model(input)\n            loss = criterion(score,target) \/ (128 * 128)\n            loss.backward()\n            optimizer.step()  \n\n  \n            loss.meter.add(loss.item())\n<\/code>","y":"Hi,\nActually, I don\u2019t know what meter is here but don\u2019t you have typo?\n\n\n\n tianle-BigRice:\n\nloss_meter = meter.AverageValueMeter()\n\n\nYou defined loss_meter but you are calling loss.meter. loss is a tensor and it does not have meter.\n\n\n\n tianle-BigRice:\n\nloss.meter.add(loss.item())\n\n\nSo, I think last line should be loss_meter.meter.add(loss.item())\nBests","z":"Hi,\nActually, I don\u2019t know what meter is here but don\u2019t you have typo?\n\n\n\n tianle-BigRice:\n\nloss_meter = meter.AverageValueMeter()\n\n\nYou defined loss_meter but you are calling loss.meter. loss is a tensor and it does not have meter.\n\n\n\n tianle-BigRice:\n\nloss.meter.add(loss.item())\n\n\nSo, I think last line should be loss_meter.meter.add(loss.item())\nBests\nthank you,i did typo . \nthe code should be loss_meter.add(loss.item) "},{"x":"I have images 128x128 and the corresponding labels are multi-element vectors of 128 elements.\nI want to use DataLoader with a custom map-style dataset, which at the moment look like this:\n<code class=\"lang-auto\"># custom dataset\nclass Dataset(Dataset):\n    def __init__(self, images, n, labels=None, transforms=None):\n        self.X = images\n        self.y = labels\n        self.n = n\n        self.transforms = transforms\n         \n    def __len__(self):\n        return (len(self.X))\n    \n    def __getitem__(self, i):\n        data = self.X.iloc[i, :]\n        data = np.asarray(data).astype(np.float).reshape(1,n, n)\n        \n        if self.transforms:\n            data = self.transforms(data)\n            \n        if self.y is not None:\n            y = self.y.iloc[i,:]\n            y = np.asarray(y).astype(np.float).reshape(n,)\n            return (data, y)\n        else:\n            return data\n<\/code>\nNow I have 20 images and 20 labels.\nimages.shape = (20, 16384)\nlabels.shape = (20, 128)\nI want to prepare two datasets: one is not normalised, while the other is normalized.\nIn order to normalise the dataset I use transforms from torchvision:\n<code class=\"lang-auto\">import torchvision.transforms as transforms\n\nmean_value = 0.0136\nstd_value = 0.0719\n\ntransform = transforms.Compose(\n    [transforms.ToPILImage(),\n     transforms.ToTensor(),\n     transforms.Normalize((mean_value,), (std_value,))\n])\n<\/code>\nThen two datasets (not normalised and normalised) are created by the following two lines:\n<code class=\"lang-auto\">train_data = Dataset(images, 128, labels, None) # not normalised\ntrain_data_normal = Dataset(images, 128, labels, transform) # normalised\n<\/code>\nSomething is not right with the second line, since when I am trying to print an element of train_data_normal, I get an error.\nInput:\n<code class=\"lang-auto\">print(train_data.__getitem__(1)[1][1])\nprint(train_data_normal.__getitem__(1)[1][1])\n<\/code>\nOutput:\n<code class=\"lang-auto\">2.616029869999999e-175\n---------------------------------------------------------------------------\nTypeError                                 Traceback (most recent call last)\n<ipython-input-120-b3db64ef4a36> in <module>\n     13 \n     14 print(train_data.__getitem__(1)[1][1])\n---> 15 print(train_data_normal.__getitem__(1)[1][1])\n     16 \n     17 # trainLoader = DataLoader(train_data, batch_size=128, shuffle=True)\n\n<ipython-input-109-9c7d8181d1e9> in __getitem__(self, i)\n     16 \n     17         if self.transforms:\n---> 18             data = self.transforms(data)\n     19 \n     20         if self.y is not None:\n\n~\\anaconda3\\lib\\site-packages\\torchvision\\transforms\\transforms.py in __call__(self, img)\n     59     def __call__(self, img):\n     60         for t in self.transforms:\n---> 61             img = t(img)\n     62         return img\n     63 \n\n~\\anaconda3\\lib\\site-packages\\torchvision\\transforms\\transforms.py in __call__(self, pic)\n    125 \n    126         \"\"\"\n--> 127         return F.to_pil_image(pic, self.mode)\n    128 \n    129     def __repr__(self):\n\n~\\anaconda3\\lib\\site-packages\\torchvision\\transforms\\functional.py in to_pil_image(pic, mode)\n    165 \n    166     if mode is None:\n--> 167         raise TypeError('Input type {} is not supported'.format(npimg.dtype))\n    168 \n    169     return Image.fromarray(npimg, mode=mode)\n\nTypeError: Input type float64 is not supported\n<\/code>\nCould you please point at where the bug is?","y":"to_pil_image wants float32 inputs, I think. It doesn\u2019t make a whole lot of sense to me to use to_pil_image right before to_tensor, but hey, what  do I know.\nBest regards\nThomas","z":"to_pil_image wants float32 inputs, I think. It doesn\u2019t make a whole lot of sense to me to use to_pil_image right before to_tensor, but hey, what  do I know.\nBest regards\nThomas\nThanks for your reply Thomas.\nI changed two lines in my Dataset class:\n<code class=\"lang-auto\">...\ndata = np.asarray(data).astype(np.float32).reshape(1,n, n)\n...\n      y = np.asarray(y).astype(np.float32).reshape(n,)\n...\n<\/code>\nAnd the error is essentially the same but float32-version of it.\nOutput:\n<code class=\"lang-auto\">0.0\n---------------------------------------------------------------------------\nTypeError                                 Traceback (most recent call last)\n<ipython-input-127-b3db64ef4a36> in <module>\n     13 \n     14 print(train_data.__getitem__(1)[1][1])\n---> 15 print(train_data_normal.__getitem__(1)[1][1])\n     16 \n     17 # trainLoader = DataLoader(train_data, batch_size=128, shuffle=True)\n.....\n.....\n.....\nTypeError: Input type float32 is not supported\n<\/code>\nPIL.Image expects int not float.\nFollowing on from previous comments, I\u2019m not sure what the transform to PILImage and then to Tensor is giving you.\n\n\n\n Capo_Mestre:\n\ntransforms.ToPILImage()\n\n\nI removed transforms.ToPILImage() and it worked. Thanks for looking into the code!\nThanks for your reply!"},{"x":"Hi\nI am new to pytorch and when I tried to use pytorch in my project I noticed that somehow it always predicts straight lines. I tried to isolate the problem and I completly failed to approximate a normal quadratic function with it. I very confused where I go wrong\u2026\n<code class=\"lang-auto\">    import torch\n    from torch import nn\n    from torch.autograd import Variable\n    import numpy as np\n    import pandas as pd\n    import matplotlib.pyplot as plt\n    from sklearn.model_selection import train_test_split\n    from sklearn.metrics import accuracy_score\n    import torch.optim as optim\n\n\n    def f_x(x):\n        return x * x \/ 64 - 5 * x \/ 4 + 25\n\n\n    # Building dataset\n    def build_dataset():\n        # Given f(x), is_f_x defines whether the function is satisfied\n        x_values = np.ones((21, 1))\n        for i in range(0, 21):\n            x_values[i] = i + 30  # True\n        return x_values\n\n\n    x_values = build_dataset()\n\n    # Building nn\n    # net = nn.Sequential(nn.Linear(1, 100), nn.ReLU(), nn.Linear(100, 100), nn.ReLU(), nn.Linear(100, 1))\n    net = nn.Sequential(nn.Linear(1, 1000), nn.ReLU(), nn.Linear(1000, 1000), nn.ReLU(), nn.Linear(1000, 1000), nn.ReLU(), nn.Linear(1000, 1))\n\n    # parameters\n    optimizer = optim.Adam(net.parameters(), lr=0.00001)\n    epochs = 200\n\n\n    def out(k):\n        # folder_name = \"Testrun1\"\n        # working_directory = pathlib.Path().absolute()\n        # output_location = working_directory \/ f'{folder_name}'\n\n        a = 30\n        b = 50\n\n        # TODO: copy graph so i only use a copy when it was still open\n\n        import matplotlib.backends.backend_pdf as pdfp\n        from pylab import plot, show, grid, xlabel, ylabel\n        import matplotlib.pyplot as plt\n        # pdf = pdfp.PdfPages(\"graph\" + str(k) + \".pdf\")\n\n        t = np.linspace(a, b, 20)\n        x = np.zeros(t.shape[0])\n        c_fig = plt.figure()\n\n        for j in range(len(t)):\n            h = torch.tensor(np.ones(1) * t[j], dtype=torch.float32)\n            x[j] = net(h)\n        plt.ylim([0, 1])\n        plot(t, x, linewidth=4)\n        xlabel('x', fontsize=16)\n        ylabel('net(x)', fontsize=16)\n        grid(True)\n        show()\n        # pdf.savefig(c_fig)\n\n        # pdf.close()\n        plt.close(c_fig)\n\n\n    def train():\n        net.train()\n        losses = []\n        for epoch in range(1, epochs):\n            x_train = Variable(torch.from_numpy(x_values)).float()\n            y_train = f_x(x_train)\n            y_pred = net(x_train)\n            loss = torch.sum(torch.abs(y_pred - y_train))\n            print(\"epoch #\", epoch)\n            print(loss.item())\n            losses.append(loss.item())\n            optimizer.zero_grad()\n            loss.backward()\n            optimizer.step()\n        return losses\n\n\n    print(\"training start....\")\n    losses = train()\n    plt.plot(range(1, epochs), losses)\n    plt.xlabel(\"epoch\")\n    plt.ylabel(\"loss train\")\n    plt.ylim([0, 100])\n    plt.show()\n\n    out(epochs)\n<\/code>","y":"That function is just not easy to learn, starting from noise. It also doesn\u2019t help that inputs and outputs (target bias=25) are far from zero.\n\namount of training may be too low (lr_rate and number of epochs). lr_scheduler may also be needed to fine-tune curves\nyou\u2019re not using quadratic loss (MSE). this means that y_pred derivative doesn\u2019t depend on distance from target, just on its sign\nexcessive network size just complicates training\n\nI actually ran you code, with:\n<code class=\"lang-auto\">net = nn.Sequential(nn.Linear(1, 50), nn.Tanh(), nn.Linear(50,50), nn.Tanh(), nn.Linear(50, 1))\noptimizer = optim.Adam(net.parameters(), lr=0.01)\nscheduler = optim.lr_scheduler.ExponentialLR(optimizer, 0.99)\nepochs = 1000\nloss = ((y_pred-y_train)**2).sum()\n<\/code>\ngives kinda reasonable curve. But with your big network (3layers x 1000, tanh), I decreased lr to 0.0001 (0.01 gave linear solutions) and 1000 epochs was not enough.\nSo, in general, it is about tuning hyperparameters, all smooth activation functions should in principle work.","z":"RELU is not sophisticated enough here - it is difficult to find good thresholds from univariate input, so optimization rather finds inferior local minimum using linear region of RELU activations.\nAlso note that adding x^2 to network inputs explicitly considerably simplifies the task.\nHi\nYou were definitly correct that RELU was not an optimal choice. I used this code\nactivation_function = nn.SELU()\nnet = nn.Sequential(nn.Linear(1, 1000), activation_function, nn.Linear(1000, 1000), activation_function, nn.Linear(1000, 1000), activation_function, nn.Linear(1000, 1))\nand plugged in all 20 non-linear activation functions from here https:\/\/pytorch.org\/docs\/stable\/nn.html#non-linear-activations-weighted-sum-nonlinearity\nthere were multiple where it was possible to see a curve but out of the 20 exactly one reached an somewhatdecent precision and that was SELU. However even the best activation function retained a loss of >= 1 regardless of the number of iterations, which I consider unacceptable given the task to approximate a quadratic function with 2 hidden layer of 1000 neurons.\nDo you have another idea that might help? I tried different optimizers as well, but Adam + Rprop are the two that work best, so I can\u2019t make an improvement there\u2026\n note: You mean I should have 2 input values, one is x and the other is x*x? I don\u2019t get why this is better but I do believe you.\nThat function is just not easy to learn, starting from noise. It also doesn\u2019t help that inputs and outputs (target bias=25) are far from zero.\n\namount of training may be too low (lr_rate and number of epochs). lr_scheduler may also be needed to fine-tune curves\nyou\u2019re not using quadratic loss (MSE). this means that y_pred derivative doesn\u2019t depend on distance from target, just on its sign\nexcessive network size just complicates training\n\nI actually ran you code, with:\n<code class=\"lang-auto\">net = nn.Sequential(nn.Linear(1, 50), nn.Tanh(), nn.Linear(50,50), nn.Tanh(), nn.Linear(50, 1))\noptimizer = optim.Adam(net.parameters(), lr=0.01)\nscheduler = optim.lr_scheduler.ExponentialLR(optimizer, 0.99)\nepochs = 1000\nloss = ((y_pred-y_train)**2).sum()\n<\/code>\ngives kinda reasonable curve. But with your big network (3layers x 1000, tanh), I decreased lr to 0.0001 (0.01 gave linear solutions) and 1000 epochs was not enough.\nSo, in general, it is about tuning hyperparameters, all smooth activation functions should in principle work.\nI am very happy with this solution.\nAs far as I can tell the most important problems were wrong activation\/loss functions. After these changes many nets seem to work, but your small net does the trick fast and very accurate. I am also very happy you showed an example of the lr_scheduler. I saw them a few days ago but forgot about them again.\nHowever I do not understand your point about target bias. What exactly is target bias and how do you come up with the exact number of 25?\nEdit: I just noticed very strange behaviour. I ran my code with all your chnges a few times and it always looks like the loss reaches a plateau at around 1 and stays there for a long time, then suddenly it gets much better. The length of this plateau varies between 200 and 800 epochs in my tests. Do you have an idea why this is? Especially when it stayed really constant at loss 1 for 800 epochs I surely would have stopped the net if it wasn\u2019t so fast.\n\n\n\n hgfedcba:\n\nHowever I do not understand your point about target bias. What exactly is target bias and how do you come up with the exact number of 25?\n\n\nYour final linear layer does computation of form dot(h,w)+b, with w and b initialized to zero-centered noise, it is very far from true function f_x(x)=P(x)+25, so this offset may be modeled through h coming from earlier layers. But h is obtained via non-linear transform, so approximations suffer somewhat.\n\n\n\n hgfedcba:\n\nI ran my code with all your chnges a few times and it always looks like the loss reaches a plateau at around 1 and stays there for a long time, then suddenly it gets much better. The length of this plateau varies between 200 and 800 epochs in my tests. Do you have an idea why this is? Especially when it stayed really constant at loss 1 for 800 epochs I surely would have stopped the net if it wasn\u2019t so fast.\n\n\nNot sure, it seems that curve fitting starts late, network has to shift things to a good region first. lr scheduling may also play a role.\nNote that tanh is actually problematic, it has tiny gradients outside -5\u20265, and you\u2019re using non-normalized inputs. So another possible reason is that some \u201cneurons\u201d are effectively disabled for a long time."},{"x":"I have a tensor in pytorch with size torch.Size([1443747, 128]). Let\u2019s name it tensor A. In this tensor, 128 represents a batch size. I have another 1D tensor with size torch.Size([1443747]). Let\u2019s call it B. I want to do element wise multiplication of B with A, such that B is multiplied with all 128 columns of tensor A (obviously in an element wise manner). In other words, I want to broadcast the element wise multiplication along dimension=1.\nHow can I achieve this in pytorch? \nIt I didn\u2019t have a batch size involved in the tensor A (batch size = 1), then normal * operator would do the multiplication easily. A*B then would have generated resultant tensor of size torch.Size([1443747]). However, I don\u2019t understand why pytorch is not broadcasting the tensor multiplication along dimension 1? Is there any way to do this?\nWhat I want is, B should be multiplied with all 128 columns of A in an element wise manner. So, the resultant tensors\u2019 size would be torch.Size([1443747, 128]).","y":"Hello Mr. Knight!\n\n\n\n nowyouseeme:\n\nI want to do element wise multiplication of B with A, such that B is multiplied with all 128 columns of tensor A\u2026\nI want to broadcast the element wise multiplication along dimension=1.\n\n\nGive B a dimension of size 1 using unsqueeze() so that it has a\ndimension from which to broadcast:\n<code class=\"lang-python\">B.unsqueeze (1) * A\n<\/code>\nBest.\nK. Frank","z":"Hello Mr. Knight!\n\n\n\n nowyouseeme:\n\nI want to do element wise multiplication of B with A, such that B is multiplied with all 128 columns of tensor A\u2026\nI want to broadcast the element wise multiplication along dimension=1.\n\n\nGive B a dimension of size 1 using unsqueeze() so that it has a\ndimension from which to broadcast:\n<code class=\"lang-python\">B.unsqueeze (1) * A\n<\/code>\nBest.\nK. Frank"},{"x":"Hi ,\nwhatever I try to print out or display\n<code class=\"lang-auto\">tensor([[-0.1028,\n         -0.0493,\n          0.0453,\n         -0.0668,\n          0.0362,\n          0.1089,\n          0.1205,\n          0.0165,\n          0.0700,\n          0.0195],\n        [-0.1050,\n         -0.0383,\n          0.0408,\n         -0.0668,\n          0.0474,\n          0.1017,\n          0.1247,\n          0.0230,\n          0.0677,\n          0.0173],\n        [-0.1120,\n         -0.0409,\n          0.0404,\n         -0.0655,\n          0.0430,\n          0.1146,\n          0.1072,\n          0.0251,\n          0.0721,\n          0.0097],\n        [-0.1108,\n         -0.0349,\n          0.0427,\n         -0.0672,\n          0.0415,\n          0.1105,\n          0.1137,\n          0.0263,\n          0.0722,\n          0.0080],\n        [-0.1079,\n         -0.0386,\n          0.0433,\n         -0.0669,\n          0.0410,\n          0.0997,\n          0.1243,\n          0.0225,\n          0.0688,\n          0.0150],\n        [-0.1027,\n         -0.0366,\n          0.0396,\n         -0.0657,\n          0.0498,\n          0.0994,\n          0.1280,\n          0.0250,\n          0.0614,\n          0.0173],\n        [-0.1010,\n         -0.0523,\n          0.0359,\n         -0.0604,\n          0.0518,\n          0.1135,\n          0.1135,\n          0.0147,\n          0.0695,\n          0.0272],\n        [-0.1037,\n         -0.0294,\n          0.0436,\n         -0.0685,\n          0.0513,\n          0.0948,\n          0.1266,\n          0.0252,\n          0.0685,\n          0.0149],\n        [-0.1153,\n         -0.0436,\n          0.0431,\n         -0.0653,\n          0.0348,\n          0.1209,\n          0.1094,\n          0.0213,\n          0.0722,\n          0.0163],\n        [-0.1083,\n         -0.0480,\n          0.0478,\n         -0.0658,\n          0.0249,\n          0.1162,\n          0.1144,\n          0.0213,\n          0.0609,\n          0.0184],\n        [-0.1084,\n         -0.0359,\n          0.0433,\n         -0.0680,\n          0.0409,\n          0.1060,\n          0.1188,\n          0.0262,\n          0.0701,\n          0.0089],\n        [-0.0987,\n         -0.0484,\n          0.0428,\n         -0.0655,\n          0.0394,\n          0.1047,\n          0.1214,\n          0.0185,\n          0.0653,\n          0.0140],\n        [-0.1025,\n         -0.0513,\n          0.0381,\n         -0.0612,\n          0.0470,\n          0.1147,\n          0.1100,\n          0.0259,\n          0.0671,\n          0.0171],\n        [-0.1082,\n         -0.0480,\n          0.0365,\n         -0.0627,\n          0.0437,\n          0.1172,\n          0.1066,\n          0.0226,\n          0.0716,\n          0.0153],\n        [-0.1082,\n         -0.0464,\n          0.0388,\n         -0.0650,\n          0.0489,\n          0.1161,\n          0.1071,\n          0.0191,\n          0.0721,\n          0.0195],\n        [-0.1019,\n         -0.0496,\n          0.0445,\n         -0.0667,\n          0.0400,\n          0.1091,\n          0.1148,\n          0.0195,\n          0.0695,\n          0.0178],\n        [-0.1088,\n         -0.0414,\n          0.0421,\n         -0.0664,\n          0.0424,\n          0.1073,\n          0.1150,\n          0.0269,\n          0.0692,\n          0.0118],\n        [-0.1067,\n         -0.0345,\n          0.0428,\n         -0.0675,\n          0.0445,\n          0.1016,\n          0.1223,\n          0.0244,\n          0.0699,\n          0.0118],\n        [-0.1058,\n         -0.0359,\n          0.0441,\n         -0.0674,\n          0.0421,\n          0.1060,\n          0.1206,\n          0.0255,\n          0.0691,\n          0.0115],\n        [-0.1113,\n         -0.0403,\n          0.0407,\n         -0.0665,\n          0.0457,\n          0.1159,\n          0.1062,\n          0.0246,\n          0.0722,\n          0.0097],\n        [-0.1064,\n         -0.0396,\n          0.0411,\n         -0.0651,\n          0.0423,\n          0.1031,\n          0.1222,\n          0.0246,\n          0.0686,\n          0.0141],\n        [-0.1092,\n         -0.0365,\n          0.0413,\n         -0.0650,\n          0.0417,\n          0.1048,\n          0.1197,\n          0.0247,\n          0.0681,\n          0.0135],\n        [-0.1147,\n         -0.0384,\n          0.0418,\n         -0.0669,\n          0.0358,\n          0.1065,\n          0.1175,\n          0.0168,\n          0.0761,\n          0.0164],\n        [-0.1039,\n         -0.0368,\n          0.0429,\n         -0.0676,\n          0.0462,\n          0.1050,\n          0.1202,\n          0.0252,\n          0.0693,\n          0.0133],\n        [-0.1034,\n         -0.0350,\n          0.0433,\n         -0.0655,\n          0.0460,\n          0.1024,\n          0.1228,\n          0.0257,\n          0.0664,\n          0.0124],\n        [-0.1051,\n         -0.0400,\n          0.0396,\n         -0.0636,\n          0.0443,\n          0.1005,\n          0.1210,\n          0.0184,\n          0.0668,\n          0.0179],\n        [-0.1036,\n         -0.0400,\n          0.0394,\n         -0.0645,\n          0.0402,\n          0.1036,\n          0.1227,\n          0.0230,\n          0.0643,\n          0.0115],\n        [-0.1014,\n         -0.0334,\n          0.0422,\n         -0.0677,\n          0.0490,\n          0.0950,\n          0.1271,\n          0.0240,\n          0.0664,\n          0.0143],\n        [-0.1079,\n         -0.0363,\n          0.0444,\n         -0.0671,\n          0.0425,\n          0.1092,\n          0.1164,\n          0.0288,\n          0.0694,\n          0.0090],\n        [-0.1032,\n         -0.0371,\n          0.0442,\n         -0.0686,\n          0.0449,\n          0.1025,\n          0.1230,\n          0.0247,\n          0.0677,\n          0.0124],\n        [-0.1108,\n         -0.0407,\n          0.0385,\n         -0.0650,\n          0.0503,\n          0.1176,\n          0.1083,\n          0.0219,\n          0.0719,\n          0.0149],\n        [-0.1132,\n         -0.0410,\n          0.0421,\n         -0.0684,\n          0.0394,\n          0.1110,\n          0.1109,\n          0.0237,\n          0.0738,\n          0.0126],\n        [-0.1036,\n         -0.0392,\n          0.0424,\n         -0.0669,\n          0.0442,\n          0.1047,\n          0.1212,\n          0.0240,\n          0.0675,\n          0.0128],\n        [-0.1126,\n         -0.0420,\n          0.0379,\n         -0.0631,\n          0.0440,\n          0.1204,\n          0.1029,\n          0.0237,\n          0.0716,\n          0.0090],\n        [-0.1116,\n         -0.0412,\n          0.0392,\n         -0.0643,\n          0.0445,\n          0.1168,\n          0.1047,\n          0.0244,\n          0.0714,\n          0.0089],\n        [-0.1180,\n         -0.0393,\n          0.0419,\n         -0.0692,\n          0.0380,\n          0.1120,\n          0.1103,\n          0.0236,\n          0.0738,\n          0.0143],\n        [-0.1140,\n         -0.0425,\n          0.0456,\n         -0.0671,\n          0.0327,\n          0.1171,\n          0.1154,\n          0.0172,\n          0.0771,\n          0.0165],\n        [-0.1108,\n         -0.0378,\n          0.0423,\n         -0.0664,\n          0.0417,\n          0.1128,\n          0.1114,\n          0.0275,\n          0.0720,\n          0.0085],\n        [-0.1044,\n         -0.0435,\n          0.0366,\n         -0.0611,\n          0.0441,\n          0.1071,\n          0.1181,\n          0.0225,\n          0.0660,\n          0.0159],\n        [-0.1039,\n         -0.0339,\n          0.0436,\n         -0.0673,\n          0.0458,\n          0.0962,\n          0.1253,\n          0.0244,\n          0.0651,\n          0.0139],\n        [-0.1061,\n         -0.0340,\n          0.0430,\n         -0.0669,\n          0.0455,\n          0.1029,\n          0.1208,\n          0.0252,\n          0.0693,\n          0.0121],\n        [-0.1004,\n         -0.0514,\n          0.0370,\n         -0.0620,\n          0.0508,\n          0.1089,\n          0.1076,\n          0.0176,\n          0.0695,\n          0.0211],\n        [-0.1013,\n         -0.0491,\n          0.0416,\n         -0.0625,\n          0.0432,\n          0.1073,\n          0.1120,\n          0.0206,\n          0.0696,\n          0.0181],\n        [-0.1108,\n         -0.0486,\n          0.0378,\n         -0.0658,\n          0.0431,\n          0.1168,\n          0.1050,\n          0.0203,\n          0.0748,\n          0.0173],\n        [-0.1022,\n         -0.0440,\n          0.0446,\n         -0.0664,\n          0.0399,\n          0.1039,\n          0.1247,\n          0.0240,\n          0.0612,\n          0.0182],\n        [-0.1058,\n         -0.0373,\n          0.0438,\n         -0.0678,\n          0.0424,\n          0.1073,\n          0.1170,\n          0.0262,\n          0.0692,\n          0.0094],\n        [-0.1061,\n         -0.0474,\n          0.0405,\n         -0.0645,\n          0.0473,\n          0.1145,\n          0.1136,\n          0.0188,\n          0.0707,\n          0.0236],\n        [-0.1065,\n         -0.0368,\n          0.0418,\n         -0.0668,\n          0.0445,\n          0.0970,\n          0.1204,\n          0.0232,\n          0.0680,\n          0.0129],\n        [-0.1039,\n         -0.0401,\n          0.0400,\n         -0.0647,\n          0.0436,\n          0.1053,\n          0.1188,\n          0.0239,\n          0.0670,\n          0.0121],\n        [-0.1056,\n         -0.0397,\n          0.0419,\n         -0.0662,\n          0.0426,\n          0.0978,\n          0.1244,\n          0.0232,\n          0.0663,\n          0.0164],\n        [-0.1079,\n         -0.0427,\n          0.0402,\n         -0.0656,\n          0.0391,\n          0.1038,\n          0.1195,\n          0.0250,\n          0.0662,\n          0.0142],\n        [-0.1095,\n         -0.0394,\n          0.0400,\n         -0.0644,\n          0.0397,\n          0.1059,\n          0.1185,\n          0.0235,\n          0.0694,\n          0.0153],\n        [-0.1074,\n         -0.0473,\n          0.0389,\n         -0.0634,\n          0.0455,\n          0.1132,\n          0.1142,\n          0.0191,\n          0.0702,\n          0.0241],\n        [-0.1021,\n         -0.0325,\n          0.0442,\n         -0.0690,\n          0.0479,\n          0.0939,\n          0.1296,\n          0.0236,\n          0.0666,\n          0.0138],\n        [-0.1101,\n         -0.0403,\n          0.0441,\n         -0.0681,\n          0.0438,\n          0.1139,\n          0.1093,\n          0.0263,\n          0.0724,\n          0.0096],\n        [-0.1063,\n         -0.0369,\n          0.0441,\n         -0.0685,\n          0.0432,\n          0.1032,\n          0.1217,\n          0.0251,\n          0.0713,\n          0.0120],\n        [-0.1023,\n         -0.0336,\n          0.0400,\n         -0.0630,\n          0.0478,\n          0.1027,\n          0.1185,\n          0.0229,\n          0.0664,\n          0.0135],\n        [-0.1101,\n         -0.0439,\n          0.0412,\n         -0.0663,\n          0.0437,\n          0.1065,\n          0.1163,\n          0.0185,\n          0.0725,\n          0.0215],\n        [-0.1073,\n         -0.0426,\n          0.0392,\n         -0.0647,\n          0.0404,\n          0.1030,\n          0.1213,\n          0.0211,\n          0.0691,\n          0.0177],\n        [-0.1071,\n         -0.0460,\n          0.0390,\n         -0.0645,\n          0.0407,\n          0.1083,\n          0.1132,\n          0.0211,\n          0.0683,\n          0.0166],\n        [-0.1104,\n         -0.0431,\n          0.0492,\n         -0.0678,\n          0.0270,\n          0.1177,\n          0.1204,\n          0.0198,\n          0.0693,\n          0.0215],\n        [-0.1031,\n         -0.0404,\n          0.0420,\n         -0.0649,\n          0.0444,\n          0.1089,\n          0.1189,\n          0.0248,\n          0.0682,\n          0.0131],\n        [-0.1113,\n         -0.0480,\n          0.0384,\n         -0.0642,\n          0.0479,\n          0.1178,\n          0.1044,\n          0.0224,\n          0.0715,\n          0.0169],\n        [-0.1076,\n         -0.0447,\n          0.0345,\n         -0.0614,\n          0.0500,\n          0.1198,\n          0.1066,\n          0.0215,\n          0.0697,\n          0.0154],\n        [-0.1071,\n         -0.0396,\n          0.0424,\n         -0.0667,\n          0.0402,\n          0.1025,\n          0.1235,\n          0.0219,\n          0.0670,\n          0.0128],\n        [-0.1040,\n         -0.0352,\n          0.0433,\n         -0.0667,\n          0.0449,\n          0.1043,\n          0.1209,\n          0.0273,\n          0.0678,\n          0.0101],\n        [-0.1062,\n         -0.0328,\n          0.0433,\n         -0.0678,\n          0.0453,\n          0.1002,\n          0.1236,\n          0.0254,\n          0.0680,\n          0.0105],\n        [-0.1095,\n         -0.0405,\n          0.0400,\n         -0.0664,\n          0.0403,\n          0.1047,\n          0.1213,\n          0.0182,\n          0.0713,\n          0.0173],\n        [-0.1095,\n         -0.0374,\n          0.0452,\n         -0.0692,\n          0.0403,\n          0.1066,\n          0.1159,\n          0.0261,\n          0.0713,\n          0.0110],\n        [-0.1090,\n         -0.0424,\n          0.0396,\n         -0.0653,\n          0.0416,\n          0.1116,\n          0.1126,\n          0.0240,\n          0.0698,\n          0.0132],\n        [-0.1091,\n         -0.0318,\n          0.0462,\n         -0.0676,\n          0.0391,\n          0.1000,\n          0.1287,\n          0.0263,\n          0.0642,\n          0.0136],\n        [-0.1090,\n         -0.0441,\n          0.0400,\n         -0.0661,\n          0.0407,\n          0.1080,\n          0.1151,\n          0.0241,\n          0.0692,\n          0.0138],\n        [-0.1086,\n         -0.0376,\n          0.0427,\n         -0.0671,\n          0.0436,\n          0.1084,\n          0.1121,\n          0.0267,\n          0.0705,\n          0.0101],\n        [-0.1061,\n         -0.0417,\n          0.0427,\n         -0.0656,\n          0.0393,\n          0.1015,\n          0.1238,\n          0.0222,\n          0.0682,\n          0.0163],\n        [-0.1072,\n         -0.0439,\n          0.0395,\n         -0.0653,\n          0.0429,\n          0.1083,\n          0.1161,\n          0.0240,\n          0.0687,\n          0.0158],\n        [-0.1074,\n         -0.0330,\n          0.0449,\n         -0.0680,\n          0.0462,\n          0.0982,\n          0.1243,\n          0.0242,\n          0.0682,\n          0.0144],\n        [-0.1059,\n         -0.0349,\n          0.0402,\n         -0.0634,\n          0.0483,\n          0.1014,\n          0.1205,\n          0.0238,\n          0.0682,\n          0.0155],\n        [-0.1036,\n         -0.0353,\n          0.0431,\n         -0.0659,\n          0.0450,\n          0.1050,\n          0.1210,\n          0.0281,\n          0.0675,\n          0.0099],\n        [-0.1074,\n         -0.0424,\n          0.0389,\n         -0.0634,\n          0.0440,\n          0.1114,\n          0.1114,\n          0.0255,\n          0.0692,\n          0.0117],\n        [-0.1010,\n         -0.0470,\n          0.0428,\n         -0.0661,\n          0.0388,\n          0.1069,\n          0.1197,\n          0.0206,\n          0.0630,\n          0.0153],\n        [-0.1061,\n         -0.0386,\n          0.0391,\n         -0.0649,\n          0.0450,\n          0.1041,\n          0.1193,\n          0.0242,\n          0.0648,\n          0.0129],\n        [-0.1123,\n         -0.0402,\n          0.0419,\n         -0.0672,\n          0.0421,\n          0.1132,\n          0.1089,\n          0.0252,\n          0.0721,\n          0.0103],\n        [-0.1097,\n         -0.0474,\n          0.0430,\n         -0.0677,\n          0.0400,\n          0.1154,\n          0.1088,\n          0.0214,\n          0.0699,\n          0.0146],\n        [-0.1046,\n         -0.0501,\n          0.0356,\n         -0.0612,\n          0.0472,\n          0.1145,\n          0.1135,\n          0.0191,\n          0.0683,\n          0.0249],\n        [-0.1047,\n         -0.0474,\n          0.0427,\n         -0.0660,\n          0.0345,\n          0.1096,\n          0.1211,\n          0.0137,\n          0.0699,\n          0.0154],\n        [-0.1080,\n         -0.0488,\n          0.0383,\n         -0.0645,\n          0.0438,\n          0.1134,\n          0.1057,\n          0.0168,\n          0.0754,\n          0.0180],\n        [-0.1069,\n         -0.0419,\n          0.0408,\n         -0.0658,\n          0.0407,\n          0.1090,\n          0.1191,\n          0.0254,\n          0.0672,\n          0.0121],\n        [-0.1067,\n         -0.0463,\n          0.0392,\n         -0.0631,\n          0.0518,\n          0.1150,\n          0.1092,\n          0.0158,\n          0.0734,\n          0.0236],\n        [-0.1042,\n         -0.0477,\n          0.0397,\n         -0.0648,\n          0.0404,\n          0.1057,\n          0.1175,\n          0.0167,\n          0.0708,\n          0.0149],\n        [-0.1041,\n         -0.0482,\n          0.0432,\n         -0.0656,\n          0.0442,\n          0.1126,\n          0.1099,\n          0.0214,\n          0.0718,\n          0.0187],\n        [-0.1084,\n         -0.0505,\n          0.0430,\n         -0.0679,\n          0.0411,\n          0.1120,\n          0.1029,\n          0.0202,\n          0.0753,\n          0.0140],\n        [-0.1096,\n         -0.0434,\n          0.0401,\n         -0.0647,\n          0.0382,\n          0.1087,\n          0.1127,\n          0.0190,\n          0.0727,\n          0.0163],\n        [-0.1106,\n         -0.0387,\n          0.0416,\n         -0.0657,\n          0.0462,\n          0.1099,\n          0.1100,\n          0.0236,\n          0.0702,\n          0.0128],\n        [-0.1011,\n         -0.0489,\n          0.0412,\n         -0.0641,\n          0.0460,\n          0.1073,\n          0.1121,\n          0.0206,\n          0.0699,\n          0.0200],\n        [-0.1091,\n         -0.0408,\n          0.0383,\n         -0.0648,\n          0.0430,\n          0.1050,\n          0.1152,\n          0.0193,\n          0.0706,\n          0.0151],\n        [-0.1092,\n         -0.0381,\n          0.0450,\n         -0.0686,\n          0.0440,\n          0.1097,\n          0.1128,\n          0.0272,\n          0.0716,\n          0.0105],\n        [-0.1118,\n         -0.0399,\n          0.0406,\n         -0.0654,\n          0.0446,\n          0.1169,\n          0.1066,\n          0.0261,\n          0.0716,\n          0.0099],\n        [-0.1064,\n         -0.0449,\n          0.0367,\n         -0.0617,\n          0.0417,\n          0.1090,\n          0.1150,\n          0.0223,\n          0.0675,\n          0.0140],\n        [-0.1087,\n         -0.0379,\n          0.0407,\n         -0.0644,\n          0.0427,\n          0.1077,\n          0.1166,\n          0.0247,\n          0.0703,\n          0.0127],\n        [-0.1079,\n         -0.0435,\n          0.0422,\n         -0.0692,\n          0.0438,\n          0.1096,\n          0.1183,\n          0.0208,\n          0.0712,\n          0.0214]],\n       grad_fn=<AddmmBackward>)\n<\/code>\nIt displays vertically or columnwise only , How can I display it  like a normal matrix?\nThanks !!","y":"Ho right!\nThis value tells how wide the. screen is. And the value is in characters.\nThe usual default is 80. But if your screen is extra wide, you can increase that to use more of it.\nIf you set it to 20, then as you saw, it will be forced to write things in column as there isn\u2019t enough space in each line to write the content.","z":"What do you mean by \u201cnormal matrix\u201d. This looks quite reasonable as a way to print a matrix of size 10 x 1.\nIf you have a vector (of size 10) or a row matrix (of size 1 x 10) it will print on the same line.\n<code class=\"lang-auto\">tensor([[-0.1028,\n         -0.0493,\n          0.0453,\n         -0.0668,\n          0.0362,\n          0.1089,\n          0.1205,\n          0.0165,\n          0.0700,\n          0.0195],\n        [-0.1050,\n         -0.0383,\n          0.0408,\n         -0.0668,\n          0.0474,\n          0.1017,\n          0.1247,\n          0.0230,\n          0.0677,\n          0.0173],\n        [-0.1120,\n         -0.0409,\n          0.0404,\n         -0.0655,\n          0.0430,\n          0.1146,\n          0.1072,\n          0.0251,\n          0.0721,\n          0.0097],\n        [-0.1108,\n         -0.0349,\n          0.0427,\n         -0.0672,\n          0.0415,\n          0.1105,\n          0.1137,\n          0.0263,\n          0.0722,\n          0.0080],\n        [-0.1079,\n         -0.0386,\n          0.0433,\n         -0.0669,\n          0.0410,\n          0.0997,\n          0.1243,\n          0.0225,\n          0.0688,\n          0.0150],\n        [-0.1027,\n         -0.0366,\n          0.0396,\n         -0.0657,\n          0.0498,\n          0.0994,\n          0.1280,\n          0.0250,\n          0.0614,\n          0.0173],\n        [-0.1010,\n         -0.0523,\n          0.0359,\n         -0.0604,\n          0.0518,\n          0.1135,\n          0.1135,\n          0.0147,\n          0.0695,\n          0.0272],\n        [-0.1037,\n         -0.0294,\n          0.0436,\n         -0.0685,\n          0.0513,\n          0.0948,\n          0.1266,\n          0.0252,\n          0.0685,\n          0.0149],\n        [-0.1153,\n         -0.0436,\n          0.0431,\n         -0.0653,\n          0.0348,\n          0.1209,\n          0.1094,\n          0.0213,\n          0.0722,\n          0.0163],\n        [-0.1083,\n         -0.0480,\n          0.0478,\n         -0.0658,\n          0.0249,\n          0.1162,\n          0.1144,\n          0.0213,\n          0.0609,\n          0.0184],\n        [-0.1084,\n         -0.0359,\n          0.0433,\n         -0.0680,\n          0.0409,\n          0.1060,\n          0.1188,\n          0.0262,\n          0.0701,\n          0.0089],\n        [-0.0987,\n         -0.0484,\n          0.0428,\n         -0.0655,\n          0.0394,\n          0.1047,\n          0.1214,\n          0.0185,\n          0.0653,\n          0.0140],\n        [-0.1025,\n         -0.0513,\n          0.0381,\n         -0.0612,\n          0.0470,\n          0.1147,\n          0.1100,\n          0.0259,\n          0.0671,\n          0.0171],\n        [-0.1082,\n         -0.0480,\n          0.0365,\n         -0.0627,\n          0.0437,\n          0.1172,\n          0.1066,\n          0.0226,\n          0.0716,\n          0.0153],\n        [-0.1082,\n         -0.0464,\n          0.0388,\n         -0.0650,\n          0.0489,\n          0.1161,\n          0.1071,\n          0.0191,\n          0.0721,\n          0.0195],\n        [-0.1019,\n         -0.0496,\n          0.0445,\n         -0.0667,\n          0.0400,\n          0.1091,\n          0.1148,\n          0.0195,\n          0.0695,\n          0.0178],\n        [-0.1088,\n         -0.0414,\n          0.0421,\n         -0.0664,\n          0.0424,\n          0.1073,\n          0.1150,\n          0.0269,\n          0.0692,\n          0.0118],\n        [-0.1067,\n         -0.0345,\n          0.0428,\n         -0.0675,\n          0.0445,\n          0.1016,\n          0.1223,\n          0.0244,\n          0.0699,\n          0.0118],\n        [-0.1058,\n         -0.0359,\n          0.0441,\n         -0.0674,\n          0.0421,\n          0.1060,\n          0.1206,\n          0.0255,\n          0.0691,\n          0.0115],\n        [-0.1113,\n         -0.0403,\n          0.0407,\n         -0.0665,\n          0.0457,\n          0.1159,\n          0.1062,\n          0.0246,\n          0.0722,\n          0.0097],\n        [-0.1064,\n         -0.0396,\n          0.0411,\n         -0.0651,\n          0.0423,\n          0.1031,\n          0.1222,\n          0.0246,\n          0.0686,\n          0.0141],\n        [-0.1092,\n         -0.0365,\n          0.0413,\n         -0.0650,\n          0.0417,\n          0.1048,\n          0.1197,\n          0.0247,\n          0.0681,\n          0.0135],\n        [-0.1147,\n         -0.0384,\n          0.0418,\n         -0.0669,\n          0.0358,\n          0.1065,\n          0.1175,\n          0.0168,\n          0.0761,\n          0.0164],\n        [-0.1039,\n         -0.0368,\n          0.0429,\n         -0.0676,\n          0.0462,\n          0.1050,\n          0.1202,\n          0.0252,\n          0.0693,\n          0.0133],\n        [-0.1034,\n         -0.0350,\n          0.0433,\n         -0.0655,\n          0.0460,\n          0.1024,\n          0.1228,\n          0.0257,\n          0.0664,\n          0.0124],\n        [-0.1051,\n         -0.0400,\n          0.0396,\n         -0.0636,\n          0.0443,\n          0.1005,\n          0.1210,\n          0.0184,\n          0.0668,\n          0.0179],\n        [-0.1036,\n         -0.0400,\n          0.0394,\n         -0.0645,\n          0.0402,\n          0.1036,\n          0.1227,\n          0.0230,\n          0.0643,\n          0.0115],\n        [-0.1014,\n         -0.0334,\n          0.0422,\n         -0.0677,\n          0.0490,\n          0.0950,\n          0.1271,\n          0.0240,\n          0.0664,\n          0.0143],\n        [-0.1079,\n         -0.0363,\n          0.0444,\n         -0.0671,\n          0.0425,\n          0.1092,\n          0.1164,\n          0.0288,\n          0.0694,\n          0.0090],\n        [-0.1032,\n         -0.0371,\n          0.0442,\n         -0.0686,\n          0.0449,\n          0.1025,\n          0.1230,\n          0.0247,\n          0.0677,\n          0.0124],\n        [-0.1108,\n         -0.0407,\n          0.0385,\n         -0.0650,\n          0.0503,\n          0.1176,\n          0.1083,\n          0.0219,\n          0.0719,\n          0.0149],\n        [-0.1132,\n         -0.0410,\n          0.0421,\n         -0.0684,\n          0.0394,\n          0.1110,\n          0.1109,\n          0.0237,\n          0.0738,\n          0.0126],\n        [-0.1036,\n         -0.0392,\n          0.0424,\n         -0.0669,\n          0.0442,\n          0.1047,\n          0.1212,\n          0.0240,\n          0.0675,\n          0.0128],\n        [-0.1126,\n         -0.0420,\n          0.0379,\n         -0.0631,\n          0.0440,\n          0.1204,\n          0.1029,\n          0.0237,\n          0.0716,\n          0.0090],\n        [-0.1116,\n         -0.0412,\n          0.0392,\n         -0.0643,\n          0.0445,\n          0.1168,\n          0.1047,\n          0.0244,\n          0.0714,\n          0.0089],\n        [-0.1180,\n         -0.0393,\n          0.0419,\n         -0.0692,\n          0.0380,\n          0.1120,\n          0.1103,\n          0.0236,\n          0.0738,\n          0.0143],\n        [-0.1140,\n         -0.0425,\n          0.0456,\n         -0.0671,\n          0.0327,\n          0.1171,\n          0.1154,\n          0.0172,\n          0.0771,\n          0.0165],\n        [-0.1108,\n         -0.0378,\n          0.0423,\n         -0.0664,\n          0.0417,\n          0.1128,\n          0.1114,\n          0.0275,\n          0.0720,\n          0.0085],\n        [-0.1044,\n         -0.0435,\n          0.0366,\n         -0.0611,\n          0.0441,\n          0.1071,\n          0.1181,\n          0.0225,\n          0.0660,\n          0.0159],\n        [-0.1039,\n         -0.0339,\n          0.0436,\n         -0.0673,\n          0.0458,\n          0.0962,\n          0.1253,\n          0.0244,\n          0.0651,\n          0.0139],\n        [-0.1061,\n         -0.0340,\n          0.0430,\n         -0.0669,\n          0.0455,\n          0.1029,\n          0.1208,\n          0.0252,\n          0.0693,\n          0.0121],\n        [-0.1004,\n         -0.0514,\n          0.0370,\n         -0.0620,\n          0.0508,\n          0.1089,\n          0.1076,\n          0.0176,\n          0.0695,\n          0.0211],\n        [-0.1013,\n         -0.0491,\n          0.0416,\n         -0.0625,\n          0.0432,\n          0.1073,\n          0.1120,\n          0.0206,\n          0.0696,\n          0.0181],\n        [-0.1108,\n         -0.0486,\n          0.0378,\n         -0.0658,\n          0.0431,\n          0.1168,\n          0.1050,\n          0.0203,\n          0.0748,\n          0.0173],\n        [-0.1022,\n         -0.0440,\n          0.0446,\n         -0.0664,\n          0.0399,\n          0.1039,\n          0.1247,\n          0.0240,\n          0.0612,\n          0.0182],\n        [-0.1058,\n         -0.0373,\n          0.0438,\n         -0.0678,\n          0.0424,\n          0.1073,\n          0.1170,\n          0.0262,\n          0.0692,\n          0.0094],\n        [-0.1061,\n         -0.0474,\n          0.0405,\n         -0.0645,\n          0.0473,\n          0.1145,\n          0.1136,\n          0.0188,\n          0.0707,\n          0.0236],\n        [-0.1065,\n         -0.0368,\n          0.0418,\n         -0.0668,\n          0.0445,\n          0.0970,\n          0.1204,\n          0.0232,\n          0.0680,\n          0.0129],\n        [-0.1039,\n         -0.0401,\n          0.0400,\n         -0.0647,\n          0.0436,\n          0.1053,\n          0.1188,\n          0.0239,\n          0.0670,\n          0.0121],\n        [-0.1056,\n         -0.0397,\n          0.0419,\n         -0.0662,\n          0.0426,\n          0.0978,\n          0.1244,\n          0.0232,\n          0.0663,\n          0.0164],\n        [-0.1079,\n         -0.0427,\n          0.0402,\n         -0.0656,\n          0.0391,\n          0.1038,\n          0.1195,\n          0.0250,\n          0.0662,\n          0.0142],\n        [-0.1095,\n         -0.0394,\n          0.0400,\n         -0.0644,\n          0.0397,\n          0.1059,\n          0.1185,\n          0.0235,\n          0.0694,\n          0.0153],\n        [-0.1074,\n         -0.0473,\n          0.0389,\n         -0.0634,\n          0.0455,\n          0.1132,\n          0.1142,\n          0.0191,\n          0.0702,\n          0.0241],\n        [-0.1021,\n         -0.0325,\n          0.0442,\n         -0.0690,\n          0.0479,\n          0.0939,\n          0.1296,\n          0.0236,\n          0.0666,\n          0.0138],\n        [-0.1101,\n         -0.0403,\n          0.0441,\n         -0.0681,\n          0.0438,\n          0.1139,\n          0.1093,\n          0.0263,\n          0.0724,\n          0.0096],\n        [-0.1063,\n         -0.0369,\n          0.0441,\n         -0.0685,\n          0.0432,\n          0.1032,\n          0.1217,\n          0.0251,\n          0.0713,\n          0.0120],\n        [-0.1023,\n         -0.0336,\n          0.0400,\n         -0.0630,\n          0.0478,\n          0.1027,\n          0.1185,\n          0.0229,\n          0.0664,\n          0.0135],\n        [-0.1101,\n         -0.0439,\n          0.0412,\n         -0.0663,\n          0.0437,\n          0.1065,\n          0.1163,\n          0.0185,\n          0.0725,\n          0.0215],\n        [-0.1073,\n         -0.0426,\n          0.0392,\n         -0.0647,\n          0.0404,\n          0.1030,\n          0.1213,\n          0.0211,\n          0.0691,\n          0.0177],\n        [-0.1071,\n         -0.0460,\n          0.0390,\n         -0.0645,\n          0.0407,\n          0.1083,\n          0.1132,\n          0.0211,\n          0.0683,\n          0.0166],\n        [-0.1104,\n         -0.0431,\n          0.0492,\n         -0.0678,\n          0.0270,\n          0.1177,\n          0.1204,\n          0.0198,\n          0.0693,\n          0.0215],\n        [-0.1031,\n         -0.0404,\n          0.0420,\n         -0.0649,\n          0.0444,\n          0.1089,\n          0.1189,\n          0.0248,\n          0.0682,\n          0.0131],\n        [-0.1113,\n         -0.0480,\n          0.0384,\n         -0.0642,\n          0.0479,\n          0.1178,\n          0.1044,\n          0.0224,\n          0.0715,\n          0.0169],\n        [-0.1076,\n         -0.0447,\n          0.0345,\n         -0.0614,\n          0.0500,\n          0.1198,\n          0.1066,\n          0.0215,\n          0.0697,\n          0.0154],\n        [-0.1071,\n         -0.0396,\n          0.0424,\n         -0.0667,\n          0.0402,\n          0.1025,\n          0.1235,\n          0.0219,\n          0.0670,\n          0.0128],\n        [-0.1040,\n         -0.0352,\n          0.0433,\n         -0.0667,\n          0.0449,\n          0.1043,\n          0.1209,\n          0.0273,\n          0.0678,\n          0.0101],\n        [-0.1062,\n         -0.0328,\n          0.0433,\n         -0.0678,\n          0.0453,\n          0.1002,\n          0.1236,\n          0.0254,\n          0.0680,\n          0.0105],\n        [-0.1095,\n         -0.0405,\n          0.0400,\n         -0.0664,\n          0.0403,\n          0.1047,\n          0.1213,\n          0.0182,\n          0.0713,\n          0.0173],\n        [-0.1095,\n         -0.0374,\n          0.0452,\n         -0.0692,\n          0.0403,\n          0.1066,\n          0.1159,\n          0.0261,\n          0.0713,\n          0.0110],\n        [-0.1090,\n         -0.0424,\n          0.0396,\n         -0.0653,\n          0.0416,\n          0.1116,\n          0.1126,\n          0.0240,\n          0.0698,\n          0.0132],\n        [-0.1091,\n         -0.0318,\n          0.0462,\n         -0.0676,\n          0.0391,\n          0.1000,\n          0.1287,\n          0.0263,\n          0.0642,\n          0.0136],\n        [-0.1090,\n         -0.0441,\n          0.0400,\n         -0.0661,\n          0.0407,\n          0.1080,\n          0.1151,\n          0.0241,\n          0.0692,\n          0.0138],\n        [-0.1086,\n         -0.0376,\n          0.0427,\n         -0.0671,\n          0.0436,\n          0.1084,\n          0.1121,\n          0.0267,\n          0.0705,\n          0.0101],\n        [-0.1061,\n         -0.0417,\n          0.0427,\n         -0.0656,\n          0.0393,\n          0.1015,\n          0.1238,\n          0.0222,\n          0.0682,\n          0.0163],\n        [-0.1072,\n         -0.0439,\n          0.0395,\n         -0.0653,\n          0.0429,\n          0.1083,\n          0.1161,\n          0.0240,\n          0.0687,\n          0.0158],\n        [-0.1074,\n         -0.0330,\n          0.0449,\n         -0.0680,\n          0.0462,\n          0.0982,\n          0.1243,\n          0.0242,\n          0.0682,\n          0.0144],\n        [-0.1059,\n         -0.0349,\n          0.0402,\n         -0.0634,\n          0.0483,\n          0.1014,\n          0.1205,\n          0.0238,\n          0.0682,\n          0.0155],\n        [-0.1036,\n         -0.0353,\n          0.0431,\n         -0.0659,\n          0.0450,\n          0.1050,\n          0.1210,\n          0.0281,\n          0.0675,\n          0.0099],\n        [-0.1074,\n         -0.0424,\n          0.0389,\n         -0.0634,\n          0.0440,\n          0.1114,\n          0.1114,\n          0.0255,\n          0.0692,\n          0.0117],\n        [-0.1010,\n         -0.0470,\n          0.0428,\n         -0.0661,\n          0.0388,\n          0.1069,\n          0.1197,\n          0.0206,\n          0.0630,\n          0.0153],\n        [-0.1061,\n         -0.0386,\n          0.0391,\n         -0.0649,\n          0.0450,\n          0.1041,\n          0.1193,\n          0.0242,\n          0.0648,\n          0.0129],\n        [-0.1123,\n         -0.0402,\n          0.0419,\n         -0.0672,\n          0.0421,\n          0.1132,\n          0.1089,\n          0.0252,\n          0.0721,\n          0.0103],\n        [-0.1097,\n         -0.0474,\n          0.0430,\n         -0.0677,\n          0.0400,\n          0.1154,\n          0.1088,\n          0.0214,\n          0.0699,\n          0.0146],\n        [-0.1046,\n         -0.0501,\n          0.0356,\n         -0.0612,\n          0.0472,\n          0.1145,\n          0.1135,\n          0.0191,\n          0.0683,\n          0.0249],\n        [-0.1047,\n         -0.0474,\n          0.0427,\n         -0.0660,\n          0.0345,\n          0.1096,\n          0.1211,\n          0.0137,\n          0.0699,\n          0.0154],\n        [-0.1080,\n         -0.0488,\n          0.0383,\n         -0.0645,\n          0.0438,\n          0.1134,\n          0.1057,\n          0.0168,\n          0.0754,\n          0.0180],\n        [-0.1069,\n         -0.0419,\n          0.0408,\n         -0.0658,\n          0.0407,\n          0.1090,\n          0.1191,\n          0.0254,\n          0.0672,\n          0.0121],\n        [-0.1067,\n         -0.0463,\n          0.0392,\n         -0.0631,\n          0.0518,\n          0.1150,\n          0.1092,\n          0.0158,\n          0.0734,\n          0.0236],\n        [-0.1042,\n         -0.0477,\n          0.0397,\n         -0.0648,\n          0.0404,\n          0.1057,\n          0.1175,\n          0.0167,\n          0.0708,\n          0.0149],\n        [-0.1041,\n         -0.0482,\n          0.0432,\n         -0.0656,\n          0.0442,\n          0.1126,\n          0.1099,\n          0.0214,\n          0.0718,\n          0.0187],\n        [-0.1084,\n         -0.0505,\n          0.0430,\n         -0.0679,\n          0.0411,\n          0.1120,\n          0.1029,\n          0.0202,\n          0.0753,\n          0.0140],\n        [-0.1096,\n         -0.0434,\n          0.0401,\n         -0.0647,\n          0.0382,\n          0.1087,\n          0.1127,\n          0.0190,\n          0.0727,\n          0.0163],\n        [-0.1106,\n         -0.0387,\n          0.0416,\n         -0.0657,\n          0.0462,\n          0.1099,\n          0.1100,\n          0.0236,\n          0.0702,\n          0.0128],\n        [-0.1011,\n         -0.0489,\n          0.0412,\n         -0.0641,\n          0.0460,\n          0.1073,\n          0.1121,\n          0.0206,\n          0.0699,\n          0.0200],\n        [-0.1091,\n         -0.0408,\n          0.0383,\n         -0.0648,\n          0.0430,\n          0.1050,\n          0.1152,\n          0.0193,\n          0.0706,\n          0.0151],\n        [-0.1092,\n         -0.0381,\n          0.0450,\n         -0.0686,\n          0.0440,\n          0.1097,\n          0.1128,\n          0.0272,\n          0.0716,\n          0.0105],\n        [-0.1118,\n         -0.0399,\n          0.0406,\n         -0.0654,\n          0.0446,\n          0.1169,\n          0.1066,\n          0.0261,\n          0.0716,\n          0.0099],\n        [-0.1064,\n         -0.0449,\n          0.0367,\n         -0.0617,\n          0.0417,\n          0.1090,\n          0.1150,\n          0.0223,\n          0.0675,\n          0.0140],\n        [-0.1087,\n         -0.0379,\n          0.0407,\n         -0.0644,\n          0.0427,\n          0.1077,\n          0.1166,\n          0.0247,\n          0.0703,\n          0.0127],\n        [-0.1079,\n         -0.0435,\n          0.0422,\n         -0.0692,\n          0.0438,\n          0.1096,\n          0.1183,\n          0.0208,\n          0.0712,\n          0.0214]],\n       grad_fn=<AddmmBackward>)\n<\/code>\nprints like this, (Edited original post also )\nDo I have to reshape it everytime?\nThe way the printing works is that we plot the last two dimensions as a 2D matrix.\nAnd extra dimensions before are stacked and these 2D matrices are written one below the other.\nNote that the same thing is done by numpy and most libraries I know.\nIn your case, because the last dimension is 1, then have only colum vectors.\nYou can remove that dimension if you want to change the way it is printed.\ntorch.Size([100, 10])\nThis is what I get when I checked the shape of above displayed tensor.  so how can I remove that or should I state somewhere dim=0?\nHo that\u2019s surprising.\nCan you try torch.set_printoptions(linewidth=120) ?  Maybe the linewidth of your terminal is not set properly?\nI have already set it to 20 , is that wrong? , Wow that solved my error , thank you very much , By the way what are the values of linewidth in general I have to use?\nHo right!\nThis value tells how wide the. screen is. And the value is in characters.\nThe usual default is 80. But if your screen is extra wide, you can increase that to use more of it.\nIf you set it to 20, then as you saw, it will be forced to write things in column as there isn\u2019t enough space in each line to write the content.\nThank you so much , It was indeed a clear explanation . for easily to be noticed by others , I will change the topic to set print options"},{"x":"Hi all, I was trying to distribute several models (say 8) into 8 gpu devices. For each model models[i], I calculated the sum of output as follows:\n<code class=\"lang-auto\">for i in range(args.num_models):\n    input_var = input_var.to(\"cuda:%d\" % (i))\n    target_var = target_var.to(\"cuda:%d\" % (i))\n    criterion = nn.CrossEntropyLoss().to(\"cuda:%d\" % (i))\n    models[i] = models[i].to(\"cuda:%d\" % (i))\n    loss += criterion(models[i](input_var), target_var)\n\nloss.backward()\n<\/code>\nBut some error occurred:\n<code class=\"lang-auto\">RuntimeError: Function AddBackward0 returned an invalid gradient at index 1 - expected device cuda:1 but got cuda:0\n<\/code>\nAny suggestions? Thanks!","y":"Try to push the losses to the same device before accumulating them.\nAlso, which PyTorch version are you using, as I cannot reproduce this issue locally (and also thought it would be fixed by now).","z":"Try to push the losses to the same device before accumulating them.\nAlso, which PyTorch version are you using, as I cannot reproduce this issue locally (and also thought it would be fixed by now).\nThank you so much! Problem solved "},{"x":"Would this break the autograd? And what is the difference between doing this and using self.register_buffer","y":"Hi,\nIn the init of the nn.Module? No that won\u2019t change anything from the point of view of the autograd.\nThe benefit of having it being a buffer is that it will be moved around when you do things like model.cuda().","z":"Hi,\nIn the init of the nn.Module? No that won\u2019t change anything from the point of view of the autograd.\nThe benefit of having it being a buffer is that it will be moved around when you do things like model.cuda()."},{"x":"Dear \nI\u2019m trying to apply Transformer tutorial from Harvardnlp with link \"https:\/\/nlp.seas.harvard.edu\/2018\/04\/03\/attention.html\", I have 4 GPUs server and, I got CUDA error: out of memory for 512 batch size.\n<code class=\"lang-auto\"># For data loading.\nfrom torchtext import data, datasets\n\nif True:\n\n    BOS_WORD = '<s>'\n    EOS_WORD = '<\/s>'\n    BLANK_WORD = \"<blank>\"\n    SRC = data.Field(tokenize=bpemb_ar.encode, pad_token=BLANK_WORD, fix_length = 50)\n    TRG = data.Field(tokenize=bpemb_ar.encode, init_token = BOS_WORD, \n                     eos_token = EOS_WORD, pad_token=BLANK_WORD, fix_length = 50)\n\n    MAX_LEN = 100\n    train, val, test = TabularDataset.splits(path='.\/data\/',train='SCUT_trai.csv',\n    validation='SCUT_vali.csv', test='test.csv', format='csv',\n    fields=[('src', SRC), ('trg', TRG)], skip_header=True, \n        filter_pred=lambda x: len(vars(x)['src']) <= MAX_LEN and \n            len(vars(x)['trg']) <= MAX_LEN)\n    \n    MIN_FREQ = 2\n    SRC.build_vocab(train.src, min_freq=MIN_FREQ)\n    TRG.build_vocab(train.trg, min_freq=MIN_FREQ)\n<\/code>\n<code class=\"lang-auto\">class MultiGPULossCompute:\n    \"A multi-gpu loss compute and train function.\"\n    def __init__(self, generator, criterion, devices, opt=None, chunk_size=5):\n        # Send out to different gpus.\n        self.generator = generator\n        self.criterion = nn.parallel.replicate(criterion, \n                                               devices=devices)\n        self.opt = opt\n        self.devices = devices\n        self.chunk_size = chunk_size\n        \n    def __call__(self, out, targets, normalize):\n        total = 0.0\n        generator = nn.parallel.replicate(self.generator, \n                                                devices=self.devices)\n        out_scatter = nn.parallel.scatter(out, \n                                          target_gpus=self.devices)\n        out_grad = [[] for _ in out_scatter]\n        targets = nn.parallel.scatter(targets, \n                                      target_gpus=self.devices)\n\n        # Divide generating into chunks.\n        chunk_size = self.chunk_size\n        for i in range(0, out_scatter[0].size(1), chunk_size):\n            # Predict distributions\n            out_column = [[Variable(o[:, i:i+chunk_size].data, \n                                    requires_grad=self.opt is not None)] \n                           for o in out_scatter]\n            gen = nn.parallel.parallel_apply(generator, out_column)\n\n            # Compute loss. \n            y = [(g.contiguous().view(-1, g.size(-1)), \n                  t[:, i:i+chunk_size].contiguous().view(-1)) \n                 for g, t in zip(gen, targets)]\n            loss = nn.parallel.parallel_apply(self.criterion, y)\n\n            # Sum and normalize loss\n            l = nn.parallel.gather(loss, \n                                   target_device=self.devices[0])\n            l = l.sum() \/ normalize\n            total += l.data\n\n            # Backprop loss to output of transformer\n            if self.opt is not None:\n                l.backward()\n                for j, l in enumerate(loss):\n                    out_grad[j].append(out_column[j][0].grad.data.clone())\n\n        # Backprop all loss through transformer.            \n        if self.opt is not None:\n            out_grad = [Variable(torch.cat(og, dim=1)) for og in out_grad]\n            o1 = out\n            o2 = nn.parallel.gather(out_grad, \n                                    target_device=self.devices[0])\n            o1.backward(gradient=o2)\n            self.opt.step()\n            self.opt.optimizer.zero_grad()\n        return total * normalize\n<\/code>\n<code class=\"lang-auto\"># GPUs to use\ndevices = [0, 1, 2, 3]\nif True:\n    pad_idx = TRG.vocab.stoi[\"<blank>\"]\n    model = make_model(len(SRC.vocab), len(TRG.vocab), N=6)\n    model.cuda()\n    criterion = LabelSmoothing(size=len(TRG.vocab), padding_idx=pad_idx, smoothing=0.1)\n    criterion.cuda()\n    BATCH_SIZE = 256 #12000\n    train_iter = MyIterator(train, batch_size=BATCH_SIZE, device=0,\n                            repeat=False, sort_key=lambda x: (len(x.src), len(x.trg)),\n                            batch_size_fn=batch_size_fn, train=True)\n    \n    valid_iter = MyIterator(val, batch_size=BATCH_SIZE, device=0,\n                            repeat=False, sort_key=lambda x: (len(x.src), len(x.trg)),\n                            batch_size_fn=batch_size_fn, train=False)\n    \n    model_par = nn.DataParallel(model, device_ids=devices)\n    # torch.device('cuda') \nNone\n<\/code>\n<code class=\"lang-auto\">\/home\/akram\/anaconda3\/lib\/python3.6\/site-packages\/ipykernel_launcher.py:20: UserWarning: nn.init.xavier_uniform is now deprecated in favor of nn.init.xavier_uniform_.\n\n---------------------------------------------------------------------------\nRuntimeError                              Traceback (most recent call last)\n<ipython-input-40-7da75baf3635> in <module>\n      4     pad_idx = TRG.vocab.stoi[\"<blank>\"]\n      5     model = make_model(len(SRC.vocab), len(TRG.vocab), N=6)\n----> 6     model.cuda()\n      7     criterion = LabelSmoothing(size=len(TRG.vocab), padding_idx=pad_idx, smoothing=0.1)\n      8     criterion.cuda()\n\n~\/anaconda3\/lib\/python3.6\/site-packages\/torch\/nn\/modules\/module.py in cuda(self, device)\n    263             Module: self\n    264         \"\"\"\n--> 265         return self._apply(lambda t: t.cuda(device))\n    266 \n    267     def cpu(self):\n\n~\/anaconda3\/lib\/python3.6\/site-packages\/torch\/nn\/modules\/module.py in _apply(self, fn)\n    191     def _apply(self, fn):\n    192         for module in self.children():\n--> 193             module._apply(fn)\n    194 \n    195         for param in self._parameters.values():\n\n~\/anaconda3\/lib\/python3.6\/site-packages\/torch\/nn\/modules\/module.py in _apply(self, fn)\n    191     def _apply(self, fn):\n    192         for module in self.children():\n--> 193             module._apply(fn)\n    194 \n    195         for param in self._parameters.values():\n\n~\/anaconda3\/lib\/python3.6\/site-packages\/torch\/nn\/modules\/module.py in _apply(self, fn)\n    191     def _apply(self, fn):\n    192         for module in self.children():\n--> 193             module._apply(fn)\n    194 \n    195         for param in self._parameters.values():\n\n~\/anaconda3\/lib\/python3.6\/site-packages\/torch\/nn\/modules\/module.py in _apply(self, fn)\n    191     def _apply(self, fn):\n    192         for module in self.children():\n--> 193             module._apply(fn)\n    194 \n    195         for param in self._parameters.values():\n\n~\/anaconda3\/lib\/python3.6\/site-packages\/torch\/nn\/modules\/module.py in _apply(self, fn)\n    191     def _apply(self, fn):\n    192         for module in self.children():\n--> 193             module._apply(fn)\n    194 \n    195         for param in self._parameters.values():\n\n~\/anaconda3\/lib\/python3.6\/site-packages\/torch\/nn\/modules\/module.py in _apply(self, fn)\n    191     def _apply(self, fn):\n    192         for module in self.children():\n--> 193             module._apply(fn)\n    194 \n    195         for param in self._parameters.values():\n\n~\/anaconda3\/lib\/python3.6\/site-packages\/torch\/nn\/modules\/module.py in _apply(self, fn)\n    197                 # Tensors stored in modules are graph leaves, and we don't\n    198                 # want to create copy nodes, so we have to unpack the data.\n--> 199                 param.data = fn(param.data)\n    200                 if param._grad is not None:\n    201                     param._grad.data = fn(param._grad.data)\n\n~\/anaconda3\/lib\/python3.6\/site-packages\/torch\/nn\/modules\/module.py in <lambda>(t)\n    263             Module: self\n    264         \"\"\"\n--> 265         return self._apply(lambda t: t.cuda(device))\n    266 \n    267     def cpu(self):\n\nRuntimeError: CUDA error: out of memory\n<\/code>\nThe whole code copy from the original article, any suggestions?","y":"Try to reduce the batch size and check which batch size would fit and how much memory is used.\nYour GPUs might have not enough memory for the code you are using or are you using exactly the setup the authors were also using?","z":"Try to reduce the batch size and check which batch size would fit and how much memory is used.\nYour GPUs might have not enough memory for the code you are using or are you using exactly the setup the authors were also using?\nYou\u2019re right Mr. , the problem is that GPU 0 shared with some other users."},{"x":"I saw:\n\n\nstackoverflow.com with link \"https:\/\/stackoverflow.com\/questions\/44328530\/how-to-get-a-uniform-distribution-in-a-range-r1-r2-in-pytorch\"\n\n\n with link \"https:\/\/stackoverflow.com\/users\/4933403\/bishwajit-purkaystha\"\n\nHow to get a uniform distribution in a range [r1,r2] in PyTorch? with link \"https:\/\/stackoverflow.com\/questions\/44328530\/how-to-get-a-uniform-distribution-in-a-range-r1-r2-in-pytorch\"\n\n\npytorch, uniform-distribution\n\n\n  asked by\n  \n  Bishwajit Purkaystha with link \"https:\/\/stackoverflow.com\/users\/4933403\/bishwajit-purkaystha\"\n  on 12:05PM - 02 Jun 17 UTC with link \"https:\/\/stackoverflow.com\/questions\/44328530\/how-to-get-a-uniform-distribution-in-a-range-r1-r2-in-pytorch\"\n\n\n\n\n\n\nand thought that was a strange way to do it.\nSo there isn\u2019t something default like torch.uniform?\n\ntheir code:\n<code class=\"lang-auto\">If U is a random variable uniformly distributed on [0, 1], then (r1 - r2) * U + r2 is uniformly distributed on [r1, r2].\n\nThus, you just need:\n\n(r1 - r2) * torch.rand(a, b) + r2\nAlternatively, you can simply use:\n\ntorch.FloatTensor(a, b).uniform_(r1, r2)\n<\/code>","y":"This is the way I found works:\n<code class=\"lang-auto\"># generating uniform variables\n\nimport numpy as np\n\nnum_samples = 3\nDin = 1\nlb, ub = -1, 1\n\nxn = np.random.uniform(low=lb, high=ub, size=(num_samples,Din))\nprint(xn)\n\nimport torch\n\nsampler = torch.distributions.Uniform(low=lb, high=ub)\nr = sampler.sample((num_samples,Din))\n\nprint(r)\n\nr2 = torch.torch.distributions.Uniform(low=lb, high=ub).sample((num_samples,Din))\n\nprint(r2)\n\n# process input\nf = nn.Sequential(OrderedDict([\n    ('f1', nn.Linear(Din,Dout)),\n    ('out', nn.SELU())\n]))\nY = f(r2)\nprint(Y)\n<\/code>\nbut I have to admit I don\u2019t know what the point of generating sampler is and why not just call it directly as I do in the one liner (last line of code).\nComments:\n\nsampler are good for it\u2019s so you can transform\/compose\/cache\/etc distributions. see https:\/\/arxiv.org\/abs\/1711.10604, and the top of the docs of https:\/\/pytorch.org\/docs\/stable\/distributions.html# and https:\/\/arxiv.org\/abs\/1506.05254\n\nyou can feed in tensors to uniform to let it know the high dimensional interval (hypercube) to generate the uniform samples (that\u2019s why it receives tensors as input rather than simply numbers)\n\n\nReference:\n\nhttps:\/\/stackoverflow.com\/questions\/44328530\/how-to-get-a-uniform-distribution-in-a-range-r1-r2-in-pytorch\/62919760#62919760\nGenerating random tensors according to the uniform distribution pytorch? with link \"https:\/\/discuss.pytorch.org\/t\/generating-random-tensors-according-to-the-uniform-distribution-pytorch\/53030\/8\"\nhttps:\/\/github.com\/pytorch\/pytorch\/issues\/24162\n","z":"toch.rand returns a tensor samples uniformly in [0, 1). Scaling it as shown in your example should work.\nAlso, the second approach is fine.\nIf you would like to have a standalone torch.uniform method, I would recommend to create an issue in github and start the discussion there. \n\n\n\n pinocchio:\n\nIf U is a random variable uniformly distributed on [0, 1], then (r1 - r2) * U + r2 is uniformly distributed on [r1, r2]. Thus, you just need: (r1 - r2) * torch.rand(a, b) + r2 Alternatively, you can simply use: torch.FloatTensor(a, b).uniform_(r1, r2)\n\n\nok! My first time doing this, hopefully this is the right way to do it:\n\n\ngithub.com\/pytorch\/pytorch with link \"https:\/\/github.com\/pytorch\/pytorch\/issues\/24162\"\n\n\n with link \"https:\/\/github.com\/renesax14\"\nIssue: Implement torch.uniform with link \"https:\/\/github.com\/pytorch\/pytorch\/issues\/24162\"\n\n\n\topened by renesax14 with link \"https:\/\/github.com\/renesax14\"\n\ton 2019-08-11 with link \"https:\/\/github.com\/pytorch\/pytorch\/issues\/24162\"\n\n\n\n\n\ud83d\ude80 Feature\nsample uniform vectors\nMotivation\nHave a out of the box uniform samples\nPitch\nx = torch.uniform(a,b)\ncode\ndef uniform(a,b):\n'''\nIf U is a random variable uniformly distributed...\n\n\n\n\n\n\n\nHello,\nyes this is true. According to the PyTorch documentation:\ntorch.rand returns a tensor filled with random numbers from a uniform distribution on the interval [0,1)\nhttps:\/\/pytorch.org\/docs\/stable\/torch.html#torch.rand\nI would appreciate if you can explain the following. I am trying to figure out how the authors of the papers generate normal and uniform distribution:\n\u201cGaussian Noise. The synthetic Gaussian noise dataset consists of 10,000 random 2D Gaussian\nnoise images, where each RGB value of every pixel is sampled from an i.i.d Gaussian distribution\nwith mean 0.5 and unit variance. We further clip each pixel value into the range [0, 1].\nUniform Noise. The synthetic uniform noise dataset consists of 10,000 images where each RGB\nvalue of every pixel is independently and identically sampled from a uniform distribution on [0, 1].\u201d\n\n\n\nGitHub with link \"https:\/\/github.com\/facebookresearch\/odin\"\n\n\n\nfacebookresearch\/odin with link \"https:\/\/github.com\/facebookresearch\/odin\"\nA simple and effective method for detecting out-of-distribution images in neural networks. - facebookresearch\/odin\n\n\n\n\n\nIn the code they seemed to use this for generating Gaussian dataset:\n<code class=\"lang-auto\">        images = torch.randn(1, 3, 32, 32) + 0.5\n        images = torch.clamp(images, 0, 1)\n<\/code>\nand this for generating Uniform dataset:\n<code class=\"lang-auto\">       images = torch.rand(1, 3, 32, 32)\n<\/code>\nAccording to your explanation the generation of the Uniform dataset is being correct.\nWhat I would like to know is how they used the rand for the Gaussian dataset, although this discussion aligns with what they did:\n\n\nstackoverflow.com with link \"https:\/\/stackoverflow.com\/questions\/51136581\/how-to-create-a-normal-distribution-in-pytorch\"\n\n\n with link \"https:\/\/stackoverflow.com\/users\/9242759\/dq-shen\"\n\nHow to create a normal distribution in pytorch with link \"https:\/\/stackoverflow.com\/questions\/51136581\/how-to-create-a-normal-distribution-in-pytorch\"\n\n\npython, statistics, pytorch, linear-algebra\n\n\n  asked by\n  \n  dq.shen with link \"https:\/\/stackoverflow.com\/users\/9242759\/dq-shen\"\n  on 12:46PM - 02 Jul 18 UTC with link \"https:\/\/stackoverflow.com\/questions\/51136581\/how-to-create-a-normal-distribution-in-pytorch\"\n\n\n\n\n\n\n\nA simple option is to use the  randn  function from the base module. It creates a random sample from the standard Gaussian distribution. To change the mean and the standard deviation you just use addition and multiplication. Below I create sample of size 5 from your requested distribution.\n<code class=\"lang-auto\">import torch\ntorch.randn(5) * 0.5 + 4 # tensor([4.1029, 4.5351, 2.8797, 3.1883, 4.3868])\n<\/code>\nI am still little confused about how they could use \u201crand\u201d function for generating Gaussian images?\nThank you\n\n\n\n dugr:\n\nI am still little confused about how they could use \u201crand\u201d function for generating Gaussian images?\n\n\nThey are not using rand but randn, which is the normal (gaussian) distribution.\nI find this very confusing naming. Why have rand and randn? It\u2019s not very descriptive names of what they are.\nFor example, I assumed the default way to sample was normal for a long time\u2026\nAnyway, do you mind providing a good example of how to use the torch.distributions.Uniform interface?\nI saw the docs and the example in the linked issue but wasn\u2019t very helpful. In particular what passing tensors of size larger than 1 do?\nmy specific question can be found here: https:\/\/github.com\/pytorch\/pytorch\/issues\/24162#issuecomment-658861894\ncopy pasted:\n\n\nI think this already exists?\n<code class=\"lang-python\">a = torch.tensor([0., 10.])\nb = torch.tensor([1., 11.])\ntorch.distributions.Uniform(a, b).sample()\n>>> tensor([ 0.8583, 10.0226])\n<\/code>\n\n<code class=\"lang-auto\">\nThis example is confusing me. What does uniform do when receiving tensors of size larger than 1x1 mean? Whats the behavior when they are greater than 1 dimension?\n\nI saw the docs but they weren't helpful nor any example or discussion of the case you provides is given: https:\/\/pytorch.org\/docs\/stable\/distributions.html#torch.distributions.uniform.Uniform\n\nWhat I want is to replicate this numpy code:\n\n<\/code>\nxn = np.random.uniform(low=-1, high=1, size=(num_samples,Din))\n<code class=\"lang-auto\"><\/code>\n\n\nAlso what is the use of creating a sampler object first?\nThis is the way I found works:\n<code class=\"lang-auto\"># generating uniform variables\n\nimport numpy as np\n\nnum_samples = 3\nDin = 1\nlb, ub = -1, 1\n\nxn = np.random.uniform(low=lb, high=ub, size=(num_samples,Din))\nprint(xn)\n\nimport torch\n\nsampler = torch.distributions.Uniform(low=lb, high=ub)\nr = sampler.sample((num_samples,Din))\n\nprint(r)\n\nr2 = torch.torch.distributions.Uniform(low=lb, high=ub).sample((num_samples,Din))\n\nprint(r2)\n\n# process input\nf = nn.Sequential(OrderedDict([\n    ('f1', nn.Linear(Din,Dout)),\n    ('out', nn.SELU())\n]))\nY = f(r2)\nprint(Y)\n<\/code>\nbut I have to admit I don\u2019t know what the point of generating sampler is and why not just call it directly as I do in the one liner (last line of code).\nComments:\n\nsampler are good for it\u2019s so you can transform\/compose\/cache\/etc distributions. see https:\/\/arxiv.org\/abs\/1711.10604, and the top of the docs of https:\/\/pytorch.org\/docs\/stable\/distributions.html# and https:\/\/arxiv.org\/abs\/1506.05254\n\nyou can feed in tensors to uniform to let it know the high dimensional interval (hypercube) to generate the uniform samples (that\u2019s why it receives tensors as input rather than simply numbers)\n\n\nReference:\n\nhttps:\/\/stackoverflow.com\/questions\/44328530\/how-to-get-a-uniform-distribution-in-a-range-r1-r2-in-pytorch\/62919760#62919760\nGenerating random tensors according to the uniform distribution pytorch? with link \"https:\/\/discuss.pytorch.org\/t\/generating-random-tensors-according-to-the-uniform-distribution-pytorch\/53030\/8\"\nhttps:\/\/github.com\/pytorch\/pytorch\/issues\/24162\n\n\n\n\n Brando_Miranda:\n\nWhy have rand and randn ?\n\n\nThese function names are taken from numpy, which adapted them from MATLAB, if I\u2019m not mistaken, so they are commonly used in scientific software.\nGood to hear your Uniform distribution works. \nThanks for the reply!\nI think what puts me off is that the interface isn\u2019t that similar to numpy. e.g. even if those names are the same there is a .random to call them in numpy but there is not in pytorch which is where my confusion and me searching for uniform came about.\nFor numpy it\u2019s numpy.random.rand or randn while in pytorch you can just do torch.randn or rand and for uniform one has to do torch.distribution.uniform.Uniform or distribution.Distribution.\nAnyway, thanks for clarifying. \nJust curious, why doesn\u2019t it match numpy more closely?\n\n\n\n Brando_Miranda:\n\nJust curious, why doesn\u2019t it match numpy more closely?\n\n\nI think these were early design decisions, which were already introduced in Torch7 (Lua).\nOnce these functions are commonly used, it\u2019s hard to change them due to backwards compatibility. Especially once you reach a non-beta version.\nThanks for taking the time to explain. \nCheers!"},{"x":"I am currently working on a transfer learning problem with a resnet-50. Below is my code for the training. It seems to be working, but the accuracy goes from 0 to 1 in second batch and then stays at 1 for the remaining batches and epochs.\n<code class=\"lang-auto\">import time\n\nepochs = 3\ndevice = torch.device(\"cuda:0\")\n# Define Optimizer and Loss Function\nloss_func = nn.NLLLoss()\noptimizer = torch.optim.Adam(res50.parameters())\n\nfor epoch in range(epochs):\n    epoch_start = time.time()\n    print(\"Epoch: {}\/{}\".format(epoch+1, epochs))\n     \n    # Set to training mode\n    res50.train()\n     \n    # Loss and Accuracy within the epoch\n    train_loss = 0.0\n    train_acc = 0.0\n     \n    valid_loss = 0.0\n    valid_acc = 0.0\n \n    for i, (inputs, labels) in enumerate(train_data):\n \n        inputs = inputs.to(device)\n        labels = labels.to(device)\n         \n        # Clean existing gradients\n        optimizer.zero_grad()\n         \n        # Forward pass - compute outputs on input data using the model\n        outputs = res50(inputs)\n         \n        # Compute loss\n        loss = loss_func(outputs, labels)\n         \n        # Backpropagate the gradients\n        loss.backward()\n         \n        # Update the parameters\n        optimizer.step()\n         \n        # Compute the total loss for the batch and add it to train_loss\n        train_loss += loss.item() * inputs.size(0)\n         \n        # Compute the accuracy\n        ret, predictions = torch.max(outputs.data, 1)\n        correct_counts = predictions.eq(labels.data.view_as(predictions))\n         \n        # Convert correct_counts to float and then compute the mean\n        acc = torch.mean(correct_counts.type(torch.FloatTensor))\n         \n        # Compute total accuracy in the whole batch and add to train_acc\n        train_acc += acc.item() * inputs.size(0)\n         \n        print(\"Batch number: {:03d}, Training: Loss: {:.4f}, Accuracy: {:.4f}\".format(i, loss.item(), acc.item()))\n<\/code>\nBelow is an example output for first five batches. I am not sure if I am calculating accuracy incorrectly?\n<code class=\"lang-auto\">Epoch: 1\/3\nBatch number: 000, Training: Loss: 2.2015, Accuracy: 0.0000\nBatch number: 001, Training: Loss: 0.1964, Accuracy: 1.0000\nBatch number: 002, Training: Loss: 0.0162, Accuracy: 1.0000\nBatch number: 003, Training: Loss: 0.0013, Accuracy: 1.0000\nBatch number: 004, Training: Loss: 0.0001, Accuracy: 1.0000\n<\/code>\nI do have a theory it has to do with classes vs targets. I had to set the dataset targets since I am working with custom dataset. When I do this, the train_data.dataset.targets outputs [2,1,2,3,\u2026,5] according to the class but, the train_data.dataset.classes outputs [\u2019.ipynb_checkpoints\u2019, \u20181\u2019]. I think this is the problem because when I print out labels in nested for loop it is all 1.\nThanks in advance for any help!","y":"There seem to be some issues:\n\nCould you move the data to another folder, without the hidden .ipynb_checkpoints folder? Currently this folder seems to be recognized as class0, which is most likely wrong.\nIt also seems that you are dealing with a single class folder called 1. If that\u2019s the case even moving this data folder to another location would only yield a single class. ImageFolder expects a root folder containing a subfolder with images for each class.\n","z":"\n\n\n natelang:\n\nwhen I print out labels in nested for loop it is all 1.\n\n\nIf that\u2019s the case, your model would only have to output the highest logit for class1 and would achieve a perfect classification.\nSince you are using a custom dataset, I would recommend to check its implementation again and make sure the real targets are returned.\nFeel free to post the Dataset implementation in case you get stuck. \nHi ptrblck thanks for quick reply!\nHere is my dataset implementation. In this, you can see that I attempt to change the data[\u2018train\u2019] \/valid\/test calsses to the correct values.\n<code class=\"lang-auto\"># Load the Data\n\n# Set train and valid directory paths\ntrain_directory = 'datasets\/train'\nvalid_directory = 'datasets\/valid'\ntest_directory = 'datasets\/test'\n \n# Batch size\nbs = 4\n \n# Number of classes\nnum_classes = 8\n \n# Load Data from folders\ndata = {\n    'train': datasets.ImageFolder(root=train_directory, transform=image_transforms['train']),\n    'valid': datasets.ImageFolder(root=valid_directory, transform=image_transforms['valid']),\n    'test': datasets.ImageFolder(root=test_directory, transform=image_transforms['test'])\n}\n \n# Size of Data, to be used for calculating Average Loss and Accuracy\ntrain_data_size = len(data['train'])\nvalid_data_size = len(data['valid'])\ntest_data_size = len(data['test'])\n\n# I have tried both .classes and .labels same result\ndata['train'].classes = train_dic\ndata['valid'].classes = valid_dic\ndata['test'].classes = test_dic\n \n# Create iterators for the Data loaded using DataLoader module\ntrain_data = torch.utils.data.DataLoader(data['train'], batch_size=bs, shuffle=True)\nvalid_data = torch.utils.data.DataLoader(data['valid'], batch_size=bs, shuffle=True)\ntest_data = torch.utils.data.DataLoader(data['test'], batch_size=bs, shuffle=True)\n \n# Print the train, validation and test set data sizes\ntrain_data_size, valid_data_size, test_data_size\n\n<\/code>\nLet me note that train_dic, valid_dic, and test_dic are lists of integers 0-7 according the specific classes. When I try to run the following code snippet of the training\n<code class=\"lang-auto\">epochs = 3\ndevice = torch.device(\"cuda:0\")\n# Define Optimizer and Loss Function\nloss_func = nn.NLLLoss()\noptimizer = torch.optim.Adam(res50.parameters())\n\nfor j, (inputs, labels) in enumerate(train_data):\n        inputs = inputs.to(device)\n        labels = labels.to(device)\n        print(inputs)\n        print(labels)\n<\/code>\nThe results for the inputs refer to the image tensors and labels I expect to be the number 0-7 as mentioned but they are all 1.\n<code class=\"lang-auto\">INPUT:\ntensor([[[[-2.1179, -2.1179, -2.1179,  ..., -2.1179, -2.1179, -2.1179],\n          [-2.1179, -2.1179, -2.1179,  ..., -2.1179, -2.1179, -2.1179],\n          [-2.1179, -2.1179, -2.1179,  ..., -2.1179, -2.1179, -2.1179],\n          ...,\n          [-2.1179, -2.1179, -2.1179,  ..., -2.1179, -2.1179, -2.1179],\n          [-2.1179, -2.1179, -2.1179,  ..., -2.1179, -2.1179, -2.1179],\n          [-2.1179, -2.1179, -2.1179,  ..., -2.1179, -2.1179, -2.1179]],\n...\nLABELS:\ntensor([1, 1, 1, 1], device='cuda:0')\n<\/code>\nAlso, I just read that Image folder creates it\u2019s own labeling so I ran\n<code class=\"lang-auto\">data['train'].class_to_idx\n<\/code>\nand got the following result which I think is the problem.\n<code class=\"lang-auto\">{'.ipynb_checkpoints': 0, '1': 1}\n<\/code>\ncan I change the classes after the fact like I tried above?\nThere seem to be some issues:\n\nCould you move the data to another folder, without the hidden .ipynb_checkpoints folder? Currently this folder seems to be recognized as class0, which is most likely wrong.\nIt also seems that you are dealing with a single class folder called 1. If that\u2019s the case even moving this data folder to another location would only yield a single class. ImageFolder expects a root folder containing a subfolder with images for each class.\n\nI figured it out. I was misunderstanding the way ImageFolder reads in the folder structure. I fixed it accordingly, and had to remove the hidden notebook checkpoint. Thanks!"}]