[
   {},
   {
      "x": "Support multiple simultaneous LR schedulers",
      "z": "A solution was implemented in #26423. Closing this PR, please feel free to re-open if needed.",
      "y": "This issue has been fixed"
   },
   {
      "x": "Want RTX 2080ti Support!!! RuntimeError: cublas runtime error : the GPU program failed to execute at /opt/conda/",
      "z": "We'll supply PyTorch binaries w/ CUDA 10 in the next release. For now you can build from source, as mentioned by @jendrikjoe",
      "y": "You need to build from source"
   },
   {
      "x": "Crash when reading pandas parquet file after importing pyTorch",
      "z": "uninstall the pyarrow installed by pip and then reinstall with conda works for me.",
      "y": "You need to uninstall the pyarrow installed by pip and then reinstall with conda"
   },
   {
      "x": "RuntimeError: Only tuples, lists and Variables supported as JIT inputs, but got dict",
      "z": "Solved.Just change the output of your model from dict to list\n",
      "y": "You need to change the output of your model from dict to list."
   },
   {
      "x": "dataparallel not working on nvidia gpus and amd cpus",
      "z": "I believe this is a duplicate of #1637. Specifically, see [this comment](https://github.com/pytorch/pytorch/issues/1637#issuecomment-338268158). I've had success on a threadripper machine by disabling IOMMU or changing the IOMMU setting to 'soft'.",
      "y": "It works fine on threadripper machine by disabling IOMMU or changing the IOMMU setting to 'soft'."
   },
   {
      "x": "Matrix multiplication operator",
      "z": "this is now fixed in master\\n",
      "y": "This issue has been fixed"
   },
   {
      "x": "Remove dampening from SGD",
      "z": "Made default to 0.\nfixed via 4eb12a2",
      "y": "This issue has been fixed"
   },
   {
      "x": "ImportError: No module named _C",
      "z": "@szagoruyko Could you please try opening torch in any directory other than repo's root? It's trying to load the `torch` dir instead of the python package and gives you this error.\\n",
      "y": "You need to open torch in any directory other than repo's root"
   },
   {
      "x": "Install Error, OSX 10.11.6, fresh miniconda install",
      "z": "i think that's because you have CC and CXX set\\n",
      "y": "The error is because you have CC and CXX set"
   },
   {
      "x": "MaxUnpool2d segfaults for some configurations",
      "z": "Probably fixed in #207. I can't reproduce it anymore, so I'm closing the issue.\\n",
      "y": "This issue has been fixed"
   },
   {
      "x": "[legacy-nn] losses need to return tensors and not numbers",
      "z": "i dont think we should fix legacy-nn",
      "y": "legacy-nn should not be modified."
   },
   {
      "x": "Optim API: per-layer learning rates etc.",
      "z": "this is easily solved by gradient rescale function, specifying per-layer learning rate is actually more pain\\n",
      "y": "this can be solved by gradient rescale function"
   },
   {
      "x": "Add logical AND/OR/NOT/XOR operations",
      "z": "Implemented in #342.",
      "y": "This issue has been fixed"
   },
   {
      "x": "Printing tensors is sometimes very slow",
      "z": "I've reproduced this, looking into fixing it.\\n",
      "y": "This issue has been fixed"
   },
   {
      "x": "numpy.__config__.show() for torch",
      "z": "This has been included in 29ea086 by @ezyang.",
      "y": "This issue has been fixed"
   },
   {
      "x": "Deterministic cudnn algorithms",
      "z": "Parallel data loader is now deterministic.\\n",
      "y": "Parallel data loader is now deterministic."
   },
   {
      "x": "torch dot function consistent with numpy",
      "z": "This has been fixed via #1563 , if we consider that we should follow the behavior of numpy.matmul instead of numpy.dot, according to the thread pointed out by @ngimel.",
      "y": "This issue has been fixed"
   },
   {
      "x": "automatically assign attributes that are variable as parameters?",
      "z": "@glample There's a very simple reason why we don't want people to save Variables as attributes - if they were created as a result of some computation, they will be kept around and they won't free the graph nodes preceding it.\\n\\nWe wanted to think of Modules as thin wrappers around functions that only hold some persistent state like parameters, never any temporary objects (this is handy e.g. for serialization - there's no need for `clearState()`, etc.).\\n\\nI agree it's quite a nice patter, but I'm not sure if we should allow this. @colesbury, any thoughts?\\n",
      "y": "Variables are not saved as attributes since they will be kept around and they won't free the graph nodes preceding it."
   },
   {
      "x": "embeddings layer with IntTensor / cuda.IntTensor inputs",
      "z": "This was added in #46758 and is now working just fine on master for dtype=torch.int.",
      "y": "This issue has been fixed"
   },
   {
      "x": "requirements.txt: cffi >= v1.4.0",
      "z": "Fixed in #161.",
      "y": "This issue has been fixed"
   },
   {
      "x": "More optimizers in torch.optim",
      "z": "Yes, the we only have tests for legacy optim at the moment, and we recommend against using it (it's only for legacy.nn). Thanks for pointing out the naming, I'll change it. #5 is also incorrect, we probably checked it because we implemented trainers and datasets.\\n\\nTo train `torch.nn` networks you should use `torch.optim`. We're aware most algorithms are missing and we'll be implementing them sometime soon. Sorry for the delays.\\n",
      "y": "They will be added soon"
   },
   {
      "x": "error: \u2018float* cblas_sgemm_alloc(CBLAS_IDENTIFIER, int, int, int)\u2019 is deprecated caused by outdated MKL",
      "z": "This is oneapi-src/oneDNN#440. According to the MKL-DNN developers, this can occur if you're compiling against an outdated MKL library. Try updating your MKL.",
      "y": "You need to update your MKL."
   },
   {
      "x": "How can i convert \u2018at::Tensor\u2019 to \u2018const float*\u2019 in libtorch?",
      "z": "`.data<float>()`",
      "y": "This can be done by `.data<float>()`"
   },
   {
      "x": "Bug in CosineAnnealingLR (division by zero)",
      "z": "I've confirmed that reverting #14010 fixes the problem. I'm going to revert it for now and then investigate more.",
      "y": "This issue has been fixed"
   },
   {
      "x": "Incorrect behaviour of min() and argmin()",
      "z": "@Markus-Goetz repasting my comment from #17738 (comment)\n\nthe only determinism we aim to have is hashed on device, and for CPU, single-threaded.\nEven across different kinds of GPUs, all bets are off.\n\nWe cannot guarantee cross-device, cross-CPU-type determinism due to severe performance cliffs that'll result from such a constraint.\n\nFor example, to guarantee that we pick the first argmax (or argmin) element, we have to add an additional pass in our CUDA kernel to sort or order the results, which costs performance.\n\nThis is inconsistent with numpy's, Eigen's, C++ STL etc.\n\nIf you see, you have given examples of CPU kernels, where this is easy to guarantee without significant performance regression.",
      "y": "The behavior is consistent for CPU, but it maybe different for multi-GPUs"
   },
   {
      "x": "[autodiff] Fix owning model used in `differentiate`",
      "z": "We seem to have agreed to leave this as is.",
      "y": "This issue cannot be resolved"
   },
   {
      "x": "Conv3d fail after curtain batch size",
      "z": "I can confirm this is now resolved!",
      "y": "This issue has been fixed."
   },
   {
      "x": "[JIT] state[input] != State::Unknown ASSERT FAILED at /pytorch/torch/csrc/jit/passes/specialize_autogradzero.cpp:57",
      "z": "@YashSinha1996\\r\\n\\r\\nI also have been working on pytorch-pretrained-BERT.\\r\\nI think that the error reported in this issue can be avoided by using reshape instead of contiguous and view. (But I'm not very sure because I'm a novice at pytorch.)\\r\\n\\r\\nThe modification could be\\r\\n```\\r\\n context_layer = torch.matmul(attention_probs, value_layer)\\r\\n- context_layer = context_layer.permute(0, 2, 1, 3).contiguous()\\r\\n+ context_layer = context_layer.permute(0, 2, 1, 3)\\r\\n new_context_layer_shape = context_layer.size()[:-2] + (self.all_head_size,)\\r\\n- context_layer = context_layer.view(*new_context_layer_shape)\\r\\n+ context_layer = context_layer.reshape(*new_context_layer_shape)\\r\\n return context_layer\\r\\n```\\r\\nin pytorch_pretrained_bert/modeling.py\\r\\n",
      "y": "This error can be avoided by using reshape instead of contiguous and view. "
   },
   {
      "x": "librosa tests on Windows don't work",
      "z": "I'm now fixing it with `conda install numba`, which will also install llvmlite.",
      "y": "Please use `conda install numba`"
   },
   {
      "x": "Batch Convolutional Layers - Similar to torch.bmm but for convolutional operators",
      "z": "This looks addressed, so I'm going to close it. Feel free to reopen if I'm mistaken.",
      "y": "This issue has been fixed."
   },
   {
      "x": "[jit] support general buffer mutation",
      "z": "Okay, assigning `self.vector = u.data` works. Since `u` had a gradient, that meant that for subsequent passes `self.vector` had a gradient, which didn't work well.",
      "y": "This issue can be resolved by `self.vector = u.data` "
   },
   {
      "x": "libtorch C++ library does not compile properly",
      "z": "@JerryShih Thanks! `export LD_LIBRARY_PATH=/home/bobw/pytorch/torch/lib/` (libiomp5.so dir) fixes it for me",
      "y": "Please set the variable `export LD_LIBRARY_PATH=/home/bobw/pytorch/torch/lib/` (libiomp5.so dir)"
   },
   {
      "x": "Excessive call to cudaGetDevice and cudaSetDevice",
      "z": "@omry you don't even need to change PyTorch source code. You can just hot patch the CUDA functions to be no-ops:\n\nhttps://gist.github.com/colesbury/b10069870419ca1fa9c3a2a8668edbe3\n\nYou'll see that outside the profiler the cudaGetDevice and cudaSetDevice functions do not affect performance. You can also use this under nvprof to avoid API tracing those functions.",
      "y": "Please use the hotfix here https://gist.github.com/colesbury/b10069870419ca1fa9c3a2a8668edbe3"
   },
   {
      "x": "[Tutorial]: Wrong example of Our Own Ring-Allreduce",
      "z": "@rohan-varma How about the following code? I think it provides the same functionality and is easier to understand.\\r\\n```python\\r\\nsend_req = dist.isend(send_buff, right)\\r\\ndist.recv(recv_buff, left) # recv is a blocking operation.\\r\\naccum[:] += recv_buff[:]\\r\\nsend_buff[:] = recv_buff[:]\\r\\n```",
      "y": "The following example is better \\r\\n```python\\r\\nsend_req = dist.isend(send_buff, right)\\r\\ndist.recv(recv_buff, left) # recv is a blocking operation.\\r\\naccum[:] += recv_buff[:]\\r\\nsend_buff[:] = recv_buff[:]\\r\\n```"
   },
   {
      "x": "the example program using libtorch is not linked against torch_cuda when USE_CUDA is ON",
      "z": "I just found out that this issue could be solved by using the `/INCLUDE` switch. torch_cuda.dll contains the function signature `?warp_size@cuda@at@@YAHXZ` (`int __cdecl at::cuda::warp_size(void)`). So adding `/INCLUDE:\\\"?warp_size@cuda@at@@YAHXZ\\\"` to the linker flags will force the library/executable to link against `torch_cuda.dll`. What do you think, @ezyang ?\\r\\n\\r\\nReference:\\r\\nhttps://docs.microsoft.com/en-us/cpp/build/reference/include-force-symbol-references?view=vs-2019",
      "y": "This issue could be solved by using the `/INCLUDE` switch"
   },
   {
      "x": "PytorchStreamReader failed reading zip archive: failed finding central directory (no backtrace available)",
      "z": "In my case, this error was caused by a corrupted saved file. So I switch to older checkpoints and the problem is gone.",
      "y": "Switching to older checkpoints can fix the problem"
   },
   {
      "x": "test_conv_transposed_large_cuda failed on Windows",
      "z": "The failure is seen in CI.",
      "y": "The failure is seen in CI."
   },
   {
      "x": "[docs] torch.onnx.export docs contains two descriptions for example_outputs arg",
      "z": "Fixed in #31826",
      "y": "This issue has been fixed."
   },
   {
      "x": "The Feature Request of Loading Quantized TorchScript Model on Windows with libtorch",
      "z": "Closing it because the original issue should be resolved. If users want 32-bit quantization support, they can open a new issue.",
      "y": "This issue has been fixed."
   },
   {
      "x": "Pip torch_nightly on macOS installs wrong build",
      "z": "I believe this should now be resolved for `macOS`:\\r\\n\\r\\n```\\r\\npytorch37 \u276f pip install --pre torch -f https://download.pytorch.org/whl/nightly/cpu/torch_nightly.html\\r\\nLooking in links: https://download.pytorch.org/whl/nightly/cpu/torch_nightly.html\\r\\nCollecting torch\\r\\n Downloading https://download.pytorch.org/whl/nightly/cpu/torch-1.5.0.dev20200110-cp37-none-macosx_10_9_x86_64.whl (82.4MB)\\r\\n```\\r\\n\\r\\nNot entirely sure about where we are on windows nightly builds though, cc @peterjc123",
      "y": "This issue has been fixed"
   },
   {
      "x": "how to set cuda stream by call Aten function",
      "z": "1. CUDAStreamGuard will reset the stream to the previously used stream once it goes out of scope, if instead you are using stream manually, you may forget to change it back, thus unexpectedly changing current stream for the user. \\r\\n2. Using setCurrentCUDAStream won't influence operations running on other streams. You need to make sure though that the default stream is synchronized with the stream your operation is running on. In many cases, using concurrent streams does not provide appreciable benefits.",
      "y": "1. CUDAStreamGuard will reset the stream to the previously used stream once it goes out of scope, if instead you are using stream manually, you may forget to change it back, thus unexpectedly changing current stream for the user. \\r\\n2. Using setCurrentCUDAStream won't influence operations running on other streams. You need to make sure though that the default stream is synchronized with the stream your operation is running on. In many cases, using concurrent streams does not provide appreciable benefits."
   },
   {
      "x": "`enable_grad` context doesn't work as expected in backward function of torch.autograd.Function",
      "z": "Thanks for the report, I will take a look why this happens. It might simply be that we don't restore the status in `autograd.grad` properly.",
      "y": "This error is because we don't restore the status in `autograd.grad` properly."
   },
   {
      "x": "dyld: Library not loaded: /usr/local/opt/openssl/lib/libssl.1.0.0.dylib Referenced from: /usr/local/bin/sccache",
      "z": "For posterity this is what CircleCI responded with:\\r\\n\\r\\n----\\r\\n\\r\\nWe have looked into this issue and have found that this is being caused by a dependency conflict.\\r\\n\\r\\nTo install libomp requires the following dependencies (taken from the build log):\\r\\n\\r\\n==> Installing dependencies for libomp: pkg-config, gdbm, openssl@1.1, readline, sqlite, xz, python, sphinx-doc and cmake\\r\\nThis package is specifically installing OpenSSL 1.1 and higher. When homebrew installs OpenSSL, it will change the symlinks in \\r\\n/usr/local/opt/openssl/ to point to the latest version (which truly resides in \\r\\n/usr/local/opt/openssl@1.1). This in turns means the file \\r\\n/usr/local/opt/openssl/lib/libssl.1.0.0.dylib is no longer in that directory as it has been replaced by \\r\\nlibssl.1.1.dylib.\\r\\n\\r\\nThe previous version of OpenSSL (1.0) can be found in \\r\\n/usr/local/opt/openssl@1.0/ which is where the previous version of the dylib, which is trying to be used, can be found (libssl.1.0.0.dylib).\\r\\n\\r\\nYour script seems to be using something that is trying to specifically use this version of the dylib. Best practise would be to instead use \\r\\n/usr/local/opt/openssl/lib/libssl.dylib which will always point to the latest version of openssl as it is not requesting a specific version. You may need to check the dependencies of the packages you are using and make sure to pin specific versions of these packages to prevent this kind of conflict.\\r\\n\\r\\nDo you know if any packages you use have been updated recently?",
      "y": "You may need to check the dependencies of the packages you are using and make sure to pin specific versions of these packages to prevent this kind of conflict."
   },
   {
      "x": "Distributed Using Gloo on Multiple Nodes Does not Work",
      "z": "I think that would create problems with the ranks. For worker_id 0, the ranks would be 0 and 1 and for worker_id 1 the ranks would be 1 and 2. Can you try with worker ids 0 and 2? Your ranks should be 0, 1, 2, 3 for world size 4.",
      "y": "Your ranks should be 0, 1, 2, 3 for world size 4."
   },
   {
      "x": "The guidelines for loading a PyTorch model in C++ do not work on Windows",
      "z": "You'll need to copy the PDBs along with your executable.",
      "y": "You'll need to copy the PDBs along with your executable."
   },
   {
      "x": "Fix 1.3.1 branch submodule fbjni dependency",
      "z": "Now that this has stayed unchanged for longer and `v1.5.0` is out, moving the tag seems less needed and less attractive (EDIT: and it can always still be done in the future). \\r\\n\\r\\nI propose to close this - will do so next week unless someone disagrees.",
      "y": "This issue has been fixed."
   },
   {
      "x": "Connection closed by peer when using L-BFGS and distributed computing (gloo)",
      "z": "Excellent!\n\nIt should be possible to come up with an optimizer that is distributed-aware, where all these decisions are made globally instead of by a single machine. Then you can use it in a distributed setting, but will have to average losses, or pick one of the machines are the primary.",
      "y": "It should be possible to come up with an optimizer that is distributed-aware, where all these decisions are made globally instead of by a single machine. Then you can use it in a distributed setting, but will have to average losses, or pick one of the machines are the primary."
   },
   {
      "x": "`torch.Size` is tranfered to`torch.Tensor`, values don't equal",
      "z": "`torch.Tensor(shape)` creates an empty tensor of shape `shape`. Don't use deprecated uppercase `Tensor`. Use `torch.tensor(shape)` to create a tensor containing the same value as `shape`.",
      "y": "`torch.Tensor(shape)` creates an empty tensor of shape `shape`. Don't use deprecated uppercase `Tensor`. Use `torch.tensor(shape)` to create a tensor containing the same value as `shape`."
   },
   {
      "x": "Why when I use torch.cuda.empty_cache(), it cost some gpu memory on other device?",
      "z": "@peterzpy see [here](https://discuss.pytorch.org/t/out-of-memory-when-i-use-torch-cuda-empty-cache/57898/3) and [here](https://github.com/pytorch/pytorch/issues/25752#issuecomment-528866347) for a solution. Basically, you need to specify the gpu device, before calling to `empty_cache`.",
      "y": "You need to specify the gpu device, before calling to `empty_cache`."
   },
   {
      "x": "Ruby Library",
      "z": "Also added a Homebrew formula so users can now do:\\r\\n\\r\\n```sh\\r\\nbrew install libtorch\\r\\n```\\r\\n\\r\\nhttps://github.com/Homebrew/homebrew-core/pull/47222",
      "y": "Users can execute :\\r\\n\\r\\n```sh\\r\\nbrew install libtorch\\r\\n```"
   },
   {
      "x": "[jit] Spurious error when type comments are found in the body of a function.",
      "z": "Oh sorry I see it has been fixed.",
      "y": "This issue has been fixed"
   },
   {
      "x": "[jit] Cannot create a `Tuple[List[T]]`",
      "z": "This is expected behavior. In python tuple(x) would return a tuple of [T, ...] which is an unknown length. In the second example you're creating a Tuple[List[T]], which has a fixed length so we can compile it.",
      "y": "This is expected behavior. In python tuple(x) would return a tuple of [T, ...] which is an unknown length."
   },
   {
      "x": "RuntimeError: stack.size() >= num_inputs INTERNAL ASSERT FAILED",
      "z": "Closed with #31724",
      "y": "This issue has been fixed"
   },
   {
      "x": "failed to convert torch.jit.ScriptModule to ONNX (crash)",
      "z": "@linronmo, as I explained above, the change for flatten could only be done for opset 11.\nSo you can export your model in opset_version=11, for that, you should use the parameter opset_version=11 in the exporter api with PyTorch nighly: torch.onnx.export(..., opset_version=11).",
      "y": "Please specify the parameter `torch.onnx.export(..., opset_version=11).`"
   },
   {
      "x": "[Bug report] RuntimeError: backward_input can only be called in training mode",
      "z": "@simon555 You only set training mode before the loop, but set eval mode in generate_predictions, which is called in the middle of the loop. So iterations after generate_predictions will fail.",
      "y": "The mode i.e. train/eval needs to be consistent throughout the training loop."
   },
   {
      "x": "[Caffe2] Windows build errors in generated file caffe2.pb.h",
      "z": "I remember I've seen this issue before.\\r\\n\\r\\nIt's related to a funny combination of `nvcc` and `protobuf` actually if my memory is correct.\\r\\n`PROTOBUF_CONSTEXPR` is defined to constexpr.\\r\\nHowever in nvcc 9.1 you need to get rid of it in order to do in-place initialization.\\r\\n\\r\\nSo...the solution is to patch protobuf...\\r\\n\\r\\nBTW this issue is on protobuf 3.5.\\r\\n`master` branch of protobuf should not have this issue but it has something else...",
      "y": "It's related to a funny combination of `nvcc` and `protobuf` actually if my memory is correct.\\r\\n`PROTOBUF_CONSTEXPR` is defined to constexpr.\\r\\nHowever in nvcc 9.1 you need to get rid of it in order to do in-place initialization.\\r\\n\\r\\nSo...the solution is to patch protobuf..."
   },
   {
      "x": "Segmentation fault on importing torch",
      "z": "I assume `pip install git+https://github.com/mwydmuch/ViZDoom` compiles ViZDoom, correct? In that case, since your system compiler is GCC 4.84, it will be incompatible with PyTorch bindings, since we compile with GCC 4.9 and the two are ABI incompatible, which leads to segfaults.\\r\\n\\r\\nYou should either:\\r\\nA) Compile PyTorch from source, with your system compiler. Then both projects will have been compiled with the same compiler and the problem goes away.\\r\\nB) Install GCC 4.9 ([see instructions here](https://gist.github.com/goldsborough/d466f43e8ffc948ff92de7486c5216d6)) and set the `CXX` environment variable before compiling VizDoom (`export CXX=g++-4.9`). Then VizDoom should be compiled with the same compiler as PyTorch and the problem should go away.\\r\\n\\r\\nEither way, you have to make sure both are compiled with the same compiler version.",
      "y": "You have to make sure both are compiled with the same compiler version. \\n Either compile PyTorch from source or install  GCC 4.9"
   },
   {
      "x": "[feature request] [pytorch] Convenience method for doing unsqueeze / squeeze several times",
      "z": "Why not using numpy manner? \\r\\n`x[..., None, None]` and `x[..., 0,0]` for your example",
      "y": "Use the following \\r\\n`x[..., None, None]` and `x[..., 0,0]` in your example"
   },
   {
      "x": "np.repeat vs torch.repeat",
      "z": "Easiest thing to do for now is to add a warning to the docs. One day we'll probably re-visit the all the differences between the Numpy API and pytorch API, but for now, just changing repeat might be disruptive to our users.\n\nFeel free to submit a pull request @PetrochukM (otherwise, I can submit one).",
      "y": "The two APIs have a different behavior."
   },
   {
      "x": "[JIT]torch._C._infer_size throws an exception when traced",
      "z": "Got it! I think this should work fine for most of our use cases, since this is only called when distribution instances are being constructed, after which the parameters are frozen and should have a fixed shape. One possible downside might be that we will need to pass tensors with the correct shape in the torch.trace annotation, and that can get tricky if it depends on the minibatch size. So eventually, it will be nice to have a more generic support for JIT. cc. @fritzo, @eb8680.",
      "y": "You need to pass tensors with the correct shape in the torch.trace annotation"
   },
   {
      "x": "question: where (if) are the caffe2 libraries?",
      "z": "Caffe2 only has packages in conda at the moment. 'conda install -c caffe2 caffe2' will install the Caffe2 libraries into an Anaconda env; they will be linked against other libraries in the Anaconda env as well.",
      "y": "To install the Caffe2 libraries, use  'conda install -c caffe2 caffe2' "
   },
   {
      "x": "failed to move parameters to GPU",
      "z": "or make self.conv = nn.ModuleList()",
      "y": "To fix this, use `self.conv = nn.ModuleList()`"
   },
   {
      "x": "Inconsistent behavior of F.conv2d(...,padding) and F.pad",
      "z": "Got it. I think you mean `torch.backends.cudnn.deterministic=True`",
      "y": "You need to set `torch.backends.cudnn.deterministic=True`"
   },
   {
      "x": "[Caffe2] ld: can't map file, errno=22 file '/usr/local/cuda/lib/stubs/cuda.framework' for architecture x86_64",
      "z": "You'll need to do a clean build from scratch. Also, you'll still need a cuda.framework fix of some sort or another. Working on a more robust version of this fix.",
      "y": "You'll need to do a clean build from scratch"
   },
   {
      "x": "[feature request] Clarify document to avoid \\\"Error Importing cuda extension\\\"",
      "z": "Once your extension is built, you can simply import it in Python, using the name you specified in your setup.py script. Just be sure to import torch first, as this will resolve some symbols that the dynamic linker must see:",
      "y": "You need to import torch first, as this will resolve some symbols that the dynamic linker must see:"
   },
   {
      "x": "RuntimeError: cuda runtime error (30) : unknown error at /pytorch/aten/src/THC/THCGeneral.cpp:70",
      "z": "is there something in your LD_LIBRARY_PATH that might override general RPATH? For example do you have cuda 9.2 in your LD_LIBRARY_PATH?",
      "y": "Please ensure that there isn't any path in LD_LIBRARY_PATH that might override general RPATH"
   },
   {
      "x": "[pytorch] [feature request] Add torch.broadcast (e.g. for using with torch.stack)",
      "z": "We have a helper `torch.distributions.utils.broadcast_all()` that should allow you to\\r\\n```py\\r\\ntorch.stack(broadcast_all(a, b))\\r\\n```",
      "y": "The helper function `torch.distributions.utils.broadcast_all()` that should allow you to\\r\\n```py\\r\\ntorch.stack(broadcast_all(a, b))\\r\\n```"
   },
   {
      "x": "Python interpreter died without Traceback when CPU Tensor divided by zero.",
      "z": "As tricky as this may be to fix, it's still a bug in a very real sense. Allowing C errors to creep into Python code is the result of a leaky abstraction.",
      "y": "This error is the result of a leaky abstraction."
   },
   {
      "x": "Inconsistent gradient results in F.grid_sample using torch.autograd.grad with create_graph=True",
      "z": " confirmed with @apaszke as well that this is expected behavior.\n\nHe says:\n\nbecause create_graph=True, this is expected.\nIn the a+b case, basically the gradient of the gradient (wrt parameters) is all 0, which means it doesn\u2019t require grad, because it effectively never touched a single variable that does require grad.",
      "y": "This is an expected behavior and not a bug."
   },
   {
      "x": "AttributeError: module 'torch._C' has no attribute '_TensorBase'",
      "z": "Can you try uninstall and reinstall? I'm assuming that you built from source. Can you also check if you have binary pytorch installed?",
      "y": "Try to reinstall and make sure that you have binary pytorch installed."
   },
   {
      "x": "Feature request: Object detection model zoo",
      "z": "Torchvision now has models for object detection and semantic segmentation officially supported",
      "y": "Torchvision now has models for object detection and semantic segmentation officially supported"
   },
   {
      "x": "dynamically change tensor with requires_grad=False by \\\"+=\\\" cause error but \\\"+\\\" doesn't",
      "z": "`x = x + y` is something very different from `x += y` for Python semantics. The latter creates a new variable, the former modifies the variable in place.",
      "y": "`x = x + y` is something very different from `x += y` for Python semantics. The latter creates a new variable, the former modifies the variable in place."
   },
   {
      "x": "python setup.py install failed. undefined references",
      "z": "We had the same issue. Problem was the version of MKL which was not good. Just running:\\r\\n\\r\\n```\\r\\nconda remove mkl mkl-include\\r\\nconda install numpy pyyaml mkl=2019.3 mkl-include setuptools cmake cffi typing\\r\\n```\\r\\n\\r\\nbefore the installation fixed the issue.",
      "y": "You need to run :\\r\\n\\r\\n```\\r\\nconda remove mkl mkl-include\\r\\nconda install numpy pyyaml mkl=2019.3 mkl-include setuptools cmake cffi typing\\r\\n```\\r\\n\\r\\nbefore the installation to fix the issue."
   },
   {
      "x": "THCudaCheck FAIL file=..\\\\aten\\\\src\\\\THC\\\\THCGeneral.cpp line=87 error=30 : unknown error",
      "z": "somehow restarting my machine fixed this issue. using torch v1.1.0",
      "y": "Restarting the machine can fix this issue."
   },
   {
      "x": "torchfile.T7ReaderException: unknown object type / typeidx: -1112529805",
      "z": "Please try appending the parameter long_size like this `load_lua(....., long_size=8)`.",
      "y": "You need to append the parameter long_size like this `load_lua(....., long_size=8)`."
   },
   {
      "x": "RuntimeError: CUDA error: unknown error",
      "z": "I had exactly the same issue. i got it fixed by installing the right pytorch version for my 10.1 CUDA with:\\r\\n`conda install pytorch torchvision cudatoolkit=10.1 -c pytorch`\\r\\ninstead of installing it with the installtion snippet generated for me by pytorch website:\\r\\n`conda install pytorch torchvision cudatoolkit=10.0 -c pytorch`\\r\\n",
      "y": "You need to install the right pytorch version for 10.1 CUDA with:\\r\\n`conda install pytorch torchvision cudatoolkit=10.1 -c pytorch`\\r\\ninstead of installing it with the installtion snippet generated by pytorch website:\\r\\n`conda install pytorch torchvision cudatoolkit=10.0 -c pytorch`\\r\\n"
   },
   {
      "x": "optim.lr_scheduler.CyclicLR (master only: not released) is buggy when not using momentum",
      "z": "I think you are correct, if you indent the second and third to last lines in the init it works.",
      "y": "It can be fixed by indenting the second and third to last lines in the init"
   },
   {
      "x": "Different deterministic behavior between CPU and CUDA for orthogonal initialization",
      "z": "This is expected. We do not guarantee that the random sequences generated on different devices will look the same. For performance reasons they are likely using different pseudo random generators, so it's almost impossible to make them equal.",
      "y": "This is expected. It is not guaranteed that the random sequences generated on different devices will look the same."
   },
   {
      "x": "C++ torch::Tensor serialization",
      "z": "Cool!! I just checked that I can both save and load torch::Tensors and std::vector<torch::Tensor>s with the latest macOS libtorch! Closing this now :)",
      "y": "Using  the latest macOS libtorch, one can both save and load torch::Tensor and std::vector<torch::Tensor>"
   },
   {
      "x": "Compiling from master yields std::runtime_error",
      "z": "@pietern Thanks for tracking this.\n\nYet, I've been able \"overcome\" this somehow by compiling PyTorch again with a git clean -fdx in between, thus I don't have the logs for the failed compilation.\n\nI just tried again with the same command line I ran this morning and the error doesn't occur. I'll investigate a little bit more on my side, to see if I can reproduce this, and will follow up on this topic.",
      "y": "You need to compile PyTorch again with a git clean -fdx in between, "
   },
   {
      "x": "Improved documentation of distributed launch utility",
      "z": "Thanks for reporting! We are aware of this confusing doc issue and are actively working on remediating it. Please track our progress in https://github.com/pytorch/pytorch/issues/60754. Closing in favor of 60754.",
      "y": "We are working on this."
   },
   {
      "x": "test_dataloader.py fails to pass test with error: Can't get attribute 'RandomDataset'... on MacOS",
      "z": "Created a conda env with Python=3.7, followed the same steps to install Pytorch and tried to reproduce the issue.\\r\\n\\r\\ntest_dataloader.py passes the test.\\r\\n\\r\\nSeems that it is an issue related to Python 3.8 on Mac",
      "y": "This is an issue related to Python 3.8 on Mac"
   },
   {
      "x": "[package] error in colab tutorial #60189",
      "z": "This error is intentional\u2014you need to specify how you will handle your dependencies. Further along in the tutorial there is guidance on how to resolve the error.",
      "y": "This error is intentional\u2014you need to specify how you will handle your dependencies. Further along in the tutorial there is guidance on how to resolve the error."
   },
   {
      "x": "USE_SYSTEM_ONNX: onnx/optimizer/optimize.h: No such file or directory",
      "z": "I got the same error, and I fixed it by updating the submodules:\\r\\n```\\r\\ngit submodule sync\\r\\ngit submodule update --init --recursive\\r\\n```",
      "y": "You need to update the submodules:\\r\\n```\\r\\ngit submodule sync\\r\\ngit submodule update --init --recursive\\r\\n```"
   },
   {
      "x": "Can I train AI If AI model is located in the another model\u2019s forward?",
      "z": "You may want to add model2 to an attribute of model1 in \\\\_\\\\_init\\\\_\\\\_. Then the parameters of model1 include that of model2 and so model2 would be trained.\\r\\n\\r\\nfyi questions are supposed to be posted to https://discuss.pytorch.org/ as it's more likely you will get help there.",
      "y": "You may want to add model2 to an attribute of model1 in \\\\_\\\\_init\\\\_\\\\_. Then the parameters of model1 include that of model2 and so model2 would be trained.\\r\\n\\r\\n"
   },
   {
      "x": "M1 Mac: `torch.dot()` returns unexpeted values for tensors of `torch.float32`",
      "z": "Will be fixed in next nightly build",
      "y": "This will be fixed in the next nightly build."
   },
   {
      "x": "it seems n_heads is not handled correctly in nn.MultiheadAttention",
      "z": "As a simple workaround, couldn't you pass embed_dim * num_heads as the embed_dim? When it is divided amongst the heads, it should result in what you want.",
      "y": "You can pass embed_dim * num_heads as the embed_dim"
   },
   {
      "x": "`test_transpose_inplace_view_xla` & `test_t_inplace_view_xla` are flaky",
      "z": "@imaginary-person This test was failing consistently on master yesterday so I disabled them for XLA first. @bdhirsh and I will look into the root cause later today. Thanks for reporting!",
      "y": "You need to diable them for XLA."
   },
   {
      "x": "Does the NCCL operation use the default stream as other computations?",
      "z": "The nccl implementation under the hood uses a custom stream pool and each time you launch collective operations, appropriate synchronizations are done automatically. So before the allreduce is launched, the nccl stream synchronizes with the default stream to capture computations appropriately. Also, after nccl allreduce is done, the default stream synchronizes with the nccl stream. As a result, you don't need to perform any synchronizations.\\r\\n\\r\\nIf you provide `async_op=True` to all_reduce, the synchronization will not be done after allreduce and only when you call `work.wait()`. So if you want to overlap computation that is not related to nccl allreduce, you can use the async_op option.",
      "y": "If you provide `async_op=True` to all_reduce, the synchronization will not be done after allreduce and only when you call `work.wait()`. So if you want to overlap computation that is not related to nccl allreduce, you can use the async_op option."
   },
   {
      "x": "Add container for recurrent nets",
      "z": "No, definitely not to `Module`. We can consider adding a base class for RNNs",
      "y": "No, definitely not to `Module`. We can consider adding a base class for RNNs"
   },
   {
      "x": "Support for einsum notation",
      "z": "Yeah, Pytorch lacks this. I can implement the feature",
      "y": "Support for einsum notation has been added"
   },
   {
      "x": "GPU usage extremely in-balance for segmentation task",
      "z": "The code hasn't been released yet. You can leave the device_ids empty and use CUDA_VISIBLE_DEVICES=0,1 to control the number of GPUs",
      "y": "Leave the device_ids empty and use CUDA_VISIBLE_DEVICES=0,1 to control the number of GPUs"
   },
   {
      "x": "How to select GPU programmatically in code",
      "z": "```\\r\\nimport os\\r\\nos.environ[\\\"CUDA_DEVICE_ORDER\\\"]=\\\"PCI_BUS_ID\\\" \\r\\nos.environ[\\\"CUDA_VISIBLE_DEVICES\\\"]=\\\"1\\\"\\r\\n```\\r\\n\\r\\nAre you looking for this?",
      "y": "```\\r\\nimport os\\r\\nos.environ[\\\"CUDA_DEVICE_ORDER\\\"]=\\\"PCI_BUS_ID\\\" \\r\\nos.environ[\\\"CUDA_VISIBLE_DEVICES\\\"]=\\\"1\\\"\\r\\n```\\r\\n"
   },
   {
      "x": "Pooling throws an exception in Tegra TX1",
      "z": "this is a weird error. Are you sure CuDNN is being detected at compile time?\n\nYou might have to set CUDNN_INCLUDE_DIR and CUDNN_LIB_DIR environment variables.\nYou can check cudnn's status with: print(torch.backends.cudnn.enabled)",
      "y": "You might have to set `CUDNN_INCLUDE_DIR` and `CUDNN_LIB_DIR` environment variables.\n"
   },
   {
      "x": "[Feature Request] Cyclical Learning Rates",
      "z": "Hi. I am the puller of LR Scheduler. IMO, you should be able to easily implement this using class LambdaLR. @ajbrock\n\n",
      "y": "This can be easily implemented using class `LambdaLR`"
   },
   {
      "x": "Get a single batch from DataLoader without iterating",
      "z": "`next(iter(data_loader))` ?",
      "y": "Use `next(iter(data_loader))` "
   },
   {
      "x": "Different behaviour of BCEWithLogitsLoss and BCELoss + Sigmoid",
      "z": "Thanks for reporting this @martinarjovsky.\n\n@soumith not sure this is a valid use of the losses - i.e. the output and target have different shapes (one is batched and one is not)?\n\nBecause of the different shapes the broadcasting has side effects (https://github.com/gchanan/pytorch/wiki/Broadcasting-Notes#backwards-compatibility)\n\nAccording to the docstrings of binary_cross_entropy and binary_cross_entropy_with_logits the input and target should have the same shape. Shall we just add a check and raise a ValueError if they are not? Happy to send a PR for this",
      "y": "The behavior is same if `binary_cross_entropy` and `binary_cross_entropy_with_logits` the input and target should have the same shape"
   },
   {
      "x": "[pylint] E1101:Module 'torch' has no 'squeeze' member",
      "z": "On VS code:\nAdd \"python.linting.enabled\": false to the settings file. Worked for me.",
      "y": "You need to change the settings for your editor. On VS code, you need to add \"python.linting.enabled\": false to the settings file."
   },
   {
      "x": "[Feature Request] Layer Normalization",
      "z": "I use this:\\r\\n\\r\\n```python\\r\\nclass LayerNorm(nn.Module):\\r\\n\\r\\n def __init__(self, features, eps=1e-6):\\r\\n super().__init__()\\r\\n self.gamma = nn.Parameter(torch.ones(features))\\r\\n self.beta = nn.Parameter(torch.zeros(features))\\r\\n self.eps = eps\\r\\n\\r\\n def forward(self, x):\\r\\n mean = x.mean(-1, keepdim=True)\\r\\n std = x.std(-1, keepdim=True)\\r\\n return self.gamma * (x - mean) / (std + self.eps) + self.beta\\r\\n```",
      "y": "This code segment can acheive layer normalization:\\r\\n\\r\\n```python\\r\\nclass LayerNorm(nn.Module):\\r\\n\\r\\n def __init__(self, features, eps=1e-6):\\r\\n super().__init__()\\r\\n self.gamma = nn.Parameter(torch.ones(features))\\r\\n self.beta = nn.Parameter(torch.zeros(features))\\r\\n self.eps = eps\\r\\n\\r\\n def forward(self, x):\\r\\n mean = x.mean(-1, keepdim=True)\\r\\n std = x.std(-1, keepdim=True)\\r\\n return self.gamma * (x - mean) / (std + self.eps) + self.beta\\r\\n```"
   },
   {
      "x": "Import fails after Conda install",
      "z": "your `cudatoolkit` from anaconda is build 2, which had a bug. Anaconda team fixed this in build 3. Do this: `conda install cudatoolkit`, and that should install build 3.",
      "y": "You need to install the latest build via `conda install cudatoolkit`"
   },
   {
      "x": "ReduceOps are breaking Pyro test",
      "z": "I found the bug, I'll send a patch soon.",
      "y": "This issue has been fixed."
   },
   {
      "x": "Linux CPU build script fails as MKL header files not found",
      "z": "If you are not using conda, pip install mkl-devel gets you the headers. I haven't looked at your CI script. @SsnL can we add this to the error message?\n\n",
      "y": "To install the header files, use `pip install mkl-devel`"
   },
   {
      "x": "RuntimeError: value cannot be converted to type uint8_t without overflow: 10000",
      "z": "yeah the tutorials needs to be updated. for now, you can try this:\\r\\n```\\r\\ncorrect += (predicted == labels).sum().item()\\r\\n```",
      "y": "You can use this:\\r\\n```\\r\\ncorrect += (predicted == labels).sum().item()\\r\\n```"
   },
   {
      "x": "CUDNN_STATUS_NOT_INITIALIZED when built from source",
      "z": "After these updates:\\r\\n- Nvidia driver: 384.81-> 387.26 \\r\\n- CUDA: V9.0.176 -> V9.1.85\\r\\n- CuDNN: 7005 -> 7102\\r\\n\\r\\nThe installation completes without error. Thanks!",
      "y": "You need to install the latest versions of Nvidia driver, CUDA, and CuDNN"
   },
   {
      "x": "I cannot initialize Tensor. (They will become torch.autograd.variable.Variable)",
      "z": "See https://github.com/pytorch/pytorch/pull/5225",
      "y": "This issue has been fixed."
   },
   {
      "x": "[feature request] Add underscore to nn.init functions",
      "z": "That makes sense, but we need to keep backward compatibility, so we need the non-`_` aliases too.",
      "y": "Non-underscore functions have been retained for backward compatibility. "
   },
   {
      "x": "How can I access the model's attribution created during forward pass when using dataparallel?",
      "z": "Returning what you want from the top-level forward function should defiantly work. You could do this pretty easily with an additional wrapper. As such:\\r\\n\\r\\n```python\\r\\n\\r\\nclass NormalModel(nn.Module):\\r\\n pass\\r\\n\\r\\nclass ExtraOutputWrapper(nn.Module):\\r\\n def __init__(self, *args, **kw):\\r\\n self.wrapped = NormalModel(*args, **kw)\\r\\n\\r\\n def forward(self, input):\\r\\n normal_output = self.wrapped(input)\\r\\n extra_output = self.wrapped.layer_of_interest.property\\r\\n return normal_output, extra_output\\r\\n```\\r\\n\\r\\nThen wrap `ExtraOutputWrapper` with `DataParallel` and it will return the property of interest. This seems fairly clean to me. Is there an issue with this?",
      "y": "You can use the following code:\\r\\n\\r\\n```python\\r\\n\\r\\nclass NormalModel(nn.Module):\\r\\n pass\\r\\n\\r\\nclass ExtraOutputWrapper(nn.Module):\\r\\n def __init__(self, *args, **kw):\\r\\n self.wrapped = NormalModel(*args, **kw)\\r\\n\\r\\n def forward(self, input):\\r\\n normal_output = self.wrapped(input)\\r\\n extra_output = self.wrapped.layer_of_interest.property\\r\\n return normal_output, extra_output\\r\\n```\\r\\n\\r\\nThen wrap `ExtraOutputWrapper` with `DataParallel` and it will return the property of interest."
   },
   {
      "x": "Linking Error: relocation R_X86_64_32 against `cpuinfo_x86_linux_init' can not be used when making a shared object",
      "z": "The most recent version of cpuinfo should fix this. We have to update the submodule.",
      "y": "Install the latest version of cpuinfo to fix this."
   },
   {
      "x": "fatal error: torch/torch.h: No such file or directory",
      "z": "Right now you have to `build install` pytorch for that header to be installed correctly. There's three options:\\r\\n1. Merge https://github.com/pytorch/pytorch/pull/5772 which fixes this and looks ready anyway,\\r\\n2. `python setup.py build install`\\r\\n3. `python run_test.py --exclude cpp_extensions`\\r\\n\\r\\n(1) is probably best",
      "y": "There are three options:\\r\\n1. Merge https://github.com/pytorch/pytorch/pull/5772 which fixes this and looks ready anyway,\\r\\n2. `python setup.py build install`\\r\\n3. `python run_test.py --exclude cpp_extensions`\\r\\n\\r\\n(1) is probably best"
   },
   {
      "x": "pytorch installation error on macOS 10.13.3",
      "z": "^ resolved by reinstalling tbb",
      "y": "This error can be resolved by reinstalling TBB."
   },
   {
      "x": "Pretrained Model Loading Error",
      "z": "If you were to load the file using pickle alone, I'd suggest trying to load it using `encoding='latin1'` for example.\\r\\n```python\\r\\nwith open(myfile, 'rb') as f:\\r\\n data = pickle.load(f, encoding='latin1')\\r\\n```\\r\\nThis has solved issues for me when deserializing some pickle files saved on python2 and loaded on python3. But to make it work out-of-the-box on `torch.load` might require extending the API.",
      "y": "Add the `encoding=latin1` argument to `pickle.load`"
   },
   {
      "x": "Bazel - Pybind - Pytorch - Undefined Symbol",
      "z": "Resolved. Turns out you need to add 'linkshared=True' and compile it as a binary",
      "y": "You need to add 'linkshared=True' and compile it as a binary"
   },
   {
      "x": "TypeError: __init__() should return None, not 'int' in validation Dataset",
      "z": "Closing, as this does not look like a bug in PyTorch, but rather a mis-use of  Python data model.\\r\\nFor example, following code raises the same runtime error:\\r\\n```\\r\\nclass Bar:\\r\\n    def __init__(self, bar):\\r\\n        self.bar = bar\\r\\n        return bar\\r\\n\\r\\nif __name__ == \\\"__main__\\\":\\r\\n    x = Bar(5)\\r\\n```\\r\\n```\\r\\n% python3 bar.py\\r\\nTraceback (most recent call last):\\r\\n  File \\\"bar.py\\\", line 7, in <module>\\r\\n    x = Bar(5)\\r\\nTypeError: __init__() should return None, not 'int'\\r\\n```\\r\\nPlease do not hesitate to comment/update an example, if you believe that PyTorch behavior is incorrect in this case.",
      "y": "This is an expected behavior."
   },
   {
      "x": "torch.linalg.cholesky fails for some PSD matrices",
      "z": "Thanks for your response. Makes total sense - the context of the problem is trying to add a small amount of (tikhonov) regularization to make cholesky stable so aware of the stability problem just my fix was mixing precisions as you say. With double precision numpy==pytorch. Feel free to close.\n\n",
      "y": "Using double precision will not give any errors."
   },
   {
      "x": "Scribe stats reporting is broken in GHA due to secrets access from fork PRs",
      "z": "Verified that scribe proxy is working now",
      "y": "This issue has been fixed."
   },
   {
      "x": "HTTP Error 403 for torch.hub ResNet",
      "z": "As a workaround, can you please adding the following line before making any \\\"torch.hub\\\" calls:\\r\\n```\\r\\ntorch.hub._validate_not_a_forked_repo=lambda a,b,c: True\\r\\n```",
      "y": "Please add the following line before making any \\\"torch.hub\\\" calls:\\r\\n```\\r\\ntorch.hub._validate_not_a_forked_repo=lambda a,b,c: True\\r\\n```"
   },
   {
      "x": "torch.permute missing in docs",
      "z": "This has been fixed in master.\\r\\n\\r\\nThat being said, the docs are not well formatted, so I'll fix that.",
      "y": "This issue has been fixed."
   },
   {
      "x": "Conv2d triggers assertion in mkl-dnn when padding=(n, 3)",
      "z": "@sakaia , I verify that MKLDNN v0.21.1 can solve this problem, we have upgrade mkldnn to 0.21.1, you can test it your side by pip install --pre torch torchvision -f https://download.pytorch.org/whl/nightly/cpu/torch_nightly.html or build the pytorch source code according to README.md. Thanks!",
      "y": "Installing MKLDNN v0.21.1 can fix this issue."
   },
   {
      "x": "PyTorch is not using the GPU specified by CUDA_VISIBLE_DEVICES",
      "z": "More comment:\\r\\n\\r\\nUsing the following command instead\\r\\n`CUDA_DEVICE_ORDER=PCI_BUS_ID CUDA_VISIBLE_DEVICES=3 python test.py`\\r\\nwould fix the problem.\\r\\n\\r\\nBut I am not sure if this is the expected behavior...",
      "y": "Use the following command \\r\\n`CUDA_DEVICE_ORDER=PCI_BUS_ID CUDA_VISIBLE_DEVICES=3 python test.py`"
   },
   {
      "x": "Allow parallel sending to device in DataLoader",
      "z": "No the push occurs inside the transform. It would be possible to design a custom collate_fn but that would be inefficient. The whole point of doing this is because pushing to GPU is so time consuming. Therefore, the best way is to send to GPU in a data transform.",
      "y": "The best way is to send to GPU in a data transform."
   },
   {
      "x": "distributed all_reduce deadlocks in v1.1",
      "z": "Pytorch 1.1 uses nccl 2.4.2 which has a known issue of hanging with long running jobs that was fixed for 2.4.6. https://github.com/NVIDIA/nccl/commit/f40ce73e8987d2990e4b9ef6c75f4b3423acce78\\r\\nWorkaround is to export NCCL_LL_THRESHOLD=0. \\r\\ncc @pietern, @mrshenli to bump nccl submodule.",
      "y": "Installing nccl 2.4.6 can fix this issue."
   },
   {
      "x": "Build error with MSVC (aten\\\\src\\\\ATen\\\\native\\\\quantized\\\\Copy.cpp)",
      "z": "move the sentence \\\"float* src_data = src.data<float>();\\\" into the function of \\\"AT_DISPATCH_QINT_TYPES ...\\\" solves my problem. It looks like:\\r\\n\\r\\n AT_DISPATCH_QINT_TYPES(self.scalar_type(), \\\"Copy\\\", [&]() {\\r\\n float* src_data = src.data<float>();\\r\\n\\tscalar_t* self_data = self.data<scalar_t>();\\r\\n for (int i = 0; i < self.numel(); ++i) {\\r\\n self_data[i] = quantize_val<scalar_t>(\\r\\n self.q_scale().to<float>(),\\r\\n self.q_zero_point().to<int32_t>(),\\r\\n src_data[i]);\\r\\n }\\r\\n });\\r\\n\\r\\n Hope that will help you.",
      "y": "Moving the sentence \\\"float* src_data = src.data<float>();\\\" into the function of \\\"AT_DISPATCH_QINT_TYPES ...\\\" can solve this problem."
   },
   {
      "x": "RuntimeError: ONNX export failed: Couldn't export operator aten::softmax",
      "z": "I just checked the nightly version! There it works fine! Thanks! :)",
      "y": "This works fine for the nightly version."
   },
   {
      "x": "MultiheadAttention is not scriptable",
      "z": "Thanks for bringing up the issue and we have fixed it. We also added two JIT unit tests to cover the applications of torch.nn.MultiheadAttention module (i.e. \"test_torchscript_multi_head_attn\" and \"test_scriptmodule_multi_head_attn_cuda\" in test_jit.py). Those two could possibly be good examples for JIT scriptable.",
      "y": "This issue has been fixed."
   },
   {
      "x": "Building libtorch-dependent project with CMake",
      "z": "I don't know the answer to this, but I suggest using new cmake 'targets' feature, more like\\r\\n\\r\\n```\\r\\nfind_package(Torch REQUIRED)\\r\\n\\r\\nadd_library(a)\\r\\ntarget_link_libraries(a PRIVATE Torch)\\r\\n```\\r\\ninstead of messing with TORCH_INCLUDE_DIRS and TORCH_LIBRARIES themselves.",
      "y": "You can use new cmake 'targets' feature, e.g.\\r\\n\\r\\n```\\r\\nfind_package(Torch REQUIRED)\\r\\n\\r\\nadd_library(a)\\r\\ntarget_link_libraries(a PRIVATE Torch)\\r\\n```"
   },
   {
      "x": "Parameter not registering if .to(device) is used",
      "z": "this is totally expected.\\r\\n\\r\\nParameters can only be leaf Tensors, not Tensors that are a result of an operation on another Tensor.\\r\\n\\r\\nWhat you want is:\\r\\n\\r\\n```\\r\\nself.par = torch.nn.Parameter(torch.rand(5, device='cuda'))\\r\\n```",
      "y": "You need to use:\\r\\n\\r\\n```\\r\\nself.par = torch.nn.Parameter(torch.rand(5, device='cuda'))\\r\\n```"
   },
   {
      "x": "Windows CPU debug build fails at linking stage",
      "z": "Hi,\\r\\nYes. With #17494 I can build without set BUILD_TEST=OFF\\r\\n\\r\\nThanks!",
      "y": "Build will succeed without setting BUILD_TEST=OFF\\r\\n\\"
   },
   {
      "x": "Conda did not install cudnn for pytorch",
      "z": "Hi,\\r\\neven if it's not listed explicitly with `conda list`, your installation should have cudnn. You can check if this is the case executing `torch.backends.cudnn.is_available()`.",
      "y": "Even if it's not listed explicitly with `conda list`, your installation should have cudnn. You can check if this is the case executing `torch.backends.cudnn.is_available()`."
   },
   {
      "x": "Tensor unfold backward is slow",
      "z": "This was fixed by gh-36612, closing.",
      "y": "This issue has been fixed."
   },
   {
      "x": "cuDNN error: CUDNN_STATUS_EXECUTION_FAILED when calling .cuda() on RNN layer",
      "z": "CUDA 9 and RTX 2080 Ti simply aren't compatible and dont play well togethere.\\r\\nAn older CuDNN version working is likely a side-effect rather than expectation.\\r\\nUse CUDA10 and CUDA10 versions of CuDNN etc. for RTX 2080 which is Turing architecture",
      "y": "Use the latest versions of CUDA10 and CuDNN"
   },
   {
      "x": "PyTorch not releasing autograd buffers associated to tensors created with `.from_numpy()`",
      "z": "Sorry I think I misunderstood your point above then, I though you were still seeing a problem even when .item() is used.\nIn that case it is (unfortunately) the expected behavior here.\n\nIt is easy to free any buffers encountered during the backward pass and raise a proper error if the user try to use them again.\nBut it is much harder to delete the graph structure itself while we're traversing it to compute the backward. Also the memory usage due to a single graph is small compared to the other objects in general and so should not be a problem (unless of course the graph keeps growing as in your case).\n\nAnother approach to solve this problem would be to use the `with torch.no_grad():` block (or the functions decorator equivalent) around any operations for which you won't need to compute backward. This way, it won't even build the graph and so will be faster and less memory hungry.",
      "y": "Enclose operations that do not require the backward computations within the  `with torch.no_grad():` block"
   },
   {
      "x": "Is it possible to integrate jax into pytorch ?",
      "z": "1.1, we already have with torch.jit\\r\\n1.2 seems interesting\\r\\n2. probably not relevant\\r\\n\\r\\n1.2 is tracked in https://github.com/pytorch/pytorch/issues/1642\\r\\n\\r\\nOverall, there isn't really the concept of \\\"jax in pytorch\\\" or \\\"pytorch in jax\\\" in the same sense that they're both frontends",
      "y": "Jax and PyTorch are both frontends and cannot be integrated."
   },
   {
      "x": "ONNX exporter for slice operation isses onnx:Slice for dimensions that are not sliced, includiung batch dimension - which breaks TRT5",
      "z": "Looks like it's not an issue with latest TRT6 and parser, so we can close it.",
      "y": "This issue has been fixed."
   },
   {
      "x": "Sharing/Transferring gradients from models across multiple GPU(s) in multiprocessing",
      "z": "@subho406 seems to work fine for me (with 1.1.0 and the latest `examples` repo), just fyi",
      "y": "This issue has been fixed."
   },
   {
      "x": "Why torch wheel is so huge (582MB)?",
      "z": "The number of GPU architectures targeted by the binary is a large contributor to binary size. If you'd like a smaller GPU binary, you can change TORCH_CUDA_ARCH_LIST accordingly. For example, if you want to only support compute 5.0 w/forward compatibility, you can set TORCH_CUDA_ARCH_LIST=5.0+PTX as opposed to the more comprehensive list built by default by the PyTorch devs.\\r\\n\\r\\n(I'm not a PyTorch developer, just someone who has built smaller versions of the library.)",
      "y": "You can specify the GPU architectures during installation to reduce the size."
   },
   {
      "x": "We should not mark non-floating point Tensors as requirering gradients, ever",
      "z": "Gradients are only defined for continuous functions. So if you have an `integer` type, gradients don't really make sense.",
      "y": "Gradients are only defined for continuous functions. So if you have an `integer` type, gradients don't really make sense."
   },
   {
      "x": "Pickling of _VariableFunctions no longer works in 1.5",
      "z": "If any newcomers come here and wonder how to fix this. Upgrading to 1.5.1 fixed the issue. Downgrading to 1.4.1 may too.\\r\\n\\r\\nThat is\\r\\n`conda install pytorch==1.5.1 torchvision==0.6.1 cudatoolkit=10.1 -c pytorch` if you use vision\\r\\n`conda install pytorch==1.5.1 cudatoolkit=10.1 -c pytorch` if you use NLP.\\r\\n\\r\\ndepending on your cuda version.",
      "y": "Installing the latest version of PyTorch can fix the issue."
   },
   {
      "x": "Defaulting to ninja build doesn't forward includes in PyTorch C++/CUDA extensions",
      "z": "include_dirs now supports both absolute and relative paths.",
      "y": "include_dirs now supports both absolute and relative paths."
   },
   {
      "x": "Empty GPU memory cache after Jupyter notebook interrupted",
      "z": "jupyter notebook holds reference to the exception when interrupted (for things like %debug), which holds reference to the stack frames, which hold reference to the variables. so empty_cache won't work. this is not really a pytorch issue per se.",
      "y": "This issue is not related to PyTorch."
   },
   {
      "x": "torch.utils.checkpoint.checkpoint + torch.cuda.amp fails",
      "z": "Thanks for raising this issue!\\r\\nMy best guess is that `CheckpointFunction.backward` uses the stored mixed-precision tensors from its `forward`, but breaks the autocasting contract for the backward.\\r\\nIf you run `scaler.scale(loss).backward()` inside the `autocast` block, it should work for now as a workaround.\\r\\n\\r\\nCC @mcarilli",
      "y": "Running `scaler.scale(loss).backward()` inside the `autocast` block, should work"
   },
   {
      "x": "Windows 10 Libtorch installation issue.",
      "z": "Okay. I resolved it myself. \ud83d\udc4d \\r\\nThe tutorial needs to add this line in CMakeLists.txt file\\r\\n``` set(CMAKE_PREFIX_PATH \\\"libtorch/share/cmake/Torch\\\") ```\\r\\n\\r\\nwhich should point to where Torch is unzipped appropriately.\\r\\nPlease update the docs. This issue is quite prelevant\\r\\n",
      "y": "You need to add this line in CMakeLists.txt file\\r\\n``` set(CMAKE_PREFIX_PATH \\\"libtorch/share/cmake/Torch\\\") ```"
   },
   {
      "x": "torch.remainder gives a remainder larger than the divisor",
      "z": "Thanks for the report!\\r\\n\\r\\nIt turns out that this problem is due to numerical precision issue when you subtract two larger numbers with only small difference, such as 100000000002 - 100000000001. Even the numpy single precision result of 1.024195 is not accurate (from c function `fmodf`). The accurate double precision result is 0.577396.\\r\\n\\r\\nThe current `torch.remainder` impl for both cpu and gpu have this problem, e.g. https://github.com/pytorch/pytorch/blob/253943d5a7bb90a420e4d94366101915823c7929/aten/src/ATen/native/cuda/BinaryArithmeticKernel.cu#L80-L85\\r\\n\\r\\nI also checked the `torch.fmod` impl which are correct for both cpu and gpu. You can temporarily use `torch.fmod` as a substitute for `torch.remainder`. I will provide a fix to `torch.remainder` soon, and let it use the native c/cuda function of `fmod`.",
      "y": "This is due to numerical precision issues. Please use `torch.fmod` instead of `torch.remainder`"
   },
   {
      "x": "torch.cdist [Cuda out of memory Tried to allocate 108GB of memory]",
      "z": "I'm closing it as after building my env from scratch it now works. (I was already on torch 1.5, so I can't directly point on the problem but ...)",
      "y": "Building environment from scracth will fix the issue."
   },
   {
      "x": "C++ API for Transformer model in libtorch 1.5.0",
      "z": "This is available since 1.7",
      "y": "This is available in the recent versions of PyTorch."
   },
   {
      "x": "RuntimeError: Error in dlopen or dlsym: libcaffe2_nvrtc.so: cannot open shared object file: No such file or directory ( torch_geometric/utils/loop.py)",
      "z": "@AugF `LD_LIBRARY_PATH` should be `/home/xxx/anaconda3/envs/pyg/lib/python3.8/site-packages/torch/lib:${LD_LIBRARY_PATH}`.\\r\\nThere should be a `/lib` after `site-packages/torch/`",
      "y": "`LD_LIBRARY_PATH` should be `/home/xxx/anaconda3/envs/pyg/lib/python3.8/site-packages/torch/lib:${LD_LIBRARY_PATH}`."
   },
   {
      "x": "Better testing on CPUs without AVX capabilities",
      "z": "In that case we can run cpp tests under qemu, which can be configured not to support any vecorized instructions.",
      "y": "You can use a machine emulator such as QEMU."
   },
   {
      "x": "[BatchNorm] Unexpected behaviour with track_running_stats",
      "z": "Closed by #38084",
      "y": "This issue has been fixed."
   },
   {
      "x": "`torch.log10` with float32 produces different results on different CPU",
      "z": "@mruberry \\r\\nYes, that makes sense. I wanted to leave a record of this finding and double check if there is something that we can work on.\\r\\n\\r\\nWhile filling out this issue I also found that `float64` produces consistent result, so we are going to use `float64` in torchaudio to resolve the issue we are having.\\r\\n\\r\\nThanks!",
      "y": "Using `float64` can resolve this issue."
   },
   {
      "x": "Falling to turn shape into Tensor",
      "z": "Do\\r\\n```python\\r\\na = torch.ones(10, 10)\\r\\nb = torch.tensor(a.shape)\\r\\nprint(b)\\r\\n# tensor([10, 10])\\r\\n```\\r\\nAlso, questions like this are better suited for https://discuss.pytorch.org/",
      "y": "This can be done via \\r\\n```python\\r\\na = torch.ones(10, 10)\\r\\nb = torch.tensor(a.shape)\\r\\nprint(b)\\r\\n# tensor([10, 10])\\r\\n```"
   },
   {
      "x": "[pytorch] [feature request] Entropy function",
      "z": "you can calculate this via `distributions.Categorical(probs=p).entropy()`?",
      "y": "This can ve calculated via `distributions.Categorical(probs=p).entropy()`"
   },
   {
      "x": "mse_loss reduction='none' is ignored when required_grads is True",
      "z": "Many thanks @li-roy, with 1.0.0.dev20181018 it works smoothly.",
      "y": "This issue has been fixed."
   },
   {
      "x": "3x regression in JIT LSTM speeds",
      "z": "Yes the slowdown was due to memory leak",
      "y": "This issue arises because of memory leaks"
   },
   {
      "x": "windows pytorch 0.4.1 error=48 : no kernel image is available for execution on the device",
      "z": "The new binaries are updated for Windows.\\r\\n\\r\\nIf you have previously installed via anaconda, you have to do:\\r\\n\\r\\n```\\r\\nconda uninstall -y pytorch\\r\\nconda clean -t -y\\r\\n```\\r\\n\\r\\nand then reinstall pytorch.\\r\\n\\r\\n\\r\\nIf you have installed via `pip` command on https://pytorch.org, first uninstall pytorch via `pip uninstall torch` and then rerun that command (so that the new wheel is downloaded and installed)",
      "y": "You need to uninstall and reinstall PyTorch."
   },
   {
      "x": "Quadro m2000m not able to get pytorch working with gpu",
      "z": "The new binaries are updated for Windows.\\r\\n\\r\\nIf you have previously installed via anaconda, you have to do:\\r\\n\\r\\n```\\r\\nconda uninstall -y pytorch\\r\\nconda clean -t -y\\r\\n```\\r\\n\\r\\nand then reinstall pytorch.\\r\\n\\r\\n\\r\\nIf you have installed via `pip` command on https://pytorch.org, first uninstall pytorch via `pip uninstall torch` and then rerun that command (so that the new wheel is downloaded and installed)",
      "y": "You need to uninstall and reinstall PyTorch."
   },
   {
      "x": "Wrong Warning \\\"compiler (c++) may be ABI-incompatible with PyTorch!\\\"",
      "z": "@johnmarkwayve no, the warning is only raised if you're using a binary build of Pytorch, i.e. from pip. It's never raised if you compile from source.",
      "y": "The error is not encountered if compiled from source."
   },
   {
      "x": "[Performance Issue] Inference time increases on CPU the more you train the model on a TitanX.",
      "z": "Yes @vadimkantorov , that was the solution!!\nI enabled \"set_flush_denorm\" in case of CPU and now different checkpoints have the same inference time. Many thanks!",
      "y": "Inference time is the same if `set_flush_denorm` is used with CPU. "
   },
   {
      "x": "CreateNet(train_net) cannot find blob created by the RunNetOnce(init_net) in Caffe2 (C++)",
      "z": "I just found out that it works, if I add \\\"filter\\\" as external input for the \\\"train_net\\\" like so:\\r\\n\\r\\n train_net.add_external_input (\\\"filter\\\");\\r\\n\\r\\nbefore executing CreateNet(train_net).",
      "y": "Adding `filter` as external input for the model works."
   },
   {
      "x": "MSELoss wrongly sums instead of averages when reduction='elementwise_mean'",
      "z": "Yes but it\u2019s fixed on master and will be included in the next release.\\n\\nOn Wed, Sep 19, 2018 at 03:52 simama <notifications@github.com> wrote:\\n\\n> I am on 0.4.1 version and it seems this bug is still not fixed.\\n>\\n> \u2014\\n> You are receiving this because you were mentioned.\\n> Reply to this email directly, view it on GitHub\\n> <https://github.com/pytorch/pytorch/issues/10148#issuecomment-422696370>,\\n> or mute the thread\\n> <https://github.com/notifications/unsubscribe-auth/AFaWZfuFXXaUvgqzfKfZO7_kPUMt0gsQks5ucfe-gaJpZM4VrZIT>\\n> .\\n>\\n",
      "y": "This issue has been fixed."
   },
   {
      "x": "Backward engine computes unnecessary dependencies",
      "z": "Fixed in #752.",
      "y": "This issue has been fixed."
   },
   {
      "x": "F.relu(inplace) followed by F.dropout(inplace) breaks backward pass",
      "z": "Yeah, that is expected. You can't apply in-place operations to leaf Variables. Just remove the `inplace` flag and it should be good.",
      "y": "Removing the `inplace` flag should make it work."
   },
   {
      "x": "torch.range is upper-bound inclusive, while python range and numpy arange are upper-bound exclusive",
      "z": "I think we should make a `torch.arange` that is equivalent to `numpy.arange` and depreceate `torch.range` properly. It is used quite extensively all over the place.\\r\\nwdyt @colesbury @apaszke ?",
      "y": "`torch.range` needs to be deprecated"
   },
   {
      "x": "LSTM forget gate bias initialization",
      "z": "Yes, the ordering of weights a biases is the same for all implementations and is `ingate, forgetgate, cellgate, outgate`. You need to initialize the values between 1/4 and 1/2 of the bias vector to the desired value.",
      "y": "The ordering of weights a biases is the same for all implementations and is `ingate, forgetgate, cellgate, outgate`. You need to initialize the values between 1/4 and 1/2 of the bias vector to the desired value."
   },
   {
      "x": "Error while saving my network",
      "z": "this is a syntax error. `torch.save` does not have the order of arguments as you've used.",
      "y": "Using the correct arguments will not give this error."
   },
   {
      "x": "Build fails on Ubuntu 14.04 + conda latest",
      "z": "CUDA Version 8.0.27 has this issue. it is a pre-release version. The stable CUDA8 version is 8.0.44",
      "y": "Installing the latest stable release fixes the issue."
   },
   {
      "x": "view() after transpose() raises non contiguous error",
      "z": "Yes, this is expected, as `view` is only supposed to work on contiguous tensors, and transposing a tensor makes it non-contiguous. You can use `.contiguous()` after `transpose` to fix your issue",
      "y": "This is expected, as `view` is only supposed to work on contiguous tensors, and transposing a tensor makes it non-contiguous. You can use `.contiguous()` after `transpose` to fix your issue"
   },
   {
      "x": "Errors import torch installed form source on macOS",
      "z": "Run python from a different directory than the repository root.",
      "y": "You need to open torch in any directory other than repo's root"
   },
   {
      "x": "Add support for variable length sequences in cuDNN RNNs",
      "z": "@glample CUDNN already supports variable length sequences through a packed-array API; PyTorch just doesn't currently use that functionality",
      "y": "CUDNN already supports variable length sequences through a packed-array API; PyTorch just doesn't currently use that functionality"
   },
   {
      "x": "Maxout Layer",
      "z": "For ones who need Maxout, I changed the above code to make it work. \\r\\n\\r\\n\\r\\n```python\\r\\nclass Maxout(nn.Module):\\r\\n\\r\\n def __init__(self, d_in, d_out, pool_size):\\r\\n super().__init__()\\r\\n self.d_in, self.d_out, self.pool_size = d_in, d_out, pool_size\\r\\n self.lin = nn.Linear(d_in, d_out * pool_size)\\r\\n\\r\\n\\r\\n def forward(self, inputs):\\r\\n shape = list(inputs.size())\\r\\n shape[-1] = self.d_out\\r\\n shape.append(self.pool_size)\\r\\n max_dim = len(shape) - 1\\r\\n out = self.lin(inputs)\\r\\n m, i = out.view(*shape).max(max_dim)\\r\\n return m\\r\\n```\\r\\n",
      "y": "The Maxout layer can be implemented as follows \\r\\n\\r\\n\\r\\n```python\\r\\nclass Maxout(nn.Module):\\r\\n\\r\\n def __init__(self, d_in, d_out, pool_size):\\r\\n super().__init__()\\r\\n self.d_in, self.d_out, self.pool_size = d_in, d_out, pool_size\\r\\n self.lin = nn.Linear(d_in, d_out * pool_size)\\r\\n\\r\\n\\r\\n def forward(self, inputs):\\r\\n shape = list(inputs.size())\\r\\n shape[-1] = self.d_out\\r\\n shape.append(self.pool_size)\\r\\n max_dim = len(shape) - 1\\r\\n out = self.lin(inputs)\\r\\n m, i = out.view(*shape).max(max_dim)\\r\\n return m\\r\\n```\\r\\n"
   },
   {
      "x": "Unefined reference to C10::Error::Error when linking against libTorch",
      "z": "I think you're missing `\\\"-D_GLIBCXX_USE_CXX11_ABI=0\\\"` in your build flags. We provide this in our TorchConfig.cmake, which is why we recommend cmake as the easiest way to build LibTorch. That said, you can add that to your QtCreator config. See https://github.com/pytorch/pytorch/blob/master/cmake/TorchConfig.cmake.in for the relevant file\\r\\n\\r\\nPlease let me know if that fixes the issue.",
      "y": "You need to use the build flags `QMAKE_CXXFLAGS += -D_GLIBCXX_USE_CXX11_ABI=0`"
   },
   {
      "x": "can't convert CUDA tensor to numpy. Use Tensor.cpu() to copy the tensor to host memory first.",
      "z": "`.numpy()` shares memory with the input CPU tensor, so `cuda_t.cpu().numpy()` is different with `cpu_t.numpy()` and we want to explicitly ask users to convert to CPU tensor.",
      "y": "`.numpy()` shares memory with the input CPU tensor, so `cuda_t.cpu().numpy()` is different with `cpu_t.numpy()` and we want to explicitly ask users to convert to CPU tensor."
   },
   {
      "x": "GPU Memery leak",
      "z": "i find a solution yesterday\\r\\n\\r\\nbatch_y_predlabel has to be **detach** so the GPU memory can be free.\\r\\n\\r\\nI don't know if it's normal that a GPU tensor transfer to the CPU in the compute graph still take GPU memory?\\r\\nfor gradient compute I imagine?",
      "y": "To free up GPU memory, you need to detach the predictions."
   },
   {
      "x": "[Build error] libnccl.so: error adding symbols: File in wrong format",
      "z": "it's very likely that you installed x64 libnccl, instead of ppc64 version",
      "y": "You need to install the correct version for your platform"
   },
   {
      "x": "Can cuda10 use pytorch-0.4.1?",
      "z": "no",
      "y": "Not, it cannot."
   },
   {
      "x": "cosine_similarity function produces results more than 1.0",
      "z": "Probably better to re-order the computations to improve numerical precision. Might want to look at SciPy:\\r\\n\\r\\nhttps://github.com/scipy/scipy/blob/453932337f4a67170e4e7fda3f808b273a787a41/scipy/spatial/distance.py#L717-L721\\r\\n\\r\\nI think the issue is that we're doing:\\r\\n\\r\\n```\\r\\nx / (sqrt(x) * sqrt(x)) # bad\\r\\n```\\r\\n\\r\\nvs.\\r\\n\\r\\n```\\r\\nx / sqrt(x * x) # good\\r\\n```\\r\\n\\r\\nI don't know enough about floating point arithmetic to argue why the second is more accurate, but it seems to be. (You can look at the min/max over a large random tensor)",
      "y": "This error can be fixed by reordering the computations to improve numerical precisions."
   },
   {
      "x": "Is mkl-dnn enabled in the latest binary distribution v1.0.1?",
      "z": "Yes.",
      "y": "Yes."
   },
   {
      "x": "nn.LSTM gives nondeterministic results with dropout and multiple layers",
      "z": "Closed and fixed in cudnn_7.6.1 @ngimel",
      "y": "This issue has been fixed."
   },
   {
      "x": "Add support for mixture models in torch.distributions",
      "z": "`MixtureSameFamily` should be easy to implement. We don't use this in Pyro since we usually keep the mixture component id as an explicit variable and enumerate over that variable:\\r\\n```py\\r\\ncomponent = pyro.sample(\\\"component\\\", dist.Categorical(probs),\\r\\n                        infer={\\\"enumerate\\\": \\\"parallel\\\"})\\r\\nassert component.reshape(-1).shape == probs.shape[-1:]\\r\\nvalue = pyro.sample(\\\"mixture\\\", MyDistribution(my_params[component]))\\r\\n```\\r\\ncc @martinjankowiak",
      "y": "`MixtureSameFamily` can be implemented as:\\r\\n```py\\r\\ncomponent = pyro.sample(\\\"component\\\", dist.Categorical(probs),\\r\\n                        infer={\\\"enumerate\\\": \\\"parallel\\\"})\\r\\nassert component.reshape(-1).shape == probs.shape[-1:]\\r\\nvalue = pyro.sample(\\\"mixture\\\", MyDistribution(my_params[component]))\\r\\n```"
   },
   {
      "x": "IndexError while trying to save torchscript",
      "z": "Closing this since I can't reproduce and we fixed some similar errors recently, but feel free to re-open if you're still getting this error",
      "y": "This issue has been fixed."
   },
   {
      "x": "[JIT] b->inputs().size() == b->outputs().size() ASSERT FAILED",
      "z": "Closing as this has been fixed (the repro in the initial post now works as intended)",
      "y": "This issue has been fixed."
   },
   {
      "x": "[CPU] several inplace functions fail since 1.0.1 on certain hw",
      "z": "I can reproduce this with OMP_NUM_THREADS=2. I haven't seen it with OMP_NUM_THREADS=1.",
      "y": "Setting `OMP_NUM_THREADS=1` will not raise an error."
   },
   {
      "x": "module' object has no attribute '_dl'",
      "z": "it's not a problem of spacy, it looks like an incomplete or corrupt pytorch install.\\r\\n\\r\\nTry:\\r\\n\\r\\n```\\r\\npip uninstall torch\\r\\npip uninstall torch\\r\\npip uninstall torch\\r\\npip install torch\\r\\n```",
      "y": "You need to uninstall and reinstall PyTorch."
   },
   {
      "x": "IndyLSTM & IndyGRU in PyTorch",
      "z": "it's not widely used enough yet, to be pushed into core (feel free to reopen once it becomes more of a standard).\\r\\n\\r\\nAdditionally, we are working on a user handwritten RNNs being fast, rather than adding more fundamental multi-layer RNNs into core.\\r\\nSo, I'm closing the feature request.",
      "y": "This feature will not be implemented."
   },
   {
      "x": "Multi-GPU RuntimeError: Expected tensor for argument #1 'input' to have the same device as tensor for argument #2 'weight'",
      "z": "your input is on gpu 1 but your net work is on gpu 0.",
      "y": "Model and data need to be on the same GPU."
   },
   {
      "x": "CUDA unavailable when pytorch 1.3.0. installed with cudatoolkit 10.1",
      "z": "I got the same issue with Pytorch 1.3.1 + CUDA 10.0. Finally, I got it resolved with:\\r\\n```\\r\\npip install torch==1.3.1+cu100 torchvision==0.4.2+cu100 -f https://download.pytorch.org/whl/torch_stable.html\\r\\n```",
      "y": "You need to downgrade your version of CUDA."
   },
   {
      "x": "error executing torch_shm_manager in cifar10_tutorial.py",
      "z": "Experienced the same issue on OSX. Setting `num_workers=0` on the DataLoaders solved it, even though the tutorial only recommends it for Windows.\\r\\nIt should probably be better clarified/fixed though.",
      "y": "Setting `num_workers=0` will not raise an error."
   },
   {
      "x": "\\\"module has no attribute 'downsample'\\\" when scripting torchvision's resnet",
      "z": "I believe you need to use a nightly version of torchvision (and thus a nightly version of PyTorch) for resnet to be scriptable.",
      "y": "You need to install the latest version of PyTorch."
   },
   {
      "x": "Failed to load model on mobile for device type \\\"c10::DeviceType::CUDA\\\"",
      "z": "@ljk53 sorry late response. \\r\\nI confirmed the model converted to cpu() is able to load on the phone.",
      "y": "You need to move the model to `.cpu()`"
   },
   {
      "x": "Didn't find kernel to dispatch to for operator 'quantized::conv2d'",
      "z": "Hi @thiyagu145, the error message you've encountered says that you're trying to run the quantized Conv2d operator, but you're passing in an unquantized tensor.\n\nPlease take a look for where we use QuantStub in the tutorial and workflow documentation. Once this is present and the values are passed through them, the issue should be solved",
      "y": "Passing a quantized vector to the operator will not raise an error."
   },
   {
      "x": "Why t.arange(0,3) create an int type Tensor?",
      "z": "torch.arange returns an int64 tensor by defaut. It mimics numpy behavior. if you want different dtype, give dtype as argument, such as `t.arange(0, 3, dtype=torch.float32)`",
      "y": "You need to specify the datatype as an argument."
   },
   {
      "x": "Loading opencv image to pytorch tensor",
      "z": "I found it at https://pytorch.org/cppdocs/api/classat_1_1_tensor.html#exhale-class-classat-1-1-tensor.\\r\\nHere is a demo:\\r\\n```C++\\r\\n tensor_image = tensor_image.permute({0, 3, 1, 2});\\r\\n```",
      "y": "The order of dimensions is different in OpenCV and PyTorch."
   },
   {
      "x": "Bug in transferring model from pytorch --> caffe2",
      "z": "I modified the tutorial and removed the `pixel_shuffle` part and composed the network only with ReLU and conv layers (just to see if `pytorch` and `caffe2` give the same output), and **now it works fine**.\\r\\n\\r\\nSince `pixel_shuffle` is only on the master branch, I assume that it's still buggy?\\r\\nAnyway, I'll just test if all the layers I personally need output the same thing between the two frameworks and proceed.",
      "y": "Remove the `pixel_shuffle` part and composed the network only with ReLU and conv layers will not give an error."
   },
   {
      "x": "Using net.cuda crashes the kernel",
      "z": "If you look into the list of types in the `got (...)` part, you'll find a mix of CPU and CUDA tensors, with `input` and `output` being on CPU, while `weight` and `bias` is on the GPU.\\r\\n\\r\\nYou probably forgot to send the input to the GPU. Alternatively, keep in mind that `.cuda()` is an out of place operation i.e.\\r\\n```python\\r\\ninput.cuda()\\r\\nmodel(Variable(input))\\r\\n```\\r\\nwill fail. You need to overwrite the reference with a new CUDA tensor:\\r\\n```python\\r\\ninput = input.cuda()\\r\\nmodel(Variable(input))\\r\\n```",
      "y": "You need to overwrite the reference with a new CUDA tensor:\\r\\n```python\\r\\ninput = input.cuda()\\r\\nmodel(Variable(input))\\r\\n```"
   },
   {
      "x": "Gumbel noise",
      "z": "Here you go. Much more readable and no modules required:\\r\\n```python\\r\\nimport torch.nn.functional as F\\r\\nfrom torch.autograd import Variable\\r\\n\\r\\ndef sampler(input, tau, temperature):\\r\\n noise = torch.rand(input.size())\\r\\n noise.add_(1e-9).log_().neg_()\\r\\n noise.add_(1e-9).log_().neg_()\\r\\n noise = Variable(noise)\\r\\n x = (input + noise) / tau + temperature\\r\\n x = F.softmax(x.view(input.size(0), -1))\\r\\n return x.view_as(input)\\r\\n```\\r\\nWe're using GitHub for bug reports only, if you have questions please post the on [our forums](https://discuss.pytorch.org).",
      "y": "The code is as follows:\\r\\n```python\\r\\nimport torch.nn.functional as F\\r\\nfrom torch.autograd import Variable\\r\\n\\r\\ndef sampler(input, tau, temperature):\\r\\n noise = torch.rand(input.size())\\r\\n noise.add_(1e-9).log_().neg_()\\r\\n noise.add_(1e-9).log_().neg_()\\r\\n noise = Variable(noise)\\r\\n x = (input + noise) / tau + temperature\\r\\n x = F.softmax(x.view(input.size(0), -1))\\r\\n return x.view_as(input)\\r\\n```"
   },
   {
      "x": "GOMP_4.0 not found",
      "z": "I fixed the problem by importing opencv before torch.",
      "y": "Importing OpenCV before PyTorch fixes the issue."
   },
   {
      "x": "Cannot install `torchvision 0.1.7` by using conda",
      "z": "i see. in this case, just remove torchvision (conda uninstall torchvision) and then install it via pip.\\r\\n\\r\\npip install torchvision",
      "y": "Installing torchvision through pip fixes this issue."
   },
   {
      "x": "nn.Module not importing parameters contained in lists",
      "z": "This behaviour is expected.\\r\\nThere is a detailed discussion in https://discuss.pytorch.org/t/list-of-nn-module-in-a-nn-module/219",
      "y": "This behaviour is expected."
   },
   {
      "x": "libTH doesn't recognize Intel MKL in its default location",
      "z": "I encounter the same error, and I solve it by running command 'conda install mkl' in my activated conda env.\\r\\n\\r\\n> File \\\"/usr/local/lib/python3.5/site-packages/torch/__init__.py\\\", line 45, in <module>\\r\\n> from torch._C import *\\r\\n> ImportError: dlopen(/usr/local/lib/python3.5/site-packages/torch/_C.cpython-35m-darwin.so, 10): Library not loaded: @rpath/libmkl_intel_lp64.dylib\\r\\n> Referenced from: /usr/local/lib/python3.5/site-packages/torch/lib/libTH.1.dylib\\r\\n> Reason: image not found",
      "y": "Running the command `conda install mkl` fixes this issue."
   },
   {
      "x": "[build/nccl] failed to build libnccl on Debian unstable",
      "z": "@apaszke Thanks, the fix is to to export the two environment variables:\\r\\n```\\r\\nexport CUDA_HOME=/usr\\r\\nexport CUDA_LIB=/usr/lib/$(shell dpkg-architecture -qDEB_HOST_MULTIARCH)\\r\\n```",
      "y": "You need to exprt the two environment variables:\\r\\n```\\r\\nexport CUDA_HOME=/usr\\r\\nexport CUDA_LIB=/usr/lib/$(shell dpkg-architecture -qDEB_HOST_MULTIARCH)\\r\\n```"
   },
   {
      "x": "Allow optimizers to skip nn.Parameters that have requires_grad=False",
      "z": "So I don't really think that it makes sense to allow such parameters. If you don't want to optimize some tensors, they're not parameters - they're fixed. You probably don't want to count them in. And if you really need to then\\r\\n```pytorch\\r\\noptimizer.SGD(filter(lambda p: p.requires_grad, model.parameters()), lr=1e-3)\\r\\n```\\r\\nshould do the trick.",
      "y": "This can be done via ```pytorch\\r\\noptimizer.SGD(filter(lambda p: p.requires_grad, model.parameters()), lr=1e-3)\\r\\n```"
   },
   {
      "x": "cuda runtime error (8) : invalid device function - adding cuda tensors",
      "z": "@adelsalehali1982 Your error is unrelated to this issue, and happens because the code you are running supposes that 4 GPUs are used (if gpus are enabled), and you probabmy have less than 4 GPUs in your machine. You can fix that by changing the line with DataParallel to only use 1 or 2 GPUs",
      "y": "You need to use a more recent GPU for this to work."
   },
   {
      "x": "\\\"Symbol not found\\\" when \\\"import torch\\\" on Mac OS",
      "z": "As a workaround, \\r\\n\\r\\n\\r\\n`pip3 install torchtext==0.4`\\r\\n\\r\\nsolved the issue.",
      "y": "Install torchtext from pip instead of source to fix this issue."
   },
   {
      "x": "How to use cudnn in pytorch\uff1f",
      "z": "To verify that pytorch uses cudnn:\\r\\n```\\r\\n>>> torch.backends.cudnn.version()\\r\\n6021\\r\\n```",
      "y": "To verify that pytorch uses cudnn:\\r\\n```\\r\\n>>> torch.backends.cudnn.version()```"
   },
   {
      "x": "[Minor Bug] Pylint E1101 Module 'torch' has no 'from_numpy' member",
      "z": "On VS code: \\r\\nAdding `\\\"python.linting.enabled\\\": false` also worked in this case.",
      "y": "You need to set the following flag `\"python.linting.enabled\\\": false` "
   },
   {
      "x": "ImportError: No module named 'tools.setup_helpers'",
      "z": "@soumith I expect people will just keep doing this until there's a working package on PyPI. That's just the first thing people are going to try for a Python project.\\r\\n\\r\\nMaybe you could upload an `0.2` wheel that just prints a more helpful error like:\\r\\n\\r\\n> Installation from PyPI not supported yet (see status at `https://github.com/pytorch/pytorch/issues/566`). For now, please uninstall this package (`pip uninstall pytorch`) and follow the instructions at `http://pytorch.org/` to install with miniconda.",
      "y": "Install using instructions from PyTorch's website."
   },
   {
      "x": "from caffe2.python import core, net_drawer, net_printer, visualize, workspace, utils",
      "z": "It is pydot.\\r\\n",
      "y": "You need to install pydot."
   },
   {
      "x": "ImportError: No module named future.utils",
      "z": "sudo pip install future\\r\\n or\\r\\neasy_install future",
      "y": "You need to install the package 'future'"
   },
   {
      "x": "Raise correct error type when passing invalid covariance matrix to MultivariateNormal",
      "z": "As mentioned in #12102, the issue is that arg checking [happens](https://github.com/pytorch/pytorch/blob/master/torch/distributions/distribution.py#L30) in `Distribution.__init__()`, but `MultivariateNormal.__init__()` performs some linear algebra before calling `super(...).__init__()`. I think the solution is to move any linear algebra operations below the `super(...).__init__()` call in `MultivariateNormal.__init__()`.",
      "y": "The solution is to move any linear algebra operations below the `super(...).__init__()` call in `MultivariateNormal.__init__()`."
   },
   {
      "x": "Install only caffe2",
      "z": "it is no longer possible to only install CAFFE2.",
      "y": "It is no longer possible to only install CAFFE2."
   },
   {
      "x": "Illegal instruction (core dumped) on Debug CPU build",
      "z": "Could you try `ATEN_CPU_CAPABILITY=default cmd_to_run_your_code`?",
      "y": "Setting `ATEN_CPU_CAPABILITY=default cmd_to_run_your_code` will fix this issue."
   },
   {
      "x": "OMP: Warning #190 because of fork not waiting for parallel region to end",
      "z": "I am also having this issue. It persists across data (MNIST, CIFAR10) and various architectures. I do not get the warning if I set `pin_memory=False`. I think it might be due to having three Dataloaders in my script like @FunkyKoki (i.e. train, test, validation) and iterating over two of them (test, val) while inside the loop of the other (train).",
      "y": "The warning does not appear if the argument `pin_memory=False` is given."
   },
   {
      "x": "torch.jit.trace returns unwrapped C type",
      "z": "@NeilWangziyu You could also consider trying building PyTorch from source, if nightly doesn't work for you. Saving should definitely work now. @driazati added it 2 months ago in this PR: #20386",
      "y": "Installing PyTorch from source fixes the error."
   },
   {
      "x": "I can't import PyTorch, libomp.dylib can't be loaded.",
      "z": "`brew install libomp` solves the problem.",
      "y": "To solve the problem, run `brew install libomp`."
   },
   {
      "x": "Slow distributed training",
      "z": "After using OMP_NUM_THREADS=1, the speed is back to normal. Thanks, @VitalyFedyunin.",
      "y": "Setting the flag `OMP_NUM_THREADS=1` makes it faster."
   },
   {
      "x": "Unable to compile an older version of PyTorch",
      "z": "Try `git submodule sync` and then `git submodule update --init`?",
      "y": "Run the following: `git submodule sync` and then `git submodule update --init`"
   },
   {
      "x": "torch.arange always generate constant result in tracing",
      "z": "Hi @lara-hdr, \\r\\n\\r\\nDo you mind linking to where you set `traceable=true` exactly? I'm not too familiar with that part of the code base.\\r\\n\\r\\n@zou3519 do you know why this has to be implemented in the python arg parser? it's also a schematized aten op. \\r\\n\\r\\n\\r\\n\\r\\n",
      "y": "This issue has been fixed."
   },
   {
      "x": "Unable to import 1.1 when installing with pip",
      "z": "Probably duplicate of #20030\\r\\ntry `brew install libomp `",
      "y": "To solve the problem, run `brew install libomp`."
   },
   {
      "x": "Is the `device=` parameter required in torch.FloatTensor and similar ones",
      "z": "`torch.FloatTensor` is a legacy constructor and doesn't support all types.\\r\\nTo answer your question, i think `torch.cuda.FloatTensor` might be what you are looking for -- but please dont use either. Just use `torch.empty(..., device='cuda')`",
      "y": "Please use  `torch.empty(..., device='cuda')` instead of `torch.cuda.FloatTensor` or `torch.FloatTensor`. "
   },
   {
      "x": "NCCL hang in PyTorch Distributed Data Parallel for Mixed Precision Training",
      "z": "I ran with n_procs=1 and suddenly a magic new error message appeared telling me to try adding `find_unused_parameters=True` to `DistributedDataParallel`. When I did that, it worked! Thank you for good error messages!!!",
      "y": "You need to set `n_proc=1` and `find_unused_parameters=True` to `DistributedDataParallel`."
   },
   {
      "x": "[FR] make IncompatibleKeys print nicer when there is no error",
      "z": "@designnner This is not an error. It's just the `repr` that isn't ideal. Feel free to ignore.",
      "y": "This error can be ignored."
   },
   {
      "x": "GRUcell has a wrong formula",
      "z": "either is fine, because it's all about semantics of z. that is, if z is considered an update gate, z * n makes sense, while (1-z)* n makes sense if we think of z as a leaky coefficient. in the end, they are equivalent.",
      "y": "The formula is correct."
   },
   {
      "x": "TensorBoard logging requires TensorBoard with Python summary writer installed. This should be available in 1.14 or above",
      "z": "I think it's a version problem.\\r\\njust run this:\\r\\n`pip install tensorboard==1.14.0`\\r\\n(not pip install tensorboard==1.14)",
      "y": "You need to install the latest version of Tensorboard."
   },
   {
      "x": "Building from source error: command 'gcc' failed with exit status 1",
      "z": "I have the same problem (`error: expected ')' before 'PRId64'` etc.) with gcc 4.8.5 on Linux with the current head ( 02450fff3)\\r\\n\\r\\nWhat works for me is to add `#define __STDC_FORMAT_MACROS` at the beginning of the following four files:\\r\\n* torch/csrc/Storage.cpp\\r\\n* torch/csrc/Tensor.cpp\\r\\n* torch/csrc/cuda/Storage.cpp\\r\\n* torch/csrc/cuda/Tensor.cpp\\r\\n\\r\\n(see also https://github.com/pytorch/pytorch/compare/master...andreh7:2017-11-10-prid64-fix -- this can be turned into a pull request very easily)\\r\\n\\r\\nThis fix is the same as #3574 but for different files. \\r\\n\\r\\n ",
      "y": "You need to add `#define __STDC_FORMAT_MACROS` at the beginning of the following four files:\\r\\n* torch/csrc/Storage.cpp\\r\\n* torch/csrc/Tensor.cpp\\r\\n* torch/csrc/cuda/Storage.cpp\\r\\n* torch/csrc/cuda/Tensor.cpp"
   },
   {
      "x": "RuntimeError when using DistributedDataParallel",
      "z": "I encountered this error message when using multiple multi-gpu machines. It did not occur when using a single multi-gpu machine - albeit with DataParallel, not DistributedDataParallel - nor when using multiple single-gpu machines (also didn't occur when using multi-gpu machines with ``CUDA_VISIBLE_DEVICES=1`` on both). I didn't try combinations of single- and multi- gpu as the OP here felt that caused his problem. PyTorch version 0.3.0.post4.\\r\\n\\r\\nI solved the problem by deleting a ``nn.Linear`` that I assigned to an attribute during the ``__init__`` of a custom ``nn.Module`` but never used during ``forward``.\\r\\n\\r\\nIMO this case deserves a better error message or should be documented with DistributedDataParallel.\\r\\n\\r\\nThanks!",
      "y": "The problem can be solved by deleting a ``nn.Linear`` that is assigned to an attribute during the ``__init__`` of a custom ``nn.Module``"
   },
   {
      "x": "GPU Memory Leak at Master Branch",
      "z": "Closing since this issue is stale, please reopen with a repro script if you still see the memory leak.",
      "y": "This issue has been fixed."
   },
   {
      "x": "Support view() on batch dimensions for non-contiguous tensors?",
      "z": "There's a good reason for the `view` invariant - most of such reshapes are impossible to pull off using stride tricks if then tensor isn't contiguous. On the other hand, making it contiguous inside `view` would mean that sometimes the returned tensor shares storage with input, and sometimes doesn't. This is important for cases like these: `x.view(-1)[::x.size(1) + 1] += c` (add `c` to diagonal of matrix `x`). If you know/suspect that tensors might sometimes be non-contiguous just add `.contiguous()` before `.view()` it's a no-op if the tensor already is contiguous",
      "y": "This feature will not be implemented."
   },
   {
      "x": "Slight memory leak for LSTM",
      "z": "Thanks @ngimel !\n\nI'll close this issue for now. If you see this issue, please upgrade to cudnn 7.1+, driver 384.69+.\n\n@Evpok @jiesutd @bangbangjim See above.",
      "y": "This issue has been fixed."
   },
   {
      "x": "Error in nll_loss - multi-target not supported",
      "z": "CrossEntropyLoss takes a 1D tensor. If your `target` has size `(32, 1)`, you need to squeeze the last dimension with `target.squeeze(1)` so it becomes a 1D tensor.",
      "y": "You need to squeeze the last dimension with `target.squeeze(1)` so it becomes a 1D tensor."
   },
   {
      "x": "softmax doesn't support negative dimensions",
      "z": "if it didn't support negative dims in 0.2.0 then i wont mark it as a release blocker for 0.3",
      "y": "This feature will not be implemented."
   },
   {
      "x": "torch.load() requires model module in the same folder",
      "z": "PyTorch internally uses pickle and it's a limitation of pickle. You can try meddling with `sys.path` to include the directory where `module.py` is. This is exactly why we recommend saving only the state dicts and not whole model objects.",
      "y": "You need to add the directory with your module to `sys.path`."
   },
   {
      "x": "Pytorch AssertionError: Torch not compiled with CUDA enabled",
      "z": "you are running this on OSX. The OSX binary of PyTorch does not come with GPU support.\\r\\n\\r\\nThe code you linked to still has some GPU stuff remaining (as you see from the stack-trace.\\r\\n\\r\\nChange these two lines to get_iterator:\\r\\nhttps://github.com/eladhoffer/captionGen/blob/48552694775ef11f5fa68c100584f40d98e4b690/main.py#L96\\r\\n\\r\\n`get_iterator(..., pin_memory=False)`\\r\\n",
      "y": "PyTorch does not support GPU on OSX"
   },
   {
      "x": "Variable methods which need to change before we combine Variable and Tensor",
      "z": "We have additional work to do before we can combined Variable and Tensor, but all these methods are implemented.",
      "y": "These methods are already implemented."
   },
   {
      "x": "using torch.utils.data.Dataset to make my dataset, find the index is out of the len defined in the __len__",
      "z": "This confirms my worst fears, python allows for pathologic iterables that have a length but never end... The fact that the `reversed` trick works is even weirder from a logical point of view: Reversing an infinite iterable results in a finite iterable... :woozy_face:\\r\\n\\r\\nAfter digging a bit through the [python reference](https://docs.python.org/3/reference/datamodel.html#object.__getitem__), the intended way of handling this is by raising an `IndexError` from `__getitem__` for invalid indices. Upon encountering this, the for loop will stop automagically. Changing my above example like below makes the iteration work as expected\\r\\n\\r\\n```python\\r\\nfrom torch.utils.data import Dataset\\r\\n\\r\\nclass TestDataset(Dataset):\\r\\n def __init__(self):\\r\\n super().__init__()\\r\\n\\r\\n def __getitem__(self, i):\\r\\n if i < 0 or i >= len(self): # These two lines\\r\\n raise IndexError() # are new\\r\\n return 0\\r\\n\\r\\n def __len__(self):\\r\\n return 10\\r\\n\\r\\n\\r\\ndataset = TestDataset()\\r\\nfor i, data in enumerate(dataset):\\r\\n print(i)\\r\\n assert i < len(dataset)\\r\\n```\\r\\n\\r\\nI believe this issue can be closed, as it's not pytorch specific. (Of course, @minghuisvn you're free to object, as I kind of highjacked your issue thread :wink:)",
      "y": "This issue is not related to PyTorch, but to Python."
   },
   {
      "x": "Windows source build fails with 'error LNK2019' at linking stage",
      "z": "I found what causing the problem, and wanted to inform you since it might be useful. `BUILD_TEST=0` variable is causing this problem at linking stage. If that not being set it builds successfully.",
      "y": "Unset the flag `BUILD_TEST=0` to fix the error."
   },
   {
      "x": "Too few arguments to vulkanOptimizeForMobile()",
      "z": "The issue was fixed in https://github.com/pytorch/pytorch/pull/45052",
      "y": "This issue has been fixed."
   },
   {
      "x": "CXXABI_* and GLIBCXX_* not found on gcc 4.8.2 after building Pre-cxx11 ABI Libtorch from source using gcc 5.4.0",
      "z": "Can you share a few more details how you've built gcc-5.4.0?\\r\\nBy default, GCC is coupled with a version of `libstdc++`, but there is a way to configure the build to use version already available in the system path. \\r\\nOtherwise, it sort of expected behaviour: shared library/executable compiled against newer version of libstdc++ is not compatible with an old one.\\r\\n\\r\\nPyTorch binary is build using `devtoolset-7`, which can be installed using something like the following \\r\\nhttps://github.com/pytorch/builder/blob/589a615fc8a8ee24690a1037ba583d32f22bc3a3/manywheel/Dockerfile#L12-L14\\r\\n\\r\\nFollowing article describes process of installing newer toolchain on CentOS in a bit more detail: https://ahelpme.com/linux/centos7/how-to-install-new-gcc-and-development-tools-under-centos-7/",
      "y": "Building PyTorch binary using `devtoolset-7` can fix this issue."
   },
   {
      "x": "nn.Module.script",
      "z": "PyText is replacing `ScriptVocab` with TorchText's `Vocab`. However we have to add this `to_ivalue` to our transforms in order to scriptify them because of this issue. It would be great if users like us don't need to add this additional function to all layers to keep our interface clean. Thanks! CC @hudeven",
      "y": "Please use  TorchText's `Vocab` instead of `ScriptVocab`."
   },
   {
      "x": "Add RMSE loss function",
      "z": "Actually, there is probably no advantage in having RMSE as a loss function. It's more computationally expensive than MSE and I don't see how it can be helpful to compensate for that. What I actually wanted is ready-made RMSE metric, not a loss function. Not sure if having a library of metrics is in scope for pytorch, so feel free to close this if the issue is not useful.",
      "y": "This feature will not be implemented."
   },
   {
      "x": "Is there a typo in ReLU6 declaration ?",
      "z": "nope if you look at the documentation of `relu6` and `hardtanh` it's reasonable to do subclass. Closing since it's resolved.",
      "y": "No, there isn't."
   },
   {
      "x": "Ellipsis support for view and reshape functions",
      "z": "I'm afraid this could be an ill-defined operation in general (specially if mixed with `-1` reshaping).\\r\\n\\r\\nFor the most common cases of flattening only a subset of dimensions, we have `tensor.flatten(start, end)`, which IMO is a good compromise in veneral.",
      "y": "This feature will not be implemented."
   },
   {
      "x": "Static Quantized model accuracy varies greatly with Calibration data",
      "z": "Using numeric suite I was able to debug and fix the issue.\nAppreciate all your help and time.",
      "y": "This issue can be fixed using numeric suite."
   },
   {
      "x": "test_nn.py returns inconsistent result in different test setup",
      "z": "Here's a reliable reproduction\\r\\n```\\r\\nimport torch\\r\\n\\r\\ninput_channels = 3\\r\\noutput_channels = 3\\r\\nbatch_size = 2\\r\\ndepth=3\\r\\nheight = 5\\r\\nwidth = 5\\r\\nkernel = 1\\r\\nstride = 1\\r\\nwith torch.backends.mkldnn.flags(enabled=False):\\r\\n conv_op = torch.nn.Conv3d(\\r\\n input_channels,\\r\\n output_channels,\\r\\n kernel,\\r\\n bias=False, # No bias\\r\\n ).to(dtype=torch.double)\\r\\n input = torch.randn(batch_size, input_channels, depth, height, width, dtype=torch.double, requires_grad=True)\\r\\n out = conv_op(input)\\r\\n gO = torch.rand_like(out)\\r\\n out.backward(gO)\\r\\n print(conv_op.weight.grad)\\r\\n```\\r\\nThe issue is on cpu, not on cuda.",
      "y": "This issue does not arise while using GPU."
   },
   {
      "x": "Nightly builds for cp38 missing for non-windows targets",
      "z": "Looks like we neglected to add them to `master`, submitted #34732 to remedy that",
      "y": "This issue has been fixed."
   },
   {
      "x": "Wrong Result when Converting Odd Integers Larger than 2^24 to Tensor",
      "z": "This is expected, as it's a float32 limitation.\\r\\nFrom the [Wikipedia article](https://en.wikipedia.org/wiki/Single-precision_floating-point_format):\\r\\n> Integers between 0 and 16777216 can be exactly represented (also applies for negative integers between \u221216777216 and 0)\\r\\nIntegers between `2**24=16777216` and `2**25=33554432` round to a multiple of 2 (even number)\\r\\nIntegers between `2**25` and `2**26` round to a multiple of 4\\r\\n...",
      "y": "This is an expected behavior."
   },
   {
      "x": "Building wheel for torch (setup.py) ... error - While running pip install torch",
      "z": "You could not do `pip install torch` because the Windows packages are not hosted on PyPI. Instead, please enter the commands in https://pytorch.org.",
      "y": "Install using instructions from PyTorch's website."
   },
   {
      "x": "load_state_dict_from_url error with weights downloaded from Google Drive ?",
      "z": "That URL is not a PyTorch file (.pth). It's a webpage with a link to a PyTorch file.\\r\\n\\r\\nYou need a direct download URL. Google drive doesn't support that directly, but you can use something like https://sites.google.com/site/gdocs2direct/",
      "y": "Please use a direct download URL, something like `https://sites.google.com/site/gdocs2direct/`"
   },
   {
      "x": "Support arbitrary types in jit",
      "z": "We have support for classes in TorchScript, is there something in particular that's missing that you're looking for? NamedTuples are also supported. We also have other things like enum.Enum and @dataclass on our roadmap but no concrete timeline yet. We're also introducing a way to bind C++ classes into TorchScript in our next release.",
      "y": "These methods are already implemented."
   },
   {
      "x": "[C++ API] Support for CIFAR10 and CIFAR100 Datasets",
      "z": "Not yet, will try to add one by this weekend as an example",
      "y": "They will be added soon"
   },
   {
      "x": "Cannot use setup.py install",
      "z": "> ModuleNotFoundError: No module named 'past'\\r\\n\\r\\nModule `past` is part of `python3-future` package on Ubuntu (or just `pip3 install future`)",
      "y": "You need to install the package 'future'"
   },
   {
      "x": "torch.cat is moving Tensors across devices silently",
      "z": "One case (hopefully supported in future) where moving between devices makes sense:\n`x = torch.cat([torch.rand(3, device = 'cuda'), 1])` for appending python scalars to a tensor",
      "y": "This is an expected behavior."
   },
   {
      "x": "Compilation error on aarch64",
      "z": "I ran into the same issue on aarch64. I am able to work around it by downloading sleef 3.5.1, install it to system and set the USE_SYSTEM_SLEEF=ON flag.\n\nHope it helps",
      "y": "You need to set the flag `USE_SYSTEM_SLEEF=ON`."
   },
   {
      "x": "[feature request] Allow `torch.unsqueeze` to insert multiple new dims",
      "z": "@tshadley \\r\\n\\r\\n> ```\\r\\n> import torch\\r\\n> t_b = t[...,(None,)*3]\\r\\n> print(t_b.shape)\\r\\n> ```\\r\\n\\r\\nTo unsqueeze at the end, you could try to use `t_b = t[(..., ) + (None, ) * 3]` (works for me on version 1.1.0).",
      "y": "You can use `t_b = t[(..., ) + (None, ) * 3]`"
   },
   {
      "x": "[feature request] add `torch.find` to find the indices of values",
      "z": "A not very optimized version of this function can be obtained with a one-liner I believe (for 1d `values`)\\r\\n```python\\r\\ndef find(tensor, values):\\r\\n return torch.nonzero(tensor[..., None] == values)\\r\\n```",
      "y": "You can use \\r\\n```python\\r\\ndef find(tensor, values):\\r\\n return torch.nonzero(tensor[..., None] == values)\\r\\n```"
   },
   {
      "x": "Segmentation Fault using dist.broadcast() with openmpi",
      "z": "Actually I tested a simple cuda program doing MPI_Bcast/MPI_allreduce and confirmed that it also segfaults there with openmpi 1.10, while it works fine on openmpi 2.1+. I'm more inclined to a openmpi issue in this case. I will make a PR to add this warning to the doc I think.",
      "y": "Installing the latest version of openmpi fixes this issue."
   },
   {
      "x": ".topk() returns incorrect values + indeces on non-contiguous tensors (CUDA)",
      "z": "The reason this fails is `topk` needs contiguous inputs and the outputs of Beta.sample aren't.\\r\\nCan you retitle the bug report to that?\\r\\nI'll submit a PR to check contiguous in topk.",
      "y": "You need to pass contiguous inputs to `topk`"
   },
   {
      "x": "OOM when using Adam optimizer compared to SGD when using same batch size.",
      "z": "Adam is more stateful than SGD, so it is expected that it uses more memory (proportional to the total size of the optimized parameters).",
      "y": "Adam is more stateful than SGD, so it is expected that it uses more memory."
   },
   {
      "x": "log_prob returns positive values for small cov",
      "z": "The values of the `pdf` can be arbitrarily large but never negative. This is completely acceptable, and I am sure this is not a bug.\\r\\n\\r\\nSimple example: normal distribution with standard deviation = 0.000001 and mean = 0. The `pdf` at `x = 0` is `1000 / (2 * pi)`, and the natural log of this is clearly greater than 0.",
      "y": "The values of the `pdf` can be arbitrarily large but never negative. "
   },
   {
      "x": "\\\"ImportError: No module named tools.setup_helpers.env\\\" when \\\"python setup.py egg_info\\\"",
      "z": "Does it work if you do \"FULL_CAFFE2=1 python setup.py install\" instead?",
      "y": "Please install using the flag `FULL_CAFFE2=1`"
   },
   {
      "x": "[bug] Multiplication of tensor with numpy scalar does not always work",
      "z": "The problem is that the left operand is the default operand to execute the `__mul__`.\\r\\nThe numpy scalar's `__mul__` will then call the tensor's `__array__` and that fails (and there isn't a way out).\\r\\nThe solution seems to be to set a high `__array_priority__`, then the scalar (and an array) will call the Tensor's `__rmul__` instead.\\r\\nStandard numpy scalars have a strongly negative priority, standard ndarrays have one of 0.\\r\\nSo in case 2:\\r\\n```\\r\\ntensor.__array_priority__ = 1000\\r\\nprint (scalar * tensor)\\r\\n```\\r\\nworks!\\r\\n\\r\\nI'm happy to send a PR.\\r\\n",
      "y": "The problem is that the left operand is the default operand to execute the `__mul__`.\\r\\nThe numpy scalar's `__mul__` will then call the tensor's `__array__` and that fails (and there isn't a way out).\\r\\nThe solution seems to be to set a high `__array_priority__`, then the scalar (and an array) will call the Tensor's `__rmul__` instead."
   },
   {
      "x": "git clone --recursive https://github.com/caffe2/caffe2.git gives error that Eigen repository is not found.",
      "z": "Sorry, I did not realize you were trying to install detectron. It looks like detectron expects to find Caffe2 through find_package, which right now requires Caffe2 to be built from source. What's causing that error message is that a source build (through Caffe2's cmake) populates a \"Caffe2 target\" that tells other cmake projects (Detectron) where to find Caffe2. These extra cmake files aren't included in the pre-built packages because you shouldn't need cmake to install and use Caffe2.\n\nWhen installing from source, do still read through https://caffe2.ai/docs/faq.html#why-do-i-get-import-errors-in-python-when-i-try-to-use-caffe2 and follow it's recommendations; it can save you from a lot of frustrating errors.",
      "y": "Installing Caffe2 from source fixes the error."
   },
   {
      "x": "[jit] support at::optional",
      "z": "cc @wanchaol \\r\\n\\r\\n@SsnL the nan is a temporary stopgap. I believe the end state is indeed to support `at::optional` or something similar.",
      "y": "This is an expected behavior."
   },
   {
      "x": "[distributions] dirichlet pathwise gradient does not work well with .expand",
      "z": "I think this should be closed now.",
      "y": "This issue has been fixed."
   },
   {
      "x": "[feature request] Add option to return matched / unmatched / unexpected in `load_state_dict`",
      "z": "After a lot of discussion, we decided not to move on with this proposal.",
      "y": "This feature will not be implemented."
   },
   {
      "x": "RuntimeError: cuda runtime error (30) on Ubuntu18,CUDA9.1,cudnn7.0.5 when torch.cuda.is_available() returns True",
      "z": "maybe \\\"sudo python\\\" can solve it",
      "y": "Use `sudo python` to fix this."
   },
   {
      "x": "Add python 3.7 to binary install page",
      "z": "python 3.7 binaries are live on PyPI, conda and on https://pytorch.org",
      "y": "python 3.7 binaries are live on PyPI, conda and on https://pytorch.org"
   },
   {
      "x": "Caffe2 Train your own image",
      "z": "@aswin1980 Check out the \\\"CIFAR10_Part1\\\" and \\\"CIFAR10_Part2\\\" tutorials in the [caffe2/tutorials](https://github.com/caffe2/tutorials/) repo. Part 1 specifically shows how to take a custom image dataset (in this case .png mirror of the CIFAR-10 dataset), format it, create image LMDBs, and train a model on them.",
      "y": "Please refer to the \\\"CIFAR10_Part1\\\" and \\\"CIFAR10_Part2\\\" tutorials in the [caffe2/tutorials](https://github.com/caffe2/tutorials/) repo. "
   },
   {
      "x": "Conda Install: PackageNotFoundError",
      "z": "@pjh5 \\r\\n\\r\\nUpdating conda to 4.5.4 and then running `conda create -n pytorch python=3` instead of `conda create -n pytorch anaconda` fixed it! I'm able to properly install pytorch now by running `conda install pytorch torchvision -c pytorch`. \\r\\n\\r\\nThanks for the help.",
      "y": "This can be fixed by updating conda to 4.5.4 and then running `conda create -n pytorch python=3` instead of `conda create -n pytorch anaconda`."
   },
   {
      "x": "Always get error \\\"ConnectionResetError: [Errno 104] Connection reset by peer\\\"",
      "z": "for me, I found set thread number equal 0 will solve this problem, namely:\\r\\nnum_workers=0\\r\\n",
      "y": "Setting `num_workers=0` will fix this problem"
   },
   {
      "x": "[Bug] Segmentation fault when importing fastText (with v0.4.0)",
      "z": "For the record, the problem was:\\r\\n - in a conda environment, I installed pytorch with `conda install`(as described on pytorch web site) and fastText with `pip install .` from their git clone.\\r\\n - that resulted in a segfault when doing `import fastText` and `import torch` \\r\\n\\r\\nReason:\\r\\n - pytorch is compiled with gcc 4.9.2\\r\\n - conda's default gcc is 4.8.5\\r\\n\\r\\nFix:\\r\\n - install gcc-4.9 in conda (e.g. `conda install -c serge-sans-paille gcc_49`)\\r\\n - install pytorch with `conda install` (in my case, `conda install pytorch torchvision cuda90 -c pytorch`)\\r\\n - install fastText with gcc-4.9 compiler: `CC=gcc-4.9 pip install .` in the fastText git clone\\r\\n\\r\\nThat's it! Thanks a lot @weiyangfb and @SsnL for your help!",
      "y": "To fix:\\r\\n - install gcc-4.9 in conda\\r\\n - install pytorch with `conda install` r\\n - install fastText with gcc-4.9 compiler: `CC=gcc-4.9 pip install .` in the fastText git clone"
   },
   {
      "x": "How to set USE_OPENVB=ON and BUILD_CAFF2=ON when build from source",
      "z": "@Tianji95 this problem is outdated. The FULL_CAFFE2 flag no longer exists and is no longer needed.",
      "y": "This issue has been fixed."
   },
   {
      "x": "[feature request] torch.isinf, torch.isfinite",
      "z": "`torch.isinf` merged in, `torch.isfinite` not implemented yet",
      "y": "`torch.isinf` has been merged; `torch.isfinite` has not been implemented yet"
   },
   {
      "x": "cannot reload on CPU model saved on GPU",
      "z": "When you do `torch.load(.....)`, set `torch.load(...., map_location='cpu')`. If you don't, since you are loading GPU tensors, PyTorch will try to reconstruct GPU tensors, and fail.",
      "y": "When you do `torch.load(.....)`, set `torch.load(...., map_location='cpu')`."
   },
   {
      "x": "setting CUDA_VISIBLE_DEVICES just has no effect",
      "z": "You need to do that before import pytorch.",
      "y": "You need to do that before import pytorch."
   },
   {
      "x": "[feature request] nn.Identity",
      "z": "You can use nn.Sequential() to simulate identity",
      "y": "You can use nn.Sequential() to simulate identity"
   },
   {
      "x": "Big drop in performance for larger batch size for otherwise same training script",
      "z": "You should tune different hyparams (e.g., lr) to accommodate different batch size. Some say that larger batch size requires larger lr too.",
      "y": "You should tune different hyparams (e.g., lr) to accommodate different batch size. Larger batch size requires larger lr too."
   },
   {
      "x": "torch.save() and nn.DataParallel()",
      "z": "I usually do:\\r\\n```\\r\\ntry:\\r\\n state_dict = model.module.state_dict()\\r\\nexcept AttributeError:\\r\\n state_dict = model.state_dict()\\r\\n```",
      "y": "Use the following :\\r\\n```\\r\\ntry:\\r\\n state_dict = model.module.state_dict()\\r\\nexcept AttributeError:\\r\\n state_dict = model.state_dict()\\r\\n```"
   },
   {
      "x": "torch.Tensor.__repr__ is slow",
      "z": "This has been fixed in master. Here are my timings: ...",
      "y": "This issue has been fixed."
   },
   {
      "x": "[feature request] Convert \\\"indices\\\" variable in \\\"torch.utils.data.dataset.random_split\\\" to list",
      "z": "I think we can just add a .tolist() after the code in here. Could you send a PR?",
      "y": "You can add a `.tolist()` after the code."
   },
   {
      "x": "problem building with ROCm",
      "z": "There's a \\\"hipify\\\" step to replace all the CUDA references with HIP/ROCm references in-place: \\\"python tools/amd_build/build_pytorch_amd.py\\\". Also, don't forget to set env var USE_ROCM to 1. After that, you should be able to build using the normal \\\"python setup.py\\\" step.",
      "y": "Please set the environment variable `USE_ROCM=1`"
   },
   {
      "x": "RNN weights are not Xavier-initialized",
      "z": "Also, adding to @Kaixhin 's point, if you really want Xavier initialization, you could try initializing manually using `nn.init.xavier_uniform` / `nn.init.xavier_normal`.",
      "y": "You cant try initializing manually using `nn.init.xavier_uniform` / `nn.init.xavier_normal`."
   },
   {
      "x": "Sequential does not allow muti-output modules such as RNN, LSTM",
      "z": "As I said in forum, this is not a bug.",
      "y": "This is an expected behavior."
   },
   {
      "x": "Weights won't update during backpropogation",
      "z": "your learning rate simply has to be increased, and as Issam pointed out SGD has quite a bit of variance in the gradients.",
      "y": "Your learning rate has to be increased."
   },
   {
      "x": "error: identifier \\\"__half_as_ushort\\\" is undefined",
      "z": "It tends out to be my own installation problem. cuda_fp16.h is not inside /usr/local/cuda/include but I've found it in /usr/local/cuda/targets/x86_64-linux/include. Maybe because I used to have an older version of cuda installed.\\r\\n\\r\\nAnyway, I completely removed cuda and reinstalled it. All the header files are now in place and built with no problem.",
      "y": "This error can be resolved by reinstalling CUDA."
   },
   {
      "x": "When I was training a CNN+GRU model with CTC loss, I got the nan loss after several batches.",
      "z": "I meet the same error on pytorch 1.0.0.dev20181115 with inner ctc loss, but did not encounter this situation at pytorch 0.4 with warpctc loss",
      "y": "Install the latest version of PyTorch to fix this."
   },
   {
      "x": "Memory leak from Function.save_for_backward() when looping over batch",
      "z": "the `@staticmethod` stuff is the right / official way. We'll change the tutorials right away.",
      "y": "Use static methods to fix this issue."
   },
   {
      "x": "Segfault in neg, introduced in 3e6e81d",
      "z": "Fixed in #3433",
      "y": "This issue has been fixed."
   },
   {
      "x": "`torch.utils.data.DataLoader`",
      "z": "this is a HDF5 issue. The problem is that HDF5 concurrent reads aren't safe: \\r\\nhttps://github.com/pandas-dev/pandas/issues/12236\\r\\nhttps://github.com/pandas-dev/pandas/issues/14692\\r\\n\\r\\nTo actually allow concurrent reads for a file you have to use SWMR feature of HDF5: https://support.hdfgroup.org/HDF5/docNewFeatures/NewFeaturesSwmrDocs.html\\r\\n\\r\\n\\r\\n",
      "y": "This is a HDF5 issue. The problem is that HDF5 concurrent reads aren't safe. To actually allow concurrent reads for a file you have to use SWMR feature of HDF5."
   },
   {
      "x": "inconsistent behavior of max",
      "z": "this was fixed in master. will be part of the next release.",
      "y": "This issue has been fixed."
   },
   {
      "x": "Kaiming/Xavier initializer cannot deal with bias term",
      "z": "Correct me if I am wrong. I think those methods are not supposed to deal with bias terms. In the paper, bias terms are initialized to zeros, which you can achieve by `layer.bias.data.zero_()`",
      "y": "This can be achieved by `layer.bias.data.zero_()`"
   },
   {
      "x": "Implementation Discussion: Native CTC",
      "z": "You would have to use 1.0rc, I wasn't quite finished in time for 0.4.1.",
      "y": "This feature has been implemented."
   },
   {
      "x": "Can I plz has determinant function?",
      "z": "fixed on master via #3816",
      "y": "This feature has been implemented."
   },
   {
      "x": "Recent bug in torch.cat() on Variables?",
      "z": "Great, this is also fixed on the `v0.3.0` branch as of 9a67882",
      "y": "This issue has been fixed."
   },
   {
      "x": "cuda out of memory error when GPU0 memory is fully utilized",
      "z": "@TomHeaven did you set CUDA_VISIBLE_DEVICES outside the python process? if that's the case pytorch should not even have driver-level access to your GPU0.\\nIdeally: `CUDA_VISIBLE_DEVICES=1 python foo.py`",
      "y": "You need to set `CUDA_VISIBLE_DEVICES` outside the python process"
   },
   {
      "x": "RuntimeError: CUDA error (3): initialization error",
      "z": "as mentioned in http://pytorch.org/docs/master/notes/multiprocessing.html#sharing-cuda-tensors\\r\\n\\r\\ninsert this to the top of your script\\r\\n\\r\\n```\\r\\nimport torch\\r\\ntorch.multiprocessing.set_start_method(\\\"spawn\\\")\\r\\n```",
      "y": "Insert this to the top of your script\\r\\n\\r\\n```\\r\\nimport torch\\r\\ntorch.multiprocessing.set_start_method(\\\"spawn\\\")\\r\\n```"
   },
   {
      "x": "RuntimeError: context has already been set(multiprocessing)",
      "z": "Hi @pancho111203 ,\\r\\n\\r\\nYou might have other files in your project which also have a `if __name__ == '__main__':`. One workaround is to call the `set_start_method` with the `force` argument as: `set_start_method('forkserver', force=True)`. This solved the issue for me.\\r\\n\\r\\nRegards\\r\\nNabarun",
      "y": "You can call the `set_start_method` with the `force` argument as: `set_start_method('forkserver', force=True)`."
   },
   {
      "x": "Gradient Ascent Cross Entropy Loss",
      "z": "@cdjhz `(-loss).backward(); optimizer.step()`\n",
      "y": "Use this: `(-loss).backward(); optimizer.step()`"
   },
   {
      "x": "ParameterList and ModuleList with named modules or parameters",
      "z": "fixed via #3505",
      "y": "This issue has been fixed."
   },
   {
      "x": "PyTorch Implementation of Michael Jordan\u2019s lab's Perturbed SGD?",
      "z": "I think adding noise to gradients is simple. A simple `apply_` call should be able to iterate through all model parameters and add the noise to the gradients, after which `step` is called.\\r\\n\\r\\nI initially proposed adding Noisy SGD, but the proposal was rejected considering its triviality. Ref: https://github.com/pytorch/pytorch/pull/4332",
      "y": "A simple `apply_` call should be able to iterate through all model parameters and add the noise to the gradients, after which `step` is called."
   },
   {
      "x": "NameError: name 'logging' is not defined",
      "z": "I think this has been fixed on master, because the point of error is non-existent on master.",
      "y": "This issue has been fixed."
   },
   {
      "x": "Builing for a specific SM number",
      "z": "Yes. By example:\\r\\n```\\r\\nTORCH_CUDA_ARCH_LIST=\\\"5.2;6.1;7.0\\\" python setup.py install\\r\\n```",
      "y": "You can do this via:\\r\\n```\\r\\nTORCH_CUDA_ARCH_LIST=\\\"5.2;6.1;7.0\\\" python setup.py install\\r\\n```"
   },
   {
      "x": "[jit] torch.empty_like is different from eager mode",
      "z": "I think I can fix this - it's probably not an issue with `empty_like` in particular, but with all optional arguments. https://github.com/pytorch/pytorch/pull/22055 fixes one case of this.",
      "y": "This is not an issue with `empty_like` in particular, but with all optional arguments."
   },
   {
      "x": "RuntimeError: cuDNN error: CUDNN_STATUS_INTERNAL_ERROR in 1.1.0",
      "z": "Try to use this command from the [website](https://pytorch.org/get-started/locally/):\\r\\n```\\r\\npip3 install torch torchvision\\r\\n```\\r\\n\\r\\nThis should install PyTorch 1.2, CUDA10.0 for Python3.6 on a Linux OS.\\r\\nJust select whatever config matches your current setup and use the shown install command.",
      "y": "Use this command:\\r\\n```\\r\\npip3 install torch torchvision\\r\\n```"
   },
   {
      "x": "CI failure points to nonexistent code...",
      "z": "fixed via #23304 by @zou3519 :D",
      "y": "This issue has been fixed."
   },
   {
      "x": "torch.onnx._export does not support tensor sum with multiple dims",
      "z": "@yil8 - yes, you can install the nightly build of PyTorch to test this.",
      "y": "Install the latest version of PyTorch to fix this."
   },
   {
      "x": "issue with ONNX and PyTorch",
      "z": "@arijit17 - It is hard to say where exactly the issue is without looking at your model and export code. Could you please share the repro? \\r\\nFYI - if your model has input-dependent control flow, then ideally you will have to convert those modules to `ScriptModule` and decorate those modules' `forward()` method with with `@torch.jit.script` to enable correct JIT compilation. But once you do that the export process itself should not change.\\r\\nIf you can share a repro, we might be able to help better.",
      "y": "You have to convert those modules to `ScriptModule` and decorate those modules' `forward()` method with with `@torch.jit.script` to enable correct JIT compilation."
   },
   {
      "x": "Wrong distribution sampled by torch.multinomial on CUDA",
      "z": "Closed by #22183",
      "y": "This issue has been fixed."
   },
   {
      "x": "Inplace error if DistributedDataParallel module that contains a buffer is called twice",
      "z": "The cause here is that by default DDP modules broadcast the contents of the root process module's buffers to all processes at every forward pass, and this broadcasting counts as an inplace operation for the purposes versioning. The fix is to disable the broadcasting by setting `broadcast_buffers=False` in the DDP module constructor. Thanks @pietern for the help.",
      "y": "To fix this , disable the broadcasting by setting `broadcast_buffers=False` in the DDP module constructor."
   },
   {
      "x": "LR scheduler design bug !",
      "z": "It's in master: you need to compile from source for now.",
      "y": "This issue has been fixed."
   },
   {
      "x": "Segmentation fault Autograd",
      "z": "@shoukang by any chance you can try installing new Pytorch, we have done a lot in terms of threading recently. And your problem might be related.",
      "y": "This issue has been fixed."
   },
   {
      "x": "Why aren't torch.functional.sigmoid and torch.nn.functional.relu deprecated like torch.nn.functional.tanh?",
      "z": "Actually, in the current code, sigmoid is also deprecated in nn https://github.com/pytorch/pytorch/blob/master/torch/nn/functional.py#L1390. But not relu. Are you now saying that sigmoid is a general purpose mathematical function but relu isn't ?",
      "y": "In the current code, sigmoid is also deprecated in nn, but not relu."
   },
   {
      "x": "Building from source failed. Multiple errors in the printout",
      "z": "You can find good answer here: https://github.com/torch/torch7/issues/1190#issuecomment-498934400 (credits to @talkenig)\\r\\n\\r\\nCopy pasting for future reference:\\r\\n\\r\\nHad the same problem and got to the bottom of it. My configuration is a Tesla T4 with 410.92 driver and CUDA 9.2, Ubuntu 18.10.\\r\\nThe thing is, the torch installer tries for some reason to use the highest compute capability supported by the device (or the driver - not sure which), but ignores the compute capability supported by the CUDA toolkit.\\r\\nSo, in my case the device supports compute capability 7.5, but CUDA 9.2 supports only 7.0 or 7.2 (not sure which one). You can guess I got the same nvcc fatal : Unsupported gpu architecture 'compute_75'\\r\\nThe solution is to force the nvcc compile options to use a lower compute capability. This can be achieved by setting the following environment variable:\\r\\n\\r\\n`export TORCH_CUDA_ARCH_LIST=\\\"7.0\\\"`\\r\\n\\r\\nJust before running ./install.sh from the torch directory.\\r\\nNote that the list may contain more than one compute capability, e.g. it can be \\\"6.0 6.2 7.0 7.2\\\". This will be reflected in the CUDA_NVCC_FLAGS -gencode arch=compute_70,code=sm_70 and those will be outputted to the terminal during the build.\\r\\n",
      "y": "To fix this, export the following environment variable:\\r\\n\\r\\n`export TORCH_CUDA_ARCH_LIST=\\\"7.0\\\"`"
   },
   {
      "x": "[dataloader] Add a context= argument for multiprocessing",
      "z": "This is expected, because thed spawned workers does not see the dataset def.\\n I think the proper way to solve this is to add a `context=` argument to data loader, so that a global start method needs not be set.",
      "y": "This is expected, because thed spawned workers does not see the dataset def.\\n The proper way to solve this is to add a `context=` argument to data loader."
   },
   {
      "x": "Torch crashes when calling torch.rand(2,3)",
      "z": "your processor is old enough that it doesn't support SSE4.1/SSE4.2/SSE4.3 instruction. We do not support processors that dont have these features in binaries.\\r\\nYour only choice is to install from source: https://github.com/pytorch/pytorch#from-source",
      "y": "Your processor is old enough that it doesn't support SSE4.1/SSE4.2/SSE4.3 instruction. You have to install from source."
   },
   {
      "x": "No method to set the timeout for distributed Gloo backend",
      "z": "@ejoebstl this would helps a lot, i used to solve this problem by changing the default timeout and recompile the whole pytorch o(\u2565\ufe4f\u2565)o",
      "y": "This can be fixed by changing the default timeout and recompiling PyTorch"
   },
   {
      "x": "[PyTorch] Build error (NCCL) on Ubuntu 16.04",
      "z": "A simple workaround is to set `WITH_SYSTEM_NCCL` to `False` to force compile with provided NCCL. https://github.com/pytorch/pytorch/blob/master/tools/setup_helpers/nccl.py#L74\\r\\n\\r\\nAfter modifying this, I can compile PyTorch without error. \\r\\n",
      "y": "A simple workaround is to set `WITH_SYSTEM_NCCL` to `False` to force compile with provided NCCL. "
   },
   {
      "x": "Anaconda3, Ubuntu 16.04 Python 3.6 Caffe2 installation issue",
      "z": "\"It looks like you installed with setup_caffe2.py, as setuptools (the Python package behind setup.py scripts) is what installs .egg files.\\r\\n\\r\\nCould you try `pip uninstall caffe2` and `conda install -c caffe2 caffe2-cuda9.0-cudnn7` ? This way should be much faster too.\\r\\n\\r\\nIf the pip uninstall caffe2 doesn't seem to work, then you can manually uninstall by deleting every file and folder under /home/sam/anaconda3/envs/caffe36/ that has 'caffe' or 'caffe2' in the name.",
      "y": "You need to uninstall and reinstall Caffe2."
   },
   {
      "x": "BatchNorm2d when batch size 1 works, what is it doing?",
      "z": "That is normalizing `[B x C x *]` over the dimensions `[*]`",
      "y": "It is normalizing `[B x C x *]` over the dimensions `[*]`"
   },
   {
      "x": "can't rebuild with NO_CUDA=1 after clean",
      "z": "Doing the following seems to have fixed it; I'm not sure which of these is okay for clean to leave around:\\r\\n`\\r\\nrm -rf aten/src/ATen/Config.h aten/build/ third_party/build/ third_party/aten/\\r\\n`",
      "y": "Please run :\\r\\n`\\r\\nrm -rf aten/src/ATen/Config.h aten/build/ third_party/build/ third_party/aten/\\r\\n`"
   },
   {
      "x": "Where is the Caffe2 website?",
      "z": "https://github.com/caffe2/caffe2.github.io",
      "y": "It is at `https://github.com/caffe2/caffe2.github.io`."
   },
   {
      "x": "Error building from source CMakeFiles/Makefile2:201: recipe for target 'src/ATen/CMakeFiles/ATen.dir/all' failed",
      "z": "I had the same problem and renamed `THGeneral.h` from `conda/envs/<ENV_NAME>/` to something similar to what @rgreenblatt  said, and it was installed normally. Good luck to friends who are having the same problem.\\r\\n\\r\\n```\\r\\nmv ~/anaconda3/envs/<ENV_NAME>/include/TH/THGeneral.h ~/anaconda3/envs/<ENV_NAME>/include/TH/THGeneral.h.old\\r\\n```",
      "y": "Please run \\r\\n```\\r\\nmv ~/anaconda3/envs/<ENV_NAME>/include/TH/THGeneral.h ~/anaconda3/envs/<ENV_NAME>/include/TH/THGeneral.h.old\\r\\n```"
   },
   {
      "x": "Add `torch.pi` like `numpy.pi` and possibly other constants",
      "z": "Aren't all of those constants in math?",
      "y": "They exist in `math`"
   },
   {
      "x": "torch.irfft produces \\\"cuFFT error: CUFFT_ALLOC_FAILED\\\" when called after torch.rfft",
      "z": "Me either, it seems to work now. Maybe you fixed something along the way :) Maybe this can be closed, having fewer temp tensors along the way also helps. Might be even better if complex product gets implemented some day.\\nIt would be nice if the OOM exception was some standard PyTorch exception, not a CUDA one.",
      "y": "Having fewer temp tensors along the way can help."
   },
   {
      "x": "[feature request] Complex multiplication",
      "z": "One verbose way may be a kwarg `complex = True` to `torch.mul`. Some alternative ideas: `mul_complex` / `mulc`",
      "y": "Add a kwarg `complex = True` argument to `torch.mul`."
   },
   {
      "x": "cuda runtime error (48): no kernel image is available for execution on the device",
      "z": "Hi, your problem is stated in this warning here\\r\\n```\\r\\n/home/azat/anaconda2/lib/python2.7/site-packages/torch/cuda/__init__.py:97: UserWarning:\\r\\n Found GPU0 GeForce 820M which is of cuda capability 2.1.\\r\\n PyTorch no longer supports this GPU because it is too old.\\r\\n```\\r\\n\\r\\nYou can build from source to use some functionality, but there are many operations that require compute capability greater than SM_21 to perform.",
      "y": "You can build from source to use some functionality. Some operations require a more recent GPU."
   },
   {
      "x": "Stop using undefined tensors to represent zero gradients in engine",
      "z": "going to close this because I believe the issues with undefined tensors in the engine have been addressed.",
      "y": "This issue has been fixed."
   },
   {
      "x": "Proposal: rename upsample to resample",
      "z": "This function has been deprecated in favor of interpolate so I think this issue has been addressed.",
      "y": "Use the `interpolate` function instead."
   },
   {
      "x": "RuntimeError for indexing with high dimensional tensor only when using cuda",
      "z": "As for pytorch 0.4 (nightly build 2018.04.20, installed via conda) this problem does not exist anymore. And the following code works:\\r\\n```python\\r\\nimport torch\\r\\nx = torch.randn(2, 2, 2, 2, 2, 2).cuda()\\r\\ni = torch.cuda.LongTensor([0, 0, 0, 0, 0, 0])\\r\\nx[i,:]\\r\\n```\\r\\nClosing the issue.",
      "y": "This issue has been fixed."
   },
   {
      "x": "RuntimeError: reduce failed to synchronize: unspecified launch failure",
      "z": "I was having similar error. Make sure your layer has values that make sense to the BCELoss. If you for example output negative values and pass them to the logarithm, the training will fail.\\r\\n\\r\\nIn my case I was missing the last sigmoid activation to shrink the numbers between 0 and 1.",
      "y": "Make sure your layer has values that make sense to the BCELoss."
   },
   {
      "x": "Zombie process when use GPU",
      "z": "For anyone who still suffer from this issue, try the following command:\\r\\n`fuser -k /dev/nvidia*`\\r\\nor\\r\\n`kill $(lsof -t /dev/nvidia*)`",
      "y": "Use the following command:\\r\\n`fuser -k /dev/nvidia*`\\r\\nor\\r\\n`kill $(lsof -t /dev/nvidia*)`"
   },
   {
      "x": "torch.HalfTensor' object has no attribute 'mean'",
      "z": "we don't have math for CPU Half type (it would be very slow), convert it to cuda or CPU Float.",
      "y": "Please convert tensor to cuda or CPU Float."
   },
   {
      "x": "import torch; libcublas.so.9.0 error",
      "z": "I had the exact same linking problem when trying to compile pytorch 0.3 with CUDA 9.1. I couldn't figure out how it manages to find and link to cublas.8. I gave up and installed the `pip` wheel package which contains CUDA, CuDNN everything inside.",
      "y": "Installing from pip fixes this error."
   },
   {
      "x": "Expose find Dangling Impls to Python",
      "z": "> Can I have instructions on how to run tests within the test_dispatch.py? I know how to run cpp tests in the codebase but not Python ones.\n \n In an open source build can do `python test/test_dispatch.py`\n \n > Since the Dispatcher::singleton().findDanglingImpls() returns std::vector<c10::OperatorHandle>, should I just return the above vector directly to the Python layer? I'm not sure if we have already had bindings for OperatorHandle.\n \n Nope there are no bindings, and probably don't want to actually directly bind OperatorHandle as it wasn't designed for Python binding (unless... you really want to?) See original:\n \n > Ideally it would be easy to get the (1) name of dangling impls and (2) filename/lineno where the dangling impl occurred.",
      "y": "In an open source build can do `python test/test_dispatch.py` \n There are no bindings, and probably don't want to actually directly bind OperatorHandle as it wasn't designed for Python binding"
   },
   {
      "x": "Inconsistent CUDA errors using PyTorch Docker image",
      "z": "We usually close the issues that cannot be reproduced and thus aren't actionable. Feel free to reopen if needed.",
      "y": "Run the example on another machine, and didn't encounter any issues. The issue lies with the specific configuration of the original machine"
   },
   {
      "x": "Parametrization goes into infinite recursion when trying to print a module",
      "z": "Note that this is not specific to parametrization, anything your do such a cycle of Modules, you will get a recursion error when printing:\n \n ```python\n \n import torch\n \n \n \n a = torch.nn.Linear(2, 2)\n \n b = torch.nn.Linear(2, 2)\n \n \n \n a.b = b\n \n b.a =a \n \n \n \n print(a) # RecursionError\n \n ```",
      "y": "Proposed approach - One can make it work modifying just the internal tensor storage via the function set_ (https://pytorch.org/docs/stable/generated/torch.Tensor.set_.html). This should work in the case when, for example, you are sharing the mask between a number of layers. You would then set a new mask as:\n \n model.linear.mask.set_(torch.zeros(model.linear.weight.shape))\n A simplified implementation could then be:\n \n import torch\n import torch.nn as nn\n import torch.nn.utils.parametrize as parametrize\n \n class WeightMaskParametrization(nn.Module):\n  def __init__(self, mask):\n  super().__init__()\n  self.mask = mask\n \n  def forward(self, w):\n  return self.mask * w\n \n model = nn.Linear(3, 4)\n model.register_buffer('mask', torch.ones_like(model.weight))\n parametrize.register_parametrization(model, 'weight', WeightMaskParametrization(model.mask))\n \n print(model.weight) # print original weight\n model.mask.set_(torch.zeros_like(model.weight))\n print(model.weight) # print zeros\n In this simplified example, it would be even better to have the mask live in the class WeightMaskParametrization rather than in nn.Linear, and thus, be managed directly by this class, but I guess that the real example motivating this issue may not allow for that.\n \n Another thing to point out is that this WeightMaskParametrization is just pruning in disguise. We will eventually move all the pruning methods into parametrizations, and the base parametrization will certainly take the form of WeightMaskParametrization, with a few more bells and whistles."
   },
   {
      "x": "Something like nn.Dropout2d which does channel dropout but for 1d data. Could be called nn.Dropout1d",
      "z": "Hey @chanshing, thanks for the suggestion! Unless I'm mistaken, contrary to what the docs say, `Dropout2d` supports entire channel dropout for 1D data without the need for a dummy axis:\n \n \n \n ```python\n \n >>> torch.nn.Dropout2d()(torch.randn(2, 3, 4))\n \n tensor([[[ 2.0013, 0.5137, 4.6231, -0.8030],\n \n  [ 0.2068, 1.2131, 1.2506, 2.1023],\n \n  [ 0.0000, -0.0000, 0.0000, -0.0000]],\n \n \n \n  [[-2.2049, -4.3484, 0.4871, 1.2764],\n \n  [-0.0000, 0.0000, 0.0000, 0.0000],\n \n  [-1.0111, -0.5624, 0.7527, -0.0970]]])\n \n ```\n \n \n \n Possible alternative may be to support a generic, properly-documented `Dropout` with configurable dims over which to dropout. See https://github.com/pytorch/pytorch/issues/46184 as well.",
      "y": "Contrary to what the docs say, `Dropout2d` supports entire channel dropout for 1D data without the need for a dummy axis:\n \n ```python\n >>> torch.nn.Dropout2d()(torch.randn(2, 3, 4))\n tensor([[[ 2.0013, 0.5137, 4.6231, -0.8030],\n  [ 0.2068, 1.2131, 1.2506, 2.1023],\n  [ 0.0000, -0.0000, 0.0000, -0.0000]],\n \n  [[-2.2049, -4.3484, 0.4871, 1.2764],\n  [-0.0000, 0.0000, 0.0000, 0.0000],\n  [-1.0111, -0.5624, 0.7527, -0.0970]]])\n ```\n \n Possible alternative may be to support a generic, properly-documented `Dropout` with configurable dims over which to dropout. See https://github.com/pytorch/pytorch/issues/46184 as well."
   },
   {
      "x": "Issue in executing the script module in C++",
      "z": "You need to call result.toTuple() instead of result.toTensor():\n ```C++\n auto result = model({inputs});\n for (auto& t : result.toTuple())\n {\n  // t is a Tensor now\n }\n ```\n \n So you utilise the same code path for all models, I recommend pushing all tensors to a vector. You can also interogate the result to find what type it is.",
      "y": "Call result.toTuple() instead of result.toTensor():\n ```C++\n auto result = model({inputs});\n for (auto& t : result.toTuple())\n {\n  // t is a Tensor now\n }\n ```\n \n Recommend to push all tensors to a vector. You can also interogate the result to find what type it is."
   },
   {
      "x": "Deterministic indexing operation fails in indices size check / missing broadcast",
      "z": "Too old `setuptools` version, the `.major` attribute came in in `49.6.0` (says https://github.com/spyder-ide/spyder/pull/15886).\n \n \n \n @andfoy that seems too recent to require, and distros may not have it. I think you need to change that in a way so it works with any `setuptools` version.\n \n \n \n FYI @ezyang who landed this - it's going to break elsewhere too.",
      "y": "Old `setuptools` version, the `.major` attribute came in in `49.6.0` (says https://github.com/spyder-ide/spyder/pull/15886)."
   },
   {
      "x": "How to get in touch about a security issue?",
      "z": "Reach out to @jspisak",
      "y": "If you believe you have found a security vulnerability in PyTorch, we encourage you to let us know right away. We will investigate all legitimate reports and do our best to quickly fix the problem.\n \n Please refer to the following page for our responsible disclosure policy, reward guidelines, and those things that should not be reported:\n \n https://www.facebook.com/whitehat"
   },
   {
      "x": "Help, nn.functional.interpolate make no sense at image resizing",
      "z": "I suspect you want to use https://pytorch.org/vision/stable/transforms.html#torchvision.transforms.Resize to resize images.\n \n And please use https://discuss.pytorch.org/ to ask questions about PyTorch APIs\n \n Do not hesitate to open a new issue if you believe that torch.nn.functional.interpolate does not work as documented.",
      "y": "Use torchvision.transforms.Resize to resize images"
   },
   {
      "x": "TensorPipe: build error: 'CU_POINTER_ATTRIBUTE_DEVICE_ORDINAL' was not declared in this scope",
      "z": "So in truth we're not exactly tracking what version is the strict minimum supported, but for certain we don't support anything older than 9.2, because in https://github.com/pytorch/pytorch/pull/36848 we removed workaround for pre-9.2. Hence I'm in fact updating that error message you pointed to in https://github.com/pytorch/pytorch/pull/61462. I hope you managed to get CUDA updated...",
      "y": "Upgrade CUDA"
   },
   {
      "x": "BN+ReLU cause \"RuntimeError: MALFORMED INPUT: bad dtype in CompareSelect\" error in fp16, traced module",
      "z": "This looks like a bug in the current fuser. While we're looking at it, you could disable it by adding\n \n torch._C._jit_override_can_fuse_on_gpu(False)",
      "y": "Smaller reproducer that doesn't require CUDA \n \n import torch\n import torch._C._te as te\n \n input_str = \"\"\"\n graph(%x : Half(3, 3, strides=[3, 1], requires_grad=0, device=cpu),\n  %weight : Half(3, strides=[1], requires_grad=0, device=cpu),\n  %bias : Half(3, strides=[1], requires_grad=0, device=cpu),\n  %running_mean : Half(3, strides=[1], requires_grad=0, device=cpu),\n  %running_var : Half(3, strides=[1], requires_grad=0, device=cpu)):\n  %5 : bool = prim::Constant[value=1]()\n  %6 : float = prim::Constant[value=0.001]()\n  %7 : float = prim::Constant[value=0.10000000000000001]()\n  %8 : bool = prim::Constant[value=0]()\n  %input.1 : Half(3, 3, strides=[3, 1], requires_grad=0, device=cpu) = aten::batch_norm(%x, %weight, %bias, %running_mean, %running_var, %8, %7, %6, %5)\n  %10 : Half(3, 3, strides=[3, 1], requires_grad=0, device=cpu) = aten::relu(%input.1)\n  return (%10)\n \"\"\"\n \n \n class kernel_arena_scope(object):\n  def __enter__(self):\n  self.scope = torch._C._te.KernelScope()\n \n  def __exit__(self, typ, val, traceback):\n  self.scope = None\n \n with kernel_arena_scope():\n  graph = torch._C.parse_ir(input_str)\n  print(graph)\n  kernel = te.TensorExprKernel(graph) # Fails\n  print(kernel.get_code_text(\"asm\"))"
   },
   {
      "x": "mean' reduction result in CrossEntropyLoss mismatches with manually computing mean",
      "z": "This isn't a bug, but the documentation isn't clear -- you'd only find the explanation if you look at the documentation for NLLLoss.\n \n \n \n See https://github.com/pytorch/pytorch/issues/31295 for the case in NLLLoss.",
      "y": "This isn't a bug, look at the documentation for NLLLoss.\n \n See https://github.com/pytorch/pytorch/issues/31295 for the case in NLLLoss."
   },
   {
      "x": "forward compatibility was attempted on non supported HW",
      "z": "Just reboot or reset nvidia drivers.\n \n <https://stackoverflow.com/a/45319156/1391392>",
      "y": "Reboot or reset nvidia drivers.\n <https://stackoverflow.com/a/45319156/1391392>"
   },
   {
      "x": "torch.cuda.is_available() is False",
      "z": "> Is CUDA available: Yes\n \n \n \n But when you run `python -m torch.utils.collect_env`, it reports that `torch.cuda.is_available()` is True. If you are asking why `torch.distributed.is_available()` is always `False` on Windows, then it is because it is not supported yet (see https://github.com/pytorch/pytorch/issues/37068).",
      "y": "`torch.distributed.is_available()` is always `False` on Windows, it is because it is not supported yet (see https://github.com/pytorch/pytorch/issues/37068)."
   },
   {
      "x": "Using all_gather() in the forward pass in DDP throws RuntimeError",
      "z": "Hey @Ze-Yang, setting `find_unused_parameters=True` in DDP ctor would avoid the error, but I don't think this is what you want. IIUC, the root cause is that `dist.all_gather` is not an autograd function, so that all operations prior to `all_gather` is not linked to the `out` tensor in the autograd graph. As a result, DDP would treat those tensors as unused parameters.\n \n \n \n To address this problem, you can either implement an autograd function for `dist.all_gather` (see [this example](https://github.com/pytorch/pytorch/blob/b35cdc5200af963e410c0a25400fd07f30b89bca/torch/nn/parallel/_functions.py) for scatter and gather) or try if [RPC and distributed autograd](https://pytorch.org/docs/master/rpc.html) (have to use master or v1.6 release cut) can handle it for you. From performance's perspective, the former option might be better.",
      "y": "To address this problem, you can either implement an autograd function for `dist.all_gather` (see [this example](https://github.com/pytorch/pytorch/blob/b35cdc5200af963e410c0a25400fd07f30b89bca/torch/nn/parallel/_functions.py) for scatter and gather) or try if [RPC and distributed autograd](https://pytorch.org/docs/master/rpc.html) (have to use master or v1.6 release cut) can handle it for you."
   },
   {
      "x": "Implement autograd functions for c10d communication operations",
      "z": "That would be awesome if we can generalize autograd functions for c10d operations. TSC folks are manually implementing backward grad propagation for alltoall and reduce_scatter",
      "y": "implemented this in chainermn"
   },
   {
      "x": "pca_lowrank memory allocation",
      "z": "> A simple solution that worked for me is changing the above (and two other similar lines) line to:\n \n > \n \n > ```\n \n > (Q, _) = matmul(A_H-M_H, Q).qr()\n \n > ```\n \n \n \n This would be even worse because this would generate more large temporary tensors (`A_H - M_H`) during the iteration.\n \n Notice that `Q` has small number of columns so that `matmul(A_H, Q) - matmul(M_H, Q)` is very memory efficient compared to `matmul(A_H - M_H, Q)`.",
      "y": "matmul doesn't support broadcasting the singleton dimension \n Define M = C.as_strided(A.shape, (0, 1)) and then use _svd_lowrank(A, q, niter=niter, M=M)."
   },
   {
      "x": "How to do matrix multiplication between two 2D sparse directly and quickly",
      "z": "I found the TensorFlow has this function:\n \n tf.sparse_matmul(\n \n  a,\n \n  b,\n \n  transpose_a=False,\n \n  transpose_b=False,\n \n  a_is_sparse=False,\n \n  b_is_sparse=False,\n \n  name=None\n \n )\n \n I sincerely hope that PyTorch also has it!",
      "y": "Look for similar function in pytorch \n tf.sparse_matmul(\n  a,\n  b,\n  transpose_a=False,\n  transpose_b=False,\n  a_is_sparse=False,\n  b_is_sparse=False,\n  name=None\n )"
   },
   {
      "x": "Unable to use Pytorch with CUDA",
      "z": "Duplicate #20635 \n \n \n \n You can add `torch.cuda.current_device()` after `import torch` and it should fix the issue temporarily",
      "y": "add `torch.cuda.current_device()` after `import torch` and it should fix the issue"
   },
   {
      "x": "CPU torch.norm gives strange results for LargeTensor on Colab",
      "z": "This is a related to https://github.com/pytorch/pytorch/issues/20551",
      "y": "PyTorch 0.4 did the accumulation using double https://github.com/pytorch/pytorch/blob/v0.4.1/aten/src/TH/generic/THTensorMath.cpp#L4307\n \n Now it's using float accumulation:\n https://github.com/pytorch/pytorch/blob/master/aten/src/ATen/native/cpu/ReduceOpsKernel.cpp#L57\n \n CUDA uses float accumulation, but is saved because the necessary parallelism forces a form of pairwise summation. We should probably do the same thing for CPU"
   },
   {
      "x": "pytorch won't install from wheel",
      "z": "I'm experiencing the same issue, except with ```torch==1.7.1+cu110```.",
      "y": "workaround works, but it's very portable...\n wget https://download.pytorch.org/whl/cpu/torch-1.6.0%2Bcpu-cp37-cp37m-linux_x86_64.whl\n pip install torch-1.6.0+cpu-cp37-cp37m-linux_x86_64.whl \n Or use ```torch==1.7.1+cu110```."
   },
   {
      "x": "cannot import name 'ngrams_iterator'",
      "z": "I think this should be reported in https://github.com/pytorch/text .\n \n By the way you may want to fill the issue template and provide as much as information as possible, otherwise no one can simply reproduce your problem.",
      "y": "report to https://github.com/pytorch/tex"
   },
   {
      "x": "PyPI is slow please create git releases",
      "z": "Releases are already hosted on S3 you can find installation instructions that relate to https://download.pytorch.org on https://pytorch.org/get-started\n \n \n \n Example:\n \n \n \n ```\n \n pip install -f https://download.pytorch.org/whl/cu102/torch_stable.html torch\n \n ```",
      "y": "Releases are already hosted on S3 you can find installation instructions that relate to https://download.pytorch.org on https://pytorch.org/get-started\n \n \n \n Example:\n \n \n \n ```\n \n pip install -f https://download.pytorch.org/whl/cu102/torch_stable.html torch\n \n ```"
   },
   {
      "x": "Tensor print format issue on Nvidia Jetson devices",
      "z": "Tried the master branch code on Jetson (GCC7), and the results are correct. \n \n \n \n Compared to v1.7.1, a major change in the master branch is that vec256_float_neon.h has a new compiler check and only compiles for clang and GCC>8.3. So, for GCC7, the code that causes this issue is skipped.\n \n \n \n So this issue is kind of already fixed in the master branch ... by essentially disabling neon for Jetson.",
      "y": "disable neon for Jetson"
   },
   {
      "x": "Roll-up: remaining TH functions",
      "z": "LegacyFunctionsCPU is now gone, as of #58780.",
      "y": "LegacyFunctionsCPU is no longer available"
   },
   {
      "x": "type_as() method change device too",
      "z": "Because `type()` is like `torch.cuda.FloatTensor`. Which contains both the device and dtype.\n For backward compatibility, this function supports all of them. But the new `.to()` API should remove that confusion by only accepting clear arguments.",
      "y": "`type()` is like `torch.cuda.FloatTensor` which contains both the device and dtype."
   },
   {
      "x": "Error instaling using PIP and CUDA on windows 10",
      "z": "Please use 64-bit Python instead.",
      "y": "Please use 64-bit Python instead."
   },
   {
      "x": "torch.tril_indices returns a float tensor on master",
      "z": "Yes specifying the `dtype` works",
      "y": "specify `dtype` to make it work"
   },
   {
      "x": "Failed to build pytorch 1.4.0 on Ubuntu 18.04.4 LTS",
      "z": "The issue is with the system-wide pybind11-dev package (Version: 2.0.1-4). After removing it pytorch builds just fine.",
      "y": "The issue is with the system-wide pybind11-dev package (Version: 2.0.1-4). After removing it pytorch builds just fine."
   },
   {
      "x": "Complex number printing inconsistent with float",
      "z": "@anjali411,\n \n \n \n In the context to pretty printing multiple complex values it would be beneficial to keep the same formatting for every entry in the tensor. It would be nice to have the following:\n \n 1. `, ` should line up across multiple lines of output.\n \n 2. `+/-` should line up across multiple lines of output.\n \n 3. When the output is not abbreviated with `...` for length, it should be possible to copy the output string into an `eval(str)` and convert the number back into a python list or pytorch tensor.",
      "y": "In the context to pretty printing multiple complex values it would be beneficial to keep the same formatting for every entry in the tensor. It would be nice to have the following:\n 1. `, ` should line up across multiple lines of output.\n 2. `+/-` should line up across multiple lines of output.\n 3. When the output is not abbreviated with `...` for length, it should be possible to copy the output string into an `eval(str)` and convert the number back into a python list or pytorch tensor."
   },
   {
      "x": "[Complex] Incorrect Complex Tensor inference",
      "z": "I think I've run into this issue before. There needed to be a way to specify a number in another number system, so `j` is a suffix to the number 2, not a variable. See the following code snippet.\n \n ``` Python\n j = 10\n c_num = eval(\"1/2j\") # c_num = -0.5j (j is the suffix of 2)\n c_num = eval(\"(1/2)*j\") #c_num = 5 (j is a variable = 10)\n ```\n \n Python is an interpreted language so it would be very difficult interpret` j` as a suffix of `1/2` without breaking the case where `j` is a variable. I think the solution is to change hypothesis to the following:\n \n ``` Python\n j = 10\n c_num = eval(\"1j/2\") # c_num = 0.5j (j is the suffix of 1)\n ```\n \n It's confusing, but this method allows people to use a variable named `j`.",
      "y": "There needed to be a way to specify a number in another number system, so `j` is a suffix to the number 2, not a variable. See the following code snippet.\n \n ``` Python\n j = 10\n c_num = eval(\"1/2j\") # c_num = -0.5j (j is the suffix of 2)\n c_num = eval(\"(1/2)*j\") #c_num = 5 (j is a variable = 10)\n ```\n \n Python is an interpreted language so it would be very difficult interpret` j` as a suffix of `1/2` without breaking the case where `j` is a variable. I think the solution is to change hypothesis to the following:\n \n ``` Python\n j = 10\n c_num = eval(\"1j/2\") # c_num = 0.5j (j is the suffix of 1)\n ```\n \n It's confusing, but this method allows people to use a variable named `j`."
   },
   {
      "x": "[feature request] random integer generator",
      "z": "@Naman-ntc I recommend you to read the blogs on pytorch.org to understand the building system and the `ATEN` document.",
      "y": "read the blogs on pytorch.org to understand the building system and the `ATEN` document."
   },
   {
      "x": "Error in Function backward when forward output is in-place of forward input and input is not leaf",
      "z": "There are a few problems with your implementation. Most notably, you fail to use `ctx.mark_dirty` to signal that you've modified `a`. Secondly, you should use `.view(1)` instead of `.resize(1)`, because the later results in the output having **unspecified** contents. Finally, I tried reproducing your issues on `master`, but it seems to work just fine, so I'm closing the issue as resolved. Thanks for the report!",
      "y": "use `.view(1)` instead of `.resize(1)`, because the later results in the output having **unspecified** contents"
   },
   {
      "x": "[BUG?]The same code behaves differently on pytorch based on py2 and py3 whose version are both 0.3.0.post_4",
      "z": "from a rough scan, this looks like that it might be the culprit:\n \n ```\n \n  scale = math.sqrt(3 / fan_in)\n \n ```\n \n in py2, `int / int` is an `int`; in py3, it is a `float`.",
      "y": "from a rough scan, this looks like that it might be the culprit:\n \n ```\n \n  scale = math.sqrt(3 / fan_in)\n \n ```\n \n in py2, `int / int` is an `int`; in py3, it is a `float`."
   },
   {
      "x": "\"torch.sum()\" over a ByteTensor gives incorrect result when the \"dim\" is specified",
      "z": "This has been fixed in 0.4. Now reductions on byte tensors return the result as a long tensor (except if the explicit `dtype` is passed)",
      "y": "This has been fixed in 0.4. Now reductions on byte tensors return the result as a long tensor (except if the explicit `dtype` is passed)"
   },
   {
      "x": "Support F.normalize on 1-dim tensors without explicit dim",
      "z": "`normalize` already works on 1-dim, it's just that default `dim` is off for 1-dim tensors. making `dim = 0` or `dim=-1` for 1-dim tensors would solve this particular case.\n \n \n \n IMO , it makes less confusion when things work on 1-dim (good for quick testing purposes) by default compared to when they don't, even if such support is added gradually (like it happened for negative dims), it saves boiler-plate.",
      "y": "`normalize` already works on 1-dim, it's just that default `dim` is off for 1-dim tensors. making `dim = 0` or `dim=-1` for 1-dim tensors would solve this particular case."
   },
   {
      "x": "Add support similar to `tf.images.resize_images`.",
      "z": "this should be done, indeed it's much more elegant to have a differentiable resize_images, and we can likely build it on top of `grid_sample` that we have.",
      "y": "We now have nn.functional.interpolate that does the same thing except for bicubic method. https://pytorch.org/docs/master/nn.html?highlight=interpolate#torch.nn.functional.interpolate"
   },
   {
      "x": "memory leaky on DataLoader",
      "z": "The reason in your last snippet is that the loss is still in scope when you do the testing loop, so it holds up the whole computation graph. You can either delete it before the testing starts `del loss` and `del classes`, or put the train loop in a separate function (and thus the scope ends with the function)",
      "y": "loss is still in scope when you do the testing loop, so it holds up the whole computation graph. You can either delete it before the testing starts `del loss` and `del classes`, or put the train loop in a separate function (and thus the scope ends with the function)"
   },
   {
      "x": "Distributed communication never frees target memory",
      "z": "Hi, thanks for the clarification. With the latest sources from master, it works for me as well (while 0.3.1 still fails). Seems to be solved, thanks! :+1:",
      "y": "suggest switching to a different backend (e.g. gloo or MPI), because TCP is really nearly a debug-mode thing and will likely not get anywhere close to peak performance."
   },
   {
      "x": "KLDivLoss behaves differently on CPU/GPU",
      "z": "Thanks for the report, @yuandong-tian and the confirmation @reynoldscem. I think I found the bug. Fix incoming.",
      "y": "Bug Fixed"
   },
   {
      "x": "nccl2 backend distributed package",
      "z": "You have to get a nccl-dev package e.g. from here http://developer.download.nvidia.com/compute/machine-learning/repos/ubuntu1604/x86_64",
      "y": "get a nccl-dev package e.g. from here http://developer.download.nvidia.com/compute/machine-learning/repos/ubuntu1604/x86_64"
   },
   {
      "x": "torch.poisson does not work with FloatTensor on master",
      "z": "I think I found the bug. Fix incoming soon.",
      "y": "Bug Fixed"
   },
   {
      "x": "[feature request] same behavior in multi-gpu DataParallel vs single GPU",
      "z": "Your criterion call is outside the data parallel. The size_average divides by the *full* batch size. There's still no dependence on the number of GPUs:\n \n \n \n ```python\n \n import torch\n \n import torch.nn as nn\n \n from torch.autograd import Variable\n \n input = Variable(torch.randn(8, 10).cuda())\n \n target = Variable(torch.ones(8).long().cuda())\n \n criterion = nn.CrossEntropyLoss().cuda()\n \n \n \n linear = nn.Linear(10, 10).cuda()\n \n dp = nn.DataParallel(linear, [0, 1])\n \n \n \n criterion(linear(input), target).backward()\n \n print(linear.weight.grad)\n \n \n \n dp.zero_grad()\n \n criterion(dp(input), target).backward()\n \n print(linear.weight.grad)\n \n ```",
      "y": "Criiterion call is outside the data parallel. The size_average divides by the *full* batch size. There's still no dependence on the number of GPUs:\n \n ```python\n import torch\n import torch.nn as nn\n from torch.autograd import Variable\n input = Variable(torch.randn(8, 10).cuda())\n target = Variable(torch.ones(8).long().cuda())\n criterion = nn.CrossEntropyLoss().cuda()\n \n linear = nn.Linear(10, 10).cuda()\n dp = nn.DataParallel(linear, [0, 1])\n \n criterion(linear(input), target).backward()\n print(linear.weight.grad)\n \n dp.zero_grad()\n criterion(dp(input), target).backward()\n print(linear.weight.grad)\n ```"
   },
   {
      "x": "Assigning to an index of a sparse tensor is a no-op",
      "z": "FWIW, scipy does not allow assigning to a COO sparse matrix, and will complain when assigning to a CSR one:\n \n ```\n \n >>> m = scipy.sparse.rand(3, 4, density=0.25)\n \n >>> m\n \n <3x4 sparse matrix of type '<class 'numpy.float64'>'\n \n with 3 stored elements in COOrdinate format>\n \n >>> m[0,0] = 100\n \n Traceback (most recent call last):\n \n  File \"<stdin>\", line 1, in <module>\n \n TypeError: 'coo_matrix' object does not support item assignment\n \n \n \n >>> m = scipy.sparse.rand(3, 4, density=0.25, format='csr')\n \n >>> m[0,0] = 100\n \n .../python3.8/site-packages/scipy/sparse/_index.py:82: \\\n \n  SparseEfficiencyWarning: Changing the sparsity structure of a csr_matrix is expensive. \\\n \n  lil_matrix is more efficient.\n \n  self._set_intXint(row, col, x.flat[0])\n \n \n \n >>> m.todense()\n \n matrix([[100. , 0. , 0. , 0.2128684 ],\n \n  [ 0. , 0. , 0. , 0.93232551],\n \n  [ 0. , 0.98662986, 0. , 0. ]])\n \n ```",
      "y": "scipy does not allow assigning to a COO sparse matrix, and will complain when assigning to a CSR one:\n ```\n >>> m = scipy.sparse.rand(3, 4, density=0.25)\n >>> m\n <3x4 sparse matrix of type '<class 'numpy.float64'>'\n with 3 stored elements in COOrdinate format>\n >>> m[0,0] = 100\n Traceback (most recent call last):\n  File \"<stdin>\", line 1, in <module>\n TypeError: 'coo_matrix' object does not support item assignment\n \n >>> m = scipy.sparse.rand(3, 4, density=0.25, format='csr')\n >>> m[0,0] = 100\n .../python3.8/site-packages/scipy/sparse/_index.py:82: \\\n  SparseEfficiencyWarning: Changing the sparsity structure of a csr_matrix is expensive. \\\n  lil_matrix is more efficient.\n  self._set_intXint(row, col, x.flat[0])\n \n >>> m.todense()\n matrix([[100. , 0. , 0. , 0.2128684 ],\n  [ 0. , 0. , 0. , 0.93232551],\n  [ 0. , 0.98662986, 0. , 0. ]])\n ```"
   },
   {
      "x": "@torch.jit.script causes compilation error on Ampere architecture (RTX 3090)",
      "z": "CUDA 11.0 supports sm_80 but nvrtc shipped with it does not... I can try to clamp it on the right side to compute_75. :crossed_fingers:",
      "y": "A look at the documentation of nvrtc included with CUDA Toolkit 11.0 reveals that it indeed does not support the Ampere architecture (compute_80). This support is introduced in CUDA Toolkit 11.1.\n \n Unfortunately, the conda repositories do not contain builds of PyTorch against CUDA 11.1."
   },
   {
      "x": "Errors in Eigen library when building pytorch mobile for Android",
      "z": "I finally figured it out. The problem was in the Clang C++ version delivered with the Android NDK. I rolled back from 22.0.6917172 to 21.0.6113669 and now it works.",
      "y": "The problem is in the Clang C++ version delivered with the Android NDK. I rolled back from 22.0.6917172 to 21.0.6113669 and now it works."
   },
   {
      "x": "nn.utils.spectral_norm() does not perform normalization of the weight matrix.",
      "z": "It does power iteration at forward, which updates the parameters. If you compute again after forward, the matrices match:\n \n ```py\n \n In [1]: import torch\n \n  ...: print(torch.__version__)\n \n  ...:\n \n  ...: linear = torch.nn.Linear(3, 4)\n \n  ...: norm_layer = torch.nn.utils.spectral_norm(linear)\n \n  ...:\n \n  ...: print('Normalized weight matrix with spectral_norm(): ', norm_layer.weight)\n \n  ...: print('Original weight matrix: ', norm_layer.weight_orig)\n \n  ...:\n \n  ...: sigma = torch.dot(norm_layer.weight_u, torch.mv(norm_layer.weight_orig, norm_layer.weight_v))\n \n  ...: print('Normalized weight matrix by hands: ', norm_layer.weight_orig / sigma)\n \n 1.7.0a0+33e2665\n \n Normalized weight matrix with spectral_norm(): tensor([[ 0.4658, 0.3948, -0.1485],\n \n  [-0.3340, 0.4385, -0.2675],\n \n  [-0.2579, 0.4863, -0.2420],\n \n  [ 0.3940, 0.4005, -0.3857]])\n \n Original weight matrix: Parameter containing:\n \n tensor([[ 0.4658, 0.3948, -0.1485],\n \n  [-0.3340, 0.4385, -0.2675],\n \n  [-0.2579, 0.4863, -0.2420],\n \n  [ 0.3940, 0.4005, -0.3857]], requires_grad=True)\n \n Normalized weight matrix by hands: tensor([[-2.9618, -2.5104, 0.9443],\n \n  [ 2.1241, -2.7887, 1.7012],\n \n  [ 1.6401, -3.0922, 1.5386],\n \n  [-2.5056, -2.5471, 2.4527]], grad_fn=<DivBackward0>)\n \n \n \n In [2]: norm_layer(torch.randn((3)))\n \n Out[2]: tensor([-0.5175, 0.6595, 1.0445, 0.0289], grad_fn=<AddBackward0>)\n \n \n \n In [3]: print('Normalized weight matrix with spectral_norm(): ', norm_layer.weight)\n \n  ...: print('Original weight matrix: ', norm_layer.weight_orig)\n \n Normalized weight matrix with spectral_norm(): tensor([[ 0.4720, 0.4001, -0.1505],\n \n  [-0.3385, 0.4444, -0.2711],\n \n  [-0.2614, 0.4928, -0.2452],\n \n  [ 0.3993, 0.4059, -0.3909]], grad_fn=<DivBackward0>)\n \n Original weight matrix: Parameter containing:\n \n tensor([[ 0.4658, 0.3948, -0.1485],\n \n  [-0.3340, 0.4385, -0.2675],\n \n  [-0.2579, 0.4863, -0.2420],\n \n  [ 0.3940, 0.4005, -0.3857]], requires_grad=True)\n \n \n \n In [4]: sigma = torch.dot(norm_layer.weight_u, torch.mv(norm_layer.weight_orig, norm_layer.weight_v))\n \n  ...: print('Normalized weight matrix by hands: ', norm_layer.weight_orig / sigma)\n \n Normalized weight matrix by hands: tensor([[ 0.4720, 0.4001, -0.1505],\n \n  [-0.3385, 0.4444, -0.2711],\n \n  [-0.2614, 0.4928, -0.2452],\n \n  [ 0.3993, 0.4059, -0.3909]], grad_fn=<DivBackward0>)\n \n \n \n ```",
      "y": "It does power iteration at forward, which updates the parameters. If you compute again after forward, the matrices match:\n \n ```py\n \n In [1]: import torch\n \n  ...: print(torch.__version__)\n \n  ...:\n \n  ...: linear = torch.nn.Linear(3, 4)\n \n  ...: norm_layer = torch.nn.utils.spectral_norm(linear)\n \n  ...:\n \n  ...: print('Normalized weight matrix with spectral_norm(): ', norm_layer.weight)\n \n  ...: print('Original weight matrix: ', norm_layer.weight_orig)\n \n  ...:\n \n  ...: sigma = torch.dot(norm_layer.weight_u, torch.mv(norm_layer.weight_orig, norm_layer.weight_v))\n \n  ...: print('Normalized weight matrix by hands: ', norm_layer.weight_orig / sigma)\n \n 1.7.0a0+33e2665\n \n Normalized weight matrix with spectral_norm(): tensor([[ 0.4658, 0.3948, -0.1485],\n \n  [-0.3340, 0.4385, -0.2675],\n \n  [-0.2579, 0.4863, -0.2420],\n \n  [ 0.3940, 0.4005, -0.3857]])\n \n Original weight matrix: Parameter containing:\n \n tensor([[ 0.4658, 0.3948, -0.1485],\n \n  [-0.3340, 0.4385, -0.2675],\n \n  [-0.2579, 0.4863, -0.2420],\n \n  [ 0.3940, 0.4005, -0.3857]], requires_grad=True)\n \n Normalized weight matrix by hands: tensor([[-2.9618, -2.5104, 0.9443],\n \n  [ 2.1241, -2.7887, 1.7012],\n \n  [ 1.6401, -3.0922, 1.5386],\n \n  [-2.5056, -2.5471, 2.4527]], grad_fn=<DivBackward0>)\n \n \n \n In [2]: norm_layer(torch.randn((3)))\n \n Out[2]: tensor([-0.5175, 0.6595, 1.0445, 0.0289], grad_fn=<AddBackward0>)\n \n \n \n In [3]: print('Normalized weight matrix with spectral_norm(): ', norm_layer.weight)\n \n  ...: print('Original weight matrix: ', norm_layer.weight_orig)\n \n Normalized weight matrix with spectral_norm(): tensor([[ 0.4720, 0.4001, -0.1505],\n \n  [-0.3385, 0.4444, -0.2711],\n \n  [-0.2614, 0.4928, -0.2452],\n \n  [ 0.3993, 0.4059, -0.3909]], grad_fn=<DivBackward0>)\n \n Original weight matrix: Parameter containing:\n \n tensor([[ 0.4658, 0.3948, -0.1485],\n \n  [-0.3340, 0.4385, -0.2675],\n \n  [-0.2579, 0.4863, -0.2420],\n \n  [ 0.3940, 0.4005, -0.3857]], requires_grad=True)\n \n \n \n In [4]: sigma = torch.dot(norm_layer.weight_u, torch.mv(norm_layer.weight_orig, norm_layer.weight_v))\n \n  ...: print('Normalized weight matrix by hands: ', norm_layer.weight_orig / sigma)\n \n Normalized weight matrix by hands: tensor([[ 0.4720, 0.4001, -0.1505],\n \n  [-0.3385, 0.4444, -0.2711],\n \n  [-0.2614, 0.4928, -0.2452],\n \n  [ 0.3993, 0.4059, -0.3909]], grad_fn=<DivBackward0>)\n \n \n \n ```"
   },
   {
      "x": "Enable PyTorch compilation on Apple Silicon",
      "z": "Hey guys, I successfully built a pytorch1.8.0a0 on my macbook air with m1 chip~~\n \n \n \n The python version I'm using is python 3.9.1 which is installed by [conda-forge](https://github.com/conda-forge/miniforge).\n \n \n \n ### 1. Building Guide:\n \n \n \n 1. fix deps:\n \n \n \n `conda install setuptools cffi typing_extensions future six requests dataclasses pkg-config libuv`\n \n I didn't install cmake because the system itself is shipped already with a cmake. You can also `conda install cmake`.\n \n \n \n \n \n 2. clone pytorch repo and build:\n \n \n \n First:\n \n  ```shell\n \n  git clone --recursive https://github.com/pytorch/pytorch\n \n  cd pytorch\n \n  ```\n \n  Then I modified two lines of the CMakeLists.txt in the pytorch git directory:\n \n  (1) `option(USE_OPENMP \"Use OpenMP for parallel code\" ON)` to `option(USE_OPENMP \"Use OpenMP for parallel code\" OFF)`\n \n  (2) `USE_MKLDNN \"Use MKLDNN. Only available on x86 and x86_64.\" ON` to `USE_MKLDNN \"Use MKLDNN. Only available on x86 and x86_64.\" OFF` \n \n \n \n Then:\n \n ```shell\n \n export CMAKE_PREFIX_PATH=${CONDA_PREFIX:-\"$(dirname $(which conda))/../\"}\n \n MACOSX_DEPLOYMENT_TARGET=11.1 CC=clang CXX=clang++ python setup.py install\n \n  ```\n \n make sure here `python` is pointed to the version installed by conda-forge. \n \n \n \n ### 2. Install from whl file\n \n \n \n I tried to build a wheel file, the download link is [torch-1.8.0a0-cp39-cp39-macosx_11_0_arm64](https://github.com/wizyoung/AppleSiliconSelfBuilds/blob/main/builds/torch-1.8.0a0-cp39-cp39-macosx_11_0_arm64.whl), you may try to install directly from it!\n \n \n \n ### 3. Speed test\n \n \n \n From the comment of https://github.com/pytorch/pytorch/issues/48145#issuecomment-730297957 we can do a simple benchmark:\n \n \n \n > The following code shows roughly 46it/s running on a MacBook Air with M1 chip, I'm literally impressed by the performance of M1:\n \n > \n \n > ```\n \n > from tqdm import tqdm\n \n > import torch\n \n > \n \n > @torch.jit.script\n \n > def foo():\n \n > x = torch.ones((1024 * 12, 1024 * 12), dtype=torch.float32)\n \n > y = torch.ones((1024 * 12, 1024 * 12), dtype=torch.float32)\n \n > z = x + y\n \n > return z\n \n > \n \n > \n \n > if __name__ == '__main__':\n \n > z0 = None\n \n > for _ in tqdm(range(10000000000)):\n \n > zz = foo()\n \n > if z0 is None:\n \n > z0 = zz\n \n > else:\n \n > z0 += zz\n \n > ```\n \n > \n \n > The Nvidia 3090 with the above code shows 670it/s. So 6.8% of 3090 running just on CPU, not bad!\n \n > Also the 3900x cpu shows just 21it/s.\n \n > \n \n > Hope gets GPU and neural engine support soon.\n \n \n \n On my macbook air, the speed is around 44~45 it/s. \n \n \n \n Update: The speed on my 2018 late Mac mini speed is 20 ~ 21 it/s.\n \n Update2: Speed on TITAN XP GPU: ~320 it/s",
      "y": "I successfully built a pytorch1.8.0a0 on my macbook air with m1 chip~~\n \n The python version I'm using is python 3.9.1 which is installed by [conda-forge](https://github.com/conda-forge/miniforge).\n \n ### 1. Building Guide:\n \n 1. fix deps:\n \n `conda install setuptools cffi typing_extensions future six requests dataclasses pkg-config libuv`\n I didn't install cmake because the system itself is shipped already with a cmake. You can also `conda install cmake`.\n \n \n 2. clone pytorch repo and build:\n \n First:\n  ```shell\n  git clone --recursive https://github.com/pytorch/pytorch\n  cd pytorch\n  ```\n  Then I modified two lines of the CMakeLists.txt in the pytorch git directory:\n  (1) `option(USE_OPENMP \"Use OpenMP for parallel code\" ON)` to `option(USE_OPENMP \"Use OpenMP for parallel code\" OFF)`\n  (2) `USE_MKLDNN \"Use MKLDNN. Only available on x86 and x86_64.\" ON` to `USE_MKLDNN \"Use MKLDNN. Only available on x86 and x86_64.\" OFF` \n \n Then:\n ```shell\n export CMAKE_PREFIX_PATH=${CONDA_PREFIX:-\"$(dirname $(which conda))/../\"}\n MACOSX_DEPLOYMENT_TARGET=11.1 CC=clang CXX=clang++ python setup.py install\n  ```\n make sure here `python` is pointed to the version installed by conda-forge. \n \n ### 2. Install from whl file\n \n I tried to build a wheel file, the download link is [torch-1.8.0a0-cp39-cp39-macosx_11_0_arm64](https://github.com/wizyoung/AppleSiliconSelfBuilds/blob/main/builds/torch-1.8.0a0-cp39-cp39-macosx_11_0_arm64.whl), you may try to install directly from it!\n \n ### 3. Speed test\n \n From the comment of https://github.com/pytorch/pytorch/issues/48145#issuecomment-730297957 we can do a simple benchmark:\n \n > The following code shows roughly 46it/s running on a MacBook Air with M1 chip, I'm literally impressed by the performance of M1:\n > \n > ```\n > from tqdm import tqdm\n > import torch\n > \n > @torch.jit.script\n > def foo():\n > x = torch.ones((1024 * 12, 1024 * 12), dtype=torch.float32)\n > y = torch.ones((1024 * 12, 1024 * 12), dtype=torch.float32)\n > z = x + y\n > return z\n > \n > \n > if __name__ == '__main__':\n > z0 = None\n > for _ in tqdm(range(10000000000)):\n > zz = foo()\n > if z0 is None:\n > z0 = zz\n > else:\n > z0 += zz\n > ```\n > \n > The Nvidia 3090 with the above code shows 670it/s. So 6.8% of 3090 running just on CPU, not bad!\n > Also the 3900x cpu shows just 21it/s.\n > \n > Hope gets GPU and neural engine support soon.\n \n On my macbook air, the speed is around 44~45 it/s. \n \n Update: The speed on my 2018 late Mac mini speed is 20 ~ 21 it/s.\n Update2: Speed on TITAN XP GPU: ~320 it/s"
   },
   {
      "x": "The `normalize` function failed in the TorchScript interpreter with AMP enabled.",
      "z": "Based on https://github.com/pytorch/pytorch/issues/38958#issuecomment-635472379:\n \n \"autocast interaction with jit scripting is very much WIP\".\n \n \n \n > Best recommendation right now is, don't run a scripted model under autocast. If you must use autocast and jit together, try tracing instead. Run the model under autocast at trace time. The tracing will include the eager casting decisions autocast makes, and casts will be baked into the resulting jitted module.\n \n \n \n Given your code snippet, running the method in `autocast` and tracing seems to work in the latest nightly:\n \n ```python\n \n def my_norm(x: Tensor):\n \n  with autocast():\n \n  return F.normalize(x, p=2.)\n \n \n \n device = torch.device('cuda:0')\n \n scripted_norm = torch.jit.trace(my_norm, torch.rand((32, 32), device=device))\n \n scripted_norm(torch.rand((32, 32), device=device))\n \n ```\n \n Note that `F.normalize` is an op that can autocast to `float32` as described [here](https://pytorch.org/docs/stable/amp.html#op-eligibility).",
      "y": "Based on https://github.com/pytorch/pytorch/issues/38958#issuecomment-635472379:\n \n \"autocast interaction with jit scripting is very much WIP\".\n \n \n \n > Best recommendation right now is, don't run a scripted model under autocast. If you must use autocast and jit together, try tracing instead. Run the model under autocast at trace time. The tracing will include the eager casting decisions autocast makes, and casts will be baked into the resulting jitted module.\n \n \n \n Given your code snippet, running the method in `autocast` and tracing seems to work in the latest nightly:\n \n ```python\n \n def my_norm(x: Tensor):\n \n  with autocast():\n \n  return F.normalize(x, p=2.)\n \n \n \n device = torch.device('cuda:0')\n \n scripted_norm = torch.jit.trace(my_norm, torch.rand((32, 32), device=device))\n \n scripted_norm(torch.rand((32, 32), device=device))\n \n ```\n \n Note that `F.normalize` is an op that can autocast to `float32` as described [here](https://pytorch.org/docs/stable/amp.html#op-eligibility)."
   },
   {
      "x": "[feature request] consistent default values for leaky_relu",
      "z": "`1e-2` is 0.01. So the default values do match.",
      "y": "1e-2 is 0.01"
   },
   {
      "x": "Segmentation fault while training",
      "z": "FYI I've had the same issue via an older horovod Dockerfile setup with the following docker config:\n \n ```\n \n FROM nvidia/cuda:9.0-devel-ubuntu16.04\n \n ENV PYTORCH_VERSION=0.4.1\n \n ENV CUDNN_VERSION=7.3.1.20-1+cuda9.0\n \n ENV NCCL_VERSION=2.3.5-2+cuda9.0\n \n ENV HOROVOD_VERSION=0.15.2\n \n ```\n \n with `openmpi-3.1.2`. \n \n \n \n Moving to \n \n ```\n \n ENV PYTORCH_VERSION=1.0.0\n \n ENV CUDNN_VERSION=7.4.1.5-1+cuda9.0\n \n ```\n \n seems to have fixed this issue.",
      "y": "FYI I've had the same issue via an older horovod Dockerfile setup with the following docker config:\n \n ```\n \n FROM nvidia/cuda:9.0-devel-ubuntu16.04\n \n ENV PYTORCH_VERSION=0.4.1\n \n ENV CUDNN_VERSION=7.3.1.20-1+cuda9.0\n \n ENV NCCL_VERSION=2.3.5-2+cuda9.0\n \n ENV HOROVOD_VERSION=0.15.2\n \n ```\n \n with `openmpi-3.1.2`. \n \n \n \n Moving to \n \n ```\n \n ENV PYTORCH_VERSION=1.0.0\n \n ENV CUDNN_VERSION=7.4.1.5-1+cuda9.0\n \n ```\n \n seems to have fixed this issue."
   },
   {
      "x": "Windows builds with CUDA 9.2",
      "z": "Update: CUDA 9.2 is added into our Windows AMI. We might want to consider adding a CUDA 9.2 Windows CI, or changing current CI to CUDA 9.2.",
      "y": "CUDA 9.2 is added into our Windows AMI"
   },
   {
      "x": "[Feature Request] Inverse Hyperbolic Functions",
      "z": "A more numerically stable atanh one-liner is `torch.log1p(2*x/(1-x)) / 2` \n \n \n \n From https://www.plunk.org/~hatch/rightway.php",
      "y": "A more numerically stable atanh one-liner is `torch.log1p(2*x/(1-x)) / 2` \n \n \n \n From https://www.plunk.org/~hatch/rightway.php"
   },
   {
      "x": "cuda runtime error(59): device-side assert when running torch.topk",
      "z": "I kind of had the same problem. I was trying to modify the transfer learning tutorial (https://pytorch.org/tutorials/beginner/transfer_learning_tutorial.html) with the hymenoptera dataset (bees and ants dataset) to make it fit to my project. The problem was that i had 3 classes and the tutorial has 2. So i found that you have to define the number of outputs from the fully connected network (the output layer). So the only thing i had to do was changing \"model_ft.fc = nn.Linear(num_ftrs, 2)\" to \"model_ft.fc = nn.Linear(num_ftrs, 3)\". Hope it helps somebody :)",
      "y": "define the number of outputs from the fully connected network (the output layer). So the only thing i had to do was changing \"model_ft.fc = nn.Linear(num_ftrs, 2)\" to \"model_ft.fc = nn.Linear(num_ftrs, 3)"
   },
   {
      "x": "Disabling MPI fails, unable to build on Centos7",
      "z": "USE_MPI is not well defined for `python setup.py install`.\n \n \n \n Here's what you need:\n \n \n \n `USE_DISTRIBUTED=0 python setup.py install`",
      "y": "USE_MPI is not well defined for `python setup.py install`.\n \n \n \n Here's what you need:\n \n \n \n `USE_DISTRIBUTED=0 python setup.py install`"
   },
   {
      "x": "[feature request] Matrix rank",
      "z": "@vishwakftw I think so :). We have all ingredients for `np.linalg_matrix_rank`. The `hermitian=True` case can use `symeig`.",
      "y": "We have all ingredients for `np.linalg_matrix_rank`. The `hermitian=True` case can use `symeig`."
   },
   {
      "x": "Initialization can be surprisingly important, and under-rated",
      "z": "Just want to point out that the initialization schemes are documented by @vishwakftw at https://github.com/pytorch/pytorch/pull/9038 . Thanks @vishwakftw !",
      "y": "See https://github.com/pytorch/pytorch/pull/9038"
   },
   {
      "x": "libtorch.so.1: undefined symbol:",
      "z": "export LD_LIBRARY_PATH=/usr/local/cuda-9.0/lib64\n This helps me.",
      "y": "export LD_LIBRARY_PATH=/usr/local/cuda-9.0/lib64"
   },
   {
      "x": "a strange torch.no_grad behaviour when used with lazy_property from distributions",
      "z": ":) I think we fixed this in https://github.com/pytorch/pytorch/pull/7708, which is part of 0.4.1. Can you try that?",
      "y": "Fixed in https://github.com/pytorch/pytorch/pull/7708, which is part of 0.4.1."
   },
   {
      "x": "[feature request] Treat tensor as tuple of tensors in torch.cat",
      "z": "I think the request of making `torch.cat` support a single tensor as input makes sense.\n \n We already consider a tensor as an iterable (we can do `for batch in torch.rand(3, 10)` and we will have tensors of size 10).\n I believe `torch.cat` should iterate over any sequence of tensors and concatenate them over the specified dimension.\n \n I'd not vote for adding `cat` as a method though. Most of the time we use it with tuples / lists, so this pattern wouldn't make sense in this case.",
      "y": "We already consider a tensor as an iterable (we can do `for batch in torch.rand(3, 10)` and we will have tensors of size 10).\n I believe `torch.cat` should iterate over any sequence of tensors and concatenate them over the specified dimension."
   },
   {
      "x": "Differences between .data and .detach",
      "z": "Here's an example. If you use `detach()` instead of `.data`, gradient computation is guaranteed to be correct..\n \n \n \n ```\n \n >>> a = torch.tensor([1,2,3.], requires_grad = True)\n \n >>> out = a.sigmoid()\n \n >>> c = out.detach()\n \n >>> c.zero_() \n \n tensor([ 0., 0., 0.])\n \n \n \n >>> out # modified by c.zero_() !!\n \n tensor([ 0., 0., 0.])\n \n \n \n >>> out.sum().backward() # Requires the original value of out, but that was overwritten by c.zero_()\n \n RuntimeError: one of the variables needed for gradient computation has been modified by an inplace operation\n \n ```\n \n \n \n As opposed to using `.data`:\n \n ```\n \n >>> a = torch.tensor([1,2,3.], requires_grad = True)\n \n >>> out = a.sigmoid()\n \n >>> c = out.data\n \n >>> c.zero_()\n \n tensor([ 0., 0., 0.])\n \n \n \n >>> out # out was modified by c.zero_()\n \n tensor([ 0., 0., 0.])\n \n \n \n >>> out.sum().backward()\n \n >>> a.grad # The result is very, very wrong because `out` changed!\n \n tensor([ 0., 0., 0.])\n \n ```\n \n \n \n I'll leave this issue open: we should add an example to the migration guide and clarify that section.",
      "y": "Here's an example. If you use `detach()` instead of `.data`, gradient computation is guaranteed to be correct..\n \n \n \n ```\n \n >>> a = torch.tensor([1,2,3.], requires_grad = True)\n \n >>> out = a.sigmoid()\n \n >>> c = out.detach()\n \n >>> c.zero_() \n \n tensor([ 0., 0., 0.])\n \n \n \n >>> out # modified by c.zero_() !!\n \n tensor([ 0., 0., 0.])\n \n \n \n >>> out.sum().backward() # Requires the original value of out, but that was overwritten by c.zero_()\n \n RuntimeError: one of the variables needed for gradient computation has been modified by an inplace operation\n \n ```\n \n \n \n As opposed to using `.data`:\n \n ```\n \n >>> a = torch.tensor([1,2,3.], requires_grad = True)\n \n >>> out = a.sigmoid()\n \n >>> c = out.data\n \n >>> c.zero_()\n \n tensor([ 0., 0., 0.])\n \n \n \n >>> out # out was modified by c.zero_()\n \n tensor([ 0., 0., 0.])\n \n \n \n >>> out.sum().backward()\n \n >>> a.grad # The result is very, very wrong because `out` changed!\n \n tensor([ 0., 0., 0.])\n \n ```\n \n \n \n I'll leave this issue open: we should add an example to the migration guide and clarify that section."
   },
   {
      "x": "Segmentation fault with cpp_extensions example",
      "z": "It seems that you are using GCC 4.8\n \n Didn't you get the following message from `cpp_extensions`?\n \n ```\n \n Your compiler (g++ 4.8) may be ABI-incompatible with PyTorch!\n \n Please use a compiler that is ABI-compatible with GCC 4.9 and above.\n \n See https://gcc.gnu.org/onlinedocs/libstdc++/manual/abi.html.\n \n \n \n See https://gist.github.com/goldsborough/d466f43e8ffc948ff92de7486c5216d6\n \n for instructions on how to install GCC 4.9 or higher.\n \n ```\n \n \n \n Could you try updating gcc to 4.9 or higher?",
      "y": "It seems that you are using GCC 4.8\n you would have get the following message from `cpp_extensions`?\n ```\n Your compiler (g++ 4.8) may be ABI-incompatible with PyTorch!\n Please use a compiler that is ABI-compatible with GCC 4.9 and above.\n See https://gcc.gnu.org/onlinedocs/libstdc++/manual/abi.html.\n \n See https://gist.github.com/goldsborough/d466f43e8ffc948ff92de7486c5216d6\n for instructions on how to install GCC 4.9 or higher.\n ```\n try updating gcc to 4.9 or higher"
   },
   {
      "x": "RecursionError when using torch.utils.checkpoint",
      "z": "I encountered the exact same issue when using massive amount of checkpoint-ing, and to workaround it I increased the max recursion depth like this \n `sys.setrecursionlimit(3000)`\n notice that you can see the current recursion limit like this:\n `sys.getrecursionlimit()`\n for me it was 1000\n \n note: I don't know why there's a deep recursion in this scenario, as the checkpoint calls are not nested, I assume that it's related to the autograd mechanism that travels on the graph or something of this nature.",
      "y": "increased the max recursion depth like this `sys.setrecursionlimit(3000)`"
   },
   {
      "x": "It seems can't compile pytorch 0.4.0 from source on macOS.",
      "z": "What is your cmake version? I can compile on a mac and my version is 3.9.4. You can see it via `cmake --version`",
      "y": "Use cmake version 3.9.4. You can see it via `cmake --version`"
   },
   {
      "x": "pip install torch error",
      "z": "`pip install torch` is not supported on Windows. Please try the commands on http://pytorch.org.",
      "y": "`pip install torch` is not supported on Windows. Please try the commands on http://pytorch.org."
   },
   {
      "x": "torch.std() returns nan for single item tensors.",
      "z": "This is more about numpy parity rather than correctness. Hence I removed the hipri flag.",
      "y": "np default is ddof=0, but pytorch has default unbiased=True."
   },
   {
      "x": "Indexing with dtype torch.uint8",
      "z": "Is it enough to only modify `dtype` in this function?\n \n ```\n \n non_final_mask = torch.tensor(tuple(map(lambda s: s is not None,\n \n  batch.next_state)), device=device, dtype=torch.bool)\n \n ```",
      "y": "modify `dtype` in this function"
   },
   {
      "x": "windows release of pytorch 1.3 is missing several required cuda libraries",
      "z": "Well, the suffix of those DLLs is changed to `_10.dll` instead of `_101.dll`.",
      "y": "The suffix of DLLs is changed to `_10.dll` instead of `_101.dll`."
   },
   {
      "x": "[jit] `torch.isfinite` is broken",
      "z": "> Hmm actually, looks like we're not correctly resolving `torch.isfinite` to torch.functional.isfinite.\n \n > \n \n > A workaround is to call `torch.functional.isfinite(x)`\n \n \n \n I've tried, it won't work. F.isfinite(x) is returning 'bool' while scripting and Tensor in Python\n \n \n \n ```\n \n Variable 'bool_tensor' is annotated with type Tensor but is being assigned to a value of type bool:\n \n at box_regression.py:84:8\n \n  \"\"\"\n \n  deltas (Tensor): some tensor\n \n  \"\"\"\n \n  bool_tensor : torch.Tensor = torch.functional.isfinite(deltas)\n \n  ~~~~~~~~~~~ <--- HERE\n \n ```",
      "y": "we're not correctly resolving `torch.isfinite` to torch.functional.isfinite.\n  A workaround is to call `torch.functional.isfinite(x)`"
   },
   {
      "x": "RuntimeError: Didn't find engine for operation quantized::conv_prepack NoQEngine",
      "z": "cc @supriyar @dskhudia @jamesr66a \n \n \n \n There are two libraries we're currently using for quantized operations: fbgemm and qnnpack. Fbgemm today doesn't support window build: https://github.com/pytorch/FBGEMM/issues/150 . Would be nice if someone gave it a try.\n \n \n \n For qnnpack - we disable it by default on non-mobile builds:\n \n https://github.com/pytorch/pytorch/blob/0ae0c9788edf4ee31c3c48b906bb9a65b871bd03/aten/src/ATen/Context.cpp#L119-L121\n \n \n \n Afaik, the performance is not that great for qnnpack on x86 cpus and there were some other stability issues. @supriyar - do you know if it's better to reenable it?\n \n \n \n As a temp workaround - try setting `torch.backends.quantized.engine = 'qnnpack'` to see whether it works. It's not an official workaround - just something to try.",
      "y": "try setting `torch.backends.quantized.engine = 'qnnpack'`"
   },
   {
      "x": "Multiprocess data loader crash/hang in v1.3",
      "z": "This was a packaging issue that was fixed via https://github.com/pytorch/builder/commit/2ac74c1d5669ad2c32b873461970d33cc4b5c93c\n \n 1.3.0.post2 or 1.3.1 both have the fix, so @pdwilson12 please upgrade.",
      "y": "This was a packaging issue that was fixed via https://github.com/pytorch/builder/commit/2ac74c1d5669ad2c32b873461970d33cc4b5c93c\n \n 1.3.0.post2 or 1.3.1 both have the fix, please upgrade."
   },
   {
      "x": "Using tensor cores",
      "z": "There is not a special API and, unfortunately, the cases where tensor cores are enabled are not very easy to describe. NVIDIA has some performance guides for Tensor Cores that can be useful references: https://developer.download.nvidia.com/video/gputechconf/gtc/2019/presentation/s9926-tensor-core-performance-the-ultimate-guide.pdf\n \n \n \n Essentially operations that internally perform GEMMs can use Tensor Cores on fp16 inputs when those inputs' sizes meet special criteria.",
      "y": "NVIDIA has some performance guides for Tensor Cores that can be useful references: https://developer.download.nvidia.com/video/gputechconf/gtc/2019/presentation/s9926-tensor-core-performance-the-ultimate-guide.pdf"
   },
   {
      "x": "\"no module named torch\". But installed pytorch 1.3.0 with conda in Ubuntu 18.04.02 Server Edition",
      "z": "You have two python environments: \n \n 1. Conda with Python 3.7 (Inferred from code snippet 3)\n \n 2. Virtualenv with Python 3.8 (Inferred from code snippet 4)\n \n \n \n As you can see from code snippet 1, torch is successfully installed into the first python environment. But from code snippet 4, it implies that torch is not installed in the second python environment. So the solution is to deactivate the second environment through `deactivate`, or to reactivate the first environment by calling `conda activate`.",
      "y": "Activate or deactive one of the enviornment"
   },
   {
      "x": "Torch from sourc and Torchvision from pip",
      "z": "`pip install --user --no-dependencies torchvision`",
      "y": "`pip install --user --no-dependencies torchvision`"
   },
   {
      "x": "Cannot compile CPP extensions for benchmarking",
      "z": "`cpuinfo` is a submodule in the pytorch repo, the missing header lives at `third_party/cpuinfo/include/cpuinfo.h`. When you see this error, it's most likely that you're trying to run the benchmarks without having installed pytorch from sources (which will instill that header file), but instead you're trying to use a pytorch install from a conda package or wheel, or you've installed into the wrong environment.",
      "y": "`cpuinfo` is a submodule in the pytorch repo, the missing header lives at `third_party/cpuinfo/include/cpuinfo.h`. When you see this error, it's most likely that you're trying to run the benchmarks without having installed pytorch from sources (which will instill that header file), but instead you're trying to use a pytorch install from a conda package or wheel, or you've installed into the wrong environment."
   },
   {
      "x": "\"undefined symbol: PySlice_Unpack\" of pytorch 1.0.0 on Ubuntu 14.04",
      "z": "@vishwakftw Thank you. It works when I use Python 3.6.3.",
      "y": "use Python 3.6.3."
   },
   {
      "x": "Can not build pytorch tag v1.0.0 from source",
      "z": "I downgraded cudnn from 7.4.1 to 7.3.0, then it works..",
      "y": "downgrade cudnn from 7.4.1 to 7.3.0"
   },
   {
      "x": "Wrong recursive module::load",
      "z": "That makes sense. I'll send a fix.",
      "y": "Fixed"
   },
   {
      "x": "torch.jit.trace incorrect for function outputting tuple of size 1",
      "z": "verified #15289 will fix this.",
      "y": "Fixed"
   },
   {
      "x": "what is the algorithm theory of torch.nn.AdaptiveMaxPool2d?",
      "z": "I originally wrote it in lua-torch to be able to reproduce Spatial Pyramid Pooling for Object Detection. I'm not sure if there are any particular papers focusing specifically on this layer. The original discussion can be found in https://github.com/torch/nn/issues/141\n \n You can find the current implementation in https://github.com/pytorch/pytorch/blob/master/aten/src/THNN/generic/SpatialAdaptiveMaxPooling.c, the cuda implementation is in the THCUNN folder",
      "y": "You can find the current implementation in https://github.com/pytorch/pytorch/blob/master/aten/src/THNN/generic/SpatialAdaptiveMaxPooling.c, the cuda implementation is in the THCUNN folder"
   },
   {
      "x": "RuntimeError: std::exception for conv2d with groups > 1",
      "z": "My thanks to @gauss256 for the suggestion of creating a new user + installing Anaconda + installing PyTorch. That solved it for me.\n \n I then compared the `LD_LIBRARY_PATH` for the two users and found that removing `/usr/local/lib` from the `LD_LIBRARY_PATH` for the first user (for whom this was not working) fixed the issue.\n \n Then I investigated the libraries at `/usr/local/lib` and found `libmkldnn.so.0`.\n \n I added back `/usr/local/lib` to `LD_LIBRARY_PATH` and the error appeared again.\n \n Since I suspected the error was related to mkldnn, I moved this file `libmkldnn.so.0` out of `/usr/local/lib` to another location.\n \n Then I tried running the code again and it worked (with `/usr/local/lib` included in `LD_LIBRARY_PATH` but with the file `libmkldnn.so.0` removed from `/usr/local/lib`)\n \n So this issue (at least for me) was caused by having a system wide mkldnn installation conflicting (and overriding) the conda mkldnn.\n \n I'm adding this comment to aid in debugging in case someone else faces a similar issue.",
      "y": "creating a new user + installing Anaconda + installing PyTorch"
   },
   {
      "x": "torch::jit::trace C++ equivalent of torch.jit.trace",
      "z": "And will it possible to convert a torch::script::Module to torch::nn::Module ?",
      "y": "It's not possible to trace scalar values in C++\n \n plan to merge script::Module and nn::Module in a future release"
   },
   {
      "x": "libcudart.so not found when compiling with NO_DISTRIBUTED=1",
      "z": "The reason for the issue is that if libcudart.so is not dynamically linked, ctypes cannot see a cudart symbol. I cant figure out why.\n Basically this line is failing:\n \n pytorch/torch/cuda/__init__.py\n \n Line 63 in 517c7c9\n \n  if hasattr(lib, 'cudaGetErrorName'): \n lib = ctypes.cdll.LoadLibrary(None)\n if hasattr(lib, 'cudaGetErrorName'):\n It so accidentally happens that when distributed backend is compiled, the CMake also dynamically links with libcudart.so, and hence this failure is not seen.\n The short-term fix is quite simple, to switch to linking against libcudart.so.\n \n The long-term fix is to figure out what ctypes' problem is",
      "y": "The short-term fix is quite simple, to switch to linking against libcudart.so.\n \n The long-term fix is to figure out what ctypes' problem is"
   },
   {
      "x": "When will the pytorch 1.0 stable version release?",
      "z": "friday",
      "y": "7th Dec,2018"
   },
   {
      "x": "DataLoader error with multiple workers - \"RuntimeError: unable to open shared memory object XXX in read-write mode\"",
      "z": "See the row `-n` in results of `ulimit -a`. You might want to increase that number. FYI I have 500,000 in my system. \n \n Closing, please feel free to reopen if that doesn't solve your problem.",
      "y": "increasing your open files limit (try ulimit)\n trying another multiprocessing sharing strategy (https://pytorch.org/docs/master/multiprocessing.html#sharing-strategies)"
   },
   {
      "x": "The sort order for tensor.unique() defaults to \"unsorted\" but in practice is ascending on gpu and descending on cpu",
      "z": "This is actually not a problem I think.\n \n \n \n By default, `sorted=False`, and the fact that the output seems to be sorted is an implementation detail.\n \n \n \n ```python\n \n a=tensor([1,1,2,3]).cuda()\n \n b=tensor([1,1,2,3]).cpu()\n \n \n \n print(a.unique(sorted=True))\n \n print(b.unique(sorted=True))\n \n ```\n \n returns, as expected\n \n ```\n \n tensor([1, 2, 3])\n \n tensor([1, 2, 3], device='cuda:0')\n \n ```\n \n \n \n See this for example\n \n ```python\n \n a = torch.tensor([1.5,1,2,3])\n \n a.unique()\n \n > tensor([3.0000, 2.0000, 1.0000, 1.5000])\n \n a.cuda().unique()\n \n > tensor([1.0000, 1.5000, 2.0000, 3.0000], device='cuda:0')\n \n ```",
      "y": "Fixed soerted = True"
   },
   {
      "x": "issues to run minimal pytorch example",
      "z": "Fixed this issue by downloading the two missing libraries from [Intels MKL Mac releases page](https://github.com/intel/mkl-dnn/releases) and copying them both (`libmklml.dylib`, `libiomp5.dylib`) to `libtorch/lib`.",
      "y": "Fixed this issue by downloading the two missing libraries from [Intels MKL Mac releases page](https://github.com/intel/mkl-dnn/releases) and copying them both (`libmklml.dylib`, `libiomp5.dylib`) to `libtorch/lib`."
   },
   {
      "x": "v1.7 requirement \"dataclasses\" is not compatible with python > 3.6",
      "z": "@lemondante \n \n You can patch the wheel to remove the requirement in the metadata.\n \n Or install it in two steps from the latest requirements (or manually) : \n \n ```\n \n pip install -r https://raw.githubusercontent.com/pytorch/pytorch/master/requirements.txt\n \n pip install <torch_wheel> --no-deps\n \n ```",
      "y": "patch the wheel to remove the requirement in the metadata.\n Or install it in two steps from the latest requirements (or manually) : \n ```\n pip install -r https://raw.githubusercontent.com/pytorch/pytorch/master/requirements.txt\n pip install <torch_wheel> --no-deps\n ```"
   },
   {
      "x": "[ONNX] error with pytorch \"RuntimeError: Unexpected node type: onnx::Cast\"",
      "z": "@oneway234 - Several updates have been added to ONNX exporter in PyTorch since PyTorch 1.3.1. Could you please try with the latest version of PyTorch (1.7) and see if this issue is resolved.",
      "y": "Use the latest version of PyTorch (1.7)"
   },
   {
      "x": "clip_grad_norm_ silently passes when not finite",
      "z": "Adding the argument seems cost free, so might as well do it.",
      "y": "argument error_if_nonfinite=True"
   },
   {
      "x": "x.shape return tuple instead of torch.Size",
      "z": "I believe this is the same bug as #47090 but with a different reproducer.\n \n \n \n Edit: Confirmed, the branch that fixed that also fixes this. Being worked on in #47110.",
      "y": "Bug Fixed"
   },
   {
      "x": "new_ones() missing 1 required positional arguments: \"size\"",
      "z": "Yes: the fast.ai library uses subclassing to add semantics and/or metadata to Tensors, eg: TensorImage, TensorMask, TensorPoint, TensorBBox, etc. Issue #22402 has some discussion and elaborates further on that.",
      "y": "new_zeros does work:\n \n from torch import Tensor\n \n class MyTensor(Tensor):\n  pass\n \n a = MyTensor([1,2,3])\n a.new_zeros(320).shape"
   },
   {
      "x": "batch_norm const running_mean/_var modified in-place",
      "z": "@cpuhrsch yes it is by design. See https://github.com/zdevito/ATen/issues/27",
      "y": "By design"
   },
   {
      "x": "torch.distributed.broadcast_object_list doesn't broadcast custom class object",
      "z": "you are printing local dummy() object, not the object broadcast from rank 0. \n \n \n \n Try to revise codes like this: \n \n \n \n  if dist.get_rank() == 0:\n \n  objects = [dummy()] \n \n  else:\n \n  objects = [None]\n \n  torch.distributed.broadcast_object_list(objects, src=0)\n \n \n \n  print(objects)",
      "y": "printing local dummy() object, not the object broadcast from rank 0. \n \n Try to revise codes like this: \n \n  if dist.get_rank() == 0:\n  objects = [dummy()] \n  else:\n  objects = [None]\n  torch.distributed.broadcast_object_list(objects, src=0)\n \n  print(objects)"
   },
   {
      "x": "Tensorboard: ValueError: Duplicate plugins for name projector",
      "z": "@Biaocsu, I also met the problem but I fixed it. My method is:\n\n1. I downloaded a test script from https://raw.githubusercontent.com/tensorflow/tensorboard/master/tensorboard/tools/diagnose_tensorboard.py\n2. I run it and it told me that I have two tensorboards with a different version. Also, it told me how to fix it.\n3. I followed its instructions and I can make my tensorboard work.\n\nI think this error means that you have two tensorboards installed so the plugin will be duplicated.  Another method would be helpful that is to reinstall the python environment using conda.\n\nHope to help you.",
      "y": " this error means that you have two tensorboards installed so the plugin will be duplicated.  Another method could be helpful is to reinstall the python environment using conda"
   },
   {
      "x": "Data Loader does not work with Hdf5 file, when num_worker >1",
      "z": "**Solution**\n \n \n \n This issue could be solved and the solution is simple: \n \n 1. Do not open hdf5 inside `__init__`\n \n 2. Open the hdf5 at the first data iteration.\n \n \n \n Here is an illustration:\n \n ```python\n \n class LXRTDataLoader(torch.utils.data.Dataset):\n \n  def __init__(self):\n \n  \"\"\"do not open hdf5 here!!\"\"\"\n \n \n \n  def open_hdf5(self):\n \n  self.img_hdf5 = h5py.File('img.hdf5', 'r')\n \n  self.dataset = self.img_hdf5['dataset'] # if you want dataset.\n \n \n \n  def __getitem__(self, item: int):\n \n  if not hasattr(self, 'img_hdf5'):\n \n  self.open_hdf5()\n \n  img0 = self.img_hdf5['dataset'][0] # Do loading here\n \n  img1 = self.dataset[1]\n \n  return img0, img1\n \n ```\n \n Then the dataloader with `num_workers` > 1 could just be normally used.\n \n ```python\n \n train_loader = torch.utils.data.DataLoader(\n \n  dataset=train_tset,\n \n  batch_size=32,\n \n  num_workers=4\n \n  )\n \n ```\n \n \n \n \n \n \n \n **Explanation**\n \n The multi-processing actually happens when you create the data iterator (e.g., when calling `for datum in dataloader:`):\n \n https://github.com/pytorch/pytorch/blob/461014d54b3981c8fa6617f90ff7b7df51ab1e85/torch/utils/data/dataloader.py#L712-L720\n \n In short, it would create multiple processes which \"copy\" the state of the current process. Thus the opened hdf5 file object would be dedicated to each subprocess if we open it at the first data iteration. \n \n \n \n If you somehow create an hdfs file in` __init__` and set up the `num_workers' > 0, it might cause two issues:\n \n 1. The writing behavior is non-determistic. (We do not need to write to hdf5, thus this issue is ignored.)\n \n 2. The state of the hdfs is copied, which might not faithfully indicate the current state. \n \n \n \n In the previous way, we bypass this two issues.",
      "y": "**Solution**\n \n \n \n This issue could be solved and the solution is simple: \n \n 1. Do not open hdf5 inside `__init__`\n \n 2. Open the hdf5 at the first data iteration.\n \n \n \n Here is an illustration:\n \n ```python\n \n class LXRTDataLoader(torch.utils.data.Dataset):\n \n  def __init__(self):\n \n  \"\"\"do not open hdf5 here!!\"\"\"\n \n \n \n  def open_hdf5(self):\n \n  self.img_hdf5 = h5py.File('img.hdf5', 'r')\n \n  self.dataset = self.img_hdf5['dataset'] # if you want dataset.\n \n \n \n  def __getitem__(self, item: int):\n \n  if not hasattr(self, 'img_hdf5'):\n \n  self.open_hdf5()\n \n  img0 = self.img_hdf5['dataset'][0] # Do loading here\n \n  img1 = self.dataset[1]\n \n  return img0, img1\n \n ```\n \n Then the dataloader with `num_workers` > 1 could just be normally used.\n \n ```python\n \n train_loader = torch.utils.data.DataLoader(\n \n  dataset=train_tset,\n \n  batch_size=32,\n \n  num_workers=4\n \n  )\n \n ```\n \n \n \n \n \n \n \n **Explanation**\n \n The multi-processing actually happens when you create the data iterator (e.g., when calling `for datum in dataloader:`):\n \n https://github.com/pytorch/pytorch/blob/461014d54b3981c8fa6617f90ff7b7df51ab1e85/torch/utils/data/dataloader.py#L712-L720\n \n In short, it would create multiple processes which \"copy\" the state of the current process. Thus the opened hdf5 file object would be dedicated to each subprocess if we open it at the first data iteration. \n \n \n \n If you somehow create an hdfs file in` __init__` and set up the `num_workers' > 0, it might cause two issues:\n \n 1. The writing behavior is non-determistic. (We do not need to write to hdf5, thus this issue is ignored.)\n \n 2. The state of the hdfs is copied, which might not faithfully indicate the current state. \n \n \n \n In the previous way, we bypass this two issues."
   },
   {
      "x": "DataLoader, when num_worker is great than 1, the data loaded are wrong, or a error a reported!",
      "z": "h5py has a parallel mode that depends on mpi4py. I had the above problem and solved it by doing the following...\n \n (i) pip uninstall h5py\n \n (ii) pip install mpi4py\n \n (iii) pip install h5py.",
      "y": "h5py has a parallel mode that depends on mpi4py. I had the above problem and solved it by doing the following...\n \n (i) pip uninstall h5py\n \n (ii) pip install mpi4py\n \n (iii) pip install h5py."
   },
   {
      "x": "DataParallel with Multi-GPU module (eg. 2 model replica on 4 GPU)",
      "z": "The walk-around we found is to `DaraParallel` the head of the model and the tail of the model separately. \n \n And wrap the data transfer in the `forward` function.\n \n Here is a working example. Hopefully, someone finds this useful. \n \n Thanks. This issue can be closed. \n \n ```\n \n \n \n class ConvBlck(nn.Module):\n \n  def __init__(self, in_channel, out_channel, kernel_size, stride=1):\n \n  super(ConvBlck, self).__init__()\n \n  self.blck = nn.Sequential( OrderedDict([\n \n  ('conv', nn.Conv2d( in_channel, out_channel, kernel_size=kernel_size, stride=stride, padding=kernel_size//2)),\n \n  ('relu', nn.ReLU() )]) )\n \n  def forward(self, x):\n \n  return self.blck(x)\n \n \n \n class SixConv(nn.Module):\n \n  def __init__(self):\n \n  super(SixConv, self).__init__()\n \n  tmp = 5\n \n  self.conv1 = nn.Sequential( ConvBlck(1,1<<tmp,5,2), ConvBlck(1<<tmp,1<<(tmp+1),5,2) )\n \n  self.conv2 = nn.Sequential( ConvBlck(1<<(tmp+1),1<<tmp,5,2), nn.AdaptiveAvgPool2d(1) , nn.Conv2d(1<<tmp, 10, 1, 1) )\n \n \n \n  def forward(self, x):\n \n  x = self.conv1(x)\n \n  next_index = x.device.index + 1\n \n  x = x.cuda(next_index)\n \n  x = self.conv2(x)\n \n  return x\n \n model = SixConv()\n \n model.conv1 = nn.DataParallel(model.conv1,device_ids=[0,2]);\n \n model.conv2 = nn.DataParallel(model.conv2,device_ids=[1,3]);\n \n crite = torch.nn.CrossEntropyLoss()\n \n optim = torch.optim.Adam( model.parameters() , lr=0.001)\n \n model.conv1.to( 'cuda:0' )\n \n model.conv2.to( 'cuda:1' )\n \n \n \n ```",
      "y": "The walk-around we found is to `DaraParallel` the head of the model and the tail of the model separately. \n \n And wrap the data transfer in the `forward` function.\n \n Here is a working example. Hopefully, someone finds this useful. \n \n ``` \n class ConvBlck(nn.Module):\n \n  def __init__(self, in_channel, out_channel, kernel_size, stride=1):\n \n  super(ConvBlck, self).__init__()\n \n  self.blck = nn.Sequential( OrderedDict([\n \n  ('conv', nn.Conv2d( in_channel, out_channel, kernel_size=kernel_size, stride=stride, padding=kernel_size//2)),\n \n  ('relu', nn.ReLU() )]) )\n \n  def forward(self, x):\n \n  return self.blck(x)\n \n \n \n class SixConv(nn.Module):\n \n  def __init__(self):\n \n  super(SixConv, self).__init__()\n \n  tmp = 5\n \n  self.conv1 = nn.Sequential( ConvBlck(1,1<<tmp,5,2), ConvBlck(1<<tmp,1<<(tmp+1),5,2) )\n \n  self.conv2 = nn.Sequential( ConvBlck(1<<(tmp+1),1<<tmp,5,2), nn.AdaptiveAvgPool2d(1) , nn.Conv2d(1<<tmp, 10, 1, 1) )\n \n \n \n  def forward(self, x):\n \n  x = self.conv1(x)\n \n  next_index = x.device.index + 1\n \n  x = x.cuda(next_index)\n \n  x = self.conv2(x)\n \n  return x\n \n model = SixConv()\n \n model.conv1 = nn.DataParallel(model.conv1,device_ids=[0,2]);\n \n model.conv2 = nn.DataParallel(model.conv2,device_ids=[1,3]);\n \n crite = torch.nn.CrossEntropyLoss()\n \n optim = torch.optim.Adam( model.parameters() , lr=0.001)\n \n model.conv1.to( 'cuda:0' )\n \n model.conv2.to( 'cuda:1' )\n \n \n \n ```"
   },
   {
      "x": "index_put_ has unreasonable checks",
      "z": "You are not passing the `indices` in the right format. `index_put_` expect the index tensor to be the transpose of what you passed.\n \n So this works\n \n \n \n ```python\n \n target = torch.zeros([5,3])\n \n indices = torch.LongTensor([[0,1], [1, 2], [2, 2], [3, 0], [4, 1]])\n \n value = torch.ones(indices.shape[0])\n \n target.index_put_(tuple(indices.t()), value)\n \n ```\n \n \n \n Also, it is generally better to use advanced indexing in such cases, as it's more clear what it is doing\n \n ```python\n \n target[idx1, idx2] = 1\n \n ```\n \n \n \n Closing as not a bug.",
      "y": "pass the `indices` in the right format"
   },
   {
      "x": "upgrade MKL-DNN 0.20.1 to DNNL 1.1",
      "z": "In the past MKL GEMMs have been faster than MKL-DNN gemms. The claim from Intel is that with DNNL 1.1 it will be faster ..ie no regressions in GEMMs.",
      "y": "The claim from Intel is that with DNNL 1.1 it will be faster"
   },
   {
      "x": "Future callbacks in RPC should capture and restore autograd context id",
      "z": "FYI, worked on a similar issue recently. https://github.com/pytorch/pytorch/pull/36395\nAdding extra argument seems more like a temporary fix rather than a scalable one.\n\nI think we should consider leveraging ThreadLocalState.cpp this time. It also transfers profiler thread-local state. Otherwise, an ad-hoc fix will still leave the profiler broken after RPC thread switch.",
      "y": "ad-hoc fix will still leave the profiler broken after RPC thread switch,  we should consider leveraging ThreadLocalState.cpp"
   },
   {
      "x": "[Feature] View a floating point tensor as complex tensor, and vice versa",
      "z": "> Another benefit is, when `view_as_floating` is implemented, then `torch.real` could be implemented as `torch.view_as_floating(z)[..., 0]` which is copy-free.\n\nAgain, a little nuisance about naming, since in theory one can imagine complex integer pairs (quantized or maybe some Zn for cryptography needs), then `view_as_floating` would be confusing",
      "y": "When implementing `view_as_floating` , `torch.real` could be implemented as `torch.view_as_floating(z)[..., 0]` which is copy-free."
   },
   {
      "x": "Should torch.real, torch.imag be implemented as a view?",
      "z": "Right now, tensor.imag is disabled for real tensors. This is because if we return a new tensor of zeros, the user would be able to update the tensor returned by tensor.imag which should not be allowed as numpy returns a read-only array, and pytorch doesn't support read-only tensors yet.",
      "y": "tensor.imag is disabled for real tensors"
   },
   {
      "x": "Can you add an overloaded operator &,|,^ to a libtorch tensor",
      "z": "Overload bitwise NOT, AND, OR, XOR operators for at::Tensor",
      "y": "We can Overload bitwise NOT, AND, OR, XOR operators"
   },
   {
      "x": "torch._C._cuda_getDriverVersion() reporting CUDA version instead of NVIDIA driver version",
      "z": "it doesn't return the CUDA version. It returns \"Returns the latest version of CUDA supported by the driver.\".\nIf you have CUDA10 installed, but driver that only supports CUDA9, it will return 9000",
      "y": "Returns the latest version of CUDA supported by the driver"
   },
   {
      "x": "gradcheck does not work for self-overlapping inputs",
      "z": "This is an error in gradcheck rather than in gradient computation. It's not specific to mv and would happen for self-overlapping inputs. Add a check for stride==0 in gradcheck ",
      "y": "This is an error in gradcheck not in gradient computation Add a check for stride==0 in gradcheck "
   },
   {
      "x": "libtorch 1.5.0 libiomp5.dylib contains erroneous link to /DLC/torch/libiomp5.dylib instead of using @rpath",
      "z": "I had the same issue and fixed it with:\n\n```shell\ninstall_name_tool -id @rpath/libiomp5.dylib libiomp5.dylib\n```\n\n",
      "y": "Use ```shell\ninstall_name_tool -id @rpath/libiomp5.dylib libiomp5.dylib\n```"
   },
   {
      "x": "RuntimeError: CUDA error: an illegal memory access was encountered",
      "z": "> I tried your \"kalman_filter.py\" script on today's master commit 1.6.0a0+21ba3b4, with small changes like this\n> \n> ```python\n> if __name__ == '__main__':\n>     device = 'cuda'\n> \n>     kf = KalmanFilter(device)\n>     mean, covariance = kf.initiate(torch.tensor([10, 15, 0.5, 10], device=device))\n>     mean, covariance = kf.predict(mean, covariance)\n>     mean, covariance = kf.update(mean, covariance, torch.tensor([12, 20, 0.6, 11], device=device))\n> \n>     mean_2, covariance_2 = kf.initiate(torch.tensor([12, 13, 0.7, 5], device=device))\n>     mean_2, covariance_2 = kf.predict(mean_2, covariance_2)\n>     mean_2, covariance_2 = kf.update(mean_2, covariance_2, torch.tensor([13, 14, 0.7, 8], device=device))\n> \n>     squared_maha = kf.gating_distance(torch.cat((mean, mean_2), dim=0),\n>                                       torch.cat((covariance, covariance_2), dim=0),\n>                                       torch.tensor([[12, 20, 0.6, 11],\n>                                                     [20, 16, 0.4, 18]], device=device))\n>     print(squared_maha)\n> ```\n> \n> The script finished well with no exception and gave same output as CPU. Can you try with nightly build (and maybe latest driver, cuda, cudnn, etc) to verify that?\n> \n> My environment (MAGMA 2.5.3)\n> \n> ```\n> PyTorch version: 1.6.0a0+21ba3b4\n> Is debug build: No\n> CUDA used to build PyTorch: 10.2\n> \n> OS: Fedora 31 (Workstation Edition)\n> GCC version: (GCC) 8.3.1 20190223 (Red Hat 8.3.1-2)\n> CMake version: version 3.15.3\n> \n> Python version: 3.7\n> Is CUDA available: Yes\n> CUDA runtime version: 10.2.89\n> GPU models and configuration: GPU 0: GeForce RTX 2070 SUPER\n> Nvidia driver version: 440.31\n> cuDNN version: /usr/local/cuda-10.2/targets/x86_64-linux/lib/libcudnn.so.7\n> \n> Versions of relevant libraries:\n> [pip3] numpy==1.17.3\n> [pip3] torch==1.6.0a0+21ba3b4\n> [pip3] torchvision==0.7.0a0+f9ef235\n> [conda] Could not collect\n> ```\n\nIn fact I have bypassed cholesky in some alternative way:\nhttps://github.com/GlassyWing/sort_torch/blob/3d4c809dee6ad35b2f7d5a4b75f9e30b07722441/deep_sort/sort/kalman_filter.py#L245\n\nIf you need to test, you need to cancel cholesky's comment and comment out the replacement method. \n\nIn previous attempts, I found that there will be no errors if run alone, but if run in multiple threads or used with other models, it may throw an exception. I have not found a reproducible pattern, but under multi-threading, each thread has a Kalman filter, and it is more likely to cause errors.",
      "y": "there will be no errors if run alone, but if run in multiple threads or used with other models, it may throw an exception."
   },
   {
      "x": "Will the next version of libtorch provide modules instead of header files in the c++20 standard?",
      "z": "You mentioned `instead of`. Even if under the most optimistic condition that all the dependencies get the support for the C++ modules before our next release, we won't stop to provide C++ headers. The reason is clear. We need to support the old toolchains and old CUDA. It is impossible to deprecate the C++ headers only in several months.",
      "y": "In order to support the old toolchains and old CUDA we need C++ headers."
   },
   {
      "x": "Some operations crash autograd if parameter size is changed.",
      "z": "I think you want to do something similar to what we do for `resize_()` [here](https://github.com/pytorch/pytorch/blob/cd9a357f32c7548d444b6e97aa52ded531d68f38/torch/csrc/autograd/VariableTypeManual.cpp#L240-L260). Basically, the Variable version of the function is made by hand, and will clear out the grad accumulator by setting it to a nullptr. And then does\n```cpp\n  {\n    at::AutoNonVariableTypeMode non_var_type_mode(true);\n    self_.set_(args);\n  }\n```\nto call the Tensor version of `set_` that will be routed to the different implementations in aten.",
      "y": "the Variable version of the function is made by hand, and will clear out the grad accumulator by setting it to a nullptr. And then does ```cpp { at::AutoNonVariableTypeMode non_var_type_mode(true); self_.set_(args); } ``` to call the Tensor version of `set_` that will be routed to the different implementations in aten."
   },
   {
      "x": "Make StorageImpl untyped for non-POD types",
      "z": "> But for the resizing functions, THStorage and THCStorage use some slightly different logic since one has to do a cuda memcopy.\n\nAh yes, this is going to be a bit tricky.\n\nA lot of the \"stuff\" you need is available (Allocator in particular lets you reallocate using the same allocator that originally allocated some data), but resizing also involves a copy to preserve the old data, and we don't have this abstracted right now. So in this case I wouldn't try to remove the resize functions, as we really do need two different versions. Does that help?",
      "y": "for the resizing functions, THStorage and THCStorage use some slightly different logic since one has to do a cuda memcopy.So in this case I wouldn't try to remove the resize functions, as we really do need two different versions"
   },
   {
      "x": "Gradient checkpointing fails to backprop in some cases",
      "z": "This is expected behavior I'm afraid.\n\n- The first one raise an error because, since you checkpoint all the way to the end, you try to backward something that does not require gradient and so it fails.\n- The second one works because the first module is not checkpointed and it's output requires grad. So the next checkpoint will work\n- The third one, the first modules won't work, but since the last one is not checkpointed, it produces an output that requires gradient and so the backward does not through a runtime error.\n- The last one works for the same reason as the second one.\n\nMaking the input to your model require gradients will fix that no?\n\nAlso looks like we have a warn-once for the \"Be careful gradients are going to be None\". Maybe we should make it more verbose :D ",
      "y": "There could be multiple reasons - The first one raise an error because, since you checkpoint all the way to the end, second one works because the first module is not checkpointed and it's output requires grad, the first modules won't work, but since the last one is not checkpointed, it produces an output that requires gradient and so the backward does not through a runtime error."
   },
   {
      "x": "max_pool2d doesn't check if input is contiguous: max_pool2d_with_indices_out_cuda_frame failed with error code 0 with specific inputs",
      "z": "Thanks for the report @fanbeatsman!\n\nIt seems the error is thrown, as `problemTensor` is not contiguous, as can be seen by the strides.\nCalling `problem_tensor = problem_tensor.contiguous()` solves the issue.\n\n@xwang233 will have a look at it.\n",
      "y": "use `problem_tensor = problem_tensor.contiguous()`"
   },
   {
      "x": "LocalFileSystem' object has no attribute 'makedirs'",
      "z": "This worked for me: \n- Uninstalled tensorflow, \n- Reinstalled Tensorboard. \nPS: Restart Kernel and tensorboard.",
      "y": "You can try to \n- Uninstall tensorflow, \n- Reinstall Tensorboard. \nRestart Kernel and tensorboard."
   },
   {
      "x": "A minor bug in unused grad_input check of run_backward()",
      "z": "[autograd] fix allow_unused checking for C++ API",
      "y": "Bux is fixed in [autograd] fix allow_unused checking for C++ API"
   },
   {
      "x": "Can't compile pytorch from source",
      "z": "https://github.com/pytorch/pytorch/pull/35157/commits/5c318611978a7f9add5b889ad70e4af5b10a9c00\ngcc don't support MOV V8.4s, V9.4s \nchange MOV V8.4s, V9.4s  to MOV V8.16b, V9.16b works",
      "y": "gcc don't support MOV V8.4s, V9.4s change MOV V8.4s, V9.4s to MOV V8.16b, V9.16b works"
   },
   {
      "x": "Runtime Error when using DistributedDataParallel with torch.no_grad()",
      "z": "Not so stick to torch1.0.0. So I used torch1.1.0 nightly and this can be solved. Thank you for your reply and contribution.",
      "y": "using torch1.1.0 nightly this can be solved"
   },
   {
      "x": "Export to ONNX doesn't support basic operations",
      "z": "This error is not from ONNX exporter. In this case, torch operator Conv2d is using an internal operator 'unfolded2d_copy' which is not implemented for float16 data type.\n\nIn this case, the issue you reported is not related to ONNX exporter.",
      "y": "torch operator Conv2d is using an internal operator 'unfolded2d_copy' which is not implemented for float16 data type, it is not an ONNX error"
   },
   {
      "x": "promotion of float with int never increases precision of float",
      "z": "> that doesn't affect this case though. Even if using half + long, the result is half, not get_default_dtype. If that's desired that's fine, but it seems odd to me to go this far with it:\n\nRight, but I don't think that's a common pattern, at least with the new APIs.  The common pattern is to just not specify a dtype or to use something like Apex to handle it for you.\n\n> Right now we have torch.promote_types(torch.half, torch.long) => torch.half (not promoting), but torch.promote_types(torch.complex64, torch.double) => torch.complex128 (promoting) and that seems sensible/reasonable but still slightly unexpected given our category/kind docs.\n\nI agree this seems a little strange but I don't think it's really a comparable situation; a `torch.double` can be perfectly represented as a `torch.complex128`; it \"is a\" `complex128`.  A `torch.int64` isn't a `torch.double`, and can't be perfectly represented as a `torch.double` anyway.  And again, if we take the view that you \"opt-in\" to using `torch.double` (because none of the APIs give you doubles unless you ask), then it seems fine to promote in that way because you've already opted-in at that point.",
      "y": "`torch.double` can be perfectly represented as a `torch.complex128`; it \"is a\" `complex128`. A `torch.int64` isn't a `torch.double`, and can't be perfectly represented as a `torch.double`"
   },
   {
      "x": "Compiling error with tag v1.4.0",
      "z": "It's my fault to forget to update submodule.\n```\n# you need to update submodule after checkout\ngit submodule sync\ngit submodule update --init --recursive\n<env variables...> python3 ./setup.py bdist_wheel\n```\n",
      "y": "``` # you need to update submodule after checkout git submodule sync git submodule update --init --recursive <env variables...> python3 ./setup.py bdist_wheel ```"
   },
   {
      "x": "[Bug] Weird bug when using ATen __rshift__() on cuda tensors",
      "z": "Sorry about this @zou3519 , but I have found a fix as well. Shall I send out a PR?",
      "y": "This issue has been fixed"
   },
   {
      "x": "MacOS conda caffe2 package incorrect protobuf versions.",
      "z": "All our packages our actually built with a statically linked custom protobuf with hidden visibility. They're not actually built against any protobuf in conda. This is to avoid tons of tricky problems involving incompatible protobuf versions and incompatible c++ compilers (e.g. ser https://github.com/caffe2/caffe2/issues/1980 ). Getting this to work requires a lot of care and separate packages for gcc4.8 or gcc5+, which is confusing to users and can mess with their conda environments.\n\nUnfortunately, I'm going to consider your use case as advanced enough that we aren't going to support it in the pre-built conda packages but rather ask that you build from source, when you can build protobuf and link it dynamically into Caffe2. If you encounter any issues linking against Caffe2 when building from source though, please open another issue to let us know as that is something that we want to be easy",
      "y": "All pytorch packages are actually built with a statically linked custom protobuf with hidden visibility. They're not actually built against any protobuf in conda. This is to avoid tons of tricky problems involving incompatible protobuf versions and incompatible c++ compilers."
   },
   {
      "x": "LSTM segmentation fault in docker",
      "z": "You can build a docker image from source with -  https://github.com/pytorch/pytorch#docker-image",
      "y": "you can build a docker image from source"
   },
   {
      "x": "Caffe2 fails to build with TensorRT on the Jetson TX2",
      "z": "This is most likely due to recent change in https://github.com/pytorch/pytorch/pull/8064.  Try cmake with `-DCAFFE2_LINK_LOCAL_PROTOBUF=OFF` and see if it unblocks you. \n\n@bddppq We make need to further patch the cmakefile... ",
      "y": "Try cmake with `-DCAFFE2_LINK_LOCAL_PROTOBUF=OFF`"
   },
   {
      "x": "GRUCELL crashes python for more than 2 layers",
      "z": "conda create -n pyto python=3.6 -y\nactivate pyto\n\nconda install numpy pyyaml mkl mkl-include setuptools cmake cffi typing pandas seaborn plotly scipy statsmodels jupyter notebook cython -y\npip install cufflinks \npip install sklearn \nconda install tbb -y\n\ncd C:\\Users\\Gabi\\Downloads\ngit clone --recursive https://github.com/pytorch/pytorch\ncd pytorch\n\nset USER_LDFLAGS=/LIBPATH:C:\\ProgramData\\Miniconda3\\envs\\pyto\\Library\\lib\nset \"VS150COMNTOOLS=C:\\Program Files (x86)\\Microsoft Visual Studio\\2017\\Enterprise\\VC\\Auxiliary\\Build\"\nset CMAKE_GENERATOR=Visual Studio 15 2017 Win64\nset DISTUTILS_USE_SDK=1\ncall \"%VS150COMNTOOLS%\\vcvarsall.bat\" x64 -vcvars_ver=14.14\nset CMAKE_INCLUDE_PATH=C:\\ProgramData\\Miniconda3\\Library\\include\nset LIB=C:\\ProgramData\\Miniconda3\\Library\\lib;C:\\ProgramData\\Miniconda3\\envs\\pyto\\Library\\lib;%LIB%\npython setup.py install",
      "y": "conda create -n pyto python=3.6 -y\nactivate pyto\n\nconda install numpy pyyaml mkl mkl-include setuptools cmake cffi typing pandas seaborn plotly scipy statsmodels jupyter notebook cython -y\npip install cufflinks \npip install sklearn \nconda install tbb -y\n\ncd C:\\Users\\Gabi\\Downloads\ngit clone --recursive https://github.com/pytorch/pytorch\ncd pytorch\n\nset USER_LDFLAGS=/LIBPATH:C:\\ProgramData\\Miniconda3\\envs\\pyto\\Library\\lib\nset \"VS150COMNTOOLS=C:\\Program Files (x86)\\Microsoft Visual Studio\\2017\\Enterprise\\VC\\Auxiliary\\Build\"\nset CMAKE_GENERATOR=Visual Studio 15 2017 Win64\nset DISTUTILS_USE_SDK=1\ncall \"%VS150COMNTOOLS%\\vcvarsall.bat\" x64 -vcvars_ver=14.14\nset CMAKE_INCLUDE_PATH=C:\\ProgramData\\Miniconda3\\Library\\include\nset LIB=C:\\ProgramData\\Miniconda3\\Library\\lib;C:\\ProgramData\\Miniconda3\\envs\\pyto\\Library\\lib;%LIB%\npython setup.py install"
   },
   {
      "x": "[Feature request] Equivalent of softmax_cross_entropy_with_logits",
      "z": "But those aren't equivalent. CrossEntropyLoss expects class indices for the targets, while softmax_cross_entropy_with_logits can work with probability distributions.",
      "y": "They are not equivalent CrossEntropyLoss expects class indices for the targets, and softmax_cross_entropy_with_logits works with probability distributions."
   },
   {
      "x": "No module named caffe2.python",
      "z": "You are right. It looks like it was my bad. I was executing it from the pytorch dir. If I change the dir to any other it is fine.",
      "y": "Make sure you are not in pytorch dir"
   },
   {
      "x": "cuda runtime error (77) : an illegal memory access at pytorch\\aten\\src\\thcunn\\generic/SpatialClassNLLCriterion.cu",
      "z": "your `masks` are having out-of-bounds memory accesses. It's likely that `masks` has value less than 0, or greater than (nClasses-1)",
      "y": "`masks` are having out-of-bounds memory accesses"
   },
   {
      "x": "public/cuda.cmake uses cuda_select_nvcc_arch_flags",
      "z": "This PR puts the discretional inclusion of Modules_CUDA_fix to public/cuda.cmake and also installs the corresponding files. Manually verified the build process but the installation path is not tested (only did a dummy install and eyeballed that the files are there).",
      "y": " Modules_CUDA_fix to public/cuda.cmake installs the corresponding files"
   },
   {
      "x": "TypeError: __init__() got an unexpected keyword argument 'target_tensor'",
      "z": "From the [docs](https://pytorch.org/docs/stable/_modules/torch/utils/data/dataset.html#TensorDataset), there is no keyword called `target_tensor` or `data_tensor`. You can pass a variable number of arguments to the constructor\n```python\nx = torch.linspace(1, 10, 10)\ny = torch.linspace(10, 1, 10)\ndataset = Data.TensorDataset(x, y)\n```",
      "y": "there is no keyword called target_tensor, you can pass variable length argument to the constructor ```python\nx = torch.linspace(1, 10, 10)\ny = torch.linspace(10, 1, 10)\ndataset = Data.TensorDataset(x, y)\n```"
   },
   {
      "x": "Does torch.Tensor work well with share memory list?",
      "z": "Because NumPy arrays don't get moved to shared memory by default (changes in one process probably won't be seen in another one). This is expected, so I'll close the issue.",
      "y": "NumPy arrays don't get moved to shared memory by default"
   },
   {
      "x": "Loaded network with load_state_dict has different shape but works anyway",
      "z": "check for exact shape match before loading   `if input_param.shape != param.shape:\n                    # local shape should match the one in checkpoint\n                    error_msgs.append('Size mismatch: copying a param of {} from checkpoint, `",
      "y": " fixed in pr `if input_param.shape != param.shape:\n                    # local shape should match the one in checkpoint\n                    error_msgs.append('Size mismatch: copying a param of {} from checkpoint, `"
   },
   {
      "x": "Batchnorm gives different results depending on whether cudnn is enabled",
      "z": "I was not able to reproduce non-deterministic results as long as bn weights are initialized to a fixed value. You are right that in your case the expected results is 0, as expected results is (x-mean)/var*weight[+bias], and, as long as mean is exactly equal to tensor values (as it should be), result is 0. However, due to some quirks of how cudnn computes output, a small (on the order of 1 ulp) error creeps in. With the default epsilon 1/var is approx 300, weight is on the order of 1, to get the result you are getting x-mean should be on the order of 1e-5. Given that x and mean are 100, it's enough for mean to have a 1e-7 relative error (which is approx 1 ulp) to produce the results you are seeing, which may happen due to fp arithmetic being inexact. ",
      "y": "the issue might be related to fp arithmetic being inexact. "
   },
   {
      "x": "Error in variance/stdv calculations",
      "z": "There is a `unbiased` option in the `var()` and `std()` function. If you set it to `False`, the answers match.",
      "y": " use `unbiased` option in the `var()` and `std()` function to match the answers."
   },
   {
      "x": "[Bug] Failure to acquire gradient with requires_grad and backward(gradient) in cuda (with 0.4.0)",
      "z": "> x = torch.randn(2, 2, requires_grad=True).cuda(0)\n\nIf you want gradients to accumulate in `x`, you should change the above line to \n``` \nx = torch.randn(2, 2, requires_grad=True, device='cuda:0')\n```\n\nThe reason why your code doesn't work is because of \n> x = torch.randn(2, 2, requires_grad=True).cuda(0)\n\nWhat this is doing is creating a \"leaf\" tensor `(torch.randn(2, 2, requires_grad=True)` and then assigning a copy of it (that is on CUDA device 0) to `x`. Gradients can only accumulate in leaf tensors.",
      "y": " The code is creating a \"leaf\" tensor `(torch.randn(2, 2, requires_grad=True)` and then assigning a copy of it (that is on CUDA device 0) to `x`. Gradients can only accumulate in leaf tensors. If you want gradients to accumulate in `x`, you should change code to\n``` \nx = torch.randn(2, 2, requires_grad=True, device='cuda:0')\n```\n"
   },
   {
      "x": "Possible bug in KL divergence",
      "z": "the documentation of KLDivLoss specifies that the `input` already has to be a set of log-probabilities, i.e. probably the output of a LogSoftmax or Sigmoid layer...\n\n> As with NLLLoss, the input given is expected to contain log-probabilities, however unlike ClassNLLLoss, input is not restricted to a 2D Tensor, because the criterion is applied element-wise.",
      "y": "the `input`  has to be a set of log-probabilities,As with NLLLoss, the input given is expected to contain log-probabilities"
   },
   {
      "x": "[Bug] Memory leak on Convnet on CPU",
      "z": "The 15GB of RAM for this input size wouldn't surprise me, because we use the (memory consuming) unfolding of the image to perform the convolution.\nIn your example, the unfolded image has size of roughly `16 * 64 * 9 * 9 * 224 * 224 * 4` which is roughly 15GB ([exact code here](https://github.com/pytorch/pytorch/blob/master/aten/src/THNN/generic/SpatialConvolutionMM.c#L192)).\n\nThe dimensions of the input image are too big for the kernel size.",
      "y": "The dimensions of the input image are too big for the kernel size, the unfolded image has size of roughly `16 * 64 * 9 * 9 * 224 * 224 * 4` which is roughly 15GB"
   },
   {
      "x": "[Bug] Model restoration on non cuda computer",
      "z": "According to [docs](http://pytorch.org/docs/master/torch.html#torch.load), I believe you should be loading your models with\n```python\ntorch.load(file, map_location='cpu')\n```",
      "y": "Load the model with \n```python\n   torch.load(file, map_location='cpu')\n```"
   },
   {
      "x": "Runtime Error with DataLoader: exited unexpectedly",
      "z": "In GPU mode (this works)..... \n\ndataset = UP_Dataset()\ntrain_loader = DataLoader(dataset=dataset,\n                          batch_size=100,\n                          shuffle=True,\n                          num_workers=0)                              # change num_workers=0\n",
      "y": "For GPU mode we can use -\n\ndataset = UP_Dataset()\ntrain_loader = DataLoader(dataset=dataset,\n                          batch_size=100,\n                          shuffle=True,\n                          num_workers=0)                              # change num_workers=0\n"
   },
   {
      "x": "ONNX support for AdaptiveMax/AvgPool ?",
      "z": "Hi @Scitator !\nYes, as long as you are just using adaptive pooling to do global pooling you can simply define the following module:\n```\nclass MyAdaptiveMaxPool2d(nn.Module):\n    def __init__(self, sz=None):\n        super().__init__()\n       \n\n    def forward(self, x): \n        inp_size = x.size()\n        return nn.functional.max_pool2d(input=x,\n                  kernel_size= (inp_size[2], inp_size[3]))\n```\n\nI also recently opened a discussion in the forums about how to replace adaptive pooling in general:\nhttps://discuss.pytorch.org/t/how-to-replicate-f-adaptive-max-pool-using-f-max-pool/16851 \n\nCheers!\n",
      "y": "use this module \n```\nclass MyAdaptiveMaxPool2d(nn.Module):\n    def __init__(self, sz=None):\n        super().__init__()\n       \n\n    def forward(self, x): \n        inp_size = x.size()\n        return nn.functional.max_pool2d(input=x,\n                  kernel_size= (inp_size[2], inp_size[3]))\n```"
   },
   {
      "x": "ONNX export does not support parallel model converted using nn.DataParallel(model) and will throw error message \"untraced buffer\"",
      "z": "```python\nstate_dict = torch.load('/path/to/your/.pth/model')\nmodel.load_state_dict(state_dict)\nmodel.eval()\ndummy_input = Variable(torch.randn(B, C, H, W))\ntorch.onnx.export(model.module, dummy_input, '/path/to/output/onnx/model', export_params = True)\n```\nThis works for me.",
      "y": "```\nstate_dict = torch.load('/path/to/your/.pth/model')\nmodel.load_state_dict(state_dict)\nmodel.eval()\ndummy_input = Variable(torch.randn(B, C, H, W))\ntorch.onnx.export(model.module, dummy_input, '/path/to/output/onnx/model', export_params = True)\n```\nThis should work"
   },
   {
      "x": "Scalar variable's .data attribute returns a non-scalar",
      "z": "Yes, we don't have scalar tensors.  But we are merging Variable and Tensor soon and .data will essentially be a no-op.",
      "y": "There are no scalar tensors"
   },
   {
      "x": "Determining `requires_grad` automatically",
      "z": "This is now in master, see the inputs= kwarg for .backward().",
      "y": "Give inputs= kwarg for .backward()"
   },
   {
      "x": "`Interrupted system call` error appearing after updating install today",
      "z": "num_workers=0 resolved it. It definitely seems to be that dataloader error, but it didn't add too much to my running time so we're set for now. Thanks for the help!",
      "y": "give num_workers=0  in dataloader"
   },
   {
      "x": "DataLoader returning non-CUDA tensors",
      "z": "@abhaikollara It reserves an area in CPU memory so pulling tensors in that area to GPU is fast. See http://pytorch.org/docs/master/notes/cuda.html#use-pinned-memory-buffers",
      "y": "It reserves an area in CPU memory so pulling tensors in that area to GPU is fast"
   },
   {
      "x": "torch.cat behaves differently on Tensor vs Variable (also a backward compatibility issue)",
      "z": "Hey @B1azingB1ade, actually after the Tensor/Variable merge they both throw this error in pytorch master. Discussed with @colesbury and we would like to keep it requiring tuple of Tensors since that's closer to the definition of 'concatenate'.\nThe cat on single Tensor can easily be done by view/reshape. Or if you would like to use torch.cat, torch.cat(list(x), dim=1) also works. ",
      "y": "The cat on single Tensor can easily be done by view/reshape. Or by torch.cat(list(x), dim=1) ."
   },
   {
      "x": "Feature Request: More general learning-rate scheduling",
      "z": "I'm a python noob and was going crazy trying to use this \"module\".\n\n```\nAttributeError: module 'torch.optim' has no attribute 'lr_scheduler'\n```\n\nFinally got it working by using:\n\n```\nfrom torch.optim import lr_scheduler\n```\n",
      "y": "```from torch.optim import lr_scheduler```"
   },
   {
      "x": "torch.arange type",
      "z": "Currently, the only way to return `long` tensors from `arange` is by using the following constructor\n```python\nidx = torch.arange(0, 10, out=torch.LongTensor())\n```\nBut this is indeed annoying, and will hopefully be fixed when https://github.com/pytorch/pytorch/issues/1433#issuecomment-330336116 is implemented",
      "y": "We can return `long` tensors from `arange` by using the following constructor ```python idx = torch.arange(0, 10, out=torch.LongTensor()) ```"
   },
   {
      "x": "torch.zeros_like() does not work",
      "z": "I was able to use torch.zeros(a.size()) as an alternative. Thanks for the info about documentation.",
      "y": "torch.zeros(a.size())  can be used as an alternative"
   },
   {
      "x": "nn.Linear requires 2D input",
      "z": "The issue has been fixed in latest release",
      "y": "fixed in latest release"
   },
   {
      "x": "Add get_lr in ReduceLROnPlateau",
      "z": "Hi, I'm the puller of lr_scheduler.\n\nActually, `get_lr` is designed to be more like an internal function for computing instead of fetching the learning rate (although it is not going to make inconsistent outcome unless the LRs are changed externally) Sorry for the confusion.\n\nFor fetching, you could use `[ group['lr'] for group in optim.param_groups ]` for now (Yep, it's tedious). Maybe what needs to be done is to rename `get_lr` to `compute_lr` and create another function called `get_lr`. But this shortcut also seems weird, as it wraps too many functionalities of the optimizer.",
      "y": "`get_lr` is designed to be more like an internal function for computing use `[ group['lr'] for group in optim.param_groups ]`"
   },
   {
      "x": "optimizer load_state_dict() problem?",
      "z": "Try moving optimizer state to the GPU memory manually after loading it from the checkpoint.\n```model = Model()\nmodel.load_state_dict(checkpoint['model'])\nmodel.cuda()\noptimizer = optim.Adam(model.parameters())\noptimizer.load_state_dict(checkpoint['optimizer'])\nfor state in optimizer.state.values():\n    for k, v in state.items():\n        if isinstance(v, torch.Tensor):\n            state[k] = v.cuda()```\nI agree that having an optimizer.cuda() method for this operation would be nice.",
      "y": "move optimizer state to the GPU memory manually\n```model = Model()\nmodel.load_state_dict(checkpoint['model'])\nmodel.cuda()\noptimizer = optim.Adam(model.parameters())\noptimizer.load_state_dict(checkpoint['optimizer'])\nfor state in optimizer.state.values():\n    for k, v in state.items():\n        if isinstance(v, torch.Tensor):\n            state[k] = v.cuda()```"
   },
   {
      "x": "register_hook-modified gradient cannot be applied to optimizer",
      "z": "Yes, it is expected. If your look at [the formula of Adam](https://arxiv.org/pdf/1412.6980.pdf), you'll find that Adam is scaling invariant. \u200aIt means that if we have some objective function f(x) and we change it to k*f(x) (where k is some constant), there will be no effect on performance. What you did is the same as scaling the objective function by a constant (since you are requiring and modifying just `a`).",
      "y": "It is expected result, for instance Adam is scaling invariant. \u200aIt means that if we have some objective function f(x) and we change it to k*f(x) (where k is some constant), there will be no effect on performance."
   },
   {
      "x": "Behavior of __setattr__ and __getattr__ not consistent",
      "z": "I don't think this is a bug, `__getattr__` is called when an attribute lookup has not found the attribute in the usual places (i.e. it is not an instance attribute nor is it found in the class tree for self). For example, this works fine:\n\n```\nimport torch\nfrom torch import nn\nmod = nn.Module()\nmod.val = 5\nprint(mod.val)\n>>>5\ngetattr(mod, 'val')\n>>>5\n```\n\nBut this `getattr(mod, 'val2')` throws the error you reference",
      "y": " `__getattr__` is called when an attribute lookup has not found the attribute in the usual places\nFor example, this works fine:\n\n```\nimport torch\nfrom torch import nn\nmod = nn.Module()\nmod.val = 5\nprint(mod.val)\n>>>5\ngetattr(mod, 'val')\n>>>5\n```\n\nBut this `getattr(mod, 'val2')` throws the error"
   },
   {
      "x": "DataParallel doesn't replicate module's member variables",
      "z": "You can register `counter` as a buffer. Then PyTorch can take care of it.\n\n`self.register_buffer('counter', torch.zeros(1))`\n\nThis way you will get consistent outputs.",
      "y": "for consistent outputs use - \n`self.register_buffer('counter', torch.zeros(1))`\n\n"
   },
   {
      "x": "CrossEntropyLoss is negative",
      "z": "negative values exist in the latest version of nn.BCELoss().\ne.g., criterion = nn.BCELoss()\nprint(criterion(torch.zeros(5,1),torch.zeros(5,1)))\nI got: tensor(1.00000e-12 *\n       -1.0001)\n\nmy workaround:\ncriterion(torch.zeros(5,1).clamp(1e-8,1-1e-7),torch.zeros(5,1))",
      "y": "negative values exist in the latest version of nn.BCELoss().\nworkaround:\ncriterion(torch.zeros(5,1).clamp(1e-8,1-1e-7),torch.zeros(5,1))"
   },
   {
      "x": "[feature request] zeros_like and ones_like for Variables",
      "z": "We'll get these functions automatically from the ATen bindings in Variable",
      "y": "For this use ATen bindings in Variable"
   },
   {
      "x": "Add unit tests that load jit models to protect against issues with API changes",
      "z": "Closing as we now have tests that do this as well as backcompat tests for operators.",
      "y": "The tests have been already added"
   },
   {
      "x": "ValueError: only one element tensors can be converted to Python scalars",
      "z": "Had this problem with pytorch 1.1.0, fixed after upgrading to 1.2",
      "y": "Upgrad to pytorch 1.2"
   },
   {
      "x": "Tensorboard: ValueError: Duplicate plugins for name projector",
      "z": "@Biaocsu, I also met the problem but I fixed it. My method is:\n\n1. I downloaded a test script from https://raw.githubusercontent.com/tensorflow/tensorboard/master/tensorboard/tools/diagnose_tensorboard.py\n2. I run it and it told me that I have two tensorboards with a different version. Also, it told me how to fix it.\n3. I followed its instructions and I can make my tensorboard work.\n\nI think this error means that you have two tensorboards installed so the plugin will be duplicated.  Another method would be helpful that is to reinstall the python environment using conda.\n\nHope to help you.",
      "y": " this error means that you have two tensorboards installed so the plugin will be duplicated.  Another method could be helpful is to reinstall the python environment using conda"
   },
   {
      "x": "Installing pytorch nightly with pip fails",
      "z": "pip install torch_nightly --no-index -f https://download.pytorch.org/whl/nightly/cu100/torch_nightly.html\nAlso note that these kind of errors can depend on the version of pip that you use (e.g. pypa/pip#4580). The nightly is definitely present, so if it doesn't work for you please upgrade to the latest pip and try again.\n\nEDIT: finally also verified that the cp37-cp37m, cp36-cp36m and cp35-cp35m wheels are all present and all from July 24th.",
      "y": "to install pytorch nightly use - \npip install torch_nightly --no-index -f https://download.pytorch.org/whl/nightly/cu100/torch_nightly.html"
   },
   {
      "x": "Build Pytorch/Libtorch with TBB support is failing",
      "z": "Confirmed that upgrading to Cmake 3.13.3 is fixing the issue! Thanks a lot @ilia-cher",
      "y": "upgrade to Cmake 3.13.3 "
   },
   {
      "x": "Difference to numpy linspace with endpoint False",
      "z": "This is not a bug, it's simply different default for dtype:\n```python\nIn [3]: torch.linspace(0, 1, 10+1).numpy()[:-1]\nOut[3]: array([0. , 0.1, 0.2, 0.3, 0.4, 0.5, 0.6, 0.7, 0.8, 0.9], dtype=float32)\n\nIn [4]: np.linspace(0, 1, 10, endpoint=False)\nOut[4]: array([0. , 0.1, 0.2, 0.3, 0.4, 0.5, 0.6, 0.7, 0.8, 0.9])\n\nIn [5]: np.linspace(0, 1, 10, endpoint=False) - torch.linspace(0, 1, 10+1).numpy()[:-1]\nOut[5]: \narray([ 0.00000000e+00, -1.49011611e-09, -2.98023223e-09, -1.19209289e-08,\n       -5.96046446e-09,  0.00000000e+00, -2.38418578e-08,  1.19209290e-08,\n       -1.19209289e-08,  2.38418579e-08])\n\nIn [6]: np.linspace(0, 1, 10, endpoint=False) - torch.linspace(0, 1, 10+1, dtype=torch.float64).numpy()[:-1]\nOut[6]: array([0., 0., 0., 0., 0., 0., 0., 0., 0., 0.])\n```\n\nThe differences are simply of order `float32.eps`. \n\nThat said, adding an `endpoint=True` keyword is a good idea.",
      "y": "it's simply different default for dtype:\n```python\nIn [3]: torch.linspace(0, 1, 10+1).numpy()[:-1]\nOut[3]: array([0. , 0.1, 0.2, 0.3, 0.4, 0.5, 0.6, 0.7, 0.8, 0.9], dtype=float32)\n\nIn [4]: np.linspace(0, 1, 10, endpoint=False)\nOut[4]: array([0. , 0.1, 0.2, 0.3, 0.4, 0.5, 0.6, 0.7, 0.8, 0.9])\n\nIn [5]: np.linspace(0, 1, 10, endpoint=False) - torch.linspace(0, 1, 10+1).numpy()[:-1]\nOut[5]: \narray([ 0.00000000e+00, -1.49011611e-09, -2.98023223e-09, -1.19209289e-08,\n       -5.96046446e-09,  0.00000000e+00, -2.38418578e-08,  1.19209290e-08,\n       -1.19209289e-08,  2.38418579e-08])\n\nIn [6]: np.linspace(0, 1, 10, endpoint=False) - torch.linspace(0, 1, 10+1, dtype=torch.float64).numpy()[:-1]\nOut[6]: array([0., 0., 0., 0., 0., 0., 0., 0., 0., 0.])\n```\n\nThe differences are simply of order `float32.eps`. "
   },
   {
      "x": "Creating an nn.Module instance with a buffer inside the init function signature of another nn.Module and then moving the main module to CUDA results in a global CUDA device being set for internal nn.Module buffers for future module instances",
      "z": "Don't use defaults like that in `def __init__(self, x, test1 = Test1()):`.\n\nThis creates a single instance that will be passed to future constructor calls as default. This is a well-known trap when using default parameters.\n\nTry:\n```\nt = Test2(torch.ones(5), Test1()).cuda()\nt = Test2(torch.ones(5), Test1()).cuda()\n```\nand your bug will vanish.",
      "y": "Don't use defaults in `def __init__(self, x, test1 = Test1()):`\nTry:\n```\nt = Test2(torch.ones(5), Test1()).cuda()\nt = Test2(torch.ones(5), Test1()).cuda()\n```"
   },
   {
      "x": "libtorch: Error when using torch::from_blob",
      "z": "use float -> torch::kFloat32",
      "y": "use float -> torch::kFloat32"
   },
   {
      "x": "Numerically inconsistent of division between numpy and torch",
      "z": "This is a little confusing and it's happening because PyTorch's default datatype is float32 while NumPy's is float64.\n\n```\nnp.divide(416, 1080)\n# 0.3851851851851852\ntorch.div(torch.tensor((416.), dtype=torch.double), torch.tensor(1080)).item()\n# 0.3851851851851852\ntorch.div(torch.tensor((416.), dtype=torch.float), torch.tensor(1080)).item()\n# 0.385185182094574\n```\n\nWhen performing the division with the same datatype the result is the same. ",
      "y": "PyTorch's default datatype is float32 while NumPy's is float64. When performing the division with the same datatype the result will be same."
   },
   {
      "x": "unable to cast Python instance to C++ type in .backward()",
      "z": "I found the bug. My fault,  to send sparse tensors using rpc, I convert the tensor to a list containing indices, values, and size. The future being returned from the parameter server for sparse layers contained a list, not a tensor. I converted the sparse format to tensor in the callback and got it working. \n\nexample of code that works\n```python\n\n    @staticmethod\n    @rpc.functions.async_execution\n    def average_gradient(\n        ps_rref,\n        received_batch_number,\n        param_loc,\n        gradient\n    ):\n        self = ps_rref.local_value()\n        if type(gradient) is list:\n            gradient = self.sparse_rpc_format_to_tensor(gradient)\n        if not self.use_cuda_rpc:\n            gradient = gradient.cuda(self.rank)\n        fut = torch.futures.Future()\n        with self.lock:\n            if self.batch_number < received_batch_number:\n                self.batch_number = received_batch_number\n                self.clear_batch_state()\n            self.process_gradient(gradient, param_loc)\n            if param_loc not in self.futures:\n                self.futures[param_loc] = []\n            self.futures[param_loc].append(fut)\n            if len(self.futures[param_loc]) == self.trainer_count:\n                self.record_straggler_end(self.param_key(param_loc))\n                param_loc_avg = self.average(param_loc)\n                if not self.use_cuda_rpc:\n                    param_loc_avg = param_loc_avg.cpu()\n                if param_loc_avg.is_sparse:\n                    param_loc_avg = self.sparse_tensor_to_rpc_format(param_loc_avg)\n                for cur_fut in self.futures[param_loc]:\n                    cur_fut.set_result(param_loc_avg)\n                self.record_batch_end(self.param_key(param_loc))\n        return fut\n\n    @staticmethod\n    def process_bucket(state, bucket):\n        cref = state.cref\n        tensor = bucket.get_tensor()\n        tensors_count = len(cref.bucket_to_parameters(bucket))\n        sparse = tensor.is_sparse\n        if not cref.use_cuda_rpc:\n            tensor = tensor.cpu()\n        if sparse:\n            tensor = cref.sparse_tensor_to_rpc_format(tensor)\n        ps = cref.ps\n        ps_args = [\n            cref.ps_rref,\n            state.batch_number,\n            state.param_loc,\n            tensor\n        ]\n        fut = cref.send_async_request(\n            state.get_key(),\n            cref.ps_rref,\n            ps.average_gradient,\n            *ps_args\n        )\n        state.param_loc += tensors_count\n\n        def callback(fut):\n            tensor = fut.wait()\n            if type(tensor) is list:\n                tensor = cref.sparse_rpc_format_to_tensor(tensor)\n            if not cref.use_cuda_rpc:\n                tensor = tensor.cuda(cref.rank)\n            return [tensor]\n\n        return fut.then(callback)\n```\n\n\n",
      "y": " convert the sparse format to tensor in the callback\nexample -\n```python\n\n    @staticmethod\n    @rpc.functions.async_execution\n    def average_gradient(\n        ps_rref,\n        received_batch_number,\n        param_loc,\n        gradient\n    ):\n        self = ps_rref.local_value()\n        if type(gradient) is list:\n            gradient = self.sparse_rpc_format_to_tensor(gradient)\n        if not self.use_cuda_rpc:\n            gradient = gradient.cuda(self.rank)\n        fut = torch.futures.Future()\n        with self.lock:\n            if self.batch_number < received_batch_number:\n                self.batch_number = received_batch_number\n                self.clear_batch_state()\n            self.process_gradient(gradient, param_loc)\n            if param_loc not in self.futures:\n                self.futures[param_loc] = []\n            self.futures[param_loc].append(fut)\n            if len(self.futures[param_loc]) == self.trainer_count:\n                self.record_straggler_end(self.param_key(param_loc))\n                param_loc_avg = self.average(param_loc)\n                if not self.use_cuda_rpc:\n                    param_loc_avg = param_loc_avg.cpu()\n                if param_loc_avg.is_sparse:\n                    param_loc_avg = self.sparse_tensor_to_rpc_format(param_loc_avg)\n                for cur_fut in self.futures[param_loc]:\n                    cur_fut.set_result(param_loc_avg)\n                self.record_batch_end(self.param_key(param_loc))\n        return fut\n\n    @staticmethod\n    def process_bucket(state, bucket):\n        cref = state.cref\n        tensor = bucket.get_tensor()\n        tensors_count = len(cref.bucket_to_parameters(bucket))\n        sparse = tensor.is_sparse\n        if not cref.use_cuda_rpc:\n            tensor = tensor.cpu()\n        if sparse:\n            tensor = cref.sparse_tensor_to_rpc_format(tensor)\n        ps = cref.ps\n        ps_args = [\n            cref.ps_rref,\n            state.batch_number,\n            state.param_loc,\n            tensor\n        ]\n        fut = cref.send_async_request(\n            state.get_key(),\n            cref.ps_rref,\n            ps.average_gradient,\n            *ps_args\n        )\n        state.param_loc += tensors_count\n\n        def callback(fut):\n            tensor = fut.wait()\n            if type(tensor) is list:\n                tensor = cref.sparse_rpc_format_to_tensor(tensor)\n            if not cref.use_cuda_rpc:\n                tensor = tensor.cuda(cref.rank)\n            return [tensor]\n\n        return fut.then(callback)\n```\n\n\n"
   },
   {
      "x": "Unclear documentation or unintended behaviour for Lazy modules",
      "z": "Yeah, it's fixed in the nightly version. Thanks a lot @albanD @gchanan !",
      "y": "it's fixed in the nightly version"
   },
   {
      "x": "PyTorch fails to detect Intel\u00ae oneAPI Math Kernel Library during build",
      "z": "Well, particularly my issue can be quick fixed by changing\nhttps://github.com/pytorch/pytorch/blob/60931611581f7d9fa8f40baee58533955d13b8ce/cmake/Modules/FindMKL.cmake#L43\n\nto something like:\n```\n  IF (EXISTS \"/opt/intel/oneapi/mkl\")\n    SET(DEFAULT_INTEL_MKL_DIR \"/opt/intel/oneapi/mkl/latest\")\n  ELSE()\n    SET(DEFAULT_INTEL_MKL_DIR \"/opt/intel/mkl\")\n  ENDIF()\n```\nHowever, there exists also **DEFAULT_INTEL_COMPILER_DIR** and **INTEL_OMP_DIR** which points to compiler runtime and threading related files and these directories structure was changed more substantially.\nAlso, platform (Win/Lin/Mac) specific paths should be corrected as well, so I guess Intel guys, who know all internal changes that come with OneAPI framework, would be very useful.",
      "y": "issue can be quick fixed by changing\nhttps://github.com/pytorch/pytorch/blob/60931611581f7d9fa8f40baee58533955d13b8ce/cmake/Modules/FindMKL.cmake#L43\n\nto :\n```\n  IF (EXISTS \"/opt/intel/oneapi/mkl\")\n    SET(DEFAULT_INTEL_MKL_DIR \"/opt/intel/oneapi/mkl/latest\")\n  ELSE()\n    SET(DEFAULT_INTEL_MKL_DIR \"/opt/intel/mkl\")\n  ENDIF()\n```"
   },
   {
      "x": "On master branch, `test_lkj_cholesky_log_prob` fails on MacOS 10.13 with `ATEN_CPU_CAPABILITY=default`",
      "z": "Hmm, replacing `==` with `torch.allclose` fixes the issue (and difference if default codepath between computed distribution and the actual value is 1e-16 )",
      "y": " replacing `==` with `torch.allclose`"
   },
   {
      "x": "Nightly PyTorch builds can not be installed using pip-21.1.1",
      "z": "Don't you need to add the `--pre` flag in order to install nightly builds?\n\n```\npython3 -mpip install -v torch -f https://download.pytorch.org/whl/nightly/cpu/torch_nightly.html --pre\n```\n\nCan't reproduce when attempting to install with `--pre` flag:\n<details>\n<summary> macOS </summary>\n\n```\n\u276f python3 -m pip install -f https://download.pytorch.org/whl/nightly/cpu/torch_nightly.html --pre torch\nLooking in links: https://download.pytorch.org/whl/nightly/cpu/torch_nightly.html\nCollecting torch\n  Downloading https://download.pytorch.org/whl/nightly/cpu/torch-1.9.0.dev20210517-cp36-none-macosx_10_9_x86_64.whl (127.8MB)\n```\n\n</details>\n\n<details>\n<summary> Linux </summary>\n\n```\n\u276f python3 -mpip install torch -f https://download.pytorch.org/whl/nightly/cpu/torch_nightly.html --pre\n\nLooking in links: https://download.pytorch.org/whl/nightly/cpu/torch_nightly.html\nCollecting torch\n  Downloading https://download.pytorch.org/whl/nightly/cpu/torch-1.9.0.dev20210517%2Bcpu-cp38-cp38-linux_x86_64.whl (175.4 MB)\n```\n\n</details>",
      "y": "add the `--pre` flag\n```\npython3 -mpip install -v torch -f https://download.pytorch.org/whl/nightly/cpu/torch_nightly.html --pre\n```"
   },
   {
      "x": "Error when doing CUDA Conv2d with 1x1 kernel.",
      "z": "To clarify, instead of:\n\n```python\nx = Variable(torch.randn(1, 1, 100, 100))\nx.cuda()  # This creates a copy on the GPU and immediately discards it. \"x\" is still on the CPU\n```\n\nYou should write:\n\n```python\nx = Variable(torch.randn(1, 1, 100, 100).cuda())\n```\n",
      "y": "instead of:\n\n```python\nx = Variable(torch.randn(1, 1, 100, 100))\nx.cuda()  # This creates a copy on the GPU and immediately discards it. \"x\" is still on the CPU\n```\n\ntry:\n\n```python\nx = Variable(torch.randn(1, 1, 100, 100).cuda())\n```"
   },
   {
      "x": "Add CrossEntropyLoss2d",
      "z": "To me these 2d modules are confusing e.g.\n[SpatialSoftmax.py](https://github.com/pytorch/pytorch/blob/0f65c9267d5ec55584b0ec65acb5374c95af9c16/torch/legacy/nn/SpatialSoftMax.py) and [Softmax.py](https://github.com/pytorch/pytorch/blob/0f65c9267d5ec55584b0ec65acb5374c95af9c16/torch/legacy/nn/SoftMax.py) look identical. In the outer wrapper code the difference seems to be an argument check, e.g. input to the Softmax module has to be 2D and input to the Softmax2d module has to be 4d - but in the end they call the same function.\n\nIn `torch.nn.functional` there are no '2d' versions of these functions but you can simply call\n`torch.nn.functional.log_softmax` or `torch.nn.functional.softmax` with 1d,2d,3d,4d tensors.\n\nIt took me a couple of minutes to understand this and I could imagine that it confuses also other users if they port a legacy model to the new function style and search for the corresponding 2d functions. Maybe it would be worth to add a hint in the documentation?",
      "y": " In the outer wrapper code the difference seems is an argument check, e.g. input to the Softmax module has to be 2D and input to the Softmax2d module has to be 4d "
   },
   {
      "x": "Gradients are zero when run on GPU (x.cuda())",
      "z": "I was actually looking at that and was scratching my head. It didn't occur to me that .cuda() was an immutable operation adding a node to the compute graph.",
      "y": ".cuda() is an immutable operation"
   },
   {
      "x": "CrossEntropyLoss masking",
      "z": "@greydanus  since it's all PyTorch in the implementation it's fine to do this manually without any performance hits. In fact, my performance even increased quite a bit because the CrossEntropyLoss does additional heavy lifting for ignoring of certain label values - a feature that most people won't use, but will pay in performance for. This almost seems like a poor choice for the library.\n\nAnyway, assume `y_hat` is `(N,C)` class of raw scores (from a fully connected layer) and `y` are the true indices as `LongTensor`. We can do:\n\n```python\nlogp = F.log_softmax(y_hat) # get (N,C) log probabilities\n# prepare an (N,C) array of 1s at the locations of ground truth\nymask = logp.data.new(logp.size()).zero_() # (N,C) all zero\nymask.scatter_(1, y.view(-1,1), 1) # have to make y into shape (N,1) for scatter_ to be happy\nymask = Variable(ymask)\n# pluck\nlogpy = (logp * ymask).sum(1) # this hurts in my heart\n```\n\n`logpy` becomes `(N,)` Tensor of the log probabilities of the correct classes. This works because during backpropagation the sum distributes the gradients equally to all channels, and then the gradient will get blocked everywhere where `ymask == 0` in each row, and will only flow through the elements of the correct classes. And if we wanted to weigh each example differently we would do\n\n```python\nnegative_log_likelihood_loss = -(logpy * per_example_weights).mean()\n```\n\n(or even pass the weights in during the `scatter_` call)\n\n\n------------------------\n**EDIT:**\nsimple gather can be very fast:\n\n```python\nlogp = F.log_softmax(y_hat)\nlogpy = torch.gather(logp, 1, y).view(-1)\n```\n",
      "y": "simple gather can be very fast:\n\n```python\nlogp = F.log_softmax(y_hat)\nlogpy = torch.gather(logp, 1, y).view(-1)\n```"
   },
   {
      "x": "rebuild pip wheels with manylinux",
      "z": "I had the same problem and I've found the solution. Basically, pip is trying to run \"pip install torch\" because torch is listed in the dependencies and it does not detect the previously build version with wheel. So just run \"pip install --no-deps torchvision\" and it should work.\n\nAnd this could be permanently fixed by updating the setup.py file in torchvision repository.",
      "y": "run \"pip install --no-deps torchvision\" "
   },
   {
      "x": "ModuleNotFoundError: No module named 'torch._C'",
      "z": "@phenixcx the problem is that you have a folder called `torch` in the same directory which is being picked up. Do this: `cd ..` (to change directory), and then start `python` and `import torch`, it should work.",
      "y": "problem is that you have a folder called `torch` in the same directory which is being picked up.\n Do this: `cd ..` (to change directory), and then start `python` and `import torch`,"
   },
   {
      "x": "MaxUnpool2d breaks for certain shaped inputs",
      "z": "this is because MaxPool's downsampling is ambiguous in input shape (several input shapes can be mapped to the same output shape). \n\nThat's why, we provide: `output_size` as the third optional argument to the call operator: https://github.com/pytorch/pytorch/blob/master/torch/nn/modules/pooling.py#L243\n(i should document this).\n\nYou can do:\n\n```\nimport torch\nfrom torch import nn\nfrom torch.autograd import Variable\n\ndata = Variable(torch.rand(1, 3, 540, 960))\n\npool = nn.MaxPool2d(2, 2, return_indices=True)\nunpool = nn.MaxUnpool2d(2, 2)\n\nout, indices1 = pool(data)\nsize1 = out.size()\nout, indices2 = pool(out)\nsize2 = out.size()\nout, indices3 = pool(out)\nsize3 = out.size()\n\nout = unpool(out, indices3, output_size=size2)\nout = unpool(out, indices2, output_size=size1)\nout = unpool(out, indices1)\n```",
      "y": "MaxPool's downsampling is ambiguous in input shape provide: `output_size` as the third optional argument to the call operato\n```\nimport torch\nfrom torch import nn\nfrom torch.autograd import Variable\n\ndata = Variable(torch.rand(1, 3, 540, 960))\n\npool = nn.MaxPool2d(2, 2, return_indices=True)\nunpool = nn.MaxUnpool2d(2, 2)\n\nout, indices1 = pool(data)\nsize1 = out.size()\nout, indices2 = pool(out)\nsize2 = out.size()\nout, indices3 = pool(out)\nsize3 = out.size()\n\nout = unpool(out, indices3, output_size=size2)\nout = unpool(out, indices2, output_size=size1)\nout = unpool(out, indices1)\n```"
   },
   {
      "x": "Flag to check if a Module is on CUDA similar to is_cuda for Tensors",
      "z": "Alternatively (assuming your model is on a single device):\n```python\nnext(model.parameters()).is_cuda\n```",
      "y": "You can check is using \n```python\nnext(model.parameters()).is_cuda\n```"
   },
   {
      "x": "CUDNN batchnorm backprop doesn't work properly in evaluation mode",
      "z": "cudnn does not support backprop in evaluate mode, we should add a fallback to nn for this case",
      "y": "cudnn does not support backprop in evaluate mode"
   },
   {
      "x": "1>E:\\software\\libtorch\\include\\torch/csrc/utils/variadic.h(195): error C2951: \u6a21\u677f \u58f0\u660e\u53ea\u80fd\u5728\u5168\u5c40\u3001\u547d\u540d\u7a7a\u95f4\u6216\u7c7b\u8303\u56f4\u5185\u4f7f\u7528",
      "z": "Please add `::` before the usage of `std` in the corresponding line.",
      "y": "add `::` before the usage of `std`"
   },
   {
      "x": "OSError: [WinError 193] %1 is not a valid Win32 application",
      "z": "lt is likely that your environment is messed up. As you can see from the traceback, there are two python environments involved here:\n1. C:\\Users\\nouma\\AppData\\Roaming\\Python\\Python37\n2. C:\\Users\\nouma\\Anaconda3\n\nPlease make sure your `PATH` is clean and you can actually remove one of them first.",
      "y": "there are two python environments involved here,make sure your `PATH` is clean and you can actually remove one of them first."
   },
   {
      "x": "PyTorch1.2 ONNX dynamic_axis is not working",
      "z": "@gbmarc1 Thanks for adding more info.\nPlease use newer version PyTorch. This issue is already resolved.\nhttps://github.com/pytorch/pytorch/blob/8a2dcff189b7b20e1d247f36ee89b902e5be5a88/torch/onnx/utils.py#L763",
      "y": "Please use newer version PyTorch."
   },
   {
      "x": "Free Memory after CUDA out of memory error",
      "z": "> I cannot reproduce this locally\n\nI kind of expected this. I noticed that, in a notebook, it happens more frequently if the exception is not handled within the function but by the notebook environment. The modified function\n\n```python\nimport torch\n\ndef oom(raise_=False):\n    try:\n        x = torch.randn(100, 10000, device=1)\n        for i in range(100):\n            l = torch.nn.Linear(10000, 10000)\n            l.to(1)\n            x = l(x)\n    except RuntimeError as e:\n        print(e)\n        print('at iteration', i)\n        if raise_:\n            raise\n```\ncalled two times with \n```python\noom(True)\n```\nalmost always causes the error in my environment.\n\n> Could you check what happens if you call torch.cuda.empty_cache() between the two calls of oom(). You must call this in the command line and not in the oom() function itself.\n\nYes, it frees a few MB of memory from the GPU (visible in `nvidia-smi`), but the remaining few GBs still remain on the GPU. Following calls of `oom()` still result in instant failure during the first iteration. The only way I can reliably free the memory is by restarting the notebook / python command line.\n\nCan this be related to the PyTorch and CUDA versions I'm using? I am limited to CUDA 9, so I sticked to PyTorch 1.0.0 instead of the newest version.",
      "y": "free the memory by restarting the notebook / python command line."
   },
   {
      "x": "There is a small wrong mistake in the pytorch official web file",
      "z": "0.3.1 docs are frozen but it\u2019s already been fixed in master, so should be ok starting from the next release",
      "y": "0.3.1 docs are frozen but it\u2019s already been fixed in master,"
   },
   {
      "x": "gpu memory not released after run `sudo kill [pytorch process id]`",
      "z": "This is usually because some process are still alive. You can find them by doing `lsof /dev/nvidia0` and finding if any python process is listed there that should not be here. Then you can kill that process.",
      "y": " some process are still alive. You can find them by doing `lsof /dev/nvidia0`"
   },
   {
      "x": "Segmentation fault when cat list with all-empty cuda tensors",
      "z": "There's a check that should exclude zero-dim tensors from `cat` (`a.dim()` should be 0 in this case), so I'm wondering why that's not happening right now...\n\nedit: Nevermind, I was running an old build. I pulled the latest master and with `a` being an empty tensor (with shape (0,)), cat crashes.",
      "y": "`a` being an empty tensor (with shape (0,)), cat crashes"
   },
   {
      "x": "ReLU(inplace=True) seems something wrong internal",
      "z": "@sonack it's an in-place operation because `+=` is the in-place add.\nYou avoid the in-place by doing\n```python\nout = out + residual\n```\nYou can verify that with other data structures as well:\n```python\na = []\nb = a  # b is the same as a\na += [5]\nprint(b)  # prints [5]\n\n# now\na = []\nb = a  # b is the same as a\na = a + [5]\nprint(b)  # prints []\n```",
      "y": "it's an in-place operation because `+=` is the in-place add\nYou avoid the in-place by doing\n```python\nout = out + residual\n```"
   },
   {
      "x": "Crash when dividing Variable(LongTensor) by a float",
      "z": "This is probably the same issue as #5008 (the float is being cast into a long). I'll take a look into it.",
      "y": "the float is being cast into a long"
   },
   {
      "x": "nn.LSTM.cuda() leads to CuDNNError",
      "z": "Hi @jiacheng-xu , your code snippet works on my pytorch0.3.1 build. Could you try deleting ~/.nv if it exists and rerun?\n",
      "y": "try deleting ~/.nv if it exists"
   },
   {
      "x": "torch.Size doesn't accept pytorch scalar",
      "z": "@adamlerer everything should be called 'Tensor' now and nothing should be called 'Variable', so that should be fixed as well\n\nAs a workaround, you can do `torch.Size([torch.arange(10).max().long().item()])` for now",
      "y": "everything should be called 'Tensor' now and nothing should be called 'Variable'\nAs a workaround, you can do `torch.Size([torch.arange(10).max().long().item()])` "
   },
   {
      "x": "BCELoss with weights for labels (like weighted_cross_entropy_with_logits in TF)",
      "z": "I follow @velikodniy to add the Weighted BCEloss, where the weights can be computed dynamically for each batch:\n``` python\ndef weighted_binary_cross_entropy(sigmoid_x, targets, pos_weight, weight=None, size_average=True, reduce=True):\n    \"\"\"\n    Args:\n        sigmoid_x: predicted probability of size [N,C], N sample and C Class. Eg. Must be in range of [0,1], i.e. Output from Sigmoid.\n        targets: true value, one-hot-like vector of size [N,C]\n        pos_weight: Weight for postive sample\n    \"\"\"\n    if not (targets.size() == sigmoid_x.size()):\n        raise ValueError(\"Target size ({}) must be the same as input size ({})\".format(targets.size(), sigmoid_x.size()))\n\n    loss = -pos_weight* targets * sigmoid_x.log() - (1-targets)*(1-sigmoid_x).log()\n\n    if weight is not None:\n        loss = loss * weight\n\n    if not reduce:\n        return loss\n    elif size_average:\n        return loss.mean()\n    else:\n        return loss.sum()\n```\n``` python\nclass WeightedBCELoss(Module):\n    def __init__(self, pos_weight=1, weight=None, PosWeightIsDynamic= False, WeightIsDynamic= False, size_average=True, reduce=True):\n        \"\"\"\n        Args:\n            pos_weight = Weight for postive samples. Size [1,C]\n            weight = Weight for Each class. Size [1,C]\n            PosWeightIsDynamic: If True, the pos_weight is computed on each batch. If pos_weight is None, then it remains None.\n            WeightIsDynamic: If True, the weight is computed on each batch. If weight is None, then it remains None.\n        \"\"\"\n        super().__init__()\n\n        self.register_buffer('weight', weight)\n        self.register_buffer('pos_weight', pos_weight)\n        self.size_average = size_average\n        self.reduce = reduce\n        self.PosWeightIsDynamic = PosWeightIsDynamic\n\n    def forward(self, input, target):\n        # pos_weight = Variable(self.pos_weight) if not isinstance(self.pos_weight, Variable) else self.pos_weight\n        if self.PosWeightIsDynamic:\n            positive_counts = target.sum(dim=0)\n            nBatch = len(target)\n            self.pos_weight = (nBatch - positive_counts)/(positive_counts +1e-5)\n\n        if self.weight is not None:\n            # weight = Variable(self.weight) if not isinstance(self.weight, Variable) else self.weight\n            return weighted_binary_cross_entropy(input, target,\n                                                 self.pos_weight,\n                                                 weight=self.weight,\n                                                 size_average=self.size_average,\n                                                 reduce=self.reduce)\n        else:\n            return weighted_binary_cross_entropy(input, target,\n                                                 self.pos_weight,\n                                                 weight=None,\n                                                 size_average=self.size_average,\n                                                 reduce=self.reduce)\n```",
      "y": " add the Weighted BCEloss, where the weights can be computed dynamically for each batch:\n``` \ndef weighted_binary_cross_entropy(sigmoid_x, targets, pos_weight, weight=None, size_average=True, reduce=True):\n    \"\"\"\n    Args:\n        sigmoid_x: predicted probability of size [N,C], N sample and C Class. Eg. Must be in range of [0,1], i.e. Output from Sigmoid.\n        targets: true value, one-hot-like vector of size [N,C]\n        pos_weight: Weight for postive sample\n    \"\"\"\n    if not (targets.size() == sigmoid_x.size()):\n        raise ValueError(\"Target size ({}) must be the same as input size ({})\".format(targets.size(), sigmoid_x.size()))\n\n    loss = -pos_weight* targets * sigmoid_x.log() - (1-targets)*(1-sigmoid_x).log()\n\n    if weight is not None:\n        loss = loss * weight\n\n    if not reduce:\n        return loss\n    elif size_average:\n        return loss.mean()\n    else:\n        return loss.sum()\n```\n``` \nclass WeightedBCELoss(Module):\n    def __init__(self, pos_weight=1, weight=None, PosWeightIsDynamic= False, WeightIsDynamic= False, size_average=True, reduce=True):\n        \"\"\"\n        Args:\n            pos_weight = Weight for postive samples. Size [1,C]\n            weight = Weight for Each class. Size [1,C]\n            PosWeightIsDynamic: If True, the pos_weight is computed on each batch. If pos_weight is None, then it remains None.\n            WeightIsDynamic: If True, the weight is computed on each batch. If weight is None, then it remains None.\n        \"\"\"\n        super().__init__()\n\n        self.register_buffer('weight', weight)\n        self.register_buffer('pos_weight', pos_weight)\n        self.size_average = size_average\n        self.reduce = reduce\n        self.PosWeightIsDynamic = PosWeightIsDynamic\n\n    def forward(self, input, target):\n        # pos_weight = Variable(self.pos_weight) if not isinstance(self.pos_weight, Variable) else self.pos_weight\n        if self.PosWeightIsDynamic:\n            positive_counts = target.sum(dim=0)\n            nBatch = len(target)\n            self.pos_weight = (nBatch - positive_counts)/(positive_counts +1e-5)\n\n        if self.weight is not None:\n            # weight = Variable(self.weight) if not isinstance(self.weight, Variable) else self.weight\n            return weighted_binary_cross_entropy(input, target,\n                                                 self.pos_weight,\n                                                 weight=self.weight,\n                                                 size_average=self.size_average,\n                                                 reduce=self.reduce)\n        else:\n            return weighted_binary_cross_entropy(input, target,\n                                                 self.pos_weight,\n                                                 weight=None,\n                                                 size_average=self.size_average,\n                                                 reduce=self.reduce)\n```"
   },
   {
      "x": "Unable to build from latest master - nccl.h not found",
      "z": "fixed by the linked PR. Please reopen if there are further issues",
      "y": "This issue is fixed"
   },
   {
      "x": "ONNX export runtime error - tuple appears in op that does not forward tuples",
      "z": "@houseroad \n\nthe problem was solved after removing `DataParallel`.",
      "y": "This can be solved by removing removing `DataParallel`."
   },
   {
      "x": "Cannot convert certain empty tensors from numpy",
      "z": "> the error still persists. Using PyTorch 1.0.1.post2\n\nIt seems the fix is in master:\nhttps://github.com/pytorch/pytorch/blob/master/aten/src/ATen/native/Resize.h#L37-L45\nBut not in 1.0.1:\nhttps://github.com/pytorch/pytorch/blob/v1.0.1/aten/src/ATen/native/Resize.h#L37-L41\nSo you'll need to use a nightly build or include a workaround in your code.",
      "y": "you'll need to use a nightly build"
   },
   {
      "x": "Default chosen cuDNN convolution algorithm for V100 uses twice as much memory",
      "z": "This is pretty old. I'm OK with just calling it fixed and someone file a new bug if there's still problems.",
      "y": "Bug is fixed"
   },
   {
      "x": "When CUDA 10 support is planned?",
      "z": "We're running CI builds against CUDA 10, and I believe the current plan for pre-built binaries is the next release.",
      "y": "It has been released"
   },
   {
      "x": "Chaining Operations - Misunderstanding?",
      "z": "after masking your tensor have less elements in method (1) and more elements with a bunch of zeros in method (2). the denominator is different.",
      "y": "after masking your tensor have less elements in method (1) and more elements with a bunch of zeros in method (2)"
   },
   {
      "x": "Suppress Scientific Notation",
      "z": "This issue has been resolved by https://github.com/pytorch/pytorch/pull/16876 and `torch.set_printoptions(sci_mode=True)`. Closing this issue.",
      "y": "Issue can be resolved using `torch.set_printoptions(sci_mode=True)`."
   },
   {
      "x": "torch.sigmoid behaves inconsistently for 32- and 64-bit NaN inputs",
      "z": "sigmoid() doesn't use avx_mathfun.h anymore. https://github.com/pytorch/pytorch/pull/7341 switched `Vec256<float>::exp()` to use Sleef_expf8_u10 (and Sleef has proper handling of NaN).\n\nThe PR that changed `Vec256<float>::exp()` predates this issue, but wasn't in the stable release (0.4.1) at the time.\n\nNote that the open linked Sleef issue is not about NaN handling. It's about about Sleef functions not getting inlined, which hurts perf (and was the reason we were using avx_mathfun.h in 0.4.1)",
      "y": "sigmoid() doesn't use avx_mathfun.h anymore."
   },
   {
      "x": "What is the net *.pb file encoding?",
      "z": "I figured this out. I just needed to read the file in binary mode, ie: mode='rb'",
      "y": "read the file in binary mode"
   },
   {
      "x": "C++ API, IValue toTensor() didn't work",
      "z": "> Could we get a repro script? (and `torchscript_version.pt` or the python source for it). The module's output might be an IValue that is a list, could you check if it contains the output you are looking for?\n\nYes, the output is a list. And I call `.toTuple()` `->elements()` `.toTensor()` and it works. Thanks for your remind.",
      "y": "call `.toTuple()` `->elements()` `.toTensor()` "
   },
   {
      "x": "Assigning a parameter to an indexed tensor that was produced by DDP no longer works in torch nightly (1.7)",
      "z": "Hi,\n\nAfter looking into this her are a few comments:\n- You should not modify the input to your net when you use DDP. As this input will sometimes be a view of the original input and sometimes not. So you will end up modifying your dataset for some samples at each forward which is most likely not what you want to do.\n- @mrshenli will look into removing the Scatter op that creates a view here as it is not always needed. This will improve perf and remove this error (even though the warning on the previous point will still apply)\n- I will send a patch to fix the internal assert error that happens because we do an inplace op where the input that is modified does not require grads but the other does. And so the inplace checks do no run as it only check for the first input's requires_grad to know if the function is differentiable.",
      "y": "You should not modify the input to your net when you use DDP"
   },
   {
      "x": "libtorch_cpu.a is huge (1 GB) with build_android.sh",
      "z": "Thanks for the fix! '-g0' flag solved the problem. It reduced overall size by ~460 MB which is perfect.\nHad no clue about NDK generating debug symbols for release builds behind the scenes, now it makes sense why the resulting binary was huge.\n\nWe do use both \"--whole-archive\" and \"--gc-sections\" at the same time. And we will consider limiting ops we use for our modules to trim down size even more.\nThanks for the quick and thorough response!",
      "y": " '-g0' flag solves the problem"
   },
   {
      "x": "`__torch_function__` does not get call for torch.Tensor's `__getitem__` syntax",
      "z": "In the case where we run into this, `MyTensor` is not a Tensor (nor does it contain one, it is a symbolic representation of a Tensor), so trying to use `as_subclass` isn't really viable. When I looked at the code for handling `__getitem__` in C++ it looks like it is only checking `self` for `__torch_function__`, but is ignoring the potential `__torch_function__` on the index variable.",
      "y": " `MyTensor` is not a Tensor so trying to use `as_subclass` isn't really viable"
   },
   {
      "x": "I'm compiling the pytorch 1.6.0 stable version on CUDA 11.1 and CUDNN 8.04 with 3090 and failed.",
      "z": "> > 1.6 does not support CUDA 11, please use a newer version\n> \n> Hi, may I get to know what version will be capable to use with cuda 11.1?\n\ngeiniyigelianjie\n\n[https://github.com/pytorch/pytorch/issues/45028]\n\nneirongdazhiruxia:\n1. shiyong nvidia-docker cuda11.0-devel\n```\nREPOSITORY                         TAG                             IMAGE ID            CREATED             SIZE\nnvidia/cuda                        11.0-cudnn8-devel-ubuntu16.04   1741134981df        4 days ago          7.46GB\n```\n2. anzhuang anaconda3\n```\napt-get update\napt-get install wget\nwget https://repo.anaconda.com/archive/Anaconda3-2020.07-Linux-x86_64.sh\nchmod +x Anaconda3-2020.07-Linux-x86_64.sh\n./Anaconda3-2020.07-Linux-x86_64.sh\n```\n3. anzhuang \"pytorch nightly\" (kenengxuyaofanqiang)\n\n`conda install pytorch torchvision cudatoolkit=11 -c pytorch-nightly`\n\n",
      "y": "1.6 does not support CUDA 11, please use a newer version"
   },
   {
      "x": "How can I fix NAN loss (or very large MSE losses)?",
      "z": "Please use [PyTorch forum](https://discuss.pytorch.org/) for this sort of questions. Higher chance of getting answers there.\n\nBtw, from what I see (didnt went through the code thoroughly) you are not iterating through the dataloader properly. Basically, you use the 1st batch over and over (`iter` gives you an iterator, and with `next` you get the first chunk).\n\nInstead of:\n```python\nfor step in range(1, len(train_loader) + 1):\n    batch = next(iter(train_loader)) # <- issue here\n    ...\n```\ntry:\n```python\nfor step, batch in enumerate(train_loader):\n    ...\n```\n\nLet me know if this solves the issue.",
      "y": "Instead of:\n```\nfor step in range(1, len(train_loader) + 1):\n    batch = next(iter(train_loader)) # <- issue here\n    ...\n```\ntry:\n```\nfor step, batch in enumerate(train_loader):\n    ...\n```"
   },
   {
      "x": "Add `inputs` argument to `autograd.backward()`",
      "z": "SGTM",
      "y": "feature added"
   },
   {
      "x": "JIT LibTorch: Profiling Mode Enabled causes memory leak",
      "z": "Hi, thanks for the issue. Do you have a repro you could share by any chance ?",
      "y": "Bug is fixed"
   },
   {
      "x": "Warnings during compiling: floating-point value does not fit in required integral type",
      "z": "These compiler-warnings arise during compilation of the templated function uniform_int(). The warnings are misleading because they arise from the way the compiler compiles templated functions, but the if-else statements in the function obviate the possibilities that the warnings describe. So, the purpose of a fix would be to fix the compiler warnings, and not to fix any sort of a bug.",
      "y": "compiler-warnings arise during compilation of the templated function uniform_int()"
   },
   {
      "x": "Couldn't build multi scaled kernel nested model",
      "z": "@Jimut123 please post this question to https://discuss.pytorch.org/\nBut if you believe this is a bug in PyTorch, please do not hesitate to re-open the issue.",
      "y": "The issue is shared on pytorch discuss"
   },
   {
      "x": "torch.median returns the smaller element when the median value lies between two elements.",
      "z": "Aside from the indices, we might need some special handling for the gradient if we'd like to take the mean of the two middle elements. ",
      "y": " need some special handling for the gradient if we'd like to take the mean of the two middle elements. "
   },
   {
      "x": "Deadlock with shared CUDA tensors and multiprocessing (spawn)",
      "z": "Thanks Sam I have fix for it. Will submit PR shortly.",
      "y": "This issue is fixed"
   },
   {
      "x": "throw error when EmbeddingBag(..., sparse=True) and slice the embedding's weight",
      "z": "@albanD \n> The thing is that even slice forward does not support it. I think this is mainly because slice returns views but returning a view for this sparse Tensor might not be possible.\n\nI don't think forward is related though. The problem is `grad_out` is sparse. When the dense view receives a sparse gradient, it should be able to backpropagate through slice okay (by offsetting the indices). ",
      "y": " `grad_out` is sparse, When the dense view receives a sparse gradient, it should be able to backpropagate through slice by offsetting the indices"
   },
   {
      "x": "torch.scatter_ returns incorrect result when running on CPU using int64 indexes",
      "z": "This is fixed on master, see #38646 ",
      "y": "This issue is fixed"
   },
   {
      "x": "Required some argument in dataloader for setting randomstate",
      "z": "The RNG consumed by [`RandomSampler`](https://github.com/pytorch/pytorch/blob/479b04e26a59d20b72ad5fdaec819caaad49af75/torch/utils/data/sampler.py#L68) (created with `shuffle=True`) is the default RNG. So seeding the default RNG will make the same code return same ordering.\n\nIf you want the ordering the be the same with different code (really code that consumes different amount of RNG), for now you can create a custom `RandomSampler` class that uses a given RNG state. But I agree that we should really add a `generator=` kwarg for both `DataLoader` and all random samplers.",
      "y": " you can create a custom `RandomSampler` class that uses a given RNG state"
   },
   {
      "x": "masked_fill_ (and possibly others) produces a different output than masked_fill on cpu",
      "z": "You are trying to do an inplace operation on a self-overlapping tensor that results from `expand`. This is not going to work. Results on the GPU are different because when you are copying tensor to the GPU it becomes contiguous and not self-overlapping. The only thing we can do here is issue a warning or disallow inplace masked_fill on a self-overlapping tensor.",
      "y": "You are trying to do an inplace operation on a self-overlapping tensor that results from `expand`."
   },
   {
      "x": "The error of `torch.nn.SyncBatchNorm.convert_sync_batchnorm`",
      "z": "sorry, I fix the issue that I create optimizer before exec `model = torch.nn.SyncBatchNorm.convert_sync_batchnorm(model)`",
      "y": "The bug is fixed"
   },
   {
      "x": "Training using \u201cmp.spawn\u201d, can not reproduce the training results",
      "z": "seed in worker function, not main. the process that runs training never executed main.",
      "y": "seed in worker function, not main"
   },
   {
      "x": "nonzero function in C++ much slower than python(CUDA)",
      "z": "@glaringlee\nThanks.  \nIt's right that sleep doesn't matters.But it really helps me to find the problem. I find the problem is in C++ model infer is also asynchronous, so it's wrong to measure nonzero function without synchronize with cuda after the model inferrence. I will write a new issue about how to call cudaDeviceSynchronize() in torch C++.",
      "y": "the problem is in C++ model infer is also asynchronous, so it's wrong to measure nonzero function without synchronize with cuda after the model inferrence"
   },
   {
      "x": "[SSL: CERTIFICATE_VERIFY_FAILED] For Inception Resnet V2 on Google Colab",
      "z": "PyTorch has nothing to do with this, it's pretrained-models's hosting issue, maybe [this one](https://github.com/Cadene/pretrained-models.pytorch/issues/193). The workaround mentioned in the comments works btw",
      "y": "it's pretrained-models's hosting issue"
   },
   {
      "x": "Add custom request headers to torch.hub.download_url_to_file",
      "z": "Hi @pmeier, we'll happily accept a PR that adding `headers={}` to `download_url_to_file` API. Would you mind sending a PR for it? Please feel free to request me as reviewer there. Thanks! ",
      "y": "Added to the pr"
   },
   {
      "x": "Missing info in Tensorboard's add_graph()",
      "z": "Duplicate of #37415\n\nI think this issue can be closed. Looks to me as if this was addressed and closed with PR #37504.",
      "y": "Bug is fixed"
   },
   {
      "x": "torch.split(..) / torch.chunk(..) does not remove one dimension from tensor in some cases",
      "z": "Neither chunk nor split are expected to remove a dimension, because they are not guaranteed to produce chunks that can be seen as tensors with fewer dimensions than the source. You probably want torch.unbind. ",
      "y": "Neither chunk nor split are expected to remove a dimension use torch.unbind. "
   },
   {
      "x": "Nightly build includes absolute path in cmake file",
      "z": "I looked into this a little and don't think it's a problem. Those libraries are only linked to statically, so the built binaries don't look for them when installed on your machine. Checking the ldd and objdump outputs on some built linux binaries confirms that there are no unexpected cuda libraries that need to be linked to",
      "y": "there are no unexpected cuda libraries that need to be linked to"
   },
   {
      "x": "torch.nn.utils.rnn.pack_padded_sequence segment fault if not in decreasing order",
      "z": "cc @nairbv",
      "y": "Bug is fixed"
   },
   {
      "x": "l1loss different results based on arguments position",
      "z": "fixed on master",
      "y": "This issue is fixed in master"
   },
   {
      "x": "How do you convert the tensor to a float",
      "z": "`x.data<float>()` and note this only works if it's contiguous",
      "y": "`x.data<float>()` "
   },
   {
      "x": "the running time difference between the ATen library and pytorch ?",
      "z": "FWIW, your Python script only times how long it takes to schedule the operations for execution. You need to synchronize on the device before the `end = time.time()` call, e.g., with `torch.cuda.synchronize()`.",
      "y": "You need to synchronize on the device before the `end = time.time()` call, e.g., with `torch.cuda.synchronize()`."
   },
   {
      "x": "Feature request:  'concat' for Variable",
      "z": "Don't call the function directly, use [`torch.cat([var1, var2, var3], dim)`](http://pytorch.org/docs/torch.html#torch.cat).",
      "y": "Don't call the function directly, use [`torch.cat([var1, var2, var3], dim)`]"
   },
   {
      "x": "torch.load() and torch.save() of big tensors is slow due to tar",
      "z": "I've rewritten `load` and `save` to no longer use tar. It's 5-10x faster on my machine now, basically matching the speed of `cat` for simple files. I have to write some tests before sending it out but you can assign this to me.",
      "y": "rewritten `load` and `save` to no longer use tar. It's 5-10x faster"
   },
   {
      "x": "LSTM output dimensions",
      "z": "The way I handle this is by making each RNN layer of size 1, and using this after each BRNN:\n\n```python\n # (TxNxD*2) -> (TxNxD) by sum\nif self.bidirectional:\n     x = x.view(x.size(0), x.size(1), 2, -1).sum(2).view(x.size(0), x.size(1), -1) \n```\nNot as simple as just taking the output of the BRNN but hopefully that helps...",
      "y": "making each RNN layer of size 1, and using this after each BRNN solves this \n ```python # (TxNxD*2) -> (TxNxD) by sum if self.bidirectional: x = x.view(x.size(0), x.size(1), 2, -1).sum(2).view(x.size(0), x.size(1), -1) ```"
   },
   {
      "x": "Resize gradients computed by basic math Functions to match original input sizes",
      "z": "this was fixed at some point, we forgot to close the issue.",
      "y": "This issue is fixed"
   },
   {
      "x": "Backprop issue I can't figure out",
      "z": "Yup, confirmed to be a cuDNN problem (costs after 1,5k iter):\n\n|       | CPU  | GPU  | cuDNN |\n|-------|------|------|-------|\n| `hn1` | 0.27 | 0.26 | 0.67  |\n| `hn2` | 0.28 | 0.28 | 0.27  |",
      "y": "cuDNN problem"
   },
   {
      "x": "torch.cat bug/unexpected behavior",
      "z": "Sure, just wanted to provide little context :)",
      "y": "bug is fixed"
   },
   {
      "x": "[RFC] Join-based API to support uneven inputs in DDP",
      "z": "If users would like to handle uneven inputs, they can try different ways, as @mrshenli  mentioned in the forum: 1) if know the number of inputs, they will know there is uneven inputs or not, and drop the tail data  2) if not know the number of input, they can pipeline data reading and identify end of data on their own, then drop the tail data\n\nFor DDP API, I think this proposal provides another option (continue training for uneven inputs) for users and will be useful for quite a lot of cases, so voting for keep current design in this proposal. \n",
      "y": " keeping current design in this proposal"
   },
   {
      "x": "Scripted Model gave totally wrong result on iOS, but was correct on C++ frontend",
      "z": "Hi @seeker-Liu, I've received your demo project from @xta0 and built & run it. I compared the value of the `ans` (the variable `ans` from your iOS code and the variable `ans` from your C++ code), as well as the value of each `onset_accessor[i][j]` and `frame_accessor[i][j]`. The error between them can be almost ignored, the average error is about 10e-8. \n\nThe error calculation is attached. \n\n- `result_cxx.txt`: values of `onset_accessor` (column 1) and `frame_accessor` (column 2) from C++\n- `result_ios.txt`: values of `onset_accessor` (column 1) and `frame_accessor` (column 2) from iOS \n- You can run `python main.py` to see the result, please make sure your `pandas` has been installed, normally it can be installed by `pip install pandas`\n\nresult:\n```\nonset average error: 1.6914125348246035e-09\nframe average error: 2.9308559916846613e-08\n```\n\n[err_calc.zip](https://github.com/pytorch/pytorch/files/4685906/err_calc.zip)\n",
      "y": "The error between them can be almost ignored, the average error is about 10e-8. \n\nThe error calculation is attached. \n\n- `result_cxx.txt`: values of `onset_accessor` (column 1) and `frame_accessor` (column 2) from C++\n- `result_ios.txt`: values of `onset_accessor` (column 1) and `frame_accessor` (column 2) from iOS \n- You can run `python main.py` to see the result, please make sure your `pandas` has been installed, normally it can be installed by `pip install pandas`\n"
   },
   {
      "x": "Can't create tensor from tensor list, but tensor from numpy arrays list works fine",
      "z": "Cool! I'll do my best on Saturday! Thanks for incentivizing! (:",
      "y": "Bug is fixed"
   },
   {
      "x": "ValueError: can't optimize a non-leaf Tensor",
      "z": "Please ask questions on the forums, discuss.pytorch.org. ",
      "y": "Solution can be found in pytorch discuss"
   },
   {
      "x": "internal assert failed bug due to argmax gradfn",
      "z": "Seems to work in the nightly: `pip3 install --upgrade --pre torch torchvision -f https://download.pytorch.org/whl/nightly/cu102/torch_nightly.html`",
      "y": "works in the nightly: `pip3 install --upgrade --pre torch torchvision -f https://download.pytorch.org/whl/nightly/cu102/torch_nightly.html`"
   },
   {
      "x": "Adding typing_extensions as a dependency of pytorch",
      "z": "Looks like it should be pretty straightforward, it's pure Python and packaged in conda as well, so don't see any major stumbling blocks. I'll give it a go.",
      "y": "Bug is fixed"
   },
   {
      "x": "Docs of KLDivLoss seems incorrect",
      "z": "The `x_n` seems to be accepted in log-probs format while `y_n` in probs format:\n```\nAs with NLLLoss, the input given is expected to contain log-probabilities and is not restricted to a 2D Tensor. The targets are given as probabilities (i.e. without taking the logarithm).\n```\nso there seems no error in the formula",
      "y": "The `x_n` seems to be accepted in log-probs format while `y_n` in probs format:\n```\nAs with NLLLoss, the input given is expected to contain log-probabilities and is not restricted to a 2D Tensor. The targets are given as probabilities (i.e. without taking the logarithm).\n```"
   },
   {
      "x": "Store TORCH_CUDA_ARCH_LIST in torch and use it for C++ extensions",
      "z": "In the meantime, one can use [CUDA bin utils](https://docs.nvidia.com/cuda/cuda-binary-utilities/index.html#cuobjdump) to determine this via scripting.  A developer could verify the list upon deploying the dependency before compiling:\n```\n$ cuobjdump ~/.local/lib/python3.6/site-packages/torch/lib/libtorch_cuda.so -lelf | awk -F. '{print $3}' | grep sm | sort -u\nsm_35\nsm_37\nsm_50\nsm_60\nsm_61\nsm_70\nsm_75\n```",
      "y": " one can use [CUDA bin utils](https://docs.nvidia.com/cuda/cuda-binary-utilities/index.html#cuobjdump) to determine this via scripting\n```\n$ cuobjdump ~/.local/lib/python3.6/site-packages/torch/lib/libtorch_cuda.so -lelf | awk -F. '{print $3}' | grep sm | sort -u\nsm_35\nsm_37\nsm_50\nsm_60\nsm_61\nsm_70\nsm_75\n```"
   },
   {
      "x": "Crash on exit in Python 3.9.0a6",
      "z": "Sounds like a duplicate of https://github.com/pytorch/pytorch/issues/50014 (destruction of GIL in autograd threads)",
      "y": "destruction of GIL in autograd threads"
   },
   {
      "x": "AssertionError: tensor(1.3351e-05) not less than or equal to 1e-05 :",
      "z": "This should have been fixed in 1.5, refer #34764.",
      "y": "This issue is fixed"
   },
   {
      "x": "Installation on windows python 3.8 32bits",
      "z": "@chenchangquan5 `set USE_MKLDNN=0`.",
      "y": " `set USE_MKLDNN=0`"
   },
   {
      "x": "jit much slower with pytorch 1.5 (on custom rnns)",
      "z": "@vincentqb  \n```python\n        torch._C._jit_set_profiling_executor(False)\n        torch._C._jit_set_profiling_mode(False)\n```\n\nAlso, please see for more details:\n\nhttps://github.com/pytorch/pytorch/blob/master/benchmarks/fastrnns/bench.py#L212-L241",
      "y": "```python\n        torch._C._jit_set_profiling_executor(False)\n        torch._C._jit_set_profiling_mode(False)\n```"
   },
   {
      "x": "Implementing deg2rad, rad2deg",
      "z": "Cool!\n\nTake a look at how similar functions, like sin, are implemented. See: https://github.com/pytorch/pytorch/blob/master/aten/src/ATen/native/UnaryOps.cpp for the device-independent part of the code. Then you have the CPU and CUDA-specific parts in ATen/native/cpu and ATen/native/cuda, respectively. \n\n",
      "y": "you have the CPU and CUDA-specific parts in ATen/native/cpu and ATen/native/cuda, respectively. \n"
   },
   {
      "x": "Implementing HyperLSTM",
      "z": "I see, that makes sense. Thanks for the response @mruberry.\n\n> If, however, there's some functionality that HyperLSTM needs that's tricky to implement in PyTorch, then it makes sense to file a more focused issue requesting that functionality.\n\nThere is one thing, however I don't know if I should open another issue or if it's not really worth discussing.\n\nInside the hyperLSTM, two regular LSTM's exist. However, one of them needs LayerNormalization applied to the variable `c` (the long term memory) before it is multiplied by the output gate. The original paper on LayerNormalization also describes the same technique. \n\nIs it worthy to open another issue to request this feature be implemented (a boolean `layerNorm` passed to the LSTM)? Or is this is a niche use-case where it would be better off if I just implement my own variant of an LSTM that includes LayerNormalization.",
      "y": "The  feature will be added"
   },
   {
      "x": "Is grad attribute in Variable immutable?",
      "z": "This is out of date. You can assign it now and it works as expected.",
      "y": "You can assign it now and it works as expected"
   },
   {
      "x": "CuDNN ConvTranspose1d issue with transposed cuda Variable (can't convert to contiguous)",
      "z": "This has been fixed on master:\n![image](https://user-images.githubusercontent.com/5652049/32459250-fd587eaa-c2fc-11e7-8dd3-ca35df49f98d.png)\nYou can build pytorch from source to avoid this bug or wait for the next release.",
      "y": "This has been fixed on master"
   },
   {
      "x": "Unable to use cat on torch.LongTensor",
      "z": "@aleSuglia it's not a bug, `torch.zeros(x.size())` returns a `torch.FloatTensor` and `x` is a `torch.LongTensor`. You need to convert `x` to `long()` as well.\n\n`torch.cat((x, torch.zeros(x.size()).type_as(x)))` or alternatively `torch.cat((x, x.new(*x.size()).fill_(0)))`",
      "y": "`torch.zeros(x.size())` returns a `torch.FloatTensor` and `x` is a `torch.LongTensor`. You need to convert `x` to `long()` as well."
   },
   {
      "x": "Low performance issue on CPU-only machine",
      "z": "@SsnL I noticed that the other modifications also have similar bugs.  \n\n- ger operation \n[Intel manual](https://software.intel.com/en-us/mkl-developer-reference-c-cblas-ger) explains that \n~~~\nFor Layout = CblasColMajor, the value of lda must be at least max(1, m).\n\nFor Layout = CblasRowMajor, the value of lda must be at least max(1, n).\n~~~\nHowerver, the modification does not check the transposing info and give the limit condition\n~~~\n(lda >= THMax(1, n)) && (lda <= INT_MAX) &&\n~~~\n\n- gemv operation\n[Intel manual](https://software.intel.com/en-us/mkl-developer-reference-c-cblas-gemv) explains that\n~~~ \nFor Layout = CblasColMajor, the value of lda must be at least max(1, m).\n\nFor Layout = CblasRowMajor, the value of lda must be at least max(1, n).\n~~~\nA similar bug also exsits.\n~~~\n(lda >= THMax(1, n)) && (lda <= INT_MAX) &&\n~~~",
      "y": "bug is fixed"
   },
   {
      "x": "indexing operator `[]` inconsistent",
      "z": "> Yes, indexing with a list of indices will never return a view (even if they form a consecutive range)\n\nOk, understood.\n\n> This works fine for me.\n> What's your PyTorch version?\n```\n(root) ~ $ cat /tmp/test3.py\nimport torch\n\nidxes = torch.LongTensor([0, 2])\na = torch.rand(5, 4)\na[idxes, 1:2] = 7  # gives exception\nprint('a', a)\n\n(root) ~ $ python /tmp/test3.py\nTraceback (most recent call last):\n  File \"/tmp/test3.py\", line 5, in <module>\n    a[idxes, 1:2] = 7  # gives exception\nTypeError: indexing a tensor with an object of type torch.LongTensor. The only supported types are integers, slices, numpy scalars and torch.LongTensor or torch.ByteTensor as the only argument.\n\n(root) ~ $ conda list | grep sou\npytorch                   0.2.0                py36_1cu75    soumith\ntorchvision               0.1.9                    py36_1    soumith\n```\n",
      "y": "indexing with a list of indices will never return a view"
   },
   {
      "x": "torch.svd() get error, w/ CUDA 9.0 w/o MAGMA",
      "z": "@zhanghang1989 now we have magma for cuda90 with `conda install magma-cuda90 -c soumith`",
      "y": "now we have magma for cuda90 with `conda install magma-cuda90 -c soumith`"
   },
   {
      "x": "Installing from source failing on Ubuntu 17.10",
      "z": "@vickylance \n\n1) Install CUDA and CUDNN from the respective .deb files.\n2) Add a PPA that contains the version of gcc/g++ that you want: ```sudo add-apt-repository ppa:ubuntu-toolchain-r/test```.\n3) ```sudo apt install g++-6 gcc-6```\n3) Set $CC and $CXX to point to the right version of the tools.\n4) Clone pytorch, install with ```python setup.py install```",
      "y": "1) Install CUDA and CUDNN from the respective .deb files.\n2) Add a PPA that contains the version of gcc/g++ that you want: ```sudo add-apt-repository ppa:ubuntu-toolchain-r/test```.\n3) ```sudo apt install g++-6 gcc-6```\n3) Set $CC and $CXX to point to the right version of the tools.\n4) Clone pytorch, install with ```python setup.py install```"
   },
   {
      "x": "CrossEntropyLoss for 3D and higher",
      "z": "agreed, would be good to have. We'll get this done.",
      "y": "feature added"
   },
   {
      "x": "undefined symbol: cudnnSetConvolutionGroupCount while running with cuDNN 7.0.3 and CUDA 9",
      "z": "There turned out to be sneaky conda package.\nFixed by doing `conda uninstall cudnn` and recompiling.",
      "y": "Fixed by doing `conda uninstall cudnn` and recompiling."
   },
   {
      "x": "[Feature Request] Make \"forward\" handle large inputs in batches",
      "z": "I'm not sure we want to add such convenience function (but others might have different opinions).\nI think that there can be weird edge cases (for example, the batch might not be the first dimension as in some rnns I think), and for the user it can be implemented in one line\n```python\ntorch.cat([model(x) for x in input.split(batch_size, 0)], 0)\n```\nWhat do you think?",
      "y": "there can be weird edge cases (for example, the batch might not be the first dimension as in some rnns I think), and for the user it can be implemented in one line\n```\ntorch.cat([model(x) for x in input.split(batch_size, 0)], 0)\n```"
   },
   {
      "x": "Sort sequences internally in pack_padded_sequence",
      "z": "Would really love to have this feature! Has this been implemented yet?",
      "y": "feature added"
   },
   {
      "x": "Documentation: Indexing output from bidirectional RNN (GRU,LSTM)",
      "z": "From what I've understand so far about non `Cell` suffixed CuDNN variants:\n\n1. You need to give them a tensor where the sequences are sorted by length in a decreasing order. If the sequences are guaranteed to always have the same length, you can skip this step. Otherwise, cf. `pack_padded_sequence()`\n2. After calling the RNN, you receive a tuple of 2 items: **packed** `Variable` of all hidden states (`hs`) and a normal `Variable` containing the last hidden states (`ht`).\n3. For `hs` you unpack it using `pad_packed_sequence()` to get a normal `Variable`.\n4. `ht` contains the **correct** forward and backward states for each sequence so you don't have to do something to recover or mask out 0's etc **but** this tensor does not concatenate the forward-backward states although `hs` returns them in a concatenated fashion.\n\n```python\n# An input of 5 timesteps and 2 sequences. The shorter one is 0-padded.\n# hidden_dim = 3, bidirectional=True, num_layers=1\nIn [525]: input_\nOut[525]:\n\n 1  3\n 3  5\n 3  2\n 2  0\n 1  0\n[torch.LongTensor of size 5x2]\n\n# hs and ht are the return values of GRU here (for LSTM you'll also have c_t)\nIn [526]: print(hs[:, 1], ht[:, 1])\n\nVariable containing:\n   (( forward states ))                      (( backward states ))\n-0.0982  0.0275 -0.3005            0.3609 -0.4958  0.3408\n-0.1710 -0.0576 -0.3759            0.2550 -0.3478  0.2796\n-0.1935  0.0484 -0.4111            0.2088 -0.2813  0.1440\n 0.0000  0.0000  0.0000            0.0000  0.0000  0.0000\n 0.0000  0.0000  0.0000            0.0000  0.0000  0.0000\n[torch.FloatTensor of size 5x6]\n\n Variable containing:\n-0.1935  0.0484 -0.4111\n 0.3609 -0.4958  0.3408\n[torch.FloatTensor of size 2x3]\n```\nHere you can see that the last state for the forward sequence (3->5->2) is the third row's first 3 elements `-0.1935  0.0484 -0.4111` that you also find in the `ht` variable in the first row.\n\nThe last state for the backward sequence (2->5->3) is the first row's second part `0.3609 -0.4958  0.3408` that you also find in the `ht`variable in the second row.\n\nSo if you want to apply attention the first tensor is the one that you'll need. If you want to just take the last states, second tensor is at your help.\n\nBut if `num_layers > 1`, the second tensor becomes a mess :) Overall, I think this part of the PyTorch API really needs more intuitive handling. \n",
      "y": "1. You need to give them a tensor where the sequences are sorted by length in a decreasing order. If the sequences are guaranteed to always have the same length, you can skip this step. Otherwise, cf. `pack_padded_sequence()`\n2. After calling the RNN, you receive a tuple of 2 items: **packed** `Variable` of all hidden states (`hs`) and a normal `Variable` containing the last hidden states (`ht`).\n3. For `hs` you unpack it using `pad_packed_sequence()` to get a normal `Variable`.\n4. `ht` contains the **correct** forward and backward states for each sequence so you don't have to do something to recover or mask out 0's etc **but** this tensor does not concatenate the forward-backward states although `hs` returns them in a concatenated fashion.\n\n```python\n# An input of 5 timesteps and 2 sequences. The shorter one is 0-padded.\n# hidden_dim = 3, bidirectional=True, num_layers=1\nIn [525]: input_\nOut[525]:\n\n 1  3\n 3  5\n 3  2\n 2  0\n 1  0\n[torch.LongTensor of size 5x2]\n\n# hs and ht are the return values of GRU here (for LSTM you'll also have c_t)\nIn [526]: print(hs[:, 1], ht[:, 1])\n\nVariable containing:\n   (( forward states ))                      (( backward states ))\n-0.0982  0.0275 -0.3005            0.3609 -0.4958  0.3408\n-0.1710 -0.0576 -0.3759            0.2550 -0.3478  0.2796\n-0.1935  0.0484 -0.4111            0.2088 -0.2813  0.1440\n 0.0000  0.0000  0.0000            0.0000  0.0000  0.0000\n 0.0000  0.0000  0.0000            0.0000  0.0000  0.0000\n[torch.FloatTensor of size 5x6]\n\n Variable containing:\n-0.1935  0.0484 -0.4111\n 0.3609 -0.4958  0.3408\n[torch.FloatTensor of size 2x3]\n```"
   },
   {
      "x": "Docker build fail",
      "z": "How do you fix this issue? I met the same issue.",
      "y": "The bug is fixed"
   },
   {
      "x": "Assert MKL conditions in THBlas.c",
      "z": "They will not pass for things like expanded tensors:\n\n```python\nx = torch.randn(1).expand(5, 5)\nx.mm(x)\n```",
      "y": "They will not pass for things like expanded tensors:\n\n```\nx = torch.randn(1).expand(5, 5)\nx.mm(x)\n```"
   },
   {
      "x": "binary cross entropy requires double tensor for target",
      "z": "@Kuzphi  @zou3519 @soumith I think he might see this page http://pytorch.org/docs/master/nn.html#torch.nn.functional.binary_cross_entropy , it is not BCELoss page. As you mentioned, the documentation is wrong.",
      "y": " the documentation is wrong"
   },
   {
      "x": "pytorch distributed timeout when running with number processes > 16",
      "z": "@apaszke, sorry for delayed response.  I've added `setTimeout()`, like follows:\n```c++\n  // NOTE: this function needs to be thread safe\n  std::shared_ptr<context_type> createContext(\n    const DataChannelGloo::Group& group,\n    const std::string& prefix\n  ) {\n    auto context = std::make_shared<context_type>(\n        group.mustGetGroupRank(_rank), group.size());\n    prefix_store_type prefix_store(prefix, *group._store);\n    context->setTimeout(std::chrono::minutes(15));\n    context->connectFullMesh(prefix_store, _device);\n    return context;\n  }\n```\n\nSeems that it helped a bit, but I will hold conclusions for a little while since there is another failure mode described below.\n\nI have added distributed validation to [ImageNet training example](https://github.com/pytorch/examples/blob/master/imagenet/main.py), like this:\n```python\n    metrics = torch.FloatTensor([losses.avg, top1.avg, top5.avg])\n    if args.distributed:\n        dist.all_reduce(metrics)\n        metrics /= dist.get_world_size()\n```\n\nUnfortunately, I get errors running that operation:\n```\n[1,4]<stderr>:Traceback (most recent call last):\n[1,4]<stderr>:  File \"torch_imagenet_gloo_dv_smooth_warmup.py\", line 376, in <module>\n[1,4]<stderr>:    main()\n[1,4]<stderr>:  File \"torch_imagenet_gloo_dv_smooth_warmup.py\", line 180, in main\n[1,4]<stderr>:    prec1 = validate(val_loader, model, criterion, epoch)\n[1,4]<stderr>:  File \"torch_imagenet_gloo_dv_smooth_warmup.py\", line 298, in validate\n[1,4]<stderr>:    dist.all_reduce(metrics)\n[1,4]<stderr>:  File \"/usr/local/lib/python2.7/dist-packages/torch/distributed/__init__.py\", line 344, in all_reduce\n[1,4]<stderr>:    return torch._C._dist_all_reduce(tensor, op, group)\n[1,4]<stderr>:RuntimeError: [/mnt/share/asergeev/torch_imagenet/pytorch/torch/lib/gloo/gloo/transport/tcp/pair.cc:696] Socket closed [xx.xx.xx.xx]:1157\n```\n\nI tried to add MPI barrier before that operation, but it did not help.\n\nI have tried the same code with NCCL2 transport and it works flawlessly and initializes much faster.",
      "y": "added `setTimeout()`, like follows:\n```c++\n  // NOTE: this function needs to be thread safe\n  std::shared_ptr<context_type> createContext(\n    const DataChannelGloo::Group& group,\n    const std::string& prefix\n  ) {\n    auto context = std::make_shared<context_type>(\n        group.mustGetGroupRank(_rank), group.size());\n    prefix_store_type prefix_store(prefix, *group._store);\n    context->setTimeout(std::chrono::minutes(15));\n    context->connectFullMesh(prefix_store, _device);\n    return context;\n  }\n```\n code with NCCL2 transport works flawlessly and initializes much faster."
   },
   {
      "x": "off_t' undeclared with gcc 4.8.5 on Linux",
      "z": "Let's consider adding CI against gcc 4.8. Probably doing just builds (not test) should be OK.",
      "y": "adding CI against gcc 4.8"
   },
   {
      "x": "conda install torch?",
      "z": "@soumith - OK, I understand now, thanks for your help!\n\nTo run https://github.com/jcjohnson/neural-style I need torch, not pytorch, and unfortunately there's no easy install via conda.\n\n> torch is a separate product from pytorch, pytorch has no depedency on torch. \n\nWell, except from the name, the expectation that `pytorch` is strongly connected to `torch` is probably a mistake that new users like me that haven't used either package before are likely to make? So I still think a sentence at http://pytorch.org either in the install or on the about page explicitly stating that one doesn't need http://torch.ch/ would be helpful (especially given their complex install).",
      "y": "torch is a separate product from pytorch, pytorch has no depedency on torch. "
   },
   {
      "x": "DISABLED test_send_recv_any_source_autograd_profiler (__main__.TestMPIWithFork)",
      "z": "Should be fixed after https://github.com/pytorch/pytorch/pull/57253",
      "y": "bug is fixed"
   },
   {
      "x": "Unable to trace RRef's when using torch.jit.script with remote functions.",
      "z": "I don't think you can create `torch.jit.Futures` in your own code; you need use `torch.jit.fork`, which returns a `Future`.",
      "y": "you need use `torch.jit.fork`, which returns a `Future`."
   },
   {
      "x": "Failed to build pytorch on mac os. Building pthreadpool",
      "z": "`pthreadpool` dependency is added on mobile build, so disabling `USE_PYTORCH_QNNPACK` solved the issue.\n\nThe final command to build:\n```\nDEBUG=1 USE_PYTORCH_QNNPACK=0 INTERN_BUILD_MOBILE=0 CC=clang CXX=clang++ BUILD_CAFFE2=0 BUILD_CUSTOM_PROTOBUF=0 USE_OPENMP=0 USE_DISTRIBUTED=0 USE_MKLDNN=0 USE_CUDA=0 BUILD_TEST=0 USE_FBGEMM=0 USE_NNPACK=0 USE_QNNPACK=0 USE_XNNPACK=0 python setup.py develop\n```\n\nThanks for sharing the link",
      "y": "`pthreadpool` dependency is added on mobile build, so disabling `USE_PYTORCH_QNNPACK` solves the issue.\n\nThe final command to build:\n```\nDEBUG=1 USE_PYTORCH_QNNPACK=0 INTERN_BUILD_MOBILE=0 CC=clang CXX=clang++ BUILD_CAFFE2=0 BUILD_CUSTOM_PROTOBUF=0 USE_OPENMP=0 USE_DISTRIBUTED=0 USE_MKLDNN=0 USE_CUDA=0 BUILD_TEST=0 USE_FBGEMM=0 USE_NNPACK=0 USE_QNNPACK=0 USE_XNNPACK=0 python setup.py develop\n```"
   },
   {
      "x": "Profiling within a callback function does not raise an error but fails to execute a print statement",
      "z": "@anj-s I think there's a typo in the code, profiler should be invoked with\n\n`with torch.autograd.profiler.profile(use_cuda=True)`, not `with torch.autograd.profiler(use_cuda=True)`. \n\nAlso we should add a `wait` to `return_future.then` which helps raise the error @mrzzd mentioned.\n\nWith these 2 changes I'm able to get the profiler to work, although get another error when exporting trace:\n```\nRuntimeError: Got the following error when running the callback: ValueError: negat\nive seek position -1\n```\n\nthis appears to be an edge case when we export trace but there is nothing in the trace (cc @ilia-cher - I think that can repro without distributed and we should fix that). Adding some tensor operations seems to mitigate that. Here is an example that works for me: P411576109 \n\n> cc @rohan-varma do you know why future's method might fail here\n\nYeah, looks like it does silently failed but we can get the failure by try/catch or calling wait. Generally if callbacks fail they will fail silently if we don't explicitly wait or catch errors on them, since they are non blocking by design. ",
      "y": "profiler should be invoked with\n\n`with torch.autograd.profiler.profile(use_cuda=True)`, not `with torch.autograd.profiler(use_cuda=True)`. \n\nAlso we should add a `wait` to `return_future.then` which helps raise the error"
   },
   {
      "x": "Nightly CUDA 11.1 Libtorch Builds failing, undefined symbol '__gmon_start__'",
      "z": "@driazati might be, although unlikely, as debug symbols should not be mapped in VM, i.e. should not affect PLT limits. But out of abundance of caution, can you please try kicking the build with option disabled?",
      "y": " try kicking the build with option disabled"
   },
   {
      "x": "macOS conda nightlies failing when importing torch, 'Library not loaded: @rpath/libmkl_intel_lp64.1.dylib'",
      "z": "https://github.com/pytorch/builder/pull/730 might resolve this, will close after next nightly build",
      "y": "the issue is fixed"
   },
   {
      "x": "PyTorch Installation for Windows using Conda gives unrecognized arguments:nvidia",
      "z": "Yea, this is introduced by https://github.com/pytorch/pytorch.github.io/pull/688",
      "y": "this is introduced by https://github.com/pytorch/pytorch.github.io/pull/688"
   },
   {
      "x": "`Warning: Leaking Caffe2 thread-pool after fork` when using `DataLoader` with `num_workers>0` and `pin_memory=True`",
      "z": "I personally believe that if this is normal behaviour, then this warning should be removed.\nThis is a very basic use case in PyTorch, which, in my opinion, shouldn't trigger a warning unless something is wrong.\n\nAlso, I don't really understand the point of the warning. \"The Caffe2 thread-pool was leaked\". As a user, do I need to do anything about that? Should I even care?",
      "y": "Bug is fixed"
   },
   {
      "x": "layer identifier in powerSGD_hook",
      "z": "I think this would be a better question for the forums\n\nhttps://discuss.pytorch.org/\n\nClosing, in favor of posting on the forums",
      "y": "closed in in favor of posting on the forums"
   },
   {
      "x": "COO to CSR tensor conversion is slow",
      "z": "How about using `torch.searchsorted` for this?",
      "y": "use  `torch.searchsorted`"
   },
   {
      "x": "`coalesce` creates overflowed indices in large sparse COO tensors",
      "z": "HIgh priority for silent wrong behavior and crashes",
      "y": "Bug is fixed"
   },
   {
      "x": "test_eig_with_eigvec_cuda_float64 (TestTensorDeviceOpsCUDA) is failing intermittently on ROCm",
      "z": "The tests from `test_torch.py` are being removed in https://github.com/pytorch/pytorch/pull/56284.\nAs for the validness of skipping the ROCm build, I can build it later and see how MAGMA-based functions work and what skips we can avoid.",
      "y": "The tests from `test_torch.py` are being removed in https://github.com/pytorch/pytorch/pull/56284."
   },
   {
      "x": "std: ambiguous in C++17.h, upgrading from LibTorch 1.6.0 to 1.8.1 with MSVC 2019 and c++17 standard ",
      "z": "@hcyang The workaround is to append `::` before `std::function` calls in `C++17.h`. Further investigation is in progress.",
      "y": "append `::` before `std::function` calls in `C++17.h`."
   },
   {
      "x": "custom collect_fn return None but collate_fn does not accept None",
      "z": "Thanks for your response and the link. I am using filtering in my Dataset to solve the problem now. Really looking forward to seeing and using the new Dataloader!",
      "y": "use filtering in Dataset to solve the problem"
   },
   {
      "x": "error: identifier \"cusparseScsrmm2\" is undefined",
      "z": "Does this failure reproduces if PyTorch-1.7 or later is used? (Please note, that CUDA-11 was not yet available when PyTorch-1.3 was released)",
      "y": "CUDA-11 was not yet available when PyTorch-1.3 was released"
   },
   {
      "x": "torch.tensor with list of arrays orders of magnitude slower than np.array",
      "z": "Thanks for the report @futscdav. This is a duplicate of gh-13918. That has 2 PRs pending to fix the issue. gh-51731 seems to be the preferred fix, but it hasn't moved in a month - I'll ping on there. And will close this as a duplicate.",
      "y": "Bug is fixed"
   },
   {
      "x": "Expected to have finished reduction in the prior iteration before starting a new one.",
      "z": "Thanks for the comments @mrshenli!, rohan-varma\n\nI passed the \"plugins=DDPPlugin(find_unused_parameters=True)\" into pl.trainer() and worked.",
      "y": "pass the \"plugins=DDPPlugin(find_unused_parameters=True)\" into pl.trainer() "
   },
   {
      "x": "Unskip CUDA grad/gradgrad checks or no longer mark as slow test",
      "z": "@krshrimali maybe something you'd be interested in investigating? We'd need to know how much time each of these tests adds to the CI, and then we can determine if we can re-enable them",
      "y": "Bug is fixed"
   },
   {
      "x": "torch.size for cfloat or cdouble",
      "z": "well, `x.numel() * (1 + x.is_complex())`? or you cam `x.view_as_complex` in forward.",
      "y": "`x.numel() * (1 + x.is_complex())`? or you cam `x.view_as_complex` in forward."
   },
   {
      "x": "Native Declared Functions Don't Support Full Aliasing Specification",
      "z": "I can also work around by removing the annotations, but i\u2019d rather not, because then my schema won\u2019t be telling the whole truth about its inplace effects and aliasing.",
      "y": "work around by removing the annotations"
   },
   {
      "x": "Meshes not showing in tensorboard",
      "z": "Thanks. \n\nI have raised it here [PyTorch Forum](https://discuss.pytorch.org/t/meshes-not-showing-in-tensorboard/62565)",
      "y": " raised it here PyTorch Forum"
   },
   {
      "x": "cuDNN built against wrong CUDA version (10.0 instead of 9.0) when building from source -> CUDNN_STATUS_NOT_INITIALIZED",
      "z": "Questions about build issues are really best asked in the forums: https://discuss.pytorch.org/.\n\nIf you go to https://pytorch.org/, however, you can find PyTorch 1.3 or nightlies with CUDA 10.1 support now. ",
      "y": "you can find PyTorch 1.3 or nightlies with CUDA 10.1 support "
   },
   {
      "x": "clang 9 segfaults when trying to compile PyTorch",
      "z": "Just to add, this now affects Xcode 11.4 (Apple Clang 11.0.3) which is based on LLVM 9.0.0.",
      "y": " this now affects Xcode 11.4 (Apple Clang 11.0.3) which is based on LLVM 9.0.0."
   },
   {
      "x": "Load data directly from GPU without copy to CPU",
      "z": "I've posted this a few times but cannot find it right now.\n\nExample for changing data from a cv::cuda::GpuMat to a torch::Tensor (supporting float type and byte type, converting to float)\n\n```C++\nvoid deleter(void *arg){};\n    torch::Tensor matToTensor(const cv::cuda::GpuMat &image, int device)\n    {\n        bool isByte = (image.type() & 0xF) < 2;\n        auto chans = image.channels();\n        std::vector<int64_t> dims = {image.rows, image.cols, chans};\n        std::vector<int64_t> strides = {(int64_t)image.step1(), chans, 1};\n        auto options = torch::TensorOptions().dtype(isByte ? torch::kByte : torch::kFloat).device(torch::kCUDA, device);\n        auto tensorImage = torch::from_blob(image.data, dims, strides, deleter, options);\n\n        if (isByte)\n        {\n            tensorImage = tensorImage.to(torch::kFloat);\n        }\n\n        return tensorImage;\n    }\n```\n\nThe part you are interested in: **torch::from_blob**",
      "y": "Example for changing data from a cv::cuda::GpuMat to a torch::Tensor (supporting float type and byte type, converting to float)\n\n```C++\nvoid deleter(void *arg){};\n    torch::Tensor matToTensor(const cv::cuda::GpuMat &image, int device)\n    {\n        bool isByte = (image.type() & 0xF) < 2;\n        auto chans = image.channels();\n        std::vector<int64_t> dims = {image.rows, image.cols, chans};\n        std::vector<int64_t> strides = {(int64_t)image.step1(), chans, 1};\n        auto options = torch::TensorOptions().dtype(isByte ? torch::kByte : torch::kFloat).device(torch::kCUDA, device);\n        auto tensorImage = torch::from_blob(image.data, dims, strides, deleter, options);\n\n        if (isByte)\n        {\n            tensorImage = tensorImage.to(torch::kFloat);\n        }\n\n        return tensorImage;\n    }```"
   },
   {
      "x": "The number of batches in epoch is affected by (num_workers + IterableDataset)",
      "z": "Thank you for confirming this behavior is by design.",
      "y": "Behaviour is by design"
   },
   {
      "x": "Upgrade pytorch to use XNNPACK instead of NNPACK for android",
      "z": "Hi Rudolf.  This is work in progress and should be done soon.  The bulk of the work is already done, but an efficient integration requires a bit of work on the PyTorch side to cache one-time computations, which is where my focus is currently.  I'll update this issue when I am done.  Cheers.",
      "y": "Bug is fixed"
   },
   {
      "x": "Problem installation via pip pytorch 1.3.0 and 1.2.0 on windows cuda 10.0",
      "z": "As for cuda 10.0 and pytorch 1.2.0, the version specifier is 1.2.0. And we don't build pytorch 1.3.x binaries with CUDA 10.0, so you should use the CUDA 10.1 ones instead, which has the version specifer of 1.3.0 and 1.3.1.",
      "y": "uda 10.0 and pytorch 1.2.0, the version specifier is 1.2.0. And we don't build pytorch 1.3.x binaries with CUDA 10.0"
   },
   {
      "x": "Add high level autograd functions",
      "z": "Hi,\n\nYes I saw this package. It is indeed very interesting but quite orthogonal to what is proposed here. They use the engine to compute quantities that the autograd engine cannot compute natively (using hooks mainly). This proposal would be using the vanilla autograd engine for all computations (only use gradients and gradients of gradients).",
      "y": "This proposal would be using the vanilla autograd engine for all computations (only use gradients and gradients of gradients)."
   },
   {
      "x": "Pytorch android Tensor.Shape() function not producing expected results.",
      "z": "Currently tensor.shape() simply returns a raw long[]: https://github.com/pytorch/pytorch/blob/master/android/pytorch_android/src/main/java/org/pytorch/Tensor.java#L351\n\nYou should be able to use \"System.out.println(**Arrays.toString**(tensor.shape()));\" to print its content correctly.\n\n@dreiss @IvanKobzarev - do you think it's worth creating a shape type?",
      "y": "tensor.shape() simply returns a raw long[]\nYou should be able to use \"System.out.println(**Arrays.toString**(tensor.shape()));\" to print its content correctly."
   },
   {
      "x": "How to convert Tensor back to BitMap or any image format in Android?",
      "z": "Hi I have the exact same issue. @ljk53 I have tried ur suggested answers but the Tensor.getRawDataBuffer() function is a private function therefore I can't use it. \nI can call getDataAsFloatArray() but I have no idea How to convert a float to a bitmap. \nWhen I try to call getDataAsBufferArray() the program will crash because my Tesnor is a float. ",
      "y": "This issue is fixed"
   },
   {
      "x": "AssertionError: Torch not compiled with CUDA enabled",
      "z": "I had the same problem (Win10, CUDA installed prior to making conda env)\n\nThe option to install using pip worked for me (inside a miniconda env, python 3.7.7)\n\nFrom the pytorch website:\n\n`pip install torch===1.5.0 torchvision===0.6.0 -f https://download.pytorch.org/whl/torch_stable.html`",
      "y": "option to install using pip worked for me (inside a miniconda env, python 3.7.7)\n\nFrom the pytorch website:\n\n`pip install torch===1.5.0 torchvision===0.6.0 -f https://download.pytorch.org/whl/torch_stable.html`"
   },
   {
      "x": "Make it an option to have a different mask for each sequence in a batch for the Transformer",
      "z": "Thanks for the proposal. Yes, it's all the way to `nn.MultiheadAttention`. We have a plan to refactor nn.MultiheadAttention by splitting the long function into several pieces. And we can make an option to accept a 3-D mask there.",
      "y": "it's all the way to `nn.MultiheadAttention`"
   },
   {
      "x": "cuda_tensor.norm(dim=(X, Y)) is broken",
      "z": "There's a simple workaround until the fix gets released. If you pass `p=2`, then it calculates it as you expect.\n\nNot sure why it works, when the default `p='fro'` doesn't.",
      "y": "pass `p=2`, then it calculates it as you expec"
   },
   {
      "x": "Distributed Package asynchronous send/receive not working as expected (Gloo)",
      "z": "Hi there! Thank you for the detailed report. You're right and this is a bug. We do properly set the completed flag for collectives but not for the send/recv operations. It's not that the operation hasn't completed yet (it has), but the `completed_` flag in `ProcessGroup::Work` isn't updated accordingly. Fixing this is a bit more complex than simply fixing the boolean, because Gloo's send/recv doesn't have a non-blocking way to check if the operation completed or not.\n\nTo fix this properly, we need to:\n1) Update Gloo to allow non-blocking check for send/recv completion.\n2) Update the bindings to call this function when `is_completed()` is called.",
      "y": "Bug is fixed\n1) Update Gloo to allow non-blocking check for send/recv completion.\n2) Update the bindings to call this function when `is_completed()` is called."
   },
   {
      "x": "conv1d fails in PyTorch 1.0",
      "z": "@gauss256 creating a new user + Anaconda + PyTorch solved it for me.\n\nThank you for the suggestion.\n\nI then compared the `LD_LIBRARY_PATH` for the two users and found that removing `/usr/local/lib` from the `LD_LIBRARY_PATH` for the first user (for whom this was not working) fixed the issue.\n\nThen I investigated the libraries at `/usr/local/lib` and found `libmkldnn.so.0`.\n\nI added back `/usr/local/lib` to `LD_LIBRARY_PATH` and the error appeared again.\n\nSince I suspected the error was related to mkldnn, I moved this file `libmkldnn.so.0` out of `/usr/local/lib` to another location.\n\nThen I tried running the code again and it worked (with `/usr/local/lib` included in  `LD_LIBRARY_PATH` but with the file `libmkldnn.so.0` removed from `/usr/local/lib`)\n\nSo this issue (at least for me) was caused by having a system wide mkldnn installation conflicting (and overriding) the conda mkldnn.\n\nI'm adding this comment to aid in debugging in case someone else faces a similar issue.",
      "y": "this issue is caused by having a system wide mkldnn installation conflicting (and overriding) the conda mkldnn."
   },
   {
      "x": "v1.0.0 nn.utils.weight_norm seems to nullify gradients of unrelated parameters if wrapped in DataParallel",
      "z": "@mrshenli Yes, we are still experiencing the issue with the 1.1.0 release too. FYI, [this repo provides a temporary workaround for `DataParallel`](https://github.com/r9y9/wavenet_vocoder/commit/6b9c932fdb2e53e406cfe460d700868891a9efb0), and [`DistributedDataParallel` does not suffer from this](https://github.com/ksw0306/FloWaveNet/pull/22).",
      "y": "Bug is fixed "
   },
   {
      "x": "[jit] CUDA fusion: a PTX JIT compilation failed",
      "z": "`PYTORCH_FUSION_DEBUG=1`  will have kernels printed for both.",
      "y": "the issue is fixed"
   },
   {
      "x": "Build command `python setup.py rebuild_libtorch` does not exist",
      "z": "Yes this has been deprecated. We've been moving setup.py commands into cmake itself. Cmake handles rebuilding only what's needed itself, though there are still some bugs around this related to ninja and nvcc. The only commands now are 'python setup.py build' or 'python setup.py develop'",
      "y": "this has been deprecated"
   },
   {
      "x": "better cmake checks on compiler version for libtorch binaries",
      "z": "@Maslino you have to use gcc >= 4.9.2 to compile. And if you use gcc >= 5.1, you have to set C++ flags: `-D_GLIBCXX_USE_CXX11_ABI=0`",
      "y": "use gcc >= 4.9.2 to compile. And if you use gcc >= 5.1, you have to set C++ flags: `-D_GLIBCXX_USE_CXX11_ABI=0`"
   },
   {
      "x": "PyTorch 1.0 fails to build with fbgemm enabled",
      "z": "cherry-picked onto v1.0.1. Closing now. Binaries will be out soon.",
      "y": "This issue is fixed"
   },
   {
      "x": "Blocking: Do modules wait?",
      "z": "every (or almost every) operation in CUDA in PyTorch is asynchronous, and thus non-blocking.\nCheck the `Asynchronous Execution` section in the docs https://pytorch.org/docs/stable/notes/cuda.html",
      "y": "every operation in CUDA in PyTorch is asynchronous, and thus non-blocking."
   },
   {
      "x": "[JIT] Trace->Script + Inplace causes shapes to be fixed where they should not",
      "z": "Indeed, it's working again now. Thank you again.",
      "y": "Bug is fixed now"
   },
   {
      "x": "error: unknown type name 'mkldnn_shuffle_desc_t'",
      "z": "Nevermind, so I solved it via \n```\ncd third_party/ideep # this is on branch \n grokmachine@Dendis-MacBook-Pro \ue0b0 \uf07c ~/dev/facebook/pytorch/third_party/ideep \ue0b0 \uf113  \uf126 remotes/origin/mkldnn_0.17 \ue0b0 git submodule update --init --recursive\n grokmachine@Dendis-MacBook-Pro \ue0b0 \uf07c ~/dev/facebook/pytorch/third_party/ideep \ue0b0 \uf113  \uf126 remotes/origin/mkldnn_0.17 \ue0b0 git checkout master\nM        mkl-dnn\nPrevious HEAD position was d06f361 Fix klocwork issues\nSwitched to branch 'master'\nYour branch is up to date with 'origin/master'.\n```\nthen I did a `MACOSX_DEPLOYMENT_TARGET=10.14 CC=cc MAX_JOBS=25 CXX=c++ python3 setup.py install` and it solved the problem",
      "y": "```\ncd third_party/ideep # this is on branch \n grokmachine@Dendis-MacBook-Pro \ue0b0 \uf07c ~/dev/facebook/pytorch/third_party/ideep \ue0b0 \uf113  \uf126 remotes/origin/mkldnn_0.17 \ue0b0 git submodule update --init --recursive\n grokmachine@Dendis-MacBook-Pro \ue0b0 \uf07c ~/dev/facebook/pytorch/third_party/ideep \ue0b0 \uf113  \uf126 remotes/origin/mkldnn_0.17 \ue0b0 git checkout master\nM        mkl-dnn\nPrevious HEAD position was d06f361 Fix klocwork issues\nSwitched to branch 'master'\nYour branch is up to date with 'origin/master'.\n```\nthen I did a `MACOSX_DEPLOYMENT_TARGET=10.14 CC=cc MAX_JOBS=25 CXX=c++ python3 setup.py install`"
   },
   {
      "x": "model.cuda() doesn't automatically detect layers initialized in a list",
      "z": "It's expected. Use `nn.ModuleList`.",
      "y": "Use `nn.ModuleList`"
   },
   {
      "x": "[JIT] undefined symbol: cuCtxGetCurrent",
      "z": "A hot fix for this is running `export LD_LIBRARY_PATH=LD_LIBRARY_PATH:/usr/local/cuda/lib64`. I can reproduce this locally with a CUDA 10 setup",
      "y": "fix for this is `export LD_LIBRARY_PATH=LD_LIBRARY_PATH:/usr/local/cuda/lib64`."
   },
   {
      "x": "[distributed] NCCL dist.barrier doesn't respect default device",
      "z": "Meanwhile, this workaround I wrote still works correctly:\n```\ndef barrier():\n    t = torch.randn((), device='cuda')\n    dist.all_reduce(t)\n    torch.cuda.synchronize()\n\n```",
      "y": "A workaround \n```\ndef barrier():\n    t = torch.randn((), device='cuda')\n    dist.all_reduce(t)\n    torch.cuda.synchronize()\n\n```"
   },
   {
      "x": "Dilated conv in v1.0.0 is too slow.",
      "z": "This problem comes from CuDNN.\n\nCuDNN has multiple algorithms to do convolutions, that they can choose from. They have a heuristic function that picks a particular algorithm on a particular GPU, for specific sizes of convolution / dilation etc.\n\nIn CuDNN 7.4, which is the latest version, and which is what PyTorch ships with, it looks like they introduced performance regression in this version compared to CuDNN 7.1 (which PyTorch used before).\n\n`torch.backends.cudnn.benchmark=True` tells PyTorch / CuDNN to try every algorithm for a given {input size, kernel size, kernel stride, dilation, padding}, and pick the fastest.\n\nThe reason it fixes the problem for @knazeri is because they must be doing something like image classification, where the `input size` does not change across iterations. So, CuDNN only searches all algorithms (for the fastest algorithm) once in training -- when it encounters `input size` for the first time.\n\nThe reason it makes @zimenglan-sysu-512 's problem much worse is because they must be doing object detection or something similar, where images of different `input size` are passed in at every iteration, so CuDNN is searching all algorithms every iteration (because `input size` changes at every iteration), and searching all the list of algorithms takes 10x longer.",
      "y": "CuDNN is searching all algorithms every iteration (because `input size` changes at every iteration), and searching all the list of algorithms takes 10x longer."
   },
   {
      "x": "libtorch hardcoded libculibos.a causes build errors for C++ programs on Arch Linux",
      "z": "Okay I see the problem. I'll look into a fix",
      "y": "This has been fixed"
   },
   {
      "x": "Exporting model to onnx increases the model size",
      "z": "> I solved it as mentioned [here](https://github.com/onnx/onnx/issues/3278#issuecomment-781948998).\n> For anyone who stumbles upon such issue, I've written a [notebook here](https://github.com/thehetpandya/onnx-shared-weights-remove/blob/main/onnx_remove_shared_weights.ipynb) that might help you.\n\nReally thanks a lot",
      "y": "This is fixed"
   },
   {
      "x": "Pytorch DataLoader freezes when num_workers > 0 in jupyter notebook(windows 10)",
      "z": "@mobassir94 \nI am not an expert for notebook, but I suggest to try to put your code into a separate file and import it to your script, then call it within `if __name__ == '__main__'`.\n\n> when i tried it on syder ide,it worked there with number of workers > 0 but it gradually increase memory usage and give OOM after few epochs,,even if i set 2 workers only,it will give me OOM and consume all memory,how to fix this? if i use 0 workers then everything works fine but training becomes painfully slow\n\nCan you share a minimum script for me to reproduce the OOM? It's too hard to read through your code. I would suggest to delete data loader and create a new one after each epoch. As I am not sure how you implement your dataset, it's possible that some reference on the data within the computation graph prevents PyTorch cleaning the cache.",
      "y": "try to put your code into a separate file and import it to your script, then call it within `if __name__ == '__main__'`."
   },
   {
      "x": "autograd.grad with set_detect_anomaly(True) will cause memory leak",
      "z": "@albanD I think your suspicions are correct. I've created a PR that /should/ fix this. At the very least we know its python related bc the following doesn't leak:\n\n```\n#include <c10/cuda/CUDACachingAllocator.h>\n\ntorch::autograd::DetectAnomalyGuard detect_anomaly;\nfor(int i = 0; i < 10; i++) {\n  auto x = torch::ones({10, 30000}).cuda().requires_grad_();\n  auto y = x.exp();\n  auto grad = torch::autograd::grad({y}, {x}, {torch::ones_like(y)}, true, true);\n\n  auto stats = c10::cuda::CUDACachingAllocator::getDeviceStats(0);\n  std::cout << \"allocated: \" << stats.allocated_bytes[0].current / (1024 * 1024) << std::endl;\n}\n\n```",
      "y": "``` #include <c10/cuda/CUDACachingAllocator.h> torch::autograd::DetectAnomalyGuard detect_anomaly; for(int i = 0; i < 10; i++) { auto x = torch::ones({10, 30000}).cuda().requires_grad_(); auto y = x.exp(); auto grad = torch::autograd::grad({y}, {x}, {torch::ones_like(y)}, true, true); auto stats = c10::cuda::CUDACachingAllocator::getDeviceStats(0); std::cout << \"allocated: \" << stats.allocated_bytes[0].current / (1024 * 1024) << std::endl; } ```"
   },
   {
      "x": "Perf regression for YoloV3 CPU eager eval",
      "z": "The yolov3 cpu train result regression has been fixed after #52909 is merged.",
      "y": "Issue is fixed"
   },
   {
      "x": "Perf regression for background matting CUDA train",
      "z": "ah, great! so iiuc you have shown that the dataloader code got slower due to respecting the num_threads, and this is expected behavior, so the new (slow) speed is indeed correct.  thanks for tracking this down.  ",
      "y": "dataloader code got slower due to respecting the num_threads, and this is expected behavior, so the new (slow) speed is indeed correct."
   },
   {
      "x": "DataLoader for Video Loading that utilizes the GPU",
      "z": "That's correct, and don't forget to use `pin_memory`.\nYou can also use `default_collate_fn` to transform batch into tensor, since there are some optimizations.",
      "y": "use `pin_memory`.\nYou can also use `default_collate_fn` to transform batch into tensor"
   },
   {
      "x": "[docs] Improve documentation for LayerNorm",
      "z": "This is also importing for porting TF code in PyTorch. E.g. LayerNormalization https://www.tensorflow.org/api_docs/python/tf/keras/layers/LayerNormalization only requires to specify the dimensions, not their shapes.\n\nAlso, it says: \"Given a tensor inputs, moments are calculated and normalization is performed across the axes specified in axis.\". (axis by default is set to -1). Is TF's instance of `LayerNormalization()` with default parameters expressable as PyTorch LayerNorm?\n\nIn TF it's also not clear whether it does flattens/aggregates only specified dimensions, all except them, or all dimensions except batch dim.",
      "y": "Documentation is updated"
   },
   {
      "x": "CUDA error: an illegal memory access was encountered: on RTX3090 (using multiple GPUs)",
      "z": "I was running into a similar problem with PyTorch's language modeling scripts (e.g. run_clm.py), and I was able to fix it by disabling \"Hardware-accelerated GPU scheduling\" in Windows settings.",
      "y": "disable \"Hardware-accelerated GPU scheduling\" in Windows settings"
   },
   {
      "x": "Nans in matrix multiplication on ARM",
      "z": "@mdouze can you please try https://download.pytorch.org/whl/nightly/cpu/torch-1.8.0.dev20201210-cp38-cp38-linux_aarch64.whl ?\nAnd I guess it's due to the bug in implementation of missing NEON intrinsics, that was fixed by  https://github.com/pytorch/pytorch/pull/50389",
      "y": "it's due to the bug in implementation of missing NEON intrinsics"
   },
   {
      "x": "If a module passed to DistributedDataParallel has no parameter required gradient, expect_sparse_gradient[0] in _ddp_init_helper function will raise error.",
      "z": "Emm, to compute perceptron loss such as fixed VGG-19.",
      "y": "bug is fixed"
   },
   {
      "x": "Wrong crossentropy loss calculation",
      "z": "Since you passed the weights, the mean should be a weighted mean.\n\nLet's say...\n`a = (-np.log(np.exp(0.5) / \u2211exp(x[j])))`\n`b = (-np.log(np.exp(0.2) / \u2211exp(x[j])))`\n\nThen...\n**sum**\n`6*a + 4*b`\n**mean**\n`(6*a + 4*b)/(6+4)`",
      "y": "you passed the weights, the mean should be a weighted mean.\n\nLet's say...\n`a = (-np.log(np.exp(0.5) / \u2211exp(x[j])))`\n`b = (-np.log(np.exp(0.2) / \u2211exp(x[j])))`\n\nThen...\n**sum**\n`6*a + 4*b`\n**mean**\n`(6*a + 4*b)/(6+4)`"
   },
   {
      "x": "assignment bug of advanced indexing",
      "z": "Already fixed in master and throws this as expected:\n`RuntimeError: unsupported operation: some elements of the input tensor and the written-to tensor refer to a single memory location. Please clone() the tensor before performing the operation`",
      "y": "This is expected behaviour"
   },
   {
      "x": "Adding Mish Activation Function",
      "z": "We don't think this method should be in pytorch core, as opposed to your own personal repository or something like https://github.com/pytorch/contrib , at least not yet in time.\n\nOur reservation is that we want to include methods that the community uses as a standard, or else the code maintenance problem balloons up for us.\nWe do show discretion based on what the paper shows as evidence, for example BatchNorm was included within weeks of it's publish date (in Torch).\nIn terms of rejected methods, we've rejected (then) newly minted papers such as Swish ( #3260 , #3182 ), Yellowfin ( https://github.com/pytorch/pytorch/issues/1960 ) and many others, and rightly so, these haven't become standardized in the community (like LSTM / Transformer / BatchNorm).\n\nIf you have a differing opinion, let us know why, and we can re-think.\n\ntl;dr: The paper doesn't show evidence that makes it a method that has obvious long-term success. If the paper does have long-term success in the field we will include it",
      "y": "want to include methods that the community uses as a standard"
   },
   {
      "x": "automatic transition to 'cuda' if available",
      "z": "The system works as designed, we don't move tensors between devices implicitly. It's a breaking compatibility changes so we are not going to do this.",
      "y": "It's a breaking compatibility changes so we are not going to do this."
   },
   {
      "x": "Replace \"NVIDIA driver\" with \"CUDA Toolkit\" in _check_driver() error messages",
      "z": "it doesn't return the CUDA version. It returns \"Returns the latest version of CUDA supported by the driver.\".\n\nIf you have CUDA10 installed, but driver that only supports CUDA9, it will return `9000`",
      "y": "It returns \"Returns the latest version of CUDA supported by the driver.\".\n\nIf you have CUDA10 installed, but driver that only supports CUDA9, it will return `9000`"
   },
   {
      "x": "Reference cycle in _LRScheduler",
      "z": "@ezyang Sure, I can do that. But I'm wondering what would be the ideal implementation? Comments in #20124 suggest that the current code for detecting calls to `optim.step` may be missing a few cases.",
      "y": "calls to `optim.step` may be missing a few cases"
   },
   {
      "x": "Move QNNPACK micro kernels under ATen",
      "z": "@AshkanAliabadi , seems like we went with the fork.  Should we keep this open or ok to close?",
      "y": "This issue has been fixed"
   },
   {
      "x": "FasterRCNN and MaskRCNN doesn't work with DataParallel or DistributedDataParallel",
      "z": "We don't need `args.local_rank` because of the `use_env` argument from launch that I pasted just before.\nSame thing for the comment on device",
      "y": "We don't need `args.local_rank` because of the `use_env` argument from launch"
   },
   {
      "x": "Unable to  subscirpt self-defined class in torchscript function",
      "z": "Thanks for the report! @eellison, can you look? I thought we got magic methods for class types wired up",
      "y": "BUg is fixed now"
   },
   {
      "x": "Implement pep 503 Simple Repository API for deployment",
      "z": "bumping priority based on user activity",
      "y": "Feature is added"
   },
   {
      "x": "Number of prefetch in DataLoader",
      "z": "TBH make the prefetch size tied to the number of workers is very inconvenient, especially in this hardcoded way.\nFor instance, while training something video-related, I might need both RGB and optical flow across multiple frames as the network input. The I/O overhead is huge and I'd like to use multiple workers to hide it.\nBut, since the prefetch size is directly tied into the number of workers, and it happened that I only have 32 or 64GB of RAM, I can only use like 20 workers and that cannot hide the overhead completely (I'm using ramdisk), otherwise, I got the OOM error.",
      "y": "TBH make the prefetch size tied to the number of workers is very inconvenient, especially in this hardcoded way."
   },
   {
      "x": "\"Using PyTorch C++ Frontend\" TorchModule generator architecture is erroneous",
      "z": "@jlin27 Would you like to look at @BynaryCobweb's suggestion? Thanks!",
      "y": "This issue is fixed"
   },
   {
      "x": "Missing pip install wheels from the PyTorch official website",
      "z": "You can use command line to download the wheels:\n\nto see the content of the URL\n`$ curl https://download.pytorch.org/whl/cu90/torch_stable.html`\n\ne.g. download torch-1.1.0-cp36-cp36m-linux_x86_64.whl \n`$ wget https://download.pytorch.org/whl/cu90/torch-1.1.0-cp36-cp36m-linux_x86_64.whl`\n\nSee relative issue #25448 .",
      "y": "use command line to download the wheels:\n\nto see the content of the URL\n`$ curl https://download.pytorch.org/whl/cu90/torch_stable.html`\n\ne.g. download torch-1.1.0-cp36-cp36m-linux_x86_64.whl \n`$ wget https://download.pytorch.org/whl/cu90/torch-1.1.0-cp36-cp36m-linux_x86_64.whl`\n\nSee relative issue #25448 ."
   },
   {
      "x": "Dropout behaves differently on different devices",
      "z": "we dont guarantee that the RNG outputs the same sequence across different GPU models.\n\nThe guarantees of RNG are in https://pytorch.org/docs/stable/notes/randomness.html?highlight=deterministic",
      "y": "we dont guarantee that the RNG outputs the same sequence across different GPU models."
   },
   {
      "x": "No type hints on nn.Identity",
      "z": "@eduidl Thank you. Do you mind opening a PR with that diff? Tag me as reviewer.",
      "y": "This issue is fixed"
   },
   {
      "x": "PyTorch C++ API as a static lib: how to compile ?",
      "z": "libtorch has download links on pytorch.org that look like this:\n\n```\nhttps://download.pytorch.org/libtorch/cu100/libtorch-cxx11-abi-shared-with-deps-1.2.0.zip\n```\n\nYou can replaced \"shared\" with \"static\" and get URLs like this:\n\n```\nhttps://download.pytorch.org/libtorch/cu100/libtorch-cxx11-abi-static-with-deps-1.2.0.zip\n```\n\nThey exist, we generate them. I just dont remember if they work properly, but they do pass some smoke tests.",
      "y": "libtorch has download links on pytorch.org that look like this:\n\n```\nhttps://download.pytorch.org/libtorch/cu100/libtorch-cxx11-abi-shared-with-deps-1.2.0.zip\n```\n\nYou can replaced \"shared\" with \"static\" and get URLs like this:\n\n```\nhttps://download.pytorch.org/libtorch/cu100/libtorch-cxx11-abi-static-with-deps-1.2.0.zip\n```"
   },
   {
      "x": "toDense is misdocumented as a method on sparse tensors",
      "z": "@Nikronic new issue please! thanks!",
      "y": "This issue is fixed"
   },
   {
      "x": "backward_hook triggered despite RemovableHandle.remove()",
      "z": "Thanks, indeed this seems to do the trick\n\n```\nimport torch\nimport torch.nn as nn\n\n\ndef tensor_hook_adder(module, input, output):\n    def tensor_backwards(grad):\n        setattr(module, 'backprops', grad)\n    output.register_hook(tensor_backwards)\n\nlayer = nn.Linear(2, 2, bias=False)\nlayer.weight.data.copy_(2*torch.eye(2))\nmodel = layer\nlayer.register_forward_hook(tensor_hook_adder)\n\noutput = model(torch.tensor([1.,2]))\nloss = output[0]**2+2*output[1]**2\nloss.backward()\nassert torch.allclose(layer.backprops, torch.tensor([4, 16.]))\n```",
      "y": "Use\n```\nimport torch\nimport torch.nn as nn\n\n\ndef tensor_hook_adder(module, input, output):\n    def tensor_backwards(grad):\n        setattr(module, 'backprops', grad)\n    output.register_hook(tensor_backwards)\n\nlayer = nn.Linear(2, 2, bias=False)\nlayer.weight.data.copy_(2*torch.eye(2))\nmodel = layer\nlayer.register_forward_hook(tensor_hook_adder)\n\noutput = model(torch.tensor([1.,2]))\nloss = output[0]**2+2*output[1]**2\nloss.backward()\nassert torch.allclose(layer.backprops, torch.tensor([4, 16.]))\n```"
   },
   {
      "x": "[NGC Container]Runtime Error - Nvidia Nsight System",
      "z": "You are right. I just tested a few containers and it seems to be missing in `19.08`.\nLet me ask our build team, what happened.",
      "y": "The issue is fixed"
   },
   {
      "x": "[InstanceNorm] Unexpected behaviour with track_running_stats set to True in evaluation mode",
      "z": "This sounds like a silent correctness issue so I marked it as high-priority but someone with more context behind the normalization layers should take a look at this.",
      "y": "Bug is fixed"
   },
   {
      "x": "[Android] Latest nightly build causes error: library \"libpytorch_jni.so\" not found",
      "z": "We are transitioning to use lite interpreter for Android, targeting smaller library size. Currently the new nightly build is lite interpreter. The new library name is `libpytorch_jni_lite.so`. \n\nTo use lite interpreter, the model can be generated following the example:\n```\nimport torch\n\nmodel = torch.hub.load('pytorch/vision:v0.7.0', 'deeplabv3_resnet50', pretrained=True)\nmodel.eval()\n\nscripted_module = torch.jit.script(model)\n# Export full jit version model (not compatible lite interpreter), leave it here for comparison\nscripted_module.save(\"deeplabv3_scripted.pt\")\n# Export lite interpreter version model (compatible with lite interpreter)\nscripted_module._save_for_lite_interpreter(\"deeplabv3_scripted.ptl\")\n```\n\nTo load the model:\n```\nimport org.pytorch.LiteModuleLoader\n...\nmModule = LiteModuleLoader.load(MainActivity.assetFilePath(getApplicationContext(), \"deeplabv3_scripted.ptl\"));\n...\n```\n\nThis tutorial will be updated soon, but it can still be used as a reference now: https://pytorch.org/tutorials/prototype/lite_interpreter.html",
      "y": "To use lite interpreter, the model can be generated following the example:\n```\nimport torch\n\nmodel = torch.hub.load('pytorch/vision:v0.7.0', 'deeplabv3_resnet50', pretrained=True)\nmodel.eval()\n\nscripted_module = torch.jit.script(model)\n# Export full jit version model (not compatible lite interpreter), leave it here for comparison\nscripted_module.save(\"deeplabv3_scripted.pt\")\n# Export lite interpreter version model (compatible with lite interpreter)\nscripted_module._save_for_lite_interpreter(\"deeplabv3_scripted.ptl\")\n```\n\nTo load the model:\n```\nimport org.pytorch.LiteModuleLoader\n...\nmModule = LiteModuleLoader.load(MainActivity.assetFilePath(getApplicationContext(), \"deeplabv3_scripted.ptl\"));\n..."
   },
   {
      "x": "NcclErrorHandlingTest.test_nccl_errors_blocking_abort frequently fails",
      "z": "@malfet Closing this out since I think this is a duplicate of https://github.com/pytorch/pytorch/issues/58856, feel free to reopen if this is actually a separate issue.",
      "y": "The issue is already fixed"
   },
   {
      "x": "Enabling AVX512 vectorization for `qadaptive_avg_pool2d_nhwc_kernel` & `qavg_pool2d_nhwc_kernel`",
      "z": "It looks like v1.9.0 is almost finalized & only low risk PRs would be accepted until tomorrow. As such #56992 is unlikely to be included in v1.9.0. So I've more time than I initially thought & will try to dig into the avg pool source code myself. Thank you!",
      "y": "This issue is fixed"
   },
   {
      "x": "Running some CI tests with `xlarge` `resource_class` VMs for testing AVX512 support",
      "z": "cc @seemethere but my guess is this is not worth resolving until GitHub Actions migrations is complete.",
      "y": "The feature has been added"
   },
   {
      "x": "I got the error \"couldn't find \"libpytorch_jni.so\"\" when using pytorch Android",
      "z": "Thank you for your answer, the problem has been solved.",
      "y": "To use lite interpreter, the model can be generated following the example:\n```\nimport torch\n\nmodel = torch.hub.load('pytorch/vision:v0.7.0', 'deeplabv3_resnet50', pretrained=True)\nmodel.eval()\n\nscripted_module = torch.jit.script(model)\n# Export full jit version model (not compatible lite interpreter), leave it here for comparison\nscripted_module.save(\"deeplabv3_scripted.pt\")\n# Export lite interpreter version model (compatible with lite interpreter)\nscripted_module._save_for_lite_interpreter(\"deeplabv3_scripted.ptl\")\n```\n\nTo load the model:\n```\nimport org.pytorch.LiteModuleLoader\n...\nmModule = LiteModuleLoader.load(MainActivity.assetFilePath(getApplicationContext(), \"deeplabv3_scripted.ptl\"));\n..."
   },
   {
      "x": "Circular padding in Conv2d applies padding across the wrong dimension (regression from 1.4)",
      "z": "could be my fault... I'll take a look.",
      "y": "The issue is fixed"
   },
   {
      "x": "Let future expose a then() API",
      "z": "Does it make sense to create both `add_done_callback(cb) -> None` and `then(cb) -> Future`? \n(Given that the former is lighter weight?)\n\n(Next, somebody will ask for `via(executor)` and `thenError()` :) )",
      "y": "Feature is added"
   },
   {
      "x": "convert_sync_batchnorm should respect device affinity",
      "z": "To add some context, we're seeing this issue come up for DDP users where the typical flow is something like:\n```\nmodel.to(cuda_idx)\nmodel = torch.nn.SyncBatchNorm.convert_sync_batchnorm(model)\nmodel = DDP(model, device_ids=[cuda_idx])\n```\n\nThe error that's raised by DDP here indicates that sync BN ended up placing some parameters back on the CPU, even though the model was moved to cuda above. The current workaround is to move to cuda after converting to sync BN, but I don't think the order should have to matter.",
      "y": "workaround is to move to cuda after converting to sync BN"
   },
   {
      "x": "TensorPipe dependency breaks build for non-X86; it expects x86intrin.h",
      "z": "Thanks for flagging this! It looks like it's a useless include which we forgot to remove. I have https://github.com/pytorch/tensorpipe/pull/164 ready to address that.",
      "y": "This issue is fixed"
   },
   {
      "x": "hardsigmoid cuda_dispatch_ptr INTERNAL ASSERT FAILED",
      "z": "closing this, since the milestone label is there we won't forget it if we do a 1.5.1.",
      "y": "Bug is fixed"
   },
   {
      "x": "DISABLED test_profiler_with_sync_rpc_udf (__main__.RpcTestWithSpawn)",
      "z": "This is likely caused by the same problem as #37557. @rohan-varma and I are working on a fix.",
      "y": "This has been fixed"
   },
   {
      "x": "Build documentation without emitting warnings",
      "z": "After gh-41068 the only warnings are emitted by [quantization](https://pytorch.org/docs/master/quantization.html) which needs some curation. In particular, it has these sections that I think should be separate pages:\n```\ntorch.nn.intrinsic\ntorch.nn.instrinsic.qat\ntorch.nn.intrinsic.quantized\ntorch.nn.qat\ntorch.nn.quantized\ntorch.nn.quantized.dynamic\n```\n\nThe remaining warnings are due to repeating names of some of the classes in those modules. For instance, ConvBn2d appears in both [torch.nn.intrinsic](https://pytorch.org/docs/master/quantization.html#id2) and [torch.nn.intrinsic-qat](https://pytorch.org/docs/master/quantization.html#torch-nn-instrinsic-qat)",
      "y": "only warnings are emitted by [quantization](https://pytorch.org/docs/master/quantization.html) which needs some curation. In particular, it has these sections that I think should be separate pages:\n```\ntorch.nn.intrinsic\ntorch.nn.instrinsic.qat\ntorch.nn.intrinsic.quantized\ntorch.nn.qat\ntorch.nn.quantized\ntorch.nn.quantized.dynamic\n```\n\nThe remaining warnings are due to repeating names of some of the classes in those modules."
   },
   {
      "x": "CUDA debug build failed on Windows",
      "z": "@malfet It should work too. And that's my plan for fixing it in the short term.",
      "y": "This has been fixed"
   },
   {
      "x": "DistributedDataSampler converts NamedTuple to regular tuple",
      "z": "cc @mrshenli",
      "y": "Bug is fixed"
   },
   {
      "x": "quantization.test_quantize.TestGraphModePostTrainingStatic fails in tsan",
      "z": "I think there are known issues with QNNPACK tests when ASAN/UBSAN is enabled. This test was recently added to run on qnnpack backend, I guess it is failing since then. I've disabled QNNPACK tests when TSAN is enabled as well now. \ncc @kimishpatel for visibility.",
      "y": "issues with QNNPACK tests when ASAN/UBSAN is enabled. This is fixed now"
   },
   {
      "x": "[Feature] Elementwise operator complex_tensor.normalize()",
      "z": "Hey @zasdfgbnm great point! We [discussed](https://github.com/pytorch/pytorch/issues/36323#issuecomment-613569847) to add this feature as `torch.sgn`",
      "y": "this feature is added as `torch.sgn`"
   },
   {
      "x": "Quantization: FakeQuant and Observers should sync enabled flags with DDP",
      "z": "https://github.com/pytorch/pytorch/blob/8d6a8d2b3fd2a6ec788378843fc518824acf274b/torch/nn/parallel/distributed.py#L147 has some more context.\n\nYep, the quantization methods should be created before DDP.  This issue is just to track that currently some of the quantization flags are not buffers, so they will not be replicated properly if wrapped with DDP and then changed in the middle of training.",
      "y": "the quantization methods should be created before DDP"
   },
   {
      "x": "pytorch-mobile memory leak or emptyCache api",
      "z": "**module code**\n```\nimport torch\n\nprint(torch.__version__)\n\n\n# simple module\n\nclass SimpleModule(torch.nn.Module):\n    def __init__(self):\n        super(SimpleModule, self).__init__()\n        self.conv = torch.nn.Sequential(\n            torch.nn.Conv2d(1, 32, 3, 2),\n            torch.nn.ReLU(),\n            torch.nn.Conv2d(32, 32, 3, 2),\n            torch.nn.ReLU()\n        )\n        \n    def forward(self, chunk):\n        chunk = torch.as_tensor(chunk).unsqueeze(0)\n        y = chunk.unsqueeze(1)  \n        y = self.conv(y)\n        b, c, t, f = y.size()\n        y.transpose(1, 2).contiguous().view(b, t, c * f)\n        return y\n    \n\nsimple_cell = SimpleModule()\n\nchunk = torch.randn(100, 80)\nprint(simple_cell(chunk))\n\nsimple_torchscript = torch.jit.script(simple_cell)\nsimple_torchscript.save(\"simple.pt\")\n```\n\n**C++ sample code\uff1a**\n```\n#include <iostream>\n#include \"torch/csrc/api/include/torch/torch.h\"\n#include \"torch/csrc/api/include/torch/utils.h\"\n#include <caffe2/utils/threadpool/ThreadPool.h>\n#include <caffe2/utils/threadpool/ThreadPoolMobile.h>\n#include \"torch/script.h\"\n#include <utility>\n#include <unistd.h>\n#include <vector>\n#include <pthread.h>\n\nusing std::cout;\nusing std::endl;\ntorch::jit::Module module_;\nstruct JITCallGuard {\n  // AutoGrad is disabled for mobile by default.\n  torch::autograd::AutoGradMode no_autograd_guard{false};\n  // VariableType dispatch is not included in default mobile build. We need set\n  // this guard globally to avoid dispatch error (only for dynamic dispatch).\n  // Thanks to the unification of Variable class and Tensor class it's no longer\n  // required to toggle the NonVariableTypeMode per op - so it doesn't hurt to\n  // always set NonVariableTypeMode for inference only use case.\n  torch::AutoNonVariableTypeMode non_var_guard{true};\n  // Disable graph optimizer to ensure list of unused ops are not changed for\n  // custom mobile build.\n  torch::jit::GraphOptimizerEnabledGuard no_optimizer_guard{false};\n};\n\nvoid *autoTest(void *args) {\n  printf(\"start load \\n\");\n  try {\n    JITCallGuard guard;\n    module_ = torch::jit::load(std::move(\"./simple.pt\"));\n  } catch (const c10::Error &e) {\n    printf(\"load module resource failed.\");\n    return 0;\n  }\n  module_.eval();\n  printf(\"load over \\n\");\n  usleep(1000 * 1000);\n\n  // run forward\n  caffe2::mobile_threadpool()->setNumThreads(1);\n  torch::jit::getProfilingMode() = false;\n  torch::jit::getExecutorMode() = false;\n  torch::jit::setGraphExecutorOptimize(false);\n  printf(\"start recycle \\n\");\n  int count = 3;\n  while (count >= 0) {\n    float *data = new float[8000];\n    torch::Tensor datas = torch::from_blob(data, {100, 80});\n    std::vector<torch::jit::IValue> inputs{};\n    inputs.emplace_back(datas);\n    JITCallGuard guard;\n    module_.forward(inputs);\n    delete[] data;\n    usleep(500 * 1000);\n    printf(\"current count is %d .\\n\", count);\n    count--;\n  }\n  pthread_exit(NULL);\n}\n\n\nint main() {\n  cout << \"Test TorchScript Memory Leak pytorch 1.5.0\" << endl;\n  for (int i = 0; i < 200; i++) {\n    cout << \"times is \" << i << endl;\n    pthread_t tid;\n    pthread_attr_t attr;\n    void *status;\n    pthread_attr_init(&attr);\n\n    pthread_attr_setdetachstate(&attr, PTHREAD_CREATE_JOINABLE);\n    int ret = pthread_create(&tid, &attr, autoTest, NULL);\n    if (ret != 0) {\n      cout << \"pthread_create error: error_code=\" << ret << endl;\n      return 0;\n    }\n    pthread_attr_destroy(&attr);\n    int rc = pthread_join(tid, &status);\n    if (rc) {\n      cout << \"Error:unable to join,\" << rc << endl;\n      exit(-1);\n    }\n    cout << \"Main: completed thread id : \"\n            \"exiting with status :\" << status << endl;\n\n  }\n  return 0;\n}\n```\n\nget memory info by\n```\npid=`ps -A | grep Mem| awk '{print $2}'`;while [ 1 ];do cat /proc/${pid}/status | grep VmRSS;sleep 3;done\n```\nCMakeList\n```\ncmake_minimum_required(VERSION 3.6)\n\nset(PLATFORM_ANDROID TRUE)\nset(ANDROID_STL c++_static)\nset(CMAKE_VERBOSE_MAKEFILE ON)\n# NDK r20b\nset(ANDROID_NDK \"path/to/your/ndk\")\nset(CMAKE_TOOLCHAIN_FILE \"${ANDROID_NDK}/build/cmake/android.toolchain.cmake\")\nset(ANDROID_ABI arm64-v8a)\nset(ANDROID_NATIVE_API_LEVEL android-28)\n\n\nproject(MemoryLeak)\n\nset(CMAKE_BUILD_TYPE Release)\n\nif (CMAKE_BUILD_TYPE MATCHES \"Debug\" OR CMAKE_BUILD_TYPE MATCHES \"None\")\n    message(STATUS \"CMAKE_BUILD_TYPE is Debug\")\n    set(CMAKE_CXX_FLAGS \"${CMAKE_CXX_FLAGS} -fPIC -O0 -Wall -g -ggdb\")\n    set(CMAKE_C_FLAGS \"${CMAKE_C_FLAGS} -fPIC -O0 -Wall -g -ggdb\")\nelseif (CMAKE_BUILD_TYPE MATCHES \"Release\")\n    message(STATUS \"CMAKE_BUILD_TYPE is Release\")\n    set(CMAKE_CXX_FLAGS \"${CMAKE_CXX_FLAGS} -std=c++14 -O3 -fPIC -fpermissive\")\n    set(CMAKE_C_FLAGS \"${CMAKE_C_FLAGS} -fPIC -O3\")\n    set(CMAKE_EXE_LINKER_FLAGS \"${CMAKE_EXE_LINKER_FLAGS} -Wl,-s\")\nelseif (CMAKE_BUILD_TYPE MATCHES \"RelWitchDebInfo\")\n    message(STATUS \"CMAKE_BUILD_TYPE is RelWitchDebInfo\")\nelseif (CMAKE_BUILD_TYPE MATCHES \"MinSizeRel\")\n    message(STATUS \"CMAKE_BUILD_TYPE is MinSizeRel\")\n    set(CMAKE_CXX_FLAGS \"${CMAKE_CXX_FLAGS} -std=c++14 -O3 -fPIC -fpermissive\")\n    set(CMAKE_C_FLAGS \"${CMAKE_C_FLAGS} -fPIC -O3\")\nelse ()\n    message(STATUS \"unknown CMAKE_BUILD_TYPE = \" ${CMAKE_BUILD_TYPE})\n    set(CMAKE_CXX_FLAGS \"${CMAKE_CXX_FLAGS} -std=c++14 -fPIC -O0 -Wall -g -ggdb\")\n    set(CMAKE_C_FLAGS \"${CMAKE_C_FLAGS} -fPIC -O0 -Wall -g -ggdb\")\nENDif ()\n# set STL C++ 14\nset(CMAKE_CXX_STANDARD 14)\n\ninclude_directories(\n        ${PROJECT_SOURCE_DIR}/pytorch/include/\n        ${PROJECT_SOURCE_DIR}/pytorch/include/torch/csrc/api/include\n)\n\nlink_directories(\n        ${PROJECT_SOURCE_DIR}/pytorch/lib/\n)\n\n\nadd_executable(MemoryLeak main.cpp)\ntarget_link_libraries(\n        MemoryLeak\n        -Wl,--gc-sections\n        -Wl,--whole-archive\n        torch.a\n        torch_cpu.a\n        -Wl,--no-whole-archive\n        c10.a\n        nnpack.a\n        XNNPACK.a\n        pytorch_qnnpack.a\n        eigen_blas.a\n        cpuinfo.a\n        clog.a\n        log\n        m\n        z\n)\n```\nI found use pthread will come out memory leak.use main function will not, NOTE: in android environment",
      "y": "**module code**\n```\nimport torch\n\nprint(torch.__version__)\n\n\n# simple module\n\nclass SimpleModule(torch.nn.Module):\n    def __init__(self):\n        super(SimpleModule, self).__init__()\n        self.conv = torch.nn.Sequential(\n            torch.nn.Conv2d(1, 32, 3, 2),\n            torch.nn.ReLU(),\n            torch.nn.Conv2d(32, 32, 3, 2),\n            torch.nn.ReLU()\n        )\n        \n    def forward(self, chunk):\n        chunk = torch.as_tensor(chunk).unsqueeze(0)\n        y = chunk.unsqueeze(1)  \n        y = self.conv(y)\n        b, c, t, f = y.size()\n        y.transpose(1, 2).contiguous().view(b, t, c * f)\n        return y\n    \n\nsimple_cell = SimpleModule()\n\nchunk = torch.randn(100, 80)\nprint(simple_cell(chunk))\n\nsimple_torchscript = torch.jit.script(simple_cell)\nsimple_torchscript.save(\"simple.pt\")\n```\n\n**C++ sample code\uff1a**\n```\n#include <iostream>\n#include \"torch/csrc/api/include/torch/torch.h\"\n#include \"torch/csrc/api/include/torch/utils.h\"\n#include <caffe2/utils/threadpool/ThreadPool.h>\n#include <caffe2/utils/threadpool/ThreadPoolMobile.h>\n#include \"torch/script.h\"\n#include <utility>\n#include <unistd.h>\n#include <vector>\n#include <pthread.h>\n\nusing std::cout;\nusing std::endl;\ntorch::jit::Module module_;\nstruct JITCallGuard {\n  // AutoGrad is disabled for mobile by default.\n  torch::autograd::AutoGradMode no_autograd_guard{false};\n  // VariableType dispatch is not included in default mobile build. We need set\n  // this guard globally to avoid dispatch error (only for dynamic dispatch).\n  // Thanks to the unification of Variable class and Tensor class it's no longer\n  // required to toggle the NonVariableTypeMode per op - so it doesn't hurt to\n  // always set NonVariableTypeMode for inference only use case.\n  torch::AutoNonVariableTypeMode non_var_guard{true};\n  // Disable graph optimizer to ensure list of unused ops are not changed for\n  // custom mobile build.\n  torch::jit::GraphOptimizerEnabledGuard no_optimizer_guard{false};\n};\n\nvoid *autoTest(void *args) {\n  printf(\"start load \\n\");\n  try {\n    JITCallGuard guard;\n    module_ = torch::jit::load(std::move(\"./simple.pt\"));\n  } catch (const c10::Error &e) {\n    printf(\"load module resource failed.\");\n    return 0;\n  }\n  module_.eval();\n  printf(\"load over \\n\");\n  usleep(1000 * 1000);\n\n  // run forward\n  caffe2::mobile_threadpool()->setNumThreads(1);\n  torch::jit::getProfilingMode() = false;\n  torch::jit::getExecutorMode() = false;\n  torch::jit::setGraphExecutorOptimize(false);\n  printf(\"start recycle \\n\");\n  int count = 3;\n  while (count >= 0) {\n    float *data = new float[8000];\n    torch::Tensor datas = torch::from_blob(data, {100, 80});\n    std::vector<torch::jit::IValue> inputs{};\n    inputs.emplace_back(datas);\n    JITCallGuard guard;\n    module_.forward(inputs);\n    delete[] data;\n    usleep(500 * 1000);\n    printf(\"current count is %d .\\n\", count);\n    count--;\n  }\n  pthread_exit(NULL);\n}\n\n\nint main() {\n  cout << \"Test TorchScript Memory Leak pytorch 1.5.0\" << endl;\n  for (int i = 0; i < 200; i++) {\n    cout << \"times is \" << i << endl;\n    pthread_t tid;\n    pthread_attr_t attr;\n    void *status;\n    pthread_attr_init(&attr);\n\n    pthread_attr_setdetachstate(&attr, PTHREAD_CREATE_JOINABLE);\n    int ret = pthread_create(&tid, &attr, autoTest, NULL);\n    if (ret != 0) {\n      cout << \"pthread_create error: error_code=\" << ret << endl;\n      return 0;\n    }\n    pthread_attr_destroy(&attr);\n    int rc = pthread_join(tid, &status);\n    if (rc) {\n      cout << \"Error:unable to join,\" << rc << endl;\n      exit(-1);\n    }\n    cout << \"Main: completed thread id : \"\n            \"exiting with status :\" << status << endl;\n\n  }\n  return 0;\n}\n```\n\nget memory info by\n```\npid=`ps -A | grep Mem| awk '{print $2}'`;while [ 1 ];do cat /proc/${pid}/status | grep VmRSS;sleep 3;done\n```\nCMakeList\n```\ncmake_minimum_required(VERSION 3.6)\n\nset(PLATFORM_ANDROID TRUE)\nset(ANDROID_STL c++_static)\nset(CMAKE_VERBOSE_MAKEFILE ON)\n# NDK r20b\nset(ANDROID_NDK \"path/to/your/ndk\")\nset(CMAKE_TOOLCHAIN_FILE \"${ANDROID_NDK}/build/cmake/android.toolchain.cmake\")\nset(ANDROID_ABI arm64-v8a)\nset(ANDROID_NATIVE_API_LEVEL android-28)\n\n\nproject(MemoryLeak)\n\nset(CMAKE_BUILD_TYPE Release)\n\nif (CMAKE_BUILD_TYPE MATCHES \"Debug\" OR CMAKE_BUILD_TYPE MATCHES \"None\")\n    message(STATUS \"CMAKE_BUILD_TYPE is Debug\")\n    set(CMAKE_CXX_FLAGS \"${CMAKE_CXX_FLAGS} -fPIC -O0 -Wall -g -ggdb\")\n    set(CMAKE_C_FLAGS \"${CMAKE_C_FLAGS} -fPIC -O0 -Wall -g -ggdb\")\nelseif (CMAKE_BUILD_TYPE MATCHES \"Release\")\n    message(STATUS \"CMAKE_BUILD_TYPE is Release\")\n    set(CMAKE_CXX_FLAGS \"${CMAKE_CXX_FLAGS} -std=c++14 -O3 -fPIC -fpermissive\")\n    set(CMAKE_C_FLAGS \"${CMAKE_C_FLAGS} -fPIC -O3\")\n    set(CMAKE_EXE_LINKER_FLAGS \"${CMAKE_EXE_LINKER_FLAGS} -Wl,-s\")\nelseif (CMAKE_BUILD_TYPE MATCHES \"RelWitchDebInfo\")\n    message(STATUS \"CMAKE_BUILD_TYPE is RelWitchDebInfo\")\nelseif (CMAKE_BUILD_TYPE MATCHES \"MinSizeRel\")\n    message(STATUS \"CMAKE_BUILD_TYPE is MinSizeRel\")\n    set(CMAKE_CXX_FLAGS \"${CMAKE_CXX_FLAGS} -std=c++14 -O3 -fPIC -fpermissive\")\n    set(CMAKE_C_FLAGS \"${CMAKE_C_FLAGS} -fPIC -O3\")\nelse ()\n    message(STATUS \"unknown CMAKE_BUILD_TYPE = \" ${CMAKE_BUILD_TYPE})\n    set(CMAKE_CXX_FLAGS \"${CMAKE_CXX_FLAGS} -std=c++14 -fPIC -O0 -Wall -g -ggdb\")\n    set(CMAKE_C_FLAGS \"${CMAKE_C_FLAGS} -fPIC -O0 -Wall -g -ggdb\")\nENDif ()\n# set STL C++ 14\nset(CMAKE_CXX_STANDARD 14)\n\ninclude_directories(\n        ${PROJECT_SOURCE_DIR}/pytorch/include/\n        ${PROJECT_SOURCE_DIR}/pytorch/include/torch/csrc/api/include\n)\n\nlink_directories(\n        ${PROJECT_SOURCE_DIR}/pytorch/lib/\n)\n\n\nadd_executable(MemoryLeak main.cpp)\ntarget_link_libraries(\n        MemoryLeak\n        -Wl,--gc-sections\n        -Wl,--whole-archive\n        torch.a\n        torch_cpu.a\n        -Wl,--no-whole-archive\n        c10.a\n        nnpack.a\n        XNNPACK.a\n        pytorch_qnnpack.a\n        eigen_blas.a\n        cpuinfo.a\n        clog.a\n        log\n        m\n        z\n)\n```\nI found use pthread will come out memory leak.use main function will not, NOTE: in android environment"
   },
   {
      "x": "Very slow for gradient penalty!",
      "z": "Gradient penalty requires computing second order derivatives, and just like computing the first one is 2-3x more expensive than the actual computation, the second one is 2-3x more expensive again, yielding a 4-9x slowdown. Closing as not a bug.",
      "y": "Gradient penalty requires computing second order derivatives, and just like computing the first one is 2-3x more expensive than the actual computation, the second one is 2-3x more expensive again, yielding a 4-9x slowdown"
   },
   {
      "x": "error when installing Caffe2: undefined reference to `void caffe2::math::BiasCHW",
      "z": "I had the same error. After #7738 I'm now able to build on TX2 with Jetpack 3.2. I had to update eigen3 to the latest version on the git mirror and cmake with the `-DUSE_MPI=OFF` flag. Thanks for the fix @yinghai",
      "y": "update eigen3 to the latest version on the git mirror and cmake with the `-DUSE_MPI=OFF` flag"
   },
   {
      "x": "Batchnorm1d cannot work with batch size == 1",
      "z": "This only fails because you have a batch size of 1 and a single feature per channel.\nI'd recommend to use `drop_last=False` on the DataLoader to avoid the issue.\nI don't really have a view on doing magic to avoid this - people will fall into traps when using a batch size of one and no training happens.",
      "y": "This only fails because you have a batch size of 1 and a single feature per channel.\nIuse `drop_last=False` on the DataLoader to avoid the issue."
   },
   {
      "x": "in-place Arithmetic assignment operators gives wrong answers",
      "z": "Thanks for the report, @calebh. When tensors share the same storage, in-place operations are incorrect. You should generally avoid in-place operations because they can have other bad side effects, like making backward computation impossible (autograd will throw an error if that is the case though).\n\nIdeally we'd throw an error or warn if the tensors involved in an in-place operation have overlapping storage, but determining that fully is equivalent to finding a solution to a linear diophantine equation in `n` variables, where `n` is proportional to the number of dimensions in the input tensors.",
      "y": "Ideally pytorch throw an error or warn if the tensors involved in an in-place operation have overlapping storage"
   },
   {
      "x": "ATen C++ tensor creation places tensors on devices inconsistently from torch.* Python calls",
      "z": "@mcarilli for context, ATen currently does not have any concept of a \"device\" beyond CPU vs. CUDA (i.e. the \"ordinal\" part is missing from ATen's model). I'm working on changing this right now, so you can expect this to be fixed soon",
      "y": "The issue is fixed"
   },
   {
      "x": "better error if file does not support seek (torch.load/save)",
      "z": "My bad, I should have provided some code.\n\nOn python 3: (in python 2 the function is just `urllib.urlopen`):\n\n```\nimport torch\nimport urllib\nresource = urllib.request.urlopen('https://download.pytorch.org/test_data/linear.pt')\nmodel = torch.load(resource)\n---------------------------------------------------------------------------\nUnsupportedOperation                      Traceback (most recent call last)\n<ipython-input-15-62e11b1f7972> in <module>()\n      2 import urllib\n      3 resource = urllib.request.urlopen('https://download.pytorch.org/test_data/linear.pt')\n----> 4 model = torch.load(resource)\n\n~/pytorch/pytorch/torch/serialization.py in load(f, map_location, pickle_module)\n    301         f = open(f, 'rb')\n    302     try:\n--> 303         return _load(f, map_location, pickle_module)\n    304     finally:\n    305         if new_fd:\n\n~/pytorch/pytorch/torch/serialization.py in _load(f, map_location, pickle_module)\n    448\n    449     f_is_real_file = _is_real_file(f)\n--> 450     if f_is_real_file and f.tell() == 0:\n    451         # legacy_load requires that f has fileno()\n    452         # only if offset is zero we can attempt the legacy tar file loader\n\nUnsupportedOperation: seek\n```\nIt would be good if we checked if the file is seekable so that we can provide a better error message. Something along the lines of \"Error: you can only torch.load from a file that is seekable. Please read the file into a seekable file on disk using request.urlretrieve or a string buffer like io.BytesIO and try to load from it instead.\"\n\n",
      "y": "Better error message has been added"
   },
   {
      "x": "[pytorch] [feature request] Flatten convenience method",
      "z": "@chopin57otu You are right, it's not visible in `stable` docs. However, it is there in the `master` docs: https://pytorch.org/docs/master/torch.html?highlight=flatten#torch.flatten",
      "y": "Feature is added in pytorch"
   },
   {
      "x": "torch.empty after construction puts a high load on CPU for a long time when the size is big",
      "z": "Sorry, my fault. \n\nI was testing in Jupyter Notebook and forgot that it added additional complexity to \"vanilla\" python. So there's no such problem in the python shell.\n\nThis problem gradually builds up with each \"Kernel Restart\". May be it's related to lazy deallocation of memory or maybe it has another reason. Anyway hard restart of Jupyter Notebook cleans out this problem.\n\nSo I don't see anything wrong on PyTorch side here.\n\nUPDATE:\nCorrection - the problem is \"Variable Inspector\" plugin.",
      "y": "the problem is \"Variable Inspector\" plugin."
   },
   {
      "x": "[pytorch] [Feature Request] SoftArgMax Function for differentiable argmax",
      "z": "Check https://github.com/MWPainter/cvpr2019/blob/master/stitched/soft_argmax.py#L117. /cc @MWPainter if he want to contribute with a PR to the pytroch repo.",
      "y": "use softmax and dot product with indices"
   },
   {
      "x": "Cannot compile Caffe2 on Mac",
      "z": "Some more datapoints. This also breaks the Anaconda builds when using the default gflags from the anaconda channel, but seems to work when you use the gflags from conda-forge instead. The gflags in the anaconda channel has not been updated for months, so that shouldn't be the cause. The cause must be some recent change in our build somewhere; the failures go back about a day, so a recent commit is probably the cause. None of the recent commits look that relevant though. Also, all the linux builds are fine, so whatever change caused this somehow only affects osx",
      "y": "The bug is fixed"
   },
   {
      "x": "Compiling C/C++ extension receive undefined symbol: _ZTIN5torch8autograd8Variable4ImplE on importing into python.",
      "z": "Hello,\n\nI figured it out by reading the tutorial (https://pytorch.org/tutorials/advanced/cpp_extension.html) more carefully\n\nOnce your extension is built, you can simply import it in Python, using the name you specified in your setup.py script. Just be sure to import torch first, as **this will resolve some symbols that the dynamic linker must see.**\n\nIf I import torch before the module it works.",
      "y": "import torch before the module it works"
   },
   {
      "x": "Tag v0.4.0 eigen submodule no longer downloads",
      "z": "I'll fix this tomorrow. I'll fix the v0.4.0 tag itself.",
      "y": "This has been fixed"
   },
   {
      "x": "torch.cdist gradients are NAN for p<1 and very small differences in a given dimension (0<delta<~e-45)",
      "z": "This seems like a nasty numerical bug, upgrading priority",
      "y": "The bug is fixed"
   },
   {
      "x": "`F.logsigmoid(input, out=blah)` crashes",
      "z": "(about naming: at some point in the past I suggested that `log_softmax` and `log_sigmoid` used underscores consistently)",
      "y": "`log_softmax` and `log_sigmoid` used underscores consistently"
   },
   {
      "x": "INTERNAL ASSERT FAILED at mmdet/ops/nms/src/nms_cpu.cpp:7",
      "z": "@modjtabaf \n\nLooks like this bug needs to be reported to https://github.com/open-mmlab/mmdetection instead. The error message says `please report a bug to PyTorch` because the code in that repo is using assert macro from PyTorch [here](https://github.com/open-mmlab/mmdetection/blob/ef58bc62d3b5ee992b37e9277a7aa7701d8c1757/mmdet/ops/nms/src/nms_cpu.cpp#L7). \n\nI am closing this issue, but please feel free to reopen if you believe something needs to be fixed in PyTorch. Thanks!\n\n",
      "y": "The issue is fixed"
   },
   {
      "x": "Multiprocessing on Python 3.8 fails with cannot pickle '_io.TextIOWrapper'",
      "z": "@seemethere @osalpekar Haven't looked too deeply, but this is likely because of a spawn vs fork multiprocessing issue. In py3.8, spawn is now default instead of fork on mac. We use a bunch of tempfiles in these tests, and spawn method can't pickle those, see below for a somewhat related repro:\n\nhttps://gist.github.com/rohan-varma/1fe828948e0b48bb6c9be38465c51647\n\nEdit: fix at https://github.com/pytorch/pytorch/pull/36542",
      "y": "this is likely because of a spawn vs fork multiprocessing issue. In py3.8, spawn is now default instead of fork on mac."
   },
   {
      "x": "Load pytorch tensor created by torch.save(tensor_name, tensor_path) in c++ libtorch failed.",
      "z": "This is likely a duplicate of https://github.com/pytorch/pytorch/issues/20356, and we can try the solution @driazati suggested: https://github.com/pytorch/pytorch/issues/20356#issuecomment-567663701",
      "y": "The feature is added"
   },
   {
      "x": "Build fails with gcc 9.1 + CUDA-10.2",
      "z": "@seemethere @malfet should we set up a GCC 9.X CI?",
      "y": "The issue is fixed"
   },
   {
      "x": "The mean method results in different outputs",
      "z": "> BTW, why does contiguous method help in this case?\n\nI haven't looked closely, but my guess is that it forces multiple smaller sums. The 4th and 1st means get implemented as the same thing basically `b.reshape(1024*224*224, 3).sum(0)/N`. The 3rd gets implemented as `tmp.sum(0).sum((2,3)) / N`.\n\nIf you need batch normalization, use the built-in batch normalization function. Otherwise, use double precision as I suggested above. I don't think this is going to be improved anytime soon.",
      "y": " it forces multiple smaller sums. The 4th and 1st means get implemented as the same thing basically `b.reshape(1024*224*224, 3).sum(0)/N`. The 3rd gets implemented as `tmp.sum(0).sum((2,3)) / N`.\n\nIf you need batch normalization, use the built-in batch normalization function. Otherwise, use double precision"
   },
   {
      "x": "torch.save does not use zipfile serialization",
      "z": "Given https://github.com/pytorch/pytorch/issues/40140 ...",
      "y": "The issue is fixed"
   },
   {
      "x": "Multi channel linear layer",
      "z": "We typically only add modules to the core PyTorch if they're widely applicable and described in a significant research paper. I don't think this reaches that bar yet. You are welcome to distribute it in your own package.\n\nI'd recommend using a torch.bmm instead of `*` and `sum`. You'll likely get better performance and lower memory usage over a wide range of sizes. For example, set up your tensor as:\n\n```\nmy_tensor = torch.randn(channel_size, batch_size, input_size)\nweight = torch.randn(channel_size, input_size, output_size)\nbias = torch.randn(channel_size, 1, output_size)\n\noutput = torch.bmm(my_tensor, weight) + bias\n```",
      "y": "using a torch.bmm instead of `*` and `sum`. You'll likely get better performance and lower memory usage over a wide range of sizes. For example, set up your tensor as:\n\n```\nmy_tensor = torch.randn(channel_size, batch_size, input_size)\nweight = torch.randn(channel_size, input_size, output_size)\nbias = torch.randn(channel_size, 1, output_size)\n\noutput = torch.bmm(my_tensor, weight) + bias\n```"
   },
   {
      "x": "Exception in find_cuda_windows_lib",
      "z": "These functions are already removed in master.",
      "y": "These functions are removed"
   },
   {
      "x": "Autograd view CreationMeta are not properly propagated when chaining views",
      "z": "I think, related: https://github.com/pytorch/pytorch/issues/31819",
      "y": "The issue is fixed"
   },
   {
      "x": "Add TracedModule attribute for the constants table",
      "z": "Looks like I'm not getting a ton of free time to work on this issue, but I'll write what needs to be done here, and if someone wants to put up a PR I am happy to review.\n\nThe way we pretty print `.code` is with `PythonPrint`. This function populates one of its arguments `tensor_table` with tensor constants (see \"Uses of tensor constants\" [here](https://github.com/pytorch/pytorch/blob/master/torch/csrc/jit/docs/serialization.md#printing-code-objects-as-python-source) for a more detailed description). Indices in this tensor table correspond to `CONSTANTS.c*` in the pretty printed code. \n\nRight now in the python binding for `.code` ([here](https://github.com/pytorch/pytorch/blob/999d7f6ab2d1196a1f6e2bb7123e9da0e5a2592d/torch/csrc/jit/python/script_init.cpp#L896)) populates this `tensor_table` and throws it away. We just want to keep it and return it to the user.\n\nSome additional notes:\n1. The constants table is only associated with a single `PythonPrint` pass, so you shouldn't store them separately from `.code` (they might get out of sync). We should probably have a `.code_with_constants` or something that returns a `(code_str, constants_table)`. \n2. Worth creating a little wrapper for the constants table in Python that overrides `getattr` so that `.c0` returns the 0th element of the table, so that you can just assign the constants table to  `CONSTANTS` and all the calls to `CONSTANTS.cN` will just work.\n",
      "y": "The feature is added"
   },
   {
      "x": "THCUNN/BCECriterion.cu:42: Assertion `input >= 0. && input <= 1.` failed.",
      "z": "i have met this question too, but when i debug this problem.\ni discovered that this may happens when your network initiated with a bad startup.\nusually the output is too big , eg , sigmoid(20.) == 1\nso, this will occurs when you want to do (sth + sigmoid(20.)) with BCE ,this output will over then one.\n\nsolution:\ntorch.nn.init.xavier_normal_(weight)\ntorch.nn.init.constant_(bias, 0)\nmay help to you ",
      "y": "this may happens when your network initiated with a bad startup.\nusually the output is too big , eg , sigmoid(20.) == 1\nso, this will occurs when you want to do (sth + sigmoid(20.)) with BCE ,this output will over then one.\n\nsolution:\ntorch.nn.init.xavier_normal_(weight)\ntorch.nn.init.constant_(bias, 0)"
   },
   {
      "x": "Add torch.sgn to return complex sign",
      "z": "@kshitij12345 Hi, yes I am working on it.",
      "y": "The feature is added"
   },
   {
      "x": "torch.triangular_solver doesn't work on batched inputs",
      "z": "Yes, the issue seems to be fixed now. Thank you!",
      "y": "The issue is fixed now"
   },
   {
      "x": "Explicitly Define Gradient for Certain Computation",
      "z": "@occam-ra-zor \nI guess this tutorial might be useful. You can implement your Function/Operator the way `MyRelu` is defined.\nhttps://pytorch.org/tutorials/beginner/examples_autograd/two_layer_net_custom_function.html",
      "y": "You can implement your Function/Operator the way `MyRelu` is defined.\nhttps://pytorch.org/tutorials/beginner/examples_autograd/two_layer_net_custom_function.html"
   },
   {
      "x": "Bug when an inplace op is done on the output of the forward of a autograd.Function and this output is a view of an intermediary result of the forward.",
      "z": "@colesbury Yes, I do. I guess I can fix this issue by modifying `CheckpointFunction`.",
      "y": "It has been fixed by modifying `CheckpointFunction`."
   },
   {
      "x": "Support matmul for scalar tensors",
      "z": "The reason for this treatment is given in PEP 465 -- https://www.python.org/dev/peps/pep-0465/#semantics\n\nIt allowed special treatment for rank-1 tensors because \"practicality beats purity\", but choose not to extend to rank-0 tensors because there's already a * operator which works for this case, so the case for practicality is not as strong.\n\nIt seems unlikely to change in numpy, and this doesn't seem like a strong-enough reason to deviate from numpy compatibility, so will close this issue.\n\nSomething like #26566 would allow a workaround for users without creating backward compatibility issues.",
      "y": "The feature is not added"
   },
   {
      "x": "Build failure (sleef.h not found) in some hard-to-understand situations",
      "z": "To fix, add to [caffe2/CMakeLists.txt](https://github.com/pytorch/pytorch/blob/50d82f51223726725317919d429a02979d9bc39c/caffe2/CMakeLists.txt#L1155) after line 1155\n\n`target_include_directories(${test_name} PRIVATE $<BUILD_INTERFACE:${CMAKE_BINARY_DIR}/include>)`\n",
      "y": "To fix, \n`target_include_directories(${test_name} PRIVATE $<BUILD_INTERFACE:${CMAKE_BINARY_DIR}/include>)`"
   },
   {
      "x": "AttributeError: module 'torch.jit' has no attribute 'unused'",
      "z": "`entrypoints = torch.hub.list('pytorch/vision', force_reload=True)` loads from vision master which depends on pytorch master. I think `entrypoints = torch.hub.list('pytorch/vision:v0.4.0', force_reload=True)` should work. Hub CI tests against latest pytorch & vision conda releases. \nI agree it's good to annotate the current stable releases for these repos on Hub webpage... ",
      "y": "`entrypoints = torch.hub.list('pytorch/vision:v0.4.0', force_reload=True)` should work"
   },
   {
      "x": "BC check test was failing on Friday and Saturday",
      "z": "Another thing I'm concerned about is whether or not the script picks up ALL bc-breaking changes, or bails out on the first one. If it bails out on the first one then after master is broken you will miss any further breaking changes.",
      "y": "bug is fixed now"
   },
   {
      "x": "Where did 1.2.0 go on the website?",
      "z": "This is fixed now, I manually fixed them.\nI have to see why they were overwritten with a previous version.",
      "y": "This is fixed now"
   },
   {
      "x": "[jit] `zip` and `enumerate` can't be used in a list comprehension",
      "z": "Similar issue to this, I believe: https://github.com/pytorch/pytorch/issues/22483",
      "y": "This has been fixed"
   },
   {
      "x": "MultiheadAttention and DDP incompatability",
      "z": "@mrshenli I have discussed with Shen offline. I will submit a PR to fix it soon.",
      "y": "This issue is fixed now"
   },
   {
      "x": "spectral_norm used in RNN causes \"parameter types mismatch\" in GPU",
      "z": "I came across the same Bug, did this problem be solved? @VitalyFedyunin @pbelevich @oh-y ",
      "y": "The issue is closed without any fix"
   },
   {
      "x": "[jit] `zeros_like` needs full Tensor options to work",
      "z": "```Tensor Options Creates Schema Mismatch for ops which have a (ScalarType dtype, Layout layout, Device device) tuple```",
      "y": "```Tensor Options Creates Schema Mismatch for ops which have a (ScalarType dtype, Layout layout, Device device) tuple```"
   },
   {
      "x": "[jit] Tensor `.tolist()` is not bound in TorchScript",
      "z": "This isn't possible, because we need to type the resulting list, and `tolist()` returns an arbitrarily nested list\n\nWe could support something like `a.to1Dlist() or a.tolist(INTEGER_LITERAL)` or `a.toList(List[List[int]])`",
      "y": "This isn't possible, because we need to type the resulting list, and `tolist()` returns an arbitrarily nested list"
   },
   {
      "x": "\"type id is VariableTensorId\" runtime error for some mobile models",
      "z": "#25597 is supposed to be 100% semantics preserving. How do we know it's the culprit commit? If it is the culprit I will need to rereview it more closely for bugs",
      "y": "The bug is fixed"
   },
   {
      "x": "Crash on assigning cuda tensor to bool-indexed cpu tensor",
      "z": "this looks like it was the type promotion bug.",
      "y": "bug is fixed "
   },
   {
      "x": "Flakiness in TestAutogradDeviceTypeCUDA",
      "z": "Oh, interesting -`mark_graph_task_completed()` effectively no longer guarantees that the future is completed before exiting - it's possible to exit when it's only in the process of being completed. Which isn't good enough for the assert that is being quoted above. \n\n(This is basically the `pthread_once()` problem underneath the hood)\n\nWhat I think might solve is simply a 1-line addition of a future->waitNoThrow() call to the early exit path in mark_graph_task_complete().\n    This will have the property of guaranteeing that the future is completed on exit - probably a property that people expect. And the cost won't be too high, since it's not the common fast-path.\n\nI'll write this up, unless we have other ideas.",
      "y": "1-line addition of a future->waitNoThrow() call to the early exit path in mark_graph_task_complete().\n    This will have the property of guaranteeing that the future is completed on exit "
   },
   {
      "x": "Export to ONNX of nop-squeeze errors out in ONNXRT",
      "z": "Thanks for the bug report, @vadimkantorov!",
      "y": "The bug is fixed"
   },
   {
      "x": "Error with CUDA for quantization aware training",
      "z": "Yes, I could reproduce this issue. The workground is to do `qat_model.to(device='cuda')` again before the loop. Maybe some tensors are not transferred to GPU after calling `torch.quantization.prepare_qat`.",
      "y": "workground is to do `qat_model.to(device='cuda')` again before the loop. Maybe some tensors are not transferred to GPU after calling `torch.quantization.prepare_qat`."
   },
   {
      "x": "Nondeterministic output from Optimizer.state_dict()",
      "z": "Sure. It will be a couple of days before I get to it, but I should be able to handle this.",
      "y": "Bug is fixed"
   },
   {
      "x": "torch.assert operator / async assertion for cuda tensors",
      "z": "It was later relanded https://github.com/pytorch/pytorch/commit/cfd9360d090842fa3fc72987c8e70c8091ad4b03",
      "y": "This feature is added"
   },
   {
      "x": "SyncBatchNorm size check",
      "z": "This does make sense and maybe the check should be `count_all == 1`? Looks like this check was added in https://github.com/pytorch/pytorch/pull/29626, @albanD I was wondering if you had thoughts on the proposal here given you reviewed that PR?",
      "y": "This feature is already added"
   },
   {
      "x": "Max-over-dim is 2,000 times slower than max",
      "z": "Thanks for reporting. \nYou are right, max() uses a better implementation than max over a dimension, especially when the output size is small (in your case one element). The short-term fix for the max performance may be\n```\nii=a.argmax(0)\nmaxval = a.gather(0, ii.unsqueeze(0)).squeeze(0)\n```\ncc @XiaobingSuper  for 5x perf difference on the cpu. ",
      "y": " max() uses a better implementation than max over a dimension, especially when the output size is small , The short-term fix for the max performance may be\n```\nii=a.argmax(0)\nmaxval = a.gather(0, ii.unsqueeze(0)).squeeze(0)\n```"
   },
   {
      "x": "How does pytorch computes on GPU?",
      "z": "Please ask questions on discuss.pytorch.org.",
      "y": "The question should be asked on pytorch discuss"
   },
   {
      "x": "Unexpected crash in autograd.grad",
      "z": "Hi,\n\nWe use github issues only for bugs or feature requests.\nPlease use the forum to ask questions: https://discuss.pytorch.org/\n\nFor your particular issue, this is not related to pytorch.\nWhat happens is that when you do `x * x` only, then the gradient return in the jacobian computation is the same as the one that is passed as input. And it is modified inplace to compute the full Jacobian. But that value is needed for the double backward computation.\nYou can fix this by:\n- Make sure that returned gradient is not the same by making the backward clone it (this is what happens when you multiply by 1)\n- Fix the jacobian code to not reuse the original Tensor by doing `jac.append(grad_x.reshape(x.shape).clone())`. ",
      "y": " Make sure that returned gradient is not the same by making the backward clone it (this is what happens when you multiply by 1)\nFix the jacobian code to not reuse the original Tensor by doing `jac.append(grad_x.reshape(x.shape).clone())`. "
   },
   {
      "x": "pytorch latest update(1.4) broke MultiStepLR: wrong LR after step from _get_closed_form_lr",
      "z": "@vincentqb thanks for your response.\n\nin my case i have sequence of lr schedulers that being replace by predefined steps (that can be dynamic, i.e. increase over time ), for example from epoch 0-5 use warmup lr scheduler from 10-55 use cosine annealing scheduler from 55 to 100 use reduce on reduce on plateau.\n\ni have a wrapper that switch between each scheduler and pass the epoch in each step, in order to make sure that each scheduler will be aware to the correct epoch.\n\n\nin any case it's seems that this issue is fixed at pytorch 1.6(i tested it at pytorch 1.4 and got 0.005 but know i am getting the correct value 0.05 so we can close it.)",
      "y": "The issue is fixed"
   },
   {
      "x": "test_DistributedDataParallel fails with parameter replication error",
      "z": "Looks like the pickle error was fixed in https://github.com/pytorch/pytorch/pull/37307, and the issue https://github.com/pytorch/pytorch/issues/37182 is now closed. So please close this issue if appropriate.",
      "y": "This pickle error is fixed"
   },
   {
      "x": "MacOS install error: Library not loaded: @rpath/libc++.1.dylib",
      "z": "Ok, I solved the issue, I don't know if it has any unwanted  impact in any way but now Torch is working.\nPlease, note I am using Mac Pycharm with a virtual environment so...\nI copied: libc++.1.dylib from usr/lib\nand I pace it in my environment: venv/lib/python3.7/site-packages/torch/lib\n\nI tested Torch with this script:\n\nfrom __future__ import print_function\nimport torch\nx = torch.rand(5, 3)\nprint(x)\n\nIt devolved:\n\ntensor([[0.3633, 0.7173, 0.6055],\n        [0.3442, 0.6892, 0.2950],\n        [0.3909, 0.4718, 0.0202],\n        [0.1055, 0.8912, 0.0667],\n        [0.3819, 0.4413, 0.1544]])\n\n\nI hope it work for others. I have been 3 days trying to find solutions in Google but not any solution found it.\n\n",
      "y": "from __future__ import print_function\nimport torch\nx = torch.rand(5, 3)\nprint(x)\n\nIt devolved:\n\ntensor([[0.3633, 0.7173, 0.6055],\n        [0.3442, 0.6892, 0.2950],\n        [0.3909, 0.4718, 0.0202],\n        [0.1055, 0.8912, 0.0667],\n        [0.3819, 0.4413, 0.1544]])"
   },
   {
      "x": "nightly nvcc fatal : Value 'c++14' is not defined for option 'std'",
      "z": "yes this is a breaking change to require c++14\n\nhere is a [link](https://discuss.pytorch.org/t/you-need-c-14-to-compile-pytorch/63185) that expaling a bit more",
      "y": "The bug is fixed"
   },
   {
      "x": "libtorch.so breaks google test",
      "z": "According to https://stackoverflow.com/a/53995379, we might be able to build googletest with `-D_GLIBCXX_USE_CXX11_ABI=0` to force it to use old ABI.",
      "y": "build googletest with `-D_GLIBCXX_USE_CXX11_ABI=0` to force it to use old ABI"
   },
   {
      "x": "Very Slow moving model to device with model.to(device)",
      "z": "@tayo If the issue is created by the context initialization, e.g. `torch.randn(1, device=\"cuda\")` should also take the same amount of time.\nDid you test it and do you see the slowdown?",
      "y": "This issue is fixed"
   },
   {
      "x": "Abort all nccl communicators explicitly when destroy process group",
      "z": "cc @jiayisuse - related to NCCL reliability/error handling",
      "y": "The issue is fixed"
   },
   {
      "x": "cannot initialize type \"WorkerId\"' crashing pytest in 1.4.0",
      "z": "thank you, that helped me figure it out. I think it's basically my fault.\n\nwhat was going on was I was basically using pip to install something which had pytorch as a dependency, then using conda to install pytorch, and I think I ended up with a weird mishmash of both versions.\n\nI think all that's happening is that conda hasn't updated their version to 1.4 yet. I'm pretty happy to close this if you are.\n\n\n\n\n\n",
      "y": "make sure there is no  mishmash of both versions pytorch and conda"
   },
   {
      "x": "Hi i found a typo in the Documentation",
      "z": "Hi thanks for the help i made a pull request to fix the typo .",
      "y": "The typo is fixed"
   },
   {
      "x": "Enable MKLDNN support  on Windows packages by default as well",
      "z": "Duplicate of #15982.",
      "y": "This is a duplicate issue"
   },
   {
      "x": "Memory leak in MaxPool2d",
      "z": "@mrshenli note that this is not linked to `max_pool2d`. In the for loop, replacing it with `torch._C._nn.max_pool2d_with_indices(x, 1)`, `torch._C._nn.log_sigmoid(x)`, `torch.threshold(x, 0, 0)` or `torch.relu(x)` all give the same behavior for me.\nAlso, this reproduces on my mac, but not on my centos machine.\n\nGiven this, could this be a similar issue as https://github.com/pytorch/vision/issues/984#issuecomment-498603088 ?",
      "y": " this is not linked to `max_pool2d`. In the for loop, replacing it with `torch._C._nn.max_pool2d_with_indices(x, 1)`, `torch._C._nn.log_sigmoid(x)`, `torch.threshold(x, 0, 0)` or `torch.relu(x)` all give the same behavior"
   },
   {
      "x": "Python 3.8 serialization error",
      "z": "As pointed in the CPython bug report, if you're unpickling from a mmap object, it's far more efficient to do so directly from memory:\n```python\ndata = pickle.loads(memoryview(mmap_object))\n```\nthan to issue `read()` calls by treating the mmap as a regular file object:\n```python\ndata = pickle.load(mmap_object)\n```\nThe former is able to do less memory copies and also avoids indirections through the file object.\n(try both on Python 3.7 and large pickled data)",
      "y": " if you're unpickling from a mmap object, it's far more efficient to do so directly from memory:\n```python\ndata = pickle.loads(memoryview(mmap_object))\n```\nthan to issue `read()` calls by treating the mmap as a regular file object:\n```python\ndata = pickle.load(mmap_object)\n```"
   },
   {
      "x": "PyTorch-1.4.0 doesn't encode numpy dependency",
      "z": "Fixed by https://github.com/pytorch/pytorch/pull/34510.",
      "y": "This issue is fixed"
   },
   {
      "x": "Pytorch 1.4.0 deadlock in multiprocessing",
      "z": "Upon revisiting the issue, the problem is actually with `mp.spawn`. I'm closing this issue and have created a new one at #32322 ",
      "y": "the problem is actually with `mp.spawn`"
   },
   {
      "x": "Second derivative fails if first derivative happens to be constant",
      "z": "Hi,\n\nThis is expected behavior.\nWe represent 0 gradient with any of the things below:\n\nTensor full of 0s\nNone (for p.grad for example)\nElement does not requires grad. Meaning that its gradient wrt all other Tensors that require gradients are 0.\nError in autograd.grad() when allow_unused=False stating that some inputs are not used and thus will have a gradient of 0.\nThe main reason why you see this behavior is because when we call backward, traversing the graph can be very expensive if we create it for everything. So in general, we try not to create the graph if we know that only 0 gradients will flow back.\nYou can see this for all the non-differentiable functions for example. Like the indices returned by max.",
      "y": "This is expected behaviour"
   },
   {
      "x": "NCCL_ROOT_DIR in Cmake FindNCCL",
      "z": "> Oh so basically cmake_cache_vars['USE_SYSTEM_NCCL'] is by default pointing to /usr/local/cuda for UNIX?\n\nGiven the above code, it will look for system NCCL using env vars `NCCL_LIBRARIES` and `NCCL_INCLUDE_DIRS`",
      "y": "cmake_cache_vars['USE_SYSTEM_NCCL'] is by default pointing to /usr/local/cuda for UNIX\n\nGiven the above code, it will look for system NCCL using env vars `NCCL_LIBRARIES` and `NCCL_INCLUDE_DIRS`"
   },
   {
      "x": "Inconsistent behaviour of `argmax` between `PyTorch` and `NumPy`",
      "z": "Removing cuda label as this issue is not specific to cuda. Adding docs label as this is a documentation issue. This is expected behavior, due to parallelization we don't make any guarantees on which element is returned, so it's not even guaranteed to be the last. See also #30708",
      "y": "This is expected behavior, due to parallelization we don't make any guarantees on which element is returned, so it's not even guaranteed to be the last."
   },
   {
      "x": "Pytorch 1.4.0 weight drop - 'LSTM' object has no attribute 'weight_hh_l0'",
      "z": "Unfortunately you can't fix it for pytorch 1.4.0, you need to get nightly package to get a fix. ",
      "y": "get nightly package to get a fix."
   },
   {
      "x": "Gaps for making template-unboxing work for all operators",
      "z": "ScalarType, Device, Layout, Dimname, Generator, and MemoryFormat can be added as types to JIT, and values to IValue. I think Generator may actually already be an unused type.\nStorage and ConstQuantizerPtr warrant more discussion. ConstQuantizerPtr is weirdly named, might be fine if it were called Quantizer. ",
      "y": "This issue is fixed"
   },
   {
      "x": "OneCycleLR mentions `verbose` as an argument on the doc page when it is not",
      "z": "Thanks for reporting! I'll leave it to our optimizer pocs to decide whether we want to change the code or docs.",
      "y": "This feature has been added"
   },
   {
      "x": "Build failed on Raspberry Pi: fatal error: gloo/algorithm.h: No such file or directory",
      "z": "Please try commenting out this line:\nhttps://github.com/pytorch/pytorch/blob/752f433a2484db25f076b2fc85c40ab191656bd9/test/cpp/rpc/CMakeLists.txt#L6\nFix is coming ",
      "y": "Please try commenting out this line:\nhttps://github.com/pytorch/pytorch/blob/752f433a2484db25f076b2fc85c40ab191656bd9/test/cpp/rpc/CMakeLists.txt#L6"
   },
   {
      "x": "torch.cuda.device not working but torch.cuda.set_device works",
      "z": "Hi,\n\n`torch.cuda.device()` is a context manager:\n```\ntorch.cuda.set_device(0)\n# On device 0\nwith torch.cuda.device(1):\n print(\"Inside device is 1\") \n # On device 1\nprint(\"Outside is still 0\")\n# On device 0\n```",
      "y": "`torch.cuda.device()` is a context manager: ``` torch.cuda.set_device(0) # On device 0 with torch.cuda.device(1): print(\"Inside device is 1\") # On device 1 print(\"Outside is still 0\") # On device 0 ```"
   },
   {
      "x": "Have torch.manual_seed seed all GPUs as well",
      "z": "Yes I think it's enough to add these two lines.\n\nIt's because there are no CUDA generators. There's only a single cuRAND state per device, and it's embedded in to THCState",
      "y": "Yes I think it's enough to add these two lines. It's because there are no CUDA generators. There's only a single cuRAND state per device, and it's embedded in to THCState"
   },
   {
      "x": "ImportError: libmkl_intel_lp64.so: cannot open shared object file",
      "z": "Try a clean re-install. \n```bash\nrm -rf build\nrm -rf torch/lib/build\n```\nThen install again",
      "y": "Try a clean re-install. \n```bash\nrm -rf build\nrm -rf torch/lib/build\n```"
   },
   {
      "x": "Variable.clone() does not clone to the same device",
      "z": "It's not expected. `clone()` should operate within a single device",
      "y": "It's not expected. `clone()` should operate within a single device"
   },
   {
      "x": "How to get raw pointer from tensors?",
      "z": "@SuperShinyEyes data_ptr for a GPU Tensor will point to a pointer to GPU memory. You cannot operate on that pointer directly, and have to give it to a CUDA kernel",
      "y": "for a GPU Tensor will point to a pointer to GPU memory. You cannot operate on that pointer directly, and have to give it to a CUDA kernel"
   },
   {
      "x": "Pytorch Freezes System",
      "z": "it's not typical at all, there's something weird going on, but it's not the GPU memory.\n\nUsually freezing indicates two things:\n- you are running out of CPU memory and you are hitting disk swap\n- a lot of hardware (say PCI-e or faulty GPU) / disk errors are being generated and the kernel is coping up with it slowly.\n\nI wonder if your case is either of them.",
      "y": "it's not typical at all, there's something weird going on, but it's not the GPU memory. Usually freezing indicates two things: - you are running out of CPU memory and you are hitting disk swap - a lot of hardware (say PCI-e or faulty GPU) / disk errors are being generated and the kernel is coping up with it slowly. I wonder if your case is either of them."
   },
   {
      "x": "Ellipsis encoding fails when printing tensors",
      "z": "I ran into this just now. What fixed it for me is to set the environment variable `LANG=C.UTF-8 LC_ALL=C.UTF-8`. This is in a Docker container running Ubuntu.",
      "y": "What fixed it for me is to set the environment variable `LANG=C.UTF-8 LC_ALL=C.UTF-8`."
   },
   {
      "x": "[JIT] jit.trace does not support parameter.requires_grad?",
      "z": "Yes, `jit.trace` only records Tensor operations. Modifying attributes of tensor objects are not recorded by design.",
      "y": "Yes, `jit.trace` only records Tensor operations. Modifying attributes of tensor objects are not recorded by design."
   },
   {
      "x": "torch.meshgrid has no docstring if typing.TYPE_CHECKING is True",
      "z": "> (and on the standard Python interpreter) returns `None`.\n\nThis means that it's not a regression.\n\nYou're using PyTorch 1.7.0, for which `meshgrid` simply does not have a docstring: https://github.com/pytorch/pytorch/blob/1.7/torch/functional.py#L352\n\nThis is fixed in 1.8.0 and in master (see https://github.com/pytorch/pytorch/blob/master/torch/functional.py#L417), so I'll close the issue.",
      "y": "This is fixed in 1.8.0 and in master"
   },
   {
      "x": "ManyLinux v1.8 release .whl for AArch64 (Arm) does not work on CentOS 8",
      "z": "Hi @malfet,\nThat works for me if I build my own whls, and I've just tested the latest 1.8.1 release from https://download.pytorch.org/whl/torch_stable.html on RHEL and Ubuntu systems here and it appears to work fine out of the box, cheers.",
      "y": "That works for me if I build my own whls, and I've just tested the latest 1.8.1 release from https://download.pytorch.org/whl/torch_stable.html on RHEL and Ubuntu systems here and it appears to work fine out of the box,"
   },
   {
      "x": "Add complex autograd support for torch.symeig",
      "z": "Just adding `'symeig'` to `GRADIENT_IMPLEMENTED_FOR_COMPLEX` of in `tools/autograd/gen_variable_type.py` resolves this issue.\nhttps://github.com/pytorch/pytorch/blob/master/tools/autograd/gen_variable_type.py#L73-L95",
      "y": "Just adding `'symeig'` to `GRADIENT_IMPLEMENTED_FOR_COMPLEX` of in `tools/autograd/gen_variable_type.py` resolves this issue"
   },
   {
      "x": "Pytorch install via Pip Error",
      "z": "verified this is fixed in 1.8.1\n`pip install torch==1.8.1+cu102 torchvision==0.9.1+cu102 torchaudio===0.8.1 -f https://download.pytorch.org/whl/torch_stable.html`\n\n![image](https://user-images.githubusercontent.com/32868157/112998017-367eb300-91a0-11eb-9ce4-b1aa031e6bcc.png)\n\nclose the issue.",
      "y": "verified this is fixed in 1.8.1 `pip install torch==1.8.1+cu102 torchvision==0.9.1+cu102 torchaudio===0.8.1"
   },
   {
      "x": "Exception in thread pool task: !completed() INTERNAL ASSERT FAILED",
      "z": "It looks like the actual crash is fixed in nightlies and we've added a couple PRs to improve error handling, so I'll go ahead and close this one out. Feel free to reopen if there are additional questions/issues.",
      "y": "It looks like the actual crash is fixed in nightlies and we've added a couple PRs to improve error handling,"
   },
   {
      "x": "[linear-algebra][discussion][proposal] Move linear algebra functions to torch.linalg, at::native::linalg",
      "z": "`torch.Tensor` does document these methods, but there isn't a formal organization (or a side-bar).\nI'm saying that if we want to improve discoverability, we probably want to do either of those.\nHowever, that will break the alphabetical-orderness of the current method list",
      "y": "`torch.Tensor` does document these methods, but there isn't a formal organization (or a side-bar)."
   },
   {
      "x": "Distributed Training shut down on second epoch",
      "z": "@wangdongxuking61 @mcarilli Thank you for your reply. The problem has been solved by building master.",
      "y": "This is fixed in 1.8.0 and in master"
   },
   {
      "x": "MultivariateNormal and potrf is slow on gpu and seems to have some memory leak",
      "z": "Thanks for the fix",
      "y": "This is fixed in 1.8.0 and in master"
   },
   {
      "x": "Quantization Model Support",
      "z": "this feature is planned for 1.1, we will have a proposal listed out soon.",
      "y": "This is fixed in 1.8.0 and in master"
   },
   {
      "x": "ImportError: cannot import name 'caffe2_pb2' (Ubuntu 18.04)",
      "z": "Don't start python from inside the pytorch directory, cd somewhere else and try again.",
      "y": "Don't start python from inside the pytorch directory, cd somewhere else and try again."
   },
   {
      "x": "RuntimeError \"sizes must be non-negative\" (0.4.1)",
      "z": "We didn't have proper empty tensor support, so neither behavior you saw for 0.4.1 and the one you saw for 0.4.0 is desired. Now on master (and in next release) we have proper empty tensor support. So you will get a proper [3, 4, 2, 0] shaped tensor soon!",
      "y": "This is fixed in 1.8.0 and in master"
   },
   {
      "x": "Unintuitive error message when loading state into incompatibly-sized model",
      "z": "This has been fixed since then.",
      "y": "This is fixed in 1.8.0 and in master"
   },
   {
      "x": "[JIT] Tracer throws runtime exception for torch.normal",
      "z": "I'm fixing this right now.",
      "y": "This is fixed in 1.8.0 and in master"
   },
   {
      "x": "RuntimeError: the derivative for 'target' is not implemented. When I use F.smooth_l1_loss(x, y, reduce=False)",
      "z": "Are you sure that the target is the second argument and not the first? You feed (GT, PRED) but it should be (PRED, GT)...",
      "y": "You feed (GT, PRED) but it should be (PRED, GT)..."
   },
   {
      "x": "nn.BatchNorm1d failed on GPU",
      "z": "Testing this on master gives this more descriptive error message:\n![image](https://user-images.githubusercontent.com/5652049/33382285-57f1cfc6-d4ee-11e7-930c-7ae29d5bf32e.png)\n\nThe problem is that you should change the `nn.BatchNorm1d` module to live on the GPU because it has weights that are initialized on the cpu:\n\n```\nimport torch\nfrom torch.autograd import Variable\na = Variable(torch.randn(2,5).cuda(), requires_grad=True)\nbatchnorm = torch.nn.BatchNorm1d(5).cuda()\ny = batchnorm(a)\n```",
      "y": "The problem is that you should change the `nn.BatchNorm1d` module to live on the GPU because it has weights that are initialized on the cpu: ``` import torch from torch.autograd import Variable a = Variable(torch.randn(2,5).cuda(), requires_grad=True) batchnorm = torch.nn.BatchNorm1d(5).cuda() y = batchnorm(a) ```"
   },
   {
      "x": "Aten compile error",
      "z": "If you have an existing version of PyTorch installed, this problem may be another manifestation of https://github.com/pytorch/pytorch/issues/3669, i.e. old headers are picked up. A workaround for https://github.com/pytorch/pytorch/issues/3669 is removing the old headers from system include path.",
      "y": "A workaround is removing the old headers from system include path."
   },
   {
      "x": "`backward` hangs in multiprocess after single-process",
      "z": "@adamlerer workaround is to add this right after `import torch.multiprocessing as mp`:\n\n```python\nif __name__ == \"__main__\":\n mp.set_start_method(\"spawn\")\n```\n\nEDIT: prefer \"spawn\" over \"forkserver\" (by @colesbury)",
      "y": "workaround is to add this right after `import torch.multiprocessing as mp`: ```python if __name__ == \"__main__\": mp.set_start_method(\"spawn\") ```"
   },
   {
      "x": "Out of memory with higher-order gradients involving batchnorm2d",
      "z": "Thanks, I can't reproduce this now in branch v0.3.",
      "y": "This is fixed in 0.3.0 and in master"
   },
   {
      "x": "0.2_4 release notes inconsistent with documentation and actual behavior of reduce functions",
      "z": "The release notes had a typo, sorry. It should've been: \n> For example torch.sum(torch.randn(10, 20), dim=0) returns a 1D Tensor.\n\nI've fixed them now.\n\nAs you see from the documentation, there are two prototypes of `torch.sum`, one with the `dim` argument and one without. the one with the `dim` argument is a global sum of the Tensor.",
      "y": "This is fixed in 1.8.0 and in master"
   },
   {
      "x": "RuntimeError: cublas runtime error : library not initialized",
      "z": "`sudo rm -rf ~/.nv` works.",
      "y": "`sudo rm -rf ~/.nv` works."
   },
   {
      "x": "Feature request: expm1",
      "z": "`torch.expm1` has been merged into master. I think this issue can be closed now.",
      "y": "This is fixed in master"
   },
   {
      "x": "How to use conda to update pytorch to 0.4 version",
      "z": "Hi,\n\n0.3 has been released now for conda and contains this feature.\nThe main website contains the informations to install 0.3 with conda.",
      "y": "The main website contains the informations to install 0.3 with conda."
   },
   {
      "x": "NVIDIA memory not deallocated after interupt",
      "z": "Hi,\n\nYou can run `lsof /dev/nvidia0` to list all processes using the GPU. One of them is taking some memory, kill it.",
      "y": "You can run `lsof /dev/nvidia0` to list all processes using the GPU. One of them is taking some memory, kill it."
   },
   {
      "x": "Error in building 0.3.0 in macOS High Sierra",
      "z": "Sure, you can download the Command Line Tools at https://download.developer.apple.com/Developer_Tools/Command_Line_Tools_macOS_10.12_for_Xcode_8.2/Command_Line_Tools_macOS_10.12_for_Xcode_8.2.dmg . I think that you need to switch to Xcode 8.2 first, and then install the package.",
      "y": "you can download the Command Line Tools at https://download.developer.apple.com/Developer_Tools/Command_Line_Tools_macOS_10.12_for_Xcode_8.2/Command_Line_Tools_macOS_10.12_for_Xcode_8.2.dmg . I think that you need to switch to Xcode 8.2 first, and then install the package."
   },
   {
      "x": "Test without backward the model will run out of memory",
      "z": "Same things apply for 0.4. For `volatile`, use `torch.no_grad()`",
      "y": "Same things apply for 0.4. For `volatile`, use `torch.no_grad()`"
   },
   {
      "x": "FAILED Build: __AVX2__ is defined (via e.g. -mavx2) \" \"but CAFFE2_PERF_WITH_AVX2 is not defined.",
      "z": "Found a better solution. It seems that the errors are not to do with build threads.\n\nIt turns out the errors were caused by cmake using Ninja instead of Make, if Ninja is installed. Adding USE_NINJA=OFF seems to fix it",
      "y": "It turns out the errors were caused by cmake using Ninja instead of Make, if Ninja is installed. Adding USE_NINJA=OFF seems to fix it"
   },
   {
      "x": "Segmentation fault (core dumped) in C++ API for centos",
      "z": "I think your GCC version is < 4.9 in Centos, which is ABI-incompatible with PyTorch, see https://github.com/pytorch/pytorch/issues/6987\n\nCould you try updating your GCC to be 4.9 or higher, following instructions from https://gist.github.com/goldsborough/d466f43e8ffc948ff92de7486c5216d6 for example?",
      "y": "try updating your GCC to be 4.9 or higher,"
   },
   {
      "x": "Non-deterministic behavior in pytorch even with seeds set",
      "z": "I can't speak about achieving determinism on GPU, but on CPU you also have to ensure that only one thread is being used, to avoid the accumulation of numerical errors resulting from performing the same operations in a slightly different order. You can use `torch.set_num_threads(1)` for this.",
      "y": "I can't speak about achieving determinism on GPU, but on CPU you also have to ensure that only one thread is being used, to avoid the accumulation of numerical errors resulting from performing the same operations in a slightly different order. You can use `torch.set_num_threads(1)` for this."
   },
   {
      "x": "torch.nn.utils.rnn.pack_padded_sequence not working in multi-GPU environments",
      "z": "You need to pass `device_ids` argument when you are wrapping your model in `DistributedDataParallel` so that each process is using only one GPU, as described in @soumith's link (Important notices, item 4).",
      "y": "You need to pass `device_ids` argument when you are wrapping your model in `DistributedDataParallel` so that each process is using only one GPU"
   },
   {
      "x": "export onnx model and load by caffe2 error",
      "z": "@cswwp onnx_caffe2 is out of date. It is merged to caffe2. We have a PR to update the tutorial: \nhttps://github.com/pytorch/tutorials/pull/348\n\nAlso here is a tutorial, which should work: https://github.com/onnx/tutorials/blob/master/tutorials/OnnxCaffe2Import.ipynb",
      "y": "onnx_caffe2 is out of date. It is merged to caffe2. Also here is a tutorial, which should work: https://github.com/onnx/tutorials/blob/master/tutorials/OnnxCaffe2Import.ipynb"
   },
   {
      "x": "Memory Error in pip install of torch 1.2.0 on Linux",
      "z": "> \n> .. `pip install --no-cache-dir install torchvision` seems to have gotten around the issue.\n\nI guess there is a typo in your command. The command which worked for me: \n\n`pip --no-cache-dir install torchvision`",
      "y": "`pip install --no-cache-dir install torchvision` seems to have gotten around the issue. I guess there is a typo in your command. The command which worked for me: `pip --no-cache-dir install torchvision`"
   },
   {
      "x": "ImportError: libcudart.so.9.0: cannot open shared object file: No such file or directory",
      "z": "Sorry, I didn't see the post carefully. Would you please try whether the following command solves your problem? `pip3 install torch===1.2.0 torchvision===0.4.0 -f https://download.pytorch.org/whl/torch_stable.html`",
      "y": "Would you please try whether the following command solves your problem? `pip3 install torch===1.2.0 torchvision===0.4.0 -f https://download.pytorch.org/whl/torch_stable.html`"
   },
   {
      "x": "torch.load issue on loading file created by torch.save",
      "z": "Thanks for the concise repro! This does indeed seem to be a bug, there is a fix in #25279.",
      "y": "This is fixed in 1.8.0 and in master"
   },
   {
      "x": "Trouble installing PyTorch for CUDA 9.0",
      "z": "> How do I install it in this case?\n\n1. Open the link [https://download.pytorch.org/whl/cu90/torch_stable.html](https://download.pytorch.org/whl/cu90/torch_stable.html) in a browser\n2. Go into the source \n3. Download the appropriate version. For `PyTorch 1.1.0`, `CUDA 9.0` and `Python 3.6` I have downloaded `torch-1.1.0-cp36-cp36m-linux_x86_64.whl`, If you have other requirements then download appropriate `.whl` file\n4. Install via `pip install [downloaded .whl file]`.",
      "y": "> How do I install it in this case?\n\n1. Open the link [https://download.pytorch.org/whl/cu90/torch_stable.html](https://download.pytorch.org/whl/cu90/torch_stable.html) in a browser\n2. Go into the source \n3. Download the appropriate version. For `PyTorch 1.1.0`, `CUDA 9.0` and `Python 3.6` I have downloaded `torch-1.1.0-cp36-cp36m-linux_x86_64.whl`, If you have other requirements then download appropriate `.whl` file\n4. Install via `pip install [downloaded .whl file]`."
   },
   {
      "x": "problem with mkldnn and march=native",
      "z": "@branfosj, we solve this problem in #25757, thanks!",
      "y": "This is fixed in 1.8.0 and in master"
   },
   {
      "x": "Unknown Error at torch/lib/THC/THCGeneral.c:66",
      "z": "cuda error unknown happens for many weird reasons.\nHere's one try to fix it:\n\n```\n$ sudo python\n>>> import torch\n>>> a = torch.randn().cuda()\n```\nThen exit.\n\nThis might initialize the device drivers properly. Sometimes, the nvidia device files are not properly created under /dev/ and sudo helps.",
      "y": "cuda error unknown happens for many weird reasons.\nHere's one try to fix it:\n\n```\n$ sudo python\n>>> import torch\n>>> a = torch.randn().cuda()\n```\nThen exit.\n\nThis might initialize the device drivers properly. Sometimes, the nvidia device files are not properly created under /dev/ and sudo helps."
   },
   {
      "x": "LSTM memory leak?",
      "z": "It was probably the autograd refactor that removed Variables from the graph (they were replaced with AccumulateGrad nodes). The leak was likely a reference cycle",
      "y": "It was probably the autograd refactor that removed Variables from the graph (they were replaced with AccumulateGrad nodes). The leak was likely a reference cycle"
   },
   {
      "x": "How to specify the cuda PATH in pytorch?",
      "z": "Try running `CUDA_HOME=/path/to/cuda-version python setup.py install`",
      "y": "Try running `CUDA_HOME=/path/to/cuda-version python setup.py install`"
   },
   {
      "x": "Confusing error msg when padding is set to float in nn.Conv1d",
      "z": "To clarify, I think that it might be helpful if the RuntimeError is updated to include the type of the incorrect tuple, so its clear that the type is expected to be an int.\n```python\nimport torch\nimport torch.nn as nn\nfrom torch.autograd import Variable\ninput = Variable(torch.randn(1, 1, 10))\noutput = nn.Conv1d(1, 1, 3, padding=1)(input) # fine\noutput = nn.Conv1d(1, 1, 3, padding=1.0)(input) # error\n```",
      "y": "I think that it might be helpful if the RuntimeError is updated to include the type of the incorrect tuple, so its clear that the type is expected to be an int.\n```python\nimport torch\nimport torch.nn as nn\nfrom torch.autograd import Variable\ninput = Variable(torch.randn(1, 1, 10))\noutput = nn.Conv1d(1, 1, 3, padding=1)(input) # fine\noutput = nn.Conv1d(1, 1, 3, padding=1.0)(input) # error\n```"
   },
   {
      "x": "Implement toCFloat() for Variables and numel() = 1 Tensors",
      "z": "this is now implemented.",
      "y": "this is now implemented."
   },
   {
      "x": "cuda 9.0 not found but detected 6.1",
      "z": "CUDA architecture is something different than the driver and toolkit version (you can think of it as version of your hardware - it will be the same no matter what's the driver). 6.1 are the Pascal GPUs. You're all good.",
      "y": "CUDA architecture is something different than the driver and toolkit version (you can think of it as version of your hardware - it will be the same no matter what's the driver). 6.1 are the Pascal GPUs. You're all good."
   },
   {
      "x": "Why the Dropout2d and BatchNorm2d's model.eval() result is poor",
      "z": "decrease momentum value in BatchNorm layer to something small like 0.0001",
      "y": "decrease momentum value in BatchNorm layer to something small like 0.0001"
   },
   {
      "x": "2 processes cannot use same GPU",
      "z": "It happens because your GPUs are in `EXCLUSIVE_PROCESS` mode, so the CUDA driver will forbid two processes from using the same GPU. You should be able to change that by running `nvidia-smi -g <GPU number> -c 0`.",
      "y": "It happens because your GPUs are in `EXCLUSIVE_PROCESS` mode, so the CUDA driver will forbid two processes from using the same GPU. You should be able to change that by running `nvidia-smi -g <GPU number> -c 0`."
   },
   {
      "x": "how could i get old version of libtorch , thanks",
      "z": "For example, \nhttps://download.pytorch.org/libtorch/cu101/libtorch-shared-with-deps-1.4.0.zip",
      "y": "https://download.pytorch.org/libtorch/cu101/libtorch-shared-with-deps-1.4.0.zip"
   },
   {
      "x": "Export to ONNX fails using F.interpolate with mode='area'",
      "z": "@omasaht - `area` mode is not explicitly supported in ONNX spec, and therefore, not supported in the exporter.",
      "y": "area` mode is not explicitly supported in ONNX spec, and therefore, not supported in the exporter."
   },
   {
      "x": "Performance regression for inference from pytorch 1.4.0 to >= 1.5.0",
      "z": "Thanks @ezyang. I tried with the nightly build (`1.7.0.dev20200705+cpu`) and I indeed see performance similar to 1.4.0. I am going to close the issue.",
      "y": "I tried with the nightly build (`1.7.0.dev20200705+cpu`) and I indeed see performance similar to 1.4.0"
   },
   {
      "x": "Error installing 0.3.0 from Anaconda on MacOS 10.13.1",
      "z": "Update conda first with `conda update conda` and try again",
      "y": "Update conda first with `conda update conda` and try again"
   },
   {
      "x": "Segmentation Fault when importing Torch",
      "z": "Append '/usr/lib/nvidia-384' or whichever nvidia driver version is being used to the LD_LIBRARY_PATH Environment variable. This worked out for me! Here is a reference - https://stackoverflow.com/questions/42678439/importerror-libnvidia-fatbinaryloader-so-375-39-cannot-open-shared-object-file",
      "y": "Append '/usr/lib/nvidia-384' or whichever nvidia driver version is being used to the LD_LIBRARY_PATH Environment variable."
   },
   {
      "x": "How to only padding the bottom when use the Conv2D ?",
      "z": "use the [functional padding method](http://pytorch.org/docs/master/nn.html#torch.nn.functional.pad)",
      "y": "use the [functional padding method](http://pytorch.org/docs/master/nn.html#torch.nn.functional.pad)"
   },
   {
      "x": "how should I cite PyTorch in the paper?",
      "z": "For now you could cite our NIPS 2017 workshop paper that discusses just the autodiff engine of PyTorch:\n\n```\n@article{paszke2017automatic,\n title={Automatic differentiation in PyTorch},\n author={Paszke, Adam and Gross, Sam and Chintala, Soumith and Chanan, Gregory and Yang, Edward and DeVito, Zachary and Lin, Zeming and Desmaison, Alban and Antiga, Luca and Lerer, Adam},\n year={2017}\n}\n```\n\nThe paper is located here: https://openreview.net/forum?id=BJJsrmfCZ\n\nOnce we release a full paper (with more authors), I will update this thread with a new citation.",
      "y": "For now you could cite our NIPS 2017 workshop paper that discusses just the autodiff engine of PyTorch:\n\n```\n@article{paszke2017automatic,\n title={Automatic differentiation in PyTorch},\n author={Paszke, Adam and Gross, Sam and Chintala, Soumith and Chanan, Gregory and Yang, Edward and DeVito, Zachary and Lin, Zeming and Desmaison, Alban and Antiga, Luca and Lerer, Adam},\n year={2017}\n}\n```\n\nThe paper is located here: https://openreview.net/forum?id=BJJsrmfCZ"
   },
   {
      "x": "non-cudnn LSTM and GRU biases have wrong shapes",
      "z": "Fix now included in https://github.com/pytorch/pytorch/pull/1683",
      "y": "Fix now included in https://github.com/pytorch/pytorch/pull/1683"
   },
   {
      "x": "Is there any API to visualize the architecture of model?",
      "z": "There's [@szagoruyko script](https://github.com/szagoruyko/functional-zoo/blob/master/visualize.py) that produces a `dot` file",
      "y": "There's (https://github.com/szagoruyko/functional-zoo/blob/master/visualize.py) that produces a `dot` file"
   },
   {
      "x": "Add a torch.matmul function and document broadcast behavior of it and delegated functions",
      "z": "Yes, `matmul` is present in master and in v0.2",
      "y": "Yes, `matmul` is present in master and in v0.2"
   },
   {
      "x": "Add SELU (Scaled ELU)",
      "z": "It looks like it can be implemented somewhat efficiently in one line:\n```python\nimport torch.nn.functional as F\ndef selu(x):\n alpha = 1.6732632423543772848170429916717\n scale = 1.0507009873554804934193349852946\n return scale * F.elu(x, alpha)\n```\nNote that pytorch `elu` has the `alpha` parameter built-in, which makes it easier to implement than in TF. For an overview of what the `alpha` parameter do, you can have a [look here](https://github.com/pytorch/pytorch/blob/master/torch/lib/THNN/generic/ELU.c#L23).",
      "y": "It looks like it can be implemented somewhat efficiently in one line: ```python import torch.nn.functional as F def selu(x): alpha = 1.6732632423543772848170429916717 scale = 1.0507009873554804934193349852946 return scale * F.elu(x, alpha) ```"
   },
   {
      "x": "What to do if CUDA doesn't work",
      "z": "> where it used to work earlier and it didn't all of a sudden. Before trying any of the solutions I restarted my computer, and it worked fine.\n\nThis usually happens when the nvidia driver gets updated.",
      "y": "where it used to work earlier and it didn't all of a sudden. Before trying any of the solutions I restarted my computer, and it worked fine. This usually happens when the nvidia driver gets updated."
   },
   {
      "x": "Missing bernoulli_ on torch.cuda.ByteTensor for nn.AlphaDropout",
      "z": "So I think we should really remove `bernoulli_` as it's very ambiguous. In this case, I think we should use `torch.bernoulli(p, out=input.data.new().byte())` (until we add `dtype`)",
      "y": "we should really remove `bernoulli_` as it's very ambiguous. In this case, I think we should use `torch.bernoulli(p, out=input.data.new().byte())` (until we add `dtype`)"
   },
   {
      "x": "RuntimeError: invalid multinomial distribution (encountering probability entry < 0)",
      "z": "When this error happens, probs_2d equals to `tensor([[nan, nan, nan, nan, nan, nan, nan]])`",
      "y": "When this error happens, probs_2d equals to `tensor([[nan, nan, nan, nan, nan, nan, nan]])`"
   },
   {
      "x": "Exception using optimize_for_mobile on retinanet from torchvision",
      "z": "Fixed on master with: https://github.com/pytorch/pytorch/pull/46285",
      "y": "Fixed on master"
   },
   {
      "x": "The return of torch.inverse contains nan sometime",
      "z": "You can still use multi-stream if you properly register all the tensors to the correct streams",
      "y": "You can still use multi-stream if you properly register all the tensors to the correct streams"
   },
   {
      "x": "[jit] create dict or list from zip",
      "z": "fixed with https://github.com/pytorch/pytorch/pull/42382",
      "y": "Fixed on master"
   },
   {
      "x": "CUDAExtension: nvcc does not pick right gcc by default",
      "z": "It seems like that this issue has been fixed by https://github.com/pytorch/pytorch/commit/4134b7abfa60a659de27736704c62277ecf291d2#diff-258d5272916c8cf5ac89aad77f7c114b585848826f6bbaeee91fe62a4391ee5e",
      "y": "Fixed on master"
   },
   {
      "x": "Update how to build PyTorch with CUDA Windows instructions",
      "z": "@mszhanyi I think we should just say that we support cuda >= 10.1 and vs >= 2019.",
      "y": "I think we should just say that we support cuda >= 10.1 and vs >= 2019."
   },
   {
      "x": "Unable to create TorchScript compatible CUDA extension",
      "z": "@wanchaol I already have a fix https://github.com/pytorch/pytorch/pull/47492 and a test https://github.com/pytorch/pytorch/pull/47524",
      "y": "Fixed on master"
   },
   {
      "x": "Migrate `set_` from the TH to Aten (CUDA)",
      "z": "https://github.com/pytorch/pytorch/pull/34403",
      "y": "Fixed in Master"
   },
   {
      "x": "Support cast of single-element tensors to numbers",
      "z": "Yup \n\n```python\n>>> import numpy as np\n>>> zero_dim = np.array(3)\n>>> scalar = np.float(3)\n>>> single_elem = np.array([[[3]]])\n>>> float(zero_dim), float(scalar), float(single_elem)\n(3.0, 3.0, 3.0)\n>>> empty = np.array([], dtype=np.float32)\n>>> float(empty)\nTraceback (most recent call last):\n File \"<stdin>\", line 1, in <module>\nTypeError: only length-1 arrays can be converted to Python scalars\n\n```",
      "y": "Yup \n\n```python\n>>> import numpy as np\n>>> zero_dim = np.array(3)\n>>> scalar = np.float(3)\n>>> single_elem = np.array([[[3]]])\n>>> float(zero_dim), float(scalar), float(single_elem)\n(3.0, 3.0, 3.0)\n>>> empty = np.array([], dtype=np.float32)\n>>> float(empty)\nTraceback (most recent call last):\n File \"<stdin>\", line 1, in <module>\nTypeError: only length-1 arrays can be converted to Python scalars\n\n```"
   },
   {
      "x": "CUDNN_STATUS_NOT_INITIALIZED",
      "z": "pytorch ships it's own cudnn",
      "y": "pytorch ships it's own cudnn"
   },
   {
      "x": "Tensor flattening on broadcast doesn't handle heterogeneous types",
      "z": "fixed in master. will be in next release.",
      "y": "fixed in master"
   },
   {
      "x": "Anaconda installation CUDA requested, cpuonly obtained",
      "z": "In case someone still runs into this: try setting `conda config --set channel_priority strict`.\n\nA similar case was reported on Slack. After some investigation, I could trigger the cpu-only install by first installing 1.5.0 and then downgrading to 1.4.0 without having `channel_priority strict` set. \n\nThis will install the `cuda10.1` package:\n```\nconda create -n pytorch14\nconda activate pytorch14\nconda install pytorch==1.4.0 -c pytorch\n```\n\nStarting again in a clean env:\n```\nconda create -n pytorch14\nconda activate pytorch14\nconda install pytorch==1.5.0 -c pytorch # installs cuda10.1 version\nconda install pytorch==1.4.0 -c pytorch # wants to install cpu_only, say \"no\"\n\nconda config --set channel_priority strict\nconda install pytorch==1.4.0 -c pytorch # installs cuda10.1 version\n```",
      "y": "In case someone still runs into this: try setting `conda config --set channel_priority strict`.\n\nA similar case was reported on Slack. After some investigation, I could trigger the cpu-only install by first installing 1.5.0 and then downgrading to 1.4.0 without having `channel_priority strict` set. \n\nThis will install the `cuda10.1` package:\n```\nconda create -n pytorch14\nconda activate pytorch14\nconda install pytorch==1.4.0 -c pytorch\n```\n\nStarting again in a clean env:\n```\nconda create -n pytorch14\nconda activate pytorch14\nconda install pytorch==1.5.0 -c pytorch # installs cuda10.1 version\nconda install pytorch==1.4.0 -c pytorch # wants to install cpu_only, say \"no\"\n\nconda config --set channel_priority strict\nconda install pytorch==1.4.0 -c pytorch # installs cuda10.1 version\n```"
   },
   {
      "x": "how to install pytorch on AMD GPU",
      "z": "What about `python -m pip install torch==1.4.0+cpu torchvision==0.5.0+cpu -f https://download.pytorch.org/whl/torch_stable.html`?",
      "y": "`python -m pip install torch==1.4.0+cpu torchvision==0.5.0+cpu -f https://download.pytorch.org/whl/torch_stable.html`?"
   },
   {
      "x": "AttributeError: module 'torch.nn.utils' has no attribute 'prune'",
      "z": "you need to `import torch.nn.utils.prune` not only `import torch`",
      "y": "you need to `import torch.nn.utils.prune` not only `import torch`"
   },
   {
      "x": "Flaky test test_cos_scalar_cpu TestAutogradDeviceTypeCPU on MacOS",
      "z": "@albanD and I agreed that we should just zero initialize `tmp_values` in this case. Since the `count != size()` case is only used for the ragged end it's fine if we add a few more instructions here, since it's not in the tight loop.",
      "y": "we should just zero initialize `tmp_values` in this case. Since the `count != size()` case is only used for the ragged end it's fine if we add a few more instructions here, since it's not in the tight loop."
   },
   {
      "x": "Numerical problems with torch.nn.functional.kl_div",
      "z": "Ok, it seems I know where the issue is. The issue is with `log(softmax)`. Computing `log(softmax)` in a straightforward fashion is not numerically stable, we can rescale a numerator and a denominator by any exponent prior to taking `log` and take into account that we actually deal with ratios. `softmax/log_softmax` perform this rescaling by premultiplying with `exp{max prob}` and deals with subtractions instead of divisions...\nSo, probabilities might have some structure to which the KL-divergence implementation is oblivious. It could be useful to accept `target` in both the original space and the log-space if the direct computation of `log(target)` is not optimal.",
      "y": "The issue is with `log(softmax)`. Computing `log(softmax)` in a straightforward fashion is not numerically stable, we can rescale a numerator and a denominator by any exponent prior to taking `log` and take into account that we actually deal with ratios. `softmax/log_softmax` perform this rescaling by premultiplying with `exp{max prob}` and deals with subtractions instead of divisions... So, probabilities might have some structure to which the KL-divergence implementation is oblivious. It could be useful to accept `target` in both the original space and the log-space if the direct computation of `log(target)` is not optimal."
   },
   {
      "x": "Instantiating `torch.distributions.Categorical` with all-zero long-dtype probabilities crashes Python",
      "z": "It's the divide-by-0 behavior. `torch.zeros(1,dtype=torch.float) / 0` gives `nan`, `torch.zeros(1,dtype=torch.long) / 0` crashes.",
      "y": "It's the divide-by-0 behavior. `torch.zeros(1,dtype=torch.float) / 0` gives `nan`, `torch.zeros(1,dtype=torch.long) / 0` crashes."
   },
   {
      "x": "RuntimeError: cuDNN error: CUDNN_STATUS_NOT_SUPPORTED. This error may appear if you passed in a non-contiguous input",
      "z": "It happens to me as well with \npytorch=1.5.0\npy3.8\ncuda10.1.243\ncudnn7.6.3_0\n\nIt happens when I reach a batch normalization layer with a huge batch size, but when I decrease the batch size the error is gone. It is probably a memory issue that happens when a batch is too big.",
      "y": "It happens when I reach a batch normalization layer with a huge batch size, but when I decrease the batch size the error is gone. It is probably a memory issue that happens when a batch is too big."
   },
   {
      "x": "Input and output tensors to `torch._C.Node`",
      "z": "You can use `.inputs()` and `.outputs()` to get the input and output values, but there are no tensors since you're inspecting a representation of a program that's not running yet. Those APIs are internal, so we won't be writing documentation for them just yet.",
      "y": "You can use `.inputs()` and `.outputs()` to get the input and output values, but there are no tensors since you're inspecting a representation of a program that's not running yet. Those APIs are internal, so we won't be writing documentation for them just yet."
   },
   {
      "x": "RuntimeError: CUDA out of memory. Tried to allocate 12.50 MiB (GPU 0; 10.92 GiB total capacity; 8.57 MiB already allocated; 9.28 GiB free; 4.68 MiB cached)",
      "z": "It is because of mini-batch of data does not fit on to GPU memory. Just decrease the batch size. When I set batch size = 256 for cifar10 dataset I got the same error; Then I set the batch size = 128, it is solved.",
      "y": "It is because of mini-batch of data does not fit on to GPU memory. Just decrease the batch size. When I set batch size = 256 for cifar10 dataset I got the same error; Then I set the batch size = 128, it is solved."
   },
   {
      "x": "tensor multinomial dependent on shape for some reason",
      "z": "For me, it was \n```\nimport torch\nimport numpy as np\nfoo3 = torch.from_numpy(np.array([[[0.25, 0.25, 0.25, 0.25]]]))\ntorch.multinomial(foo3, 10, True)\n# tensor([[0, 0, 0, 0, 0, 0, 0, 0, 0, 0]])\n```",
      "y": "For me, it was \n```\nimport torch\nimport numpy as np\nfoo3 = torch.from_numpy(np.array([[[0.25, 0.25, 0.25, 0.25]]]))\ntorch.multinomial(foo3, 10, True)\n# tensor([[0, 0, 0, 0, 0, 0, 0, 0, 0, 0]])\n```"
   },
   {
      "x": "Implement target derivative for smooth L1 loss",
      "z": "In the meantime, you can implement `smooth_l1_loss` via the following:\n```python\ndef smooth_l1_loss(input, target, beta=1, size_average=True):\n \"\"\"\n very similar to the smooth_l1_loss from pytorch, but with\n the extra beta parameter\n \"\"\"\n n = torch.abs(input - target)\n cond = n < beta\n loss = torch.where(cond, 0.5 * n ** 2 / beta, n - 0.5 * beta)\n if size_average:\n return loss.mean()\n return loss.sum()\n```",
      "y": "In the meantime, you can implement `smooth_l1_loss` via the following:\n```python\ndef smooth_l1_loss(input, target, beta=1, size_average=True):\n \"\"\"\n very similar to the smooth_l1_loss from pytorch, but with\n the extra beta parameter\n \"\"\"\n n = torch.abs(input - target)\n cond = n < beta\n loss = torch.where(cond, 0.5 * n ** 2 / beta, n - 0.5 * beta)\n if size_average:\n return loss.mean()\n return loss.sum()\n```"
   },
   {
      "x": "When can PyTorch support for RTX series GPU?",
      "z": "closing this issue, because PyTorch now works and ships with CUDA10.",
      "y": "PyTorch works and ships with CUDA10."
   },
   {
      "x": "Naming conflict of `test_api` in Windows release builds for wheels",
      "z": "@JerrikEph It is actually mkldnn that is out of sync. So running the following script should help.\n```bash\ngit submodule sync\ngit submodule update --init --recursive\n```",
      "y": "@JerrikEph It is actually mkldnn that is out of sync. So running the following script should help.\n```bash\ngit submodule sync\ngit submodule update --init --recursive\n```"
   },
   {
      "x": "Lack of Square Function",
      "z": "In the one hand, we have a `sqrt` function, so adding a `square` function makes sense. In the other hand, I'd pretty much prefer avoid bloating the API with \"redundant\" functions (and we get the same behavior with less typing by just doing `a ** 2`).\nBut I'll let @soumith decide on that.",
      "y": "In the one hand, we have a `sqrt` function, so adding a `square` function makes sense. In the other hand, I'd pretty much prefer avoid bloating the API with \"redundant\" functions (and we get the same behavior with less typing by just doing `a ** 2`)."
   },
   {
      "x": "Combine Variable and Tensor APIs (Perform autograd directly on torch.Tensor)",
      "z": "This is done. (Still some clean-up to do)",
      "y": "Fixed in Master"
   },
   {
      "x": "How to convert to older version of pytorch v0.1.12?",
      "z": "`conda install pytorch=0.1.12 cuda80 -c soumith`\nWe'll update the docs soon to give links to older versions.",
      "y": "`conda install pytorch=0.1.12 cuda80 -c soumith`\n"
   },
   {
      "x": "add documentation / links for old binaries",
      "z": "you can find the linux wheels\n- http://download.pytorch.org/whl/cu80/torch-0.1.12.post2-cp27-none-linux_x86_64.whl (cuda 8)\n- http://download.pytorch.org/whl/cu75/torch-0.1.12.post2-cp27-none-linux_x86_64.whl (cuda 7.5)\n\nand the mac wheel: \n- http://download.pytorch.org/whl/torch-0.1.12.post2-cp27-none-macosx_10_7_x86_64.whl",
      "y": "you can find the linux wheels\n- http://download.pytorch.org/whl/cu80/torch-0.1.12.post2-cp27-none-linux_x86_64.whl (cuda 8)\n- http://download.pytorch.org/whl/cu75/torch-0.1.12.post2-cp27-none-linux_x86_64.whl (cuda 7.5)\n\nand the mac wheel: \n- http://download.pytorch.org/whl/torch-0.1.12.post2-cp27-none-macosx_10_7_x86_64.whl"
   },
   {
      "x": "datasets.ImageFolder error \"in __getattr__ raise AttributeError(name) AttributeError: __exit__\"",
      "z": "I had the same issue, and upgrading the Pillow library resolved it.\nTry this:\n`pip install --upgrade Pillow `",
      "y": "I had the same issue, and upgrading the Pillow library resolved it.\nTry this:\n`pip install --upgrade Pillow `"
   },
   {
      "x": "RuntimeError: cuda runtime error (46) : all CUDA-capable devices are busy or unavailable at /opt/conda/conda-bld/pytorch_1502004572321/work/torch/lib/THC/generic/THCStorage.cu:66",
      "z": "Restart the program with os.environ[\"CUDA_VISIBLE_DEVICES\"] = '0'",
      "y": "Restart the program with os.environ[\"CUDA_VISIBLE_DEVICES\"] = '0'"
   },
   {
      "x": "CondaError: CondaHTTPError: HTTP 000 CONNECTION FAILED for url <https://conda.anaconda.org/soumith/linux-64/pytorch-0.2.0-py35hb1547bd_4cu80.tar.bz2>",
      "z": "hmmm, could you try the `pip` based install (see instructions on pytorch.org ). Maybe conda.anaconda.org is blocked in your network.\nSince there is not much we can do at pytorch end, i am closing the issue.",
      "y": "could you try the `pip` based install (see instructions on pytorch.org ). Maybe conda.anaconda.org is blocked in your network."
   },
   {
      "x": "Same ADD operation, Tensor has a different result from Numpy",
      "z": "> @siyuhsu did you close this issue on purpose?\n\nThis problem is solved by upgrading PyTorch version. Only Pytorch `1.3.0` has this problem. Please see https://github.com/pytorch/pytorch/issues/54650#issuecomment-806389825",
      "y": "This problem is solved by upgrading PyTorch version."
   },
   {
      "x": "`@torch.jit.ignore` and `@property`",
      "z": "`@ignore` = cannot run in TorchScript and is executed in Python if called, disables export\n`@unused` = cannot run in TorchScript nor Python and throws an error if called",
      "y": "`@ignore` = cannot run in TorchScript and is executed in Python if called, disables export\n`@unused` = cannot run in TorchScript nor Python and throws an error if called"
   },
   {
      "x": "torch.linalg.svd out of memory",
      "z": "The code in LinearAlgebraUtils.h for svd is wrong it incorrectly initializes large matrix and then narrows it if full_matrices=False. The whole svd code was not refactored during recent linalg updates, only linalg_svd wrapper of the old code was added.",
      "y": "The code in LinearAlgebraUtils.h for svd is wrong it incorrectly initializes large matrix and then narrows it if full_matrices=False. The whole svd code was not refactored during recent linalg updates, only linalg_svd wrapper of the old code was added."
   },
   {
      "x": "Please verify ONNX v1.10.0 Release Candidate",
      "z": "Tests passed locally with onnx submodule at a57bc99daa6ddeef2ad535f8f78d1847f57216f0, which I guess is two commits behind RC2, but the 2 missing commits seem not problematic.\nI ran:\n`python test/onnx/test_pytorch_onnx_onnxruntime.py`",
      "y": "Tests passed locally with onnx submodule at a57bc99daa6ddeef2ad535f8f78d1847f57216f0, which I guess is two commits behind RC2, but the 2 missing commits seem not problematic.\nI ran:\n`python test/onnx/test_pytorch_onnx_onnxruntime.py`"
   },
   {
      "x": "Unable to install 1.2",
      "z": "Pytorch 1.2 just went live on conda for Windows",
      "y": "Pytorch 1.2 just went live on conda for Windows"
   },
   {
      "x": "problem with torch.util.tensorboard add_graph()",
      "z": "I too am getting a graph page that is empty. I did flush and close the SummaryWriter.\nAttaching screenshot including Chrome's console which shows an error that may be related.\n\nNote:\n* I do see the textual graph being dumped to the command line console and it seems correct there.\n\n**Configuration:**\n* PyTorch 1.2.0\n* TensoBoard 1.14.0\n* Python 3.5.2\n\n![image](https://user-images.githubusercontent.com/345348/63727390-07120280-c815-11e9-9439-e9ee0827ebd4.png)",
      "y": "I too am getting a graph page that is empty. I did flush and close the SummaryWriter.\nAttaching screenshot including Chrome's console which shows an error that may be related.\n\nNote:\n* I do see the textual graph being dumped to the command line console and it seems correct there.\n\n**Configuration:**\n* PyTorch 1.2.0\n* TensoBoard 1.14.0\n* Python 3.5.2\n\n![image](https://user-images.githubusercontent.com/345348/63727390-07120280-c815-11e9-9439-e9ee0827ebd4.png)"
   },
   {
      "x": "Implement torch.uniform",
      "z": "I think this already exists?\n\n```python\na = torch.tensor([0., 10.])\nb = torch.tensor([1., 11.])\ntorch.distributions.Uniform(a, b).sample()\n>>> tensor([ 0.8583, 10.0226])\n```",
      "y": "\n```python\na = torch.tensor([0., 10.])\nb = torch.tensor([1., 11.])\ntorch.distributions.Uniform(a, b).sample()\n>>> tensor([ 0.8583, 10.0226])\n```"
   },
   {
      "x": "How to install torchaudio on Mac M1 ARM?",
      "z": "Hi, this issue is being tracked on the torchaudio repository, https://github.com/pytorch/audio/issues/1573, please add input there.",
      "y": "Hi, this issue is being tracked on the torchaudio repository, https://github.com/pytorch/audio/issues/1573"
   },
   {
      "x": "Pytorch 1.2.0 RuntimeError: code is too big",
      "z": "The fix is released in [Intel MKL-DNN v0.20.2](https://github.com/intel/mkl-dnn/releases/tag/v0.20.2). \n\n@gujinghui, @jgong5, could you please help with instruction on building Pytorch with updated Intel MKL-DNN?",
      "y": "The fix is released in [Intel MKL-DNN v0.20.2](https://github.com/intel/mkl-dnn/releases/tag/v0.20.2)."
   },
   {
      "x": "Very Slow Moving Tensor to CUDA device (CUDA 10.1 with PyTorch 1.3)",
      "z": "This issue is now fixed with newly updated binaries.\nUninstalling and reinstalling PyTorch from Anaconda will fix it.",
      "y": "This issue is now fixed with newly updated binaries.\nUninstalling and reinstalling PyTorch from Anaconda will fix it."
   },
   {
      "x": "Indexed, in-place multiplication segfaults/drops values",
      "z": "Yes, it has been fixed.",
      "y": "it has been fixed."
   },
   {
      "x": "Install from source on Centos7 doesn't work",
      "z": "Have you tried the following?\n* `python setup.py clean --all` before compiling\n* creating a new conda environment to compile in, using `conda create -n pytorch python=3.6` and `conda activate pytorch`\n* compiling without CUDA: `NO_CUDA=1 python setup.py install`\n* making sure you are on the lastest master commit",
      "y": "Have you tried the following?\n* `python setup.py clean --all` before compiling\n* creating a new conda environment to compile in, using `conda create -n pytorch python=3.6` and `conda activate pytorch`\n* compiling without CUDA: `NO_CUDA=1 python setup.py install`\n* making sure you are on the lastest master commit"
   },
   {
      "x": "Cholesky Error for positive definite matrix",
      "z": "FYI, Cholesky algorithms work independent of condition number, as long as the input is symmetric and positive definite.",
      "y": "Cholesky algorithms work independent of condition number, as long as the input is symmetric and positive definite."
   },
   {
      "x": "libtorch cannot find CUDA",
      "z": "I found that if I set CUDA_HOME in already opened terminal, then cmake fails to find CUDA. We should add the export lines in .bashrc",
      "y": " if I set CUDA_HOME in already opened terminal, then cmake fails to find CUDA. We should add the export lines in .bashrc"
   },
   {
      "x": "Does tensors got from torch.distributed.all_gather in order?",
      "z": "> To answer your question, though: yes, that is correct. But double check the API for `all_gather`, since you don't get back a single tensor, but a list of tensors. If you want to combine them into a single tensor, check out [`torch.cat`](https://pytorch.org/docs/stable/torch.html#torch.cat).\n\nHi, thank you for answering. But in our expeiments, it looks like returned in random order. Is it a bug?",
      "y": "yes, that is correct. But double check the API for `all_gather`, since you don't get back a single tensor, but a list of tensors. If you want to combine them into a single tensor, check out [`torch.cat`](https://pytorch.org/docs/stable/torch.html#torch.cat)."
   },
   {
      "x": "What is the torchvision version for pytorch-nightly? Use 0.3.0 to report errors",
      "z": "It should be `torchvision-nightly` that matches with `pytorch-nightly`, but they are not provided currently.",
      "y": "It should be `torchvision-nightly` that matches with `pytorch-nightly`, but they are not provided currently."
   },
   {
      "x": "running into error installing from source",
      "z": "What cudnn version are you using? Try cudnn v6.",
      "y": "Try cudnn v6."
   },
   {
      "x": "padding_idx doesn't work.",
      "z": "Remove this line: e.weight = nn.Parameter(torch.rand(3, 2))\nFrom nn.Embedding source, you could see:\n```\n if self.padding_idx is not None:\n self.weight.data[self.padding_idx].fill_(0)\n```",
      "y": "Remove this line: e.weight = nn.Parameter(torch.rand(3, 2))\nFrom nn.Embedding source, you could see:\n```\n if self.padding_idx is not None:\n self.weight.data[self.padding_idx].fill_(0)\n```"
   },
   {
      "x": "Undefined symbol \"_state\"",
      "z": "Are you sure this bug only appears on machines without CUDA? That's strange because my machine does have CUDA 8 (as proof, I have tensorflow-gpu installed and running). I think my cuda files live in `/usr/local/cuda/lib` and not `/usr/local/cuda/lib64`, but I looked through the default linker arguments and it looks like we include both as -L options?\n\nAlso, I started using `MACOSX_DEPLOYMENT_TARGET=10.11` instead of `10.9` because I realized that was the wrong version, but it didn't fix anything.\n\nAlso, I ran `install_name_tool -add_rpath /usr/local/cuda/lib ~/Desktop/pytorch/torch/lib/tmp_install/lib/libTHPP.1.dylib` and this seemed to solve the error listed in my edit.",
      "y": "I started using `MACOSX_DEPLOYMENT_TARGET=10.11` instead of `10.9` because I realized that was the wrong version, but it didn't fix anything. Also, I ran `install_name_tool -add_rpath /usr/local/cuda/lib ~/Desktop/pytorch/torch/lib/tmp_install/lib/libTHPP.1.dylib` and this seemed to solve the error listed in my edit."
   },
   {
      "x": "batch matrix-vector multiplication (bmv)",
      "z": "according to broadcasting rules, batch2 wont auto-expand because the expansion is to not prepend dimensions but to append in this case.\n\nI believe you can do:\n`batch2 = batch2.unsqueeze(2)`",
      "y": "according to broadcasting rules, batch2 wont auto-expand because the expansion is to not prepend dimensions but to append in this case.\n\nI believe you can do:\n`batch2 = batch2.unsqueeze(2)`"
   },
   {
      "x": "nonzero doesn't squeeze dimension",
      "z": "\ud83d\udc4d for making `nonzero()` compatible with numpy. Current nonzero() output cannot be used for indexing e.g. `z[x.nonzero()] = 2.0` would be convenient. AFAIK it cannot be used with index_select or index_fill as well.",
      "y": "Current nonzero() output cannot be used for indexing e.g. `z[x.nonzero()] = 2.0` would be convenient. AFAIK it cannot be used with index_select or index_fill as well."
   },
   {
      "x": "numerical stability for logSigmoid",
      "z": "Using function below seems to be a better choice.\n\ndef log_sigmoid(x):\n return torch.clamp(x, max=0)-torch.log(torch.exp(-torch.abs(x))+1)\n\nUpdate: The gradient at x=0 for function above is not the same as log sigmoid. I just add an additional term to eliminate the difference.\ndef log_sigmoid(x):\n return torch.clamp(x, max=0) - torch.log(torch.exp(-torch.abs(x)) + 1) + 0.5 * torch.clamp(x, min=0, max=0)",
      "y": "Using function below seems to be a better choice. def log_sigmoid(x): return torch.clamp(x, max=0)-torch.log(torch.exp(-torch.abs(x))+1)"
   },
   {
      "x": "ModuleNotFoundError: No module named 'torch.autograd'",
      "z": "pip install torchvision \n\nTry This and Be happy ... Its working",
      "y": "pip install torchvision"
   },
   {
      "x": "torch.std giving incorrect results",
      "z": "They are correct. The difference lies in [Bessel's correction](https://en.wikipedia.org/wiki/Bessel%27s_correction).",
      "y": "They are correct. The difference lies in [Bessel's correction](https://en.wikipedia.org/wiki/Bessel%27s_correction)."
   },
   {
      "x": "Missing \u2018get_trace_graph\u2019 function in Torch1.4",
      "z": "Yes, `get_trace_graph` was an internal function that was intended for use by ONNX export only. As an alternative, please get the graph like: \n```\ntraced = torch.jit.trace(model, inputs)\ntraced_graph = traced.graph\n```\nLet me know if you're having any issues!",
      "y": "Yes, `get_trace_graph` was an internal function that was intended for use by ONNX export only. As an alternative, please get the graph like: ``` traced = torch.jit.trace(model, inputs) traced_graph = traced.graph ```"
   },
   {
      "x": "Didn't find engine for operation quantized::conv_prepack NoQEngine (operator () at ..\\aten\\src\\ATen\\native\\quantized\\cpu\\qconv_prepack.cpp:264) (no backtrace available)",
      "z": "@uchihaltachi If you are on Windows, then you'll need to update to 1.5.0.",
      "y": "If you are on Windows, then you'll need to update to 1.5.0."
   },
   {
      "x": "cpu usage is too high on the main thread after pytorch version 1.1 (and 1.2) (not data loader workers )",
      "z": "Pytorch 1.1 and above utilize more CPU threads than Pytorch 1.0.1. If you want to return to previous behavior (slower code, using less cores) run your code with `OMP_NUM_THREADS=1` or any other suitable value.",
      "y": "Pytorch 1.1 and above utilize more CPU threads than Pytorch 1.0.1. If you want to return to previous behavior (slower code, using less cores) run your code with `OMP_NUM_THREADS=1` or any other suitable value."
   },
   {
      "x": "Source of `torch.testing.assert_close` reaches a dead end",
      "z": "Fixed by https://github.com/pytorch/pytorch.github.io/commit/5a2028452f93fdc5e44a65e761e03ed658048336 and https://github.com/pytorch/pytorch.github.io/commit/82eaa25adc292d81414e2bd33e95466c52e78dea",
      "y": "Fixed by https://github.com/pytorch/pytorch.github.io/commit/5a2028452f93fdc5e44a65e761e03ed658048336 and https://github.com/pytorch/pytorch.github.io/commit/82eaa25adc292d81414e2bd33e95466c52e78dea"
   },
   {
      "x": "fx: unable to symbolically trace BatchNorm2d due to control flow",
      "z": "I see, thanks, makes sense. The use case we are working with is hitting this because a custom child of `nn.BatchNorm` is calling `super().forward(x)`:\n\n```\nclass BatchNorm2d(torch.nn.BatchNorm2d):\n def forward(self, x):\n if x.numel() > 0:\n return super(BatchNorm2d, self).forward(x)\n # get output shape\n output_shape = x.shape\n return _NewEmptyTensorOp.apply(x, output_shape)\n```\n\nsince we chatted offline that we can likely remove the `x.numel() > 0` check, this behavior can be deleted as well. I'll follow up on this specific case.",
      "y": "The use case we are working with is hitting this because a custom child of `nn.BatchNorm` is calling `super().forward(x)`: ``` class BatchNorm2d(torch.nn.BatchNorm2d): def forward(self, x): if x.numel() > 0: return super(BatchNorm2d, self).forward(x) # get output shape output_shape = x.shape return _NewEmptyTensorOp.apply(x, output_shape) ```"
   },
   {
      "x": "[JIT] Support Dict comprehension",
      "z": "instead of writing a forloop manually, one interesting workaround is to use list comprehension: `x = dict([(i,i) for i in range(2)])`",
      "y": "instead of writing a forloop manually, one interesting workaround is to use list comprehension: `x = dict([(i,i) for i in range(2)])`"
   },
   {
      "x": "Build simple c++ example-cpp using Libtorch fails on arm with undefined reference to c10::Error::Error",
      "z": "I managed to successfully compile it on Jetson TX2 following these steps:\n\n1.Build PyTorch from source. `python3 setup.py build`\n\n2.No need to download LibTorch from the official website, which is the x86_64 build.\n\n3.build the example-cpp using compiled tmp_install directory: `cmake -DCMAKE_PREFIX_PATH=/absolute/path/to/pytorch/torch/lib/tmp_install ..`",
      "y": "successfully compile it on Jetson TX2 follow these steps: 1.Build PyTorch from source. `python3 setup.py build` 2.No need to download LibTorch from the official website, which is the x86_64 build. 3.build the example-cpp using compiled tmp_install directory: `cmake -DCMAKE_PREFIX_PATH=/absolute/path/to/pytorch/torch/lib/tmp_install ..`"
   },
   {
      "x": "ONNX export failure: Exporting the operator hardsigmoid to ONNX opset version 12 is not supported",
      "z": "For Hardsigmoid symbolic you can also try exporting to ONNX::Hardsigmoid directly.",
      "y": "Directly export to ONNX::Hardsigmoid for Hardsigmoid symbols."
   },
   {
      "x": "ROCm CI is intermittently failing with std::out_of_range",
      "z": "Add range assert in autograd engine queue lookup",
      "y": "In the autograd engine queue lookup, add a range assert."
   },
   {
      "x": "Building with MSVC 2019 Community Edition fails",
      "z": "Reproduced locally and can be fixed by adding the assignment operator for `torch::OrderedDict<Key, Value>::Item`.\n \n \n ```cpp\n \n \n  Item& operator=(const Item& other)\n \n \n  {\n \n \n  pair_ = other.pair_;\n \n \n  return *this;\n \n \n  }\n \n \n ```",
      "y": "Add the assignment operator for `torch::OrderedDict<Key, Value>::Item`."
   },
   {
      "x": "torch.log() returns -inf/nan on exponential input",
      "z": "Issue is that the `pow` does not promote dtypes (dtype of x is inferred to be `Long/int64`) and overflows leading to incorrect values. However the behavior is consistent with numpy. \n \n \n \n \n \n The reason it works in Python and Math is that plain int is unbounded and `2**100` and others do not overflow which math.log treats as double precision float and everything works.\n \n \n \n \n \n (simplest solution is to initialise `x` as `x=torch.tensor(2.)`, post which the results match as expected)\n \n \n \n \n \n ```python\n \n \n import torch\n \n \n import numpy as np\n \n \n from math import *\n \n \n \n \n \n x = torch.tensor(2)\n \n \n a = x.numpy()\n \n \n \n \n \n print(\"NUMPY POWER\")\n \n \n print(a**62 )\n \n \n print(a**63 )\n \n \n print(a**64 )\n \n \n print(a**100 )\n \n \n \n \n \n print(\"TORCH POWER\")\n \n \n print(x**62, torch.pow(x, 62).dtype)\n \n \n print(x**63, torch.pow(x, 63).dtype)\n \n \n print(x**64, torch.pow(x, 64).dtype)\n \n \n print(x**100, torch.pow(x, 100).dtype)\n \n \n \n \n \n print(\"NUMPY LOG\")\n \n \n print(np.log(a ** 62)) # valid\n \n \n print(np.log(a ** 63)) # invalid (nan)\n \n \n print(np.log(a ** 64)) # invalid (-inf)\n \n \n print(np.log(a **100)) # invalid (-inf)\n \n \n print(100 * np.log(a)) # valid\n \n \n \n \n \n print(\"TORCH LOG\")\n \n \n print(torch.log(x ** 62)) # valid\n \n \n print(torch.log(x ** 63)) # invalid (nan)\n \n \n print(torch.log(x ** 64)) # invalid (-inf)\n \n \n print(torch.log(x **100)) # invalid (-inf)\n \n \n print(100 * torch.log(x)) # valid\n \n \n \n \n \n print(\"MATH LOG\")\n \n \n # all valid (return float)\n \n \n print(log(2 **62))\n \n \n print(log(2 **63))\n \n \n print(log(2 **64))\n \n \n print(log(2 **100))\n \n \n print(100 *log(2))\n \n \n ```\n \n \n \n \n \n Output:\n \n \n <details>\n \n \n \n \n \n ```\n \n \n NUMPY POWER\n \n \n 4611686018427387904\n \n \n -9223372036854775808\n \n \n 0\n \n \n 0\n \n \n TORCH POWER\n \n \n tensor(4611686018427387904) torch.int64\n \n \n tensor(-9223372036854775808) torch.int64\n \n \n tensor(0) torch.int64\n \n \n tensor(0) torch.int64\n \n \n NUMPY LOG\n \n \n 42.97512519471661\n \n \n test_log.py:20: RuntimeWarning: invalid value encountered in log\n \n \n  print(np.log(a ** 63)) # invalid (nan)\n \n \n nan\n \n \n test_log.py:21: RuntimeWarning: divide by zero encountered in log\n \n \n  print(np.log(a ** 64)) # invalid (-inf)\n \n \n -inf\n \n \n test_log.py:22: RuntimeWarning: divide by zero encountered in log\n \n \n  print(np.log(a **100)) # invalid (-inf)\n \n \n -inf\n \n \n 69.31471805599453\n \n \n TORCH LOG\n \n \n tensor(42.9751)\n \n \n tensor(nan)\n \n \n tensor(-inf)\n \n \n tensor(-inf)\n \n \n tensor(69.3147)\n \n \n MATH LOG\n \n \n 42.97512519471661\n \n \n 43.66827237527655\n \n \n 44.3614195558365\n \n \n 69.31471805599453\n \n \n 69.31471805599453\n \n \n ```\n \n \n \n \n \n </details>",
      "y": "Use float datatype inputs or convert it to numpy."
   },
   {
      "x": "__torch_function__ documentation for inheriting from tensor seems incomplete or incorrect",
      "z": "Ah, yes. This happens when we try to print the `__repr__` of `args` without skipping `__torch_function__`, and it comes back into `__torch_function__` unconditionally. This should be solved by #55093.",
      "y": "This happens when we try to print the `__repr__` of `args` without skipping `__torch_function__`"
   },
   {
      "x": "`nn.Conv3d` throws incorrect error message",
      "z": "Thanks @dlmgary for reporting this! I agree it's a confusing error message.\n \n \n \n In the details, this comes from: https://github.com/pytorch/pytorch/blob/980d6f258912e8d4474c5ccfe1a1e4f88f226af5/aten/src/ATen/native/ConvolutionMM3d.cpp#L243\n \n \n \n Since the in-place op is called on the output tensor (with the same dtype as the input), the error message centers around output's dtype.\n \n \n \n May be worth clearing this up with a check higher up.",
      "y": "the in-place op is called on the output tensor (with the same dtype as the input)"
   },
   {
      "x": "Is there a way to remove all zero elements in one line?",
      "z": "> I think torch.nonzero might interest you.\n \n > Reference: https://pytorch.org/docs/stable/generated/torch.nonzero.html\n \n > \n \n > **Note** : I think this question is more suited for the [forums](https://discuss.pytorch.org/)\n \n \n \n Thanks, @kshitij12345. I found a solution in the forums here: https://discuss.pytorch.org/t/how-to-remove-an-element-from-a-1-d-tensor-by-index/23109/6.\n \n Sorry for that, I'm new to PyTorch and programming, but I now know how to ask questions. Have a nice day: )",
      "y": "remove an element from 1-d tensor"
   },
   {
      "x": "Backprop second time on spectral_norm",
      "z": "Looks like there are some inplace ops in there that break double backward. We should fix that.",
      "y": "there are some inplace ops in there that break double backward"
   },
   {
      "x": "quantize_per_tensor and quantize_per_channel should work on fp16 tensors",
      "z": "correct. The model would still be in fp32, it's just the forward pass which is run in fp16 to speed up training. This should be independent from any post training quantization strategies.",
      "y": "The model would still be in fp32, it's just the forward pass which is run in fp16 to speed up training. This should be independent from any post training quantization strategies."
   },
   {
      "x": "Query on PyTorch threading model",
      "z": "The text seems to describe this scenario - two application threads use openmp (e.g. `parallel for` or use ops that use openmp), in this case OpenMP would create two thread pools. It can be checked by e.g. running `matmul` in parallel with `matmul` launched with `torch.jit.fork`.",
      "y": "two application threads use openmp (e.g. `parallel for` or use ops that use openmp), in this case OpenMP would create two thread pools. It can be checked by e.g. running `matmul` in parallel with `matmul` launched with `torch.jit.fork`."
   },
   {
      "x": "torch.cat results does not have `requires_grad` if under an autograd function",
      "z": "hmm this is expected? things run in `Function::forward` are without autograd graph tracking.",
      "y": "Things run in `Function::forward` are without autograd graph tracking."
   },
   {
      "x": "How to access model embedded functions?",
      "z": "please use [PyTorch forum](https://discuss.pytorch.org/) for future questions. Thanks!",
      "y": "Use <model>.<function>"
   },
   {
      "x": "It'd best to set persistent_workers default value as True on Windows",
      "z": "Thank your explanation.",
      "y": "Default persistent_worker from False to True on Windows"
   },
   {
      "x": "torch.autograd.Function with multiple outputs returns outputs not requiring grad",
      "z": "Well, returning tuples from a python function is always dangerous, because doing `return a[0], a[1]` is the same as `return a`.\n \n So effectively, in your custom function here, you actually return the Tensors \"unpacked\" and so they are properly post-processed by the custom Function. And since there were not inputs that required gradients (no Tensor in this case), then the outputs don't need to require gradients either.",
      "y": "Returning tuples from a python function is always dangerous, because doing `return a[0], a[1]` is the same as `return a`.\n \n So effectively, in your custom function here, you actually return the Tensors \"unpacked\" and so they are properly post-processed by the custom Function. And since there were not inputs that required gradients (no Tensor in this case), then the outputs don't need to require gradients either."
   },
   {
      "x": "OpInfo addmv port from method_tests errors",
      "z": "The test_variant_consistency_eager failure might be the result of a bad torch.addmv_() bug that you discovered, @Lilyjjo.\n \n \n \n ```\n \n a = torch.randn((1,))\n \n b = torch.randn((3, 4))\n \n c = torch.randn((4,))\n \n \n \n # this should error out\n \n a.addmv_(b, c)\n \n : tensor([ 2.2854, 1.4802, -1.4424])\n \n ```\n \n \n \n It looks like torch.addmv_() allows inplace operations to change the size of the tensor they're operating on! I filed https://github.com/pytorch/pytorch/issues/55589 to track that more particular issue.",
      "y": "torch.addmv_() allows inplace operations to change the size of the tensor they're operating on"
   },
   {
      "x": "RuntimeError: Expected to have finished reduction in the prior iteration before starting a new one.",
      "z": "It works. Thanks. @mrshenli",
      "y": "include the loss computation in the forward function and let the forward function directly return the loss tensors. Then by setting find_unused_parameters=True, DDP should be able to traverse the graph from the loss and identify unused ones."
   },
   {
      "x": "`Softplus` forward and backward discrepancy",
      "z": "Thank you for reporting this issue, @shmsong. The team agrees it's a high priority.",
      "y": "The fix converts the binary TensorIterator used by softplus backwards to a ternary one, adding in the original input for comparison against beta * threshold."
   },
   {
      "x": "addmv_() allows resizing the tensor it operates on and produces wrong results",
      "z": "It not only resizes, it produces bogus values:\n \n ```\n \n In [3]: a = torch.full((1,), 0.5, device=\"cuda\")\n \n  ...: \n \n  ...: b = torch.ones((3, 4), device=\"cuda\")\n \n  ...: c = torch.ones((4,), device=\"cuda\")\n \n In [4]: out=torch.addmv(a,b,c)\n \n In [5]: out\n \n Out[5]: tensor([4.5000, 4.5000, 4.5000], device='cuda:0')\n \n In [6]: a\n \n Out[6]: tensor([0.5000], device='cuda:0')\n \n In [7]: a.addmv_(b,c)\n \n Out[7]: tensor([4.5000, 8.5000, 8.5000], device='cuda:0')\n \n ```",
      "y": "It produces bogus values."
   },
   {
      "x": "Verify that attempting to resize a tensor with an inplace operation throws a runtime error",
      "z": "Programmatic detection can work in `test_variant_consistency_eager`, where we actually compute the forward of the input (so we can check the shape of the output), however it won't work in `test_inplace_grad` and `test_inplace_gradgrad` (unless we add forward checks just for that).\n \n \n \n So I think it would be better to mark the SampleInput. \n \n * `test_variant_consistency_eager` will confirm that inplace on that sample actually fails (i.e. samples can't be incorrectly marked).\n \n * `test_inplace_grad{grad}` can just trust the marking and skip accordingly.\n \n * Information about whether the input broadcasts will be visible at sampling itself.\n \n \n \n Reference for `test_inplace_grad{grad}`\n \n \n \n https://github.com/pytorch/pytorch/blob/c7312f5271b9ce9ac988fffde90818354e5841b8/test/test_ops.py#L104-L117",
      "y": "Add broadcasts_input and verifies the behaviour for inplace_variant."
   },
   {
      "x": "Modification of tests for one method causes failures for some others",
      "z": "> IMO we should reseed before sampling randomly for each test, so that test ordering changes don't perturb RNG state.\n \n \n \n Porting the test to an OpInfo will achieve this because when using OpInfos the generation of sample inputs occurs WITHIN each test, not outside of it, unlike the deprecated method_tests().\n \n \n \n We could fix method_tests(), but the real fix is to kill them (by porting them to OpInfos).",
      "y": "Port them to OpInfos."
   },
   {
      "x": "torch.linalg: unclear \"synchronizes that device with the CPU\"",
      "z": "Thank you for reporting this issue @twoertwein. @raisinglc's explanation is in the right ballpark that we're trying to explain these algorithms move data from the GPU to the CPU. Typically GPU kernels are enqueued by the CPU and run asynchronously on the GPU, allowing both the CPU and GPU to run in a parallel. When an algorithm requires data from the GPU, however, the CPU waits for the GPU to provide it, and the GPU has to transfer the data to the CPU. This can be a surprising performance hit, and most PyTorch operators do not perform this cross-device data transfer.\n \n \n \n Exactly WHAT data is being moved from the GPU to the CPU isn't clear from this documentation, but its intent is to warn users who may find this behavior surprising. Maybe there's a clearer way to express this.",
      "y": "Typically GPU kernels are enqueued by the CPU and run asynchronously on the GPU, allowing both the CPU and GPU to run in a parallel. When an algorithm requires data from the GPU, however, the CPU waits for the GPU to provide it, and the GPU has to transfer the data to the CPU. This can be a surprising performance hit, and most PyTorch operators do not perform this cross-device data transfer."
   },
   {
      "x": "test_zero_redundancy_optimizer.py fails when run more than 4 GPU setup",
      "z": "Sorting or not the parameters will not change the distribution AFAIK, or it will change the precise partitioning but will not be more effective size-wise or speed-wise, the partitioning is still greedy so it will go through all the buckets and fill up the emptiest one, then repeat. In the end, space-wise there's only so much you can do with full tensors, another take is to flatten all the tensors (like FSDP does for instance) and equal-partition from there. Space is not really an issue empirically, except for pathological models (with one really really big layer), I don't think that there's much to win there.\n \n \n \n Speed wise other things could be done though, started in https://github.com/facebookresearch/fairscale/pull/598 but not really worth it in the end I think, redundant work with FSDP. What's lost right now is that all the ranks have the parameters in no particular order, so during the backward pass you have to wait for all the reductions before all the optimizers can step. *If* you partition along the real order of the parameters (ie: not `parameters()` but the orders with which the tensors are being used, which you can get during the backward pass), then the rank's optimizer could step() as soon as it got its grads, and start broadcasting the updated shards early on, during the backward pass. I would expect the speed up to be significant (given enough CUDA streams), but again this is duplicated engineering on the FairScale side given FSDP\n \n cc @mrshenli",
      "y": "I would expect the speed up to be significant (given enough CUDA streams), but again this is duplicated engineering on the FairScale side given FSDP."
   },
   {
      "x": "CUDA error when used torch.mm() with gpu in pytorch1.8.0",
      "z": "I can reproduce the problem using torch-1.8 on RTX 2080 (sm_75) for the following trivial case:\n \n ```\n \n $ python -c \"import torch;x=torch.eye(3, 3, device='cuda');print(torch.mm(x,x))\"\n \n ```\n \n torch-1.8 (unlike 1.7) is shipped without sm_75 cubins due to the size considerations:\n \n ```\n \n Analyzing /home/nshulga/.local/lib/python3.7/site-packages/torch/lib/libtorch_cuda.so\n \n .nv_fatbin size 705.5MiB\n \n  sm_37: 72.5MiB\n \n  sm_50: 171.1MiB\n \n  sm_60: 179.7MiB\n \n  sm_70: 193.0MiB\n \n  sm_35: 39.0MiB\n \n  sm_61: 50.2MiB\n \n __nv_relfatbin size 35.4MiB\n \n  sm_35: 5.3MiB\n \n  sm_50: 7.4MiB\n \n  sm_60: 7.8MiB\n \n  sm_70: 14.9MiB\n \n  sm_37: 54.5KiB\n \n ```",
      "y": "torch-1.8 (unlike 1.7) is shipped without sm_75 cubins due to the size considerations."
   },
   {
      "x": "Failed to compute shorthash for libnvrtc.so when compiling application with libtorch 1.8.0",
      "z": "I'm having the same issue, and it looks like the problem is not with `CUDA_NVRTC_LIB`, but rather that the `PYTHON_EXECUTABLE` isn't defined\n \n \n \n I added this message after the call the `execute_process`:\n \n \n \n ```\n \n execute_process(\n \n  COMMAND \"${PYTHON_EXECUTABLE}\" -c\n \n  \"import hashlib;hash=hashlib.sha256();hash.update(open('${CUDA_NVRTC_LIB}','rb').read());print(hash.hexdigest()[:8])\"\n \n  RESULT_VARIABLE _retval\n \n  OUTPUT_VARIABLE CUDA_NVRTC_SHORTHASH)\n \n \n \n message(STATUS \"PYTHON_EXECUTABLE=(${PYTHON_EXECUTABLE}), _retval=${_retval}. CUDA_NVRTC_LIB=${CUDA_NVRTC_LIB}\")\n \n ```\n \n \n \n and this is the output:\n \n ```\n \n -- PYTHON_EXECUTABLE=(), _retval=No such file or directory. CUDA_NVRTC_LIB=/usr/local/cuda-11.1/lib64/libnvrtc.so\n \n CMake Warning at torch-v1.8.0/share/cmake/Caffe2/public/cuda.cmake:199 (message):\n \n  Failed to compute shorthash for libnvrtc.so\n \n Call Stack (most recent call first):\n \n  torch-v1.8.0/share/cmake/Caffe2/Caffe2Config.cmake:88 (include)\n \n  torch-v1.8.0/share/cmake/Torch/TorchConfig.cmake:68 (find_package)\n \n  CMakeLists.txt:5 (find_package)\n \n ```",
      "y": "Problem is not with `CUDA_NVRTC_LIB`, but rather that the `PYTHON_EXECUTABLE` isn't defined."
   },
   {
      "x": "AdamW variable referenced before assignment",
      "z": "Hi, you're right, missread the commit. Should be fixed then.",
      "y": "The current error is that beta1 is referenced before assignment."
   },
   {
      "x": "Selection of test classes / methods through run_test.py is broken",
      "z": "Nice investigation @pmeier. Now that you found the root cause, I'd say fix it anyway if it's easy to do so. If it takes more than 1-2 hours then never mind, given that it will become irrelevant once we manage to switch to pytest.",
      "y": "enable test selection for default test handler."
   },
   {
      "x": "Status of 64-bit Arm support in PyTorch 1.8",
      "z": "Hi @malfet,\n \n \n \n I've just been giving the PyTorch 1.8 whl a go. I `pip install`ed `torch-1.8.0-cp36-cp36m-manylinux2014_aarch64.whl` on a CentOS 8.3.2011 machine, but I'm getting the following error:\n \n \n \n ```\n \n OSError: <path-to-my-python-venv>/lib64/python3.6/site-packages/torch/lib/libtorch_global_deps.so: ELF load command alignment not page-aligned\n \n ```\n \n \n \n This appears to be an issue previously encountered with NumPy ManyLinux builds (https://github.com/numpy/numpy/issues/16677) where the whl was built with the wrong pagesize.\n \n \n \n The same issue appears to affect the nightly .whls from https://download.pytorch.org/whl/nightly/cpu/torch_nightly.html too.\n \n \n \n Ubuntu 20.10 with Python 3.8.6 appears to work as expected.",
      "y": "issue previously encountered with NumPy ManyLinux builds."
   },
   {
      "x": "Backward compatibility problem of LSTMs in v1.8.0",
      "z": "not opposed to a fix for this particular issue. Happy to review the PR if you open it!",
      "y": "Users should be only saving state_dicts to avoid such issues."
   },
   {
      "x": "[bug] test_autograd sometimes runs CUDA tests on CPU",
      "z": "To add on this, while `det` in forward is using a LU decomposition, the `det_backward` is using `svd_backward`, which is only double backward stable for inputs with distinct singular values. We will need to reimplement `det_backward` which uses `lu_backward` instead. `lu_backward` needs to be implemented in ATen, as the current backward for LU is done via `autograd.Function`. I will submit an issue about it.",
      "y": "`det` in forward is using a LU decomposition, the `det_backward` is using `svd_backward`, which is only double backward stable for inputs with distinct singular values."
   },
   {
      "x": "Lazy Module Documentation suggestions",
      "z": "Thanks for the heads up, will try to work on it this week.",
      "y": "Lazy Modules Documentation Clarifications."
   },
   {
      "x": "Undocumented change of behavior for Embeddings in PyTorch 1.8",
      "z": "I'm not convinced that either #46714 or #47184 are fully correct. Following is what I consider to be the \"ideal semantics\" for `padding_idx` that adhere to the principle of least astonishment, and the current state of the semantics in 1.8.0:\n \n \n \n ### Ideal semantics\n \n \n \n  The crux of the issue is that `padding_idx` is used within `Embedding` to accomplish two things:\n \n 1. To \"fix\" the value of the embedding vector at `padding_idx` by preventing gradient updates to it\n \n 2. To pad the output with zeros wherever `padding_idx` is encountered in the input\n \n \n \n The important part is 1, and 2 is unnecessary to handle separately because it can be achieved from 1. Ideally, `padding_idx` should only be used to indicate which embedding vector should be exempt from gradient updates. Otherwise, indexing should work as normal with no special handling needed - if the embedding vector at `padding_idx` is all zeros (AKA its initial value), normal indexing will produce the behavior of 2. It's important to note that 2 is not desired for all use cases!\n \n \n \n This should be the case for both the module and functional forms, and it gives the user maximum flexibility to set the embedding vector at `padding_idx` to whatever is useful for their purposes without confusing semantics (arg naming aside).\n \n \n \n ### 1.8.0 semantics\n \n \n \n `padding_idx` passed to the `Embedding` constructor:\n \n * Embedding vector at `padding_idx` will be initialized to all zeros\n \n * Embedding vector at `padding_idx` can be manually set to something else after initialization\n \n * Embedding vector at `padding_idx` will not receive gradient updates\n \n * Calling the module with `padding_idx` as an input gives zeros unconditionally (changed accidentally due to #46714)\n \n  * Note that this seems to be where the majority of the backlash is coming from. It used to be the case that the embedding vector at `padding_idx` was always returned, whether it was all zeros or some manually set value, and those are the expected semantics\n \n  * **Ideal behavior**: always return the embedding vector at `padding_idx`, regardless of whether it's all zeros or some manually set value\n \n \n \n `padding_idx` passed to `from_pretrained()`:\n \n * Embedding vector at `padding_idx` will be overwritten with all zeros (changed due to #47184)\n \n  * **Ideal behavior**: do not override the vector with all zeros during loading\n \n * Embedding vector at `padding_idx` can be manually set to something else **after loading**\n \n * Embedding vector at `padding_idx` will not receive gradient updates\n \n * Calling the module with `padding_idx` as an input gives either zeros or the manual value set **after loading** (changed due to #47184)\n \n  * Note that the old behavior allowed for saving / loading a non-zero padding embedding vector. After #47184, if a non-zero padding embedding vector is saved, it will be overwritten with zeros when loaded. As a user, I'd find this surprising, although it is possible to work around by setting a manual value after loading\n \n  * **Ideal behavior**: always return the embedding vector at `padding_idx`, regardless of whether it's all zeros or some manually set value\n \n \n \n `padding_idx` passed to `F.embedding()`:\n \n * Embedding vector at `padding_idx` will not receive gradient updates\n \n * Calling the function with `padding_idx` as an input gives zeros unconditionally (changed due to #46714)\n \n  * Note that the old behavior returned the embedding vector at `padding_idx` regardless of whether or not it was all zeros. After #46714, this changed to always produce zeros\n \n  * **Ideal behavior**: always return the embedding vector at `padding_idx`, regardless of whether it's all zeros or some manually set value\n \n \n \n ### TL;DR\n \n \n \n I think both #46714 and #47184 stray from the ideal `padding_idx` semantics and should not have been introduced. It seems they both came about as a way to reconcile with confusing documentation. Imo `padding_idx` should always have been documented as a way to set a specific embedding vector to a fixed value (default all zeros) that it is not affected by gradient updates.",
      "y": "always return the embedding vector at `padding_idx`, regardless of whether it's all zeros or some manually set value"
   },
   {
      "x": "DDP.no_sync() function + torch.cuda.synchronize(device=rank)",
      "z": "Closing issue as this is a question and not a bug report, please use http://discuss.pytorch.org/c/distributed/ for questions instead. Thank you!",
      "y": "calling `backward()` on a loss tensor produced by DDP will always sync grads if not under no_sync() context manager. To manually sync grads, you can use a local model on all ranks and call `dist.all_reduce()` on gradients after backwards pass is completed, but we recommend DDP for performance reasons."
   },
   {
      "x": "ImportError: libtinfo.so.5",
      "z": "I can confirm this issue on my Ubuntu. The fix is rather easy, for me I just install the `libncurses5` using:\n \n \n \n ```text\n \n sudo apt install libncurses5\n \n ```\n \n \n \n I wonder what introduced this particular dependency, though. Fact that `libncureses5` is somewhat an \"out-of-data\" version. The newer version is `libncurses6`.",
      "y": "install the `libncurses5` using:\n ```text\n \n sudo apt install libncurses5\n \n ```"
   },
   {
      "x": "SIGSEGV in torch.linalg.inv",
      "z": "The problem was that one of the arguments to the LAPACK call was not correct for 0x0 matrices. I submitted a fix for that.\n \n Even though our tests are failing because of 0x0 test cases, PyTorch v1.8.0 compiled with OpenBLAS is expected to work correctly for non-empty inputs.\n \n \n \n The `BLAS=Eigen` option is misleading and it affects only the Caffe2 code and is not used in ATen. https://github.com/pytorch/pytorch/blob/e5ecd1ddf84787789dfdadc8ff8af34a180180fd/cmake/Dependencies.cmake#L124-L126\n \n \n \n The following message is printed with `BLAS=Eigen`:\n \n ```\n \n -- Trying to find preferred BLAS backend of choice: Eigen\n \n CMake Warning at cmake/Dependencies.cmake:175 (message):\n \n  Preferred BLAS (Eigen) cannot be found, now searching for a general BLAS\n \n  library\n \n ```\n \n Then BLAS is searched with `find_package(BLAS)`. https://github.com/pytorch/pytorch/blob/e5ecd1ddf84787789dfdadc8ff8af34a180180fd/cmake/Dependencies.cmake#L174-L176\n \n CMake log is needed to know what BLAS and LAPACK libraries are picked actually. Or at least `print(torch.__config__.show())`, it should contain the line `Build settings: BLAS_INFO=...`.\n \n \n \n I assume OpenBLAS is picked up. I compiled with OpenBLAS 0.3.12 and I see the segfault for 0x0 input. `torch.inverse` is fine because it explicitly returns for this case without relying on the LAPACK library to do that.",
      "y": "one of the arguments to the LAPACK call was not correct for 0x0 matrices"
   },
   {
      "x": "Library location assumption in test/jit/test_backends.py, test/jit/test_torchbind.py, and test/test_fx.py",
      "z": "#61960 has been landed and solve this issue with an additional step of installing the torch wheel package",
      "y": "installing the torch wheel package"
   },
   {
      "x": "PyTorch 1.8 CUDNN_STATUS_NOT_INITIALIZED",
      "z": "same here, was fine on 1080ti, but broke for 2080ti.",
      "y": "was fine on 1080ti, but broke for 2080ti."
   },
   {
      "x": "libtorch 1.8.0 with CUDA 11.1: CUDA error: no kernel image is available for execution..",
      "z": "I downloaded the nighly build instead using CUDA 10.2 and that worked fine. The nightly build with CUDA 11.1 had the same problem that all CUDA based tests failed.",
      "y": "download the nighly build instead using CUDA 10.2"
   },
   {
      "x": "The signature of `torch.tensordot` in Python should be compatible with its signature in TorchScript",
      "z": "`tensordot` should be added to https://github.com/pytorch/pytorch/blob/master/torch/jit/_builtins.py#L101\n \n \n \n Also, we should make a pass on which other ops should be added\n \n [Optional] Create a mechanism to remind deves who touch `functional.py` to modify list in `builtins.py` as well.",
      "y": "`tensordot` should be added."
   },
   {
      "x": "OpInfo tests for inplace variants use the same sample inputs as the method",
      "z": "Probably a duplicate of https://github.com/pytorch/pytorch/issues/50747",
      "y": "add a new field to SampleInput to notify if self gets broadcasted or not."
   },
   {
      "x": "RuntimeError: cuDNN error: CUDNN_STATUS_NOT_SUPPORTED. This error may appear if you passed in a non-contiguous input.",
      "z": "I'll send a PR to take 256 length inputs to the native backend.",
      "y": "take 256 length inputs to the native backend."
   },
   {
      "x": "Compiling release/1.5 broken on Ubuntu 18.04",
      "z": "You may need to sync the submodules.\n \n ```bash\n \n git submodule sync\n \n git submodule update --init --recursive\n \n ```",
      "y": "sync the submodules.\n \n ```bash\n \n git submodule sync\n \n git submodule update --init --recursive\n \n ```"
   },
   {
      "x": "Sums of expanded and repeated tensors are different",
      "z": "With #39512 \n \n ```\n \n tensor(1.0100e+08) tensor(1.0100)\n \n tensor(1.0012e+08) tensor(1.0012)\n \n tensor(1.0100e+08) tensor(1.0100)\n \n ```",
      "y": "```\n \n tensor(1.0100e+08) tensor(1.0100)\n \n tensor(1.0012e+08) tensor(1.0012)\n \n tensor(1.0100e+08) tensor(1.0100)\n \n ```"
   },
   {
      "x": "Bad performance with python threads",
      "z": "#37461 might be good too - we would still want to enable original settings if user uses ATen/Parallel API, it is just we can't do much when user creates a new thread themselves and then directly executes code that uses OpenMP",
      "y": "enable original settings if user uses ATen/Parallel API"
   },
   {
      "x": "Torch norm error on gpu tensors",
      "z": "offtopic: why complex conjugate was being called for non-complex tensor? https://github.com/pytorch/pytorch/blob/master/aten/src/ATen/native/LinearAlgebra.cpp#L556. Probably this is what got fixed, right?\n \n \n \n Also I wonder if `sqrt((self*self).sum(dim)` has again numerical instability / gradient blowup at 0 of sqrt and no eps used",
      "y": "`sqrt((self*self).sum(dim)` has again numerical instability / gradient blowup at 0 of sqrt and no eps used"
   },
   {
      "x": "torch.cuda.is_available() get false on cuda10",
      "z": "> Removing high priority. 1.5 C47 (10.1 and 10.2 only)\n \n \n \n is this information mentioned somewhere? i was looking for any indication about this in the [release](https://github.com/pytorch/pytorch/releases/tag/v1.5.0) page, and there is none. i just installed pytorch 1.5 using `pip install torch` on a gpu device with cuda 10.0 and pytorch did not detect the gpu. i was suspecting to run to this issue since the pytorch [page](https://pytorch.org/) points to cuda 10.2 to install the latest version.\n \n \n \n if pytorch 1.5 supports only cuda 10.1/2, it would be helpful to mention it in the release page or in the doc. i was trying to upgrade from pytorch 1.4 until i run into this thread.\n \n thanks a lot.\n \n \n \n p.s. it is surprising. this [lib](https://pytorch-geometric.readthedocs.io/en/latest/notes/installation.html) seems to be able to test pytroch 1.5 with cuda 10.0 and pytorch seems to detect the gpu.",
      "y": "pytorch version is available for download, however, it does not support cuda 10"
   },
   {
      "x": "copy.deepcopy() breaks when pruning is set on sequential",
      "z": "cc @mickypaganini might be interested in this.",
      "y": "Reparametrizing the network necessarily results in the creation of new parameters in terms of the leaf params.\n Now, I don't know exactly why in deepcopy in tensor.py we explicitly check for leaves-only, and whether that can be relaxed.\n \n Yes, workaround would be to call prune.remove before the deepcopy (but that might not be suitable if the masks or the original parameters are needed), or to copy first and then prune."
   },
   {
      "x": "torch.norm is numerically unstable at zero for multidim reductions",
      "z": "I was mistaken! @mrshenli pointed out this issue does appear to be fixed with torch.linalg.norm. I must have failed to save my updated version of the script. Here's the script I'm using:\n \n \n \n ```\n \n import torch\n \n a = torch.zeros(3, 3, 3, requires_grad = True)\n \n print(torch.autograd.grad(torch.linalg.norm(a, dim = (1,)).sum(), (a,))[0])\n \n print(torch.autograd.grad(torch.linalg.norm(a, dim = (1, 2)).sum(), (a,))[0])\n \n print(torch.__version__)\n \n ```\n \n \n \n And the result:\n \n \n \n ```\n \n tensor([[[0., 0., 0.],\n \n  [0., 0., 0.],\n \n  [0., 0., 0.]],\n \n \n \n  [[0., 0., 0.],\n \n  [0., 0., 0.],\n \n  [0., 0., 0.]],\n \n \n \n  [[0., 0., 0.],\n \n  [0., 0., 0.],\n \n  [0., 0., 0.]]])\n \n tensor([[[0., 0., 0.],\n \n  [0., 0., 0.],\n \n  [0., 0., 0.]],\n \n \n \n  [[0., 0., 0.],\n \n  [0., 0., 0.],\n \n  [0., 0., 0.]],\n \n \n \n  [[0., 0., 0.],\n \n  [0., 0., 0.],\n \n  [0., 0., 0.]]])\n \n 1.7.0a0+24eea36\n \n ```\n \n \n \n Since we'll soon be deprecating torch.norm in favor of torch.linalg.norm, I think we can close this issue.",
      "y": "this issue does appear to be fixed with torch.linalg.norm."
   },
   {
      "x": "Upgrade NCCL submodule",
      "z": "Want to bump this error.\n \n \n \n \n \n PyTorch Version (e.g., 1.0): master\n \n OS (e.g., Linux): Linux\n \n How you installed PyTorch (conda, pip, source): pip\n \n Build command you used (if compiling from source): N/A\n \n Python version: 3.8\n \n CUDA/cuDNN version: 10.2\n \n GPU models and configuration: V100",
      "y": "Update NCCL from 2.4.8 to 2.7.3."
   },
   {
      "x": "Pytorch crashes while training a simple MNIST classification problem",
      "z": "Reproduced locally.\n \n ```cmd\n \n C:\\Users\\peter>pip install torch===1.5.0 torchvision===0.6.0 -f https://download.pytorch.org/whl/torch_stable.html\n \n Looking in indexes: https://pypi.tuna.tsinghua.edu.cn/simple\n \n Looking in links: https://download.pytorch.org/whl/torch_stable.html\n \n Collecting torch===1.5.0\n \n  Downloading https://download.pytorch.org/whl/cu102/torch-1.5.0-cp37-cp37m-win_amd64.whl (899.1MB)\n \n  | | 61kB 75kB/s eta 3:18:16\n \n ERROR: Operation cancelled by user\n \n WARNING: You are using pip version 19.2.3, however version 20.0.2 is available.\n \n You should consider upgrading via the 'python -m pip install --upgrade pip' command.\n \n \n \n C:\\Users\\peter>pip install torch==1.5.0 torchvision==0.6.0 -f https://download.pytorch.org/whl/torch_stable.html\n \n Looking in indexes: https://pypi.tuna.tsinghua.edu.cn/simple\n \n Looking in links: https://download.pytorch.org/whl/torch_stable.html\n \n Collecting torch==1.5.0\n \n  Downloading https://download.pytorch.org/whl/cu92/torch-1.5.0%2Bcu92-cp37-cp37m-win_amd64.whl (693.1MB)\n \n  | | 204kB 211kB/s eta 0:54:42\n \n ERROR: Operation cancelled by user\n \n WARNING: You are using pip version 19.2.3, however version 20.0.2 is available.\n \n You should consider upgrading via the 'python -m pip install --upgrade pip' command.\n \n ```\n \n Would you please fix the install commands? cc @seemethere @soumith",
      "y": "fix the install commands"
   },
   {
      "x": "RuntimeError: has_marked_unused_parameters_ INTERNAL ASSERT FAILED at /tmp/pip-req-build-p5q91txh/torch/csrc/distributed/c10d/reducer.cpp:290, please report a bug to PyTorch.",
      "z": "after I modified \"find_unused_parameters=True\" in torch.nn.parallel.DistributedDataParallel(model, device_ids=opt.gpu_ids, find_unused_parameters=True)\n \n it could run",
      "y": "modify \"find_unused_parameters=True\" in torch.nn.parallel.DistributedDataParallel(model, device_ids=opt.gpu_ids, find_unused_parameters=True)"
   },
   {
      "x": "In-place leakyReLu backward calculation is triggered with a non-positive slope which is not supported",
      "z": "The slope == 0.0 case is disabled for inplace backward calculation since I've thought the gradient calculation for slope==0 case is not predictable under our autograd infrastructure. We just revisited our infrastructure and confirmed the slope == 0 case can be supported. We will fix this shortly.",
      "y": "The slope == 0.0 case is disabled for inplace backward calculation"
   },
   {
      "x": "Unexpected result on c10::ArrayRef",
      "z": "Yeah, this is user error. Unfortunately it's not so easy to get a compiler warning in this situation; it's a general problem with view types in C++. Hopefully the \"Ref\" is enough to tell you that there is a view going on.",
      "y": "it's a general problem with view types in C++"
   },
   {
      "x": "Port torch/csrc/jit/runtime/register_prim_ops_c10.cpp to new operator registration API",
      "z": "Sure, I like working on the A54, and a new issue #37579 has been created for the port.",
      "y": "New port for registrations in `register_distributed_ops.cpp`"
   },
   {
      "x": "Inconsistent behavior in model.parameters() in Pytorch 1.5.0",
      "z": "I checked the history of parameters on Module and it seems that it has always returned an iterator. So I am not sure how your original code ever worked. But yes, `parameters()` is specified to return an iterator, and iterators are only iterable over once, so this is as expected. If you need to iterate multiple times, call `m.parameters()` multiple times.",
      "y": "`parameters()` is specified to return an iterator, and iterators are only iterable over once"
   },
   {
      "x": "Support custom gradient reduction algorithm in DDP",
      "z": "> For different processes using different batch sizes, will we need to resolve #33148 to support uneven batch sizes before doing this?\n \n \n \n Sorry that I didn't state that clearly. No, we don't need to resolve #33148 first. By batch size I mean the number of samples in one batch. The issue tracked in #33148 is for different number of batches/iterations. More specially, if we have 2 DDP processes, where one processing 1 batch and another processing 2 batch, it would hang. But if both of them process 1 batch and one process's batch contains 10 sample and another contains 20 sample, it won't hang. However, the gradient might need to be averaged in a weighted manner depending on the loss function, sth like `(10 * grad1 + 20 * grad2) / 30`.",
      "y": "if we have 2 DDP processes, where one processing 1 batch and another processing 2 batch, it would hang. But if both of them process 1 batch and one process's batch contains 10 sample and another contains 20 sample, it won't hang. However, the gradient might need to be averaged in a weighted manner depending on the loss function, sth like `(10 * grad1 + 20 * grad2) / 30`."
   },
   {
      "x": "torch.multinomial behaves abnormally with CUDA tensor",
      "z": "I could give it a try once 'triage review' is dropped.",
      "y": "Both with and withoutReplacement kernels don't properly advance rng state (they move it by 4, whereas each thread is likely generating much more than 4 numbers). For withoutReplacement, it happens when there are multiple distributions, but for withReplacement, all samples are generated in the single kernel, so in addition to ndistributions multiplier there's also nsamples/nwarps multiplier (nwarps = 4)."
   },
   {
      "x": "torch.utils.tensorboard.writer.SummaryWriter.add_graph missing documentation",
      "z": "@orionr Correct, tried your fix and it renders like before.",
      "y": "`torch._C._log_api_usage_once(\"tensorboard.logging.add_graph\")` should be below the docstring."
   },
   {
      "x": "torchfetch - detect hardware capabilities for PyTorch install",
      "z": "https://github.com/pytorch/pytorch/blob/master/torch/utils/collect_env.py already does some of this",
      "y": "Already done by https://github.com/pytorch/pytorch/blob/master/torch/utils/collect_env.py"
   },
   {
      "x": "module.h(483): error: a member with an in-class initializer must be const",
      "z": "I think CONSTEXPR_EXCEPT_WIN_CUDA just needs to be `const` when it's not `constexpr`.",
      "y": "CONSTEXPR_EXCEPT_WIN_CUDA just needs to be `const` when it's not `constexpr`."
   },
   {
      "x": "CUDA out of memory in subprocesses spawned by unit tests in Windows",
      "z": "This can be fixed by add 'poll()' after 'communicate()' maybe because the function poll will do job for recycling resources.\n \n But this test can be passed on windows for latest code after https://github.com/pytorch/pytorch/pull/42627, because the communicate() has been replaced by check_output(), then the processes execute serially.\n \n The test is enabled on windows by https://github.com/pytorch/pytorch/pull/42796",
      "y": "can be fixed by add 'poll()' after 'communicate()'"
   },
   {
      "x": "master Build failure : undefined reference to `void fbgemm::CodeGenBase<unsigned char, signed char, int, int>::storeCRegs<asmjit::x86::Zmm, 64>(asmjit::x86::Emitter*, int, int, asmjit::x86::Gp, asmjit::x86::Gp, bool)",
      "z": "Duplicate of #42415 . Fixed by https://github.com/pytorch/FBGEMM/pull/396.",
      "y": "Explicitly instantiate `storeCRegs<x86::Zmm, 64>` template"
   },
   {
      "x": "NCCL Alltoall Process Group introducing time-out of other NCCL tests",
      "z": "torch_python should never be directly linked with nccl, but rather relies on the function exported from torch_cuda (we already have some wrappers that re-rexport C nccl API as C++ nccl::send nccl::recv and so on...",
      "y": "torch_python should not be directly linked with nccl"
   },
   {
      "x": "NCCL operation fails with torch.int16 tensors",
      "z": "@mrshenli Thank you for the clarification. \n \n \n \n I am actually trying to reduce the number of bits communicated during allreduce. Using 32bit int or 16bit floats would increase communication. I will try to find a workaround. \n \n \n \n Anyways I am closing this issue for now.",
      "y": "NCCL's data types don't have 16-bits integer"
   },
   {
      "x": "inplace operation error when using nn.ReLU without setting inplace=True",
      "z": "> You must be missing something. The only thing that `model.train()` does is that it changes the `self.traning` flag of the main module(model) and its submodules(nn.Dropout(), etc.). This flag is important in cases where the module's behaviour between the training phase and the evaluation differs (nn.Dropout, nn.BatchNorm, etc.). `nn.ReLU` just ignores that flag.\n \n \n \n Thanks, I will check it!",
      "y": "The only thing that `model.train()` does is that it changes the `self.traning` flag of the main module(model) and its submodules(nn.Dropout(), etc.). This flag is important in cases where the module's behaviour between the training phase and the evaluation differs (nn.Dropout, nn.BatchNorm, etc.). `nn.ReLU` just ignores that flag."
   },
   {
      "x": "torch.as_strided segfault when stride is empty tuple",
      "z": "Just checked. It is gone in nightly version. Thanks! I will close this issue",
      "y": "It is gone in nightly version."
   },
   {
      "x": "Caffe2 Error: more than one operator \"+\" matches these operands, windows and Cuda 11",
      "z": "Please see https://github.com/pytorch/pytorch/pull/42420/files#diff-9ed210e75701d8760a0b7cc9e11498b3R40.",
      "y": "Add CUDA 11 builds for Windows CI"
   },
   {
      "x": "Exception raised in backward() loses backtrace information",
      "z": "Hi,\n \n \n \n Thanks for opening an issue for this (it was mentioned in https://github.com/pytorch/pytorch/issues/41659 but better to have an issue for it).\n \n This comes from the fact that the Future API eats up the original error and only throws a basic std::exception with the original message.\n \n We should change that as our custom error types contain much more info (python error type, cpp stack traces, etc).\n \n \n \n cc @pritamdamania87",
      "y": "Future API eats up the original error and only throws a basic std::exception with the original message.\n \n We should change that as our custom error types contain much more info (python error type, cpp stack traces, etc)."
   },
   {
      "x": "View/Reinterpret Tensor as a different type w/o copying",
      "z": "Related: https://github.com/pytorch/pytorch/issues/29013. Also somewhat related are current special view_as_real / view_as_complex (they are partly reinterpret as well, since they change dtype, but not storage)",
      "y": "Current special view_as_real / view_as_complex"
   },
   {
      "x": "LSTMCell and GRUCell need autocast patching",
      "z": "I saw your patch for RNNs has been merged (#42385) and many, many thanks for the patch! \n \n \n \n I would be very grateful if you would be so kind to to let me know what's the status on this one?\n \n \n \n PS: while they have a different interface, I know LSTM can be used instead of LSTMCell, but would still like to know about the patch before I start converting the whole code base...",
      "y": "The cudnn RNN API expects weights to occupy a flat buffer in memory with a particular layout. This PR implements a \"speed of light\" fix: _cudnn_rnn_cast_reflatten (the autocast wrapper assigned to _cudnn_rnn) copies weights to the right slices of a flat FP16 buffer with a single read/write per weight (as opposed to casting them to FP16 individually then reflattening the individual FP16 weights, which would require 2 read/writes per weight)."
   },
   {
      "x": "pyinstaller exe generated fails to run Faster RCNN model due to runtime exception cannot find nms function",
      "z": "It might be helpful for you issue, https://github.com/pytorch/vision/issues/1899#issuecomment-598200938",
      "y": "patch torchvision 0.2.2 or Keep recent torchvision & patch torch.jit in your entry point"
   },
   {
      "x": "`torch.cholesky_solve` free(): invalid pointer, Aborted (core dumped)",
      "z": "I just reported another issue with this API in #42695, please take a look. Apparently, it produces so many kinds of error message with abort, and even segfault. Thank you!",
      "y": "It produces many kinds of error message with abort, and even segfault"
   },
   {
      "x": "Linking torch_cpu files if compiled in a path containing whitespace",
      "z": "Tested that it works.",
      "y": "try to surround `$<TARGET_FILE:${SRC}>` with \\\" here:\n pytorch/cmake/public/utils.cmake\n \n Line 55 in 40ac95d\n \n ` ${DST} INTERFACE -WHOLEARCHIVE:$<TARGET_FILE:${SRC}>) `"
   },
   {
      "x": "Failed to export an ONNX attribute 'onnx::Div', since it's not constant",
      "z": "Hi ,\n \n The node where you are seeing this error is a \"Pad\" operation. In older versions of ONNX, the Pad operation took the lengths of the paddings as an attribute, i.e. these lengths have to be constant at compile time. in the U-Net model, the lengths of the paddings come from the output of previous nodes in the graph, which is why you could not export the model to ONNX. In version 11, the Pad operation in ONNX changed to take the lengths of the padding as an input, so if you try to export your model using `opset_version=11`, you should be able to export the model to ONNX.",
      "y": "In older versions of ONNX, the Pad operation took the lengths of the paddings as an attribute, i.e. these lengths have to be constant at compile time. in the U-Net model, the lengths of the paddings come from the output of previous nodes in the graph, which is why you could not export the model to ONNX. In version 11, the Pad operation in ONNX changed to take the lengths of the padding as an input, so if you try to export your model using `opset_version=11`, you should be able to export the model to ONNX."
   },
   {
      "x": "PyTorch 1.5 failed to import c:\\miniconda3-x64\\envs\\test\\lib\\site-packages\\torch\\lib\\caffe2_nvrtc.dll",
      "z": "Like peterjc123 said, I downloaded additional .dll files from [https://drive.google.com/u/0/uc?id=1injUyo3lnarMgWyRcXqKg4UGnN0ysmuq&export=download](https://drive.google.com/u/0/uc?id=1injUyo3lnarMgWyRcXqKg4UGnN0ysmuq&export=download) and copied them to `C:\\Windows\\System32` and it worked.",
      "y": "download additional .dll files from [https://drive.google.com/u/0/uc?id=1injUyo3lnarMgWyRcXqKg4UGnN0ysmuq&export=download](https://drive.google.com/u/0/uc?id=1injUyo3lnarMgWyRcXqKg4UGnN0ysmuq&export=download) and copy them to `C:\\Windows\\System32`"
   },
   {
      "x": "Avoid keeping two copies of gradients (param.grad and buckets) in DDP",
      "z": "I'm wondering how to make param.grad to point to offsets in bucket and meanwhile send out the data to all reduce? are you saying to let bucket content and param.grad to share the same storage? \n \n \n \n I saw comments in codes to avoid that, see \n \n \n \n \" // Assert that the grad tensor and the bucket don't share storage.\n \n  // If they did, we could avoid the copy altogether.\n \n  // The reason for not doing this is that existing code calls\n \n  // `detach_` from `zero_grad`, which is incompatible with views.\"\n \n \n \n But I do not find where 'zero_grad' is called. do you have context about it?",
      "y": "The problem is that the view/inplace logic links the history of all these Tensors together. And so doing a .detach_() would be tricky. For example we allow it for the base even if it has views and this lead to the following weird behavior:\n import torch\n \n a = torch.rand(10, requires_grad=True).clone()\n b = a.narrow(0, 0, 2)\n \n print(\"a: \", a.requires_grad, a.grad_fn)\n print(\"b: \", b.requires_grad, b.grad_fn)\n \n print(\"Detaching a inplace\")\n a.detach_()\n \n print(\"a: \", a.requires_grad, a.grad_fn)\n print(\"b: \", b.requires_grad, b.grad_fn)\n \n print(\"Modifying b inplace\")\n b += 1\n \n print(\"a: \", a.requires_grad, a.grad_fn)\n print(\"b: \", b.requires_grad, b.grad_fn)\n Giving\n \n a: True <CloneBackward object at 0x100e18350>\n b: True <SliceBackward object at 0x100e18350>\n Detaching a inplace\n a: False None\n b: True <SliceBackward object at 0x100e18350>\n Modifying b inplace\n a: True <CopySlices object at 0x100e18390>\n b: True <AsStridedBackward object at 0x100e18390>\n We do detach_ to try and keep references to the old Tensor valid yes. And it is also slightly more efficient if you don't need a new Tensor object to do this as you only set 2 fields."
   },
   {
      "x": "PyTorch C++ multithreading support on GPU",
      "z": "Pytorch engine does support multithreading in general, but there are some vaiables that are thread-local, in particular dispatch keys. You error indicates that tensor type dispatch key is not correctly propagated.",
      "y": "dispatch key is not correctly propagated"
   },
   {
      "x": "Segmentation fault when building LibTorch 1.5.0 on macOS Catalina",
      "z": "Can you please check if this fix to SobolEngineOps.cpp fixes it for you? \n \n https://github.com/pytorch/pytorch/pull/36711",
      "y": "fix to SobolEngineOps.cpp"
   },
   {
      "x": "torch.sum() CPU is much slower for bool/uint8 tensors than int32/float32 tensors",
      "z": "Yes, this sounds like a reasonable fix. Don't forget that uint8 tensors values can be up to 255, so you can safely sum only pretty small tensors. It probably makes sense to limit the fix to bool tensors only. Please don't change type promotion behavior (i.e. the output type by default should be int64, like it is now). Also can you please check if it will fix masked_select performance in #33269, at least for bool masks?",
      "y": "uint8 tensors values can be up to 255, so you can safely sum only pretty small tensors. It probably makes sense to limit the fix to bool tensors only."
   },
   {
      "x": "[Poll] Add Windows support to torch.distributed package",
      "z": "Hi all,\n \n The feature is now available https://pytorch.org/tutorials/intermediate/ddp_tutorial.html , It supports supports Gloo backend, FileStore and TcpStore. Could you give it a spin and let us know how did it go?",
      "y": "The feature is now available https://pytorch.org/tutorials/intermediate/ddp_tutorial.html , It supports supports Gloo backend, FileStore and TcpStore."
   },
   {
      "x": "torch.isfinite() doesn't work for fp16 on CPU",
      "z": "this should be addressed in the latest nighly build, can you please verify?",
      "y": "addressed in the latest nighly build"
   },
   {
      "x": "Nightly failed buiding on OSX with CUDA because of tuple undefined",
      "z": "cudnn 9.0 is no longer supported. Supported cuda versions are 9.2, 10.1, 10.2",
      "y": "cudnn 9.0 is no longer supported."
   },
   {
      "x": "Link error, Libtorch 1.5 on Windows",
      "z": "Hi, everyone. \n \n this problem still appears in libtorch version 1.6, while trying to add libtorch c++ cuda to my existing project.\n \n \n \n For future reference, the solution is: \n \n add `/INCLUDE:\"?warp_size@cuda@at@@YAHXZ\"` in Configuration Properties -> Linker -> Command Line -> Additional Options . by reading this comment from @peterjc123 https://github.com/pytorch/pytorch/issues/31611#issuecomment-594383154. use \"/\" not \"-\"\n \n \n \n thank you very much for the help.",
      "y": "add `/INCLUDE:\"?warp_size@cuda@at@@YAHXZ\"` in Configuration Properties -> Linker -> Command Line -> Additional Options"
   },
   {
      "x": "Incremental pruning reorders module hooks",
      "z": "could you please take a look? Thanks.",
      "y": "The `Parametrization` framework will handle this issue correctly. Two Pruning methods will only be folded into one if they were applied on the same weight one after the other with no other Parametrizations between them."
   },
   {
      "x": "torch.distributions.utils.broadcast_all does not support Tensor-like objects",
      "z": "Feel free to suggest a fix in the form of a PR.",
      "y": "Fix broadcast_all crashing on Tensor-likes"
   },
   {
      "x": "torch.cdist produces nan gradients in Pytorch 1.5, but not Pytorch 1.4",
      "z": "A more concise way to reproduce this:\n \n \n \n 1. [Download the tensors](https://figshare.com/s/47d6035b990fa62d730d)\n \n 2. Run the code\n \n ```python\n \n import torch\n \n \n \n emb1, emb2, cdist_grad = torch.load('cdist_grad.pt')\n \n \n \n emb1.retain_grad()\n \n d = torch.cdist(emb1, emb2)\n \n d.backward(cdist_grad)\n \n \n \n print(emb1.grad[0, 17])\n \n ```",
      "y": "Fix cdist backward calculation for p=2"
   },
   {
      "x": "CUBLAS_STATUS_EXECUTION_FAILED for ConvTranspose2d backward with FP16 inputs",
      "z": "At the [spot where the error occurs](https://github.com/pytorch/pytorch/blob/master/aten/src/ATen/native/cuda/NaiveConvolutionTranspose2d.cu#L736), the gemv arguments have the expected shape. If replace the gemv call with dummy in-place `add_(0.0)`s applied to the argument tensors, the repro runs end to end under cuda-memcheck without errors, so the argument tensors appear properly allocated.\n \n \n \n Looks more and more like a misconfigured cublas call or an internal cublas error. Tomorrow i'll figure out which.",
      "y": "misconfigured cublas call or an internal cublas error"
   },
   {
      "x": "pytorch1.5 does not support CUDA",
      "z": "Running on an EC2 p2 with the Deep Learning Ubuntu AMI and torch.cuda.is_available() was True. Then I pip installed torchvision, which also upgraded torch to 1.5, and now torch.cuda.is_available() is False.",
      "y": "Deep Learning Ubuntu AMI and torch.cuda.is_available() was True. Then I pip installed torchvision, which also upgraded torch to 1.5, and now torch.cuda.is_available() is False."
   },
   {
      "x": "Libtorch C++ model predict/forward propagation crashed on windows10, CUDA 10.0, VS 2017 15.7.6 ,RTX 2080, but libtorch C++ works with cpu successfully",
      "z": "Does this make any difference?\n \n \n \n It's the same code but loads directly to GPU instead of to CPU.\n \n \n \n ```\n \n auto module = torch::jit::load(argv[1], torch::kCUDA);\n \n auto inputs = torch::ones({ 1, 3, 224, 224 }, torch::kCUDA);\n \n auto output2 = module->forward({inputs});\n \n ```",
      "y": "```\n \n auto module = torch::jit::load(argv[1], torch::kCUDA);\n \n auto inputs = torch::ones({ 1, 3, 224, 224 }, torch::kCUDA);\n \n auto output2 = module->forward({inputs});\n \n ```"
   },
   {
      "x": "libtorch gpu efficiency",
      "z": "CUDA is asynchronous. The time duration you observed is the time needed to forward though the network.\n \n \n \n \n \n \n \n \n \n \n \n \n I also think this is a bug. I convert a pytorch model to cpp w/ and w/o mask_seleced operator. The inference time in cpp will increase about 60ms.\n \n  \n \n You are receiving this because you authored the thread.\n \n Reply to this email directly, view it on GitHub, or mute the thread.",
      "y": "CUDA is asynchronous. The time duration you observed is the time needed to forward though the network."
   },
   {
      "x": "[JIT] isinstance(m, nn.Linear) returns False in ScriptModules",
      "z": "workaround: \n \n ```\n \n linear = nn.Linear()\n \n # do stuff to it\n \n self.linear = linear\n \n ```\n \n An solution to this as pointed out is \"add another metaclass with an __instancecheck__ it looks like to make the isinstance check work right\", we would love to accept a PR for it so making it low pri for now.",
      "y": "add another metaclass with an __instancecheck__ it looks like to make the isinstance check work right"
   },
   {
      "x": "Linking torch libraries and yaml-cpp gives undefined reference to yaml-cpp libraries",
      "z": "I've experienced the same issue recently, having a complex project with yarp, gazebo, cuda and fcl. Project was compiling, but adding ${TORCH_LIBRARIES} in cmake's target_link_libraries resulted in multiple \"undefined references\" from fcl and yarp. Notably, all problematic functions had std::string in the signature. \n \n Solution that worked for me: downloading cxx11ABI libtorch (cpu version) from https://pytorch.org/get-started/locally/ (current link - https://download.pytorch.org/libtorch/cpu/libtorch-cxx11-abi-shared-with-deps-1.5.1%2Bcpu.zip), and replacing initially installed version (seems to me that by default users still download pre-cxx11abi release).\n \n \n \n Hope that helps.",
      "y": "downloading cxx11ABI libtorch (cpu version) from https://pytorch.org/get-started/locally/ (current link - https://download.pytorch.org/libtorch/cpu/libtorch-cxx11-abi-shared-with-deps-1.5.1%2Bcpu.zip), and replacing initially installed version (seems to me that by default users still download pre-cxx11abi release)."
   },
   {
      "x": "Provide a context manager to silence PyTorch exporter warning",
      "z": "As stated in the onnxruntime issue you might trip the assert when different input shapes are used, so be careful. Closing since ignoring warnings with just the `warning` module is pretty straightforward, you can do something like this to get rid of the `TracerWarning`s:\n \n \n \n ```python\n \n import warnings\n \n with warnings.catch_warnings():\n \n  warnings.filterwarnings(\n \n  action='ignore',\n \n  category=torch.jit.TracerWarning,\n \n  module=r'.*'\n \n  )\n \n  \n \n  assert x.shape[0] > 2, \"checking\"\n \n ```",
      "y": "you might trip the assert when different input shapes are used, so be careful. Ignoring warnings with just the `warning` module is pretty straightforward, you can do something like this to get rid of the `TracerWarning`s:\n \n \n \n ```python\n \n import warnings\n \n with warnings.catch_warnings():\n \n  warnings.filterwarnings(\n \n  action='ignore',\n \n  category=torch.jit.TracerWarning,\n \n  module=r'.*'\n \n  )\n \n  \n \n  assert x.shape[0] > 2, \"checking\"\n \n ```"
   },
   {
      "x": "ONNX export regression: examples/fast_neural_style fails to export (pytorch_nightly)",
      "z": "I will take a look.",
      "y": "The issue here seems to be with upsample upon export."
   },
   {
      "x": "nn.Linear module weight initialization does not match the documentation",
      "z": "We cant assign it to you in the UI (github doesnt' allow), but just assume you've been assigned.",
      "y": "nn.linear module weight initialization fix"
   },
   {
      "x": "Sending small model to GPU leads to 2GB increase in RAM",
      "z": "Loading CUDNN for the first time will likely load a lot of stuff in to RAM. It's the massive libraries (CUDNN is about 500MB alone) as well as some scratch stuff and kernels. This should be a once off occurance.\n \n \n \n If you then load another model after this, how much does your RAM increase?",
      "y": "It's the massive libraries (CUDNN is about 500MB alone) as well as some scratch stuff and kernels. This should be a one off occurance."
   },
   {
      "x": "Headers use 'slots' keyword which conflict with QT code.",
      "z": "If you don't want to do the text replacement, something like:\n \n ```\n \n #undef slots\n \n #include \"torch/torch.h\"\n \n #def slots Q_SLOTS\n \n ```\n \n is apparently what the QT documentation recommends. \n \n \n \n I'll put up a PR to remove `slot` and `slots` from ivalue.h but it will be hard to guard regressions like this in the future, so the above might be more robust.",
      "y": "text replacement, or:\n \n ```\n \n #undef slots\n \n #include \"torch/torch.h\"\n \n #def slots Q_SLOTS\n \n ```"
   },
   {
      "x": "pip install does not work",
      "z": "Hey I'm trying this as well and i get the same error if I use pytorch or torch. with pip or pip3\n \n \n \n I get this error \n \n > ERROR: Failed building wheel for torch\n \n > Running setup.py clean for torch\n \n > ERROR: Command errored out with exit status 1:\n \n > command: 'c:\\users\\visse\\appdata\\local\\programs\\python\\python37-32\\python.exe' -u -c 'import >sys, setuptools, tokenize; sys.argv[0] = '\"'\"'C:\\\\Users\\\\visse\\\\AppData\\\\Local\\\\Temp\\\\pip-install->yavfiact\\\\torch\\\\setup.py'\"'\"'; __file__='\"'\"'C:\\\\Users\\\\visse\\\\AppData\\\\Local\\\\Temp\\\\pip-install->yavfiact\\\\torch\\\\setup.py'\"'\"';f=getattr(tokenize, '\"'\"'open'\"'\"', open)(__file__);code=f.read().replace('\"'\"'\\r\\n'\"'\"', '\"'\"'\\n'\"'\"');f.close();exec(compile(code, __file__, >'\"'\"'exec'\"'\"'))' clean --all\n \n  > cwd: C:\\Users\\visse\\AppData\\Local\\Temp\\pip-install-yavfiact\\torch\n \n  >Complete output (2 lines):\n \n > running clean\n \n > error: [Errno 2] No such file or directory: '.gitignore'\n \n >\n \n  > ERROR: Failed cleaning build dir for torch\n \n >Failed to build torch\n \n >Installing collected packages: torch\n \n > Running setup.py install for torch: started\n \n > Running setup.py install for torch: finished with status 'error'\n \n > ERROR: Command errored out with exit status 1:\n \n > command: 'c:\\users\\visse\\appdata\\local\\programs\\python\\python37-32\\python.exe' -u -c >'import sys,setuptools, tokenize; sys.argv[0] = '\"'\"'C:\\\\Users\\\\visse\\\\AppData\\\\Local\\\\Temp\\\\pip->install-yavfiact\\\\torch\\\\setup.py'\"'\"'; __file__='\"'\"'C:\\\\Users\\\\visse\\\\AppData\\\\Local\\\\Temp\\\\pip->install-yavfiact\\\\torch\\\\setup.py'\"'\"';f=getattr(tokenize, '\"'\"'open'\"'\"', open)(__file__);code=f.read().replace('\"'\"'\\r\\n'\"'\"', '\"'\"'\\n'\"'\"');f.close();exec(compile(code, __file__, >'\"'\"'exec'\"'\"'))' install --record 'C:\\Users\\visse\\AppData\\Local\\Temp\\pip-record-8cw6stqy\\install->record.txt' --single-version-externally-managed --compile\n \n  cwd: C:\\Users\\visse\\AppData\\Local\\Temp\\pip-install-yavfiact\\torch\\\n \n  Complete output (23 lines):\n \n  running install\n \n  running build_deps\n \n  Traceback (most recent call last):\n \n  File \"<string>\", line 1, in <module>\n \n  File \"C:\\Users\\visse\\AppData\\Local\\Temp\\pip-install-yavfiact\\torch\\setup.py\", line 265, in <module>\n \n  description=\"Tensors and Dynamic neural networks in Python with strong GPU acceleration\",\n \n  File \"c:\\users\\visse\\appdata\\local\\programs\\python\\python37-32\\lib\\site-packages\\setuptools\\__init__.py\", line 145, in setup\n \n  return distutils.core.setup(**attrs)\n \n  File \"c:\\users\\visse\\appdata\\local\\programs\\python\\python37-32\\lib\\distutils\\core.py\", line 148,in setup\n \n  dist.run_commands()\n \n  File \"c:\\users\\visse\\appdata\\local\\programs\\python\\python37-32\\lib\\distutils\\dist.py\", line 966,in run_commands\n \n  self.run_command(cmd)\n \n  File \"c:\\users\\visse\\appdata\\local\\programs\\python\\python37-32\\lib\\distutils\\dist.py\", line 985,in run_command\n \n  cmd_obj.run()\n \n  File \"C:\\Users\\visse\\AppData\\Local\\Temp\\pip-install-yavfiact\\torch\\setup.py\", line 99, in run\n \n  self.run_command('build_deps')\n \n  File \"c:\\users\\visse\\appdata\\local\\programs\\python\\python37-32\\lib\\distutils\\cmd.py\", line 313, in run_command\n \n  self.distribution.run_command(command)\n \n  File \"c:\\users\\visse\\appdata\\local\\programs\\python\\python37-32\\lib\\distutils\\dist.py\", line 985,in run_command\n \n  cmd_obj.run()\n \n  File \"C:\\Users\\visse\\AppData\\Local\\Temp\\pip-install-yavfiact\\torch\\setup.py\", line 51, in run\n \n  from tools.nnwrap import generate_wrappers as generate_nn_wrappers\n \n  ModuleNotFoundError: No module named 'tools.nnwrap'\n \n ERROR: Command errored out with exit status 1: 'c:\\users\\visse\\appdata\\local\\programs\\python\\python37-32\\python.exe' -u -c 'import sys, setuptools, tokenize; sys.argv[0] = '\"'\"'C:\\\\Users\\\\visse\\\\AppData\\\\Local\\\\Temp\\\\pip-install-yavfiact\\\\torch\\\\setup.py'\"'\"'; __file__='\"'\"'C:\\\\Users\\\\visse\\\\AppData\\\\Local\\\\Temp\\\\pip-install-yavfiact\\\\torch\\\\setup.py'\"'\"';f=getattr(tokenize, '\"'\"'open'\"'\"', open)(__file__);code=f.read().replace('\"'\"'\\r\\n'\"'\"', '\"'\"'\\n'\"'\"');f.close();exec(compile(code, __file__, '\"'\"'exec'\"'\"'))' install --record 'C:\\Users\\visse\\AppData\\Local\\Temp\\pip-record-8cw6stqy\\install-record.txt' --single-version-externally-managed --compile Check the logs for full command output.`",
      "y": "get the same error if use pytorch or torch. with pip or pip3"
   },
   {
      "x": "Option to enable/disable autograd.profiler, move Chrome trace processing to C++",
      "z": "Yeah, I think it would help us a lot if you could give an example where it's not easy to restructure the code so that you trace only one iteration.",
      "y": "not easy to restructure the code so that you trace only one iteration."
   },
   {
      "x": "Document that autograd::Profiler::RecordFunction is available in Python",
      "z": "> because I doubt anyone imported it as an unqualified name\n \n \n \n true. but from experience, it's worth it now even just to avoid the drive by contributions from people who run style checkers and think it's really important to not shadow builtins. for numpy and scipy we have the same discussions about the same functions about once every 1-2 years ...",
      "y": "avoid the drive by contributions from people who run style checkers and think it's really important to not shadow builtins. for numpy and scipy we have the same discussions about the same functions"
   },
   {
      "x": "DistributedDataParallelTest .test_dist_broadcast_coalesced_gloo is flaky",
      "z": "Found the culprit in #19183. This is now broken on master.",
      "y": "Remove usages of TypeID"
   },
   {
      "x": "How install old version pytorch 0.4.1 from source?",
      "z": "Sorry the 0.4.1 source build is broken. IIRC, the nervanagpu submodule is optional. Here are steps to build:\n \n \n \n 1. `git clone --branch v0.4.1 https://github.com/pytorch/pytorch.git pytorch-0.4.1`\n \n 2. `cd pytorch-0.4.1`\n \n 3. `git rm --cached third_party/nervanagpu`\n \n 4. Delete these lines:\n \n \n \n https://github.com/pytorch/pytorch/blob/v0.4.1/.gitmodules#L19-L21\n \n \n \n 5. `git submodule update --init --recursive`\n \n 6. `python setup.py install`\n \n \n \n Be aware that git submodules have some weird state, so if you run into trouble I would recommend deleting your checkout and starting again.",
      "y": "the nervanagpu submodule is optional. Here are steps to build:\n \n \n \n 1. `git clone --branch v0.4.1 https://github.com/pytorch/pytorch.git pytorch-0.4.1`\n \n 2. `cd pytorch-0.4.1`\n \n 3. `git rm --cached third_party/nervanagpu`\n \n 4. Delete these lines:\n \n \n \n https://github.com/pytorch/pytorch/blob/v0.4.1/.gitmodules#L19-L21\n \n \n \n 5. `git submodule update --init --recursive`\n \n 6. `python setup.py install`"
   },
   {
      "x": "torch.potri example doesn't work because cholesky defaults to lower triangle and potri defaults to upper",
      "z": "Thank you for opening this issue. I will be sending in a PR soon that renames `potri` to `cholesky_inverse` where the `upper` argument will default to `False` to remain consistent with other `cholesky` functions (`cholesky` and `cholesky_solve`).",
      "y": "rename `potri` to `cholesky_inverse` where the `upper` argument will default to `False` to remain consistent with other `cholesky` functions"
   },
   {
      "x": "Sparse tensor creation ignores indices placement",
      "z": "I find `torch.sparse.FloatTensor' to be a good walk around.",
      "y": "`torch.sparse.FloatTensor'"
   },
   {
      "x": "Boolean tensor transpose bug in 1.3.0",
      "z": "As discussed in other reports, this breaks potentially a lot of code, including our own examples.",
      "y": "Compute correct strides after type promotion"
   },
   {
      "x": "Inconsistent results from `mask == 0`",
      "z": "Sure, could you just rename it to state that the issue is with the `mask == 0` line and has nothing to do with the masked_fill function?",
      "y": "issue is with the `mask == 0` line"
   },
   {
      "x": "Scripting module with type annotations and torch.jit.ignore fails",
      "z": "This was fixed by https://github.com/pytorch/pytorch/pull/29300",
      "y": "Use real argument names for Python functions"
   },
   {
      "x": "Conversion to .bfloat16() makes require_grad False",
      "z": "Yes. Adam says I should add complex. Would doing so and adding a comment to is complex that it means cont be acceptable to you?",
      "y": "should add complex"
   },
   {
      "x": "Unexpected behaviour of torch.BoolTensor on `not` operator",
      "z": "Hi,\n \n \n \n This is expected behavior. Constructors with capital letters (torch.FloatTensor, torch.BoolTensor etc) give you uninitialized memory. So The content could be anything.\n \n In some sense, not undefined is also undefined.",
      "y": "Constructors with capital letters (torch.FloatTensor, torch.BoolTensor etc) give you uninitialized memory."
   },
   {
      "x": "torch.mm seems to be broken. It doesn't enforce the dimensionality of out tensor",
      "z": "So I don't think this specific issue is high priority.\n \n \n \n And we'd need to get general agreement on the semantics before pursuing a fix.",
      "y": "The semantic of out= is definitely that it will resize if needed.\n The point of this api is to be able to reuse existing Tensors as buffers to reduce memory usage if needed."
   },
   {
      "x": "masked_scatter change elements even the the self and source tensor are of same values",
      "z": "I think your expectation about `masked_scatter` is not correct here and it does not (necessarily) satisfy the equation you showed `t.masked_scatter(mask, t) == t`.\n \n If carefully read documentation (of [masked_scatter_](https://pytorch.org/docs/stable/tensors.html#torch.Tensor.masked_scatter_)), it says:\n \n \n \n > The mask operates on the `self` tensor, not on the given `source` tensor.\n \n \n \n which means, for example, it does something like this:\n \n \n \n ```python\n \n import torch\n \n self = torch.tensor([1, 2, 3, 4, 5, 6, 7])\n \n mask = torch.tensor([1, 0, 0, 1, 0, 1, 1], dtype=torch.bool)\n \n src = torch.tensor([100, 200, 300, 400]) # src entry is picked up just \"from left to right\"\n \n \n \n self.masked_scatter(mask, src)\n \n # => tensor([100, 2, 3, 200, 5, 300, 400])\n \n ```\n \n \n \n So, I believe, it's only required that:\n \n \n \n - `mask.sum() <= src.numel()` (i.e. number of true entries in `mask` <= number of element of `src`)\n \n - `mask.shape() == self.shape()` (or `mask` is broadcast-able to `self`)\n \n \n \n For the reference, it's implemented like this (for cpu):\n \n \n \n https://github.com/pytorch/pytorch/blob/5da932ad728ed61f3fe09ccb7dfa8a4795a45efb/aten/src/TH/generic/THTensorEvenMoreMath.cpp#L241-L267\n \n \n \n Anyway, I hope my example helps you.",
      "y": "The mask operates on the `self` tensor, not on the given `source` tensor."
   },
   {
      "x": "[ONNX] incorrect export of opset10 slice with dynamic_axis",
      "z": "This warning is at autograd level for slice indexing, not exporter directly. But we might be able to remove it now. I'll check that. I think we can close the issue.",
      "y": "warning is at autograd level for slice indexing, not exporter directly"
   },
   {
      "x": "argmax for half datatype.",
      "z": "This is serious, we'll fix it.",
      "y": "``` if (iter.dtype(1) == kHalf) {\n  argmax_kernel_cuda_impl<at::Half, float>(iter);```"
   },
   {
      "x": "Python crashes after repeatedly calling `refine_names` to get named tensors",
      "z": "nvm, I just manually increase refcount for None after each refine_names call, similar to this https://stackoverflow.com/questions/17732816/is-there-any-way-to-manually-decrease-the-reference-count-of-an-object-in-python.",
      "y": "increase refcount for None after each refine_names call"
   },
   {
      "x": "GPU Memory Leak when using lambdas in nn.Modules",
      "z": "Thank you for the fast reply and for the hints. \n \n You are right, when using gc.collect() and empty_cache the gpu memory is freed. So, I guess there is no bug.",
      "y": "when using gc.collect() and empty_cache the gpu memory is freed"
   },
   {
      "x": "[type promotion] torch.exp does not work on integral types",
      "z": "I think we definitely want to support this. NumPy already has this, although I'm not sure precisely what algorithm they use for determining the output dtype.",
      "y": "NumPy already has this"
   },
   {
      "x": "Error Exporting Quantized model to ONNX",
      "z": "Any plan to support that?",
      "y": "We currently don't support ONNX for the quantized kernels."
   },
   {
      "x": "[JIT] `iterator expression` is expected to accept `Tensor`",
      "z": "I have a PR for this but it's not landed yet. This is a dup of https://github.com/pytorch/pytorch/issues/27255, closing.",
      "y": "iterator expression is expected to accept Tensor"
   },
   {
      "x": "pytorch 1.3, addition of different data typed tensors after permute giving incorrect results",
      "z": "> Why was this issue closed? I don't believe 1.3.1 has been released?\n \n \n \n Fixed on master. See here for 1.3.1 release tracking:\n \n https://github.com/pytorch/pytorch/issues/28919",
      "y": "fixed on master.\n ```\n >>> t1+m\n tensor([[2., 6.],\n  [4., 8.]]) ```"
   },
   {
      "x": "warn if user tries to build from source on windows & python 2.7",
      "z": "We should probably have a runtime warning if one runs on Windows under python 2.7",
      "y": "runtime warning if one runs on Windows under python 2.7"
   },
   {
      "x": "CUDA error: no kernel image is available for execution on the device Error from operator: output",
      "z": "Remove the quotation marks and try again.\n \n That is:\n \n ```cmd\n \n set TORCH_CUDA_ARCH_LIST=6.1\n \n ```",
      "y": "Remove the quotation marks"
   },
   {
      "x": "[pytorch] [feature request] torch.eye_like",
      "z": "I am sending in a PR.",
      "y": "rather not include this function as a built-in because it's different enough from the other _like functions (which work across all shapes) to be misleading. If we were to support N-dimensional eye(s) then it would be okay, but I don't think we have any rank 2+ functions defined for which identity makes sense."
   },
   {
      "x": "ModuleNotFoundError: No module named 'peachpy.x86_64.avx'",
      "z": "Are you submodules fully updated? Use `git submodule update --init --recursive` to update them.\n \n \n \n We are trying to keep the exact same installation instructions working as we integrate Caffe2 into Pytorch. For example, `python setup.py install` will still install Pytorch correctly. If you also want to build Caffe2, then you can use `FULL_CAFFE2=1 python setup.py install` to build all of Caffe2 along with Pytorch.",
      "y": "Use `git submodule update --init --recursive` to update submodules."
   },
   {
      "x": "DLL load failed: The specified module could not be found. After installed using pip in python 3.5 windows.",
      "z": "Hello. I experiencing this issue only on one of my window systems. I think my problem might be visual studio related but no Idea what else to do to fix it. I first tried installing the VC Redist package. Then, I installed the community edition for the Visual Studio 2017. However, I am still getting:\n \n \n \n C:\\Users\\Mauricio>where api-ms-win-crt-utility-l1-1-0.dll\n \n INFO: Could not find files for the given pattern(s).\n \n \n \n When I try the dependency walker on the \"_C.cp35-win_amd64.pyd\", it marks that all references are correct and the \"api-ms-win-crt-utility-l1-1-0.dll\" is being mapped to \"C:\\Windows\\system32\\ucrtbase.dll\", but on the python console I am still getting the following error:\n \n \n \n  from torch._C import *\n \n ImportError: DLL load failed: The specified procedure could not be found.\n \n \n \n I even moved from the CUDA 9.2 to cpu-only to reduce the dependencies, but still no luck. Any suggestions?",
      "y": "problem might be visual studio related"
   },
   {
      "x": "Got CUDNN_STATUS_NOT_INITIALIZED although PyTorch recognizes CUDA & CuDNN",
      "z": "I wouldn't rule out driver upgrade solving the issue.",
      "y": "driver upgrade solving the issue."
   },
   {
      "x": "concat tensor",
      "z": "You can specify the dimension where to concatenate the tensors.\n \n ```python\n \n a = torch.rand(3, 3)\n \n b = torch.rand(3, 6)\n \n c = torch.cat((a, b), dim=1)\n \n ```\n \n \n \n Also, questions like those are better suited for https://discuss.pytorch.org/",
      "y": "``python\n \n a = torch.rand(3, 3)\n \n b = torch.rand(3, 6)\n \n c = torch.cat((a, b), dim=1)\n \n ```"
   },
   {
      "x": "[feature request] Low-discrepancy quasi-random sampler (Sobol sequences)",
      "z": "Sorry for the delay. I did go through the CPython implementation, and I think this can be ported into ATen. I should be able to start working on it in a couple days and will keep you posted.",
      "y": "this can be ported into Aten"
   },
   {
      "x": "[feature request] Multivariate Gamma function",
      "z": "`ATen/native/random.cpp` if you are adding rng functions, and `ATen/native/misc.cpp` otherwise, probably?",
      "y": "`ATen/native/random.cpp` if you are adding rng functions, and `ATen/native/misc.cpp` otherwise"
   },
   {
      "x": "tensor.chunk returns wrong number of chunks",
      "z": "I can work on this. But first, I want to address how to deal with deprecation. Since we want a function whose behavior matches `numpy.array_split`, I suggest that we create a new function called `torch.tensor_split`, and then we can deprecate the `torch.chunk` method (or keep it, if there's any good reason). Does that plan sound good?",
      "y": "Since we want a function whose behavior matches `numpy.array_split`, I suggest that we create a new function called `torch.tensor_split`, and then we can deprecate the `torch.chunk` method"
   },
   {
      "x": "[pytorch] Intel MKL ERROR when doing torch.eig on a CUDA tensor",
      "z": "btw, just tried on AWS DLAMI (with MKL + PyTorch configured) and the crash isn't there anymore\n \n \n \n ```\n \n import os, torch, gzip\n \n os.system('wget https://github.com/pytorch/pytorch/files/2189021/S.pt7.gz')\n \n \n \n a = torch.load(gzip.open('S.pt7.gz'))\n \n \n \n print(a.device)\n \n # cuda:0\n \n \n \n a.eig()\n \n ```",
      "y": "tried on AWS DLAMI (with MKL + PyTorch configured) and the crash isn't there anymore"
   },
   {
      "x": "backward pass different behaviors with inplace operation",
      "z": "in the second case, b is the result of `1 / x`, and the derivative wrt `x` is `-1 / x^2`, which does not require `b` in the formula. In this case, if you make `c = ((-a).exp()+1); b = 1 / c` and then do an in-place operation on `c`, the backward will complain the same.",
      "y": "Dependant variables cause issue."
   },
   {
      "x": "[pytorch] [feature request] Pairwise distances between all points in a set (a true pdist)",
      "z": "> Reading the above thread has not made it clear to me what the currently best feasible solution is for batched pairwise distance. Can maybe someone who understands the details discussed summarize the thread and provide the current best way to code the function `parwise_dist` as used below?\n \n > \n \n > ```\n \n > X = torch.from_numpy(np.random.normal(size=(B, N, D)))\n \n > Y = torch.from_numpy(np.random.normal(size=(B, M, D)))\n \n > parwise_dist(X, Y) # Should be B x N x M\n \n > ```\n \n \n \n This issue has been addressed in the latest version of Pytorch 1.1.0. The documentation for it is still not up in the pytorch docs but you can see `torch.cdist` added in the release notes: https://github.com/pytorch/pytorch/releases/tag/v1.1.0",
      "y": "> ```\n \n > X = torch.from_numpy(np.random.normal(size=(B, N, D)))\n \n > Y = torch.from_numpy(np.random.normal(size=(B, M, D)))\n \n > parwise_dist(X, Y) # Should be B x N x M\n \n > ```"
   },
   {
      "x": "[jit][script] support tensor.expand([-1,-1,10,-1,-1])",
      "z": "It sounds like the conclusion is that the JIT should have better support for this",
      "y": "JIT should have better support for this"
   },
   {
      "x": "[jit] Can not pickle torch.futures.Future",
      "z": "that error message is actually part of the test, and is an expected log message when that test runs, if you look at the log, the test actually passed and just logged that error message. It is a bit confusing since Dr. CI picks it up as the failure reason. It looks like the actual CI error in that PR is coming from:\n \n \n \n ```\n \n Aug 17 18:06:29 ======================================================================\n \n Aug 17 18:06:29 ERROR [61.826s]: test_backward_ddp_inside (__main__.ProcessGroupDdpUnderDistAutogradTestWithSpawn)\n \n Aug 17 18:06:29 ----------------------------------------------------------------------\n \n Aug 17 18:06:29 Traceback (most recent call last):\n \n Aug 17 18:06:29 File \"/Users/distiller/workspace/miniconda3/lib/python3.7/site-packages/torch/testing/_internal/common_distributed.py\", line 223, in wrapper\n \n Aug 17 18:06:29 self._join_processes(fn)\n \n Aug 17 18:06:29 File \"/Users/distiller/workspace/miniconda3/lib/python3.7/site-packages/torch/testing/_internal/common_distributed.py\", line 330, in _join_processes\n \n Aug 17 18:06:29 self._check_return_codes(elapsed_time)\n \n Aug 17 18:06:29 File \"/Users/distiller/workspace/miniconda3/lib/python3.7/site-packages/torch/testing/_internal/common_distributed.py\", line 363, in _check_return_codes\n \n Aug 17 18:06:29 raise RuntimeError(error)\n \n Aug 17 18:06:29 RuntimeError: Processes 5 exited with error code 10\n \n Aug 17 18:06:29 \n \n Aug 17 18:06:29 --------------------------------------------------------------------\n \n ```\n \n \n \n Which looks like it is a known flaky test: https://github.com/pytorch/pytorch/issues/40434",
      "y": "Flaky test passed but error occurs."
   },
   {
      "x": "backward of torch.repeat slower than for torch.repeat_interleave",
      "z": "Regardless of a particular case, backward of `repeat` is inefficient due to https://github.com/pytorch/pytorch/blob/e7564b076c13325fba1704d9d07844a26041f10f/torch/csrc/autograd/FunctionsManual.cpp#L645. `sum_tensorlist` is inefficiently implemented as a sequential sum. `grad` is guaranteed to be evenly split into necessary-size pieces, so instead of chunk + sum_tensorlist `grad` tensor can be reshaped and `sum` can be called on it, that should give similar performance to`expand`.",
      "y": "backward of `repeat` is inefficient"
   },
   {
      "x": "TorchScript sets requires_grad to True under some circumstances",
      "z": "This only occurs with the legacy executor, which will no longer be on by default in the 1.7 release so this is low-pri",
      "y": "no longer be on by default in the 1.7 release"
   },
   {
      "x": "[JIT] Unable to cast Python instance to C++ type (compile in debug mode for details)",
      "z": "Looks like the error is coming from pybind11. I am not entirely sure how to enable debug mode, but I am fairly sure more info can be exposed if change this [line](https://github.com/pybind/pybind11/blob/dabbbf315d61d71e9d0a2cb68e0b9a86e4204459/include/pybind11/cast.h#L1707), more info will be displayed.\n \n \n \n Also, if you build Pytorch with \"DEBUG=1\" environment variable, you can use a c++ debugger to step into problematic code and get more info.",
      "y": "if you build Pytorch with \"DEBUG=1\" environment variable, you can use a c++ debugger to step into problematic code and get more info."
   },
   {
      "x": "Link error in windows build",
      "z": "cc malfet",
      "y": "In visual studio project settings, it's not necessary to add quota for the directory."
   },
   {
      "x": "F.mse_loss(a, b, reduction='elementwise_mean') value is incorrect and doesn't show deprecation warning when 2nd argument requires gradient",
      "z": "This has been fixed by https://github.com/pytorch/pytorch/pull/44437 and other PRs in the stack fixed similar issues for other loss functions.",
      "y": "modify the MSELoss CriterionTests to verify that the target derivative is checked."
   },
   {
      "x": "\u00e3\u20ac\u0090ERROR\u00e3\u20ac\u2018connection reset by peer, when using infiniband",
      "z": "I found out the solution!!\n \n \n \n Because our platform used IPonIB with Infiniband devices, then we need to set RDMA port by $ export NCCL_IB_HCA=mlx5_0, otherwise some DOWN state IB ports will be used by NCCL...",
      "y": "set RDMA port by $ export NCCL_IB_HCA=mlx5_0, otherwise some DOWN state IB ports will be used by NCCL"
   },
   {
      "x": "What dose _ctx in UniqueVoidPtr do?",
      "z": "In some cases, the `data_` pointer does not directly give you enough information to deallocate it; for example, suppose that you are pointing at some data owned by a dlpack structure; you are not allowed to directly deallocate the data, you have to deallocate the dlpack struct. Context lets you point to some side channel data that gives you enough information to deallocate it.",
      "y": "you are not allowed to directly deallocate the data, you have to deallocate the dlpack struct."
   },
   {
      "x": "Add IsEmpty() to the at::Tensor in libtorch",
      "z": "Why not `t.numel() == 0`?",
      "y": "`t.numel() == 0`"
   },
   {
      "x": "Neon intrinsic types issue",
      "z": "a patch from sebpop to address the [missing intrinsics in GCC7 ](https://github.com/pytorch/pytorch/pull/43683)",
      "y": "address the missing intrinsics"
   },
   {
      "x": "torch-1.6.0 contains AVX instruction in the default codepath",
      "z": "Probably can fix this by removing all of the inline methods from TensorIterator. Not sure how to fix this at a deeper level though.",
      "y": "remove all of the inline methods from TensorIterator."
   },
   {
      "x": "Question, why Data moving to CUDA is slightly different",
      "z": "Because `model` is a 'container' for parameters/tensors. `model.to()` will loop over all parameters and call `.to()` on those parameters. A simplified version of how `to()` is implemented for a model is this:\n \n ```python\n \n for key, param in self._parameters.items():\n \n  self._parameters[key] = param.to(device)\n \n ```\n \n \n \n I don't see a way around this. The model will get modified in-place since it is just a container. Note though that you can still do `model = model.to(device)` so that your code will be consistent.",
      "y": "Because `model` is a 'container' for parameters/tensors."
   },
   {
      "x": "Proposal: Generic Triplet-Margin Loss",
      "z": "Update from offline conversation :\n \n \n \n This seems like a cool improvement on an existing module, but the work to implement it in core might be significant. If it's pursued, it's recommend to start by prototyping the C++ changes required in a BC-preserving way.",
      "y": "work to implement it in core might be significant. If it's pursued, it's recommend to start by prototyping the C++ changes required in a BC-preserving way."
   },
   {
      "x": "Possible return value bug when a function is registered in boxed form and called unboxed",
      "z": "not a bad question at all, this code is tricky and the use-case is very particular. The motivation is explained in [this comment](https://github.com/pytorch/pytorch/blob/master/aten/src/ATen/core/boxing/impl/boxing.h#L233-L238):\n \n ```\n \n // 4. signatures returning a tuple of Tensor references, and taking the same\n \n // number of Tensor refs as their initial arguments.\n \n //\n \n // Note that the passed kernels are assumed to be for inplace/outplace ops,\n \n // and the generated BoxedKernelWrapper specializations will return a tuple\n \n // of those initial arguments.\n \n ```\n \n Basically, the \"inplace/outplace\" ops referred to in the comment are ops that fit the common pattern of returning one or more of their Tensor arguments - this specialization implements boxing wrappers for the \"or more\" case. (The [preceding one](https://github.com/pytorch/pytorch/blob/master/aten/src/ATen/core/boxing/impl/boxing.h#L191-L228) handles the single-return case.)\n \n \n \n Also note that these specializations are only over mutable Tensor references, so we won't lose in-place updates made by the kernel - they'll have modified the Tensor referent; the kernel won't be pushing a new reference onto the stack. (BTW if you're wondering why we go the trouble of carefully scooping off the actual passed arg(s) like this in the first place, the answer is pretty arcane, but comes down to avoiding an extra refcount in the Tensors being returned.)",
      "y": "the \"inplace/outplace\" ops referred to in the comment are ops that fit the common pattern of returning one or more of their Tensor arguments - this specialization implements boxing wrappers for the \"or more\" case."
   },
   {
      "x": "Query regarding support for RISC-V Vector ISA",
      "z": "If you have a RISC-V compiler you're welcome to try (cross-)compiling to it, but I'm pretty sure no one here has every tried.",
      "y": "try (cross-)compiling to RISC-V"
   },
   {
      "x": "Autocompletion does not work in torch.nn module",
      "z": "I can reproduce this with 1.6.0, and see it's already fixed in master.",
      "y": "fixed in master"
   },
   {
      "x": "[Feature Request] Deformable Convolution",
      "z": "Does pytorch have any plan to add deformable convolution to its library? so researchers who use pytorch can compete with state of the art methods that use deformable convolution as their baseline?",
      "y": "add deformable convolution to pytorch library"
   },
   {
      "x": "SSL Handshake Error when getting pretrained model",
      "z": "Note: I was able to temporarily hack around the problem with:\n \n \n \n  import torchvision.models\n \n  from torchvision.models.vgg import model_urls\n \n  \n \n  model_urls['vgg16'] = model_urls['vgg16'].replace('https://', 'http://')\n \n  vgg16 = torchvision.models.vgg16(pretrained=True)",
      "y": "``` import torchvision.models\n \n  from torchvision.models.vgg import model_urls\n \n  \n \n  model_urls['vgg16'] = model_urls['vgg16'].replace('https://', 'http://')\n \n  vgg16 = torchvision.models.vgg16(pretrained=True)\n \n ```"
   },
   {
      "x": "BCEWithLogitsLoss TypeError",
      "z": "We currently don't support automatic type casting in pytorch, and while I agree allowing long types for the target would make it more similar to `nll_loss`, we would then have incompatibilities with `mse_loss` and others (which only accepts the same type as the input for the target). Also note that the target in `bce_with_logits` is not restricted to `{0,1}`, but instead `[0,1]`.\n \n Not sure what is the best to do here (leave as is, or change all losses where it makes sense to accept `long` as well for the target).",
      "y": "No support for automatic type casting in pytorch at present."
   },
   {
      "x": "Model params change with 0 learning rate",
      "z": "Do you have `weight_decay`? That could also explain the change.",
      "y": "have `weight_decay`."
   },
   {
      "x": "Import fails on 0.2 release installed with Conda",
      "z": "i'm working on fixing this, it'll be fixed in about 4 hours (new fixed binaries are being generated)",
      "y": "deactivate your current environment and try the import again"
   },
   {
      "x": "0.2 building from source",
      "z": "I think that the real question is why python2 is being used to build when the invoking interpreter was python3!",
      "y": "while building ATen, it invokes \"python\" instead of which python was invoked in setup.py install.\n \n worth fixing via an env variable that's set in setup.py."
   },
   {
      "x": "LSTM mask / remove zeros",
      "z": "Just sum the time axis and divide it with length tensor.",
      "y": "Sum the time axis and divide it with length tensor."
   },
   {
      "x": "Errors on jupyter",
      "z": "Hi all! I had the same problem.\n \n For ubuntu the solution is\n \n ```\n \n sudo apt-get install libtcmalloc-minimal4\n \n export LD_PRELOAD=\"/usr/lib/libtcmalloc_minimal.so.4\"\n \n ```",
      "y": "For ubuntu the solution is\n \n ```\n \n sudo apt-get install libtcmalloc-minimal4\n \n export LD_PRELOAD=\"/usr/lib/libtcmalloc_minimal.so.4\"\n \n ```"
   },
   {
      "x": "Variable' object has no attribute 'shape' [v0.2]",
      "z": "This has just been fixed on master, and will be present in the next release https://github.com/pytorch/pytorch/pull/2306",
      "y": "add shape to pass-throughs"
   },
   {
      "x": "Advanced Indexing Error",
      "z": "A few other things that don't work:\n \n ```python\n \n a = torch.zeros(2, 2)\n \n \n \n a[[0,1], 0] # doesn't work\n \n a[torch.LongTensor([0, 1]), 0] # doesn't work\n \n a[torch.LongTensor([0, 1])][:, 0] # works\n \n \n \n a[[0, 1]][:, 0] # doesn't work\n \n a[[0, 1], :1] # doesn't work\n \n a[[0, 1], 0:1] # doesn't work\n \n a[[0, 1], 0:2] # works\n \n ```",
      "y": "``` a[torch.LongTensor([0, 1])][:, 0]\n a[[0, 1], 0:2]\n ```"
   },
   {
      "x": "Unhandled CUDA Error (1)",
      "z": "Has there been any progress on this issue? I have two GTX 1080 Ti cards and I've been getting this error since I upgraded to v0.2 (using conda). I tried \"pip install\" as suggested above, and it works, but it causes other problems.",
      "y": "try \"pip install\""
   },
   {
      "x": "Python3 crashes when importing torch when referencing it.",
      "z": "seems similar to https://github.com/pytorch/pytorch/issues/2314\n \n we'll look into this, so weird.",
      "y": "the problem occurs only if we install wheel files, and not via conda."
   },
   {
      "x": "BrokenPipeError: [Errno 32] Broken pipe",
      "z": "You can set num_workers to 0 to see the actual error. Did you have your `plotter` correctly configured?",
      "y": "set num_workers to 0 to see the actual error."
   },
   {
      "x": "Does CosineEmbeddingLoss support CUDA tensors?",
      "z": "try this instead:\n \n ```\n \n y = Variable(torch.cuda.FloatTensor([1.0] * input1.size()[0]))\n \n ```\n \n \n \n BTW, questions like this are better discussed on https://discuss.pytorch.org/.",
      "y": "```\n \n y = Variable(torch.cuda.FloatTensor([1.0] * input1.size()[0]))\n \n ```"
   },
   {
      "x": "jupyter kernel died at division",
      "z": "This is probably due to division by 0 due to conversion of 0.1 to 0.",
      "y": "Due to division by 0 due to conversion of 0.1 to 0."
   },
   {
      "x": "I want to know how to use the select(int64_t dim, int64_t index) in at::Tensor?What is the definition of a parameter ?",
      "z": "it's selecting a slice at a certain index of a certain dimension",
      "y": "Select a slice at an index of a dimension."
   },
   {
      "x": "Wrong results when exporting a traced LSTM",
      "z": "Issue is no longer reproducible in `torch-nightly-1.0.0.dev20190304`.",
      "y": "No longer reproducible in `torch-nightly-1.0.0.dev20190304`."
   },
   {
      "x": "[Windows] subprocess.CalledProcessError (exit status 1) while installing pytorch by python-32",
      "z": "> A suggestion: CMake / setup.py to error out if Python 32-bit version is used\n \n \n \n I don't mind, but it should be mentioned in readme anyway: https://github.com/pytorch/pytorch/pull/17689 .",
      "y": "CMake / setup.py to error out if Python 32-bit version is used"
   },
   {
      "x": "torch.arange dtype mismatch with and without jit",
      "z": "thanks, we'll look into this!",
      "y": "Fix dtype of arange in JIT and remove dtype of arange from TorchANI"
   },
   {
      "x": "Can i use torch.nn.modules.pixelshuffle for commercial products?",
      "z": "We are not lawyers and we dont know :) (I didn't open that patent link and I am not going to, fyi)",
      "y": "Patent won't impede."
   },
   {
      "x": "big difference between numpy.matmul and torch.matmul",
      "z": "Hi,\n \n \n \n Welcome to the wonderful world of float operations.\n \n tldr: this is expected behavior because float operations are inexact.\n \n The order in which the ops are done will change the result and if you accumulate a large number of values (millions in your case), this difference will grow quite a lot.\n \n The idea for matmul is the same as for sum below.\n \n \n \n Here is a small code sample where the exact same array is summed but in different order. You can check that changing the number of threads will give different results as well.\n \n \n \n ```python\n \n import torch\n \n import numpy as np\n \n \n \n \n \n # Moving to double should reduce the difference to something very small ~1e-10\n \n used_type = torch.float\n \n # used_type = torch.double\n \n \n \n # Changing the number of threads will potentially change the order of execution\n \n # And thus change all the results\n \n num_threads = 12\n \n \n \n torch.manual_seed(1)\n \n torch.set_num_threads(num_threads)\n \n big_size = 1000000\n \n a = torch.randn((big_size, 20), dtype=used_type)\n \n print(\"Running with type {} and {} threads\".format(used_type, num_threads))\n \n \n \n sum_a = a.sum().item()\n \n sum_a_np = np.sum(a.numpy())\n \n print(\"original diff: \", sum_a - sum_a_np)\n \n \n \n print(\"diffs: torch vs torch \\t| torch vs np \\t| np vs np\")\n \n for _ in range(20):\n \n  idx = torch.randperm(big_size)\n \n  shuffled_a = a.index_select(0, idx)\n \n  new_sum_a = shuffled_a.sum().item()\n \n  new_sum_a_np = np.sum(shuffled_a.numpy())\n \n  print(\"diffs: {} \\t| {} \\t| {}\".format(sum_a - new_sum_a, new_sum_a - new_sum_a_np, sum_a_np - new_sum_a_np))\n \n ```",
      "y": "Expected behavior because float operations are inexact.\n \n The order in which the ops are done will change the result and if you accumulate a large number of values (millions in your case), this difference will grow quite a lot."
   },
   {
      "x": "The function of torch.max?",
      "z": "> out=max_temp\n \n \n \n `max_temp` is a view. You asked to insert the value back into the view.",
      "y": "Find max value in a tensor along a dimension."
   },
   {
      "x": "NNPack support for arm ( linux )",
      "z": "It's not so much a tutorial, though I always thought about making one: https://lernapparat.de/pytorch-android/",
      "y": "Building with NNPack and QNNPack works out of the box on both raspbian and arm64"
   },
   {
      "x": "Assigning a slice of the same tensor is not consistent on GPU",
      "z": "```\n \n tensor[:-500] = tensor[500:]\n \n ```\n \n \n \n In such an operation, there are significant data races as you are copying overlapping sets of elements. While one Tensor location is modified, another might read from it, and the ordering here will not be consistent, it'll depend on which block on the GPU gets free at what nanosecond.\n \n \n \n We dont aim to have such an operation be ordered. There is no specified notion of correctness in this operation, unless you write it as:\n \n \n \n ```\n \n x = tensor[500:].clone()\n \n tensor[:-500] = x\n \n ```\n \n \n \n because, if you write it as a single-line in-place operation, one has to assume a particular ordering of traversing the indices (but there's no standard).\n \n \n \n There are many situations where you will get wrong results when inputs and outputs are the same, for example: `torch.sum(x, dim=1, out=x)`.\n \n \n \n The fact that it worked on the CPU is merely because the CPU algorithm is probably serial.\n \n \n \n There are a few other issues where we did specify this, but I haven't found one based on a quick glance.",
      "y": "If you write it as a single-line in-place operation, one has to assume a particular ordering of traversing the indices (but there's no standard)."
   },
   {
      "x": "torch.max and torch.min inconsistent on cpu/gpu for tensors with zero elements.",
      "z": "I don't think the GPU behavior is completely wrong, for a mathematical reason: it should be the case that `torch.max(torch.cat(xs, ys)) == torch.max(torch.max(xs), torch.max(ys))` for any xs and ys, including empty. In that case, `-Inf` is the correct neutral element to pick.\n \n \n \n The way Numpy gets out of this situation, is they ask you for an initial element to handle the case explicitly, and error if you don't provide it. So in the end I agree with your suggested course of action.",
      "y": "It should be the case that `torch.max(torch.cat(xs, ys)) == torch.max(torch.max(xs), torch.max(ys))` for any xs and ys, including empty. In that case, `-Inf` is the correct neutral element to pick."
   },
   {
      "x": "Device agnostic gradient reduction",
      "z": "Not for the forward pass, but the backwards pass needs to be aware that gradients may be produced on different devices. I meant the mode of execution where you use multiple devices in the forward pass, as something to be aware of when hooking into the backwards pass. I.e. we keep a list of gradients per device, assuming that we see the same gradient on every device. This is not the case when doing combined model parallel and data parallel. Between this and model replication/broadcast that is device aware (so that you can run 2 or more models in the same process, like DDP supports today), there is some work needed to support combined model parallelism within process and data parallelism across processes.\n \n \n \n For example, let's say you use devices 0 and 1 and wrap your model with DDP, you'd want to execute:\n \n \n \n ```\n \n model = DDP(model, device_ids=[[0,1], [2,3]])\n \n ```\n \n \n \n Currently, model replication only works if the model uses a single GPU.\n \n \n \n Though you could argue that this mode of execution is not preferred and you should be using different processes anyway, I think that we should keep it in mind when designing this new reducer.",
      "y": "The mode of execution where you use multiple devices in the forward pass, as something to be aware of when hooking into the backwards pass. I.e. we keep a list of gradients per device, assuming that we see the same gradient on every device. This is not the case when doing combined model parallel and data parallel."
   },
   {
      "x": "Default forward method for ModuleList and ModuleDict",
      "z": "the exact `forward` methods to be specified will get controversial, because the most natural usage of ModuleList or ModuleDict are not obvious.\n \n \n \n However, the biggest danger is that people will use the `forward` expecting it to do something smart wrt memory (or worse, do what people want which is again not obvious), but it'll default-compute all outputs.\n \n \n \n For example, viewing a `forward` for `ModuleList` similar to `Sequential` is actually not far-fetched (I actually thought of a `forward` for ModuleList as a first-reaction to be similar to Sequential)\n \n \n \n I thought a bit about it, and I'm a bit ambivalent, but am leaning towards conservatively not adding the functionality.",
      "y": "Viewing a `forward` for `ModuleList` similar to `Sequential` is actually not far-fetched (I actually thought of a `forward` for ModuleList as a first-reaction to be similar to Sequential)"
   },
   {
      "x": "[CI] Is approval of reviewers reasonable while one of CI facilities is failure?",
      "z": "There are a few reasons:\n \n \n \n 1. CircleCI randomly craps out on a small percentage of runs, before we even run any code. We've told CircleCI about this but a fix has been very slow coming\n \n 2. We have a few flaky tests. It takes a bit of time to diagnose and disable them, so a few of them stick around and intermittently terrorize diffs. Search for \"flaky\" to see some standing issues on them. They need some time and debugging to figure out why they are flaky; some involve quite complex multithreaded systems.\n \n 3. ROCm is still a little unstable, and is often a culprit.\n \n \n \n One of the things I intend to work on in the near future is a better system for reporting failures to users, and letting them know if they are likely spurious or not.",
      "y": "1. CircleCI randomly craps out on a small percentage of runs, before we even run any code. We've told CircleCI about this but a fix has been very slow coming\n \n 2. We have a few flaky tests. It takes a bit of time to diagnose and disable them, so a few of them stick around and intermittently terrorize diffs. Search for \"flaky\" to see some standing issues on them. They need some time and debugging to figure out why they are flaky; some involve quite complex multithreaded systems.\n \n 3. ROCm is still a little unstable, and is often a culprit."
   },
   {
      "x": "Binomial.log_prob returns -inf when actual probability is 1 if logit is large",
      "z": "My bad - adapted the example code to use the named `logits` arguments.",
      "y": "Both n and p are required."
   },
   {
      "x": "RuntimeError: Only tuples, lists and Variables supported as JIT inputs, but got NoneType",
      "z": "Thanks for the report! Can you provide a script that we can run to reproduce the error you are seeing? That would help us investigate.",
      "y": "For add_graph, you need to pass through a sample bit of data in order to trace the graph."
   },
   {
      "x": "Tweak Java API before 1.3 release",
      "z": "> Proposed change: consolidate factory methods.\n \n \n \n I like! Seems closer to the existing API.\n \n \n \n > Proposed non-change: getters.\n \n \n \n I don't like :(. They're not really \"getters\" in the Java sense, right? Like, for a given IValue, you can only call one of those methods without erroring. When I see `getX` I assume it's basically a data field on a struct and I'm more or less directly retrieving it. `IValue::toX` is more \"perform a checked cast of me to type X\".",
      "y": "Consolidate factory methods."
   },
   {
      "x": "Python tests use torch.Doubletensor as their default tensor type",
      "z": "There actually is an existing decorator, @default_tensor_type, defined in some test files https://github.com/pytorch/pytorch/blob/a3ef0251fb3dfd85e3f4c874fcf004188eb509fd/test/test_jit.py#L90 but it's better if you can use the @dtypes decorator https://github.com/pytorch/pytorch/blob/a3ef0251fb3dfd85e3f4c874fcf004188eb509fd/test/common_device_type.py#L287 The latter instantiates a variant of your test for each dtype you list, in addition to giving you precise control over dtypes. It's also thread safe. \n \n \n \n @dtypes does require you write your test generically, however. A non-generic version of it may be interesting, too.",
      "y": "use the @dtypes decorator"
   },
   {
      "x": "record_stream() on a shifted view tensor doesn't work",
      "z": "Thanks for reporting the issue. Given that there is discussion in the PR you put up I'm closing this one.",
      "y": "record_stream() for shifted view tensors"
   },
   {
      "x": "Check DataPtr's deleter to determine if it is allocated by CUDA in record_stream",
      "z": "This might be a good bootcamp task.",
      "y": "Raise error if a block can not be found from a CUDA tensor."
   },
   {
      "x": "the list has inconsistent indentation.",
      "z": "Looks corrected now:\n \n ![image](https://user-images.githubusercontent.com/8042156/66335650-4f324380-e8f0-11e9-8e1b-326356d0d50e.png)",
      "y": "Documentation now corrected."
   },
   {
      "x": "CTCLoss cuda backend computes wrong gradient when target (i.e. label) length is greater than 896 for double inputs or 1024 for float inputs",
      "z": "Thank you for the bug report with detailed analysis! This is gold to reproduce and fix bugs.\n \n Indeed at first glance it looks like there is some looping missing when the number of threads is smaller than length. This should not happen.",
      "y": "There is some looping missing when the number of threads is smaller than length"
   },
   {
      "x": "Issues with ConvReLU2d and LinearReLU doc issues",
      "z": "Can you help us with the linking issue (#1 above)?\n \n \n \n Can you focus on the \"same as X\" becoming \"as X (same)\" issue?",
      "y": "\"same as X\" becoming \"as X (same)\"."
   },
   {
      "x": "add_zero_attn in MultiheadAttention breaks causality",
      "z": "I have not yet fully analyzed the problem and I am not sure if the observed effect is a bug or by design.\n \n \n \n It seems that in the self-attention variant with the added zero k & v sequence-entries (produced internally by `add_zero_attn=True`) the gradient is composed of a component coming from the query and one from values and keys. The gradient from the query is causing the undesired effect. \n \n \n \n I modified the repro script a bit in order to get the effect without `add_zero_attn=True` (initial in-projection bias should be 0) and to split the contribution of query and key & value: \n \n \n \n ```\n \n import torch\n \n \n \n embedding_dim = 1\n \n batch_size = 1\n \n num_heads = 1\n \n seq_len = 4\n \n \n \n net = torch.nn.MultiheadAttention(embedding_dim, num_heads, add_zero_attn=False)\n \n mask = torch.cat([torch.ones(seq_len, seq_len).triu(), torch.zeros(seq_len, 1)], dim=1)\n \n mask[mask==1]=float('-inf')\n \n print(mask)\n \n \n \n for i in range(seq_len):\n \n  x = torch.ones(seq_len, batch_size, embedding_dim, requires_grad=True)\n \n  y = torch.ones(seq_len, batch_size, embedding_dim, requires_grad=True)\n \n  z = torch.cat([y, torch.zeros(1, 1, embedding_dim)]) # add zero sequence element\n \n  o, w = net(x, z, z, attn_mask=mask)\n \n  #print(w)\n \n  # o.shape is (seq_len, batch_size, embedding_dim)\n \n  o.mean([1, 2])[i].backward()\n \n  print(i, 'x:', x.grad.abs().sum([1, 2]).view(-1))\n \n  print(i, 'y:', y.grad.abs().sum([1, 2]).view(-1))\n \n ``` \n \n \n \n Output is:\n \n ```\n \n tensor([[-inf, -inf, -inf, -inf, 0.],\n \n  [0., -inf, -inf, -inf, 0.],\n \n  [0., 0., -inf, -inf, 0.],\n \n  [0., 0., 0., -inf, 0.]])\n \n 0 x: tensor([0., 0., 0., 0.])\n \n 0 y: tensor([0., 0., 0., 0.])\n \n 1 x: tensor([0.0000, 0.0148, 0.0000, 0.0000])\n \n 1 y: tensor([0.2801, 0.0000, 0.0000, 0.0000])\n \n 2 x: tensor([0.0000, 0.0000, 0.0127, 0.0000])\n \n 2 y: tensor([0.1798, 0.1798, 0.0000, 0.0000])\n \n 3 x: tensor([0.0000, 0.0000, 0.0000, 0.0105])\n \n 3 y: tensor([0.1323, 0.1323, 0.1323, 0.0000])\n \n ```",
      "y": "in the self-attention variant with the added zero k & v sequence-entries (produced internally by `add_zero_attn=True`) the gradient is composed of a component coming from the query and one from values and keys. The gradient from the query is causing the undesired effect."
   },
   {
      "x": "torch.histc segfaults if array has inf",
      "z": "Will take a look",
      "y": "torch.histc added a finite range check to resolve segfaults if tensor has inf. also added checks for nan values, min>max"
   },
   {
      "x": "No module named _C",
      "z": "You could try running the test from another directory, this sometimes happens if there is a file named `torch.py` or (more likely in this case) a folder named `torch/` with an `__init__.py` inside, then `import torch` tries to import that directly instead of the installed/symlinked version in your Python's `site-packages`.\n \n \n \n Something might be getting installed incorrectly, the `import my_module` resolution rules can be found [here](https://docs.python.org/3/tutorial/modules.html#the-module-search-path). \n \n \n \n Similar issues #574, #17203, #7",
      "y": "happens if there is a file named `torch.py` or (more likely in this case) a folder named `torch/` with an `__init__.py` inside, then `import torch` tries to import that directly instead of the installed/symlinked version in your Python's `site-packages`."
   },
   {
      "x": "Jacobian-vector equation in autograd_tutorial font size is too small",
      "z": "OK, I'll close this then?",
      "y": "PyTorch tutorials are using MathJax library for the equation\n Change the setting of Mathjax\n Math render -> preview HTML instead of HTML-CSS"
   },
   {
      "x": "[JIT] builtin function attributes do not recursively compile",
      "z": "Well, it doesn't work if it's a builtin function:\n \n ```\n \n class Add(nn.Module):\n \n  def __init__(self):\n \n  super(Double, self).__init__()\n \n  self.add = torch.add\n \n \n \n  def forward(self, input):\n \n  return self.add(input)\n \n ```",
      "y": "Doesn't work if it's a builtin function."
   },
   {
      "x": "[ONNX] Export torch.meshgrid",
      "z": "There is an open PR adding support for meshgrid in https://github.com/pytorch/pytorch/pull/26037",
      "y": "Add support for meshgrid"
   },
   {
      "x": "Raspberry Pi Zero W build fails",
      "z": "There was activity on https://github.com/Maratyszcza/pthreadpool by @Maratyszcza last week.\n \n \n \n This must be related.",
      "y": "run the command:\n git submodule update --remote third_party/protobuf"
   },
   {
      "x": "torch.split with tensor sizes fails in tracing",
      "z": "I think this also affects ONNX exporter.",
      "y": "Issue with the tracer to handle traced Tensor inputs where int/int list is expected."
   },
   {
      "x": "How can I implement \"nn.unFold\" on 5D tensor?",
      "z": "You might be able to get some inspiration from the implementation we had in the old torch7 library [here](https://github.com/torch/nn/blob/872682558c48ee661ebff693aa5a41fcdefa7873/lib/THNN/generic/VolumetricConvolutionMM.c#L99-L263).\n \n I'm not sure how efficient it will be though.",
      "y": "Refer to the implementation in the old torch7 library"
   },
   {
      "x": "JIT profiling executor does not fuse Relu and Dropout for a GPU",
      "z": "I love the option's name `--bad-jit` @kevinstephano \u00f0\u0178\u02dc\u201e \n \n \n \n I think I know what the problem is here, I'll test the fix in the next few days.",
      "y": "Turn off profiling graph exec"
   },
   {
      "x": "[quant] QuantizedCUDA",
      "z": "Sorry that it took this much time. I was unexpectedly busy with another matter. Please, see the linked PR for the current implementation, it compiles and successfully passes all tests. \n \n \n \n Please, could you review it in the nearest time? I have unexpected free 1-2 weeks due to the global quarantine and will be able to fix any suggestions quickly. I had to make several decisions during development(that should be managed by internal team agreements that I am not aware of), so it might require some adjustments.",
      "y": "QuantizedCUDA implementation"
   },
   {
      "x": "kthvalue/median with scalar and dim=1 inconsistent between CPU and CUDA",
      "z": "This is out of scope of what I'm currently fixing. I'm currently fixing the part of scalar handling across the TH->ATen boundary that is: \"you returned to me a size `(1,)` tensor, but did you really mean to return me a size `()` Tensor?\"\n \n \n \n Scalar handing within ops is a bit of a different issue and I believe @ezyang's analysis of this specific case is correct.",
      "y": "Scalar handing within ops"
   },
   {
      "x": "Regression on split operator benchmark after __torch_function__ merge",
      "z": "Just to clarify before I start putting in pull requests, the way forward is:\n \n \n \n 1. ASAP remove the `torch_function_dispatch` decorator from everything in `torch.functional`\n \n 2. Look into either:\n \n  A) Speed up the decorator so the overhead is smaller. I don't think we can ever get the overhead to zero but we can probably at least get it much lower by doing C++-level checks for if parameters are `torch.tensor` instances or not\n \n  B) Rewrite the operators in `torch.functional` to be in C++",
      "y": "remove the `torch_function_dispatch` decorator from everything in `torch.functional`"
   },
   {
      "x": "DISABLED test_max_pool2d (__main__.TestQuantizedOps)",
      "z": "Is this still an issue?",
      "y": "Hypothesis deadline testing is now disabled altogether so we can reenable this test (if needed, on a plane and haven\u2019t checked)"
   },
   {
      "x": "backward hooks on parameters don't work with distributed autograd.",
      "z": "FWIW, this is also needed to enable DDP.\n \n \n \n If done, we can easily mix and match RPC with DDP for combined model and data parallelism.",
      "y": "Needed to mix and match RPC with DDP for combined model and data parallelism."
   },
   {
      "x": "error occures when trying to import torch, \"ImportError: cannot import name 'ClassType'\"",
      "z": "I had a similar issue with the error message: ImportError: cannot import name 'nan'. The fix was to do: \n \n \n \n `pip3 uninstall torch`\n \n \n \n multiple times (3, in my case) untill the message `Cannot uninstall requirement torch, not installed` appeared. After this, a fresh install via:\n \n \n \n `pip3 install torch==0.3.1`\n \n \n \n seemed to fix the problem. I realise we are using different versions of pytorch, but hopefully this helps.",
      "y": "`pip3 install torch==0.3.1`"
   },
   {
      "x": "I can't build pytorch 1.3.1 from sources.",
      "z": "It worked with USE_NINJA=OFF. Thank you guys for your support.",
      "y": "USE_NINJA=OFF."
   },
   {
      "x": "Windows Build fails",
      "z": "This is the reply from MS. I think we should make the change.\n \n \n \n > See the C++ standard, [meta] / p4\n \n > \n \n > Unless otherwise specified, the behavior of a program that adds specializations for any of the templates\n \n > specified in this subclause [meta] is undefined.",
      "y": "See the C++ standard, [meta] / p4\n Unless otherwise specified, the behavior of a program that adds specializations for any of the templates.\n Specified in this subclause [meta] is undefined."
   },
   {
      "x": "[jit] runtime error with backward of matmul and squeeze",
      "z": "Hm, this does not reproduce for me on master. Can you install the latest nightly and see if it reproduces in your environment?",
      "y": "Install the latest nightly and see if it reproduces in your environment"
   },
   {
      "x": ".sum() not return ideal result?",
      "z": "it's probably overflowing your ByteTensor, try converting it to a larger type and doing the sum on that.",
      "y": "Try converting it to a larger type and doing the sum on that."
   },
   {
      "x": "Compilation error: libATen.so.1: undefined reference to `convolve_5x5_sse'",
      "z": "Your problem is probably different; you probably have two versions of cudnn installed and are linking against the wrong one; uninstalling one copy will help (we have a bug tracking at #4860",
      "y": "You probably have two versions of cudnn installed and are linking against the wrong one."
   },
   {
      "x": "Runtime error(s) for Conv2d second gradient",
      "z": "i could reproduce this as well. we'll look into this.",
      "y": "Always define outputs of ConvBackwardBackward"
   },
   {
      "x": "layer-by-layer profiling feature",
      "z": "In case this is useful to anyone else, I've created a python package for layer by layer profiling that extends the autograd profiler. It only supports forward computation.\n \n https://github.com/awwong1/torchprof\n \n https://pypi.org/project/torchprof/",
      "y": "Python package for layer by layer profiling that extends the autograd profiler."
   },
   {
      "x": "additional continuous builds",
      "z": "Windows CI is now enabled",
      "y": "Windows CI has been enabled."
   },
   {
      "x": "add torch.stft and torch.fft",
      "z": "Yep, that's what I did. I surely didn't implement my own fft.",
      "y": "For proper fft, I think we should use existing wheels, e.g., cuFFT and MKL/Eigen."
   },
   {
      "x": "saved_tensors attribute for grad_fn",
      "z": "I think now (or soon?) it might be better to export the model to ONNX and then use the visualization tool they provide to inspect the graph?",
      "y": "Export the model to ONNX and then use the visualization tool they provide to inspect the graph."
   },
   {
      "x": "tensor.max(other) not documented",
      "z": "Maybe we should just disable that.",
      "y": "Document it or disable it."
   },
   {
      "x": "broadcasting behaves differently on CPU and GPU",
      "z": "Testing environment:\n \n - Ubuntu 16.04 LTS\n \n - CUDA 8.0.61 (GeForce GTX 960)\n \n - pytorch 0.2.0_3",
      "y": "Manually try to broadcast before calling the fused kernels."
   },
   {
      "x": "Problem when I load a state dictionary",
      "z": "Same problem. Here is a quick script I used to solve the problem. Help for future visitors here! \u00b8 \n \n \n \n ```\n \n import argparse\n \n import torch\n \n \n \n parser = argparse.ArgumentParser()\n \n parser.add_argument(\"--source\", type=str, required=True)\n \n parser.add_argument(\"--dest\", type=str, required=True)\n \n \n \n args = parser.parse_args()\n \n \n \n model_state = torch.load(args.source)\n \n new_model_state = {}\n \n \n \n for key in model_state.keys():\n \n  new_model_state[key[7:]] = model_state[key]\n \n \n \n torch.save(new_model_state, args.dest)\n \n ```\n \n Run it as: `python remove_module.py --source <source pickle file> --dest <destination pickle file>`",
      "y": "```\n \n import argparse\n \n import torch\n \n \n \n parser = argparse.ArgumentParser()\n \n parser.add_argument(\"--source\", type=str, required=True)\n \n parser.add_argument(\"--dest\", type=str, required=True)\n \n \n \n args = parser.parse_args()\n \n \n \n model_state = torch.load(args.source)\n \n new_model_state = {}\n \n \n \n for key in model_state.keys():\n \n  new_model_state[key[7:]] = model_state[key]\n \n \n \n torch.save(new_model_state, args.dest)\n \n ```"
   },
   {
      "x": "Commit breaks CUDA9 builds [NativeFunctions: support backend-specific dispatch]",
      "z": "ArchLinux w/ gcc5, able to build successfully by the following change:\n \n ```diff\n \n diff --git i/setup.py w/setup.py\n \n index e484692f..75e1836e 100644\n \n --- i/setup.py\n \n +++ w/setup.py\n \n @@ -90,7 +90,7 @@ import distutils.sysconfig\n \n  cfg_vars = distutils.sysconfig.get_config_vars()\n \n  for key, value in cfg_vars.items():\n \n  if type(value) == str:\n \n - cfg_vars[key] = value.replace(\"-Wstrict-prototypes\", \"\")\n \n + cfg_vars[key] = value.replace(\"-Wstrict-prototypes\", \"\").replace(\"-fno-plt\", \"\")\n \n \n \n  ################################################################################\n \n  # Custom build commands\n \n ```",
      "y": "```diff\n \n diff --git i/setup.py w/setup.py\n \n index e484692f..75e1836e 100644\n \n --- i/setup.py\n \n +++ w/setup.py\n \n @@ -90,7 +90,7 @@ import distutils.sysconfig\n \n  cfg_vars = distutils.sysconfig.get_config_vars()\n \n  for key, value in cfg_vars.items():\n \n  if type(value) == str:\n \n - cfg_vars[key] = value.replace(\"-Wstrict-prototypes\", \"\")\n \n + cfg_vars[key] = value.replace(\"-Wstrict-prototypes\", \"\").replace(\"-fno-plt\", \"\")\n \n \n \n  ################################################################################\n \n  # Custom build commands\n \n ```"
   },
   {
      "x": "Calling float() in modules converts integer buffers or parameters to floating point",
      "z": "this makes sense. i think you should go for it and make integer types immune to .float(), .half() and .double() calls.",
      "y": "Make integer types immune to .float(), .half() and .double() calls."
   },
   {
      "x": "Scheduler.step() doesn't perform optimizer.step()",
      "z": "This confuses me as well, instead I was thinking scheduler.step() will implicitly perform optimizer.step() as well since scheduler wraps optimizer.\n \n It would be appreciated if one comment (or example code) can be added to the official documentation, saying that scheduler.step() performs on epoch-level that only changes learning rate, while optimizer.step() performs on batch-level that does update to parameters.\n \n Many thanks",
      "y": "Scheduler.step() performs on epoch-level that only changes learning rate, while optimizer.step() performs on batch-level that does update to parameters."
   },
   {
      "x": "Building from source on master is broken",
      "z": "you can do the following:\n \n \n \n I will assume you have all the dependencies mentioned at https://github.com/pytorch/pytorch#from-source\n \n ```\n \n git clone --recursive https://github.com/pytorch/pytorch.git\n \n cd pytorch\n \n git pull origin pull/3831/head:build_fix\n \n git checkout build_fix\n \n export CMAKE_PREFIX_PATH=\"$(dirname $(which conda))/../\" # [anaconda root directory]\n \n python setup.py install | tee build.log # Writes the build logs to build.log\n \n \n \n ```",
      "y": "```\n \n git clone --recursive https://github.com/pytorch/pytorch.git\n \n cd pytorch\n \n git pull origin pull/3831/head:build_fix\n \n git checkout build_fix\n \n export CMAKE_PREFIX_PATH=\"$(dirname $(which conda))/../\" # [anaconda root directory]\n \n python setup.py install | tee build.log # Writes the build logs to build.log\n \n \n \n ```"
   },
   {
      "x": "python3--no module named PIL",
      "z": "I would recommend uninstalling Pillow, PIL (if they exist) and reinstalling pillow.\n \n Something like:\n \n ```\n \n pip uninstall Pillow\n \n pip uninstall PIL\n \n pip install Pillow\n \n ```\n \n In the future you should post your question to https://discuss.pytorch.org/ , we like to keep issues on github to PyTorch-only bugs.",
      "y": "Uninstall and reinstall PIL"
   },
   {
      "x": "torch.tensor does not support list of complex tensor",
      "z": "I think the root cause is because tensor objects do not have `__complex__`:\n \n ```\n \n >>> complex(torch.tensor(1j))\n \n Traceback (most recent call last):\n \n  File \"<stdin>\", line 1, in <module>\n \n RuntimeError: value cannot be converted to type double without overflow: (0,1)\n \n >>> torch.tensor(1j).__complex__()\n \n Traceback (most recent call last):\n \n  File \"<stdin>\", line 1, in <module>\n \n AttributeError: 'Tensor' object has no attribute '__complex__'\n \n ```",
      "y": "Tensor objects do not have `__complex__`"
   },
   {
      "x": "segfault together with \"import cv2\"",
      "z": "This turns out to be a glibc bug https://sourceware.org/bugzilla/show_bug.cgi?id=20839 triggered by some libs in opencv. The latest opencv-python has added a workaround for it. (details at https://github.com/skvark/opencv-python/issues/381)",
      "y": "glibc bug triggered by some libs in opencv."
   },
   {
      "x": "MultivariateNormal backprop performance issue related to broadcasting",
      "z": "It seems to me that broadcasting tensors together and then slicing them after is a bad idea and you should avoid doing it. If there are ways we can make it easier to do the right thing in the API, those might be worth adding.",
      "y": "Broadcasting tensors together and then slicing them after is a bad idea"
   },
   {
      "x": "CrossEntropyLoss does not raise target out of bounds error on gpu",
      "z": "Fixed in 1.5.1, 1.6 and master. Closing, please reopen if issue persists.",
      "y": "Fixed in 1.5.1, 1.6 and master."
   },
   {
      "x": "Missing `IndexError` when accessing elements outside of shape of CUDA tensor",
      "z": "I agree that the CUDA one should indeed raise...\n \n On the CPU error message part, it's actually \"makes sense\" that you're essentially doing zz[0][2] where `2` is the size of dimension 0. If you call `zz[0, 2]` it'll correctly report the dimension. We could probably live the current error message for now but the CUDA issue might be a high pri.",
      "y": "If you call `zz[0, 2]` it'll correctly report the dimension."
   },
   {
      "x": "torch.cat is over 300x slower than torch.index_copy / numpy.concatenate on CPU",
      "z": ">Just to clarify, I see these issues regardless of the number of threads (torch.set_num_threads), but increasing the number of processes through mp.spawn exacerbates the slowdown.\n \n \n \n How are you using `torch.set_num_threads` and how many CPU cores do you have? With all 10 processes operating single threaded on a 24-core CPU, I get `cat`, `index_copy` and `np.concatenate` all taking around 1-4 ms. With your original script I see `cat` on some processes taking only 1-2 ms but also sometimes taking much longer; as high as 50 ms or even 100 ms. I presume you've cherry picked the figures in the original report to emphasize these values.\n \n \n \n What I think is happening is that each process creates its own OpenMP thread pool, causing over subscription of hardware threads. NumPy doesn't suffer because it's single threaded, `index_copy` does many smaller copies (one for each index) all of which are less than [`at::internal::GRAIN_SIZE`](https://github.com/pytorch/pytorch/blob/ccea3726da3aca338eb7753e84ba370a13148855/aten/src/ATen/Parallel.h#L14) which means the copies aren't multithreaded.\n \n \n \n To confirm this, I can increase the `shape` parameter to `(8, 128, 1000)` where `128*1000` is greater than the grain size, therefore `index_copy` is multithreaded. As expected, I see a huge drop in performance of `index_copy`, in line with the `cat` performance. If I run again with `torch.set_num_threads(2)` it uses at most 20 threads of my 24-core machine and both `cat` and `index_copy` slightly out-perform NumPy. \n \n \n \n My suggestion is to use `torch.set_num_threads(n_threads)` so that `n_threads * n_procs <=` your actual hardware threads and that way you won't run into over-subscription issues.",
      "y": "Each process creates its own OpenMP thread pool, causing over subscription of hardware threads. NumPy doesn't suffer because it's single threaded, `index_copy` does many smaller copies (one for each index) all of which are less."
   },
   {
      "x": "C++ torch::min_values, torch::max_values no longer found in current libtorch",
      "z": "In the latest version, the following equivalents are available torch::amin and torch::amax.",
      "y": "Latest version has torch::amin and torch::amax."
   },
   {
      "x": "Documentation mention parameters `verbose` for torch.optim.lr_scheduler but it does not exist",
      "z": "Verbose param was added very recently to the schedulers (last month). It should be there in the nightly release (link to the [source](https://github.com/pytorch/pytorch/blob/master/torch/optim/lr_scheduler.py)). I guess the docs (for version 1.6.0) are not synced properly with the source, but it should be fixed in next stable release.",
      "y": "Verbose param was added recently to the schedulers and it should be there in the nightly release and hence the docs are not synced properly with the source."
   },
   {
      "x": "disable instruction set when build libtorch",
      "z": "PyTorch should not have an AVX instruction in the default codepath, so compiling in the host that supports AVX is fine. But if you want to avoid compiling the code that contains AVX/AVX2 instruction, you can try to build with DISABLE_AVX/DISABLE_AVX2/C_AVX_FOUND/C_AVX2_FOUND set to OFF",
      "y": "Build with DISABLE_AVX/DISABLE_AVX2/C_AVX_FOUND/C_AVX2_FOUND set to OFF."
   },
   {
      "x": "Integer overflow when doing 1x1 convolution on very large tensor",
      "z": "Closing as a duplicate of #43476, I'll reprioritize that one.",
      "y": "Integer overflow when doing 1x1 convolution on very large tensor."
   },
   {
      "x": "fx: unable to symbolically trace simple nn.Sequential model",
      "z": "The source generated here is:\n \n ```\n \n def forward(self, input):\n \n  0 = getattr(self, \"0\")(input)\n \n  \n \n \n \n  return 0\n \n ```\n \n \n \n And the error is a result of `Sequential` using integers in strings as keys (which is invalid in Python and thus why the `getattr` is emitted rather than a `self.0` attribute fetch).\n \n \n \n I'll see how hard it is to sanitize the identifiers in this scenario",
      "y": "Error is a result of `Sequential` using integers in strings as keys (which is invalid in Python and thus why the `getattr` is emitted rather than a `self.0` attribute fetch)."
   },
   {
      "x": "Xcode 12 Build Error: LibTorch/install/lib/libpytorch_qnnpack.a , building for iOS, but linking in object file built for macOS",
      "z": "The issue was in CMake's version. Obviously, 3.14 doesn't work well with XCode 12. We just tried 3.18, it works fine. Anyway, the solution would be\n \n \n \n 1. upload a bug fix version to cocoapods - 1.6.1\n \n 2. If you guys can't wait, you can build the static libs from source. Make sure you're using the latest CMake (3.18).",
      "y": "1. upload a bug fix version to cocoapods - 1.6.1\n \n 2. Build the static libs from source. Make sure you're using the latest CMake (3.18)."
   },
   {
      "x": "Installation with non-root access",
      "z": "Have you tried `python setup.py install --user`?",
      "y": "Try `python setup.py install --user`."
   },
   {
      "x": "Add sparse softmax/log_softmax functionality (ignore zero entries)",
      "z": "Reopening as the PR 36305 resolves the issue for CPU, not for CUDA.",
      "y": "Sparse softmax support (CUDA)"
   },
   {
      "x": "The expanded size of the tensor (13) must match the existing size (12) at non-singleton dimension 1. Target sizes: [3, 13]. Tensor sizes: [12]",
      "z": "Please wait we will try to provide more context / traceback.",
      "y": "It's very likely a problem with a downstream library."
   },
   {
      "x": "Back propagation trough slicing with list breaks",
      "z": "I don't think this is a bug. You are backproping twice through the x->y graph.\n \n \n \n The difference in x->y is that the first version does a copy where the the second does a view. The copy relies on a buffer kept alive to to backprop while the second doesn't need one. So backproping the first version twice will error, as buffers are freed at backproping.\n \n \n \n In generally you should only backprop multiple times with `retain_graph=True`, as said in the error message.",
      "y": "You are backproping twice through the x->y graph."
   },
   {
      "x": "Official and nightly wheel structure plan",
      "z": "UPDATE: We're also planning to rename the `torch_nightly` wheels to `torch`, in order to ensure that you never have two copies of torch installed in the same environment.",
      "y": "Rename the `torch_nightly` wheels to `torch`."
   },
   {
      "x": "Mention future package dependency for python 2.7 in documentation",
      "z": "This should probably just be phrased as a suggestion for fixing the generic install documentation as requirements.txt includes future.",
      "y": "Fix the generic install documentation as requirements.txt includes future."
   },
   {
      "x": "Versions of TorchScript",
      "z": "No official schedule as of yet, but probably within the next few weeks",
      "y": "We've made some breaking serialization changes between versions (those will be detailed in the 1.2 release notes soon). We try to maintain backwards compatibility between versions, but we don't make any guarantees about forward compatibility at this point."
   },
   {
      "x": "Conv1D output changes",
      "z": "You are manually setting weight, but what about bias?",
      "y": "Set `bias=False.`"
   },
   {
      "x": "Documentation for Tensor.record_stream()",
      "z": "Tensor.record_stream can simplify this sort of use-case and we'll happily accept a PR documenting the method.\n \n \n \n The timeline (with record_stream commented out) looks like:\n \n \n \n ```\n \n GPU 1 default : [allocate copied] + -> + -> [copied.sum()] -> [delete copied]\n \n GPU 0 copy_stream: +-> + -> [copied.copy_(to_copy)] + ->\n \n GPU 0 default : [allocate to_copy] -> [fill to_copy with ones] -+-> [delete to_copy] -> [to_calc * 100] ->\n \n ```\n \n \n \n The `+` denote synchronization and the `->` denote sequential ordering within a stream. There are three synchronizations (ignoring the torch.cuda.synchronize calls at the beginning). The copy_stream.wait_stream is the first. The inter-GPU copy `to_copy.to(1)` does a synchronization before and after between copy_stream (on GPU 0) and the current stream on GPU 1 (which is GPU 1's default stream in this case)\n \n \n \n Note that there is no synchronization preventing the deletion of to_copy from happening before the use. (Also deletions always \"occur\" in the allocation stream regardless of the current stream).\n \n \n \n You can fix this with:\n \n \n \n ```python\n \n import torch\n \n \n \n # We will use GPU-0 and GPU-1.\n \n torch.cuda.synchronize(0)\n \n torch.cuda.synchronize(1)\n \n \n \n to_copy = torch.ones(100, device=0)\n \n to_calc = torch.rand(100, device=0)\n \n \n \n # Introduce a separate stream for copy and synchronize to the default stream.\n \n # The copy will be started when \"to_copy\" is ready.\n \n default_stream = torch.cuda.default_stream(0)\n \n copy_stream = torch.cuda.Stream(0)\n \n copy_stream.wait_stream(default_stream)\n \n \n \n with torch.cuda.stream(copy_stream):\n \n  # Both the copy and computation on \"to_calc\" will start at the same time.\n \n  # But the copy will be finished later.\n \n  torch.cuda._sleep(100000000)\n \n  copied = to_copy.to(1)\n \n \n \n # Here's any computation which allocates some new tensors on the default stream.\n \n to_calc * 100\n \n \n \n # Free \"to_copy\".\n \n default_stream.wait_stream(copy_stream)\n \n del to_copy\n \n \n \n to_calc * 200\n \n \n \n print(copied.sum().item())\n \n ```\n \n \n \n Now the timeline looks like:\n \n \n \n ```\n \n GPU 1 default : [allocate copied] + -> + -> [copied.sum()] -> [delete copied]\n \n GPU 0 copy_stream: +-> + -> [copied.copy_(to_copy)] + -> + \n \n GPU 0 default : [allocate to_copy] -> [fill to_copy with ones] -+-> [to_calc * 100] -> + -> [del to_copy] -> [to_cal * 200]\n \n ```",
      "y": "Tensor.record_stream can simplify this sort of use-case."
   },
   {
      "x": "Jetson TX2 crashes when running simple nn on GPU using libtorch",
      "z": "I am currently updating the jetson so will try again with the newer version in the next day or so",
      "y": "Update the jetson and try with the newer version."
   },
   {
      "x": "Dedicated channel for PyTorch nightlies; no more munging package names",
      "z": "I added you to the pytorch-nightly account.",
      "y": "Move nightlies to the new channel for Windows."
   },
   {
      "x": "Torch.Quantization: functions for determining supported modules/functionals/methods for quantized tensors",
      "z": "wouldn't it be sufficient (and pythonic) for the user to do:\n \n \n \n `dir(torch.nn.quantized)` and other equivalents?",
      "y": "`dir(torch.nn.quantized)`"
   },
   {
      "x": "Add Type Promotion for Bool types",
      "z": "It's also convenient to be able to do `(some_int64_tensor - other_int64_tensor).mean()`, this kind of computation is frequent in accuracy metrics calculation. Currently one should explicitly cast to float() before doing mean(): `RuntimeError: Can only calculate the mean of floating types. Got Bool instead.`",
      "y": "Currently one should explicitly cast to float() before doing mean()."
   },
   {
      "x": "conda installs CPU only version of pytorch-nightly during new package uploads (CPU and win64 are uploaded first)",
      "z": "The new nightly channel at https://anaconda.org/pytorch-nightly/pytorch should be fully operational. Please let me know if things don't work.\n \n \n \n FYI the cpu-only package was renamed to cpuonly.",
      "y": "The cpu-only package was renamed to cpuonly."
   },
   {
      "x": "Drop Python 2 support",
      "z": "Well, if 10% is the bar, looks like we're on track to drop support in 2020 :)",
      "y": "We're on track to drop support in 2020."
   },
   {
      "x": "Difference in the implementation of rmsprop with Tensorflow",
      "z": "Since I'm cc'd on this thread, I've had great success with my variant of RMSProp that tries to stay true to the TF version. I've trained quite a number of models with excellent results and so have quite a few others. Trying similar hparams with the PyTorch RMSProp results in unstable training and often immediate blow ups in training, it's basically not usable in my trials and I've never managed acceptable results.\n \n \n \n There are 3 main differences:\n \n * https://github.com/rwightman/pytorch-image-models/blob/master/timm/optim/rmsprop_tf.py#L80\n \n * https://github.com/rwightman/pytorch-image-models/blob/master/timm/optim/rmsprop_tf.py#L105\n \n * https://github.com/rwightman/pytorch-image-models/blob/master/timm/optim/rmsprop_tf.py#L111 \n \n \n \n I also tried changing a few order of ops to closer match TF but I doubt there was any impact whatsoever.",
      "y": "Variant of RMSProp that tries to stay true to the TF version."
   },
   {
      "x": "Weird result for inplace operation for a tensor with itself",
      "z": "duplicate of #8212",
      "y": "This is expected behavior and will not be fixed."
   },
   {
      "x": "pytorch cannot be installed under Windows 10 if Python 3.7 was installed from Microsoft Store",
      "z": "Okay, so for anyone else looking for the workaround: In the Windows Registry, within HKEY_LOCAL_MACHINE\\SYSTEM\\CurrentControlSet\\Control\\FileSystem you have to set LongPathsEnabled to 1 and reboot.",
      "y": "In the Windows Registry, within HKEY_LOCAL_MACHINE\\SYSTEM\\CurrentControlSet\\Control\\FileSystem you have to set LongPathsEnabled to 1 and reboot."
   },
   {
      "x": "[MKLDNN] Corrupted malloc metadata in mkldnn_convolution_backward_input",
      "z": "We will take a look at this. I can reproduce this issue on my skylake machine.",
      "y": "This problem has disappeared after using jemalloc by runing:\n `export LD_PRELOAD=jemalloc/lib/libjemalloc.so`\n So this problem is the alloctor's problem used by mkldnn."
   },
   {
      "x": "[JIT] Support mixed int/float math in python",
      "z": "Closed with https://github.com/pytorch/pytorch/pull/13793",
      "y": "Add ops between float & int, and change list equality output to be a boolean."
   },
   {
      "x": "[JIT] Don't support varargs in script",
      "z": "While discussing in https://github.com/pytorch/pytorch/issues/8814 about not supporting deprecated functions, I thought about this issue.\n \n \n \n We now have two ways of constructing tensors with `torch.zeros`, `torch.rand` etc: by either passing a tuple with the sizes (as in numpy, `torch.zeros((2, 3))`) or by passing the varargs (`torch.zeros(2, 3)`).\n \n \n \n I am personally used to passing the varargs (less things to type), but we before only had the `out` keyword argument, and it was rarely used. But now we have `dtype` / `device` etc and they are very handy and are going to be present a lot in the code.\n \n \n \n My question is: are we considering the `varargs` constructor deprecated in favor of the tuple construct? In this case, we might want to not support the varargs.\n \n \n \n > There should be one-- and preferably only one --obvious way to do it.\n \n > The Zen of Python\n \n \n \n cc @apaszke",
      "y": "We now have two ways of constructing tensors with `torch.zeros`, `torch.rand` etc: by either passing a tuple with the sizes (as in numpy, `torch.zeros((2, 3))`) or by passing the varargs (`torch.zeros(2, 3)`).\n \n \n \n I am personally used to passing the varargs (less things to type), but we before only had the `out` keyword argument, and it was rarely used. But now we have `dtype` / `device` etc and they are very handy and are going to be present a lot in the code."
   },
   {
      "x": "[JIT] Don't support pass tuple in a PythonOp",
      "z": "This is supported now as long as the Python Op has annotations",
      "y": "This is supported now for as long as the Python Op contains annotations."
   },
   {
      "x": "[CAffe2] Issue compiling with Cuda 9.2, YellowFinOp error",
      "z": "I solved it by compiling with gcc 6 instead of 7. I think some of the syntax isn't compatible with gcc 7.",
      "y": "Compile with gcc 6 instead of 7. Some of the syntax isn't compatible with gcc 7."
   },
   {
      "x": "Fatal error: THC/THC.h: no such file or directory",
      "z": "I think I'm seeing the same thing as you @vishwakftw, will let you know if I figure out what's up...",
      "y": "Try `export CUDA_NVCC_EXECUTABLE=$(which nvcc)`"
   },
   {
      "x": "Problem with terminating dataloader workers",
      "z": "Thank you!\n \n \n \n I just wanted to make sure that it is an expected behavior in pytorch <= 0.4 and not a bug.",
      "y": "It is an expected behavior in pytorch <= 0.4 and not a bug."
   },
   {
      "x": "[JIT] Don't support torch.tensor/Tensor/FloatTensor in Script",
      "z": "I believe torch.tensor() is already supported in JIT now, you can try it in 1.0.1 and in pytorch master.",
      "y": "torch.tensor() is already supported in JIT now."
   },
   {
      "x": "[bug report] ConvTranspose is not padding output with zero",
      "z": "It doesn't adds zeros to input either. It is used to resolve the ambiguity when one have `stride > 1`. E.g., consider a 3x3 conv with stride = 2. You would have:\n \n 1. `3x3 input => 1x1 output`\n \n 2. `4x4 input => 1x1 output`\n \n 3. `5x5 input => 2x2 output`\n \n 4. `6x6 input => 2x2 output`.\n \n \n \n So it is a many-to-one mapping. For convT to \"revert\" this shape change, it needs an extra argument like `output_padding` to know (e.g. from `2x2 input` whether it should get `5x5 output` or `6x6 output`).",
      "y": "It is used to resolve the ambiguity when one have `stride > 1`."
   },
   {
      "x": "Argmax performance slower than numpy",
      "z": "You can try to benchmark that by breaking it into two lines and using some cpp benchmarking. But I highly doubt that it is the issue because it exists in so many cuda function calls.",
      "y": "Benchmark that by breaking it into two lines and by using a cpp benchmarking."
   },
   {
      "x": "Pytorch 0.4.0: Model behavior changes heavily after save and load weights",
      "z": "Hi, in your `mydataset.py` (full script [here](https://gist.github.com/SsnL/84dea832cebf4ccc44c6f7126352508f)), there is this part\n \n ```py\n \n  for i, lab in enumerate(set(self.label_nms)):\n \n  self.label_ids[lab] = i\n \n ```\n \n \n \n Since `set` is unordered, the mapping between `label_nm` and `label_id` is not deterministic. (They maybe are Py2, but that is undefined behavior.) So you see different results on train vs test, and different test runs. \n \n \n \n Since those in `self.label_nms` are just strings of digits, I changed the dataset to just store `int(label_nm)` as labels. Train + test works as expected. [Here](https://gist.github.com/SsnL/5946f101e2ed7e83292054a1af258632) is a modified dataset file.\n \n \n \n ```\n \n Test set: Average loss: 1.7855, Accuracy: 44/90 (49%)\n \n Train Epoch: 20 [0/90 (0%)] Loss: 1.765595 Accuuracy: 0.48\n \n Test set: Average loss: 1.7540, Accuracy: 43/90 (48%)\n \n Test set: Average loss: 1.7540, Accuracy: 43/90 (48%)\n \n acc2= 0.4777777777777778\n \n \u00e2\u017e\u0153 ld_weights_debug python validate.py\n \n 0.4777777777777778\n \n \u00e2\u017e\u0153 ld_weights_debug\n \n ```",
      "y": "Since `set` is unordered, the mapping between `label_nm` and `label_id` is not deterministic. (They maybe are Py2, but that is undefined behavior.) So you see different results on train vs test, and different test runs."
   },
   {
      "x": "output_padding constraint",
      "z": "I see.\n \n I removed those checks, and handled that case in Conv, because it was needed for Conv double backward. When Conv with stride > 1, dilation > 1 happens, the double-backward will involve this condition, if I remember.",
      "y": "When Conv with stride > 1, dilation > 1 happens, the double-backward will involve this condition."
   },
   {
      "x": "Build CUDA extension in windows 10",
      "z": "C++ extensions are well supported on Windows, thanks to @peterjc123. \n \n \n \n @xiaoxiangyeyuwangye please check out https://github.com/pytorch/extension-cpp. C++ extensions are easier, actively maintained and the future.",
      "y": "C++ extensions are well supported on Windows."
   },
   {
      "x": "Runtime Error thrown when using Optimizer in a Pytorch Function: element 0 of tensors does not require grad and does not have a grad_fn",
      "z": "I think all you need to do is to wrap the call of the function (in the autograd.Function forward):\n \n ```\n \n  with torch.enable_grad():\n \n  theta = solve_logistic_regression(X, y, lamb)\n \n ```\n \n ...oh, and return something more reasonable than `0`.",
      "y": "Wrap the call of the function (in the autograd.Function forward) and return something more reasonable than `0`."
   },
   {
      "x": "torch.cuda.sparse.FloatTensor is not enabled.",
      "z": "Please shout if you have this problem, so I can prioritize it accordingly.",
      "y": "It was because we weren't initializing CUDA on all codepaths that we should have. Basically, if you do some cuda operation before you invoke torch.cuda.sparse.FloatTensor, that will be sufficient to workaround."
   },
   {
      "x": "NaN loss when using half precision",
      "z": "This worked for me only with changing eps=1e-4\n \n thanks!",
      "y": "Change eps=1e-4"
   },
   {
      "x": "import torch: DLL load failed: The operating system cannot run %1. (pip installation)",
      "z": "Replacing the existing numpy with `numpy?1.14.5+mkl?cp36?cp36m?win_amd64.whl` from https://www.lfd.uci.edu/~gohlke/pythonlibs/#numpy resolved the issue for me (but not before wasting hours in try-this-and-that).\n It would've been nice if this issue was mentioned on the PyTorch installation page.",
      "y": "Replace the existing numpy with `numpy?1.14.5+mkl?cp36?cp36m?win_amd64.whl`"
   },
   {
      "x": "[print] improve ModuleList print format",
      "z": "Yeah, we shouldn't print the values. But we don't show classes like `torch.FloatTensor` anymore, and explicit using these classes are discouraged. We should show `dtype` and `device` instead.\n \n \n \n We should remove the extra line either way.",
      "y": "Shouldn't print the values. But we don't show classes like `torch.FloatTensor` anymore, and explicit using these classes are discouraged. We should show `dtype` and `device` instead."
   },
   {
      "x": "[complex] torch.pow : Incorrect output",
      "z": "you need to modify your x initialization to something like `auto x = std::complex<float>(argc > 1? atof(argv[1]) : -1001.2,-1001.2);`, otherwise modern compilers are smart enough to evaluate const expression at compile time using double precision arithmetic",
      "y": "Modify x initialization to something like `auto x = std::complex<float>(argc > 1? atof(argv[1]) : -1001.2,-1001.2);`, If not, modern compilers are smart enough to evaluate const expression at compile time using double precision arithmetic."
   },
   {
      "x": "The RandomSampler is not be set as expected.",
      "z": "For the branch of false `self.replacement`, we should use `generator` rather than `self.generator`.\n \n We would like to accept a PR to fix the bug.",
      "y": "Use `generator` rather than `self.generator`."
   },
   {
      "x": "[JIT] nn.Sequential of nn.Module with input type List[torch.Tensor] inferred to torch.Tensor",
      "z": "The fix for this is to subclass `nn.Sequential` and redeclare `forward` with the input typed as a list of tensors. We'll add a note to the documentation, as it's not necessarily intuitive",
      "y": "Subclass `nn.Sequential` and redeclare `forward` with the input typed as a list of tensors."
   },
   {
      "x": "Why isn't torch._aminmax() present in the docs?",
      "z": "In [`/aten/src/ATen/native/native_functions.yaml`](https://github.com/pytorch/pytorch/blob/master/aten/src/ATen/native/native_functions.yaml), I see that the functions starting with an underscore are reserved for internal use, and are not a part of the public-facing API. So, I'm closing this issue.",
      "y": "Functions starting with an underscore are reserved for internal use, and are not a part of the public-facing API."
   },
   {
      "x": "Replace deprecated AT_ERROR with `TORCH_CHECK(false,` in `c10`",
      "z": "I want to take a shot at this one.",
      "y": "AT_ERROR should be replaced with TORCH_CHECK(false, ...) for user-facing errors, but TORCH_INTERNAL_ASSERT(...) for errors caused or encountered by the system that are independent of the user."
   },
   {
      "x": "can't run dictionary with string key as input to jit model in libtorch",
      "z": "Hey, the problem is that you're missing the double underscores on `__init___` and you have incorrect spacing. Missing the underscores means that you're defining a custom method called `init`, so Python generates a default implementation of `__init__` for you. This code works:\n \n \n \n ```python\n \n import torch\n \n from typing import Dict\n \n \n \n class M(torch.nn.Module):\n \n  def __init__(self, aligned_height, aligned_width, spatial_scale):\n \n  super(M, self).__init__()\n \n  self.aligned_width = int(aligned_width)\n \n  self.aligned_height = int(aligned_height)\n \n  self.spatial_scale = float(spatial_scale)\n \n \n \n  def forward(self, d: Dict[str, torch.Tensor]) -> torch.Tensor:\n \n  return d[\"feature\"]\n \n \n \n \n \n sc = torch.jit.script(M(1, 1, 0.125))\n \n torch.jit.save(sc, 'scriptmodule.pt')\n \n loaded = torch.jit.load(\"scriptmodule.pt\")\n \n d = dict(feature=torch.rand(2, 3))\n \n loaded(d)\n \n ```",
      "y": "Missing the double underscores on `__init___` and there is incorrect spacing. Missing the underscores means defining a custom method called `init`, so Python generates a default implementation of `__init__` ."
   },
   {
      "x": "conda installation from nighlty causes package conflicts and fails to install PyTorch",
      "z": "I guess the official main page should be updated, it doesn't even list availability of CUDA11.1, while it (wrongly) lists availability of nightly of 11.0.\n \n \n \n Stable:\n \n ![image](https://user-images.githubusercontent.com/1041752/109400054-c1bd1c80-7946-11eb-80d5-7a46021f1e60.png)\n \n \n \n Nightly:\n \n ![image](https://user-images.githubusercontent.com/1041752/109400091-f8933280-7946-11eb-81a0-89e98b6079fd.png)",
      "y": "Official main page should be updated, it doesn't even list availability of CUDA11.1, while it (wrongly) lists availability of nightly of 11.0."
   },
   {
      "x": "torch.norm behavior for negative p when reducing over empty dimension",
      "z": "I'm not a fan of the `np.linalg.norm` behavior, I'd have preferred if it acted like `min`/`max`.\n \n \n \n One argument for that is that a reduction should have a proper mathematical and not just a numerical identity. `sum([]) == 0` and `prod([]) == 1` holds for all dtypes, but if `max([])` would be `inf` rather than raise, what do you return for integer input ? Something like `iinfo(int32).max` would lead to a lot of bugs, and having a dtype-dependent output shape or exception seems wrong.",
      "y": "A reduction should have a proper mathematical and not just a numerical identity."
   },
   {
      "x": "Incorrect type annotation for DataLoader",
      "z": "Sounds good to fix all of those @ejguan. Note that `mypy-strict.ini` already has the `--no-implicit-optional` setting, which is applied to some of the key files like codegen and autograd. There's also other settings that differ between the two mypy ini files. Maybe we should open a new tracking issue and figure out which settings to apply to the whole codebase. For example, `implicit_reexport` is likely worth doing too, to avoid more `pyright`-user complaints.",
      "y": "`mypy-strict.ini` already has the `--no-implicit-optional` setting, which is applied to some of the key files like codegen and autograd. There's also other settings that differ between the two mypy ini files."
   },
   {
      "x": "Import Torch Error. TypeError: function() argument 1 must be code, not str",
      "z": "In that case, I suggest you should file an issue against https://github.com/tqdm/tqdm \n \n Please note, that tqdm dependency is optional so you can uninstall the package to workaround the issue.\n \n And for the reference, can you please share the version of tqdm package you are using? (Can be queried by running `import tqdm; print(tqdm.__version__)` cell)",
      "y": "tqdm dependency is optional so you can uninstall the package to workaround the issue."
   },
   {
      "x": "Illegal memory access in cuda max pooling for large inputs",
      "z": "Doing multiplication in int64_t is good, can you please submit a fix? Posting a couple benchmarks would also be good.",
      "y": "Multiplication in int64_t is good."
   },
   {
      "x": "setup.py sdist does not include third party submodules",
      "z": "> \n \n > \n \n > Worse than that, it actually does not include most of the C++ source code :(\n \n \n \n That portion should be easy to solve though with a `MANIFEST.in`: https://packaging.python.org/guides/using-manifest-in/",
      "y": "That portion should be easy to solve though with a `MANIFEST.in`."
   },
   {
      "x": "Make `aten::adaptive_avg_pool3d` work for quantized Tensor inputs",
      "z": "On it",
      "y": "Quantized adaptive_avg_pool3d"
   },
   {
      "x": "[dist_autograd] GPU continuations does not work in distributed autograd",
      "z": "It looks like the RPC tutorial at https://github.com/pytorch/examples/blob/master/distributed/rpc/parameter_server/rpc_parameter_server.py is also broken, with the same error @pritamdamania87 posted. This was working earlier, not sure at what point it regressed.",
      "y": "Add basic GPU support to distributed autograd."
   },
   {
      "x": "[JIT] Support `with torch.autograd.profiler.record_function`",
      "z": "Someone has submitted a PR for this.",
      "y": "TorchScript Context Manager Support."
   },
   {
      "x": "RuntimeError: Only Tensors of floating point dtype can require gradients",
      "z": "Hi,\n \n \n \n You should **never** use `.data` :D By using it you can many many things that you should not be able to do. We're in the progress of deprecating and removing it.\n \n \n \n > After this line, the module's weight will be a long Tensor with requires_grad=True\n \n \n \n Yes this test looks like it should be updated. Making nn.Parameter with long type does not make sense (and that's why the test does forward only and has to use the internal `_apply` API to be able to achieve this).\n \n I think we should just change these to regular Tensors to avoid these issues.",
      "y": "Never use `.data`."
   },
   {
      "x": "Py pip installation error,zipfile.BadZipFile: Bad CRC-32 for file 'torch/lib/cudnn64_7.dll",
      "z": "you may also want to try the `no-cache` option, wondering whether it is caused by a corrupted ache.\n \n \n \n `pip install torch===1.5.1 torchvision===0.6.1 -f https://download.pytorch.org/whl/torch_stable.html --no-cache`",
      "y": "Try the `no-cache` option."
   },
   {
      "x": "RuntimeError: _th_exp_out not supported on CUDAType for Long",
      "z": "BCEWithLogitsLoss expects both preds and labels to be floating point (they are not labels, they are logits btw, so it makes sense that they are floating point.",
      "y": "BCEWithLogitsLoss needs both preds and labels to be floating point"
   },
   {
      "x": "scatter_ throwing a RunTimeError",
      "z": "I can confirm this bug exists on 1.5.0 and 1.5.1 but appears to be fixed in master. I am closing this issue because it has been fixed on master.",
      "y": "Allow index/src to have one dimension more than `self.dim` if their sizes are 1"
   },
   {
      "x": "Exponential distribution invalid parameter",
      "z": "Distributions have a `validate_args` argument (default is False). If you set it True, I think it will do what you're looking for.",
      "y": "Set `validate_args` argument to True."
   },
   {
      "x": "\"log_softmax_lastdim_kernel_impl\" not implemented for 'Long'",
      "z": "CrossEntropyLoss expects floating point inputs and long labels.",
      "y": "CrossEntropyLoss needs floating point inputs and long labels."
   },
   {
      "x": "mypy doesn't recognize torch functions that start with `_`",
      "z": "gh-40499 indeed fixed this, so closing. thanks @diego-plan9, all",
      "y": "Add __all__ to torch/_C/_VariableFunctions.pyi."
   },
   {
      "x": "typing is missing for most optimizers",
      "z": "That may very well be true. 1.6.0 is coming soon so it's unlikely there will be a 1.5.2 release to fix this issue.",
      "y": "1.6.0 is coming soon so it's unlikely there will be a 1.5.2 release to fix this issue."
   },
   {
      "x": "[v1.6.0] Release Tracker",
      "z": "See discussion in [#40452](https://github.com/pytorch/pytorch/pull/40452#issuecomment-657649332) the user wants to see at least change to CUDAStream.h so that they can build 1.6 with cuda11. Let me know if you want a PR with change to CUDAStream.h change only, or if we should cherry-pick all of #40452\n \n \n \n --- \n \n @malfet: Cherry-picked https://github.com/pytorch/pytorch/pull/40452 as https://github.com/pytorch/pytorch/pull/41543 and merged.",
      "y": "Change to CUDAStream.h so that we can build 1.6 with cuda11"
   },
   {
      "x": "libcurand.so.8.0: cannot open shared object file",
      "z": "you have an older pytorch install somehow on your machine.",
      "y": "Install latest PyTorch version."
   },
   {
      "x": "AttributeError: 'Conv2d' object has no attribute 'padding_mode' when loading model from pytorch 1.0 to 1.1",
      "z": "This should work, we can probably fix it by making the deserializer smarter.",
      "y": "Comment out the lines that use self.padding_mode in module.py, the model can then be imported, and saving the state_dictionary instead of the whole model allows loading into an unmodified version of pytorch 1.1."
   },
   {
      "x": "torch.hub does not close the resource before removing",
      "z": "In the _get_cache_or_reload function, add the \"cached_zipfile.close()\" call after the cached_zipfile.extractall(hub_dir) function to fix the bug.",
      "y": "Add the \"cached_zipfile.close()\" call after the cached_zipfile.extractall(hub_dir) function in the _get_cache_or_reload function."
   },
   {
      "x": "grid_sample is not aligned",
      "z": "Yes. I agree that the convention for `F.grid_sample` should be updated to match that of `align_corners=False` in `F.interpolate`.\n \n This would allow `F.grid_sample` to be agnostic to the size of the sampled image.\n \n \n \n The way it is currently defined, -1 and 1 refer to the centers of the edge pixels, which is not a consistent location on images of different sizes. To make this size-agnostic, these should refer to the edges of the image themselves, which doesn't change upon upsampling/downsampling the image.\n \n \n \n I have been working for almost a year now with a wrapped version of `F.grid_sample` which does just that, similar to what you use, since I have needed very precise behavior for my research. Mine is a bit more simplified, since note, for example, that in the wrapping function it is sufficient to multiply the grid by `size` and divide by `size - 1` (your conversion simplifies to this if you work it out analytically).\n \n It seems appropriate, however, that this should be the default behavior.\n \n \n \n If we decide to update the convention here, then we should also update the convention in `F.affine_grid` to match this as well.\n \n \n \n One potential issue with updating the convention is that it would break any existing user code that uses `F.grid_sample` on grids not generated from `F.affine_grid`. A possible way forward could be to do something similar to the way `F.upsample` and `F.interpolate` were changed, by adding a flag that reverts to the old behavior, so that users can keep the existing convention if they choose.\n \n \n \n @SsnL and @soumith What do you think?",
      "y": "Convention for `F.grid_sample` should be updated to match that of `align_corners=False` in `F.interpolate`.\n This would allow `F.grid_sample` to be agnostic to the size of the sampled image."
   },
   {
      "x": "libtorch terminate called after throwing an instance of 'c10::Error'",
      "z": "This is the correct error.\n \n \n \n The return value of model->forward is a C10::IValue.\n \n Faceboxes model returns a Tuple, not a Tensor.\n \n \n \n You need to do this:\n \n \n \n ```C++\n \n torch::jit::Stack outputs = model->forward({input}).toTuple()->elements();\n \n ```\n \n \n \n The variable `outputs` now contains your bounding boxes and your confidence tensors.",
      "y": "```C++\n \n torch::jit::Stack outputs = model->forward({input}).toTuple()->elements();\n \n ```\n \n \n \n The variable `outputs` now contains your bounding boxes and your confidence tensors."
   },
   {
      "x": "pytorch1.1.0 windows than one operator \" \" matches these operands",
      "z": "No, you don't need to do that. Just change `setup.py` to the following:\n \n ```python\n \n #!/usr/bin/env python3\n \n import os\n \n import torch\n \n \n \n from setuptools import setup, find_packages\n \n from torch.utils.cpp_extension import BuildExtension, CUDAExtension\n \n \n \n cxx_args = ['-std=c++11']\n \n \n \n nvcc_args = [\n \n '-gencode', 'arch=compute_50,code=sm_50',\n \n '-gencode', 'arch=compute_52,code=sm_52',\n \n '-gencode', 'arch=compute_60,code=sm_60',\n \n '-gencode', 'arch=compute_61,code=sm_61',\n \n '-gencode', 'arch=compute_70,code=sm_70',\n \n '-gencode', 'arch=compute_70,code=compute_70',\n \n '-D__CUDA_NO_HALF_OPERATORS__' # <-- Just add this line\n \n ]\n \n \n \n setup(\n \n name='correlation_cuda',\n \n ext_modules=[\n \n CUDAExtension('correlation_cuda', [\n \n 'correlation_cuda.cc',\n \n 'correlation_cuda_kernel.cu'\n \n ], extra_compile_args={'cxx': cxx_args, 'nvcc': nvcc_args})\n \n ],\n \n cmdclass={\n \n 'build_ext': BuildExtension\n \n })\n \n ```",
      "y": "Change `setup.py` to the following:\n \n ```python\n \n #!/usr/bin/env python3\n \n import os\n \n import torch\n \n \n \n from setuptools import setup, find_packages\n \n from torch.utils.cpp_extension import BuildExtension, CUDAExtension\n \n \n \n cxx_args = ['-std=c++11']\n \n \n \n nvcc_args = [\n \n '-gencode', 'arch=compute_50,code=sm_50',\n \n '-gencode', 'arch=compute_52,code=sm_52',\n \n '-gencode', 'arch=compute_60,code=sm_60',\n \n '-gencode', 'arch=compute_61,code=sm_61',\n \n '-gencode', 'arch=compute_70,code=sm_70',\n \n '-gencode', 'arch=compute_70,code=compute_70',\n \n '-D__CUDA_NO_HALF_OPERATORS__' # <-- Just add this line\n \n ]\n setup(\n \n name='correlation_cuda',\n \n ext_modules=[\n \n CUDAExtension('correlation_cuda', [\n \n 'correlation_cuda.cc',\n \n 'correlation_cuda_kernel.cu'\n \n ], extra_compile_args={'cxx': cxx_args, 'nvcc': nvcc_args})\n \n ],\n \n cmdclass={\n \n 'build_ext': BuildExtension\n \n })\n \n ```"
   },
   {
      "x": "expected ) but found 'ident' here: quantized::fake_quantize_per_tensor_affine_forward",
      "z": "For me, the problem occurs when using Sphinx Autodoc. It seems that the module import fails when running Sphinx, but it works in plain Python...\n \n \n \n In case anybody else encounters this issue with Sphinx, setting `autodoc_mock_imports = ['torch']` in the conf.py file is another way to work around this problem.",
      "y": "Problem occurs when using Sphinx Autodoc. It seems that the module import fails when running Sphinx, but it works in plain Python."
   },
   {
      "x": "Stabilize gradient for unfolded Tensor",
      "z": "You are right. I didn't properly think this through. Thanks for the heads up.",
      "y": "If you do finite differences, the gradient check wont match I think, except if you change the behavior during forward to divide the pixels of the output image by the number of overlapping regions."
   },
   {
      "x": "10% difference noticed between jit and python model",
      "z": "If you are hard blocked by this, one way you can help the JIT team out is by minimizing the test case. So remove parts of the transformer network while preserving the numerical discrepancy.",
      "y": "Remove parts of the transformer network while preserving the numerical discrepancy."
   },
   {
      "x": "torch.Size is not pickleable in Python 2",
      "z": "yes that would be great!",
      "y": "torch dtype object is already serializable."
   },
   {
      "x": "NervanaSystems/nervanagpu Repository not found for 0.4.0",
      "z": "I remove `nervanagpu` by doing this:\n \n ```bash\n \n git submodule deinit third_party/nervanagpu\n \n git rm --cached third_party/nervanagpu\n \n ```",
      "y": "```bash\n \n git submodule deinit third_party/nervanagpu\n \n git rm --cached third_party/nervanagpu\n \n ```"
   },
   {
      "x": "Different behavior numpy / pytorch with broadcasting & in-place operators",
      "z": "This is a duplicate of https://github.com/pytorch/pytorch/issues/906 , https://github.com/pytorch/pytorch/issues/10756 and https://github.com/pytorch/pytorch/issues/8212\n \n \n \n We are discussing on a potential solution to this problem by solving a diophantine equation like numpy does https://github.com/pytorch/pytorch/issues/8212, but to the best of my knowledge nobody is working on it.\n \n \n \n I'm closing this issue as a duplicate, but I'm bumping the priority of of https://github.com/pytorch/pytorch/issues/8212 to high, because it's a recurrent issue.",
      "y": "Solution to this problem is by solving a diophantine equation like numpy does."
   },
   {
      "x": "cublas runtime error on torch.bmm() with CUDA10 and RTX2080Ti",
      "z": "You need cuda 10 version of pytorch to run on RTX.",
      "y": "cuda 10 version of pytorch needs to be installed to run on RTX."
   },
   {
      "x": "BatchNorm2d implementation returns different results than expected",
      "z": "you are not in eval mode",
      "y": "Need to be in eval mode."
   },
   {
      "x": "A bug for torch.clone() when clone a MKLDNN tensor",
      "z": "I have submitted a [PR](https://github.com/pytorch/pytorch/pull/20943) about it, thanks!",
      "y": "Fix the bug for torch.clone() when clone a MKLDNN tensor"
   },
   {
      "x": "torch.empty(n, dtype=torch.int) produces non-deterministic arrays, not empty ones",
      "z": "That's the expected behavior. `torch.empty` doesn't initialize the data, and just returns whatever was in that position in memory. If you want the data to be initialized with some number, use `torch.zeros` or `torch.full`",
      "y": "`torch.empty` doesn't initialize the data, and just returns whatever was in that position in memory. If you want the data to be initialized with some number, use `torch.zeros` or `torch.full`"
   },
   {
      "x": "[jit] Changes to TorchScript API",
      "z": "We should deprecate inheriting from `ScriptModule`. It's a very weird API that I often find hard to explain to people (in contrary to the simple `torch.jit.script` annotation).",
      "y": "Inheriting from `ScriptModule` should not be allowed."
   },
   {
      "x": "Tensor identity comparisons not working / unclear",
      "z": "Tensor.data returns an alias, just like detach() -- same underlying data but different Tensor object. Two evaluations of `x.data` do **not** refer to the same thing.\n \n \n \n > Shouldn't either both of the last two statements be True, or neither? It seems odd that 'is' gives a different result than id()==id().\n \n \n \n No, this is because `id` returns the memory address and memory addresses can be reused once an object goes out scope. `a.data` goes out of scope before `b.data` is evaluated. `id` only uniquely identify an object **during that object's lifetime!** The same often happens with core Python objects:\n \n \n \n ```\n \n id({}) == id({}) # True\n \n {} is {} # False\n \n ```\n \n \n \n If you keep the object in scope the id is unique:\n \n \n \n ```\n \n tmp0 = a.data\n \n tmp1 = b.data\n \n id(tmp0) == id(tmp1) # False\n \n ```",
      "y": "Tensor.data returns an alias like detach() -- same underlying data but different Tensor object. Two evaluations of `x.data` do **not** refer to the same thing."
   },
   {
      "x": "assignment to a twice-sliced tensor does nothing",
      "z": "assignment will call __newindex__ on the same tensor",
      "y": "Because assignment will call __newindex__ on the same tensor"
   },
   {
      "x": "Windows 10/Python 3.5 Pytorch 1.1.0 CPU increase versus 1.0.0 mnist",
      "z": "because of a stupid bug, 1.0.0 did not ship with OpenMP enabled. 1.1.0 shipped with OpenMP enabled, and uses multiple cores. This is in line with expectations. MNIST as a workload is really small, so OpenMP's multithread optimizations are probably slowing down the workload because most of the time is simply spent in thread overhead.\n \n This is fixed on master because we moved away from OpenMP to our own threadpool which has more reasonable characteristics.",
      "y": "Due to a bug, version 1.0.0 did not ship with OpenMP enabled. 1.1.0 shipped with OpenMP enabled, and uses multiple cores."
   },
   {
      "x": "JIT-compiled function produces incorrect results",
      "z": "Thanks for the repro steps. Looking into this.",
      "y": "The problem is that there is a dependency from `sqrt` to `+` in function `f`, due to which we need to add `SyncThreads` between those statements."
   },
   {
      "x": "Model output difference between android device and desktop",
      "z": "can you share the qconfig you used? If you are running things on `fbgemm`, `reduce_range` needs to be set to True to avoid overflow in some of the kernels.",
      "y": "On `fbgemm`, `reduce_range` needs to be set to True to avoid overflow in some of the kernels."
   },
   {
      "x": "[complex] torch.sigmoid: sigmoid does not support automatic differentiation for outputs with complex dtype",
      "z": "Thank you for filing this issue, @kshitij12345. Note that SciPy's expit does not even support complex input:\n \n \n \n ```\n \n scipy.special.expit(np.array(1, dtype=np.cdouble))\n \n : TypeError: ufunc 'expit' not supported for the input types, and the inputs could not be safely coerced to any supported types according to the casting rule ''safe''\n \n ```\n \n \n \n However, if the derivative is well-defined for complex inputs then we would accept a PR implementing the behavior.\n \n \n \n As an aside, PyTorch's use of the name \"sigmoid\" for this function seems odd. \"Logistic\" or \"expit\" are probably better names.",
      "y": "SciPy's expit does not support complex input."
   },
   {
      "x": "NCCL backend doesn't use MASTER_PORT during reconnect",
      "z": "In that case, is it ok to close out this issue? As Omkar mentioned above you need to restart everything and reinitialize a new process group or use torchelastic.",
      "y": "Restart everything and reinitialize a new process group or use torchelastic."
   },
   {
      "x": "PyTorch cannot be linked (libtorch_cuda.so) at the last step of compilation with CUDA 11.1 with PyTorch/builder",
      "z": "Try `export TORCH_CUDA_ARCH_LIST=\"8.6\"`.",
      "y": "`export TORCH_CUDA_ARCH_LIST=\"8.6\"`"
   },
   {
      "x": "[FR] add huber option for smooth_l1_loss",
      "z": "I'd start a little earlier in the discussions -- why did we add beta without huber? (See https://github.com/pytorch/pytorch/issues/16357 for the request for beta). Does anyone actually want `beta` and not `huber`?\n \n \n \n Maybe @fmassa has an opinion.",
      "y": "Add a new `huber` arg (default `False`) to the existing `SmoothL1Loss`."
   },
   {
      "x": "torch.nn.Module.named_parameters has wrong type annotation",
      "z": "I believe this issue can be closed since gh-49045 was merged.",
      "y": "add type annotations to torch.nn.modules.module"
   },
   {
      "x": "Torchscript does not work with type: ignore comments for mypy",
      "z": "The proper fix has landed, you shouldn't need to use the workaround in my earlier comment any more (provided you use recent enough pytorch build)",
      "y": "The bug is due to an assumption in TorchScript when parsing source code: In function declaration, any line that contains `# type:` is considered to be a type line. Then TorchScript lexer expects the type line to contain only type comment, nothing else."
   },
   {
      "x": "Enhanced generators with grad-mode decorators",
      "z": "Thank you for the suggestion. If you have a PR in mind, please do not hesitate to publish it.",
      "y": "Support for enhanced generators (generator-based coroutines) by grad-mode decorators."
   },
   {
      "x": "__torch_function__ PR may cause performance regression on GPU training",
      "z": "> @robieta Thanks for the fix. I've tested the head of #48966 PR at [6287057](https://github.com/pytorch/pytorch/commit/6287057c90a5a0450a0bfd2e0c1d0fbe8fa02b23). The performance of resnet is well restored. I hope to see that PR gets merged soon.\n \n \n \n Awesome! Yeah, it should go in shortly.",
      "y": "Skip the `if not torch.jit.is_scripting():` guards on functional and nn.functional by directly registering `has_torch_function` and `object_has_torch_function` to the JIT as statically False."
   },
   {
      "x": "[online docs] anchors to source code are missing",
      "z": "Thanks for the report. I think we tracked down the issue. I should have a fix in today.",
      "y": "In 1.6.0 the `id=\"Module.eval\"` exists."
   },
   {
      "x": "[docs] torch.nn.functional.one_hot docs missing",
      "z": "Just to help those that stumble across this question (like I did), you can find the documentation here:\n \n https://pytorch.org/docs/stable/nn.functional.html",
      "y": "Find the documentation here:\n \n https://pytorch.org/docs/stable/nn.functional.html"
   },
   {
      "x": "Missing gradient when autograd called inside a function on Multi-GPU (eg gradient penalty)",
      "z": "> curious: does this work pre [89d56ae](https://github.com/pytorch/pytorch/commit/89d56ae435373e54932566806c8432634e6a5cfa)\n \n \n \n Verified: it works on 89d56ae^1 and breaks on 89d56ae.\n \n \n \n @gchanan any suggestion on fixes?",
      "y": "The `use_count` of function `shared_ptr` are incorrect on C++ side as we create multiple `shared_ptr` from raw pointers when replicate modules."
   },
   {
      "x": "Apply for translation of the Chinese version, I hope to get authorization!",
      "z": "We are very happy to do this meaningful thing.\n \n We have iterated three Chinese versions, and we are very grateful to the translation and participation of many Chinese contributors. Of course, the most should be the pytorch tutorial, which really helps us to learn more comprehensive and systematic learning of deep learning.\n \n Everyone said that they really like your framework and your team. This is why we have been insisting on translation.",
      "y": "Iterated three Chinese versions."
   },
   {
      "x": "C++ link error",
      "z": "In case anyone has a similar issue, build the lib from source.\n \n This seems like an issue which your local project compiler is incompatible with the compiler building libtorch. \n \n More detail in #15138",
      "y": "Build the lib from source."
   },
   {
      "x": "cannot initialize type \"_CudaDeviceProperties\" error",
      "z": "OK, it is indeed a race. This diff, for example, \"fixes\" it:\n \n ```\n \n diff --git a/torch/cuda/__init__.py b/torch/cuda/__init__.py\n \n index 411cfb7315..5abb645fc5 100644\n \n --- a/torch/cuda/__init__.py\n \n +++ b/torch/cuda/__init__.py\n \n @@ -165,6 +165,7 @@ def _lazy_init():\n \n  global _initialized, _cudart, _original_pid, _queued_calls\n \n  if _initialized:\n \n  return\n \n + _initialized = True\n \n  if _in_bad_fork:\n \n  from sys import version_info\n \n  if version_info < (3, 4):\n \n @@ -181,7 +182,6 @@ def _lazy_init():\n \n  _cudart.cudaGetErrorName.restype = ctypes.c_char_p\n \n  _cudart.cudaGetErrorString.restype = ctypes.c_char_p\n \n  _original_pid = os.getpid()\n \n - _initialized = True\n \n  # Important to do this after _initialized, since some queued calls\n \n  # may themselves call _lazy_init()\n \n  for queued_call, orig_traceback in _queued_calls:\n \n ```\n \n \n \n If you look at the relevant segment:\n \n \n \n ```\n \n  torch._C._cuda_init()\n \n  _cudart = _load_cudart()\n \n  _cudart.cudaGetErrorName.restype = ctypes.c_char_p\n \n  _cudart.cudaGetErrorString.restype = ctypes.c_char_p\n \n  _original_pid = os.getpid()\n \n  _initialized = True\n \n ```\n \n \n \n The problem is that `_cuda_init` release the GIL at some point, which means that another Python thread can come in and trigger the same initialization (we aren't protected against the lock until we set `_initialized = True`.",
      "y": "`_cuda_init` release the GIL at some point, which means that another Python thread can come in and trigger the same initialization (we aren't protected against the lock until we set `_initialized = True`."
   },
   {
      "x": "High scope of error in how view() may be used",
      "z": "`view()` is the same as `reshape`, which is the same as [numpy's reshape](https://docs.scipy.org/doc/numpy-1.15.0/reference/generated/numpy.reshape.html).\n \n \n \n If you feel that more examples should be added to the doc, we will accept PR for that.",
      "y": "`view()` is the same as `reshape`."
   },
   {
      "x": "Error on backward pass with sparse matmul with transposed tensor",
      "z": "I think I figured out what is causing this, if so will put up a PR to fix it today",
      "y": "Don't attempt to multiply by a sparse matrix."
   },
   {
      "x": "Why is `torch.mean()` so different from `numpy.average()`?",
      "z": "I developed a [pytorch_math](https://github.com/DanielAtKrypton/pytorch_math) package that bridges the difference gap. See usage example [here](https://github.com/DanielAtKrypton/pytorch_math/blob/master/tests/average_test.py#L62).",
      "y": "`pytorch_math` package that bridges the difference gap."
   },
   {
      "x": "Make it possible to use mypy to typecheck code that uses PyTorch",
      "z": "> The main issue after that is how to make sure that the annotations are actually correct.\n \n \n \n After gh-52408 and gh-54234 we now have a nice compact way to add tests for type annotations. And anecdotally, the number of type annotation errors slipping through or breaking in CI has reduced over time.\n \n \n \n Mypy is upgraded to the latest version (0.812) and runs on almost all files now, there's only a handful of `ignore_errors` for files under `torch/` and (with 2 exceptions for which PRs have been open for some time). So we're in pretty decent shape here.\n \n \n \n There are some open issues left with `module: typing`, but those are all smaller things and can be handled like any other issue. I think we can declary victory here and close this tracking issue.",
      "y": "Mypy is upgraded to the latest version (0.812) and runs on almost all files now, there's only a handful of `ignore_errors` for files under `torch/` ."
   },
   {
      "x": "Weird behavior of torch.int()",
      "z": "number of precision for printing tensors is 4 by default. try [torch.set_printoptions](https://pytorch.org/docs/master/torch.html#torch.set_printoptions).\n \n ```python\n \n torch.set_printoptions(precision=10)\n \n v1 = torch.Tensor([1.00001, 0.99999])\n \n v2 = torch.Tensor([-0.99999, -1.000001])\n \n print(v1, v2) # tensor([1.0000100136, 0.9999899864]) tensor([-0.9999899864, -1.0000009537])\n \n print(v1.int(), v2.int()) # tensor([1, 0], dtype=torch.int32) tensor([ 0, -1], dtype=torch.int32)\n \n ```",
      "y": "Number of precision for printing tensors by default is 4."
   },
   {
      "x": "[JIT] Sometimes rewrite argument name",
      "z": "Amazing @zdevito :)",
      "y": "Preserve method parameter names."
   },
   {
      "x": "torch::serialization.load() doesn't support pathlib.Path object for the input argument",
      "z": "Seems reasonable to add. We'll happily accept a PR with the change and a test.",
      "y": "Load success from the any kind of '*.pth.tar' or some other extensions."
   },
   {
      "x": "test_proper_exit is flaky",
      "z": "Quick tip: \"module: build\" means build system problems; it doesn't refer to ci flakiness :)",
      "y": "\"module: build\" means build system problems; it doesn't refer to ci flakiness."
   },
   {
      "x": "Potential instability in ConvTranspose2d with cudnn",
      "z": "apparently a workaround is to use `cudnn.benchmark=True`. @ngimel is looking if a fix is possible. We'll try our best to get it into v1.0.1",
      "y": "Use `cudnn.benchmark=True` ."
   },
   {
      "x": "Allow C++ inference of JITted modules to run different modules on different streams",
      "z": "Thanks for the suggestion!\n \n \n \n On adding support of `with torch.cuda.stream(...):` in TorchScript, we'd prefer to avoid it or at least wait until it's absolutely necessary. The reason is that this API hard-codes pretty explicitly device (gpu) and it'd be harder to move the saved model to CPU later. Similarly, for inference cases, it might be preferable to control stream behavior in a central fashion - for example, be able to instruct jit::Interpreter to run only on X stream. Design-wise, it's preferable to expose more abstract parallelization primitives. We have a fork/join prototype (https://github.com/pytorch/pytorch/blob/master/test/test_jit.py#L12172) and it would be logical to extend this mechanism to introduce parallel streams.\n \n \n \n However, nothing prevents us from manipulating streams directly in C++. `torch.cuda.stream` are just a thin wrappers around c10::Stream and the current stream gets stored in a thread-local variable. Thus if one puts `CUDAStreamGuard` around module invocation in C++ it should make all computation run on that stream transparently. Let me know if it doesn't work for some reason.",
      "y": "Add support of `with torch.cuda.stream(...):` in TorchScript."
   },
   {
      "x": "torch.to_dense() adds random values",
      "z": "Your random indices tensor can generate multiple entries with value `1` but same index, hence the `2`s (and even `3`s if you are lucky enough).",
      "y": "Random indices tensor can generate multiple entries with value `1` but same index."
   },
   {
      "x": "Installing Windows 10",
      "z": "Hey @MohMehrnia \n \n \n \n Can you verify that you have the 64-bit installation of Python? By running this command: `python -c \"import struct; print(struct.calcsize('P') * 8)\"`. If it's 32-bit, try re-installing 64-bit Python.\n \n \n \n Others might have another idea well.\n \n \n \n Thanks\n \n James",
      "y": "Have the 64-bit installation of Python."
   },
   {
      "x": "About a conditional expression of cutoffs in torch.nn.AdaptiveLogSoftmaxWithLoss.",
      "z": "I think this has been fixed already (sorry @Joel-hanson ). The relevant work was done on Fix AdaptiveLogSoftmaxWithLoss's constructor by @t-ken1 and @wbydo",
      "y": "Fix AdaptiveLogSoftmaxWithLoss's constructor"
   },
   {
      "x": "[JIT] Additional list methods in script",
      "z": "Current status:\n \n \n \n - [x] clear (https://github.com/pytorch/pytorch/pull/17050)\n \n - [x] pop (https://github.com/pytorch/pytorch/pull/17015)\n \n - [x] reverse (https://github.com/pytorch/pytorch/pull/17001)\n \n - [x] copy (https://github.com/pytorch/pytorch/pull/17092)\n \n - [x] extend (https://github.com/pytorch/pytorch/pull/17092)\n \n - [x] insert (https://github.com/pytorch/pytorch/pull/17200)\n \n - [x] remove (https://github.com/pytorch/pytorch/pull/17200)\n \n - [x] index (https://github.com/pytorch/pytorch/pull/17446)\n \n - [x] count (https://github.com/pytorch/pytorch/pull/17446)\n \n - [ ] sort",
      "y": "Clear, Pop, Reverse, Copy, Extend, Insert, Remove, Index, Count are added. Sort is yet to be added."
   },
   {
      "x": "JIT Script module support for pad_packed_sequence and pack_padded_sequence",
      "z": "just to confirm: you get an error similar to this, right?\n \n \n \n ```python\n \n for operator (Tensor 0, Tensor 1, Tensor 2, Tensor 3) -> Tensor:\n \n expected a value of type Tensor for argument '2' but found bool\n \n \n \n rnn_input = pack_padded_sequence(\n \n  embedded,\n \n  seq_lengths.int(), \n \n  True,\n \n  ~~~~~~ <--- HERE\n \n  True)\n \n ```",
      "y": "Support added for pack_padded_sequence and pad_packed_sequence."
   },
   {
      "x": "[Feature] Sparse tensor persistence/save.",
      "z": "if I mark this as high-priority will it get fixed?",
      "y": "Implement pickle support for sparse tensors and torch.layout instances."
   },
   {
      "x": "[jit] Autodiff crash when some chunk outputs aren't used in backward",
      "z": "thanks! @wanchaol can you take a look?",
      "y": "Change the chunk autodiff formula to handle this case where some of the gradients are undefined."
   },
   {
      "x": "[Feature] 'None' for arbitrarily shaped buffer/parameter when loading models.",
      "z": "what you are asking for is to legitimize a hacky, possibly removable feature of `register_buffer`, so no :)\n \n \n \n I'd say, `state_dict` is such a simple stupid format, that writing a `load_model(model, state_dict)` that works for your model is the way to go",
      "y": "Write a `load_model(model, state_dict)` that works for your model."
   },
   {
      "x": "[python setup.py install] g++ error: stub.o file format not recognized",
      "z": "Closing this issue. I found what was wrong and **I fixed it**. The whole problem lies in the fact that Anaconda distribution comes with its own `ld` linker that is located in `/opt/anaconda/compiler_compat/` and it overshadows system `ld` residing at `/usr/bin`.\n \n \n \n You can see that the build command (which caused my error) uses Anaconda's `ld` instead of system `ld` as it specifies `-B` option which points to Anaconda's folder:\n \n ```bash\n \n g++ -pthread -shared -B /opt/anaconda/compiler_compat -L/opt/anaconda/lib -Wl,-rpath=/opt/anaconda/lib -Wl,--no-as-needed -Wl,--sysroot=/ build/temp.linux-x86_64-3.7/torch/csrc/stub.o -L/home/manjaro/Downloads/pytorch/torch/lib -lshm -ltorch_python -o build/lib.linux-x86_64-3.7/torch/_C.cpython-37m-x86_64-linux-gnu.so -Wl,-rpath,$ORIGIN/lib\n \n ```\n \n \n \n To fix my error I ran `python setup.py clean` and then I temporarily renamed Anaconda's `ld` linker to `ld-old` to make it _invisible_ during PyTorch installation. Removing `-B` option from the installation stage could fix it too. \n \n \n \n Probably, there is some incompatibility between my `ld` (version `GNU ld (GNU Binutils) 2.31.1\n \n `) and Anaconda's `ld` (version `GNU ld (crosstool-NG 1.23.0.444-4ea7) 2.31.1\n \n `) that caused the error. Or another explanation is that my system `g++` compiled `.o` object file that is incompatible with Anaconda's `ld` in the subsequent step of linking.\n \n \n \n What about changing `setup.py` to not to allow Anaconda's `ld` overshadow system `ld`?\n \n \n \n Thank you.",
      "y": "Run `python setup.py clean` and then temporarily rename Anaconda's `ld` linker to `ld-old` to make it _invisible_ during PyTorch installation or remove `-B` option from the installation stage."
   },
   {
      "x": "Incorrect size for __rpow__(scalar, float)",
      "z": "I've got a fix ready locally. Could I send in a PR?",
      "y": "Fix issue with scalars and __rpow__ ."
   },
   {
      "x": "[Feature Request] LuaRocks style package manager for community made packages",
      "z": "We have no specific plans to do a separate package manager. Use conda, which allows community channels.",
      "y": "Use conda."
   },
   {
      "x": "Inconsistency: can instantiate Tensor from List, but cannot instantiate Variable",
      "z": "`Variable`s can only be built from an existing `Tensor`. This is expected behavior.",
      "y": "`Variable`s can only be built from an existing `Tensor`."
   },
   {
      "x": "Reductions returning scalars cause implicit sync-point",
      "z": "Pretty pictures of this by the way:\n \n \n \n - in the absence of syncing everything back hostside:\n \n ![gpupipelinemultiple](https://user-images.githubusercontent.com/123560/27936428-9912147e-62a8-11e7-8e1c-9c8851dcb227.png)\n \n \n \n - in the presence of hostside syncpoints:\n \n ![reduceall_pipelinestall](https://user-images.githubusercontent.com/123560/27936437-a15714c2-62a8-11e7-8d7b-256fb3c3aabe.png)",
      "y": "Introduce a scalar type into PyTorch, autograd compatible. With this, the sync-points issue will be solved."
   },
   {
      "x": "[Feature request] Allow exceptions in load_state_dict",
      "z": "How would `ingored_keys` work? A workaround I'd recommend for partial updates is this:\n \n ```python\n \n new_params = model.state_dict()\n \n new_params.update(updated_params)\n \n model.load_state_dict(new_params)\n \n ```",
      "y": "```python\n \n new_params = model.state_dict()\n \n new_params.update(updated_params)\n \n model.load_state_dict(new_params)\n \n ```"
   },
   {
      "x": "Broadcasting doesn't match docs for conda package",
      "z": "indeed :)",
      "y": "Broadcasting will drop in the next release."
   },
   {
      "x": "torch.save is saving too much",
      "z": "This is especially needed because you can save something like: `torch.save([x[:10], x[2:4]])` and then when you load back, the expectation is that the loaded values will share storage (i.e. be two Tensors viewing a larger storage)",
      "y": "You can save something like: `torch.save([x[:10], x[2:4]])` and then when you load back, the expectation is that the loaded values will share storage."
   },
   {
      "x": "gpu version of pytorch not working on docker image",
      "z": "I cant reproduce it myself. One thing that'd help is if you post your docker build log.\n \n By default, the Dockerfile does find CUDA, so this is surprising / weird.",
      "y": "Issue not found with latest version of docker and pytorch."
   },
   {
      "x": "Documentation for how pytorch tensor.view() rearranges dimensions?",
      "z": "`torch.view` only changes the sizes of the tensor, while the underlying content remains the same, pretty much like numpy reshape, so the order is the same as the order in the underlying tensor.\n \n As an example, say we have a tensor `x` of size `5x4x3x2`. Doing `x.view(5, -1)` just rearranges the sizes (you can see by creating a `torch.arange(0, 5 * 4 * 3 * 2)` tensor, view it and view back).\n \n \n \n For the source code, it was moved to `C` for efficiency, but you can find the old python implementation [here](https://github.com/pytorch/pytorch/blob/518864a7e094f32044ef4c0de82c0f20f4ed99c2/torch/tensor.py#L178-L217).\n \n \n \n One more note, if your tensor is of shape `N x C x H x W`, and you want to apply softmax on `C`, you need to transpose it before viewing it. Something like\n \n \n \n ```python\n \n a = torch.rand(N, C, H, W)\n \n a_t = a.permute(0, 2, 3, 1).contiguous().view(-1, C)\n \n # ... perform softmax\n \n # view back\n \n res = res_t.view(N, H, W, C).permute(0, 3, 1, 2)\n \n ```",
      "y": "`torch.view` only changes the sizes of the tensor, while the underlying content remains the same, pretty much like numpy reshape, so the order is the same as the order in the underlying tensor."
   },
   {
      "x": "type error when calling 'adagrad' optimization method",
      "z": "I was encountering the same bug when i define the optmizer before move the model to cuda.\n \n \n \n optimizer = Adagrad(model.parameters()) # TypeERROR\n \n model.cuda() # TypeERROR\n \n \n \n however, change their order is FINE.\n \n \n \n model.cuda() # FINE\n \n optimizer = Adagrad(model.parameters()) # FINE",
      "y": "Interchange the order of model and optimizer definition."
   },
   {
      "x": "cublas runtime error when both bmm's arguments have been expanded",
      "z": "thanks for the bug report, we'll fix this.",
      "y": "Fix baddbmm for expanded tensors."
   },
   {
      "x": "[Feature Request] Add NoisyLinear layer",
      "z": "They've only just appeared in one paper from DM and I'm not totally convinced of the results myself, so \"reasonable adoption\" might take a while yet. On the other hand, noisy linear layers are fairly generic layers and could see use in problems outside of RL.",
      "y": "Noisy linear layers are fairly generic layers and could see use in problems outside of RL."
   },
   {
      "x": "MacOSX, CUDA, OS call failed or operation not supported on this OS",
      "z": "May be related? https://github.com/facebookresearch/deepmask/issues/85. Someone solved it by updating OSX from 10.11 to 10.12",
      "y": "Upgrade to CUDA 10.12."
   },
   {
      "x": "[Feature request] unique operation",
      "z": "Has there been progress on this ? this is an important operation! :)\n \n \n \n My code also looks like yours, but it seems very inefficient in cases where the number of classes in the dataset is 1000 and the minibatch has 1 or 2 classes.",
      "y": "Automatic update of fbcode/onnx."
   },
   {
      "x": "undefined symbol: THLongStorage_inferSizeN",
      "z": "updated.\n \n \n \n It seems like your do not need to re-install anaconda, find `libTH*` in your directory, you may find\n \n ```\n \n /opt/anaconda/lib/python3.6/site-packages/torch/lib/libTH.so.1\n \n /opt/anaconda/lib/python3.6/site-packages/torch/lib/libTHC.so.1\n \n /opt/anaconda/lib/python3.6/site-packages/torch/lib/libTHCS.so.1\n \n /opt/anaconda/lib/python3.6/site-packages/torch/lib/libTHCUNN.so.1\n \n /opt/anaconda/lib/python3.6/site-packages/torch/lib/libTHD.so.1\n \n /opt/anaconda/lib/python3.6/site-packages/torch/lib/libTHNN.so.1\n \n /opt/anaconda/lib/python3.6/site-packages/torch/lib/libTHPP.so.1\n \n /opt/anaconda/lib/python3.6/site-packages/torch/lib/libTHS.so.1\n \n /opt/anaconda/lib/libTH.so.1\n \n /opt/anaconda/lib/libTHNN.so.1\n \n /opt/anaconda/lib/libTHPP.so.1\n \n /opt/anaconda/lib/libTHS.so.1\n \n /opt/anaconda/pkgs/libtorch-0.1.12-0/lib/libTHS.so.1\n \n /opt/anaconda/pkgs/libtorch-0.1.12-0/lib/libTHPP.so.1\n \n /opt/anaconda/pkgs/libtorch-0.1.12-0/lib/libTHNN.so.1\n \n /opt/anaconda/pkgs/libtorch-0.1.12-0/lib/libTH.so.1\n \n /opt/anaconda/pkgs/pytorch-0.2.0-py36h04dd36b_2cu75/lib/python3.6/site-packages/torch/lib/libTHS.so.1\n \n /opt/anaconda/pkgs/pytorch-0.2.0-py36h04dd36b_2cu75/lib/python3.6/site-packages/torch/lib/libTHNN.so.1\n \n /opt/anaconda/pkgs/pytorch-0.2.0-py36h04dd36b_2cu75/lib/python3.6/site-packages/torch/lib/libTHCS.so.1\n \n /opt/anaconda/pkgs/pytorch-0.2.0-py36h04dd36b_2cu75/lib/python3.6/site-packages/torch/lib/libTHD.so.1\n \n /opt/anaconda/pkgs/pytorch-0.2.0-py36h04dd36b_2cu75/lib/python3.6/site-packages/torch/lib/libTHPP.so.1\n \n /opt/anaconda/pkgs/pytorch-0.2.0-py36h04dd36b_2cu75/lib/python3.6/site-packages/torch/lib/libTH.so.1\n \n /opt/anaconda/pkgs/pytorch-0.2.0-py36h04dd36b_2cu75/lib/python3.6/site-packages/torch/lib/libTHCUNN.so.1\n \n /opt/anaconda/pkgs/pytorch-0.2.0-py36h04dd36b_2cu75/lib/python3.6/site-packages/torch/lib/libTHC.so.1\n \n ```\n \n \n \n So, it is ok to uninstall `libtorch` from conda, and everything goes ok\n \n ```\n \n conda uninstall libtorch\n \n ```\n \n \n \n Thanks for updating to the exciting `0.2.0`.",
      "y": "It is ok to uninstall `libtorch` from conda."
   },
   {
      "x": "shared cuda tensor consumes GPU memory in every process",
      "z": "I ran that code in ubuntu 14.04, python 3.5.2.\n \n When I ran that code, main process consumed 327Mb of memory and sub processes consumed 311Mb so I thought that tensor is not properly shared.\n \n However, when I changed the size of the tensor to (1000,1000,200), main process consumed 1837Mb and sub processes consumed same 311Mb.\n \n I don't know why sub processes consumes so much memory but I think my problem is solved:) Thank you!",
      "y": "Change the size of the tensor to orders of (1000,1000,200)."
   },
   {
      "x": "An error occurred when installing pytorch from source",
      "z": "`nvcc` will use the temp directory for intermediate compilation files. The way to avoid this is by setting environment variable `TMPDIR` to some other directory: http://docs.nvidia.com/cuda/cuda-compiler-driver-nvcc/index.htmlcompiler-driver-nvcc/index.html #keeping-intermediate-phase-files",
      "y": "Set environment variable `TMPDIR` to some other directory."
   },
   {
      "x": "Completely deterministic network, but result is different from the past and different on different machines.",
      "z": "This is expected, some of our kernels are not deterministic (specially during backward).\n \n Might be good to refer to https://github.com/pytorch/pytorch/issues/15359\n \n \n \n I'm closing this issue in order to keep the discussion centralized.",
      "y": "Some of our kernels are not deterministic."
   },
   {
      "x": "Padding size should be less than the corresponding input dimension",
      "z": "> The expected behavior should be a warning to let the user decide if they still want to use this padding mode for images.\n \n \n \n Hmm, really? There is no proper output in this case, and you don't expect an error?",
      "y": "A warning to let the user decide if they still want to use this padding mode for images."
   },
   {
      "x": "training with Multi-GPU hangs",
      "z": "Thanks for you reply. I have solved the problem, it's the opened ACS of the PLX PCI-e switch caused the problem, so after i closed the switch, it's ok. Just like [here](https://www.supermicro.org.cn/support/faqs/faq.cfm?faq=20732)",
      "y": "Close ACS of the PLX PCI-e switch."
   },
   {
      "x": "logdet evals to -inf in an overly pessimistic way",
      "z": "lol we posted literally 10min apart. see https://github.com/pytorch/pytorch/issues/18448\n \n \n \n and pr is https://github.com/pytorch/pytorch/pull/18449",
      "y": "Improve numerical precision of (s)logdet."
   },
   {
      "x": "ImportError: cannot import name '_update_worker_pids'",
      "z": "pip install torch==1.0.1 torchvision==0.2.2",
      "y": "Install torch 1.0.1 and torchvision 0.2.2."
   },
   {
      "x": "Wrong BatchNorm2d momentum value in ONNX export",
      "z": "I can repro this on my side. I am taking a look.",
      "y": "Fix momentum setting in BatchNorm forward pass."
   },
   {
      "x": "Error trying to convert to JitScript",
      "z": "Oops - not sure what happened there: https://gist.github.com/emrul/3644b54326794068d0f43e1fce4c1308 (once again, please ignore the poor code - I am trying to incrementally do this work and just figuring out things from the error messages I get)",
      "y": "I just ran your gist on master and did not get an error."
   },
   {
      "x": "[docs] torch.(a)range dtype doc is wrong",
      "z": "https://pytorch.org/docs/master/torch.html#torch.arange already says \n \n \n \n ```\n \n dtype (torch.dtype, optional) the desired data type of returned tensor. Default: if None, uses a global default (see torch.set_default_tensor_type()). If dtype is not given, infer the data type from the other input arguments. If any of start, end, or stop are floating-point, the dtype is inferred to be the default dtype, see get_default_dtype(). Otherwise, the dtype is inferred to be torch.int64.\n \n ```\n \n \n \n #18604 fixes range though",
      "y": "Fixed range."
   },
   {
      "x": "torch.svd seems to have accuracy problem",
      "z": "You have a bug at https://github.com/wangg12/IRLS_tensorflow2/blob/721675bb9dd0ba19223e10b346aa639a159a58da/src/IRLS_pytorch.py#L111\n \n \n \n `y` has shape `torch.Size([32561, 1])` yet `y_pred` has shape `torch.Size([32561])`.\n \n \n \n For the record, you don't need to add explicit `expand` to broadcast everywhere, pytorch does automatic broadcasting, just like numpy.\n \n \n \n I fixed that, and use the same `pinv` method as you did in your `tf` script, removed every discrepancy between pt and tf scripts (e.g., l2 reg), and got the same accuracy.\n \n \n \n Here is the script: https://gist.github.com/SsnL/acd7ae2e096b01daab0fc7621dd47bc3\n \n \n \n Here is the output:\n \n \n \n ```\n \n Random Seed: 2196\n \n start training...\n \n L2 param(lambda): 20\n \n iter: 0\n \n  neg log likelihood: 23909.580078125\n \n  train acc: 0.24080955982208252, test acc: 0.23622629046440125\n \n  L2 norm of w: 0.11135528236627579\n \n iter: 1\n \n  neg log likelihood: 12469.365234375\n \n  train acc: 0.8444765210151672, test acc: 0.8457711935043335\n \n  L2 norm of w: 2.1622300148010254\n \n  diff of w_old and w: 2.1692447662353516\n \n iter: 2\n \n  neg log likelihood: 11109.1064453125\n \n  train acc: 0.8469641804695129, test acc: 0.8494564294815063\n \n  L2 norm of w: 3.1962523460388184\n \n  diff of w_old and w: 1.1507797241210938\n \n iter: 3\n \n  neg log likelihood: 10792.7138671875\n \n  train acc: 0.8475169539451599, test acc: 0.8501321077346802\n \n  L2 norm of w: 3.856876850128174\n \n  diff of w_old and w: 0.8543247580528259\n \n iter: 4\n \n  neg log likelihood: 10746.015625\n \n  train acc: 0.8479469418525696, test acc: 0.8507463335990906\n \n  L2 norm of w: 4.167870044708252\n \n  diff of w_old and w: 0.49175599217414856\n \n iter: 5\n \n  neg log likelihood: 10743.853515625\n \n  train acc: 0.8479162454605103, test acc: 0.8506848812103271\n \n  L2 norm of w: 4.240683078765869\n \n  diff of w_old and w: 0.1431959867477417\n \n iter: 6\n \n  neg log likelihood: 10743.8466796875\n \n  train acc: 0.8479162454605103, test acc: 0.8507463335990906\n \n  L2 norm of w: 4.244931221008301\n \n  diff of w_old and w: 0.01007823646068573\n \n iter: 7\n \n  neg log likelihood: 10743.84375\n \n  train acc: 0.8479162454605103, test acc: 0.8507463335990906\n \n  L2 norm of w: 4.244948387145996\n \n  diff of w_old and w: 4.568263830151409e-05\n \n training done.\n \n \n \n ```",
      "y": "No need to add explicit `expand` to broadcast everywhere, pytorch does automatic broadcasting, just like numpy."
   },
   {
      "x": "grid_sample cuDNN error",
      "z": "@acheketa Thanks, now it works",
      "y": "https://github.com/pytorch/pytorch/issues/18561#issuecomment-477906432"
   },
   {
      "x": "Checkpointing evaluates irrelevant tasks",
      "z": "This now has the correct behavior in master.\n \n Please re-open if you see more issues.",
      "y": "Documentation and warning about it would be a better choice."
   },
   {
      "x": "[FR] warn/error when MAGMA is built with a different CUDA",
      "z": "Related to https://github.com/pytorch/pytorch/issues/3990.",
      "y": "Raise an error when using magma built against wrong version of cuda."
   },
   {
      "x": "c++ error:\"std\":ambiguous symbol it is in visual studio 2017.",
      "z": "I tested on different configurations and found that problem can be solved by change the `conformance mode` property to **`No`**. You can find that option in `C/C++->Language`. That's a new feature in vs2017 and 2019.",
      "y": "change the `conformance mode` property to **`No`**."
   },
   {
      "x": "Fail in repeated evaluation of 2nd derivative of a custom autograd function",
      "z": "Fixed by https://github.com/pytorch/pytorch/pull/22983",
      "y": "Invert ownership between PyFunction and THPFunction."
   },
   {
      "x": "torch.matmul not working in cuda 9.0",
      "z": "cuda 9 doesn't support RTX cards afaik",
      "y": "Cuda 9 doesn't support NVidia RTX cards."
   },
   {
      "x": "torch.matrix_exp() doc is missing a signature",
      "z": "Hey, I raise a PR and hope it can solve this issue",
      "y": "Invert ownership between PyFunction and THPFunction."
   },
   {
      "x": "torch.linalg.eigh fails on gradcheck with complex Hermitian matrix",
      "z": "I think what is causing troubles here is that the sign or phase of eigenvectors is not unique, so is the choice for normalization of eigenvectors.\n \n The following `fcn` function passes the test\n \n ```python\n \n import torch\n \n mat = torch.randn((2, 2), dtype=torch.complex128).requires_grad_()\n \n def fcn(mat):\n \n  mat = (mat + mat.transpose(-2, -1).conj()) * 0.5\n \n  result = torch.linalg.eigh(mat)\n \n  return result[0], abs(result[1])\n \n \n \n torch.autograd.gradcheck(fcn, mat)\n \n ```",
      "y": "The sign or phase of eigenvectors is not unique, so is the choice for normalization of eigenvectors."
   },
   {
      "x": "Request for a test code snippet of Distributed Data Parallel.",
      "z": "this is the API document https://pytorch.org/docs/master/generated/torch.nn.parallel.DistributedDataParallel.html",
      "y": "Here is the API document - https://pytorch.org/docs/master/generated/torch.nn.parallel.DistributedDataParallel.html"
   },
   {
      "x": "Preserve PyObject even when it is dead from Python side",
      "z": "Yes, so the PyObject reference Tensor has to be made non-owning. We can use @swolchok's MaybeOwned class to conveniently do this.",
      "y": "PyObject reference Tensor has to be made non-owning."
   },
   {
      "x": "PYTORCH_TESTING_DEVICE_ONLY_FOR environment variable don't play well with development environment.",
      "z": "Are PYTORCH_TESTING_DEVICE_ONLY_FOR and PYTORCH_TESTING_DEVICE_EXCEPT_FOR documented somewhere?\n \n \n \n Is the idea that if the environment variable PYTORCH_TESTING_DEVICE_ONLY_FOR is set it contains one or more device types, and only device generic tests for those device types are run? If so, I would expect it to intersect its set with the available device types.\n \n \n \n Similarly, if PYTORCH_TESTING_DEVICE_EXCEPT_FOR is designed to eliminate one or more device types then I would expect it would remove the test bases for those device types.",
      "y": "If the environment variable PYTORCH_TESTING_DEVICE_ONLY_FOR is set it contains one or more device types, and only device generic tests for those device types are run."
   },
   {
      "x": "Error in optim/adamw.py",
      "z": "I confirm that `pytorch-1.8.1` doesn't have this fix included. And getting the same problem.",
      "y": "`pytorch-1.8.1` doesn't have this fix."
   },
   {
      "x": "_ConvNd weight initialization does not match docs",
      "z": "The docstring is correct in this case. For example:\n \n \n \n ```python\n \n import math\n \n import matplotlib.pyplot as plt\n \n \n \n import torch\n \n import torch.nn as nn\n \n \n \n in_channels = 120\n \n groups = 2\n \n kernel = (3, 8)\n \n m = nn.Conv2d(in_channels=in_channels, groups=groups,\n \n  out_channels=100, kernel_size=kernel)\n \n \n \n k = math.sqrt(groups / (in_channels * math.prod(kernel)))\n \n print(f\"k: {k:0.6f}\")\n \n \n \n print(f\"min weight: {m.weight.min().item():0.6f}\")\n \n print(f\"max weight: {m.weight.max().item():0.6f}\")\n \n ```\n \n \n \n outputs:\n \n ```\n \n k: 0.026352\n \n min weight: -0.026352\n \n max weight: 0.026352\n \n ```\n \n \n \n And when we plot the distribution, it is uniform with the correct bounds:\n \n \n \n ```python\n \n _ = plt.hist(m.weight.detach().numpy().ravel())\n \n ```\n \n \n \n ![Unknown](https://user-images.githubusercontent.com/5402633/119552979-21ba3800-bd69-11eb-8e10-e067c943abe3.png)\n \n \n \n \n \n I opened #58931 that adds a code comment to avoid future confusion.",
      "y": "Docstring is correct in this case."
   },
   {
      "x": "[ROCm] Tests fail on my system.",
      "z": "[here you go](https://gist.githubusercontent.com/Wulfsta/5d8b2b902e1068433aaf737657453a25/raw/18028b37ea909905d23b60805399df4ea0c6172b/gistfile1.txt). \n \n \n \n Edit: I obviously haven't parsed this whole file, but the problems appear to be limited to gradients and it looks like a lot of equality checks on NaNs are happening?",
      "y": "Problems appear to be limited to gradients and it looks like a lot of equality checks on NaNs are happening."
   },
   {
      "x": "JIT: torch.jit.Future annotations cannot be spread over multiple lines",
      "z": "This is now fixed in #56148. Now closing. If the issue still exists, please feel free to re-open the issue.",
      "y": "Add support for refinement for torch.jit.Future"
   },
   {
      "x": "\"Add annotations\" workflow fails when base branch is pinned to old commit",
      "z": "> Those are good pros and cons. Do you have a recommendation?\n \n \n \n @mruberry I plan to implement approach (2) soon.",
      "y": "Using a general strategy of expecting malformed input will prevent these sorts of failures from happening when future changes to these workflows are made."
   },
   {
      "x": "\"Build\" step in CircleCI outputs a ton of pthreadpool warnings",
      "z": "It's a duplicate of #33760, also I noticed it was only happening with certain flags on (i.e. in the logs above I had changed the `CFLAGS` to include `-gsplit-dwarf`.)",
      "y": "It is only happening with certain flags."
   },
   {
      "x": "Unable to build PyTorch without Numpy installed (again)",
      "z": "That's a caffe2 bug :-) Workaround is `BUILD_CAFFE2=0 BUILD_CAFFE2_OPS=0`. One day, it will become default :-)",
      "y": "`BUILD_CAFFE2=0 BUILD_CAFFE2_OPS=0`."
   },
   {
      "x": "[bug] run test local",
      "z": "Oh thanks. Didn't notice that!",
      "y": "Allow tests to run locally without setting environment variables."
   },
   {
      "x": "[doc] is_grad_enabled is not documented",
      "z": "Duplicate of https://github.com/pytorch/pytorch/issues/4474?",
      "y": "`torch.is_grad_enabled` isn't searchable."
   },
   {
      "x": "Print Bug of Tensor Grad After Backward",
      "z": "Unfortunately, this is expected. The multiplication `x[0, :, 0, 0] * var` saves `x[0, :, 0, 0]` for backwards, but then you mutate it and we are then unable to use it for backwards. The inplace formula doesn't have this problem because it knows the clobber is coming and does something special. A simple fix is `x[0, :, 0, 0] = x[0, :, 0, 0].clone() * var`. Could you tell us a little more about what you're trying to do?",
      "y": "Multiplication `x[0, :, 0, 0] * var` saves `x[0, :, 0, 0]` for backwards, but then you mutate it and we are then unable to use it for backwards."
   },
   {
      "x": "Add Kaiming initialization functions to C++ API",
      "z": "Hey @JoshVarty, thanks for the idea. I'd love it if you tried implementing this! It indeed sounds like a nicely contained, manageable task for a first PR. It shouldn't be too hard to port the Python code to C++ since our C++ tensor API is almost 1:1 the same.\n \n \n \n For the enum question, I'd much prefer using C++ enums over strings. You can create an `enum class Mode` in the `torch::nn::init` namespace with the appropriate members.\n \n \n \n I'd say, just give it a shot, and I'll give you supportive feedback once you have a minimum viable PR ready.\n \n \n \n And yes, you'd have to implement `calculate_gain` too.",
      "y": "It shouldn't be too hard to port the Python code to C++ since our C++ tensor API is almost 1:1 the same."
   },
   {
      "x": "torch.nn.Conv2d will try to allocate gpu memory far more than it needs.",
      "z": "Well cudnn automatically select the fastest possible algorithm (as long as enough memory is available).\n \n You can try and set `torch.backends.cudnn.deterministic=True` just before doing the forward on the problematic kernel. That will force it to use the default deterministic algorithm and hopefully use less memory.",
      "y": "Cudnn automatically select the fastest possible algorithm. Set `torch.backends.cudnn.deterministic=True` just before doing the forward on the problematic kernel."
   },
   {
      "x": "Mean and max slow",
      "z": "`mean` goes through recently added reduction kernels in TensorIterator. It is not without problems, as we've discussed in #12115, but it is fast. `max` uses older kernels in THC. At some point all THC reductions should be moved to TensorIterator reductions, but this future has not arrived yet (PRs welcome).",
      "y": "`mean` goes through recently added reduction kernels in TensorIterator and is fast. `max` uses older kernels in THC."
   },
   {
      "x": "Build libtorch with -D_GLIBCXX_USE_CXX11_ABI=1",
      "z": "Initially I want to use trained model from pytorch to detect hand gesture in C++. My original problem is that I can cmake the code calling either libtorch or OpenCV library successfully, but I cannot make it when I try to cmake the code calling both.\n \n I have solved the similar problems by changing to a lower OpenCV version. Here are some details.\n \n \n \n Original problem:\n \n \n \n CMakeFiles/ LibtorchTest.dir/main . CPP. O: in the function 'main':\n \n /home/gao/CLionProjects/LibtorchTest/ main.cpp:14 : undefined reference to 'CV:: imread (STD:: String const & int)'\n \n /home/gao/CLionProjects/LibtorchTest/ main.cpp:15 : for 'CV:: imshow (STD:: String const &, CV::_ Undefined reference in inputarray const &) '\n \n collect2: error: ld returned 1 exit status\n \n CMakeFiles/ LibtorchTest.dir/build . make:133 : recipe for target 'LibtorchTest' failed\n \n make[3]: *** [LibtorchTest] Error 1\n \n CMakeFiles/Makefile2:95: recipe for target 'CMakeFiles/ LibtorchTest.dir/all ' failed\n \n make[2]: *** [CMakeFiles/ LibtorchTest.dir/all ] Error 2\n \n CMakeFiles/Makefile2:102: recipe for target 'CMakeFiles/ LibtorchTest.dir/rule ' failed\n \n make[1]: *** [CMakeFiles/ LibtorchTest.dir/rule ] Error 2\n \n Makefile:138 : recipe for target 'LibtorchTest' failed\n \n make: *** [LibtorchTest] Error 2\n \n \n \n Problem solved:\n \n \n \n -- Caffe2: CUDA detected: 10.2\n \n -- Caffe2: CUDA nvcc is: /usr/local/cuda/bin/nvcc\n \n -- Caffe2: CUDA toolkit directory: /usr/local/cuda\n \n -- Caffe2: Header version is: 10.2\n \n -- Found cuDNN: v8.0.3 (include: /usr/include, library: /usr/lib/x86_64-linux-gnu/libcudnn.so)\n \n -- Autodetected CUDA architecture(s): 6.1\n \n -- Added CUDA NVCC flags for: -gencode;arch=compute_61,code=sm_61\n \n -- Torch library status:\n \n -- libraries: \n \n -- Found OpenCV: /home/gao/opencv-3.4.12/build (found version \"3.4.12\") \n \n -- OpenCV library status:\n \n -- version: 3.4.12\n \n -- libraries: opencv_calib3d;opencv_core;opencv_dnn;opencv_features2d;opencv_flann;opencv_highgui;opencv_imgcodecs;opencv_imgproc;opencv_ml;opencv_objdetect;opencv_photo;opencv_shape;opencv_stitching;opencv_superres;opencv_video;opencv_videoio;opencv_videostab;opencv_viz\n \n -- include path: ...too long, so I omit it)\n \n -- Configuring done\n \n -- Generating done\n \n -- Build files have been written to: /home/gao/CLionProjects/LibtorchTest/cmake-build-debug\n \n Scanning dependencies of target LibtorchTest\n \n [ 50%] Building CXX object CMakeFiles/LibtorchTest.dir/main.cpp.o\n \n [100%] Linking CXX executable LibtorchTest\n \n [100%] Built target LibtorchTest\n \n \n \n Hope my solved problem may help.",
      "y": "Change to a lower OpenCV version."
   },
   {
      "x": "load_lua() does not know how to deserialize Lua class nn.PixelShuffle.",
      "z": "if you want to load the weights from a lua torch .t7 file into python, you can use https://github.com/bshillingford/python-torchfile which should still be working afaik.",
      "y": "Use \"https://github.com/bshillingford/python-torchfile\" ."
   },
   {
      "x": "Inconsistent behavior for log_prob when values are outside of support",
      "z": "Hi @chentc777,\n \n \n \n You can consider setting `validate_args` to `True` for the distributions individually, or by setting a global flag using:\n \n ```\n \n torch.distributions.Distribution.set_default_validate_args(True)\n \n ```\n \n \n \n This will raise a `ValueError` if the values are outside the support.\n \n \n \n ```python\n \n >>> torch.distributions.Distribution.set_default_validate_args(False)\n \n >>> Beta(torch.tensor(1.),torch.tensor(1.)).log_prob(torch.tensor(5.))\n \n tensor(nan)\n \n >>> Exponential(torch.tensor(2.)).log_prob(torch.tensor(-5.))\n \n tensor(11.6094)\n \n >>> Gamma(torch.tensor(1.),torch.tensor(1.)).log_prob(torch.tensor(-1.))\n \n tensor(nan)\n \n >>> Geometric(torch.tensor(5.)).log_prob(torch.tensor(-1.))\n \n tensor(nan)\n \n ```\n \n \n \n ```python\n \n >>> torch.distributions.Distribution.set_default_validate_args(True)\n \n >>> Beta(torch.tensor(1.),torch.tensor(1.)).log_prob(torch.tensor(5.))\n \n ValueError: The value argument must be within the support\n \n >>> Exponential(torch.tensor(2.)).log_prob(torch.tensor(-5.))\n \n ValueError: The value argument must be within the support\n \n >>> Gamma(torch.tensor(1.),torch.tensor(1.)).log_prob(torch.tensor(-1.))\n \n ValueError: The value argument must be within the support\n \n >>> Geometric(torch.tensor(5.)).log_prob(torch.tensor(-1.))\n \n ValueError: The parameter probs has invalid values\n \n ```",
      "y": "Set `validate_args` to `True` for the distributions individually or by setting a global flag."
   },
   {
      "x": "Deepcopy doesn't copy to same gpu",
      "z": "I ran into the same problem when I tried to deepcopy a module.\n \n I'm now working around by adding a `.to(device=desired_device)` after deepcopy",
      "y": "Add a `.to(device=desired_device)` after deepcopy."
   },
   {
      "x": "test_distributed - failed gloo test_all_reduce_multigpu and test_barrier_group_cuda on 8-GPU machines",
      "z": "The barrier one is likely because of excessive initialization time when using CUDA, triggering the native timeout. Bumping that a bit should solve it. Looking into the allreduce multi GPU one.",
      "y": "Likely because of excessive initialization time when using CUDA, triggering the native timeout. Bumping that a bit should solve it."
   },
   {
      "x": "libtorch c++API: terminate called after throwing an instance of 'c10::Error'",
      "z": "> > > I have the same problem, too. I upgraded the package as you offered(https://drive.google.com/open?id=13qS1gup6E7nJ-l8Ahf4YUsA9gGGVzKWE ), but the bug still appear. What is wrong with this ?\n \n > > \n \n > > \n \n > > Maybe u should use pytorch-dev11.28 to trace the model. And make sure you have cmake and make your project again? I solve the problem by use libtorch 11.28 under cuda8 and cudnnV6.\n \n > \n \n > How to degrade to pytorch-dev11.28?\n \n > I am using libtorch downloaded on 11.23, but I still met this error\n \n \n \n u can find the 11.28 version package in conda repository(https://anaconda.org/pytorch/pytorch-nightly/files).",
      "y": "Use pytorch-dev11.28 to trace the model."
   },
   {
      "x": "[libtorch] build failed with opencv-4.0.0 using cmake (CPU and GPU)",
      "z": "Problem resolved, related issue is here: #14620",
      "y": "Export model.pt and build libtorch from the same version of Pytorch."
   },
   {
      "x": "Large documentation pages take a long time to load",
      "z": "This was fixed in the doc restructure in gh-37419. https://pytorch.org/docs/master/nn.html loads fast now. So closing this.",
      "y": "Fixed in the doc restructure in gh-37419."
   },
   {
      "x": "Comply with XDG (X Design Group) Base Directory Specification",
      "z": "Cheers! I found the specific commit here:\n \n \n \n https://github.com/pytorch/pytorch/pull/18758/commits/d079a6fbcafa59b419b3c6355a3b8a01d44a8b47",
      "y": "Comply with XDG_CACHE_HOME and keep TORCH_HOME ."
   },
   {
      "x": "tensor() and Tensor() return different values for a list of bools",
      "z": "Thank you for reporting this issue, @CamiloHernandez. We encourage people to use `torch.tensor` and not `torch.Tensor`, but I think we would take a PR fixing this behavior, anyway, since it's so clearly wrong.",
      "y": "Use `torch.tensor` and not `torch.Tensor` ."
   },
   {
      "x": "Which torch version is this?",
      "z": "Turns out need to export PYTORCH_BUILD_VERSION=1.4.0 PYTORCH_BUILD_NUMBER=1 for the version to be reported correctly.\n \n https://github.com/pytorch/pytorch/issues/9926\n \n This should be better documented.",
      "y": "Export PYTORCH_BUILD_VERSION=1.4.0 PYTORCH_BUILD_NUMBER=1 for the version to be reported correctly."
   },
   {
      "x": "Windows nightly build failed (CUDA + DEBUG + LIBTORCH)",
      "z": "No, it has nothing to do with magma. The warning is just about debug info since I didn't put `magma.pdb` in the package.",
      "y": "Calling benchmark.Timer with default num_threads=1 disables parallelism."
   },
   {
      "x": "Libtorch : Comparing identical shapes(sizes) always returns false when in debug mode",
      "z": "I couldnt replicate this issue in a new project. even the very same project that exibits this issue, doesnt behave incorrectly when copied and executed from another location in the same system. This makes me come to the conclusion that this might be very well a Visual Studio 2019 issue of some kind! Despite my attempts, I couldnt find out the cause so I close this issue and will reopen it in case I find something new. \n \n Thanks a lot for your generous time and kind help.",
      "y": "A Visual Studio 2019 issue."
   },
   {
      "x": "Difference behavior between FrozenBatchNorm2d and BatchNorm2d",
      "z": "they have different eps https://github.com/pytorch/vision/blob/fc69c22576cbccb59c581ddbaca4dedbdb279688/torchvision/ops/misc.py#L54",
      "y": "They have different eps."
   },
   {
      "x": "JIT changes method kwarg argument names",
      "z": "I recently fixed argument name capturing in tracing and I verified the repro provided and it is no longer throwing exception. Closing.",
      "y": "Fix argument name capturing in tracing."
   },
   {
      "x": "Add a torch.hub.load_local() function that can load models from any local directory with a hubconf.py",
      "z": "This was considered a lower pri in our plan - we'd be happy to take a PR implementing this functionality.",
      "y": "Allow torch.hub.load() to load models from any local directory with a hubconf.py."
   },
   {
      "x": "[jit] determine whether ModuleList is empty or its length",
      "z": "Hi, thanks for the issue and good repro! Hmm, I thought this was fixed for 1.6. It is fixed on master, though, so closing.\n \n ```\n \n model_jit = torch.jit.script(Test())\n \n print(model_jit.forward.code)\n \n \n \n def forward(self,\n \n  x: Tensor) -> Tensor:\n \n  if torch.gt((self.layers).__len__(), 0):\n \n  x1 = (getattr(self.layers, \"0\")).forward(x, )\n \n  x0 = x1\n \n  else:\n \n  x0 = x\n \n  return x0\n \n ```",
      "y": "```\n \n model_jit = torch.jit.script(Test())\n \n print(model_jit.forward.code)\n \n \n \n def forward(self,\n \n  x: Tensor) -> Tensor:\n \n  if torch.gt((self.layers).__len__(), 0):\n \n  x1 = (getattr(self.layers, \"0\")).forward(x, )\n \n  x0 = x1\n \n  else:\n \n  x0 = x\n \n  return x0\n \n ```"
   },
   {
      "x": "Make torch.nn.Hardswish ONNX exportable",
      "z": "You can get around this by using \n \n ```\n \n class Hardswish(nn.Module): # export-friendly version of nn.Hardswish()\n \n  @staticmethod\n \n  def forward(x):\n \n  # return x * F.hardsigmoid(x) # for torchscript and CoreML\n \n  return x * F.hardtanh(x + 3, 0., 6.) / 6. # for torchscript, CoreML and ONNX\n \n ```\n \n But pytorch told me to open a bug so here it is",
      "y": "```\n \n class Hardswish(nn.Module): # export-friendly version of nn.Hardswish()\n \n  @staticmethod\n \n  def forward(x):\n \n  # return x * F.hardsigmoid(x) # for torchscript and CoreML\n \n  return x * F.hardtanh(x + 3, 0., 6.) / 6. # for torchscript, CoreML and ONNX\n \n ```"
   },
   {
      "x": "torch\\include\\ATen/core/ivalue_inl.h... cl.exe' failed with exit status 2",
      "z": "@shihlun1208 can you please post the actual build logs rather than screenshots? Also, what version of pytorch you are trying to build? \n \n And is there a reason why you can't upgrade to a more recent version of VC++?",
      "y": "VS2019 is not supported with cuda 10."
   },
   {
      "x": "torch.empty_like supports memory_format=torch.preserve_format",
      "z": "Seems reasonable to me, I would just open a PR @vfdev-5.",
      "y": "Change the order of `TORCH_CHECK` and `if (options.layout() == kSparse && self.is_sparse())`"
   },
   {
      "x": "embedding_bag running on CPU produces wrong result when weight tensor is non-contiguous.",
      "z": "Mark high priority, cpu has a wrong behavior, this should be treated as a regression.",
      "y": "Let ModuleTest raise when they fail on non-contiguous inputs. Fix legacy modules.\n Fix BN (both THNN and cuDNN) not working on non-contiguous inputs.\n Fix CUDA EmbeddingBag not working on non-contiguous inputs. To prevent calling .contiguous() on in both forward and backward,\n a. prefix all current embedding_bag* functions with _, indicating that they require input to be contiguous (there is a check in each function).\n b. create embedding_bag, which makes input arguments .contiguous(), and calls _embedding_bag\n Make many ATen embedding* functions to work on non-contiguous inputs so we don't need to call input = input.contiguous() in Python nn.functional.embedding.\n Fix dense-sparse addition when the sparse input is not coalesced and indices or values tensor is not contiguous. This came up in the test cases of Embedding modules with sparse=True. Added tests.\n Update TensorUtils.cpp to use AT_* macros."
   },
   {
      "x": "torch.jit bug torch.cat() not working properly",
      "z": "> Hm, interesting, I picked one of the branches that had \"release\" and \"1.6\" in its name, maybe it was still a wrong one. Is it reproducible with more recent releases or master?\n \n \n \n No. Master build and nightly are fine. I tried 1.7 and it is fine too.",
      "y": "There's been a lot of changes in fusers/executors since 1.6, so it probably is no longer an issue."
   },
   {
      "x": "Make non-literal indexing for ModuleList / ModuleDict work with ModuleInterface type in TorchScript",
      "z": "This is one of the most common issues I face when trying to convert models to TorchScript. And i believe a quick temporary fix is possible. Simply iterating over every list with IF check to bypass unwanted iterations almost always works as a fix but is terribly inefficient.",
      "y": "Iterating over every list with IF check to bypass unwanted iterations almost always works as a fix but is terribly inefficient."
   },
   {
      "x": "nightly.py error",
      "z": "a fix is in at https://github.com/pytorch/pytorch/pull/43771",
      "y": "Nightly robustness fixes for linking across devices"
   },
   {
      "x": "[ERROR] PyTorch dependency on Windows Python Package not working",
      "z": "Well, the underlying problem is that the Windows packages are not hosted on PYPI. So instead of `pip install -r requirements.txt`, you may need to use `pip install -r requirements.txt -f https://download.pytorch.org/whl/torch_stable.html`.",
      "y": "Use `pip install -r requirements.txt -f https://download.pytorch.org/whl/torch_stable.html`."
   },
   {
      "x": "RuntimeError(\"{} is a zip archive (did you mean to use torch.jit.load()?)\".format(f.name)) when loading model weights",
      "z": "Hi feiyangsuo,\n \n \n \n You are right that it is indeed a zip file. See [PyTorch 1.6 release notes](https://github.com/pytorch/pytorch/releases/tag/v1.6.0) about the change (search for \"zip\" in the linked webpage). I would suggest upgrading to 1.6 and the issue should be gone. If that's not the case, please update the bug and we will investigate. Or if you have reasons to not upgrade to 1.6, let us know and we will see what we can do.\n \n \n \n Thanks,",
      "y": "Upgrade to 1.6."
   },
   {
      "x": "Fix exception chaining all over the codebase",
      "z": "Thank you for your quick response.\n \n \n \n Sure, I'll remove `raise_from()` and use `raise .. from ..` instead all over the codebase.\n \n \n \n > Although not all instances require exception chaining, for example implicit chaining is totally fine in cases that you've mentioned in the issue description:\n \n > https://github.com/pytorch/pytorch/blob/c7787f7fbf388f9fe326d581726e7ef3f2dce74b/torch/jit/annotations.py#L32-L35\n \n \n \n I agree that not all instances require exception chaining. However, I think we should not use implicit chaining in the above case because its error message `During handling of the above exception, another exception occurred` is confusing.\n \n \n \n For example, if we run the script below (`test.py`):\n \n ```python\n \n d = {\"dog\": 1}\n \n key = \"cat\"\n \n try: \n \n  print(d[key]) # raises KeyError\n \n except KeyError:\n \n  raise RuntimeError(\"no member called {}\".format(key))\n \n ```\n \n it'll give us an error message like this:\n \n ```\n \n Traceback (most recent call last):\n \n  File \"/home/nitta/tmp/test.py\", line 4, in <module>\n \n  print(d[key]) # raises KeyError\n \n KeyError: 'cat'\n \n \n \n During handling of the above exception, another exception occurred:\n \n \n \n Traceback (most recent call last):\n \n  File \"/home/nitta/tmp/test.py\", line 6, in <module>\n \n  raise RuntimeError(\"no member called {}\".format(key))\n \n RuntimeError: no member called cat\n \n ```\n \n \n \n `During handling of the above exception, another exception occurred` is somewhat confusing when debugging because the truth is that another exception occurred to explain the first exception (`KeyError` in this case) in a friendlier way, not because another **unexpected** exception occured while handling the first exception.\n \n \n \n In order to fix confusing messages like this, one option is:\n \n ```python\n \n try: \n \n  print(d[key]) # raises KeyError\n \n except KeyError as e:\n \n  raise RuntimeError(\"no member called {}\".format(key)) from e\n \n ```\n \n which outputs\n \n ```\n \n Traceback (most recent call last):\n \n  File \"/home/nitta/tmp/test.py\", line 4, in <module>\n \n  print(d[key]) # raises KeyError\n \n KeyError: 'cat'\n \n \n \n The above exception was the direct cause of the following exception:\n \n \n \n Traceback (most recent call last):\n \n  File \"/home/nitta/tmp/test.py\", line 6, in <module>\n \n  raise RuntimeError(\"no member called {}\".format(key)) from e\n \n RuntimeError: no member called cat\n \n ```\n \n \n \n However, in this case, the above message is a bit noisy and redundant.\n \n If we want to suppress the first error message, we should use `raise .. from None` so as to suppress the first exception message (see [Suppress Context in PEP 3134](https://www.python.org/dev/peps/pep-3134/#open-issue-suppressing-context)).\n \n ```python\n \n try: \n \n  print(d[key]) # raises KeyError\n \n except KeyError:\n \n  raise RuntimeError(\"no member called {}\".format(key)) from None\n \n ```\n \n which outputs\n \n ```\n \n Traceback (most recent call last):\n \n  File \"/home/nitta/tmp/test.py\", line 6, in <module>\n \n  raise RuntimeError(\"no member called {}\".format(key)) from None\n \n RuntimeError: no member called cat\n \n ```\n \n \n \n Do you think using `raise .. from None` is reasonable in the below case or simliar cases?\n \n https://github.com/pytorch/pytorch/blob/c7787f7fbf388f9fe326d581726e7ef3f2dce74b/torch/jit/annotations.py#L32-L35",
      "y": "``` try: \n  print(d[key]) # raises KeyError\n except KeyError as e:\n  raise RuntimeError(\"no member called {}\".format(key)) from e\n ```"
   },
   {
      "x": "PyTorch's div, which performs true division, cannot be exported to ONNX with consistent semantics",
      "z": "Sure thing: draft is https://github.com/pytorch/pytorch/pull/42907.",
      "y": "Throw a runtime error if a user tries to use div to perform integer division in 1.6"
   },
   {
      "x": "Segmentation fault in DataLoader worker in PyTorch 1.8.0 if set_num_threads is called beforehand",
      "z": "I managed to obtain a backtrace of the segfault following the instructions in #53894. It seems to happen in `set_num_threads`. I'm guessing that `pthreadpool_destroy` is being called on a thread pool that no longer exists after forking... but if that's the case why does it not crash when the initial `set_num_threads` is called with a low number? Anyway, I hope this helps narrow down the problem.\n \n \n \n ```\n \n #0 0x00007ffff7fa1aab in __pthread_clockjoin_ex (threadid=140735666181888, thread_return=0x0, clockid=0, abstime=0x0, block=true) at pthread_join_common.c:89\n \n  pd = 0x7fff9363d700\n \n  self = <optimized out>\n \n  result = <optimized out>\n \n  pd_result = <optimized out>\n \n #1 0x00007fffe70b88db in pthreadpool_destroy () from /home/ubuntu/.local/share/virtualenvs/test-2nMYMFG7/lib/python3.7/site-packages/torch/lib/libtorch_cpu.so\n \n No symbol table info available.\n \n #2 0x00007fffe4cc6a07 in caffe2::PThreadPool::set_thread_count(unsigned long) () from /home/ubuntu/.local/share/virtualenvs/test-2nMYMFG7/lib/python3.7/site-packages/torch/lib/libtorch_cpu.so\n \n No symbol table info available.\n \n #3 0x00007fffe38dacbf in at::set_num_threads(int) () from /home/ubuntu/.local/share/virtualenvs/test-2nMYMFG7/lib/python3.7/site-packages/torch/lib/libtorch_cpu.so\n \n No symbol table info available.\n \n #4 0x00007ffff571a9d6 in THPModule_setNumThreads(_object*, _object*) () from /home/ubuntu/.local/share/virtualenvs/test-2nMYMFG7/lib/python3.7/site-packages/torch/lib/libtorch_python.so\n \n No symbol table info available.\n \n #5 0x000055555566e803 in _PyCFunction_FastCallKeywords ()\n \n No symbol table info available.\n \n #6 0x0000555555703ed4 in ?? ()\n \n No symbol table info available.\n \n #7 0x0000555555700fe2 in _PyEval_EvalFrameDefault ()\n \n No symbol table info available.\n \n #8 0x00005555556716ba in _PyFunction_FastCallDict ()\n \n No symbol table info available.\n \n #9 0x00005555556fe0fd in _PyEval_EvalFrameDefault ()\n \n No symbol table info available.\n \n #10 0x0000555555670a0a in _PyFunction_FastCallKeywords ()\n \n No symbol table info available.\n \n #11 0x0000555555703dcb in ?? ()\n \n No symbol table info available.\n \n #12 0x00005555556fcc78 in _PyEval_EvalFrameDefault ()\n \n No symbol table info available.\n \n #13 0x0000555555670a0a in _PyFunction_FastCallKeywords ()\n \n No symbol table info available.\n \n #14 0x0000555555703dcb in ?? ()\n \n No symbol table info available.\n \n #15 0x00005555556fcc78 in _PyEval_EvalFrameDefault ()\n \n No symbol table info available.\n \n #16 0x0000555555670a0a in _PyFunction_FastCallKeywords ()\n \n No symbol table info available.\n \n #17 0x0000555555703dcb in ?? ()\n \n No symbol table info available.\n \n #18 0x00005555556fcc78 in _PyEval_EvalFrameDefault ()\n \n No symbol table info available.\n \n #19 0x0000555555671cfa in _PyObject_Call_Prepend ()\n \n No symbol table info available.\n \n #20 0x00005555556c6d1c in ?? ()\n \n No symbol table info available.\n \n #21 0x00005555556c2f19 in ?? ()\n \n No symbol table info available.\n \n #22 0x000055555566fd65 in _PyObject_FastCallKeywords ()\n \n No symbol table info available.\n \n #23 0x0000555555703f51 in ?? ()\n \n No symbol table info available.\n \n #24 0x00005555556fcbe8 in _PyEval_EvalFrameDefault ()\n \n No symbol table info available.\n \n #25 0x0000555555670a0a in _PyFunction_FastCallKeywords ()\n \n No symbol table info available.\n \n #26 0x0000555555703dcb in ?? ()\n \n No symbol table info available.\n \n #27 0x0000555555700fe2 in _PyEval_EvalFrameDefault ()\n \n No symbol table info available.\n \n #28 0x0000555555670a0a in _PyFunction_FastCallKeywords ()\n \n No symbol table info available.\n \n #29 0x0000555555703dcb in ?? ()\n \n No symbol table info available.\n \n #30 0x0000555555700fe2 in _PyEval_EvalFrameDefault ()\n \n No symbol table info available.\n \n ```",
      "y": "Register pthread_atfork that would leak pthread pool."
   },
   {
      "x": "RRef.to_here() does not synchronize CUDA Streams properly",
      "z": "Yeah the idea of storing CUDA events on the RRef makes sense to me. Although it must be said that I know nothing about RRefs so I don't know if other approaches could suit better.\n \n \n \n As for passing the devices to the callback, I am guessing that this is in order to avoid the callback itself having to inspect the value and extract the devices it uses, right? (And we know that inspecting a value is not easy) If so then, yes, I think passing a set of devices to the callback could make sense. Just to clarify though: this would be just a change in an internal API, it wouldn't mean that all end users will now need to expect an extra argument in their Python functions too right?",
      "y": "The ctx in the following code is an instance of CudaLazyStreamContext when CUDA is enabled. Its streams_\n field holds the devices and current streams where this request will be run on. The proposed solution is to\n extract the devices vector from the ctx and pass that to cb_->operator()(requestMessage); invocation, and\n then propogate it to the created RRef accordingly."
   },
   {
      "x": "Single-matrix cholesky much slower than batch mode with batch_size=1?",
      "z": "Thanks for the comments @rfeinman ! I think we don't have a `ldl` operator in pytorch right now, but that sounds like a good idea! If you didn't find an existing issue for this, feel free to file a new issue with your feature request and comments so that we can discuss and track it.",
      "y": "Have an `ldl` operator in pytorch."
   },
   {
      "x": "Irrelevant named tensor warnings",
      "z": "We've fixed the problems with max_pool{1, 2, 3}d on master with https://github.com/pytorch/pytorch/pull/60059. The fix will be in our next release.",
      "y": "Fixed the problems with max_pool{1, 2, 3}d."
   },
   {
      "x": "Currently, LibTorch cannot be downloaded",
      "z": "Created https://github.com/pytorch/pytorch.github.io/pull/660 to fix",
      "y": "Update libtorch links for CUDA 10.2 ."
   },
   {
      "x": "The latest code cannot build due to missing file CPUFuntion.h",
      "z": "Yes, I use pip install numpy pyyaml mkl mkl-include setuptools cmake cffi typing_extensions future six requests dataclasses\n \n except ninja, since install ninja will lead to other issues.\n \n I will pull top of tree again to see whether this issue can reproduce today.\n \n \n \n > Can you ensure that you have installed all dependencies in the repro steps? https://github.com/pytorch/pytorch#install-dependencies",
      "y": "Pulled the latest code, and compiled successfully."
   },
   {
      "x": "Improve torch.linalg documentation",
      "z": "Together with Mario, we've found out that some Unicode symbols (for example \u00e2\u20ac\u0153\u00e2\u201a\u2122\u00e2\u20ac\u009d (U+2099)) do not render properly on Mac with the system default font.\n \n Quick google search tells that this problem could probably be fixed with CSS [@font-face](https://developer.mozilla.org/en-US/docs/Web/CSS/@font-face) that allows using a custom font from a server or locally from the computer if available.",
      "y": "Some Unicode symbols (for example \u00e2\u20ac\u0153\u00e2\u201a\u2122\u00e2\u20ac\u009d (U+2099)) do not render properly on Mac with the system default font."
   },
   {
      "x": "skip_if_not_multigpu decorator skipping/passing tests",
      "z": "1) The decorator implementation for `@skip_if_not_multigpu` is wrong so it is not running any of the tests that use it. PR to resolve: https://github.com/pytorch/pytorch/pull/54916.\n \n 2) That works, thank you!\n \n 3) That particular test garrett was running was only for RPC (tensorpipe)",
      "y": "The decorator implementation for `@skip_if_not_multigpu` is wrong so it is not running any of the tests that use it."
   },
   {
      "x": "torch.ceil wrong formula",
      "z": "cc @kshitij12345, our most recent unary ops maintainer",
      "y": "`ceil(x) != floor(x) + 1` when x is already an integer."
   },
   {
      "x": "Doc of SVD is erroneous and inconsistent",
      "z": "This should be solved by https://github.com/pytorch/pytorch/pull/54002 once it's merged.\n \n There's also the larger issued https://github.com/pytorch/pytorch/issues/54878 that aims to improve the docs of all `torch.linalg`.",
      "y": "Improve the docs of all `torch.linalg`."
   },
   {
      "x": "Maybe one more '=' sign?",
      "z": "Thanks for catching this! We would accept a PR to fix this.",
      "y": "Update previous-versions.md."
   },
   {
      "x": "CUDA error: device-side assert triggered(torch1.8.1+cuda11.1)",
      "z": "I think this issue has the same root cause as #54245 and #52663 and it was actually not fixed by switching from thrust to cub. The cause should be the `STB_GNU_UNIQUE` symbols as pointed out by @VoVAllen in https://github.com/NVIDIA/thrust/issues/1401#issuecomment-806403746. \n \n \n \n To verify the above claim, I ran [this python script](http://ppwwyyxx.com/blog/2021/Patch-STB_GNU_UNIQUE/) on `libtorch_cuda_cpp.so` inside pytorch, and the bug disappears. The script hides all symbols with `STB_GNU_UNIQUE` type.\n \n \n \n A proper fix is probably to try the `-fno-gnu-unique` option as mentioned in https://github.com/pytorch/pytorch/issues/52663#issuecomment-809092114.",
      "y": "Try the `-fno-gnu-unique` option."
   },
   {
      "x": "AccessDeniedAccess when download libtorch 1.8.1 for Linux for CUDA 10.2 from download.pytorch.org",
      "z": "Duplicate of #54855",
      "y": "Unable to download libtorch (1.8.1) from pytorch.org."
   },
   {
      "x": "The results of pipeline parallelism cannot be reproduced when using different numbers of partitions",
      "z": "I've fixed the tutorials to handle batch size correctly. Feel free to re-open this issue if there is still a problem that needs to be resolved.",
      "y": "Integrate Sentence Embedding training and fine-tuning in DVC pipeline."
   },
   {
      "x": "torch.jit.trace is not working and causing program stop working",
      "z": "thanks for your help I've fixed it by my self wrong pytorch version was installed:)",
      "y": "Check for wrong pytorch version."
   },
   {
      "x": "DDP checkpointing tests, test failures",
      "z": "to unblock you to land your PR, you can help skip the two tests, I will enable it after fixing. thanks!",
      "y": "Enable static graph training in DDP."
   },
   {
      "x": "F.embedding has unexpected behavior with non-2d weights",
      "z": "Thanks for reporting this! I was able to reproduce it on master.\n \n \n \n [F.embedding()](https://pytorch.org/docs/stable/nn.functional.html#embedding) is not intended to support non-2D weight, so we would accept a PR adding a dim check to ensure this.",
      "y": "F.embedding() is not intended to support non-2D weight."
   },
   {
      "x": "[torch.jit.script] Python type cannot be used as a value:",
      "z": "This is because `nn.Module` initialization isn't supported in TorchScript. Instead, it's recommended to be constructed as a submodule of another module.\n \n \n \n ```\n \n class ParentModule(torch.nn.Module):\n \n  def __init__(self):\n \n  super().__init__()\n \n  self.softmax = torch.nn.Softmax(dim=-1)\n \n  \n \n  def forward(self, x: torch.Tensor):\n \n  return self.softmax(x)\n \n \n \n m = ParentModule()\n \n scripted_m = torch.jit.script(m)\n \n print(scripted_m(torch.rand([1, 2, 3])))\n \n ```",
      "y": "`nn.Module` initialization isn't supported in TorchScript."
   },
   {
      "x": "[libtorch] tensor.to(torch::Device(torch::kCPU)) is VERY SLOW",
      "z": "Hi, it would be very helpful if you could post a script of your test. One possibility that comes to mind is that your measurement for the forward propagation is not synchronizing the CUDA device so when you move the data back to the CPU you have to wait for the computation for finish first.",
      "y": "Measurement for the forward propagation is not synchronizing the CUDA device."
   },
   {
      "x": "Improve torch.distributed.new_group() documentation in the context of SyncBatchNorm",
      "z": "> > it is not clear that when creating subgroups, every rank still needs to make the call regardless of whether they are in the group or not, as per the docs in https://pytorch.org/docs/master/distributed.html#torch.distributed.new_group.\n \n > \n \n > Don't the docs already mention this:\n \n > \n \n > > This function requires that all processes in the main group (i.e. all processes that are part of the distributed job) enter this function, even if they are not going to be members of the group. Additionally, groups should be created in the same order in all processes.\n \n \n \n I meant the docs/example in sync_batchnorm itself: https://pytorch.org/docs/stable/generated/torch.nn.SyncBatchNorm.html#torch.nn.SyncBatchNorm.convert_sync_batchnorm, where the example doesn't really show that very clearly. Although the user can also check the `new_group` docs itself of course, updating that example would probably also be useful.",
      "y": "This function requires that all processes in the main group (i.e. all processes that are part of the distributed job) enter this function, even if they are not going to be members of the group."
   },
   {
      "x": "Enable linear algebra functions on ROCm platform",
      "z": "> Some changes in code would need to be made so that only magma v2 API magma_v2.h is used (maybe v1 works fine as well, but it includes only cublas.h by default, while v2 seems to work with hipblas.h.\n \n \n \n In #49978 I try to change the magma includes to magma_v2.h.",
      "y": "Code changes so that only magma v2 API magma_v2.h is used (maybe v1 works fine as well, but it includes only cublas.h by default, while v2 seems to work with hipblas.h."
   },
   {
      "x": "the result of torch.addcmul(input, tensor1, tensor2, value) != the result of input\u00e2\u20ac\u2039+value*tensor1\u00e2\u20ac\u2039*tensor2",
      "z": "The difference is well below the order of 1e-5, which is expected for fp32 arithmetics. Afterall, floating point computation is not exact.",
      "y": "Difference is well below the order of 1e-5 and is expected for fp32 arithmetics."
   },
   {
      "x": "torch.multinomial selects elements with zero weight",
      "z": "Will follow up with @ngimel on how reliable is the failure. (I can reproduce it on my Linux box, but never on a Mac, so can it be a compiler problem?)",
      "y": "Problem where \"0\" was generated at the same position as a non-zero probability, effectively masking it."
   },
   {
      "x": "Support autograd in `torch.svd` with complex inputs",
      "z": "certainly!",
      "y": "Add autograd tests for complex matrix norm nuclear and +/-2 ."
   },
   {
      "x": "Problem with building from source in win7",
      "z": "Extracted error message:\n \n ```\n \n  Creating library lib\\cuda_atomic_ops_test.lib and object lib\\cuda_atomic_ops_test.exp\n \n cuda_atomic_ops_test_generated_cuda_atomic_ops_test.cu.obj : error LNK2019: unresolved external symbol \"float __cdecl pow(float,int)\" (?pow@@YAMMH@Z) referenced in function \"void __cdecl test_atomic_mul<float>(void)\" (??$test_atomic_mul@M@@YAXXZ)\n \n  Hint on symbols that are defined and could potentially match:\n \n  \"float __cdecl pow(float,float)\" (?pow@@YAMMM@Z)\n \n cuda_atomic_ops_test_generated_cuda_atomic_ops_test.cu.obj : error LNK2019: unresolved external symbol \"double __cdecl pow(double,int)\" (?pow@@YANNH@Z) referenced in function \"void __cdecl test_atomic_mul<double>(void)\" (??$test_atomic_mul@N@@YAXXZ)\n \n  Hint on symbols that are defined and could potentially match:\n \n  \"float __cdecl pow(float,float)\" (?pow@@YAMMM@Z)\n \n ```\n \n Looks like we should do an explicit type casting there.",
      "y": "Do an explicit type casting there."
   },
   {
      "x": "aarch64 CMake checks are incorrect on Apple Silicon",
      "z": "cc @janeyx99",
      "y": "No updates will be submitted to QNNPACK as it is an archive repository. None of the PyTorch operators depend on third_party/QNNPACK, only some legacy caffe2 ops."
   },
   {
      "x": "Test self.assertEqual with msg does not print numerical comparison results",
      "z": "\"testing\" could probably have a better, more distinct, name, but it's intended to be used for issues related to the torch.testing module vs. PyTorch's actual tests (which get \"module: tests\"). \n \n \n \n I think we would accept a PR combining custom messages and the default message. I don't have an immediate design for what these messages would look like. Maybe some other testing libraries have similar functionality and formatting we can use for inspiration? We also don't need to overthink it. We could probably print the \"default\" message and then just append the the provided msg string, for example.",
      "y": "It's intended to be used for issues related to the torch.testing module vs. PyTorch's actual tests."
   },
   {
      "x": "Compile error in Windows SDK",
      "z": "pthreadpool isn't our code. And we already cleaned up the imports in our code with https://github.com/pytorch/pytorch/pull/48009.",
      "y": "Reorganize and refine the Windows.h import in C++ files."
   },
   {
      "x": "Transcendental functions broken on Jetson Xavier NX",
      "z": "Thank you for looking into this issue! I included the output of this script in the issue description (the first comment at the top of the page).",
      "y": "If PyTorch for AARCH64 is compiled by clang, problem goes away."
   },
   {
      "x": "pytorch 1.4 can not load model saved by 1.7",
      "z": "Pytorch doesn't guarantee forward compatibility, but for this particular issue using `torch.save(_use_new_zipfile_serialization=False)` in 1.7 and load it in 1.4 might work. Please feel free to reopen if it doesn't fix. Thanks!",
      "y": "`torch.save(_use_new_zipfile_serialization=False)` in 1.7 and load it in 1.4."
   },
   {
      "x": "Could we wrap some pytorch operations into a large symbolic Op and export it to onnx ?",
      "z": "Hi, @spandantiwari \n \n How to turn off trace and turn on trace while in exporting to onnx.\n \n Following is my sample code and it won't work.\n \n \n \n ```\n \n import torch\n \n \n \n \n \n class TestBatchedNMSop(torch.autograd.Function):\n \n \n \n  @staticmethod\n \n  def forward(ctx,\n \n  boxes,\n \n  scores,\n \n  num_classes,\n \n  topk,\n \n  keep_topk,\n \n  score_threshold,\n \n  iou_threshould,\n \n  is_normalized=False,\n \n  clip_boxes=False,\n \n  share_location=False,\n \n  background_label_id=-1):\n \n  \n \n  return TestBatchedNMSop.output\n \n \n \n  @staticmethod\n \n  def symbolic(g,\n \n  boxes,\n \n  scores,\n \n  num_classes,\n \n  topk,\n \n  keep_topk,\n \n  score_threshold,\n \n  iou_threshould,\n \n  is_normalized=False,\n \n  clip_boxes=False,\n \n  share_location=False,\n \n  background_label_id=-1):\n \n  return g.op(\n \n  'BatchedNMS_TRT',\n \n  boxes,\n \n  scores,\n \n  numClasses_i=num_classes,\n \n  topK_i=topk,\n \n  keepTopK_i=keep_topk,\n \n  scoreThreshold_f=score_threshold,\n \n  iouThreshold_f=iou_threshould,\n \n  isNormalized_i=is_normalized,\n \n  clipBoxes_i=clip_boxes,\n \n  shareLocation_i=share_location,\n \n  backgroundLabelId_i=background_label_id,\n \n  outputs=len(TestBatchedNMSop.output))\n \n \n \n class TestModel(torch.nn.Module):\n \n  def __init__(self,\n \n  num_classes=80,\n \n  topk=10000,\n \n  keep_topk=1000,\n \n  score_threshold=0.05,\n \n  iou_threshould=0.5,\n \n  is_normalized=False,\n \n  clip_boxes=False,\n \n  share_location=False,\n \n  background_label_id=-1):\n \n  super(TestModel, self).__init__()\n \n  self.num_classes = num_classes\n \n  self.topk = topk\n \n  self.keep_topk = keep_topk\n \n  self.score_threshold = score_threshold\n \n  self.iou_threshould = iou_threshould\n \n  self.is_normalized = is_normalized\n \n  self.clip_boxes = clip_boxes\n \n  self.share_location = share_location\n \n  self.background_label_id = background_label_id\n \n  \n \n  def forward(self, boxes, scores):\n \n  # turn off tracing\n \n  #####################Dose not Work #####################\n \n  state = torch._C._get_tracing_state() # does not work\n \n  #####################Dose not Work #####################\n \n  # do normal process\n \n  batch_size = boxes.shape[0]\n \n  num_detections = torch.zeros(batch_size, 1) + 100\n \n  nmsed_boxes = torch.ones(batch_size, self.keep_topk, 4)\n \n  nmsed_scores = torch.ones(batch_size, self.keep_topk)\n \n  nmsed_classes = torch.ones(batch_size, self.keep_topk)\n \n  # directly save output, so our temporarily\n \n  # Customop does not need to implement any\n \n  # special code\n \n  output = (num_detections, nmsed_boxes, nmsed_scores, nmsed_classes)\n \n  setattr(TestBatchedNMSop, 'output', output)\n \n  # open tracing\n \n  #####################Dose not Work #####################\n \n  torch._C._set_tracing_state(state) # does not work\n \n  #####################Dose not Work #####################\n \n  # we do not need to save the output\n \n  # just call it for creating a correspond\n \n  # node in graph\n \n  return TestBatchedNMSop.apply(boxes, \n \n  scores, \n \n  self.num_classes,\n \n  self.topk, \n \n  self.keep_topk, \n \n  self.score_threshold, \n \n  self.iou_threshould,\n \n  self.is_normalized, \n \n  self.clip_boxes, \n \n  self.share_location, \n \n  self.background_label_id)\n \n \n \n boxes = torch.ones(1, 8732, 80, 4)\n \n scores = torch.ones(1, 8732, 80)\n \n \n \n model = TestModel().eval()\n \n torch.onnx.export(\n \n  model, \n \n  (boxes, scores),\n \n  \"batchednms_trt.onnx\",\n \n  input_names=['boxes', 'scores'],\n \n  output_names=['num_detections', 'nmsed_boxes', 'nmsed_scores', 'nmsed_classes'],\n \n  export_params=True,\n \n  keep_initializers_as_inputs=True,\n \n  enable_onnx_checker=False,\n \n  verbose=True)\n \n ```\n \n \n \n Could anyone please help me with this?\n \n Thanks.",
      "y": "Can export a PyTorch op as a custom op to ONNX through the PyTorch-ONNX exporter."
   },
   {
      "x": "On macOS Big Sur, parallel DataLoader causes global variables to be reinitialised",
      "z": "Seems spawn vs fork behavior. If you do **either** of the two things below, you should see the same behavior on your previous osx (which likely contains an earlier python version that still uses fork by default on osx):\n \n 1. move `set_var` outside the `if __name__ == '__main__':`\n \n 2. use `multiprocessing_context='fork'` in creating `DataLoader`",
      "y": "1. move `set_var` outside the `if __name__ == '__main__':`\n OR\n 2. use `multiprocessing_context='fork'` in creating `DataLoader`"
   },
   {
      "x": "`test_variant_consistency_jit` for BFloat16 are skipped in a confusing way",
      "z": "A way to make the code cleaner would just to skip bfloat16 for all of the JIT test cases instead of screening them out on the OpInfo level",
      "y": "Skip bfloat16 for all of the JIT test cases instead of screening them out on the OpInfo level."
   },
   {
      "x": "Profiling distributed NCCL collectives deadlocks when profiler run with use_cuda=True",
      "z": "just to clarify - this doesn't seem profiler specific (profiler just creates a bunch of cuda events on the devices) and this deadlock is triggered in other scenarios - @rohan-varma to confirm",
      "y": "It happens as a result of profiler creating events on each device. We can probably mitigate this by setting `CUDA_VISIBLE_DEVICES` properly."
   },
   {
      "x": "How to do polymorphism on torch::nn::ModuleHolder?",
      "z": "Thanks for your reply.\n \n I finally decide to use std::shared_ptr<M>, I will be losing the more natural constructor, but it is straight forward and it works.",
      "y": "Use std::shared_ptr<M> ."
   },
   {
      "x": "test_cholesky_solve_batched_many_batches_cuda_complex128 has cuda illegal memory access",
      "z": "I can repro that with 100% rate on cuda 11.1, 2070 or 3090. The failed line says it's `torch.Size([256, 256, 5, 5]) False`\n \n \n \n https://github.com/pytorch/pytorch/blob/d307601365c3b848072b8b8381208aedc1a0aca5/test/test_linalg.py#L1845",
      "y": "Workaround for MAGMA accessing illegal memory in batched cholesky."
   },
   {
      "x": "`torch.digamma`: Inconsistent with SciPy",
      "z": "This got fixed in https://github.com/pytorch/pytorch/pull/56689",
      "y": "Fix inconsistency of digamma with SciPy."
   },
   {
      "x": "If test suite triggers CUDA assert, should stop running tests",
      "z": "wdym by `failfast` option? If it's aborting after any failure in cuda test, then no, that's not what's requested here. We want to abort only if one of context-invalidating (sticky) errors happened.",
      "y": "Abort only if one of context-invalidating (sticky) errors happened."
   },
   {
      "x": "RuntimeError : PyTorch was compiled without NumPy support",
      "z": "You have to install another version of numpy.\n \n ```\n \n pip install numpy==1.15.0\n \n Collecting numpy==1.15.0\n \n  Downloading https://files.pythonhosted.org/packages/27/92/c01d3a6c58ceab0e6ec36ad3af41bc076014cc916afcb979ab4c9558f347/numpy-1.15.0-cp37-cp37m-manylinux1_x86_64.whl (13.8MB)\n \n  100% |\u00e2\u2013\u02c6\u00e2\u2013\u02c6\u00e2\u2013\u02c6\u00e2\u2013\u02c6\u00e2\u2013\u02c6\u00e2\u2013\u02c6\u00e2\u2013\u02c6\u00e2\u2013\u02c6\u00e2\u2013\u02c6\u00e2\u2013\u02c6\u00e2\u2013\u02c6\u00e2\u2013\u02c6\u00e2\u2013\u02c6\u00e2\u2013\u02c6\u00e2\u2013\u02c6\u00e2\u2013\u02c6\u00e2\u2013\u02c6\u00e2\u2013\u02c6\u00e2\u2013\u02c6\u00e2\u2013\u02c6\u00e2\u2013\u02c6\u00e2\u2013\u02c6\u00e2\u2013\u02c6\u00e2\u2013\u02c6\u00e2\u2013\u02c6\u00e2\u2013\u02c6\u00e2\u2013\u02c6\u00e2\u2013\u02c6\u00e2\u2013\u02c6\u00e2\u2013\u02c6\u00e2\u2013\u02c6\u00e2\u2013\u02c6| 13.9MB 2.9MB/s \n \n Installing collected packages: numpy\n \n  Found existing installation: numpy 1.15.4\n \n  Uninstalling numpy-1.15.4:\n \n  Successfully uninstalled numpy-1.15.4\n \n Successfully installed numpy-1.15.0\n \n ```\n \n Above downgrading 1.15.4 -> 1.15.0.\n \n \n \n Then install another torch version.\n \n ```\n \n pip install torch==0.4.1.post2\n \n Collecting torch==0.4.1.post2\n \n  Downloading https://files.pythonhosted.org/packages/d3/91/1b2871d6c8ca079254deae5872af32e02e9a85f07dd0834e8b3489ce138f/torch-0.4.1.post2-cp37-cp37m-manylinux1_x86_64.whl (519.5MB)\n \n  100% |\u00e2\u2013\u02c6\u00e2\u2013\u02c6\u00e2\u2013\u02c6\u00e2\u2013\u02c6\u00e2\u2013\u02c6\u00e2\u2013\u02c6\u00e2\u2013\u02c6\u00e2\u2013\u02c6\u00e2\u2013\u02c6\u00e2\u2013\u02c6\u00e2\u2013\u02c6\u00e2\u2013\u02c6\u00e2\u2013\u02c6\u00e2\u2013\u02c6\u00e2\u2013\u02c6\u00e2\u2013\u02c6\u00e2\u2013\u02c6\u00e2\u2013\u02c6\u00e2\u2013\u02c6\u00e2\u2013\u02c6\u00e2\u2013\u02c6\u00e2\u2013\u02c6\u00e2\u2013\u02c6\u00e2\u2013\u02c6\u00e2\u2013\u02c6\u00e2\u2013\u02c6\u00e2\u2013\u02c6\u00e2\u2013\u02c6\u00e2\u2013\u02c6\u00e2\u2013\u02c6\u00e2\u2013\u02c6\u00e2\u2013\u02c6| 519.5MB 170kB/s \n \n Installing collected packages: torch\n \n  Found existing installation: torch 0.4.1\n \n  Uninstalling torch-0.4.1:\n \n  Successfully uninstalled torch-0.4.1\n \n Successfully installed torch-0.4.1.post2\n \n ```\n \n Here upgrade to torch-0.4.1.post2 from torch-0.4.1",
      "y": "Install another version of numpy."
   },
   {
      "x": "Cannot find libcaffe2_gpu.so",
      "z": "What is the build command that you used to build Caffe2? \"from caffe2.python import core\" is not a build command. This is a snippet of python code which the Python executable will try to run based on the libraries that it has installed. You must install Caffe2 for Python to use it correctly, and to install Caffe2 you must either build it from source or download a prebuilt binary from conda.\n \n \n \n Please also post your full cmake output.\n \n \n \n You should read through https://caffe2.ai/docs/faq.html , it will help you debug many of these types of errors.",
      "y": "Add the location of the file to the `LD_LIBRARY_PATH` ."
   },
   {
      "x": "[Report] nightly build with Jupyter stuck",
      "z": "I have just cloned and installed the latest version of pytorch in a new environment, installed jupyter notebook after updating pip, and I have no problems now importing pytorch in a jupyter notebook.\n \n \n \n ![image](https://user-images.githubusercontent.com/9110200/39118797-8282cf1a-46ea-11e8-8829-67cba16d5da9.png)\n \n \n \n \n \n I'm closing this issue now as fixed, but let me know if it still doesn't work for you",
      "y": "Clone and install the latest version of pytorch in a new environment, install jupyter notebook after updating pip."
   },
   {
      "x": "[Caffe2] Unable to clone Caffe2 repo",
      "z": "The instructions from @dbermond are completely correct. The caffe2 code is at https://github.com/pytorch/pytorch",
      "y": "`$ git clone --recursive https://github.com/pytorch/pytorch.git`"
   },
   {
      "x": "[ONNX] Cannot export upsample op",
      "z": "I receive a PyTorch - ONNX Export Error with Opset 11 caused by F.Interpolate/nn.Upsampling...2d. See https://github.com/pytorch/pytorch/issues/30681\n \n \n \n PyTorch model export to ONNX fails when using PyTorch 1.3.1 or nightly and ONNX 1.5.0. Failure is caused by all upsample operations: \n \n \n \n - `nn.UpsamplingBilinear2d(scale_factor=2)`\n \n - `nn.UpsamplingNearest2d(scale_factor=2)`\n \n - `F.interpolate(x, scale_factor=2)`\n \n \n \n \n \n Error message:\n \n ```bash\n \n torch version 1.4.0.dev20191127\n \n onnx version 1.5.0\n \n torch.Size([1, 3, 256, 256]) torch.Size([1, 3, 512, 512])\n \n Traceback (most recent call last):\n \n  File \"/Users/glennjocher/PycharmProjects/iD/upsample_error_reproduce.py\", line 28, in <module>\n \n  onnx.checker.check_model(oml)\n \n  File \"/Users/glennjocher/opt/anaconda3/envs/pytorch_nightly/lib/python3.7/site-packages/onnx/checker.py\", line 86, in check_model\n \n  C.check_model(model.SerializeToString())\n \n onnx.onnx_cpp2py_export.checker.ValidationError: Node (Resize_23) has input size 4 not in range [min=2, max=2].\n \n \n \n ==> Context: Bad node spec: input: \"input\" input: \"20\" input: \"20\" input: \"23\" output: \"24\" name: \"Resize_23\" op_type: \"Resize\" attribute { name: \"coordinate_transformation_mode\" s: \"asymmetric\" type: STRING } attribute { name: \"cubic_coeff_a\" f: -0.75 type: FLOAT } attribute { name: \"mode\" s: \"nearest\" type: STRING } attribute { name: \"nearest_mode\" s: \"floor\" type: STRING }\n \n ```\n \n \n \n ## To Reproduce\n \n \n \n ```python\n \n # conda install pytorch torchvision -c pytorch-nightly\n \n # conda install -c conda-forge onnx\n \n import torch\n \n import torch.nn as nn\n \n import torch.nn.functional as F\n \n import onnx\n \n \n \n \n \n class TestModel(nn.Module):\n \n  def __init__(self):\n \n  super(TestModel, self).__init__()\n \n \n \n  def forward(self, x):\n \n  # m = nn.UpsamplingBilinear2d(scale_factor=2) # fails\n \n  # m = nn.UpsamplingNearest2d(scale_factor=2) # fails\n \n  x = F.interpolate(x, scale_factor=2) # fails\n \n  return x\n \n \n \n \n \n print('torch version %s' % torch.__version__)\n \n print('onnx version %s' % onnx.__version__)\n \n img = torch.randn(1, 3, 256, 256)\n \n tml = TestModel()\n \n print(img.shape, tml(img).shape)\n \n torch.onnx.export(tml, img, 'model.onnx', verbose=False, opset_version=11)\n \n \n \n oml = onnx.load('model.onnx') # onnx model\n \n onnx.checker.check_model(oml)\n \n ```",
      "y": "ONNX Interpolate Add Scales Params."
   },
   {
      "x": "[Caffe2] Unable to compile Caffe 2 with CUDA 9",
      "z": "Just tried. Yup works with 5.5.0",
      "y": "works with 5.5.0."
   },
   {
      "x": "error when loading model saved under newer version",
      "z": "loading model is backward compatible but not forward compatible.",
      "y": "Loading model is not forward compatible. It is backward compatible."
   },
   {
      "x": "[caffe2] EigenTranspose problem in math_cpu.cc",
      "z": "I think we can close the issue now because the eigen transpose is disabled in the latest commit #7112 .",
      "y": "Eigen transpose is disabled in the latest commit."
   },
   {
      "x": "backward not working properly in svd (pytorch-nightly)",
      "z": "It's a shorthand for \"reproduced\", which means that the bug still hasn't been fixed. Sorry, we'll try to get to it soon.",
      "y": "Fix SVD backward on non-square matrices when some=False ."
   },
   {
      "x": "Arguments are located on different GPU with torch.nn.EmbeddingBag",
      "z": "Thanks for reporting! This appears to be fixed on both 0.4 release (very soon) and master.",
      "y": "Fixed on both 0.4 release and master."
   },
   {
      "x": "[PyTorch] Printing large tensors is slow",
      "z": "I'm looking into this",
      "y": "Fix half tensor printing plus speedup large tensor printing."
   },
   {
      "x": "[PyTorch] Don't use scientific notation for printing integer tensors",
      "z": "duplicate (sort of) of https://github.com/pytorch/pytorch/issues/6811",
      "y": "Integer tensors won't be printed in scientific notation.\n Removed scaling factor for all tensors.\n Only leave spaces for negative signs if one of the printed elements is going to have an negative sign."
   },
   {
      "x": "[pytorch] randperm lacks CUDA implementation",
      "z": "Ok, I solved the issue by changing two lines in the `utils/data/sampler.py` file.\n \n \n \n ```python\n \n class RandomSampler(Sampler):\n \n  r\"\"\"Samples elements randomly, without replacement.\n \n \n \n  Arguments:\n \n  data_source (Dataset): dataset to sample from\n \n  \"\"\"\n \n \n \n  def __init__(self, data_source):\n \n  self.data_source = data_source\n \n \n \n  def __iter__(self):\n \n  cpu = torch.device('cpu')\n \n  return iter( torch.randperm( len(self.data_source), device = cpu).tolist())\n \n \n \n  def __len__(self):\n \n  return len(self.data_source)\n \n ```\n \n \n \n **if** it is okay, I can PR.",
      "y": "Change two lines in the `utils/data/sampler.py` file.\n \n \n \n ```python\n \n class RandomSampler(Sampler):\n \n  r\"\"\"Samples elements randomly, without replacement.\n \n \n \n  Arguments:\n \n  data_source (Dataset): dataset to sample from\n \n  \"\"\"\n \n \n \n  def __init__(self, data_source):\n \n  self.data_source = data_source\n \n \n \n  def __iter__(self):\n \n  cpu = torch.device('cpu')\n \n  return iter( torch.randperm( len(self.data_source), device = cpu).tolist())\n \n \n \n  def __len__(self):\n \n  return len(self.data_source)\n \n ```"
   },
   {
      "x": "Compilation errors on master, Ubuntu 17.10, CUDA 9, GCC 6.4 / GCC 5.4.1",
      "z": "Here is the output:\n \n \n \n ```\n \n $ grep \"_GLIBCXX_USE_C99\" /usr/include/x86_64-linux-gnu/c++/4.8/bits/c++config.h\n \n /* #undef _GLIBCXX_USE_C99 */\n \n #define _GLIBCXX_USE_C99_COMPLEX 1\n \n #define _GLIBCXX_USE_C99_COMPLEX_TR1 1\n \n #define _GLIBCXX_USE_C99_CTYPE_TR1 1\n \n #define _GLIBCXX_USE_C99_FENV_TR1 1\n \n #define _GLIBCXX_USE_C99_INTTYPES_TR1 1\n \n #define _GLIBCXX_USE_C99_INTTYPES_WCHAR_T_TR1 1\n \n /* #undef _GLIBCXX_USE_C99_MATH */\n \n #define _GLIBCXX_USE_C99_MATH_TR1 1\n \n #define _GLIBCXX_USE_C99_STDINT_TR1 1\n \n ```\n \n \n \n ```\n \n $ apt-cache showpkg libstdc++-4.8-dev\n \n Package: libstdc++-4.8-dev\n \n Versions: \n \n 4.8.5-4ubuntu6 (/var/lib/apt/lists/gb.archive.ubuntu.com_ubuntu_dists_artful_universe_binary-amd64_Packages) (/var/lib/dpkg/status)\n \n  Description Language: \n \n  File: /var/lib/apt/lists/gb.archive.ubuntu.com_ubuntu_dists_artful_universe_binary-amd64_Packages\n \n  MD5: a197f2aec835e5fc6f8f76039d8a7c4e\n \n  Description Language: \n \n  File: /var/lib/apt/lists/gb.archive.ubuntu.com_ubuntu_dists_artful_universe_binary-i386_Packages\n \n  MD5: a197f2aec835e5fc6f8f76039d8a7c4e\n \n  Description Language: en\n \n  File: /var/lib/apt/lists/gb.archive.ubuntu.com_ubuntu_dists_artful_universe_i18n_Translation-en\n \n  MD5: a197f2aec835e5fc6f8f76039d8a7c4e\n \n \n \n \n \n Reverse Depends: \n \n  g++-4.8,libstdc++-4.8-dev 4.8.5-4ubuntu6\n \n  libstdc++-4.8-dev:i386,libstdc++-4.8-dev 4.8.5-4ubuntu6\n \n  libstdc++-4.8-dev:i386,libstdc++-4.8-dev 4.8.5-4ubuntu6\n \n  libx32stdc++6-4.8-dbg,libstdc++-4.8-dev 4.8.5-4ubuntu6\n \n  libx32stdc++-4.8-dev,libstdc++-4.8-dev 4.8.5-4ubuntu6\n \n  libstdc++6-4.8-dbg,libstdc++-4.8-dev 4.8.5-4ubuntu6\n \n  libstdc++-4.8-pic,libstdc++-4.8-dev 4.8.5-4ubuntu6\n \n  libboost1.63-dev,libstdc++-4.8-dev\n \n  lib32stdc++6-4.8-dbg,libstdc++-4.8-dev 4.8.5-4ubuntu6\n \n  lib32stdc++-4.8-dev,libstdc++-4.8-dev 4.8.5-4ubuntu6\n \n  libboost1.62-dev,libstdc++-4.8-dev\n \n Dependencies: \n \n 4.8.5-4ubuntu6 - gcc-4.8-base (5 4.8.5-4ubuntu6) libgcc-4.8-dev (5 4.8.5-4ubuntu6) libstdc++6 (2 4.8.5-4ubuntu6) libc6-dev (2 2.13-0ubuntu6) libg++2.8-dev (0 (null)) libg++27-dev (0 (null)) libg++272-dev (3 2.7.2.8-1) libstdc++2.10-dev (3 1:2.95.3-2) libstdc++2.8-dev (0 (null)) libstdc++2.9-dev (0 (null)) libstdc++2.9-glibc2.1-dev (0 (null)) libstdc++3.0-dev (0 (null)) libstdc++-4.8-doc (0 (null)) libstdc++-4.8-dev:i386 (35 4.8.5-4ubuntu6) libstdc++-4.8-dev:i386 (38 4.8.5-4ubuntu6) \n \n Provides: \n \n 4.8.5-4ubuntu6 - libstdc++-dev (= ) \n \n Reverse Provides: \n \n ```\n \n \n \n ---\n \n \n \n Edit: I managed to get it to compile by defining the following in `c++config.h`:\n \n `#define _GLIBCXX_USE_C99 1`\n \n `#define _GLIBCXX_USE_C99_MATH 1`\n \n \n \n Thanks @sdmonov!",
      "y": "Define the following in `c++config.h`:\n \n `#define _GLIBCXX_USE_C99 1`\n \n `#define _GLIBCXX_USE_C99_MATH 1`"
   },
   {
      "x": "Unnecessary memcopies emitted by autograd engine",
      "z": "Redefining nn/functional.py linear as (as you recommend in 2):\n \n ```.py\n \n def linear(input, weight, bias=None):\n \n  \"\"\" \n \n  Applies a linear transformation to the incoming data: :math:`y = xA^T + b`. \n \n  \n \n  Shape: \n \n  - Input: :math:`(N, *, in\\_features)` where `*` means any number of \n \n  additional dimensions \n \n  - Weight: :math:`(out\\_features, in\\_features)` \n \n  - Bias: :math:`(out\\_features)` \n \n  - Output: :math:`(N, *, out\\_features)` \n \n  \"\"\"\n \n \n \n  input = input.contiguous()\n \n  sizes = input.size()[:-1]\n \n  input = input.view(-1, input.size(-1))\n \n  if input.dim() == 2 and bias is not None:\n \n  # fused op is marginally faster \n \n  output = torch.addmm(bias, input, weight.t())\n \n  return output.view(*sizes, -1)\n \n \n \n  output = input.matmul(weight.t())\n \n  if bias is not None:\n \n  output += bias\n \n  return output\n \n ```\n \n Was enough to avoid the mem copies in fairseq. Should we just route though to addmm for now until someone tackles 1?",
      "y": "Redefine nn/functional.py linear as :\n \n ```.py\n \n def linear(input, weight, bias=None):\n \n  \"\"\" \n \n  Applies a linear transformation to the incoming data: :math:`y = xA^T + b`. \n \n  \n \n  Shape: \n \n  - Input: :math:`(N, *, in\\_features)` where `*` means any number of \n \n  additional dimensions \n \n  - Weight: :math:`(out\\_features, in\\_features)` \n \n  - Bias: :math:`(out\\_features)` \n \n  - Output: :math:`(N, *, out\\_features)` \n \n  \"\"\"\n \n \n \n  input = input.contiguous()\n \n  sizes = input.size()[:-1]\n \n  input = input.view(-1, input.size(-1))\n \n  if input.dim() == 2 and bias is not None:\n \n  # fused op is marginally faster \n \n  output = torch.addmm(bias, input, weight.t())\n \n  return output.view(*sizes, -1)\n \n \n \n  output = input.matmul(weight.t())\n \n  if bias is not None:\n \n  output += bias\n \n  return output\n \n ```"
   },
   {
      "x": "[feature request] support batch diag",
      "z": "how about `torch.diag(batch=True)`? We can have similar extensions to other operators...",
      "y": "`torch.diag(batch=True)` and similar to other operators."
   },
   {
      "x": "[feature request] torch.where to support Tensors and python scalars",
      "z": "Tensors will be gone within a week or two. Won't fix.",
      "y": "Tensors will be gone within a week or two. Won't fix."
   },
   {
      "x": "discuss.pytorch.org is down",
      "z": "it'is not work.\n \n ![1](https://user-images.githubusercontent.com/5284540/69575091-a85a5180-0fda-11ea-9e7c-a4ec46201dd7.png)",
      "y": "Ran into unexpected maintenance."
   },
   {
      "x": "RuntimeError: cuda runtime error (2) : out of memory when using loss function",
      "z": "Because as long as output Variables are in scope, the underlying graphs can't be freed. You're meant to let go the loss at the end of iteration.",
      "y": "As long as output Variables are in scope, the underlying graphs can't be freed."
   },
   {
      "x": "Error message when wrong call to forward of RNN (nn.GRU and also nn.LSTM)",
      "z": "After a few attempts, it turns out that, at least in my case, the error occurs when the dimension 2 of the input is not correct.\n \n ```python\n \n import torch\n \n from torch import nn\n \n from torch.autograd import Variable\n \n \n \n model = nn.LSTM(5, 1)\n \n \n \n input = Variable(torch.rand(1, 1, 3)) # input = Variable(torch.rand(1, 1, 5)) to get it work :)\n \n h0 = Variable(torch.rand(1, 1, 1))\n \n c0 = Variable(torch.rand(1, 1, 1))\n \n \n \n print(model(input, (h0, c0)))\n \n \n \n '''\n \n ---------------------------------------------------------------------------\n \n NameError Traceback (most recent call last)\n \n <ipython-input-1-68e4878dc5e1> in <module>()\n \n  9 c0 = Variable(torch.rand(1, 1, 1))\n \n  10 \n \n ---> 11 print(model(input, (h0, c0)))\n \n \n \n /usr/local/lib/python3.5/dist-packages/torch/nn/modules/module.py in __call__(self, *input, **kwargs)\n \n  355 result = self._slow_forward(*input, **kwargs)\n \n  356 else:\n \n --> 357 result = self.forward(*input, **kwargs)\n \n  358 for hook in self._forward_hooks.values():\n \n  359 hook_result = hook(self, input, result)\n \n \n \n /usr/local/lib/python3.5/dist-packages/torch/nn/modules/rnn.py in forward(self, input, hx)\n \n  188 flat_weight = None\n \n  189 \n \n --> 190 self.check_forward_args(input, hx, batch_sizes)\n \n  191 func = self._backend.RNN(\n \n  192 self.mode,\n \n \n \n /usr/local/lib/python3.5/dist-packages/torch/nn/modules/rnn.py in check_forward_args(self, input, hidden, batch_sizes)\n \n  139 raise RuntimeError(\n \n  140 'input.size(-1) must be equal to input_size. Expected {}, got {}'.format(\n \n --> 141 fn.input_size, input.size(-1)))\n \n  142 \n \n  143 if is_input_packed:\n \n \n \n NameError: name 'fn' is not defined\n \n '''\n \n ```",
      "y": "The error occurs when the dimension 2 of the input is not correct."
   },
   {
      "x": "CUDNN_STATUS_EXECUTION_FAILED with RNN on GPU",
      "z": "When I replace:\n \n \n \n ```\n \n \n \n  def init_hidden(self):\n \n  document_rnn_init_h = nn.Parameter(nn.init.xavier_uniform(torch.Tensor(self.nb_layers, self.batch_size, self.embedding_size).type(torch.FloatTensor)), requires_grad=True)\n \n  if self.mode == 'GRU':\n \n  return document_rnn_init_h\n \n  elif self.mode == 'LSTM':\n \n  document_rnn_init_c = nn.Parameter(nn.init.xavier_uniform(torch.Tensor(self.nb_layers, self.batch_size, self.embedding_size).type(torch.FloatTensor)), requires_grad=True)\n \n  return (document_rnn_init_h, document_rnn_init_c)\n \n ```\n \n \n \n with\n \n \n \n ```\n \n  def init_hidden(self):\n \n  document_rnn_init_h = nn.Parameter(nn.init.xavier_uniform(torch.Tensor(self.nb_layers, self.batch_size, self.embedding_size).type(torch.FloatTensor).cuda()), requires_grad=True)\n \n  if self.mode == 'GRU':\n \n  return document_rnn_init_h\n \n  elif self.mode == 'LSTM':\n \n  document_rnn_init_c = nn.Parameter(nn.init.xavier_uniform(torch.Tensor(self.nb_layers, self.batch_size, self.embedding_size).type(torch.FloatTensor).cuda()), requires_grad=True)\n \n  return (document_rnn_init_h, document_rnn_init_c)\n \n ```\n \n \n \n (that is, ensuring that the hidden parameters are put onto CUDA appropriately), it works. So it sounds like we're missing (1) some checks that the tensors we're running are indeed CUDA tensors, and maybe (2) some implicit conversions which no longer exist.",
      "y": "Missing (1) some checks that the tensors we're running are indeed CUDA tensors, and maybe (2) some implicit conversions which no longer exist."
   },
   {
      "x": "ONNX export issue for model with multiple outputs",
      "z": "Can you share broader chunk of code? Or at least 'print' the output to see its type. What is the output type of the model? Afaik, only Variables or lists or tuples of Variables (maybe nested) are supported.\n \n \n \n @houseroad",
      "y": "Only Variables or lists or tuples of Variables (maybe nested) are supported."
   },
   {
      "x": "Documentation code for sampling from categorical distribution throws an error on master",
      "z": "I think this has been fixed on `master`.",
      "y": "Appears to be a bug in `.view()` ."
   },
   {
      "x": "Compilation errors, when compiling pytorch on Ubuntu 17.10 or newer with GCC 5",
      "z": "> I've build gcc7 and add it into `$PATH`. But it still get this error.\n \n > \n \n > ```shell\n \n > The C++ compiler does not support required functions. This is very likely\n \n > due to a known bug in GCC 5 (and maybe other versions) on Ubuntu 17.10 and\n \n > newer. For more information, see:\n \n > https://github.com/pytorch/pytorch/issues/522\n \n > ```\n \n > \n \n > The log is as follows.\n \n > \n \n > ```shell\n \n > (base) xxx@TENCENT64:~/gpt_model/pytorch/pytorch> python setup.py install\n \n > Building wheel torch-1.7.0a0+7a64b0c\n \n > -- Building version 1.7.0a0+7a64b0c\n \n > cmake -GNinja -DBUILD_PYTHON=True -DBUILD_TEST=True -DCMAKE_BUILD_TYPE=Release -DCMAKE_INSTALL_PREFIX=/data1/mm64/xxx/gpt_model/pytorch/pytorch/torch -DCMAKE_PREFIX_PATH=/home/xxx/anaconda3/lib/python3.7/site-packages -DJAVA_HOME=/home/mmdev/devpkg/java/jdk1.7.0_04 -DNUMPY_INCLUDE_DIR=/home/xxx/anaconda3/lib/python3.7/site-packages/numpy/core/include -DPYTHON_EXECUTABLE=/home/xxx/anaconda3/bin/python -DPYTHON_INCLUDE_DIR=/home/xxx/anaconda3/include/python3.7m -DPYTHON_LIBRARY=/home/xxx/anaconda3/lib/libpython3.7m.so.1.0 -DTORCH_BUILD_VERSION=1.7.0a0+7a64b0c -DUSE_NUMPY=True /data1/mm64/xxx/gpt_model/pytorch/pytorch\n \n > -- Performing Test SUPPORT_GLIBCXX_USE_C99\n \n > -- Performing Test SUPPORT_GLIBCXX_USE_C99 - Failed\n \n > CMake Error at cmake/MiscCheck.cmake:81 (message):\n \n > The C++ compiler does not support required functions. This is very likely\n \n > due to a known bug in GCC 5 (and maybe other versions) on Ubuntu 17.10 and\n \n > newer. For more information, see:\n \n > https://github.com/pytorch/pytorch/issues/5229\n \n > Call Stack (most recent call first):\n \n > CMakeLists.txt:475 (include)\n \n > \n \n > \n \n > -- Configuring incomplete, errors occurred!\n \n > See also \"/data1/mm64/xxx/gpt_model/pytorch/pytorch/build/CMakeFiles/CMakeOutput.log\".\n \n > See also \"/data1/mm64/xxx/gpt_model/pytorch/pytorch/build/CMakeFiles/CMakeError.log\".\n \n > Traceback (most recent call last):\n \n > File \"setup.py\", line 748, in <module>\n \n > build_deps()\n \n > File \"setup.py\", line 332, in build_deps\n \n > cmake=cmake)\n \n > File \"/data1/mm64/xxx/gpt_model/pytorch/pytorch/tools/build_pytorch_libs.py\", line 59, in build_caffe2\n \n > rerun_cmake)\n \n > File \"/data1/mm64/xxx/gpt_model/pytorch/pytorch/tools/setup_helpers/cmake.py\", line 329, in generate\n \n > self.run(args, env=my_env)\n \n > File \"/data1/mm64/xxx/gpt_model/pytorch/pytorch/tools/setup_helpers/cmake.py\", line 141, in run\n \n > check_call(command, cwd=self.build_dir, env=env)\n \n > File \"/home/xxx/anaconda3/lib/python3.7/subprocess.py\", line 363, in check_call\n \n > raise CalledProcessError(retcode, cmd)\n \n > subprocess.CalledProcessError: Command '['cmake', '-GNinja', '-DBUILD_PYTHON=True', '-DBUILD_TEST=True', '-DCMAKE_BUILD_TYPE=Release', '-DCMAKE_INSTALL_PREFIX=/data1/mm64/xxx/gpt_model/pytorch/pytorch/torch', '-DCMAKE_PREFIX_PATH=/home/xxx/anaconda3/lib/python3.7/site-packages', '-DJAVA_HOME=/home/mmdev/devpkg/java/jdk1.7.0_04', '-DNUMPY_INCLUDE_DIR=/home/xxx/anaconda3/lib/python3.7/site-packages/numpy/core/include', '-DPYTHON_EXECUTABLE=/home/xxx/anaconda3/bin/python', '-DPYTHON_INCLUDE_DIR=/home/xxx/anaconda3/include/python3.7m', '-DPYTHON_LIBRARY=/home/xxx/anaconda3/lib/libpython3.7m.so.1.0', '-DTORCH_BUILD_VERSION=1.7.0a0+7a64b0c', '-DUSE_NUMPY=True', '/data1/mm64/xxx/gpt_model/pytorch/pytorch']' returned non-zero exit status 1.\n \n > (base) xxx@TENCENT64:~/gpt_model/pytorch/pytorch> gcc --version \n \n > gcc (GCC) 7.5.0\n \n > Copyright (C) 2017 Free Software Foundation, Inc.\n \n > This is free software; see the source for copying conditions. There is NO\n \n > warranty; not even for MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE.\n \n > ```\n \n \n \n Solved this problem by remove the `build` directory and add `gcc7` into `$PATH`",
      "y": "Remove the `build` directory and add `gcc7` into `$PATH` ."
   },
   {
      "x": "\"Bus error\" on /dev/shm OOM; hard/impossible to fix",
      "z": "[We have always been doing the `write` check](https://github.com/pytorch/pytorch/blob/master/aten/src/TH/THAllocator.c#L324-L333). The problem is that touching only the first element will not catch all errors, since the OS is allocating the pages lazily and will send the bus error only once it runs out of them.",
      "y": "Touching only the first element will not catch all errors, since the OS is allocating the pages lazily and will send the bus error only once it runs out of them."
   },
   {
      "x": "Pytorch inconsistent behavior for boundary checks",
      "z": "Like @neerajprad mentioned, I think it would be useful to have an optional debug flag which, when set, would cause pytorch to check for boundary conditions for all distributions instead of having checks for individual distributions. Tensorflow has a similar flag, [validate_args](https://www.tensorflow.org/versions/r1.0/api_docs/python/tf/contrib/distributions/Bernoulli#validate_args), which is an optional argument for all distributions. Also, it seems like torch already has some of these checks in place for some distributions(may not be all). I would be happy to contribute the feature and the test cases, if this proposal sounds reasonable. Any suggestions on how to proceed?",
      "y": "It would be nice to have an optional debug flag which, when set, would cause pytorch to check for boundary conditions for all distributions instead of having checks for individual distributions. Tensorflow has a similar flag, [validate_args](https://www.tensorflow.org/versions/r1.0/api_docs/python/tf/contrib/distributions/Bernoulli#validate_args), which is an optional argument for all distributions. Also, it seems like torch already has some of these checks in place for some distributions(may not be all)."
   },
   {
      "x": "Training and learning rate finder utilities",
      "z": "the examples make a good start, and there's a lot more that the examples dont cover.\n \n \n \n Promising trainer abstractions in the pytorch community are:\n \n \n \n - ignite: http://github.com/pytorch/ignite\n \n - lightning: https://github.com/williamFalcon/pytorch-lightning\n \n - skorch: https://github.com/skorch-dev/skorch\n \n \n \n There are a few nice ones, but didn't get as much traction.",
      "y": "Promising trainer abstractions in the pytorch community are:\n \n \n \n - ignite: http://github.com/pytorch/ignite\n \n - lightning: https://github.com/williamFalcon/pytorch-lightning\n \n - skorch: https://github.com/skorch-dev/skorch"
   },
   {
      "x": "Crash using PyTorch built with TBB support and multiple DataLoader instance with pin_memory=True",
      "z": "Yes, we support TBB. @ilia-cher should be able to help.",
      "y": "Relax restrictions on set_num_threads."
   },
   {
      "x": "Remove use of Variable wrapper in test files?",
      "z": "go for it!",
      "y": "Migrate away from using legacy Variable constructor in test_nn.py."
   },
   {
      "x": "cdist gradient computation is broken",
      "z": "This happens when `x1` has a size `1xn`. Then the result and the incoming gradient have the size `1xm` (where m is the number of vectors in x2), `grad.t()` is reported as contiguous, but it's last stride is still `m`, and computing offsets based on the last stride here becomes incorrect https://github.com/pytorch/pytorch/blob/fbf991d06279ea6828ec133b186dbef3bc16522b/aten/src/ATen/native/cpu/DistanceOpsKernel.cpp#L392 (`grad_k += gs`).",
      "y": "This happens when `x1` has a size `1xn`. Then the result and the incoming gradient have the size `1xm` (where m is the number of vectors in x2), `grad.t()` is reported as contiguous, but it's last stride is still `m`, and computing offsets based on the last stride here becomes incorrect."
   },
   {
      "x": "Significant GPU Memory leak",
      "z": "Thank you for the answer.\n \n \n \n In case anyone is wondering, here is how to set process specific env variables:\n \n ```python\n \n import torch.multiprocessing as _mp\n \n import torch\n \n import os\n \n \n \n mp = _mp.get_context('spawn')\n \n \n \n \n \n class Process(mp.Process):\n \n  def __init__(self):\n \n  super().__init__()\n \n  print(\"Init Process\")\n \n  return\n \n \n \n  def run(self):\n \n  print(\"Hello World!\")\n \n  os.environ['CUDA_VISIBLE_DEVICES'] = '1'\n \n  print(torch.cuda.device_count())\n \n  print(torch.cuda.current_device())\n \n \n \n if __name__ == \"__main__\":\n \n  num_processes = 2\n \n  os.environ['CUDA_VISIBLE_DEVICES'] = '0,1,2,3'\n \n  processes = [Process() for i in range(num_processes)]\n \n  [p.start() for p in processes]\n \n  print(\"main: \" + os.environ['CUDA_VISIBLE_DEVICES'])\n \n  [p.join() for p in processes]\n \n ```\n \n It is important to set it in the run method of the process, as the `__init__` method is still called in the main process, therefore setting the env vars of the main process when set there.",
      "y": "Set it in the run method of the process, as the `__init__` method is still called in the main process, therefore setting the env vars of the main process when set there."
   },
   {
      "x": "multi_head_attention_forward produces NaN",
      "z": "It turns out there was an error in my padding mask, as I marked the padded locations with 0s and the non-padded locations with 1s. This resulted in totally masked rows, which caused the softmax to output NaNs as explained in #25110.",
      "y": "Error in padding mask, as padded locations were marked with 0s and the non-padded locations with 1s. This resulted in totally masked rows, which caused the softmax to output NaNs."
   },
   {
      "x": "lstm module with None in input on CUDA segfaults.",
      "z": "Oh, sorry, I read the repro completely wrong, there is a PackedSequence and it's being created with a CUDA tensor. Thank you for fact checking me, @ailzhang! If we're confident that the above example doesn't segfault without PackedSequence, then we should close this.",
      "y": "Example doesn't segfault without PackedSequence ."
   },
   {
      "x": "Exclude generated source docs from Google",
      "z": "Conversely, though I agree that they shouldn't appear in search engines, I do find reading the source useful, so it would be nice to have [source] links in the docs.",
      "y": "Have [source] links in the docs."
   },
   {
      "x": "[ONNX] export with dynamic_axes should generate Shape node for intermediate tensor .shape calls even without jit.script",
      "z": "This is not because of size. The problem is with arange op which is exported as a constant. This is fixed in this PR: https://github.com/pytorch/pytorch/issues/20075",
      "y": "Set traceable=true in arange's arg parser."
   },
   {
      "x": "TorchScript fails to compile methods with misindented comments",
      "z": "Duplicate of https://github.com/pytorch/pytorch/issues/25043",
      "y": "Allow for source code comments at any level of indentation."
   },
   {
      "x": "Allreduce for sparse tensors is not working",
      "z": "Could you take a look and repro?",
      "y": "Put sparse all reduce results to input tensors."
   },
   {
      "x": "On pytorch mobile (android), module forward returns org.pytorch.IValue type. But on my detection module, supposes to return a tuple IValue.",
      "z": "Hello @Sakulaki ,\n \n \n \n IValue works as a 'tagged union' for all supported types, extracting them calling appropriate for your type `IValue.to${TYPE}` method.\n \n So for the case of tuple it should be used smth like this:\n \n ```\n \n  final IValue output = mModule.forward(IValue.from(mInputTensor));\n \n  IValue[] outputTuple = output.toTuple();\n \n  for (int i = 0; i < outputTuple.length; i++) {\n \n  IValue tupleElement = outputTuple[i];\n \n  }\n \n ```\n \n Or did not I get your problem right?",
      "y": "IValue works as a 'tagged union' for all supported types, extracting them calling appropriate for your type `IValue.to${TYPE}` method."
   },
   {
      "x": "[android] initHybrid missing/broken in pytorch 1.4.0 nightly",
      "z": "The fix was merged into master as https://github.com/pytorch/pytorch/commit/3a19980b78b6638235edef367784ecc3dc37e364\n \n I have republished android nightlies, the error should not happen if you update to [the nightlies with version >= 78](https://oss.sonatype.org/service/local/repositories/snapshots/content/org/pytorch/pytorch_android/1.4.0-SNAPSHOT/pytorch_android-1.4.0-20191220.225919-78.aar) (gradle key `--refresh-dependencies`)",
      "y": "The error should not happen if you update to the nightlies with version >= 78 (gradle key `--refresh-dependencies`)"
   },
   {
      "x": "Matrix multiplication returns wrong result when middle dimension is 0",
      "z": "Looks like cublas bug. With the repro script and .ltrace.conf from this gist (put .ltrace.conf in the root directory) \n \n https://gist.github.com/ngimel/780f09d431906a522c4df2da3cd16fbd\n \n I'm getting reasonable looking arguments for cublas call, yet output seems not to be overwritten:\n \n ```\n \n (pytorch) ngimel@xxxxxxx:~/playground$ ltrace -f -e 'cublasSgemmStridedBatched' python bmm.py\n \n [pid 23443] --- Called exec() ---\n \n [pid 23444] --- Called exec() ---\n \n [pid 23444] +++ exited (status 0) +++\n \n [pid 23443] --- SIGCHLD (Child exited) ---\n \n [pid 23443] +++ exited (status 0) +++\n \n [pid 23439] --- SIGCHLD (Child exited) ---\n \n [pid 23439] libtorch_cuda.so->cublasSgemmStridedBatched(0x562bbf906850, 0, 0, 43200, 1, 0, 1.000000, 0, 43200, 43200, 0, 1, 1, 0.000000, 0x7ff76a400000, 43200, 43200, 10) = 0\n \n tensor([[[1., 1., 1., ..., 1., 1., 1.]],\n \n \n \n  [[1., 1., 1., ..., 1., 1., 1.]],\n \n \n \n  [[1., 1., 1., ..., 1., 1., 1.]],\n \n \n \n  ...,\n \n \n \n  [[1., 1., 1., ..., 1., 1., 1.]],\n \n \n \n  [[1., 1., 1., ..., 1., 1., 1.]],\n \n \n \n  [[1., 1., 1., ..., 1., 1., 1.]]], device='cuda:0')\n \n ```\n \n We could implement a workaround zeroing the output for this case, actual cublas bugfixes take a long time. \n \n cc @ptrblck",
      "y": "We could implement a workaround zeroing the output for this case, actual cublas bugfixes take a long time."
   },
   {
      "x": "Should we expose CircleCI CUDA10 tests as XImportant and run on PRs?",
      "z": "Let's do it!",
      "y": "Switch important CI from CUDA 9 to 10.1."
   },
   {
      "x": "Power8/P100 node pytorch compilation from source with cuda 10.1: bus error - out of memory",
      "z": "If I may... Haven't tried, but looks like setting `MAX_JOBS` in the env will do it:\n \n \n \n https://github.com/pytorch/pytorch/blob/master/setup.py#L11",
      "y": "Set `MAX_JOBS` in the env."
   },
   {
      "x": "Missing edge case information in BCELoss documentation",
      "z": "Your clamped log update makes sense. You can take over the issue @kurtamohler.",
      "y": "It is using a safe log function that replaces `log(0)` specifically with `log(EPS)` where EPS = 1e-12."
   },
   {
      "x": "torch.gather in pytorch.onnx and onnxruntime",
      "z": "> orch from master branch and it's now 1.5 and ONNX 1.6\n \n \n \n I tried this environment: torch1.4 + onnx 1.6. But there are still problems. This time the problem is on the clip OP. I'm not sure if there is a problem with the gather OP. Can you help me to transform the Class SpatialTransformer to the onnx file and verify on onnxruntime\u00ef\u00bc\u0178\n \n Here is my Class SpatialTransformer:\n \n ```python\n \n class SpatialTransformer(nn.Module):\n \n  def __init__(self):\n \n  super(SpatialTransformer, self).__init__()\n \n \n \n  def forward(self, right_input, disparity_samples):\n \n  B, C, H, W = right_input.shape\n \n  device = right_input.get_device()\n \n  left_y_coordinate = torch.arange(0.0, W).repeat(H)\n \n \n \n  left_y_coordinate = left_y_coordinate.view(H, W)\n \n \n \n  left_y_coordinate = left_y_coordinate.unsqueeze(0)\n \n  left_y_coordinate = left_y_coordinate.unsqueeze(0)\n \n  \n \n \n \n  right_y_coordinate = left_y_coordinate + disparity_samples\n \n \n \n  right_y_coordinate_a = torch.floor(right_y_coordinate)\n \n  right_y_coordinate_b = right_y_coordinate_a + 1\n \n \n \n  wa = right_y_coordinate_b - right_y_coordinate\n \n  wb = right_y_coordinate - right_y_coordinate_a\n \n \n \n  wa = wa.repeat(1, C, 1, 1)\n \n  wb = wb.repeat(1, C, 1, 1)\n \n \n \n  right_y_coordinate_a = right_y_coordinate_a.repeat(1, C, 1, 1).long()\n \n  right_y_coordinate_b = right_y_coordinate_b.repeat(1, C, 1, 1).long()\n \n \n \n \n \n  right_y_coordinate_a = torch.clamp(right_y_coordinate_a, min=0, max= W - 1)\n \n  right_y_coordinate_b = torch.clamp(right_y_coordinate_b, min=0, max= W - 1)\n \n \n \n  warped_right_feature_map_a = torch.ones_like(right_input)\n \n  warped_right_feature_map_b = torch.ones_like(right_input)\n \n \n \n \n \n  warped_right_feature_map_a = right_input.gather(dim=3, index=right_y_coordinate_a.long())\n \n  warped_right_feature_map_b = torch.gather(right_input, dim=3, index=right_y_coordinate_b.long()) \n \n \n \n  warped_right_feature_map = wa * warped_right_feature_map_a + wb * warped_right_feature_map_b\n \n \n \n  right_y_coordinate_1 = right_y_coordinate.repeat(1, C, 1, 1)\n \n  \n \n  warped_right_feature_map = (1.0 - ((right_y_coordinate_1 < 0).float() +\n \n  (right_y_coordinate_1 > torch.tensor([W - 1], dtype=torch.float32))).float()) * \\\n \n  (warped_right_feature_map) + torch.zeros_like(warped_right_feature_map)\n \n \n \n  print(\"new stn\")\n \n  return warped_right_feature_map \n \n ```",
      "y": "With the \"verbose = True\", torch.gather(right_input,dim=3, index=right_y_coordinate_a.long()) ."
   },
   {
      "x": "BrokenPipeError on Windows when setting the num_workers as 4 in DataLoader",
      "z": "Re-opening while the discussion goes on.",
      "y": "Increase the size of the page file."
   },
   {
      "x": "torch.as_tensor returns different value for 'is_leaf' for different dtypes",
      "z": "isn't this expected? it is a noop for all but the 3rd usage.",
      "y": "It is a noop for all but the 3rd usage."
   },
   {
      "x": "torch.no_grad() context manager seems to leak memory",
      "z": "The decorator `torch.no_grad` effectively creates a new function:\n \n ```python\n \n def decorate_no_grad(*args, **kwargs):\n \n  with torch.no_grad():\n \n  return apply_model(*args, **kwargs)\n \n ``` \n \n \n \n Now since `apply_model` is a generator function, it doesn't actually perform any computation when called. Instead it immediately returns a python generator object. i.e. the `torch.no_grad` context ends before any computation is done.\n \n \n \n It may be possible to detect that `apply_model` is a generator function and instead produce something like:\n \n ```python\n \n def generator_no_grad(*args, **kwargs):\n \n gen = apply_model(*args, **kwargs)\n \n while True:\n \n  with torch.no_grad():\n \n  try:\n \n  yield next(gen)\n \n  except StopIteration:\n \n  break\n \n ```",
      "y": "Since `apply_model` is a generator function, it doesn't actually perform any computation when called. Instead it immediately returns a python generator object. i.e. the `torch.no_grad` context ends before any computation is done."
   },
   {
      "x": "cuDNN convolution memory usage",
      "z": "For your testing purposes the workaround is to set `torch.backends.cudnn.deterministic=True` (and `benchmark=False`), it will pick implicit gemm algorithm that is 1) supported for all cases 2) uses relatively little memory. It is also a nice fallback for your second question, as far as I understand there should not be a situation where implicit gemm algorithm is not applicable. I thought pytorch should fall back to it when it fails to allocate requested workspace but looks like I'm wrong.",
      "y": "Set `torch.backends.cudnn.deterministic=True` (and `benchmark=False`)."
   },
   {
      "x": "PyTorch nightly fails on Windows",
      "z": "Duplicate of https://github.com/pytorch/pytorch/issues/31370.",
      "y": "Make fully_qualified_type_name_impl() compatible with VS2017 15.9"
   },
   {
      "x": "RuntimeError: Error in dlopen or dlsym: libcaffe2_nvrtc.so: cannot open shared object file: No such file or directory",
      "z": "I found a solution: include the lib path of anaconda into LD_LIBRARY_PATH before running pycharm.\n \n \n \n **For my own case:** run the following code line in the command line.\n \n \n \n export LD_LIBRARY_PATH=/data/xiaoshua/miniconda3/lib/python3.7/site-packages/torch/lib:$LD_LIBRARY_PATH\n \n \n \n **For your case:** change the path of \"/data/xiaoshua/miniconda3/\" to your own directory. Then, run the above export code line.",
      "y": "Include the lib path of anaconda into LD_LIBRARY_PATH before running pycharm."
   },
   {
      "x": "fused multiply add returns wrong result for bool tensor",
      "z": "The actual and expected result are the same? I think the expected result should be Falses.",
      "y": "Enabled 'add_cuda' for bool and fixed alpha scalar bug."
   },
   {
      "x": "RuntimeError when using multiple DistributedDataParallel model",
      "z": "Is there any way to see which parameters on earth are unused? It is useless to simply give this RuntimeError or set find_unused_parameters to True.",
      "y": "train your model on a single node without the DDP wrapper. after loss.backward() and before optimizer.step() call add the below lines\n \n ```for name, param in model.named_parameters():\n  if param.grad is None:\n  print(name)```\n This will print any param which did not get used in loss calculation, their grad will be None."
   },
   {
      "x": "2 tensors for the output...Abou the libtorch | torch.jit.trace",
      "z": "Since you are returning two Tensors, the result value is actually going to be a Tuple, not a Tensor, so toTensor will fail. You can call `toTuple` to get the Tuple object and then extract the tensor:\n \n ```\n \n auto output = module->forward(inputs);\n \n auto t = output.toTuple()->elements()[0].toTensor(); // 1st output;\n \n ```",
      "y": "Call `toTuple` to get the Tuple object and then extract the tensor."
   },
   {
      "x": "torch.nn.functional.gumbel_softmax yields NaNs",
      "z": "I ran into the same issue after installing the pip wheel https://download.pytorch.org/whl/cu100/torch-1.1.0-cp36-cp36m-linux_x86_64.whl\n \n Forking and making the changes proposed by @vlievin worked around the issue.",
      "y": "gumbel_softmax stability issue."
   },
   {
      "x": "Reduction operations magical auto-casting non documented",
      "z": "We have (since 0.4 or 1.0) type promotion for reduction, so the result of summing any integer type (including `uint8`) returns a `int64` by default.\n \n \n \n This is the same behavior as in numpy.\n \n \n \n If you want to return the same type, you can specify a `dtype` in the function.\n \n \n \n ---\n \n tl;dr: the docs might be outdated, but the behavior is intended",
      "y": "Specify a `dtype` in the function."
   },
   {
      "x": "Pytorch 1.1 with distributed data parallel",
      "z": "Yes, `export OMP_NUM_THREADS=1` works for me.",
      "y": "`export OMP_NUM_THREADS=1` ."
   },
   {
      "x": "Conv3d fails with bigger batch size",
      "z": "We can work around this in pytorch, by splitting a single convolution into a few calls, each with less than 2**31 elements. One of our team members will work on this.",
      "y": "Split a single convolution into a few calls, each with less than 2**31 elements."
   },
   {
      "x": "RuntimeError: CUDA error: invalid configuration argument",
      "z": "Same issue with the launch config, I should have done a better job with my last PR :man_facepalming:\n \n \n \n Although I added striding on channel dimension, I forgot to limit the launch config on GPU *grid* properties (https://docs.nvidia.com/cuda/cuda-runtime-api/structcudaDeviceProp.html#structcudaDeviceProp_192d195493a9d36b2d827aaf3ffd89f1e)\n \n \n \n Will tag my PR shortly.",
      "y": "Add striding on channel dimension and limit the launch config on GPU *grid* properties."
   },
   {
      "x": "nn.parallel.DistributedDataParallel nccl backend multiple-gpu multiple-node deadlock",
      "z": "There is a known problem with NCCL 2.4.2 where it locks up. You can try exporting the environment variable `NCCL_LL_THRESHOLD=0` to see if that fixes it. Otherwise, please try a newer version of PyTorch (e.g. the nightlies) that uses a newer version of NCCL where that particular bug is fixed.\n \n \n \n You can also try running with the `gloo` backend to double check there is nothing wrong with your code.",
      "y": "Export the environment variable `NCCL_LL_THRESHOLD=0`"
   },
   {
      "x": "DataLoader runs too many threads",
      "z": "First of all, I want to apologize for an inaccuracy in my previous reply. The threads (apart from the few `Queue` putter threads) could come from either the sampler or the possible copy in `pin_memory`. \n \n \n \n Both the sampler and the copy, if they actually occur, are necessary. So there is no such thing as slowing data processing down. Sampler initialization is one time at construction time, and the copy is after fetching each batch. They are not blocking data processing because they are part of the process of fetching data.",
      "y": "Both the sampler and the copy, if they actually occur, are necessary. So there is no such thing as slowing data processing down. Sampler initialization is one time at construction time, and the copy is after fetching each batch. They are not blocking data processing because they are part of the process of fetching data."
   },
   {
      "x": "SyncBatchNorm test mode",
      "z": "In triage, we decided that this relaxation seemed like a reasonable to do.\n \n \n \n (Unrelated to this issue: @fmassa was wondering if there wasn't a reason why BatchNorm didn't just take a sync argument. @mrshenli will investigate.)",
      "y": "If it is just for local test, you can initialize the process group using `world_size=1`."
   },
   {
      "x": "RuntimeError: expected backend CPU and dtype Double but got backend CPU and dtype Float - in torch normalize function",
      "z": "This has been fixed in the master version of torchvision",
      "y": "Fixed in the master version of torchvision."
   },
   {
      "x": "Batched Triu And Tril Incorrect for Some Inputs",
      "z": "I'm able to reproduce the issue on CPU. Working on fixing it.",
      "y": "Fix behavior of `torch.triu` / `torch.tril` on certain unsqueezed tensors that lead to uninitialized values on CPU."
   },
   {
      "x": "Drop 'add extra samples to make it evenly divisible' constraint in DistributedSampler",
      "z": "I tried doing what you proposed in `maskrcnn-benchmark`, but it used to give deadlocks (at least with an older version of PyTorch and DDP), precisely because some GPUs would compute a different number of forward operations.\n \n \n \n This might have been fixed since then, but I believe what we need to have is a set of evaluation metrics which work on distributed settings (like the one I have in `torchvision/references/detection` for COCO).",
      "y": "Have a set of evaluation metrics which work on distributed settings (like the one I have in `torchvision/references/detection` for COCO)."
   },
   {
      "x": "New VSX support breaks compilation if using g++ v7 (ppc64le)",
      "z": "for example. I believe for vec_signed it should be sufficient defining them this way:\n \n ```\n \n #if !defined(vec_signed)\n \n \n \n  vint32 vec_signed(const vfloat32& vec_in) {\n \n  vint32 vec_out;\n \n  __asm__(\"xvcvspsxws %x0,%x1\" : \"=wa\"(vec_out) : \"wf\"(vec_in));\n \n  return vec_out;\n \n }\n \n \n \n  vint64 vec_signed(const vfloat64& vec_in) {\n \n  vint64 vec_out;\n \n  __asm__(\"xvcvdpsxds %x0,%x1\" : \"=wa\"(vec_out) : \"wd\"(vec_in));\n \n  return vec_out;\n \n }\n \n #endif\n \n ```\n \n ```\n \n wa - any vsx register\n \n wf - vsx float\n \n wd - vsx double\n \n %x - vsx modifier\n \n mostly asm operations follow this:\n \n \n \n  asm_op out_reg, input_reg\n \n __asm__( \"asm_op %x0, %x1\" : \"=wa\"(output): \"wa\"(input));\n \n ```\n \n [godbolt example for vec_signed](https://godbolt.org/z/n4MjbP)",
      "y": "Inline in `vsx_helpers.h` is simply missing the `return vec_out;`"
   },
   {
      "x": "CUDNN_STATUS_EXECUTION_FAILED when using AMP",
      "z": "I ran your code for 7h on an RTX2080Ti using the PyTorch `1.7.1` conda binary with CUDA10.2 and unfortunately cannot reproduce this issue.\n \n Have you had a chance to run it on bare metal without mps?",
      "y": "Faulty kernel might be in cudnn7.6.5."
   },
   {
      "x": "distributions.Independent does not correctly update distribution's support",
      "z": "Actually I'm just about to push a fix, as part of an unrelated effort...",
      "y": "Fix a number of inconsistencies in torch.distributions.constraints as used for parameters and supports of probability distributions.\n \n 1. Add a `constraints.independent` and replaces `real_vector` with `independent(real, 1)`. (this pattern has long been used in Pyro)\n 2. Add an `.event_dim` attribute to all constraints.\n 3. Test that `constraint.check(data)` has the correct shape. (Previously the shapes were incorrect).\n 4. Add machinery to set `static .is_discrete` and `.event_dim` for `constraints.dependent`.\n 5. Fix constraints for a number of distributions."
   },
   {
      "x": "torch.nn.parallel.scatter_gather.gather cannot handle namedtuple as output",
      "z": "cc @rohan-varma Since this seems like an enhancement on top of https://github.com/pytorch/pytorch/pull/44220.",
      "y": "Fix torch.nn.parallel.scatter_gather.gather to handle NamedTuples and handle moving output to CPU."
   },
   {
      "x": "Make it easier to run mypy locally",
      "z": "> @rgommers Good point, sorry for my unclear wording; what I meant to say was that that wiki page doesn't have any instructions for editor integration like the Flake8 page does. Should I add VS Code instructions to that page, or to the [Lint as you type](https://github.com/pytorch/pytorch/wiki/Lint-as-you-type) page, or neither?\n \n \n \n I don't have much of a preference here. There's so many editors that I typically don't add editor-specific info to development guides, but if you think it's useful I'd say please go ahead and add them I'd say. The type annotation page looks like the more appropriate page.\n \n \n \n > @rgommers Those timings are surprising, since I am using an SSD (relatively recent 16\" MacBook Pro). Are there any other possible causes? I have the repository checked out with all submodules (recursively); does that affect the timing?\n \n \n \n Perhaps build differences - I usually build with `USE_DISTRIBUTED=0`, `USE_QNNPACK=0`, `USE_MKLDNN=0`, `USE_FBGEMM=0`, `USE_NNPACK=0`. It would be surprising to me if that would be it, but if it is then maybe it's fixable.\n \n \n \n > I do agree that the page should be linked from `CONTRIBUTING.md`, so I just opened #50540 to do that.\n \n \n \n Thanks!",
      "y": "Build with `USE_DISTRIBUTED=0`, `USE_QNNPACK=0`, `USE_MKLDNN=0`, `USE_FBGEMM=0`, `USE_NNPACK=0`."
   },
   {
      "x": "rpc memory leak",
      "z": "Looks like the memory leak is happening due to the timeoutMap_: https://github.com/pytorch/pytorch/blob/master/torch/csrc/distributed/rpc/tensorpipe_agent.cpp#L727. We add an entry to timeoutMap_ on RPC send, but once we receive a response we never remove it from the map. The entire timeout duration needs to expire before we actually end up removing it from the map.",
      "y": "Add an entry to timeoutMap_ on RPC send, but once we receive a response we never remove it from the map. The entire timeout duration needs to expire before we actually end up removing it from the map."
   },
   {
      "x": "When I was compiling, these errors appeared, what should I do",
      "z": "Can you please share the full build log (please `rm -rf build` folder before running `setup.py` again)",
      "y": "Switch to 1.7."
   },
   {
      "x": "The computation of weighted CrossEntropyLoss is not reasonable",
      "z": "Yes. I can work on it. Thank you.",
      "y": "Enable min & max for Float16 & BFloat16."
   },
   {
      "x": "`ATen/cpu/vec256/` should be a header only library for all but quantized types",
      "z": "Also, removing triage review since there are already a PR that addresses the issue.",
      "y": "Make Vec256 header only library."
   },
   {
      "x": "torch.linalg.qr ignores zero batched dimensions",
      "z": "probably it's obvious, but it's worth noting that `torch.qr` exhibits the same behavior (not surprising, since one is implemented on top of the other):\n \n ```pycon\n \n >>> import torch\n \n >>> A = torch.randn(shape)\n \n >>> Q, R = torch.qr(A)\n \n >>> Q.shape\n \n torch.Size([4, 4])\n \n >>> R.shape\n \n torch.Size([4, 5])\n \n ```",
      "y": "Fix MAGMA qr for empty batched inputs."
   },
   {
      "x": "Learning rate scheduler in C++ API",
      "z": "Sry about the delay, I was sick during the weekend and probably need rest today as well. I will go over this code no later then tomorrow and update here. Then we can start an official PR to the pytorch repo.\n \n \n \n @jamesbut @ssbotelh \n \n pytorch 1.8 deadline is already passed. I am targeting to include this into pytorch 1.9. But welcome to use it immediately once it is landed into pytorch master repo.",
      "y": "Implement ReduceLROnPlateau and StepLR."
   },
   {
      "x": "Add a `vectorize` flag to torch.autograd.functional.{jacobian, hessian}",
      "z": "> One question: do we need to do special handling when the `create_graph=True` flag is set?\n \n \n \n Other than setting create_graph=True when we call autograd.grad, no, I don't think so.",
      "y": "Set `create_graph=True` when we call autograd.grad."
   },
   {
      "x": "Add batched grad checking to Opinfo",
      "z": "Initial support added in #50818",
      "y": "1. add new `check_batched_grad=True` and `check_batched_gradgrad=True`\n attributes to OpInfo. These are `True` by default because we expect most\n operators to support batched gradient computation.\n 2. If `check_batched_grad=True`, then `test_fn_grad` invokes gradcheck\n with `check_batched_grad=True`.\n 3. If `check_batched_gradgrad=True`, then `test_fn_gradgradgrad` invokes\n `gradgradcheck` with `check_batched_grad=True`."
   },
   {
      "x": "C++ API: `at::empty(size)` modifies `size` ArrayRef in-place when it owns the ArrayRef",
      "z": "In particular, the reference may not even be valid to dereference after assignment. I think ASAN should complain if, for example, you resize from dim() greater than 5 to, say, dim() 3.",
      "y": "Output from `t.sizes()` is `IntArrayRef` which is a reference type, so shouldn't really have expected this to work. Re-assigning the tensor shouldn't provide any guarantees about whether or not the reference returned from t.sizes() gets modified."
   },
   {
      "x": "Don't support torch.Size() in the script",
      "z": "BTW, the main reason why we created torch.Size was to be able to use the legacy torch.Tensor constructor, which was overloaded on size / data.\nNow that we are mainly moving towards having a data constructor torch.tensor and a size constructor torch.empty, I believe we could deprecate at some point torch.Size, and once (if) we drop support for torch.Tensor altogether, we can probably just make tensor.size() return a tuple as well.",
      "y": "By the way, this is the major reason we built torch. To be able to use the legacy torch, you needed to be of a certain size. The tensor function Object() { [native code] } was overloaded in terms of size and data.\nNow that we have a data function Object() { [native code] } torch.tensor and a size function Object() { [native code] } torch, we are mostly heading towards having a data function Object() { [native code] } torch.tensor and a size function Object() { [native code] } torch.\nI believe we could deprecate torch at some point.\nSize, and when (if) we eliminate torch support.\nWe can probably just make tensors as a whole."
   },
   {
      "x": "opening nn page is very slow",
      "z": "For example, see https://pytorch.org/docs/stable/nn.html#torch.nn.functional.normalize\n\nTwo reasons:\n\nIt renders a lot of mathjax.\nWe put both modules and functionals on the same html.\nCan we speed it up possibly with some refactoring?",
      "y": "https://pytorch.org/docs/stable/nn.html#torch.nn.functional.normalize as an example.\nThere are two factors at play:\nA lot of mathjax is rendered.\nOn the same html, we included both modules and functionals.\nIs it possible to refactor it to make it go faster?"
   },
   {
      "x": "Add document page for torch.distributed.rpc/autograd",
      "z": "For 2, I think since that rpc and dist autograd interact a lot together and a user would probably use both of them if they were using one, they should live in the same page",
      "y": "For 2, I believe that since rpc and dist autograd interact so much and a user would almost certainly use both if they only used one, they should be on the same page."
   },
   {
      "x": "[jit] support self._modules in TorchScript",
      "z": "_modules itself shouldn't be exposed, but the accessor methods (e.g. modules(), named_children(), named_modules(), children() should be\n\nmost of these are iterators unfortunately.. Maybe we should override them in ScriptModule and return list instead.",
      "y": "The accessor methods (e.g. modules(), named children(), named modules(), children()) should be provided, but not _modules itself.\n\nUnfortunately, the majority of these are iterators. Perhaps we should override them in ScriptModule and instead return a list."
   },
   {
      "x": "Proposal to change the offsets for the EmbeddingBag operator in PyTorch",
      "z": "Yeah, we need a new flag in the frontend to have this new change. And this new change is not just for convenience. To lower this op to accelerator, we usually create a larger buffer and zero pad. For example, we want to look up indices [0, 1, 2, 3], but since we allocate the indices as max size of 8, the input to accelerator becomes [0, 1, 2, 3, 0, 0, 0, 0]. Since 0 is a valid lookup index, we don't know where we are going to end just from current last offset. One argument is that we can pad something like -1 to have [0, 1, 2, 3, -1, -1, -1, -1]. But padding -1 is probably not as efficient and we have to add more branching logic to check whether we hit -1 or not.",
      "y": "To implement this new change, we'll need a new flag in the frontend. This new move isn't merely for the sake of convenience. We normally add a larger buffer and zero pad to reduce this op to accelerator. For example, we want to look up the indices [0, 1, 2, 3], but because the indices have a maximum size of 8, the input to the accelerator is [0, 1, 2, 3, 0, 0, 0, 0]. We don't use 0 because it's a valid lookup index."
   },
   {
      "x": "register_forward_hooks (and register_backward_hooks) support for TorchScript.",
      "z": "Thanks for the feature request! We plan to implement hooks in the near future, but I don't have an exact timeline yet. I'll update this ",
      "y": "Thank you for requesting a feature!\nWe intend to implement hooks in the near future, but I don't know when that will be.\nI'll keep this updated. "
   },
   {
      "x": "PyTorch deadlocks when freeing memory in embedded Pybind interpreter",
      "z": "I was talking about GIL deadlocks and I noticed a few bugs in the original example that lead to the deadlock (or crashes in some case). Additionally, pybind11 v.2.3.0 (used in the repro) had a [critical bug](https://github.com/pybind/pybind11/issues/1364) that would lead to crashes even when the example is fixed. You'll need to use at least v.2.4.0 (I tested with v2.5.0).\n\nHere is my fixed version of main.cpp:\n\n```c++\n\n#include <pybind11/embed.h>\n\n#include <thread>\n\n\n\nnamespace py = pybind11;\n\n\n\nint main() {\n\n    py::scoped_interpreter guard{};\n\n    auto threading = py::module::import(\"threading\"); // (1)\n\n    auto thread = std::thread([]() {\n\n      py::gil_scoped_acquire acquire; // (2)\n\n      auto hello = py::module::import(\"hello\");\n\n      py::object a =  hello.attr(\"A\")();\n\n      a.attr(\"run\")();\n\n    });\n\n    py::gil_scoped_release release;  // (3)\n\n    thread.join();\n\n}\n\n1) Python 3.7 expects that the threading module is initialized before additional threads are created. Otherwise you get an error `assert tlock.locked()` on exit.\n\n2) The C++ thread must acquire the GIL before calling into Python.\n\n3) The main thread should release the GIL before calling the blocking operation `thread.join()`. Otherwise (2) will never be able to acquire the GIL.\n\nThese three changes (combined with the updated pybind11) fix the deadlocks in the example below",
      "y": "I was discussing GIL deadlocks when I realised a couple flaws in the original example that resulted in the deadlock (or crashes in some case).\nAdditionally, even after the example was patched, pybind11 v.2.3.0 (used in the repro) had a [major bug](https://github.com/pybind/pybind11/issues/1364) that caused crashes.\nYou'll need at least version 2.4.0. (I tested with v2.5.0).\n\n\n\nHere is my fixed version of main.cpp:\n\n```c++\n\n#include <pybind11/embed.h>\n\n#include <thread>\n\n\n\nnamespace py = pybind11;\n\n\n\nint main() {\n\n    py::scoped_interpreter guard{};\n\n    auto threading = py::module::import(\"threading\"); // (1)\n\n    auto thread = std::thread([]() {\n\n      py::gil_scoped_acquire acquire; // (2)\n\n      auto hello = py::module::import(\"hello\");\n\n      py::object a =  hello.attr(\"A\")();\n\n      a.attr(\"run\")();\n\n    });\n\n    py::gil_scoped_release release;  // (3)\n\n    thread.join();\n\n}"
   },
   {
      "x": "initializedContextIds_ field in DistEngine grows without bound",
      "z": " The \"high priority\" label means it goes to triage review (i.e. is the right person working on this issue right now). I doubt that was your intention here.\n\nIs there a better label for per-(module|topic|queue) prioritization? ",
      "y": "The designation of \"high priority\" denotes that it will be reviewed by a triage team (i.e. is the right person working on this issue right now).\nThat wasn't your aim, I'm sure.\n\n\nIs there a better term for prioritisation by (module|topic|queue)? "
   },
   {
      "x": "[Feature request] Provide binaries for Python 3.8",
      "z": "Python is upgraded to 3.8 in arch linux based distros recently, I suppose many arch or manjaro users are waiting for 3.8 binaries.",
      "y": "Python 3.8 was recently upgraded on arch linux based distros, and I'm sure many arch or manjaro users are eagerly awaiting 3.8 binaries. "
   },
   {
      "x": "magma functionality isn't working on K80 GPU with official binaries",
      "z": "the torch.solve thing does work!",
      "y": "The torch.solve method actually works! "
   },
   {
      "x": "Indexed assignment of quantized Tensors yields unexpected results",
      "z": "copy_and_clobber_?",
      "y": "copy and clobber_? "
   },
   {
      "x": "RPC tests are flaky",
      "z": "Turning this issue into a tracking issue, let's continue to post new examples of flakiness here to help ",
      "y": "To assist turn this into a tracked issue, let's keep posting additional cases of flakiness here. "
   },
   {
      "x": "Windows CI build is flaky: cannot delete workspace... because it is being used by another process",
      "z": "The plan on record is to move these CI jobs to CircleCI and then we kill the Jenkins job.",
      "y": "On paper, the plan is to migrate these CI jobs to CircleCI, then kill the Jenkins job."
   },
   {
      "x": "Gradle build is flaky: \"could not get resource\"",
      "z": "Working on it.",
      "y": "I'm working on it right now. "
   },
   {
      "x": "PytorchStreamWriter failed opening archive.",
      "z": "tried change the saving path worked for me",
      "y": "Changed the save route and it worked for me. "
   },
   {
      "x": "Blank value for _GLIBCXX_USE_CXX11_ABI when compiling with Homebrew's GCC (on macOS)",
      "z": "I'm not sure we support building with gcc on mac os, we generally recommend using clang on mac. (see instructions at https://github.com/pytorch/pytorch)",
      "y": "We don't think we support building with gcc on Mac OS X; instead, we propose clang.\n(See https://github.com/pytorch/pytorch for instructions) "
   },
   {
      "x": "CPU memory leak when using torch.no_grad()",
      "z": "Cross post from the forum: https://discuss.pytorch.org/t/memory-leak-if-use-torch-no-grad/97484\n",
      "y": "https://discuss.pytorch.org/t/memory-leak-if-use-torch-no-grad/97484"
   },
   {
      "x": "test_torch.py/test_inverse_cuda causes illegal memory access on some platforms",
      "z": "The PR has been merged, so it should solve the problem you saw on 10.1.105. If you have verified that and don't see a crash elsewhere, feel free to close it. Thanks! ",
      "y": "The PR has been merged, therefore it should fix the issue you were experiencing on 10.1.105.\nIf you've double-checked that and don't see any other crashes, feel free to close it.\nThanks! "
   },
   {
      "x": "Why doesn't amp improve training speed",
      "z": "See\n\nhttps://pytorch.org/tutorials/recipes/recipes/amp_recipe.html#speedup-with-amp-is-minor\n\nhttps://pytorch.org/tutorials/recipes/recipes/tuning_guide.html\n\nfor general and amp guidance.\n\n\n\nIf your model/batch size are small and achieve low device utilization, amp won't help, and may hurt.  Also, if your GPU is not Tensor Core enabled (in other words if it's Pascal or earlier) amp isn't expected to deliver much speedup.  What model GPU are you using?\n\n\n\nUNet has 3D convolutions (right?) so it's also possible you're hitting bad cudnn heuristics.  `torch.backends.cudnn.benchmark = True` at the top of your script may help.  If you find a particular layer/conv shape that performs poorly with FP16 even after `benchmark = True` I will report it to cudnn devs.\n\n",
      "y": "See\n\n\nhttps://pytorch.org/tutorials/recipes/recipes/amp recipe.html#speedup-with-amp-is-minor\n\n\nhttps://pytorch.org/tutorials/recipes/recipes/tuning guide.html\n\n\nfor general and amplification advice\n\n\n\n\nAmp won't assist, and may hurt, if your model/batch size is tiny and you obtain poor device use.\nAlso, amp isn't expected if your GPU isn't Tensor Core enabled (in other words, if it's Pascal or before). "
   },
   {
      "x": "torch.autograd.profiler.load_nvprof() crash",
      "z": "PR  #45004 fixes the issue, will pick relevant parts of the fix into 1.7",
      "y": "PR #45004 resolves the issue; relevant parts of the patch will be incorporated into 1.7."
   },
   {
      "x": "Wrong signature for torch.max_pool(1d, 2d, 3d) in get_testing_overrides()",
      "z": "I believe one was to be created, but I don't know if it has been yet. But if it isnot, `module: numpy` is what we have",
      "y": "\nI assume one was supposed to be made, but I'm not sure if it has yet.\nHowever, if it isn't, we have module: numpy. "
   },
   {
      "x": "JIT Profiling executor is not fusing Dropout while legacy executor does",
      "z": "Just to motivate the problem, in case you don't have the context, the dropout operator is heavily used in Transformer/Bert NLP networks and being able to fuse to dropout is an important perf opportunity in those networks.",
      "y": "To give you a sense of the difficulty, the dropout operator is widely utilised in Transformer/Bert NLP networks, and being able to fuse to dropout is a significant efficiency opportunity in those networks. "
   },
   {
      "x": "Tensors with complex dtypes fail to be cloned contiguously",
      "z": "Hi,\nYes this design difference is done on purpose.\n\nThe plan we have to solve this issue is to build on top of https://github.com/pytorch/pytorch/issues/49171 so that the user can give the weights as proper inputs to the function and thus work well with the function autograd API.",
      "y": "Hi,\n\nYes, the design contrast is deliberate.\n\nThe goal we have to fix this problem is to build on top of https://github.com/pytorch/issues/49171 so that the user may submit the weights as suitable inputs to the function and therefore interact nicely with the autograd API. "
   },
   {
      "x": "No documentation for `torch.sgn`",
      "z": "It is actually documented but it's missing an torch.rst entry so it doesn't appear in the built docs. See here:\nhttps://github.com/pytorch/pytorch/blob/68a6e4637903dba279c60daae5cff24e191ff9b4/torch/_torch_docs.py#L7517\n\n\n\n",
      "y": "It is documented, but the torch.rst entry is missing, therefore it does not appear in the built docs.\nHere's an example:\n\nhttps://github.com/pytorch/pytorch/blob/68a6e4637903dba279c60daae5cff24e191ff9b4/torch/ torch docs.py#L7517 "
   },
   {
      "x": "libtorch_cuda.so is missing fast kernels from libcudnn_static.a, therefore statically linked cuDNN could be much slower than ",
      "z": "So, does this issue explain why NGC containers have been consistently faster than the official conda builds for a number of PyTorch versions now? NGC == dynamic link, conda/pip = static w/ this issue? This has pretty significant impact if that is the case.\n\n\n\nI ran some benchmarks trying to figure out what was happening as I've kept bumping into it with new releases... https://gist.github.com/rwightman/bb59f9e245162cee0e38bd66bd8cd77f",
      "y": "Is this the reason why NGC containers have consistently outperformed official conda builds for a number of PyTorch versions?\nWith this issue, NGC == dynamic link, conda/pip = static.\nIf that is the case, it has a huge influence.\n\nI performed some benchmarks to see what was going on because I kept running into it with fresh releases...\nhttps://gist.github.com/rwightm "
   },
   {
      "x": " Broken formatting for the function signature of `torch.randperm`",
      "z": "We would accept a PR to fix this (unless it turns out that there's a reason why this isn't documented)",
      "y": "We'd accept a PR to address this (unless there's a reason why this isn't documented). "
   },
   {
      "x": "Broken formatting on `torch.repeat_interleave`",
      "z": "We would accept a PR to fix this.",
      "y": "We'd be happy to take a PR to address this. "
   },
   {
      "x": "Add torch.linalg.vector_norm and torch.linalg.matrix_norm functions",
      "z": "All vector norms and frobenius norm are implemented in the same way, via reduction. For matrix norm indeed splitting into separate functions might make sense, but even with that I'd leave  +/-inf and +/-one matrix norm in the same function, they share a lot of implementation. That leaves nuclear norm and +/-2-norms that could also live together because they both rely on first computing svd. Question: why is frobenius norm implemented as \n\n```\n\n    if (self.is_complex()){\n\n      result_ = at::sqrt(at::sum(at::real(self.conj() * self), dim_, keepdim));\n\n    } else {\n\n      result_ = at::sqrt(at::sum((self * self), dim_, keepdim));\n\n    }\n\n``` \n\n? This is super inefficient compared to vector norm, and produces the same result.    ",
      "y": "The reduction method is used to implement all vector norms and frobenius norms.\nSplitting matrix norm into separate functions makes sensible, however I'd still keep +/-inf and +/-one matrix norm in the same function because they share a lot of implementation.\nThat leaves nuclear norm and +/-2-norms, which can coexist because they both employ svd as the first computation step. What is the purpose of Frobenius norm? ```\n\n    if (self.is_complex()){\n\n      result_ = at::sqrt(at::sum(at::real(self.conj() * self), dim_, keepdim));\n\n    } else {\n\n      result_ = at::sqrt(at::sum((self * self), dim_, keepdim));\n\n    }\n\n``` \n\n? This is super inefficient compared to vector norm, and produces the same result.    "
   },
   {
      "x": "Let loss_func(empty_inputs, reduction=\"mean\") return 0",
      "z": "I do agree that mean should return `nan` for empty Tensors and it is consistent with numpy:\n",
      "y": "I agree that for empty Tensors, mean should return nan, and this is compatible with numpy: "
   },
   {
      "x": "Can you add higher order derivative support for torch's embedding function? ",
      "z": "There are known issues in building PyTorch with CUDA11.2 we are facing now regarding the functionality as well as performance. We are actively working with the CUDA team to isolate and fix this issues. Until we have resolutions we discourage users to use CUDA 11.2 with PyTorch.",
      "y": "There are known concerns with creating PyTorch with CUDA11.2 that we are currently experiencing in terms of functionality and performance.\nWe're working closely with the CUDA team to isolate and resolve these issues.\nUsers should avoid using CUDA 11.2 with PyTorch until we have resolutions. "
   },
   {
      "x": "Discuss options for implementing einsum_path",
      "z": "I've never seen `einsum_path` used much, I would not add it as a separate function. Adding only the `optimize` keyword makes ",
      "y": "I've never seen einsum path used very often, so I wouldn't include it as a separate function.\nThe keyword \"optimise\" is all that is needed. "
   },
   {
      "x": "Docs build failures are difficult to identify",
      "z": "The easiest thing to do would be to improve the error message at the end of the build step, either in the `Makefile` or in the circleCI script that calls `make html` to state \"if this step fails, look back in the log for WARNING: (in capital letters)\". A more advanced solution would be to do something like\n\n```\n\nset -o pipefail\n\nmake html | tee /tmp/doc_build.log\n\nif [ $? -ne 0 ]; then \n\n    echo \"doc build error detected:\"; \n\n    grep WARNING /tmp/doc_build.log;\n\nfi\n\n```\n\n\n\nI wonder if there is a way to write a test to make sure it works as advertised",
      "y": "The simplest solution is to change the error message at the conclusion of the build step, either in the Makefile or in the circleCI script that calls make html, to say \"if this step fails, look back in the log for WARNING: (in capital letters)\".\nSomething like this would be a more advanced solution.\n"
   },
   {
      "x": "Categorical logits argument is treated as log probabilities",
      "z": "Thanks for the clarification. I never really thought about the inconsistency between binary and multinomial logits and that tripped me up.\n\nPerhaps the documentation could be clearer on this point as logits refer to different things in `torch.logit` and the `logits` argument to `Categorical`.",
      "y": "Thank you for clarifying.\nThe discrepancy between binary and multinomial logits had never occurred to me before, and it had tripped me up.\n\n\nBecause logits relate to different objects in torch.logit and the logits parameter to Categorical, perhaps the documentation could be clearer on this point. "
   },
   {
      "x": "`Expected self.scalar_type() == ScalarType::Float to be true, but got false.` when doing quantization aware training?\n",
      "z": "Thanks for the clarification. I never really thought about the inconsistency between binary and multinomial logits and that tripped me up.\n\nPerhaps the documentation could be clearer on this point as logits refer to different things in `torch.logit` and the `logits` argument to `Categorical`.",
      "y": "logits relate to different objects in torch.logit and the logits parameter to Categorical, perhaps the documentation could be clearer on this point"
   },
   {
      "x": "Casting from long to int may cause wrong result",
      "z": "The code still will have an issue. You will need to use an intermediate Tensor variable like\n```\nauto int_t = t.to(at::kInt);\n\nauto p = int_t.data_ptr<int>();",
      "y": "There will still be a problem with the code. You'll need to utilise an intermediary Tensor variable like\nauto int_t = t.to(at::kInt);\nauto p = int t.data ptrint>();"
   },
   {
      "x": "torch.cat different Windows vs Linux behavior",
      "z": "torch.__version__\n> '1.4.0'\n torch.cat(( torch.tensor([]), torch.tensor([1,2,3], dtype=torch.long)))\nTraceback (most recent call last):\nFile \"\", line 1, in \n RuntimeError: Expected object of scalar type Float but got scalar type Long for sequence element 1 in sequence \nThis may have been fixed in v1.6.0:\nhttps://github.com/pytorch/pytorch/releases/tag/v1.6.0",
      "y": "torch. version > '1.4.0'\ntorch.cat(( torch.tensor([]), torch.tensor([1,2,3], dtype=torch.long)), torch.tensor([1,2,3], dtype=torch.long)))\n(Last call) Traceback (most recent call):\nRuntimeError: file \"\", line 1 For sequence element 1 in sequence, I expected a scalar type Float object, but instead got a scalar type Long object.\nThis issue may have been resolved in v1.6.0: https://github.com/pytorch/pytorch/releases/tag/v1.6.0\n\u00a0torch.cat has been updated to include a missing type prom."
   },
   {
      "x": "ProcessGroupNCCL::all_reduce is not profiled correctly",
      "z": "In the new PyTorch profiler (kineto based profiler), `record_function before/end` invocation only emits a host side event. GPU side events are traced in an async way via CUPTI separately. And profiler will correlate host/device events together in the end.\n\nTo align with the design principle, seems `general idea # 2` will be enough.\n> Report only the time it took to launch allreduce (i.e. enqueue it on the nccl stream). This may be reasonable if the profiler is only enabled without use_cuda=True as it gives the host time that was spent enqueueing the nccl kernel. However, this is probably not the output that the user wants if they would like to determine GPU time.\n\n@gdankel @ilia-cher for more comments.",
      "y": "The record function before/end invocation only produces a host side event in the new PyTorch profiler (kineto based profiler). CUPTI is used to track GPU side events in an async manner. In the end, the profiler will correlate host/device events.\nTo stay true to the design philosophy, it appears that broad idea #2 will suffice.\nOnly include the time it took to launch allreduce (i.e. enqueue it) in your report."
   },
   {
      "x": "\u00f0\u0178\u0090\u203a memory leaks with coalesce method",
      "z": "We would definitely accept a PR fixing these!",
      "y": "In the issue reproducer we can replace torch.sparse.sum(S) with S.coalesce() and get the same memory leak. The reason is that calling coalesce() on an already coalesced tensor returns self. With autograd, the result gets it's grad_fn set to a node that contains a reference to the input tensor, creating a reference cycle. Cloning the tensor fixes this, so coalesce always returns a new tensor.\n\nAs an aside, torch.sparse.sum(S) doesn't need to coalesce. The result should be the same either way."
   },
   {
      "x": "Impossible to export ONNX as RAW",
      "z": "That's happening in #60249",
      "y": " a copy of the site with these changes here: https://garymm.github.io/onnx.html\n\nProbably best to review that"
   },
   {
      "x": "Pytorch autograd sometimes fails with CUDNN_STATUS_INTERNAL_ERROR",
      "z": "Thanks for reporting this issue!\nI'm able to reproduce it using cudnn8.0.5 on an RTX2080Ti and this issue is apparently fixed in the cudnn8.1 release.\n\nTo test it, you could install the `1.8` release candidate via:\n```\nconda install pytorch cudatoolkit=11.2 -c pytorch-test -c conda-forge\n```\nwhich ships with cudnn8.1:\n```python\n>>> import torch\n>>> torch.__version__\n'1.8.0'\n>>> torch.version.cuda\n'11.2'\n>>> torch.backends.cudnn.version()\n8100\n```",
      "y": "Issue is reproduced using cudnn8.0.5 on an RTX2080Ti and this issue is apparently fixed in the cudnn8.1 release.\nTo test it, you could install the `1.8` release candidate via:\n```\nconda install pytorch cudatoolkit=11.2 -c pytorch-test -c conda-forge\n\nwhich ships with cudnn8.1:\n```python\n>>> import torch\n>>> torch.__version__\n'1.8.0'\n>>"
   },
   {
      "x": "[nnc][perf] 50x performance regression from `_jit_override_can_fuse_on_cpu(True)`",
      "z": "For the current behavior, I'd suggest `_jit_override_can_fuse_on_cpu(True)` throws an error if you don't have LLVM.  Once fusing becomes the default maybe we need to switch to the `NO_LLVM` flag.",
      "y": "For the current behavior, I'd suggest `_jit_override_can_fuse_on_cpu(True)` throws an error if you don't have LLVM.  Once fusing becomes the default maybe we need to switch to the `NO_LLVM` flag."
   },
   {
      "x": "tutorials :: beginner :: audio_classifier_tutorial",
      "z": "Sorry, that document was an artifact of a tutorial that was removed. I did a redirect to home. \nThanks for the report @madkote!  \nFYI @holly1238 ",
      "y": "That document was a remnant of a tutorial that was removed, so please accept my apologies. I made a home redirect."
   },
   {
      "x": "Errors in scripting when using `_pair_from_first`",
      "z": "The error being seen here is because hypothesis is trying to re-run a failing test, but once there is a jit compiler error related to the compilation of the Conv1d module, it gets left in an inconsistent state and future versions of compiling it report the 'Can't redefine method'. If you must use hypothesis in places where script compilation may fail, I suggest calling this at the start of the test to clean up the jit compilation state.\n```        \nfrom torch.testing._internal.jit_utils import clear_class_registry\nclear_class_registry()\n```",
      "y": "Hypothesis is attempting to re-run a failing test, but once there is a jit compiler error connected to the compilation of the Conv1d module, it is left in an inconsistent state, and further compilations report the 'Can't rename method' error. If you need to use hypothesis in a situation where script compilation can fail, \u00a0calling this at the start of the test to clean up the jit compilation state.\n```\nfrom torch.testing._internal.jit_utils import clear_class_registry\nclear_class_registry()"
   },
   {
      "x": "Build failure with \"TORCH_CUDA_API\" is undefined and more",
      "z": "Hi, \n\nMaybe related. I ran once into this issue when I had a different pytorch version in my PATH\n\nPATH=/third_party/libtorch/include 1.XXX   <= From my install dir\npytorch1.YYY/build$ make <= Current compile \n\n-DCMAKE_INSTALL_PREFIX=**/usr/local/src/pytorch/src/pytorch-git/torch**\n**/usr/local/src/pytorch/src/pytorch-git/torch** is your install dir. \n**/usr/local/src/pytorch/src/pytorch-git/torch**/include/THC/THCGeneral.h(39): error: identifier \"TORCH_CUDA_API\" is undefined\n\nmake clean first or remove prev install.\n\nPascal",
      "y": "PATH=/third party/libtorch/include 1.XXX=======================================\n\npytorch1.YYY/build\n\n= make $ Currently running compilation\n\n-DCMAKE INSTALL PREFIX=**/usr/local/src/pytorch/src/pytorch-git/torch**\n\nThe install directory is /usr/local/src/pytorch/src/pytorch-git/torch.\n\n**/usr/local/src/pytorch/src/p"
   },
   {
      "x": "torch.empty_like example seems wrong",
      "z": "The example presented for torch.empty_like seems wrong:\n\n>>> torch.empty((2,3), dtype=torch.int64)\ntensor([[ 9.4064e+13,  2.8000e+01,  9.3493e+13],\n        [ 7.5751e+18,  7.1428e+18,  7.5955e+18]])\nI guess the desided example would be something like:\n\n>>> a = torch.empty((2,3), dtype=torch.int64)\n>>> torch.empty_like(a, dtype=torch.float64)\ntensor([[ 9.4064e+13,  2.8000e+01,  9.3493e+13],\n        [ 7.5751e+18,  7.1428e+18,  7.5955e+18]])\ncc @brianjo @mruberry @gchanan",
      "y": "The example presented for torch.empty_like seems wrong:\n\n>>> torch.empty((2,3), dtype=torch.int64)\ntensor([[ 9.4064e+13,  2.8000e+01,  9.3493e+13],\n        [ 7.5751e+18,  7.1428e+18,  7.5955e+18]])\nI guess the desided example would be something like:\n\n>>> a = torch.empty((2,3), dtype=torch.int64)\n>>> torch.empty_like(a, dtype=torch.float64)\ntensor([[ 9.4064e+13,  2.8000e+01,  9.3493e+13],\n        [ 7.5751e+18,  7.1428e+18,  7.5955e+18]])\n"
   },
   {
      "x": "Result of  torch.Tensor.__mod__  is different from torch.fmod() function.",
      "z": "FWIW, numpy has the same semantics as we do:\n```\nimport numpy as np\nx = np.array([5., 5. -5. , -5.])\ny = np.array([3., -3., 3.,  -3.])\n\n[ins] In [9]: x % y\nOut[9]: array([ 2., -1.,  1., -2.])\n\n[ins] In [10]: np.fmod\nOut[10]: <ufunc 'fmod'>\n\n[ins] In [11]: np.fmod(x, y)\nOut[11]: array([ 2.,  2., -2., -2.])\n\n[ins] In [12]: x % y\nOut[12]: array([ 2., -1.,  1., -2.])\n\n[ins] In [13]: x %= y; print(x)\n[ 2. -1.  1. -2.]\n```",
      "y": "FWIW, numpy has the same semantics as we do:\n```\nimport numpy as np\nx = np.array([5., 5. -5. , -5.])\ny = np.array([3., -3., 3.,  -3.])\n\n[ins] In [9]: x % y\nOut[9]: array([ 2., -1.,  1., -2.])\n\n[ins] In [10]: np.fmod\nOut[10]: <ufunc 'fmod'>\n\n[ins] In [11]: np.fmod(x, y)\nOut[11]: array([ 2.,  2., -2., -2.])\n\n[ins] In [12]: x % y\nOut[12]: array([ 2., -1.,  1., -2.])\n\n[ins] In [13]: x %= y; print(x)\n[ 2. -1.  1. -2.]\n```"
   },
   {
      "x": "training hanged with torch.distributed.launch",
      "z": " The issue got resolved using `NCCL_SOCKET_IFNAME=lo` as reported [here](https://github.com/huggingface/transformers/issues/10151#issuecomment-781982908) based on this [thread](https://github.com/NVIDIA/nccl/issues/352).",
      "y": " It's working with NCCL_SOCKET_IFNAME=lo from this thread.\n\nboth of the below were working now -\n\n!NCCL_SOCKET_IFNAME=lo python -m torch.distributed.launch --nproc_per_node=1 ./Seq2Seq.py --output_dir ./out_dir/results --overwrite_output_dir --do_train \\\n--do_eval --per_device_train_batch_size 4 --per_device_eval_batch_size 4 --learning_rate 3e-5 --weight_decay 0.01 \\\n--num_train_epochs 1 --load_best_model_at_end --local_rank 0\nand\n\n!NCCL_SOCKET_IFNAME=lo deepspeed ./Seq2Seq.py --output_dir ./out_dir/results --overwrite_output_dir --do_train \\\n--do_eval --per_device_train_batch_size 12 --per_device_eval_batch_size 12 --learning_rate 3e-5 --weight_decay 0.01 \\\n--num_train_epochs 1 --load_best_model_at_end --local_rank 0 --deepspeed ds_config.json"
   },
   {
      "x": "nn.Conv1d non-deterministic behavior ",
      "z": "you created two layers with differently sampled parameters. how could the result be the same?",
      "y": "you created two layers with differently sampled parameters. how could the result be the same?"
   },
   {
      "x": "Windows CXX11-ABI libtorch debug builds for CUDA 11.1 fail with unresolved external symbol",
      "z": "@skyline75489 Yes, I guess TORCH_CUDA_SPLIT is only enabled on CUDA 11.1.",
      "y": "TORCH_CUDA_SPLIT is only enabled on CUDA 11.1."
   },
   {
      "x": "Windows libtorch nightly builds for CUDA 11.1 failing binary smoke tests with unresolved symbols",
      "z": "https://github.com/pytorch/builder/pull/661 should fix this.",
      "y": "https://github.com/pytorch/builder/pull/661 should fix this."
   },
   {
      "x": "Torchscript tutorial example is broken",
      "z": "Already fixed, waiting for review: https://github.com/pytorch/tutorials/pull/866",
      "y": "Already fixed, waiting for review: https://github.com/pytorch/tutorials/pull/866"
   },
   {
      "x": "Negative impact of Python versions on using Pytorch with Cuda11",
      "z": "> Executing these two commands\nconda create -n myenv python=3.8.5 and conda install pytorch cudatoolkit=11.0 -c pytorch results in installing pytorch=1.5.1., (not pytorch=1.7.1.) as shown below.\n\nThat's not the case for me and also it seems that `1.7.1` is installed on your system:\n```\n(myenv) datascienceadmin@limbo:~$ conda install pytorch cudatoolkit=11.0 -c pytorch\n[...]\nThe following NEW packages will be INSTALLED:\n[...]\n  pytorch            pytorch/linux-64::pytorch-1.7.1-py3.8_cuda11.0.221_cudnn8.0.5_0\n```\n\nHowever, since `1.5.1` is found in your REPL, I guess you might have multiple PyTorch versions installed, where the `1.5.1` version is picked. This would also explain the reported \"hang\", since `1.5.1` doesn't ship with CUDA11.0 and the CUDA JIT would kick in, which might be slow.\n\nI would recommend to uninstall all conda, pip, and source builds in the current environment, make sure that `import torch` fails, and reinstall `1.7.1`.\n",
      "y": "> Executing these two commands\nconda create -n myenv python=3.8.5 and conda install pytorch cudatoolkit=11.0 -c pytorch results in installing pytorch=1.5.1., (not pytorch=1.7.1.) as shown below.\n```\n(myenv) datascienceadmin@limbo:~$ conda install pytorch cudatoolkit=11.0 -c pytorch\n[...]\nThe following NEW packages will be INSTALLED:\n[...]\n  pytorch            pytorch/linux-64::pytorch-1.7.1-py3.8_cuda11.0.221_cudnn8.0.5_0\n```However, since `1.5.1` is found in your REPL, I guess you might have multiple PyTorch versions installed, where the `1.5.1` version is picked. This would also explain the reported \"hang\", since `1.5.1` doesn't ship with CUDA11.0 and the CUDA JIT would kick in, which might be slow.\n\nI would recommend to uninstall all conda, pip, and source builds in the current environment, make sure that `import torch` fails, and reinstall `1.7.1`.\n"
   },
   {
      "x": "AMP + CUDA + tensor.sum op leads to dtype mismatch",
      "z": "I don't think the output `dtype` is a type mismatch and based on the [`float32` cast list](https://pytorch.org/docs/stable/amp.html#ops-that-can-autocast-to-float32) is the expected result, no?",
      "y": "I don't believe the output dtype is a type mismatch, and the predicted result based on the [float32 cast list](https://pytorch.org/docs/stable/amp.html#ops-that-can-autocast-to-float32) isn't it?"
   },
   {
      "x": "error making: python-torchvision-cuda",
      "z": "Please, post this issue in the torchvision repo https://github.com/pytorch/vision instead, thank you.",
      "y": "Please, post this issue in the torchvision repo https://github.com/pytorch/vision instead"
   },
   {
      "x": "Cannot import torch on latest nightly",
      "z": "~Closing https://github.com/pytorch/pytorch/issues/57209 in favor of this one~\nActually I think they're two separate issues",
      "y": "~Closing https://github.com/pytorch/pytorch/issues/57209 in favor of this one~\nActually I think they're two separate issues"
   },
   {
      "x": "[JIT] Profiling executor is not fusing Dropout while legacy executor does",
      "z": "@therealjjj77, after [some googling](https://discuss.pytorch.org/t/compiling-pytorch-from-source-failed/105439/2?u=imaginary), I found that this issue happens if you're missing some `libpython` headers on your system. Can you reinstall `python` with `apt-get install python3.7-dev`, if you're using Ubuntu?\n\nI verified with Python 3.9.4 that installing `python-dev` instead of `python` fixes the issue.\nThe reason I was able to build with python 3.8.5 earlier is that `python3.8-dev` was preinstalled in Ubuntu 20.04.",
      "y": "(https://discuss.pytorch.org/t/compiling-pytorch-from-source-failed/105439/2?u=imaginary), I found that this issue happens if you're missing some `libpython` headers on your system. Can you reinstall `python` with `apt-get install python3.7-dev`, if you're using Ubuntu?\nI verified with Python 3.9.4 that installing `python-dev` instead of `python` fixes the issue.\nThe reason I was able to build with python 3.8.5 earlier is that `python3.8-dev` was preinstalled in Ubuntu 20.04."
   },
   {
      "x": "Tensors with complex dtypes fail to be cloned contiguously",
      "z": "Being fixed in #45487.",
      "y": "Being fixed in #45487."
   },
   {
      "x": "[Feature Request] Hessian support for model weights (nn.Parameter)",
      "z": "Yes this design difference is done on purpose.\nThe plan we have to solve this issue is to build on top of https://github.com/pytorch/pytorch/issues/49171 so that the user can give the weights as proper inputs to the function and thus work well with the function autograd API.",
      "y": "Yes this design difference is done on purpose.\nThe plan we have to solve this issue is to build on top of https://github.com/pytorch/pytorch/issues/49171 so that the user can give the weights as proper inputs to the function and thus work well with the function autograd API."
   },
   {
      "x": "No documentation for `torch.sgn`",
      "z": "It is actually documented but it's missing an torch.rst entry so it doesn't appear in the built docs. See here:\n\nhttps://github.com/pytorch/pytorch/blob/68a6e4637903dba279c60daae5cff24e191ff9b4/torch/_torch_docs.py#L7517\n",
      "y": "It is documented, but the torch.rst entry is missing, therefore it does not appear in the built docs. Here's an example:\n\nhttps://github.com/pytorch/pytorch/blob/68a6e4637903dba279c60daae5cff24e191ff9b4/torch/ torch docs.py#L7517"
   },
   {
      "x": "libtorch_cuda.so is missing fast kernels from libcudnn_static.a, therefore statically linked cuDNN could be much slower than dynamically linked",
      "z": "So, does this issue explain why NGC containers have been consistently faster than the official conda builds for a number of PyTorch versions now? NGC == dynamic link, conda/pip = static w/ this issue? This has pretty significant impact if that is the case.\nI ran some benchmarks trying to figure out what was happening as I've kept bumping into it with new releases... https://gist.github.com/rwightman/bb59f9e245162cee0e38bd66bd8cd77f",
      "y": "So, does this issue explain why NGC containers have been consistently faster than the official conda builds for a number of PyTorch versions now? NGC == dynamic link, conda/pip = static w/ this issue? This has pretty significant impact if that is the case.\n\nI ran some benchmarks trying to figure out what was happening as I've kept bumping into it with new releases... https://gist.github.com/rwightman/bb59f9e245162cee0e38bd66bd8cd77f"
   },
   {
      "x": "Negative impact of Python versions on using Pytorch with Cuda11",
      "z": "I don't think the output `dtype` is a type mismatch and based on the [`float32` cast list](https://pytorch.org/docs/stable/amp.html#ops-that-can-autocast-to-float32) is the expected result, no?",
      "y": "I don't believe the output dtype is a type mismatch, and the predicted result based on the [float32 cast list](https://pytorch.org/docs/stable/amp.html#ops-that-can-autocast-to-float32) isn't it?"
   },
   {
      "x": "error making: python-torchvision-cuda",
      "z": "Please, post this issue in the torchvision repo https://github.com/pytorch/vision instead, thank you.",
      "y": "Post this issue in the torchvision repo https://github.com/pytorch/vision instead"
   },
   {
      "x": "Support broadcasting for clip / clamp",
      "z": "This is a duplicate of #2793 which is implemented in #52695.",
      "y": "This is a duplicate of #2793 which is implemented in #52695."
   },
   {
      "x": "Cannot import torch on latest nightly",
      "z": "~Closing https://github.com/pytorch/pytorch/issues/57209 in favor of this one~\n\nActually I think they're two separate issues",
      "y": "~Closing https://github.com/pytorch/pytorch/issues/57209 in favor of this one~\n\nActually I think they're two separate issues"
   },
   {
      "x": "Cmake Error on Custom Pytorch Build from Source",
      "z": "@therealjjj77, after [some googling](https://discuss.pytorch.org/t/compiling-pytorch-from-source-failed/105439/2?u=imaginary), I found that this issue happens if you're missing some `libpython` headers on your system. Can you reinstall `python` with `apt-get install python3.7-dev`, if you're using Ubuntu?\n\nI verified with Python 3.9.4 that installing `python-dev` instead of `python` fixes the issue.\nThe reason I was able to build with python 3.8.5 earlier is that `python3.8-dev` was preinstalled in Ubuntu 20.04.",
      "y": "After doing some research (https://discuss.pytorch.org/t/compiling-pytorch-from-source-failed/105439/2?u=imaginary), I discovered that this problem occurs when some libpython headers are missing from your system. If you're using Ubuntu, can you reinstall Python with apt-get install python3.7-dev?\n\nI confirmed that installing python-dev instead of python addresses the issue with Python 3.9.4."
   },
   {
      "x": "Kineto dependency CUPTI detection should use standard cmake mechanisms",
      "z": "find_package(CUDA) sets CUDA_cupti_LIBRARY.",
      "y": "find_package(CUDA) sets CUDA_cupti_LIBRARY."
   },
   {
      "x": "`getattr` does not support default parameter when jitted",
      "z": "actually, the right syntax tree to emit is a prim::If node with two blocks, one for True one for False\n\nsomething whose structure is similiar to this Python function and its corresponding generated code:\n\n`\ndef g(x):\na = T()\nreturn 1 if a.my == \"HI\" else 2\n\nGraph:\n\ngraph(%x : Tensor):\n%5 : str = prim::Constantvalue=\"HI\" # ts1.py:13:21\n%8 : int = prim::Constantvalue=1 # ts1.py:13:8\n%9 : int = prim::Constantvalue=2 # ts1.py:13:31\n%a.1 : torch.T = prim::CreateObject()\n%2 : NoneType = prim::CallMethodname=\"init\" # ts1.py:12:5\n%my : str = prim::GetAttrname=\"my\"\n%6 : bool = aten::eq(%my, %5) # ts1.py:13:13\n%10 : int = prim::If(%6) # ts1.py:13:8\nblock0():\n-> (%8)\nblock1():\n-> (%9)\nreturn (%10)",
      "y": "the right syntax tree to emit is a prim::If node with two blocks, one for True one for False\n\nsomething whose structure is similiar to this Python function and its corresponding generated code:\n\n`\ndef g(x):\na = T()\nreturn 1 if a.my == \"HI\" else 2\n\nGraph:\n\ngraph(%x : Tensor):\n%5 : str = prim::Constantvalue=\"HI\" # ts1.py:13:21\n%8 : int = prim::Constantvalue=1 # ts1.py:13:8\n%9 : int = prim::Constantvalue=2 # ts1.py:13:31\n%a.1 : torch.T = prim::CreateObject()\n%2 : NoneType = prim::CallMethodname=\"init\" # ts1.py:12:5\n%my : str = prim::GetAttrname=\"my\"\n%6 : bool = aten::eq(%my, %5) # ts1.py:13:13\n%10 : int = prim::If(%6) # ts1.py:13:8\nblock0():\n-> (%8)\nblock1():\n-> (%9)\nreturn (%10)"
   },
   {
      "x": "`getattr` does not support default parameter when jitted",
      "z": "actually, the right syntax tree to emit is a prim::If node with two blocks, one for True one for False\n\nsomething whose structure is similiar to this Python function and its corresponding generated code:\n\n`\ndef g(x):\na = T()\nreturn 1 if a.my == \"HI\" else 2\n\nGraph:\n\ngraph(%x : Tensor):\n%5 : str = prim::Constantvalue=\"HI\" # ts1.py:13:21\n%8 : int = prim::Constantvalue=1 # ts1.py:13:8\n%9 : int = prim::Constantvalue=2 # ts1.py:13:31\n%a.1 : torch.T = prim::CreateObject()\n%2 : NoneType = prim::CallMethodname=\"init\" # ts1.py:12:5\n%my : str = prim::GetAttrname=\"my\"\n%6 : bool = aten::eq(%my, %5) # ts1.py:13:13\n%10 : int = prim::If(%6) # ts1.py:13:8\nblock0():\n-> (%8)\nblock1():\n-> (%9)\nreturn (%10)\n`",
      "y": "the right syntax tree to emit is a prim::If node with two blocks, one for True one for False\nsomething whose structure is similiar to this Python function and its corresponding generated code:\n\n`\ndef g(x):\na = T()\nreturn 1 if a.my == \"HI\" else 2\n\nGraph:\n\ngraph(%x : Tensor):\n%5 : str = prim::Constantvalue=\"HI\" # ts1.py:13:21\n%8 : int = prim::Constantvalue=1 # ts1.py:13:8\n%9 : int = prim::Constantvalue=2 # ts1.py:13:31\n%a.1 : torch.T = prim::CreateObject()\n%2 : NoneType = prim::CallMethodname=\"init\" # ts1.py:12:5\n%my : str = prim::GetAttrname=\"my\"\n%6 : bool = aten::eq(%my, %5) # ts1.py:13:13\n%10 : int = prim::If(%6) # ts1.py:13:8\nblock0():\n-> (%8)\nblock1():\n-> (%9)\nreturn (%10)\n`"
   },
   {
      "x": "Compiling Libtorch in Vs 2019?",
      "z": "I think PyTorch never really supports ICC. The only compiler supported on Windows is MSVC and the code is C++14 at the moment.",
      "y": "PyTorch never really supports ICC. The only compiler supported on Windows is MSVC and the code is C++14 currently."
   },
   {
      "x": "[feature request] incomplete cholesky factorization",
      "z": "Hi @rfeinman, thank you for filing this feature request! There is an ongoing PR https://github.com/pytorch/pytorch/pull/56724 that adds a variant that allows suppressing the error check.",
      "y": " There is an ongoing PR https://github.com/pytorch/pytorch/pull/56724 that adds a variant that allows suppressing the error check."
   },
   {
      "x": "Backward hook is not registered for traced modules ",
      "z": "I commented on the internal post; JIT doesn't have support for backward hooks. See this issue for tracking: https://github.com/pytorch/pytorch/issues/34329 ",
      "y": "I commented on the internal post; JIT doesn't have support for backward hooks. See this issue for tracking: https://github.com/pytorch/pytorch/issues/34329 "
   },
   {
      "x": "[grad] index_fill : Incorrect gradient when index has duplicates",
      "z": "Forward should not error out, all the indexing operations accept duplicate indices. \nPerf hit in backward it ok and unavoidable.",
      "y": "Forward should not error out, all the indexing operations accept duplicate indices. \nPerf hit in backward it ok and unavoidable."
   },
   {
      "x": "about the torch.randperm in torch1.8.1+cuda11.1   \u00e5\u2026\u00b3\u00e4\u00ba\u017dtorch1.8.1\u00e5\u2019\u0152cuda11.1\u00e7\u017d\u00af\u00e5\u00a2\u0192\u00e4\u00b8\u2039randperm\u00e5\u2021\u00bd\u00e6\u2022\u00b0\u00e7\u0161\u201e\u00e5\u00a5\u2021\u00e6\u20ac\u00aa\u00e8\u00be\u201c\u00e5\u2021\u00ba",
      "z": "This was fixed on master in #55292",
      "y": "This was fixed on master in #55292"
   },
   {
      "x": "torch.cholesky with upper=True is wrong for batched CUDA inputs",
      "z": "> When we deprecate torch.cholesky in favor of torch.linalg.cholesky and write its deprecation guide, will that address this issue?\n\nYes, because `torch.linalg.cholesky` always uses the lower triangular part of the input.\n\nAlso, the ongoing PR for `cholesky_ex` (https://github.com/pytorch/pytorch/pull/56724) includes some rewrites of how MAGMA is called, so this problem would be fixed there, I need to add the tests. Perhaps I should split that PR into smaller ones.",
      "y": "Will this issue be resolved when we deprecate torch.cholesky in favour of torch.linalg.cholesky and develop its deprecation guide\n\nTorch.linalg.cholesky always uses the lower triangular part of the input, hence the answer is yes.\n\nAlso, the cholesky ex PR (https://github.com/pytorch/pull/56724) involves some rewrites of how MAGMA is called, so this issue would be fixed there, I believe."
   },
   {
      "x": "CI check `binary_linux_libtorch_3_7m_cpu_devtoolset7_shared-with-deps_build` is failing intermittently",
      "z": "I think we encountered some system piping performance issue before when MAX_JOBS were set too high last year. But not sure why this doesn't occur to other binary job or nighlty jobs.\nWill \n* dig into other binary CI pipelines\n* try and see if using nproc/2 reduces flakiness.\n\n@seemethere could you share any comments/suggestions you might have on this? ",
      "y": "Last year, when MAX JOBS were set too high, I believe we had some system pipe performance issues. But I'm not clear why this doesn't happen with other binary or near-zero jobs.\n* look into different binary continuous integration pipelines\n* See whether switching to nproc/2 lessens flakiness\n"
   },
   {
      "x": "nn.Linear weight initalization - uniform or kaiming_uniform?",
      "z": "> We should probably document the factor of sqrt(3) that we use. I remember that there was some justification for it\n\nUniform distribution's stdv is basically multiplied by sqrt(3) when you draw: https://physics.nist.gov/cuu/Uncertainty/typeb.html and https://physics.stackexchange.com/questions/110242/why-is-uncertainty-divided-by-sqrt3\n\nIf you notice in my \"equivalent logic\" [snippet](https://github.com/pytorch/pytorch/blob/v0.4.1/torch/nn/modules/linear.py#L48-L52), one thing worth noticing here is that by default, we don't multiply the stdv with sqrt(3).\nThe reason we don't do that because Collobert at al. in some historical past have figured out that this not-multiplying and having a slight gain in the uniform distributions heuristically works better. This is not recorded in literature anywhere but has been recorded in code since Lush, Senna, Torch5, Torch7 and now PyTorch which is somewhat unfortunate. A detailed discussion of this was recorded on Google Plus, which is now defunct and that discussion has been erased from the internet. However, I've revived a copy of that discussion here: https://soumith.ch/assets/20141213_gplus_nninit_discussion.htm",
      "y": " We should probably document the sqrt(3) factor we're using. I recall there being some justification for it.\n\nWhen you draw a uniform distribution, the stdv is basically multiplied by sqrt(3): www.nist.gov/cuu/Uncertainty/typeb.html and https://physics.stackexchange.com/questions/110242/why-is-uncertainty-divided-by-sqrt3\n\nIf you look at my \"equivalent logic\" [snippet](https:/), you'll discover that"
   },
   {
      "x": "Build failure with USE_CUDA=1",
      "z": "Not sure if this is what you are looking for but you can turn on https://github.com/pytorch/pytorch/blob/master/caffe2/CMakeLists.txt#L17-L19",
      "y": "Not sure if this is what you are looking for but you can turn on https://github.com/pytorch/pytorch/blob/master/caffe2/CMakeLists.txt#L17-L19"
   },
   {
      "x": "[test] index_fill : gradgradcheck CUDA takes around 40 seconds",
      "z": "The same happens with `index_copy` iirc.\nIt comes from the fact that the current tests are very thorough. They have cases for all cases of contiguous / non-contiguous inputs with all the possible flags.\n\nMaking the input sizes smaller would alleviate but not completely solve this problem ",
      "y": "The same happens with `index_copy` iirc.\nIt comes from the fact that the current tests are very thorough. They have cases for all cases of contiguous / non-contiguous inputs with all the possible flags.\nMaking the input sizes smaller would alleviate but not completely solve this problem "
   },
   {
      "x": "Expose RNN cells in C++ frontend",
      "z": "Is it not exposed already? lstm_cell is in the docs...\nhttps://pytorch.org/cppdocs/api/function_namespaceat_1a189463e15e5542e5d21f544fd70ee967.html?highlight=lstm_cell",
      "y": "Is it not exposed already? lstm_cell is in the docs...\nhttps://pytorch.org/cppdocs/api/function_namespaceat_1a189463e15e5542e5d21f544fd70ee967.html?highlight=lstm_cell"
   },
   {
      "x": "Port multinomial to Aten",
      "z": "Warning: there may be a bunch of transitive dependencies you have to port first before you can do this.",
      "y": "Warning: there may be a bunch of transitive dependencies you have to port first before you can do this."
   },
   {
      "x": "[C++/pytorch] data loading and working with complex data structures with the C++ frontend",
      "z": "@zeryx I think you have two options:\n\n1. If you can do all modifications on the matrix/tensor in-place, you could call `from_blob` without cloning, mutate the tensor in-place, and since the data would be shared between the tensor and the `cv::Mat` the mutations would be reflected in the original `cv::Mat`, which you could then use for further processing/visualization. E.g.\n\n    ```cpp\n     void process(cv::Mat mat) {\n         at::Tensor tensor = torch::from_blob(...); // No clone, just a \"view\" of the mat as a tensor\n         tensor.mul_(2); // The multiplication by two is visible in both the tensor and the original `mat`\n         visualize(mat); // Since we mutated the underlying data, we can just use the original `mat`\n     }\n    ```\n\n    However, if you are calling a `ScriptModule` like you do in your example, that mutation will likely not be in-place. As in, the result of `module->forward(...)` will likely be a new tensor pointing at new memory, not associated with any `cv::Mat` (yet). So this does not apply.\n\n2. If the end result of your processing is a brand new tensor, you likely want to convert it back into a `cv::Mat`. For this you can use [this constructor](https://docs.opencv.org/trunk/d3/d63/classcv_1_1Mat.html#a922de793eabcec705b3579c5f95a643e) from OpenCV, like the `cv::Mat warp(3, 3, CV_32FC1, w.data<float>());` I used above. In fact, if you read through the \"Detailed Description\" section, you'll notice that their data model is the same as for our tensors: creating a `cv::Mat` from some data blob only creates a view, and you can use the [`clone()`](https://docs.opencv.org/trunk/d3/d63/classcv_1_1Mat.html#adff2ea98da45eae0833e73582dd4a660) method to create a deep-copy of the data.\n\nAll of that said, I think the path for you is:\n\n1. You start out with a `cv::Mat mat`,\n2. If all you're doing is passing the Mat to `module->forward` immediately, you can create a tensor from it without copying, i.e. `tensor = from_blob(mat.ptr<float>(), sizes)` without cloning. For this make sure the original `mat` is still alive while you call `module->forward`.\n3. Get a new tensor when invoking the module: `auto new_tensor = module->forward(tensor);`\n4. Then create a `cv::Mat` from the resulting tensor: `cv::Mat output(sizes, CV_32FC1, new_tensor.data<float>())`. At this point the `output` mat will be pointing at data owned by the `new_tensor`. If you simply want to visualize it straight away, no need to copy the data into the `Mat`.\n5. If you want to return the `Mat` from a function or otherwise store it, call `clone()` on the `Mat` to take ownership of the data by copying it.\n\nNotice that there is a case where no additional copies are incurred at all, which is when you're only converting to a tensor to call `module->forward`, and then only converting back to a `Mat` to visualize it. Just be aware of how the lifetimes are bound to each other and when you need to call `clone()`. Hope this helps.",
      "y": "1. If you can do all modifications on the matrix/tensor in-place, you could call `from_blob` without cloning, mutate the tensor in-place, and since the data would be shared between the tensor and the `cv::Mat` the mutations would be reflected in the original `cv::Mat`, which you could then use for further processing/visualization. E.g.\n\n    ```cpp\n     void process(cv::Mat mat) {\n         at::Tensor tensor = torch::from_blob(...); // No clone, just a \"view\" of the mat as a tensor\n         tensor.mul_(2); // The multiplication by two is visible in both the tensor and the original `mat`\n         visualize(mat); // Since we mutated the underlying data, we can just use the original `mat`\n     }\n    ```\n\n    However, if you are calling a `ScriptModule` like you do in your example, that mutation will likely not be in-place. As in, the result of `module->forward(...)` will likely be a new tensor pointing at new memory, not associated with any `cv::Mat` (yet). So this does not apply.\n\n2. If the end result of your processing is a brand new tensor, you likely want to convert it back into a `cv::Mat`. For this you can use [this constructor](https://docs.opencv.org/trunk/d3/d63/classcv_1_1Mat.html#a922de793eabcec705b3579c5f95a643e) from OpenCV, like the `cv::Mat warp(3, 3, CV_32FC1, w.data<float>());` I used above. In fact, if you read through the \"Detailed Description\" section, you'll notice that their data model is the same as for our tensors: creating a `cv::Mat` from some data blob only creates a view, and you can use the [`clone()`](https://docs.opencv.org/trunk/d3/d63/classcv_1_1Mat.html#adff2ea98da45eae0833e73582dd4a660) method to create a deep-copy of the data.\n\nAll of that said, I think the path for you is:\n\n1. You start out with a `cv::Mat mat`,\n2. If all you're doing is passing the Mat to `module->forward` immediately, you can create a tensor from it without copying, i.e. `tensor = from_blob(mat.ptr<float>(), sizes)` without cloning. For this make sure the original `mat` is still alive while you call `module->forward`.\n3. Get a new tensor when invoking the module: `auto new_tensor = module->forward(tensor);`\n4. Then create a `cv::Mat` from the resulting tensor: `cv::Mat output(sizes, CV_32FC1, new_tensor.data<float>())`. At this point the `output` mat will be pointing at data owned by the `new_tensor`. If you simply want to visualize it straight away, no need to copy the data into the `Mat`.\n5. If you want to return the `Mat` from a function or otherwise store it, call `clone()` on the `Mat` to take ownership of the data by copying it.\n\nNotice that there is a case where no additional copies are incurred at all, which is when you're only converting to a tensor to call `module->forward`, and then only converting back to a `Mat` to visualize it. Just be aware of how the lifetimes are bound to each other and when you need to call `clone()`. Hope this helps."
   },
   {
      "x": "ModuleNotFoundError: No module named 'torchvision'",
      "z": "It is in a separate repository https://github.com/pytorch/vision, should be quite easy to install from source since you already compiled pytorch.",
      "y": "It is in a separate repository https://github.com/pytorch/vision, should be quite easy to install from source since you already compiled pytorch."
   },
   {
      "x": "Expose RNN cells in C++ frontend",
      "z": "this should definitely be deleted. Would you mind sending a PR for it? If not we can route it to one of the core devs",
      "y": "this should definitely be deleted. Would you mind sending a PR for it? If not we can route it to one of the core devs"
   },
   {
      "x": "torch.cholesky with upper=True is wrong for batched CUDA inputs",
      "z": "Yes, it's probably picking up a header/library from the old install.",
      "y": "picking up a header/library from the old install."
   },
   {
      "x": "[grad] index_fill : Incorrect gradient when index has duplicates",
      "z": "We dont ship 0.4.0 with CUDA 9.2. You can pick a link for 0.4.0 here: https://pytorch.org/get-started/previous-versions/",
      "y": "We dont ship 0.4.0 with CUDA 9.2. You can pick a link for 0.4.0 here: https://pytorch.org/get-started/previous-versions/"
   },
   {
      "x": "Cannot import torch on latest nightly",
      "z": "@bhack it's not, but a `::to` is being added in https://github.com/pytorch/pytorch/pull/12710 which will enable it for C++ interfaces.",
      "y": "`::to` is being added in https://github.com/pytorch/pytorch/pull/12710 which will enable it for C++ interfaces."
   },
   {
      "x": "Printing: mismatch result with numpy in pytorch 0.4",
      "z": "I just found out that this is a printing issue: setting `torch.set_printoptions(precision=8)` prints the right values.\n\nI also think visual inspection is not the right way to check; ~we could use something like `torch.allclose` to check the results.~ edit: I realized that we are comparing a tensor and ndarray, sorry.",
      "y": " this is a printing issue: setting `torch.set_printoptions(precision=8)` prints the right values."
   },
   {
      "x": "undefined reference to `vmdLog2'",
      "z": "This looks like an MKL error. cc: @soumith ",
      "y": "This looks like an MKL error. "
   },
   {
      "x": "Add `return_counts` to `torch.unique`",
      "z": "Expose RNN cells in C++ frontend",
      "y": "Expose RNN cells in C++ frontend"
   },
   {
      "x": "Cross entropy Loss : Ignore list of indexes ",
      "z": "while this is a \"nice to have\", it's not popular enough that we want to complicate the function even more in it's implementation.\nYou can do some pre-processing on your `targets` to get to the same place, or as you mentioned, use the weight=0 trick. In fact, `weight` is provided for this precise reason.",
      "y": "it's not popular enough that we want to complicate the function even more in it's implementation.\nYou can do some pre-processing on your `targets` to get to the same place, or as you mentioned, use the weight=0 trick. In fact, `weight` is provided for this precise reason."
   },
   {
      "x": "caffe2  GetMutableTensor",
      "z": "Linking against glog, and explicitly using BlobGetMutableTensor(Blob* blob, DeviceType device_type) did the job for me. But I think that it should also work as presented above. Changes to the CMakeLists.txt and the main.cpp are:\n\ncmake:\n\n```\ncmake_minimum_required(VERSION 3.11 FATAL_ERROR)\nset(CMAKE_CXX_STANDARD 11)\n\nfind_library(CAFFE2_LIB caffe2)\nfind_package(Protobuf 3 REQUIRED)\nfind_library(GLOG_LIB glog) # MODIFIED\n\nadd_executable(main main.cpp)\ntarget_link_libraries(main ${CAFFE2_LIB} ${PROTOBUF_LIBRARIES} ${GLOG_LIB}) # MODIFIED\n```\n\nmain:\n\n```\n#include <caffe2/core/init.h>\n#include <caffe2/core/tensor.h>\n#include <caffe2/core/workspace.h>\n\nusing namespace std;\nusing namespace caffe2;\n\nint main(int argc, char** argv) {\n\n\tcaffe2::GlobalInit(&argc, &argv);\n\n\t// Tensor.\n\tvector<int64_t> dims({10});\n\tvector<float> values(10, 1);\n\n\tTensor tensor(dims, CPU);\n\ttensor.ShareExternalPointer(values.data());\n\n\t// Workspace.\n\tWorkspace workspace;\n\n\t//Tensor* data = workspace.CreateBlob(\"data\")->GetMutable<TensorCPU>(); // Doesn't work for me.\n\tTensor* data = BlobGetMutableTensor(workspace.CreateBlob(\"data\"), CPU); // MODIFIED\n\tdata->ResizeLike(tensor); // works\n\n\t\n\tgoogle::protobuf::ShutdownProtobufLibrary();\n\t\n\treturn 0;\n}\n```\n\nI don't know if it is intended to use the caffe2 cpp api like this for future releases, or whether it is simply a bug on my system.",
      "y": "Linking against glog, and explicitly using BlobGetMutableTensor(Blob* blob, DeviceType device_type) did the job for me. But I think that it should also work as presented above. Changes to the CMakeLists.txt and the main.cpp are:\n\ncmake:\n\n```\ncmake_minimum_required(VERSION 3.11 FATAL_ERROR)\nset(CMAKE_CXX_STANDARD 11)\n\nfind_library(CAFFE2_LIB caffe2)\nfind_package(Protobuf 3 REQUIRED)\nfind_library(GLOG_LIB glog) # MODIFIED\n\nadd_executable(main main.cpp)\ntarget_link_libraries(main ${CAFFE2_LIB} ${PROTOBUF_LIBRARIES} ${GLOG_LIB}) # MODIFIED\n```\n\nmain:\n\n```\n#include <caffe2/core/init.h>\n#include <caffe2/core/tensor.h>\n#include <caffe2/core/workspace.h>\n\nusing namespace std;\nusing namespace caffe2;\n\nint main(int argc, char** argv) {\n\n\tcaffe2::GlobalInit(&argc, &argv);\n\n\t// Tensor.\n\tvector<int64_t> dims({10});\n\tvector<float> values(10, 1);\n\n\tTensor tensor(dims, CPU);\n\ttensor.ShareExternalPointer(values.data());\n\n\t// Workspace.\n\tWorkspace workspace;\n\n\t//Tensor* data = workspace.CreateBlob(\"data\")->GetMutable<TensorCPU>(); // Doesn't work for me.\n\tTensor* data = BlobGetMutableTensor(workspace.CreateBlob(\"data\"), CPU); // MODIFIED\n\tdata->ResizeLike(tensor); // works\n\n\t\n\tgoogle::protobuf::ShutdownProtobufLibrary();\n\t\n\treturn 0;\n}\n```\n\nI don't know if it is intended to use the caffe2 cpp api like this for future releases, or whether it is simply a bug on my system."
   },
   {
      "x": "Using nn.Parameter as args to torch.distributions.Normal",
      "z": "Fixed on master",
      "y": "Fixed on master"
   },
   {
      "x": "nn.html page has too much content",
      "z": "I second this. At least the `functional`  part could go to an entirely new webpage\n1. most items there have a link to relevant \"objective\" `nn` item anyway\n2. they double the keywords in objective `nn`, which means even ctrl+f doesn't really do its job\n\nAnother pain point is that (at least in firefox) the math formulae are rendered after the page itself, which interacts with links. When I click a link to an item halfway through the `nn` page (often from the corresponding `functional` item), I will be directed to the right item, and after a second it will slide off my screen due to all the formulae above expanding and taking its place. Not that I have a good idea how to deal with that...",
      "y": "I second this. At least the `functional`  part could go to an entirely new webpage\n1. most items there have a link to relevant \"objective\" `nn` item anyway\n2. they double the keywords in objective `nn`, which means even ctrl+f doesn't really do its job\n\nAnother pain point is that (at least in firefox) the math formulae are rendered after the page itself, which interacts with links. When I click a link to an item halfway through the `nn` page (often from the corresponding `functional` item), I will be directed to the right item, and after a second it will slide off my screen due to all the formulae above expanding and taking its place. Not that I have a good idea how to deal with that..."
   },
   {
      "x": "Dataset loader ToCuda() transform collate_fn fails",
      "z": "@dizcza as pointed in the link, you have to use `torch.multiprocessing.set_start_method('spawn')` to get CUDA + Multiprocessing working",
      "y": "use `torch.multiprocessing.set_start_method('spawn')` to get CUDA + Multiprocessing working"
   },
   {
      "x": "OMP: Error about doubly-linked OpenMP when loading CIFAR10 dataset",
      "z": "A workaround seems to be\n```\nimport os\n\nos.environ['KMP_DUPLICATE_LIB_OK'] = 'True'\n```\nbut it would be better to find the root cause.",
      "y": "A workaround seems to be\n```\nimport os\n\nos.environ['KMP_DUPLICATE_LIB_OK'] = 'True'\n```\nbut it would be better to find the root cause."
   },
   {
      "x": "5x slowdown on Windows vs Ubuntu",
      "z": "I am facing the same problem. Used PyTorch 1.5 with CUDA-10.2 in both Windows-10 and Ubuntu-18.04. Both OS are on the same machine. Speed using the diagnosis code provided [at the beginning of the thread](https://github.com/pytorch/pytorch/issues/13807#issue-379409817), shows that PyTorch is ~4 times slower in Windows.\nI was running [another project](https://github.com/NVlabs/Deep_Object_Pose) which uses PyTorch. I adapted it to use python 3.6 and PyTorch 1.5. Same speed problem between Windows and Ubuntu.\n",
      "y": "Used PyTorch 1.5 with CUDA-10.2 in both Windows-10 and Ubuntu-18.04. Both OS are on the same machine. Speed using the diagnosis code provided [at the beginning of the thread](https://github.com/pytorch/pytorch/issues/13807#issue-379409817), shows that PyTorch is ~4 times slower in Windows.\nI was running [another project](https://github.com/NVlabs/Deep_Object_Pose) which uses PyTorch. I adapted it to use python 3.6 and PyTorch 1.5. Same speed problem between Windows and Ubuntu.\n"
   },
   {
      "x": "Unable to do transfer tensors to GPU using `.cuda()` when using multiprocessing.Process with `fork`",
      "z": "repeating a message from Olexa on slack, closing this as wontfix::\n\n```\n@Amir, when I wrote in #general that CUDA is totally incompatible with forking, I meant this in the most serious and literal sense of those words possible.\n\nThis simply *cannot* be fixed in CUDA, PyTorch or anyone else. The problems inherent in `fork()`'ing *any* _multithreaded_ program are fundamentally unsolvable, and simply beyond the power of anyone to fix, at least not until a revolution in OS design happens.\n\nI can only plead that you accept the general difficulty of safely forking within a multithreaded program and try to do things another way.\n\nThere's several blogs out there discussing why it's dangerous to fork in a multithreaded program, such as https://thorstenball.com/blog/2014/10/13/why-threads-cant-fork/\n\nThe gist of it is that when a thread `fork()`'s, *in the child process, all other threads _die instantly._* It doesn't matter what they were doing, they're _gone_.\n\n- If they had locked a mutex, that mutex will never be unlocked again.\n- If they were modifying a data structure, that data structure might be invalid.\n- If they `malloc()`'ed some memory, that memory might never be deallocated, and `malloc()` may use locks and data structures anyways.\n- If a thread was doing useful work, that work will never be complete, because the threads no longer exist.\n- You can't join these non-existent threads.\n- `pthread_atfork()` is a function meant to solve the problems above, but it's simply incapable of doing what it was meant to do safely, and that's why the POSIX.1 standard explicitly says that this function may be formally deprecated in the next version of the standard. It was a mistake.\n\nSo almost the only safe thing to do if you `fork()`'ed from a multi-threaded process is to call `exec()`. That's what `spawn` does.\n\nBecause the CUDA runtime uses threads to implement its runtime and asynchronous streams, once the CUDA runtime is initialized, it's insanely dangerous to `fork()`. Don't do it.\n```",
      "y": "closing this as it cannot be fixed::\n\n\n CUDA is totally incompatible with forking.\nThis simply *cannot* be fixed in CUDA, PyTorch or anyone else. The problems inherent in `fork()`'ing *any* _multithreaded_ program are fundamentally unsolvable, and simply beyond the power of anyone to fix, at least not until a revolution in OS design happens.\n\nThere's several blogs out there discussing why it's dangerous to fork in a multithreaded program, such as https://thorstenball.com/blog/2014/10/13/why-threads-cant-fork/\n\nThe gist of it is that when a thread `fork()`'s, *in the child process, all other threads _die instantly._* It doesn't matter what they were doing, they're _gone_.\n\n- If they had locked a mutex, that mutex will never be unlocked again.\n- If they were modifying a data structure, that data structure might be invalid.\n- If they `malloc()`'ed some memory, that memory might never be deallocated, and `malloc()` may use locks and data structures anyways.\n- If a thread was doing useful work, that work will never be complete, because the threads no longer exist.\n- You can't join these non-existent threads.\n- `pthread_atfork()` is a function meant to solve the problems above, but it's simply incapable of doing what it was meant to do safely, and that's why the POSIX.1 standard explicitly says that this function may be formally deprecated in the next version of the standard. It was a mistake.\n\nSo almost the only safe thing to do if you `fork()`'ed from a multi-threaded process is to call `exec()`. That's what `spawn` does.\n\nBecause the CUDA runtime uses threads to implement its runtime and asynchronous streams, once the CUDA runtime is initialized, it's not advisable to `fork()`\n```"
   },
   {
      "x": "In-place matrix multiplication gives erroneous results.",
      "z": "Setting `out` to overlap with input is highly advised against. It is really unstable.",
      "y": "Setting `out` to overlap with input is highly advised against. It is really unstable."
   },
   {
      "x": "Cannot load a saved torch.jit.trace using C++'s torch::jit::load",
      "z": "@houseroad The model is serialized exactly the same way as in the tutorial. ",
      "y": "The model is serialized exactly the same way as in the tutorial. "
   },
   {
      "x": "Importing an opencv GpuMat",
      "z": "Fixing in https://github.com/pytorch/pytorch/pull/13982",
      "y": "Fixing in https://github.com/pytorch/pytorch/pull/13982"
   },
   {
      "x": "Better error messaging for requiring ModuleList to be constant",
      "z": "You need to append the resnet blocks into an array and then initialize the module_list\n\nFor example\n\nm = []\nm.append(blah)\nself.module_list = nn.ModuleList(m)",
      "y": "Need to append the resnet blocks into an array and then initialize the module_list\nFor example\n\nm = []\nm.append(blah)\nself.module_list = nn.ModuleList(m)"
   },
   {
      "x": "Build broken with NO_CUDNN = 1 flag",
      "z": "Hi @chintak - thanks for reporting. A fix should be landing soon in https://github.com/pytorch/pytorch/pull/8340",
      "y": "A fix should be landing soon in https://github.com/pytorch/pytorch/pull/8340"
   },
   {
      "x": "out= parameters don't check for aliasing (so inplace broadcast is incorrectly accepted)",
      "z": "Both of the following fail now:\n```\n\nIn [3]: x.add_(torch.ones(2,2))\n---------------------------------------------------------------------------\nRuntimeError                              Traceback (most recent call last)\n<ipython-input-3-5a1004c71ec3> in <module>\n----> 1 x.add_(torch.ones(2,2))\n\nRuntimeError: output with shape [2] doesn't match the broadcast shape [2, 2]\n\nIn [4]: x = torch.zeros(2)\n\nIn [5]: torch.add(x, torch.ones(2,2), out=x)\n---------------------------------------------------------------------------\nRuntimeError                              Traceback (most recent call last)\n<ipython-input-5-c79cf56ad398> in <module>\n----> 1 torch.add(x, torch.ones(2,2), out=x)\n\nRuntimeError: output with shape [2] doesn't match the broadcast shape [2, 2]\n\n```\n\nso I think we can close this now. cc @VitalyFedyunin ",
      "y": "Both of the following fail now:\n```\n\nIn [3]: x.add_(torch.ones(2,2))\n---------------------------------------------------------------------------\nRuntimeError                              Traceback (most recent call last)\n<ipython-input-3-5a1004c71ec3> in <module>\n----> 1 x.add_(torch.ones(2,2))\n\nRuntimeError: output with shape [2] doesn't match the broadcast shape [2, 2]\n\nIn [4]: x = torch.zeros(2)\n\nIn [5]: torch.add(x, torch.ones(2,2), out=x)\n---------------------------------------------------------------------------\nRuntimeError                              Traceback (most recent call last)\n<ipython-input-5-c79cf56ad398> in <module>\n----> 1 torch.add(x, torch.ones(2,2), out=x)\n\nRuntimeError: output with shape [2] doesn't match the broadcast shape [2, 2]\n\n```\n\n"
   },
   {
      "x": "Cosine Embedding Loss",
      "z": "According to the documentation, y is supposed to be 1 or -1 to indicate whether the two vectors are supposed to be similar or not. Why are you creating a distribution of y instead? Shouldn't you pass a tensor containing only 1's and -1's?",
      "y": "According to the documentation, y is supposed to be 1 or -1 to indicate whether the two vectors are supposed to be similar or not. Why are  Pass a tensor containing only 1's and -1's."
   },
   {
      "x": "import error",
      "z": "More specifically, if you are using conda, add this to your ~/.bashrc\nexport LD_LIBRARY_PATH=\"/home/username/miniconda3/lib:$LD_LIBRARY_PATH\"",
      "y": "If you are using conda, add this to your ~/.bashrc\nexport LD_LIBRARY_PATH=\"/home/username/miniconda3/lib:$LD_LIBRARY_PATH\""
   },
   {
      "x": "torch.utils.ffi--\u00e2\u20ac\u0153cuda.h\u00e2\u20ac\u009d: No such file or directory",
      "z": "@HuangJunJie2017 You may have to specify `include_dirs` of the `Extension` object, making it point to `%CUDA_PATH%/include`.",
      "y": "May specify `include_dirs` of the `Extension` object, making it point to `%CUDA_PATH%/include`."
   },
   {
      "x": "Error when building pytorch under develop mode",
      "z": "Why is that? It doesn't match what we used to do, and what `develop` should do (`build` should be one of its steps).",
      "y": " what `develop` should do (`build` should be one of its steps)."
   },
   {
      "x": "Linux CentOS pytorch build from source ignores CC env variable",
      "z": "So I was running into the same error on CentOS 6 and was able to get pytorch to compile by adding `env` at the beginning like so:  \n`env CXX=$(which c++) python setup.py install`  \nwhere `c++` was a compiler in my path that supported c++11.  \n\nIt would be nice if the official released binaries were linked against glibc 2.12 if pytorch doesn't depend on anything >2.12. That is why I was building from source.",
      "y": "Same error on CentOS 6 and was able to get pytorch to compile by adding `env` at the beginning like so:  \n`env CXX=$(which c++) python setup.py install`  \nwhere `c++` was a compiler in my path that supported c++11.  \n\nIt would be nice if the official released binaries were linked against glibc 2.12 if pytorch doesn't depend on anything >2.12. "
   },
   {
      "x": "[Bug] Segmentation fault when importing sentencepiece (with v0.4.0)",
      "z": "@bheinzerling @ryonakamura See @t-vi 's comment above. PyTorch binaries are compiled with gcc 4.9.2. However, gcc before and after that version are not incompatible. Hence the segfault you see when pulling them into the same address space. Using a sentencepiece compiled with later gcc will solve the issue.",
      "y": "PyTorch binaries are compiled with gcc 4.9.2. However, gcc before and after that version are not incompatible. Hence the segfault you see when pulling them into the same address space. Using a sentencepiece compiled with later gcc will solve the issue."
   },
   {
      "x": "[feature request] add deviceTensor interface to ATen",
      "z": "So this link to @ClementPinard's proof of concept works better for me:\nhttps://github.com/ClementPinard/extension-cpp/tree/deviceTensorExperiments\n\nIs there a rough vision of how this ATen DeviceTensors should look like yet?",
      "y": "This link works better :\nhttps://github.com/ClementPinard/extension-cpp/tree/deviceTensorExperiments\n\nIs there a rough vision of how this ATen DeviceTensors should look like yet?"
   },
   {
      "x": "PyTorch can't build LongTensor from Numpy Long tensor",
      "z": "About your point that we need to specify the type of the tensor, we actually have type inference now, so that if you pass\n```python\na = torch.tensor([1, 2])  # long tensor\nb = torch.tensor([1.])  # float tensor\nc = torch.tensor([True])  # byte tensor\n```\nwhich is very handy.\n\nBut it seems that we are missing support for numpy integer types:\n```python\na = torch.tensor(np.float64(1))  # works\nb = torch.tensor(np.int64(1))  # fails\n```",
      "y": "About your point that we need to specify the type of the tensor, we actually have type inference now, so that if you pass\n```python\na = torch.tensor([1, 2])  # long tensor\nb = torch.tensor([1.])  # float tensor\nc = torch.tensor([True])  # byte tensor\n```\nwhich is very handy.\n\nBut it seems that we are missing support for numpy integer types:\n```python\na = torch.tensor(np.float64(1))  # works\nb = torch.tensor(np.int64(1))  # fails\n```"
   },
   {
      "x": "[caffe2] segmentation fault for running 'from caffe2.python import core'",
      "z": "@pjh5 Thank you for you answer. I have solved this problem. Because I am installing caffe2 on a debian OS without sudo right, I cannot use `apt install` to install nccl. My solution is to install nccl on my own laptop and copied libnccl.2.x.x to this PC. Now it works well. Thank you again!",
      "y": "I am installing caffe2 on a debian OS without sudo right, I cannot use `apt install` to install nccl. My solution is to install nccl on my own laptop and copied libnccl.2.x.x to this PC. Now it works well. "
   },
   {
      "x": "torch.where produces confusing error messages on cpu/cuda mismatches",
      "z": "I remember that this was fixed on master. Thanks for the report @cle-ros, and thank you for verifying @vishwakftw ",
      "y": "I remember that this was fixed on master. "
   },
   {
      "x": "[ONNX] Export fails when the same instance of a module appears multiple times in a module list",
      "z": "using `copy.deepcopy()` over the modules seem to solve the problem:\n\n```python\nfrom copy import deepcopy\n\nself.enc0 = nn.Sequential(deepcopy(self.resnet.conv1), deepcopy(self.resnet.bn1), deepcopy(self.resnet.relu), deepcopy(self.resnet.maxpool))\nself.enc1 = deepcopy(self.resnet.layer1)\n...\n```",
      "y": "using `copy.deepcopy()` over the modules seem to solve the problem:\n\n```python\nfrom copy import deepcopy\n\nself.enc0 = nn.Sequential(deepcopy(self.resnet.conv1), deepcopy(self.resnet.bn1), deepcopy(self.resnet.relu), deepcopy(self.resnet.maxpool))\nself.enc1 = deepcopy(self.resnet.layer1)\n...\n```"
   },
   {
      "x": "[jit] Remove symbolic autodiff's reliance on shape information",
      "z": "This sounds like it should work. It's also interesting to note that existing formulas with multiple inputs, e.g., as in convolution double backwards (in `aten/src/ATen/native/Convolution.cpp`) already handle undefined inputs for `ggI`, `ggO`, etc. correctly.\n\nThat being said:\n\n> assuming that the backwards function has a single input and multiple outputs (which is the case for all of the ops we currently handle, and 90%+ of autograd formulas in general), we can treat AutogradZero as a poison value -- a single input that is AutogradZero makes an op's output become AutogradZero.\n\nYou shouldn't assume this if you want double backwards to work :)\n\n> To ensure we only generate AutogradZero in gradient graphs, the autodiff can introduce autogradZeroIfUndef nodes for any undefined input, which will be replaced during the autogradZero propagation pass with autogradZero.\n\nI would rather we encode linearity as an intrinsic property of operations, so that we can apply this optimization more widely (not just to backward graphs). It seems dangerous to assume that *any* computation reachable from `AutogradZero` is eligible for zeroing.",
      "y": "Existing formulas with multiple inputs, e.g., as in convolution double backwards (in `aten/src/ATen/native/Convolution.cpp`) already handle undefined inputs for `ggI`, `ggO`, etc. correctly.\n\n> assuming that the backwards function has a single input and multiple outputs (which is the case for all of the ops we currently handle, and 90%+ of autograd formulas in general), we can treat AutogradZero as a poison value -- a single input that is AutogradZero makes an op's output become AutogradZero.\n\nYou shouldn't assume this if you want double backwards to work :\n\n> To ensure we only generate AutogradZero in gradient graphs, the autodiff can introduce autogradZeroIfUndef nodes for any undefined input, which will be replaced during the autogradZero propagation pass with autogradZero.\n\nI would rather we encode linearity as an intrinsic property of operations, so that we can apply this optimization more widely (not just to backward graphs). It seems dangerous to assume that *any* computation reachable from `AutogradZero` is eligible for zeroing."
   },
   {
      "x": "Just updated source tree, build fails.",
      "z": "I don't really have anything to do with this. This started with the recent build system changes. Maybe @orionr can look into this. I also observe in my builds that these warnings are emitted, but the builds proceed fine.",
      "y": " This started with the recent build system changes. Maybe @orionr can look into this. I also observe in my builds that these warnings are emitted, but the builds proceed fine."
   },
   {
      "x": "Is a there a function like np.vectorize() that can realize an efficient value mapping?",
      "z": "Cross post from the forum: https://discuss.pytorch.org/t/how-to-realize-the-function-of-looking-up-table-with-pytorch-in-a-most-efficient-way/70747\n\nWe only answer questions on the forum and we use git issues only for bug/features. So we will answer there.",
      "y": "Cross post from the forum: https://discuss.pytorch.org/t/how-to-realize-the-function-of-looking-up-table-with-pytorch-in-a-most-efficient-way/70747\n\nWe only answer questions on the forum and we use git issues only for bug/features. So we will answer there."
   },
   {
      "x": "torch.Tensor does not have zeros_like_",
      "z": "Thank you for your report, we will accept a PR fixing this. ",
      "y": "Thank you for your report, we will accept a PR fixing this. "
   },
   {
      "x": "Error in forward() docs for RNNCell",
      "z": "Thanks for pointing that out, @valsworthen. We'll either fix the docs or add that functionality. It seems useful for the hidden state to default to 0 in my opinion so I'm leaning towards a solution involving adding the functionality, especially if it used to exist.",
      "y": "We'll either fix the docs or add that functionality. It seems useful for the hidden state to default to 0 in my opinion so I'm leaning towards a solution involving adding the functionality, especially if it used to exist."
   },
   {
      "x": "Is there a way to specify the -j flag for number of threads used to build the library?",
      "z": "Hi, you can specify number of parallel build jobs by setting `MAX_JOBS` env variable\n \n this should work: `env MAX_JOBS=4 python setup.py build`",
      "y": "Hi, you can specify number of parallel build jobs by setting `MAX_JOBS` env variable\n \n this should work: `env MAX_JOBS=4 python setup.py build`"
   },
   {
      "x": "Tensor length mismatch in index_put_ with torch.uint8 indices",
      "z": "Thank you for your report. This is indeed a bug, but the bug is not what it appears to be. Before bool tensors were supported, uint8 tensors could be used for indexing, but they were interpreted only as boolean mask, not as actual indices values. If your example is rewritten to use uint8 tensor as a boolean mask, it works:\n \n ```\n \n In [24]: import torch \n \n  ...: \n \n  ...: N = 21 \n \n  ...: \n \n  ...: dt = torch.int8 \n \n  ...: # device = \"cuda:0\" \n \n  ...: device = \"cpu\" \n \n  ...: \n \n  ...: a = torch.ones(N, dtype=dt, device=device) \n \n  ...: indices = torch.ones(N, dtype=torch.uint8, device=device) \n \n  ...: values = torch.ones(N, dtype=dt, device=device) \n \n  ...: \n \n  ...: print(a.shape) \n \n  ...: print(indices.shape) \n \n  ...: print(values.shape) \n \n  ...: \n \n  ...: a.index_put_((indices, ), values, accumulate=True) \n \n  ...: \n \n  ...: print(a) \n \n ```\n \n but we indeed should fix misinterpretation of uint8 tensor as actual indices values - that should be simply forbidden and error out. Setting high priority because of a crash.",
      "y": "Thank you for your report. This is indeed a bug, but the bug is not what it appears to be. Before bool tensors were supported, uint8 tensors could be used for indexing, but they were interpreted only as boolean mask, not as actual indices values. If your example is rewritten to use uint8 tensor as a boolean mask, it works:\n \n ```\n \n In [24]: import torch \n \n  ...: \n \n  ...: N = 21 \n \n  ...: \n \n  ...: dt = torch.int8 \n \n  ...: # device = \"cuda:0\" \n \n  ...: device = \"cpu\" \n \n  ...: \n \n  ...: a = torch.ones(N, dtype=dt, device=device) \n \n  ...: indices = torch.ones(N, dtype=torch.uint8, device=device) \n \n  ...: values = torch.ones(N, dtype=dt, device=device) \n \n  ...: \n \n  ...: print(a.shape) \n \n  ...: print(indices.shape) \n \n  ...: print(values.shape) \n \n  ...: \n \n  ...: a.index_put_((indices, ), values, accumulate=True) \n \n  ...: \n \n  ...: print(a) \n \n ```\n \n but we indeed should fix misinterpretation of uint8 tensor as actual indices values - that should be simply forbidden and error out. Setting high priority because of a crash."
   },
   {
      "x": "Flaky test test_quantized_rnn on Windows",
      "z": "Thanks for reporting! Temporarily disable the test: https://github.com/pytorch/pytorch/pull/33750",
      "y": "Thanks for reporting! Temporarily disable the test: https://github.com/pytorch/pytorch/pull/33750"
   },
   {
      "x": "Mypy cannot find torch.optim.AdamW",
      "z": "@yf225 You can close this issue as it has been resolved in [#34299](https://github.com/pytorch/pytorch/pull/34299)",
      "y": "close this issue as it has been resolved in [#34299](https://github.com/pytorch/pytorch/pull/34299)"
   },
   {
      "x": "Adam optimizer is using a deprecated add_() signature",
      "z": "This is fixed in #33428, waiting to be merged.",
      "y": "This is fixed in #33428, waiting to be merged."
   },
   {
      "x": "loss.backward() keeps running for hours",
      "z": "pytorch version 0.4.1 --->1.2.0, I also encountered the same situation",
      "y": "pytorch version 0.4.1 --->1.2.0, I also encountered the same situation"
   },
   {
      "x": "RFC: JIT C++ source code reorganization",
      "z": "LGTM.\n \n \n \n Where does Operator-related stuff go? e.g. `register_prim_ops`",
      "y": "LGTM.\n \n \n \n Where does Operator-related stuff go? e.g. `register_prim_ops`"
   },
   {
      "x": "Can't save to ostream using documentation example",
      "z": "@anjali411 Would you like to take a look at this?",
      "y": "Would you like to take a look at this?"
   },
   {
      "x": "Scalar cuda Tensors are automatically moved to different device",
      "z": "We should support this in autograd:\n \n \n \n 1) For backwards compatibility. There's likely code that depends on this (in the \"forward\" pass).\n \n 2) Even before we supported mixed-type operations, we supported mixing types with a scalar. The same logic applies to devices.\n \n \n \n It should be fixable at the place where the error is raised in the autograd engine, without too much work. If the `output_device != metadata.device()` (and the shape is a scalar), just copy to the metadata.device().",
      "y": "We should support this in autograd:\n \n \n \n 1) For backwards compatibility. There's likely code that depends on this (in the \"forward\" pass).\n \n 2) Even before we supported mixed-type operations, we supported mixing types with a scalar. The same logic applies to devices.\n \n \n \n It should be fixable at the place where the error is raised in the autograd engine, without too much work. If the `output_device != metadata.device()` (and the shape is a scalar), just copy to the metadata.device()."
   },
   {
      "x": "Segfault upon calling torch.Tensor on a gpu tensor",
      "z": "similar issue encounter here",
      "y": "similar issue encounter here"
   },
   {
      "x": "[jit] inplace `_construct` RecursiveScriptModule",
      "z": "I think it would if the annotations are preserved when copying these functions from `nn.Module` -> `ScriptModule`. Let me try and see.",
      "y": "I think it would if the annotations are preserved when copying these functions from `nn.Module` -> `ScriptModule`. Let me try and see."
   },
   {
      "x": "torch.blkdiag [A way to create a block-diagonal matrix]",
      "z": "u said it, then we better implement a sparse matrix version and a normal version. I have to say, a sparse `blkdiag` is faster than `blkdiag` of dense matrix, let alone a multiplication on dense matrix...",
      "y": "u said it, then we better implement a sparse matrix version and a normal version. I have to say, a sparse `blkdiag` is faster than `blkdiag` of dense matrix, let alone a multiplication on dense matrix..."
   },
   {
      "x": "Device Object doesn't work as JIT annotation in PY3",
      "z": "It's supposed to be `torch.device`",
      "y": "It's supposed to be `torch.device`"
   },
   {
      "x": "[libtorch, jit] Unexpected right-hand found in assignment in class body. This is not yet supported.",
      "z": "cc @driazati for Final constant handling",
      "y": "for Final constant handling"
   },
   {
      "x": "RuntimeError: CUDA error: no kernel image is available for execution on the device",
      "z": "```bash\n \n git clone https://github.com/pytorch/pytorch.git\n \n cd pytorch\n \n git checkout v1.3.1\n \n git submodule sync\n \n git submodule update --init --recursive\n \n export TORCH_CUDA_ARCH_LIST=\"3.5\"\n \n python setup.py install\n \n ```",
      "y": "```bash\n \n git clone https://github.com/pytorch/pytorch.git\n \n cd pytorch\n \n git checkout v1.3.1\n \n git submodule sync\n \n git submodule update --init --recursive\n \n export TORCH_CUDA_ARCH_LIST=\"3.5\"\n \n python setup.py install\n \n ```"
   },
   {
      "x": "Tutorials should be run in CI",
      "z": "I think it is too much to run pytorch/example CI on every pytorch/pytorch PR push or merge. What would you think of this strategy:\n \n - Run the CI on pytorch/examples on a daily cron trigger\n \n - If it fails open a new issue in that repo, we could use something like this github action https://github.com/rishabhgupta/git-action-issue.",
      "y": "I think it is too much to run pytorch/example CI on every pytorch/pytorch PR push or merge. What would you think of this strategy:\n \n - Run the CI on pytorch/examples on a daily cron trigger\n \n - If it fails open a new issue in that repo, we could use something like this github action https://github.com/rishabhgupta/git-action-issue."
   },
   {
      "x": "Memory Leak & Performance Decrease on AMD CPU with PyTorch >1.3",
      "z": "Sorry, I wasnt aware that this was still of any use. When cleaning my repos, I did not remember the use case of the repo and deleted it. However, I found a local copy on my machine and uploaded it again at:\n \n https://github.com/milutter/pytorch_debugging",
      "y": "When cleaning my repos, I did not remember the use case of the repo and deleted it. However, I found a local copy on my machine and uploaded it again at:\n \n https://github.com/milutter/pytorch_debugging"
   },
   {
      "x": "MKLDNN convolution leaks memory when input sizes vary",
      "z": "@SsnL @FerranPares , it seems a same problem said in https://github.com/pytorch/pytorch/issues/27971 which IDEEP have primitive cache for the performance, you can set **LRU_CACHE_CAPACITY** to a smaller number(the default number is 1024, you can set it to 1) to reduce the memory use. Thanks!",
      "y": "Same problem said in https://github.com/pytorch/pytorch/issues/27971 which IDEEP have primitive cache for the performance, you can set **LRU_CACHE_CAPACITY** to a smaller number(the default number is 1024, you can set it to 1) to reduce the memory use."
   },
   {
      "x": "Calling tensor.backward() from within a C++ extension hangs",
      "z": "FYI, @albanD as a forum responser.",
      "y": "as a forum responser."
   },
   {
      "x": "torch.mean keepdim is not working correctly",
      "z": "I think it's `[0]` that removes the first dimension",
      "y": "I think it's `[0]` that removes the first dimension"
   },
   {
      "x": "deterministic sampling",
      "z": "Just take the argmax of your distribution, or is there something I'm missing ?",
      "y": "Just take the argmax of your distribution"
   },
   {
      "x": "np.mean breaks when called on a list of cuda tensors",
      "z": "@ezyang is right that you shouldn't expect this to work, and the error is generated by NumPy, not PyTorch. Think of it as if you're sticking a strange Python object into a NumPy function: the NumPy function goes to look for an attribute but discovers it's not there. Fixing this likely requires changes in NumPy.",
      "y": "the error is generated by NumPy, not PyTorch. Think of it as if you're sticking a strange Python object into a NumPy function: the NumPy function goes to look for an attribute but discovers it's not there. Fixing this likely requires changes in NumPy."
   },
   {
      "x": "RuntimeError: block->inputs().size() >= num_initializers ASSERT FAILED",
      "z": "@robbine could you check if you apply https://github.com/pytorch/pytorch/pull/18139, whether your problem will go away?",
      "y": "check if you apply https://github.com/pytorch/pytorch/pull/18139, whether your problem will go away?"
   },
   {
      "x": "Multinomial (GPU ONLY) without replacement generates repeated items",
      "z": "So I ran Soumith's script on my branch of #14957 (which has a ~1 week old version of master, it's a debug build) and it didn't find anything in 10000 seeds (took ~8 hours on my GTX1080Ti).\n \n @ptrblck also ran it for a while (thanks!), but it seems nothing turned up.\n \n \n \n I'd be happy to look at it, but I'd need help with a reliable repro on master - I don't have many more GPU hours to spare.",
      "y": "I ran script on my branch of #14957 (which has a ~1 week old version of master, it's a debug build) and it didn't find anything in 10000 seeds (took ~8 hours on my GTX1080Ti).ran it for a while (thanks!), but it seems nothing turned up.\n \n \n \n I'd be happy to look at it, but I'd need help with a reliable repro on master - I don't have many more GPU hours to spare."
   },
   {
      "x": "Support for Nvidia Tesla T4",
      "z": "After installing Pytorch with the following instructions (as per https://pytorch.org/):\n \n ```\n \n pip install torch==1.6.0+cu101 torchvision==0.7.0+cu101 -f https://download.pytorch.org/whl/torch_stable.html\n \n ```\n \n \n \n I'm getting this error message:\n \n ```\n \n GeForce RTX 2080 Ti with CUDA capability sm_75 is not compatible with the current PyTorch installation.\n \n The current PyTorch install supports CUDA capabilities sm_37 sm_50 sm_60 sm_70.\n \n ```\n \n Is sm_75 code not included for cuda 10.1? Training still runs, but it seems not to take advantage of the instruction set.",
      "y": "After installing Pytorch with the following instructions (as per https://pytorch.org/):\n \n ```\n \n pip install torch==1.6.0+cu101 torchvision==0.7.0+cu101 -f https://download.pytorch.org/whl/torch_stable.html\n \n ```\n \n \n \n I'm getting this error message:\n \n ```\n \n GeForce RTX 2080 Ti with CUDA capability sm_75 is not compatible with the current PyTorch installation.\n \n The current PyTorch install supports CUDA capabilities sm_37 sm_50 sm_60 sm_70.\n \n ```\n \n Is sm_75 code not included for cuda 10.1? Training still runs, but it seems not to take advantage of the instruction set."
   },
   {
      "x": "enforce fail at inline_container.cc:137] . PytorchStreamReader failed reading zip archive: failed finding central directory",
      "z": "Thank you for the response. Our Python code is similar to this one:\n \n \n \n https://github.com/LaurentMazare/deep-models/blob/5aa353f88065150b0a024578f4af09d5c4f2d651/cifar-10/train.py\n \n \n \n We used the same way to generate \"model.pt\".\n \n \n \n Btw, when we cmake our testing app, we pointed to the libtorch downloaded (2/4/2019) with the following configurations:\n \n ![screen shot 2019-02-05 at 2 40 48 pm](https://user-images.githubusercontent.com/24870971/52309199-3aae3d80-2954-11e9-85dd-4ad09e76708c.png)",
      "y": "Thank you for the response. Our Python code is similar to this one:\n \n \n \n https://github.com/LaurentMazare/deep-models/blob/5aa353f88065150b0a024578f4af09d5c4f2d651/cifar-10/train.py\n \n \n \n We used the same way to generate \"model.pt\".\n \n \n \n Btw, when we cmake our testing app, we pointed to the libtorch downloaded (2/4/2019) with the following configurations:\n \n ![screen shot 2019-02-05 at 2 40 48 pm](https://user-images.githubusercontent.com/24870971/52309199-3aae3d80-2954-11e9-85dd-4ad09e76708c.png)"
   },
   {
      "x": "Defined layers by @property to allow better modularity",
      "z": "I see your point, and I totally agree! Thank you",
      "y": "I see your point, and I totally agree! Thank you"
   },
   {
      "x": "Implementing Normalizing Flows with TransformedDistribution",
      "z": "Would it make sense to define the TransformModule in PyTorch itself?",
      "y": "Would it make sense to define the TransformModule in PyTorch itself?"
   },
   {
      "x": "Pip upgrade to torch 1.0.1 breaks under Python 2.7 (No module named typing)",
      "z": "`pip install typing` should fix your problem.\n \n \n \n Did we add a runtime dependency on mypy? cc @ezyang",
      "y": "`pip install typing` should fix your problem.\n \n \n \n Did we add a runtime dependency on mypy? cc @ezyang"
   },
   {
      "x": "batch mode torch.cholesky does not raise RuntimeError on GPU if not pos. def.",
      "z": "I will take a look at this. Thank you for reporting.",
      "y": "I will take a look at this. Thank you for reporting."
   },
   {
      "x": "Namedtuples and compatibility for python < 3.2",
      "z": "I am currently reading these lines of tuple checks and report the unexpected behavior right now.",
      "y": "I am currently reading these lines of tuple checks and report the unexpected behavior right now."
   },
   {
      "x": "pytorch cpp api the derivative for 'target' is not implemented",
      "z": "it's still a bug, that we should fix. I believe we shouldn't have to `detach` explicitly, because we dont need to do that in python",
      "y": "it's still a bug, that we should fix. I believe we shouldn't have to `detach` explicitly, because we dont need to do that in python"
   },
   {
      "x": "cuDNN error: CUDNN_STATUS_INTERNAL_ERROR on cudnn.benchmark = True",
      "z": "We believe we have identified the root cause of this issue and are working on a fix in cuDNN.",
      "y": "We believe we have identified the root cause of this issue and are working on a fix in cuDNN."
   },
   {
      "x": "[Feature Request] HParams Class of TensorFlow",
      "z": "That would be a first step to support hparams in tensorboard. This is a really important missing feature.\n \n \n \n <img width=\"1225\" alt=\"Screen Shot 2019-05-20 at 16 59 07\" src=\"https://user-images.githubusercontent.com/8831940/58031415-a9d52980-7b20-11e9-9fa2-7c7f2d1a15cc.png\">\n \n \n \n EDIT: Should I create a feature request for that ?",
      "y": "That would be a first step to support hparams in tensorboard. This is a really important missing feature.\n \n \n \n <img width=\"1225\" alt=\"Screen Shot 2019-05-20 at 16 59 07\" src=\"https://user-images.githubusercontent.com/8831940/58031415-a9d52980-7b20-11e9-9fa2-7c7f2d1a15cc.png\">\n \n \n \n EDIT: Should I create a feature request for that ?"
   },
   {
      "x": "[jit] Support static lists/dicts as trace inputs",
      "z": "@thuyen try annotating your function. \n \n ```\n \n @torch.jit.script\n \n def dict_test(x: Dict[str, Tensor]):\n \n  return x['a']\n \n \n \n dict_input = dict(a=torch.rand(2, 10), b=torch.rand(2, 10))\n \n y = dict_test(dict_input)\n \n ```",
      "y": "try annotating your function. \n \n ```\n \n @torch.jit.script\n \n def dict_test(x: Dict[str, Tensor]):\n \n  return x['a']\n \n \n \n dict_input = dict(a=torch.rand(2, 10), b=torch.rand(2, 10))\n \n y = dict_test(dict_input)\n \n ```"
   },
   {
      "x": "Unable to cast .item() number to int",
      "z": "This worked! :)",
      "y": "This worked! :)"
   },
   {
      "x": "Not easy to use JIT from libtorch due to lack of explicit nvrtc dependency",
      "z": "@ebetica a try/catch `dlopen` is what would solve it on the cient end.",
      "y": "a try/catch `dlopen` is what would solve it on the cient end."
   },
   {
      "x": "Forward attribute access on DataParallel to underlying `module` attribute.",
      "z": "A shorter one:\n \n \n \n ```python\n \n class DataParallelPassthrough(torch.nn.DataParallel):\n \n  def __getattr__(self, name):\n \n  try:\n \n  return super().__getattr__(name)\n \n  except AttributeError:\n \n  return getattr(self.module, name)\n \n ```",
      "y": "A shorter one:\n \n \n \n ```python\n \n class DataParallelPassthrough(torch.nn.DataParallel):\n \n  def __getattr__(self, name):\n \n  try:\n \n  return super().__getattr__(name)\n \n  except AttributeError:\n \n  return getattr(self.module, name)\n \n ```"
   },
   {
      "x": "Number of CPU threads for the python process",
      "z": "In this case, you are seeing multithread usage from OpenMP parallelized CPU kernels, which run on multiple threads. You can control the number of threads with https://pytorch.org/docs/stable/torch.html#torch.set_num_threads although this was broken on some versions of PyTorch. You can also try setting `OMP_NUM_THREADS=1` and `MKL_NUM_THREADS=1` if that doesn't work for you.\n \n \n \n CC @SsnL, in case you have anything to add.",
      "y": "In this case, you are seeing multithread usage from OpenMP parallelized CPU kernels, which run on multiple threads. You can control the number of threads with https://pytorch.org/docs/stable/torch.html#torch.set_num_threads although this was broken on some versions of PyTorch. You can also try setting `OMP_NUM_THREADS=1` and `MKL_NUM_THREADS=1` if that doesn't work for you.\n "
   },
   {
      "x": "cpp api - IntList loses initial values",
      "z": "No, that's just UB and has nothing to do with the type you use. Basically, `ArrayRef`s don't own the memory (unlike e.g. `std::vector`s). So in your case, you construct an `ArrayRef` that points to the (stack allocated!) initializer list you pass to `a`, which is destructed immediately after the constructor finishes, so you end up using freed memory in `Print()`. Please use `std::vector` if you need to manage lifetime of your memory.",
      "y": "No, that's just UB and has nothing to do with the type you use. Basically, `ArrayRef`s don't own the memory (unlike e.g. `std::vector`s). So in your case, you construct an `ArrayRef` that points to the (stack allocated!) initializer list you pass to `a`, which is destructed immediately after the constructor finishes, so you end up using freed memory in `Print()`. Please use `std::vector` if you need to manage lifetime of your memory."
   },
   {
      "x": "Port `sign` operator from the TH code to Aten",
      "z": "Thanks @fmassa, I started working on this and CPU backend is available there (draft #22861)\n \n \n \n I'll be looking at CUDA later today.",
      "y": "I started working on this and CPU backend is available there (draft #22861)\n \n \n \n I'll be looking at CUDA later today."
   },
   {
      "x": "Inconsistent gradients through torch.symeig",
      "z": "@fmassa sorry I couldn\u00e2\u20ac\u2122t get this to earlier. I think we will have to modify the implementation of the gradients to make them symmetric. I can send in a PR with this fix tomorrow, if that\u00e2\u20ac\u2122s fine.",
      "y": " I think we will have to modify the implementation of the gradients to make them symmetric. I can send in a PR with this fix tomorrow, if that\u00e2\u20ac\u2122s fine."
   },
   {
      "x": "RuntimeError 'DivBackward0' nan values in its 0th output, but works when tensors loaded from disk?",
      "z": "Update: Finomnis on Stack Overflow found the issue - `with autograd.detect_anomaly():` was the cause of the problem. Removing that statement allows the model to train unimpeded. So now the question is - why was `autograd.detect_anomaly():` raising an error?",
      "y": "Update: Finomnis on Stack Overflow found the issue - `with autograd.detect_anomaly():` was the cause of the problem. Removing that statement allows the model to train unimpeded. So now the question is - why was `autograd.detect_anomaly():` raising an error?"
   },
   {
      "x": "Script freezes with no output when using DistributedDataParallel",
      "z": "You have timing code in there. It is possible that one process exits before the other one, causing the hang.\n \n \n \n If you do want to rely on timing, make sure to use timing information from a single process by broadcasting it, instead of having each process take its own measurements. With the snippet you posted it is possible for one process to break out of the loop before others do.",
      "y": "You have timing code in there. It is possible that one process exits before the other one, causing the hang.\n make sure to use timing information from a single process by broadcasting it, instead of having each process take its own measurements. With the snippet you posted it is possible for one process to break out of the loop before others do."
   },
   {
      "x": "Zero gradients beyond a certain buffer size on CUDA",
      "z": "Yes, in particular probably the specialization for \"accum=True\":\n \n \n \n https://github.com/pytorch/pytorch/blob/2ee0f0bc3a8b81e554e9cf7eded598ac7ecf33be/aten/src/ATen/native/cuda/Indexing.cu#L30-L40\n \n \n \n The `y` and `z` components of a grid of thread blocks have a maximum value of 65535.\n \n (https://docs.nvidia.com/cuda/cuda-c-programming-guide/index.html#features-and-technical-specifications)\n \n \n \n cc @ngimel",
      "y": "particular probably the specialization for \"accum=True\":\n\n https://github.com/pytorch/pytorch/blob/2ee0f0bc3a8b81e554e9cf7eded598ac7ecf33be/aten/src/ATen/native/cuda/Indexing.cu#L30-L40\n \n The `y` and `z` components of a grid of thread blocks have a maximum value of 65535.\n \n (https://docs.nvidia.com/cuda/cuda-c-programming-guide/index.html#features-and-technical-specifications)\n"
   },
   {
      "x": "Generated type hints failed with latest mypy version (0.720)",
      "z": "Confirmed fixed on master by #22841 (thanks @vishwakftw!)",
      "y": "Confirmed fixed on master by #22841 (thanks @vishwakftw!)"
   },
   {
      "x": "Argmax is not deterministic",
      "z": "NumPy returns the first occurrence. It might be worth matching their behavior.\n \n \n \n https://docs.scipy.org/doc/numpy/reference/generated/numpy.argmax.html\n \n \n \n \"In case of multiple occurrences of the maximum values, the indices corresponding to the first occurrence are returned.\"",
      "y": "NumPy returns the first occurrence. It might be worth matching their behavior.\n \n \n \n https://docs.scipy.org/doc/numpy/reference/generated/numpy.argmax.html\n \n \n \n \"In case of multiple occurrences of the maximum values, the indices corresponding to the first occurrence are returned.\""
   },
   {
      "x": "High CPU usage by torch.Tensor",
      "z": "@AravindChandradoss\n \n  \n \n Try : torch.set_num_threads(1)\n \n ```python\n \n import torch\n \n torch.set_num_threads(1)\n \n x = torch.randn(8, 224, 224, 3).permute(0, 3, 1, 2)\n \n while True:\n \n  x.cuda()\n \n ```",
      "y": "\n Try : torch.set_num_threads(1)\n \n ```python\n \n import torch\n \n torch.set_num_threads(1)\n \n x = torch.randn(8, 224, 224, 3).permute(0, 3, 1, 2)\n \n while True:\n \n  x.cuda()\n \n ```"
   },
   {
      "x": "Build Fails for Raspberry Pi 4 B: libcaffe2.so: undefined reference to `__atomic_xxx_8'",
      "z": "I spent a whole day and resolved this issue by the following steps:\n \n 1. sudo apt-get install libatomics-ops-dev\n \n 2. Change CMAKE_CXX_FLAGS variable in CMakeLists.txt file (in the main directory). i.e. add line ` set(CMAKE_CXX_FLAGS \"${CMAKE_CXX_FLAGS} -latomic\")`\n \n \n \n I also uploaded the torch 1.0.0 .whl file if anyone needs it: https://drive.google.com/file/d/1D3A5YSWiY-EnRWzWbzSqvj4YdY90wuXq/view?usp=sharing",
      "y": "I spent a whole day and resolved this issue by the following steps:\n \n 1. sudo apt-get install libatomics-ops-dev\n \n 2. Change CMAKE_CXX_FLAGS variable in CMakeLists.txt file (in the main directory). i.e. add line ` set(CMAKE_CXX_FLAGS \"${CMAKE_CXX_FLAGS} -latomic\")`\n \n \n \n I also uploaded the torch 1.0.0 .whl file if anyone needs it: https://drive.google.com/file/d/1D3A5YSWiY-EnRWzWbzSqvj4YdY90wuXq/view?usp=sharing"
   },
   {
      "x": "torch.flatten() returns a 0-dim (not 1-dim) tensor for 0-dim tensors",
      "z": "personal opinion: I think it should return a 1-dimensional tensor. It's essentially turning the range of dimensions (start_dim to end_dim) into a 1-dimensional tensor, and then stacking the dimensions before start_dim and after end_dim onto it.",
      "y": "personal opinion: I think it should return a 1-dimensional tensor. It's essentially turning the range of dimensions (start_dim to end_dim) into a 1-dimensional tensor, and then stacking the dimensions before start_dim and after end_dim onto it."
   },
   {
      "x": "TorchScript/ONNX unsupported constant kind s",
      "z": "This was fixed in https://github.com/pytorch/pytorch/pull/27566, which should go out as part of 1.4",
      "y": "This was fixed in https://github.com/pytorch/pytorch/pull/27566, which should go out as part of 1.4"
   },
   {
      "x": "Inconsistent Behavior in torch.distributions log_prob for float input in uniform distribution.",
      "z": "This should be an easy fix. Those lines should be made such that they work with Python floats as well, so replacing `ge` etc with `>=` would be a first step, or wrapping `value` in a tensor if it's a python float.\n \n \n \n https://github.com/pytorch/pytorch/blob/7a99f3987bde3f3508707990680a5f1f06d4c901/torch/distributions/uniform.py#L73-L75",
      "y": "This should be an easy fix. Those lines should be made such that they work with Python floats as well, so replacing `ge` etc with `>=` would be a first step, or wrapping `value` in a tensor if it's a python float.\n \n \n \n https://github.com/pytorch/pytorch/blob/7a99f3987bde3f3508707990680a5f1f06d4c901/torch/distributions/uniform.py#L73-L75"
   },
   {
      "x": "Substraction returns wrong result for bool tensors",
      "z": "Note that NumPy doesn't allow subtraction for bool:\n \n \n \n ```\n \n TypeError: numpy boolean subtract, the `-` operator, is deprecated, use the bitwise_xor, the `^` operator, or the logical_xor function instead.\n \n ```\n \n \n \n I'm not sure we should allow it either.",
      "y": "Note that NumPy doesn't allow subtraction for bool:\n \n \n \n ```\n \n TypeError: numpy boolean subtract, the `-` operator, is deprecated, use the bitwise_xor, the `^` operator, or the logical_xor function instead.\n \n ```\n \n \n \n I'm not sure we should allow it either."
   },
   {
      "x": "Conv2d ONNX export via TorchScript",
      "z": "Closing this as this issue is fixed in master. Try it out on the latest nightly release and let us know if it works. Feel free to reopen if it's not working and we can follow up. Thanks!",
      "y": "Closing this as this issue is fixed in master. Try it out on the latest nightly release and let us know if it works. Feel free to reopen if it's not working and we can follow up. Thanks!"
   },
   {
      "x": "Typing Error in nn.Module",
      "z": "This will get fixed when we move the Generic include inline into module.py... assuming I can fix the JIT bugs in this PR: https://github.com/pytorch/pytorch/pull/38211",
      "y": "This will get fixed when we move the Generic include inline into module.py... assuming I can fix the JIT bugs in this PR: https://github.com/pytorch/pytorch/pull/38211"
   },
   {
      "x": "DistributedSampler can't shuffle the dataset",
      "z": "@kangcp you need to call `set_epoch` as here: https://github.com/pytorch/examples/blob/master/imagenet/main.py#L239",
      "y": "you need to call `set_epoch` as here: https://github.com/pytorch/examples/blob/master/imagenet/main.py#L239"
   },
   {
      "x": "Improve gradient stability of logsumexp, softmax, log_softmax, logsigmoid at -inf (replace nan by zero)",
      "z": "At this time we have a workaround like below, it works well on all these three cases,\n \n 1. containing only `-float(\"inf\")`\n \n 2. containing no `-float(\"inf\")`\n \n 3. containing some `-float(\"inf\")`\n \n \n \n ```python\n \n import torch\n \n from torch import Tensor\n \n from torch import jit\n \n \n \n \n \n @jit.script\n \n def logsumexp(x: Tensor, dim: int) -> Tensor:\n \n  m, _ = x.max(dim=dim)\n \n  mask = m == -float('inf')\n \n \n \n  s = (x - m.masked_fill_(mask, 0).unsqueeze(dim=dim)).exp().sum(dim=dim)\n \n  return s.masked_fill_(mask, 1).log() + m.masked_fill_(mask, -float('inf'))\n \n \n \n \n \n def check(x: Tensor, fn):\n \n  x.grad = None\n \n  y = fn(x, dim=-1)\n \n  print(f'y => {y.view(-1)}')\n \n  y.backward(torch.ones_like(y))\n \n  print(f'x.grad => {a.grad.view(-1)}')\n \n \n \n \n \n if __name__ == '__main__':\n \n  a = torch.full((2, 2), -float('inf'), requires_grad=True)\n \n \n \n  check(a, logsumexp)\n \n  check(a, torch.logsumexp)\n \n \n \n  # y => tensor([-inf, -inf], grad_fn=<ViewBackward>)\n \n  # x.grad => tensor([0., 0., 0., 0.])\n \n  # y => tensor([-inf, -inf], grad_fn=<ViewBackward>)\n \n  # x.grad => tensor([nan, nan, nan, nan])\n \n \n \n  a = torch.randn((2, 2), requires_grad=True)\n \n \n \n  check(a, logsumexp)\n \n  check(a, torch.logsumexp)\n \n \n \n  # y => tensor([ 1.0678, -0.0330], grad_fn=<ViewBackward>)\n \n  # x.grad => tensor([0.7920, 0.2080, 0.8099, 0.1901])\n \n  # y => tensor([ 1.0678, -0.0330], grad_fn=<ViewBackward>)\n \n  # x.grad => tensor([0.7920, 0.2080, 0.8099, 0.1901])\n \n \n \n  a = torch.randn((2, 2))\n \n  a[0, 0] = -float('inf')\n \n  a.requires_grad_(True)\n \n \n \n  check(a, logsumexp)\n \n  check(a, torch.logsumexp)\n \n \n \n  # y => tensor([-0.0910, 1.5311], grad_fn=<ViewBackward>)\n \n  # x.grad => tensor([0.0000, 1.0000, 0.2983, 0.7017])\n \n  # y => tensor([-0.0910, 1.5311], grad_fn=<ViewBackward>)\n \n  # x.grad => tensor([0.0000, 1.0000, 0.2983, 0.7017])\n \n ```",
      "y": "At this time we have a workaround like below, it works well on all these three cases,\n \n 1. containing only `-float(\"inf\")`\n \n 2. containing no `-float(\"inf\")`\n \n 3. containing some `-float(\"inf\")`\n \n \n \n ```python\n \n import torch\n \n from torch import Tensor\n \n from torch import jit\n \n \n \n \n \n @jit.script\n \n def logsumexp(x: Tensor, dim: int) -> Tensor:\n \n  m, _ = x.max(dim=dim)\n \n  mask = m == -float('inf')\n \n \n \n  s = (x - m.masked_fill_(mask, 0).unsqueeze(dim=dim)).exp().sum(dim=dim)\n \n  return s.masked_fill_(mask, 1).log() + m.masked_fill_(mask, -float('inf'))\n \n \n \n \n \n def check(x: Tensor, fn):\n \n  x.grad = None\n \n  y = fn(x, dim=-1)\n \n  print(f'y => {y.view(-1)}')\n \n  y.backward(torch.ones_like(y))\n \n  print(f'x.grad => {a.grad.view(-1)}')\n \n \n \n \n \n if __name__ == '__main__':\n \n  a = torch.full((2, 2), -float('inf'), requires_grad=True)\n \n \n \n  check(a, logsumexp)\n \n  check(a, torch.logsumexp)\n \n \n \n  # y => tensor([-inf, -inf], grad_fn=<ViewBackward>)\n \n  # x.grad => tensor([0., 0., 0., 0.])\n \n  # y => tensor([-inf, -inf], grad_fn=<ViewBackward>)\n \n  # x.grad => tensor([nan, nan, nan, nan])\n \n \n \n  a = torch.randn((2, 2), requires_grad=True)\n \n \n \n  check(a, logsumexp)\n \n  check(a, torch.logsumexp)\n \n \n \n  # y => tensor([ 1.0678, -0.0330], grad_fn=<ViewBackward>)\n \n  # x.grad => tensor([0.7920, 0.2080, 0.8099, 0.1901])\n \n  # y => tensor([ 1.0678, -0.0330], grad_fn=<ViewBackward>)\n \n  # x.grad => tensor([0.7920, 0.2080, 0.8099, 0.1901])\n \n \n \n  a = torch.randn((2, 2))\n \n  a[0, 0] = -float('inf')\n \n  a.requires_grad_(True)\n \n \n \n  check(a, logsumexp)\n \n  check(a, torch.logsumexp)\n \n \n \n  # y => tensor([-0.0910, 1.5311], grad_fn=<ViewBackward>)\n \n  # x.grad => tensor([0.0000, 1.0000, 0.2983, 0.7017])\n \n  # y => tensor([-0.0910, 1.5311], grad_fn=<ViewBackward>)\n \n  # x.grad => tensor([0.0000, 1.0000, 0.2983, 0.7017])\n \n ```"
   },
   {
      "x": "test_debug_info is flaky",
      "z": "The error is:\n \n \n \n ```\n \n Jan 03 21:53:38 ......Process process 0:\n \n Jan 03 21:53:38 Traceback (most recent call last):\n \n Jan 03 21:53:38 File \"/opt/python/3.5/lib/python3.5/multiprocessing/process.py\", line 252, in _bootstrap\n \n Jan 03 21:53:38 self.run()\n \n Jan 03 21:53:38 File \"/opt/python/3.5/lib/python3.5/multiprocessing/process.py\", line 93, in run\n \n Jan 03 21:53:38 self._target(*self._args, **self._kwargs)\n \n Jan 03 21:53:38 File \"/var/lib/jenkins/workspace/test/common_distributed.py\", line 198, in _run\n \n Jan 03 21:53:38 getattr(self, test_name)()\n \n Jan 03 21:53:38 File \"/var/lib/jenkins/workspace/test/dist_utils.py\", line 96, in new_test_method\n \n Jan 03 21:53:38 return_value = old_test_method(self, *arg, **kwargs)\n \n Jan 03 21:53:38 File \"/var/lib/jenkins/workspace/test/rpc_test.py\", line 1325, in test_debug_info\n \n Jan 03 21:53:38 self.assertEqual(expected.keys(), info.keys())\n \n Jan 03 21:53:38 File \"/var/lib/jenkins/workspace/test/common_utils.py\", line 867, in assertEqual\n \n Jan 03 21:53:38 allow_inf=allow_inf)\n \n Jan 03 21:53:38 File \"/var/lib/jenkins/workspace/test/common_utils.py\", line 848, in assertEqual\n \n Jan 03 21:53:38 super(TestCase, self).assertEqual(x, y, message)\n \n Jan 03 21:53:38 File \"/opt/python/3.5/lib/python3.5/unittest/case.py\", line 829, in assertEqual\n \n Jan 03 21:53:38 assertion_func(first, second, msg=msg)\n \n Jan 03 21:53:38 File \"/opt/python/3.5/lib/python3.5/unittest/case.py\", line 1202, in assertMultiLineEqual\n \n Jan 03 21:53:38 self.fail(self._formatMessage(msg, standardMsg))\n \n Jan 03 21:53:38 File \"/opt/python/3.5/lib/python3.5/unittest/case.py\", line 670, in fail\n \n Jan 03 21:53:38 raise self.failureException(msg)\n \n Jan 03 21:53:38 AssertionError: 'num_pending_users' != 'thread_pool_size'\n \n Jan 03 21:53:38 - num_pending_users\n \n Jan 03 21:53:38 + thread_pool_size\n \n ```\n \n \n \n Seems to be related to adding `num_pending_users` in https://github.com/pytorch/pytorch/pull/31539? Although not sure why the test does not fail every time. Will investigate.",
      "y": "The error is:\n \n \n \n ```\n \n Jan 03 21:53:38 ......Process process 0:\n \n Jan 03 21:53:38 Traceback (most recent call last):\n \n Jan 03 21:53:38 File \"/opt/python/3.5/lib/python3.5/multiprocessing/process.py\", line 252, in _bootstrap\n \n Jan 03 21:53:38 self.run()\n \n Jan 03 21:53:38 File \"/opt/python/3.5/lib/python3.5/multiprocessing/process.py\", line 93, in run\n \n Jan 03 21:53:38 self._target(*self._args, **self._kwargs)\n \n Jan 03 21:53:38 File \"/var/lib/jenkins/workspace/test/common_distributed.py\", line 198, in _run\n \n Jan 03 21:53:38 getattr(self, test_name)()\n \n Jan 03 21:53:38 File \"/var/lib/jenkins/workspace/test/dist_utils.py\", line 96, in new_test_method\n \n Jan 03 21:53:38 return_value = old_test_method(self, *arg, **kwargs)\n \n Jan 03 21:53:38 File \"/var/lib/jenkins/workspace/test/rpc_test.py\", line 1325, in test_debug_info\n \n Jan 03 21:53:38 self.assertEqual(expected.keys(), info.keys())\n \n Jan 03 21:53:38 File \"/var/lib/jenkins/workspace/test/common_utils.py\", line 867, in assertEqual\n \n Jan 03 21:53:38 allow_inf=allow_inf)\n \n Jan 03 21:53:38 File \"/var/lib/jenkins/workspace/test/common_utils.py\", line 848, in assertEqual\n \n Jan 03 21:53:38 super(TestCase, self).assertEqual(x, y, message)\n \n Jan 03 21:53:38 File \"/opt/python/3.5/lib/python3.5/unittest/case.py\", line 829, in assertEqual\n \n Jan 03 21:53:38 assertion_func(first, second, msg=msg)\n \n Jan 03 21:53:38 File \"/opt/python/3.5/lib/python3.5/unittest/case.py\", line 1202, in assertMultiLineEqual\n \n Jan 03 21:53:38 self.fail(self._formatMessage(msg, standardMsg))\n \n Jan 03 21:53:38 File \"/opt/python/3.5/lib/python3.5/unittest/case.py\", line 670, in fail\n \n Jan 03 21:53:38 raise self.failureException(msg)\n \n Jan 03 21:53:38 AssertionError: 'num_pending_users' != 'thread_pool_size'\n \n Jan 03 21:53:38 - num_pending_users\n \n Jan 03 21:53:38 + thread_pool_size\n \n ```\n \n \n \n Seems to be related to adding `num_pending_users` in https://github.com/pytorch/pytorch/pull/31539? Although not sure why the test does not fail every time. Will investigate."
   },
   {
      "x": "The histogram is empty",
      "z": "Ah my bad, probably because I trained without calling `net.train()`, then try writes to histogram.\n \n ````python\n \n import torch\n \n from torch.utils.tensorboard import SummaryWriter\n \n \n \n writer = SummaryWriter(log_dir='runs/temp')\n \n net = torch.hub.load('RF5/danbooru-pretrained', 'resnet50')\n \n criterion = torch.nn.CrossEntropyLoss()\n \n optimizer = torch.optim.SGD(net.parameters(), lr=0.001, momentum=0.9)\n \n \n \n labels = torch.randint(10, size=(2,))\n \n for i in range(5):\n \n  # net.train() <--- uncomment this line then no error\n \n  print(i)\n \n  optimizer.zero_grad()\n \n  output = net(torch.randn(2, 3, 1, 1))\n \n  loss = criterion(output, labels)\n \n  loss.backward()\n \n  optimizer.step()\n \n \n \n  net.eval() # <--- or comment this line then no error as well\n \n  for name, w in net.named_parameters():\n \n  writer.add_histogram(name, w, i)\n \n ````",
      "y": "Ah my bad, probably because I trained without calling `net.train()`, then try writes to histogram.\n \n ````python\n \n import torch\n \n from torch.utils.tensorboard import SummaryWriter\n \n \n \n writer = SummaryWriter(log_dir='runs/temp')\n \n net = torch.hub.load('RF5/danbooru-pretrained', 'resnet50')\n \n criterion = torch.nn.CrossEntropyLoss()\n \n optimizer = torch.optim.SGD(net.parameters(), lr=0.001, momentum=0.9)\n \n \n \n labels = torch.randint(10, size=(2,))\n \n for i in range(5):\n \n  # net.train() <--- uncomment this line then no error\n \n  print(i)\n \n  optimizer.zero_grad()\n \n  output = net(torch.randn(2, 3, 1, 1))\n \n  loss = criterion(output, labels)\n \n  loss.backward()\n \n  optimizer.step()\n \n \n \n  net.eval() # <--- or comment this line then no error as well\n \n  for name, w in net.named_parameters():\n \n  writer.add_histogram(name, w, i)\n \n ````"
   },
   {
      "x": "About learning rate scheduler",
      "z": "Please update to the latest version of PyTorch.\n \n \n \n As discussed in #26423, `get_lr` was not meant to be the appropriate way of getting the learning rate. A warning is raised in #31125. \n \n \n \n The correct way is `scheduler.get_last_lr` or `optimizer.param_groups[0][\"lr\"]`.\n \n \n \n ```\n \n optimizer = optim.SGD(net.parameters(), lr=0.1)\n \n scheduler = optim.lr_scheduler.StepLR(optimizer, step_size=5, gamma=0.5)\n \n \n \n for i in range(15):\n \n  lr = scheduler.get_last_lr()[0]\n \n  lr1 = optimizer.param_groups[0][\"lr\"]\n \n  print(i, lr, lr1)\n \n  scheduler.step()\n \n ```\n \n \n \n I'll close this PR, but please feel free to re-open if the problem persists.",
      "y": "Please update to the latest version of PyTorch.\n \n \n \n As discussed in #26423, `get_lr` was not meant to be the appropriate way of getting the learning rate. A warning is raised in #31125. \n \n \n \n The correct way is `scheduler.get_last_lr` or `optimizer.param_groups[0][\"lr\"]`.\n \n \n \n ```\n \n optimizer = optim.SGD(net.parameters(), lr=0.1)\n \n scheduler = optim.lr_scheduler.StepLR(optimizer, step_size=5, gamma=0.5)\n \n \n \n for i in range(15):\n \n  lr = scheduler.get_last_lr()[0]\n \n  lr1 = optimizer.param_groups[0][\"lr\"]\n \n  print(i, lr, lr1)\n \n  scheduler.step()\n \n ```\n \n \n \n I'll close this PR, but please feel free to re-open if the problem persists."
   },
   {
      "x": "A pattern fuse or pattern replace tool for quantization.",
      "z": "@parvizp I just re-enables it few days ago: https://github.com/pytorch/pytorch/pull/29220\n \n Ideally we should actually fuse ConvBn in the aten function level, but that requires more refactoring and cleanup, that's why we are doing it in the module level right now.\n \n It is ready for common vision models as long as you can script them.",
      "y": "@parvizp I just re-enables it few days ago: https://github.com/pytorch/pytorch/pull/29220\n \n Ideally we should actually fuse ConvBn in the aten function level, but that requires more refactoring and cleanup, that's why we are doing it in the module level right now.\n \n It is ready for common vision models as long as you can script them."
   },
   {
      "x": "Third party PyTorch models may execute arbitrary code during deserialization",
      "z": "One option that might be worth considering is using https://docs.python.org/3/library/pickle.html#restricting-globals to try to make the unpickling more safe. (With perhaps an additional flag to remove those protections).",
      "y": "One option that might be worth considering is using https://docs.python.org/3/library/pickle.html#restricting-globals to try to make the unpickling more safe. (With perhaps an additional flag to remove those protections)."
   },
   {
      "x": "PyTorch serialization formats",
      "z": "@driazati I have little to say about the exact details of how we are version testing, or what the variants of the versions should be named (aligning them with PyTorch releases sounds reasonable). What I don't see in this discussion is whether or not the team is going to commit to accurately reporting versions on the file format going forward, and if so, what mechanisms we can put in place to make sure that we update it when we make changes to the format (since it seems the lightweight mechanism of code review isn't working). A simple stopgap is to have it report the version of PyTorch which exported the model...",
      "y": " little to say about the exact details of how we are version testing, or what the variants of the versions should be named (aligning them with PyTorch releases sounds reasonable, whether or not the team is going to commit to accurately reporting versions on the file format going forward, and if so, what mechanisms we can put in place to make sure that we update it when we make changes to the format (since it seems the lightweight mechanism of code review isn't working). A simple stopgap is to have it report the version of PyTorch which exported the model..."
   },
   {
      "x": "INT tensor with gradients",
      "z": "It seems that the quantized tensor is mainly **for inference** and the quantization-aware training utilizes fake quant and the data flow with gradient is still in float format. The quantized tensor does not support autograd and requres_grad (The following code snippet still raises \"**only Tensors of floating point dtype can require gradients**\"). \n \n ```\n \n int_tensor = torch.randint(0, 10, size=((4,4,3,3)), dtype=torch.uint8)\n \n scale, zero_point = 1e-1, 0\n \n q = torch._make_per_tensor_quantized_tensor(int_tensor, scale, zero_point)\n \n q.requires_grad = True\n \n ```\n \n \n \n However, we want to directly use INT8 tensor with gradients rather than fake quant.\n \n Then we can accelerate the forward process and save memory consumption **for training**.\n \n I wonder that if it is possible to support autograd and requires_grad for INT tensor.\n \n \n \n Thanks!\n \n \n \n @soumith",
      "y": "It seems that the quantized tensor is mainly **for inference** and the quantization-aware training utilizes fake quant and the data flow with gradient is still in float format. The quantized tensor does not support autograd and requres_grad (The following code snippet still raises \"**only Tensors of floating point dtype can require gradients**\"). \n \n ```\n \n int_tensor = torch.randint(0, 10, size=((4,4,3,3)), dtype=torch.uint8)\n \n scale, zero_point = 1e-1, 0\n \n q = torch._make_per_tensor_quantized_tensor(int_tensor, scale, zero_point)\n \n q.requires_grad = True\n \n ```\n \n \n \n However, we want to directly use INT8 tensor with gradients rather than fake quant.\n \n Then we can accelerate the forward process and save memory consumption **for training**.\n \n I wonder that if it is possible to support autograd and requires_grad for INT tensor.\n \n \n \n Thanks!\n \n \n \n @soumith"
   },
   {
      "x": "Segmentation fault in \"torch::autograd::Engine::evaluate_function\" in 1.3.1",
      "z": "Reading the code, I think this is just a simple data race. Patch coming.",
      "y": "Reading the code, I think this is just a simple data race. Patch coming."
   },
   {
      "x": "Output of batch doesn't contain the output of a subet of a batch",
      "z": "In the future, please ask questions on our forums, discuss.pytorch.org. We like to keep our issue tracker filled with issues and feature requests.",
      "y": "In the future, please ask questions on our forums, discuss.pytorch.org. We like to keep our issue tracker filled with issues and feature requests."
   },
   {
      "x": "[feature request] Set limit on GPU memory use",
      "z": "Wanted to support this feature request as well. \n \n \n \n For context, this could be used with the work done by Alibaba for kubernetes (See [Kubernetes #52757](https://github.com/kubernetes/kubernetes/issues/52757#issuecomment-464645334)). While there is probably a significant speed/latency penalty, it may still be beneficial for certain tasks.",
      "y": "Wanted to support this feature request as well. \n \n \n \n For context, this could be used with the work done by Alibaba for kubernetes (See [Kubernetes #52757](https://github.com/kubernetes/kubernetes/issues/52757#issuecomment-464645334)). While there is probably a significant speed/latency penalty, it may still be beneficial for certain tasks."
   },
   {
      "x": "[jit] Python built-in function support",
      "z": "are pull requests welcome for this issue? I am a first time contributor so dont know",
      "y": "are pull requests welcome for this issue? I am a first time contributor so dont know"
   },
   {
      "x": "distributed.all_gather function stuck when using NCCL backend",
      "z": "@lecoan\n \n \n \n 1. Before init the process group, call `torch.cuda.set_device(args.rank)` to assign different GPUs to different processes. \n \n 2. Change `tmp = [t.randn(5).cuda()] * 2` to `tmp = [t.randn(5).cuda() for _ in range(2)]`, otherwise you are using the same tensor to receive two outputs. \n \n \n \n Does this solve your problem?",
      "y": "\n 1. Before init the process group, call `torch.cuda.set_device(args.rank)` to assign different GPUs to different processes. \n \n 2. Change `tmp = [t.randn(5).cuda()] * 2` to `tmp = [t.randn(5).cuda() for _ in range(2)]`, otherwise you are using the same tensor to receive two outputs. \n \n \n \n Does this solve your problem?"
   },
   {
      "x": "<torch/torch.h> should be <torch/script.h>",
      "z": "@SPark9625 You want `torch/extension.h` for cpp extensions; `torch/torch.h` is valid for regular C++ use of Torch. `torch/script.h` is if you specifically want TorchScript functionality. I'm not sure about your include path question.\n \n \n \n You do have a legitimate bug report, which is that we are recommending `torch/torch.h` when it is inappropriate. I refiled your issue in tutorials https://github.com/pytorch/tutorials/issues/473 I verified the two occurrences of this include in PyTorch repo are proper:\n \n \n \n ```\n \n docs/cpp/source/installing.rst\n \n 47: #include <torch/torch.h>\n \n 56:of the PyTorch C++ API, including `torch/torch.h` is the most sure-proof way of\n \n \n \n docs/cpp/source/frontend.rst\n \n 42: #include <torch/torch.h>\n \n ```",
      "y": "You want `torch/extension.h` for cpp extensions; `torch/torch.h` is valid for regular C++ use of Torch. `torch/script.h` is if you specifically want TorchScript functionality. I'm not sure about your include path question.\n \n \n \n You do have a legitimate bug report, which is that we are recommending `torch/torch.h` when it is inappropriate. I refiled your issue in tutorials https://github.com/pytorch/tutorials/issues/473 I verified the two occurrences of this include in PyTorch repo are proper:\n \n \n \n ```\n \n docs/cpp/source/installing.rst\n \n 47: #include <torch/torch.h>\n \n 56:of the PyTorch C++ API, including `torch/torch.h` is the most sure-proof way of\n \n \n \n docs/cpp/source/frontend.rst\n \n 42: #include <torch/torch.h>\n \n ```"
   },
   {
      "x": "Magic value not found when loading traced model",
      "z": "```\n \n root@b54eacad8078:/data4/zjf/code# python collect_env.py\n \n Collecting environment information...\n \n PyTorch version: 1.0.0a0+4ec6bd7\n \n Is debug build: No\n \n CUDA used to build PyTorch: 9.1.85\n \n \n \n OS: Ubuntu 16.04.6 LTS\n \n GCC version: (Ubuntu 5.4.0-6ubuntu1~16.04.11) 5.4.0 20160609\n \n CMake version: version 3.5.1\n \n \n \n Python version: 3.6\n \n Is CUDA available: Yes\n \n CUDA runtime version: 9.1.85\n \n GPU models and configuration:\n \n GPU 0: Tesla P40\n \n GPU 1: GeForce GTX 1080 Ti\n \n GPU 2: GeForce GTX 1080 Ti\n \n GPU 3: GeForce GTX 1080 Ti\n \n GPU 4: Tesla P40\n \n \n \n Nvidia driver version: 390.25\n \n cuDNN version: /usr/lib/x86_64-linux-gnu/libcudnn.so.7.1.2\n \n \n \n Versions of relevant libraries:\n \n [pip3] msgpack-numpy==0.4.3.1\n \n [pip3] numpy==1.15.1\n \n [pip3] torch==1.0.0a0+4ec6bd7\n \n [pip3] torchfile==0.1.0\n \n [pip3] torchtext==0.3.1\n \n [pip3] torchvision==0.2.1\n \n [pip3] torchvision-nightly==0.2.1\n \n [conda] Could not collect\n \n ```",
      "y": "```\n \n root@b54eacad8078:/data4/zjf/code# python collect_env.py\n \n Collecting environment information...\n \n PyTorch version: 1.0.0a0+4ec6bd7\n \n Is debug build: No\n \n CUDA used to build PyTorch: 9.1.85\n \n \n \n OS: Ubuntu 16.04.6 LTS\n \n GCC version: (Ubuntu 5.4.0-6ubuntu1~16.04.11) 5.4.0 20160609\n \n CMake version: version 3.5.1\n \n \n \n Python version: 3.6\n \n Is CUDA available: Yes\n \n CUDA runtime version: 9.1.85\n \n GPU models and configuration:\n \n GPU 0: Tesla P40\n \n GPU 1: GeForce GTX 1080 Ti\n \n GPU 2: GeForce GTX 1080 Ti\n \n GPU 3: GeForce GTX 1080 Ti\n \n GPU 4: Tesla P40\n \n \n \n Nvidia driver version: 390.25\n \n cuDNN version: /usr/lib/x86_64-linux-gnu/libcudnn.so.7.1.2\n \n \n \n Versions of relevant libraries:\n \n [pip3] msgpack-numpy==0.4.3.1\n \n [pip3] numpy==1.15.1\n \n [pip3] torch==1.0.0a0+4ec6bd7\n \n [pip3] torchfile==0.1.0\n \n [pip3] torchtext==0.3.1\n \n [pip3] torchvision==0.2.1\n \n [pip3] torchvision-nightly==0.2.1\n \n [conda] Could not collect\n \n ```"
   },
   {
      "x": "\"dim\" argument for .unfold method doesn't work",
      "z": "Related to https://github.com/pytorch/pytorch/issues/8698",
      "y": "Related to https://github.com/pytorch/pytorch/issues/8698"
   },
   {
      "x": "Finding the source function",
      "z": "Yes.",
      "y": "Yes."
   },
   {
      "x": "[CUDA] torch.sum over smaller inner dimension is 4x slower than it should be",
      "z": "@colesbury please feel free to unassign yourself or deprioritize as necessary.",
      "y": "please feel free to unassign yourself or deprioritize as necessary."
   },
   {
      "x": "Wrong derivative backpropagating through Cholesky factorization?",
      "z": "Upon further investigation, I think that the derivative is correct, but misleading.\n \n \n \n As it is implemented, `mat.grad` produced by `torch.cholesky` is lower triangular. This is because `mat.grad[i, j]` is assigned the gradient for `mat[i, j] = mat[j, i]`.\n \n \n \n Take the PD matrix:\n \n ```python\n \n mat = torch.tensor([\n \n  [3, 1, 0.5, 0],\n \n  [1, 3, -1, 1],\n \n  [0.5, -1, 4, 2],\n \n  [0, 1, 2, 4]\n \n ])\n \n ```\n \n \n \n If you construct `mat` in the following way:\n \n ```python\n \n off_diag = torch.tensor([\n \n  [0.0000, 0.0000, 0.0000, 0.0000],\n \n  [ 1.0000, 0.0000, 0.0000, 0.0000],\n \n  [ 0.5000, -1.0000, 0.0000, 0.0000],\n \n  [ 0.0000, 1.0000, 2.0000, 0.0000]\n \n ], requires_grad=True)\n \n diag = torch.tensor([3, 3, 4, 4], requires_grad=True)\n \n mat = off_diag + off_diag.t() + diag.diag()\n \n ```\n \n \n \n then the gradients of `off_diag` and `diag` be the same for Cholesky solves as well as direct inversion solves.",
      "y": "Upon further investigation, I think that the derivative is correct, but misleading.\n \n \n \n As it is implemented, `mat.grad` produced by `torch.cholesky` is lower triangular. This is because `mat.grad[i, j]` is assigned the gradient for `mat[i, j] = mat[j, i]`.\n \n \n \n Take the PD matrix:\n \n ```python\n \n mat = torch.tensor([\n \n  [3, 1, 0.5, 0],\n \n  [1, 3, -1, 1],\n \n  [0.5, -1, 4, 2],\n \n  [0, 1, 2, 4]\n \n ])\n \n ```\n \n \n \n If you construct `mat` in the following way:\n \n ```python\n \n off_diag = torch.tensor([\n \n  [0.0000, 0.0000, 0.0000, 0.0000],\n \n  [ 1.0000, 0.0000, 0.0000, 0.0000],\n \n  [ 0.5000, -1.0000, 0.0000, 0.0000],\n \n  [ 0.0000, 1.0000, 2.0000, 0.0000]\n \n ], requires_grad=True)\n \n diag = torch.tensor([3, 3, 4, 4], requires_grad=True)\n \n mat = off_diag + off_diag.t() + diag.diag()\n \n ```\n \n \n \n then the gradients of `off_diag` and `diag` be the same for Cholesky solves as well as direct inversion solves."
   },
   {
      "x": "Inconsistent recovery from CUDA OOMs",
      "z": "This is quite serious and should be dealt with before the next PyTorch release. It appears to be a leak in the autograd engine -- note that if you remove the `loss.backward()` line in @stephenroller's test case all the tests pass (except for ddp_multi which has a different error that also should be fixed).\n \n \n \n Basically, if you're lucky and the OOM happens during forward it's recoverable. If it happens during backward() it leaks memory and is effectively unrecoverable. Unclear if this is a regression or also a bug in older versions of PyTorch.\n \n \n \n I would suspect that some tasks live on in some of the Engine state. (Are the ready_queues empty after an error?)",
      "y": "This is quite serious and should be dealt with before the next PyTorch release. It appears to be a leak in the autograd engine -- note that if you remove the `loss.backward()` line in @stephenroller's test case all the tests pass (except for ddp_multi which has a different error that also should be fixed).\n \n \n \n Basically, if you're lucky and the OOM happens during forward it's recoverable. If it happens during backward() it leaks memory and is effectively unrecoverable. Unclear if this is a regression or also a bug in older versions of PyTorch.\n \n \n \n I would suspect that some tasks live on in some of the Engine state. (Are the ready_queues empty after an error?)"
   },
   {
      "x": "matmul uses too much memory in some batched cases",
      "z": "The bug is still there for A.ndim is 3 and B.ndim is 3\n \n please run the code below with version 1.8.1\n \n ```\n \n x = torch.randn(1, 4096, 4096)\n \n y = torch.randn(192, 4096, 1)\n \n z1 = torch.bmm(x.expand(192, 4096, 4096), y) # it works\n \n z2 = torch.matmul(x, y) # out of memory",
      "y": "The bug is still there for A.ndim is 3 and B.ndim is 3\n \n please run the code below with version 1.8.1\n \n ```\n \n x = torch.randn(1, 4096, 4096)\n \n y = torch.randn(192, 4096, 1)\n \n z1 = torch.bmm(x.expand(192, 4096, 4096), y) # it works\n \n z2 = torch.matmul(x, y) # out of memory"
   },
   {
      "x": "cannot import name '_update_worker_pids' from 'torch._C'",
      "z": "You should remove your previous install first. Try \n \n ```sh\n \n conda remove pytorch torchvision -y\n \n pip uninstall torch -y\n \n pip uninstall torch -y # yes twice\n \n conda install pytorch torchvision -c pytorch\n \n ```",
      "y": "You should remove your previous install first. Try \n \n ```sh\n \n conda remove pytorch torchvision -y\n \n pip uninstall torch -y\n \n pip uninstall torch -y # yes twice\n \n conda install pytorch torchvision -c pytorch\n \n ```"
   },
   {
      "x": "Using F.affine_grid caused cuDNN error: CUDNN_STATUS_EXECUTION_FAILED (PyTorch version: 1.0.1.post2)",
      "z": "i use pytorch 1.0.1.post2, the same problem.",
      "y": "i use pytorch 1.0.1.post2, the same problem."
   },
   {
      "x": "addmv performance 2.8x worse for non-square matrix",
      "z": "I am able to reproduce the results on my MBP. Will take a look and try to understand what's going on.",
      "y": "I am able to reproduce the results on my MBP. Will take a look and try to understand what's going on."
   },
   {
      "x": "multinomial performance regressed 2x from 0.4.1 to 1.0.1",
      "z": "closed via #17121",
      "y": "closed via #17121"
   },
   {
      "x": "Autograd custom functions do not work with keyword arguments",
      "z": "You can do:\n \n \n \n ```python\n \n # You can move the default value declaration here and remove it from the Function.\n \n def my_sin(input, factor=1):\n \n  return MySin.apply(input, factor)\n \n \n \n # Or if you prefer to keep the default value in the Function.\n \n def my_sin(input, factor=None):\n \n  if factor is None:\n \n  return MySin.apply(input)\n \n  else:\n \n  return MySin.apply(input, factor)\n \n ```",
      "y": "You can do:\n \n \n \n ```python\n \n # You can move the default value declaration here and remove it from the Function.\n \n def my_sin(input, factor=1):\n \n  return MySin.apply(input, factor)\n \n \n \n # Or if you prefer to keep the default value in the Function.\n \n def my_sin(input, factor=None):\n \n  if factor is None:\n \n  return MySin.apply(input)\n \n  else:\n \n  return MySin.apply(input, factor)\n \n ```"
   },
   {
      "x": "Full-range random_() generation broken for cuda.IntTensor, cuda.LongTensor and LongTensor.",
      "z": "```\n \n In [1]: import torch\n \n \n \n In [2]: \"{0:b}\".format(torch.empty(1, dtype=torch.int64, device='cpu').random_(torch.iinfo(torch.int64).max, None)[0])\n \n Out[2]: '111111111111111111111111111111111111111111111111111111111111111'\n \n \n \n In [3]: \"{0:b}\".format(torch.empty(1, dtype=torch.int64, device='cuda').random_(torch.iinfo(torch.int64).max, None)[0])\n \n Out[3]: '111111111111111111111111111111111111111111111111111111111111111'\n \n ```",
      "y": "```\n \n In [1]: import torch\n \n \n \n In [2]: \"{0:b}\".format(torch.empty(1, dtype=torch.int64, device='cpu').random_(torch.iinfo(torch.int64).max, None)[0])\n \n Out[2]: '111111111111111111111111111111111111111111111111111111111111111'\n \n \n \n In [3]: \"{0:b}\".format(torch.empty(1, dtype=torch.int64, device='cuda').random_(torch.iinfo(torch.int64).max, None)[0])\n \n Out[3]: '111111111111111111111111111111111111111111111111111111111111111'\n \n ```"
   },
   {
      "x": "Building test_cpp_extensions fails for macOS + CUDA",
      "z": "I just tested this on Linux, and yes, `ccache` does seem to cause it.",
      "y": "I just tested this on Linux, and yes, `ccache` does seem to cause it."
   },
   {
      "x": "cpp_extension._is_binary_build() is not reliable",
      "z": "1. I've stopped relying on `_is_binary_build` for CXX ABI flags, so hopefully it shouldn't matter anymore for cpp extensions purposes.\n \n \n \n 2. 1.1.0 tag has `1.1.0` as the version, no git hash.",
      "y": "1. I've stopped relying on `_is_binary_build` for CXX ABI flags, so hopefully it shouldn't matter anymore for cpp extensions purposes.\n \n \n \n 2. 1.1.0 tag has `1.1.0` as the version, no git hash."
   },
   {
      "x": "namespaces torch.cuda, torch.device missing from pyi",
      "z": "I am using `torch==1.0.1.post2` and `Pycharm professional 2019.1`. I tested and sorted out the situation mentioned by everyone in front.\n \n \n \n #### can't find reference\n \n \n \n * `torch.cat`\n \n * `torch.clamp`\n \n * `torch.exp`\n \n * `torch.FloatTensor` **deprecated**\n \n * `torch.from_numpy` **deprecated**\n \n * `torch.log`\n \n * `torch.LongTensor` **deprecated**\n \n * `torch.max`\n \n * `torch.min`\n \n * `torch.ones`\n \n * `torch.randn`\n \n * `torch.reshape`\n \n * `torch.sum`\n \n * `torch.zeros`\n \n \n \n #### not callable\n \n \n \n * `torch.tensor`\n \n * `torch.as_tensor`",
      "y": "I am using `torch==1.0.1.post2` and `Pycharm professional 2019.1`. I tested and sorted out the situation mentioned by everyone in front.\n \n \n \n #### can't find reference\n \n \n \n * `torch.cat`\n \n * `torch.clamp`\n \n * `torch.exp`\n \n * `torch.FloatTensor` **deprecated**\n \n * `torch.from_numpy` **deprecated**\n \n * `torch.log`\n \n * `torch.LongTensor` **deprecated**\n \n * `torch.max`\n \n * `torch.min`\n \n * `torch.ones`\n \n * `torch.randn`\n \n * `torch.reshape`\n \n * `torch.sum`\n \n * `torch.zeros`\n \n \n \n #### not callable\n \n \n \n * `torch.tensor`\n \n * `torch.as_tensor`"
   },
   {
      "x": "Batchnormalization fails with CUDA on very large batches",
      "z": "https://github.com/pytorch/pytorch/blob/f8d4a14f6dc7c709c86b8bb25ccaaa486470e4a7/aten/src/ATen/native/cuda/Normalization.cuh#L443\n \n \n \n This limiter on the grid dimension might exceeds the maximum 65535 on y/z dimension.\n \n Should be a one liner fix. Let me get that.",
      "y": "https://github.com/pytorch/pytorch/blob/f8d4a14f6dc7c709c86b8bb25ccaaa486470e4a7/aten/src/ATen/native/cuda/Normalization.cuh#L443\n \n \n \n This limiter on the grid dimension might exceeds the maximum 65535 on y/z dimension.\n \n Should be a one liner fix. Let me get that."
   },
   {
      "x": "Tensor class should implement __contains__",
      "z": "You may also need `__reversed__` in newer versions of Python",
      "y": "You may also need `__reversed__` in newer versions of Python"
   },
   {
      "x": "cuDNN version mismatch (again)",
      "z": "@tangxiangru I installed a lower version of cudnn and it worked in my case\n \n try conda install cudnn=7.1.2",
      "y": " I installed a lower version of cudnn and it worked in my case\n \n try conda install cudnn=7.1.2"
   },
   {
      "x": "Generate CircleCI config.yml from a script",
      "z": "Sounds reasonable. It's really funny how you always end up writing a yaml generator for this kind of thing lol",
      "y": "Sounds reasonable. It's really funny how you always end up writing a yaml generator for this kind of thing lol"
   },
   {
      "x": "Previous dll loading tricks don't work anymore with newer Anaconda Python on Windows",
      "z": "Hi, I'm from Anaconda. We'd like to actually not require this change on your side, if we can avoid it. It's already gone in, and that's fine, but we want to un-break the existing packages if we can. We've already done a lot of work to re-sync PATH changes in our latest efforts on that python patch (https://github.com/AnacondaRecipes/python-feedstock/blob/master/recipe/0020-Add-CondaEcosystemModifyDllSearchPath.patch). For some reason, cuda detection is still not working correctly. Are there any specific libraries or paths from which loads happen that we should watch out for? We generally use procmon to debug this stuff, and knowing the name of a library that should be loaded can dramatically reduce the amount of stuff that we need to look through.",
      "y": " We'd like to actually not require this change on your side, if we can avoid it. It's already gone in, and that's fine, but we want to un-break the existing packages if we can. We've already done a lot of work to re-sync PATH changes in our latest efforts on that python patch (https://github.com/AnacondaRecipes/python-feedstock/blob/master/recipe/0020-Add-CondaEcosystemModifyDllSearchPath.patch). For some reason, cuda detection is still not working correctly. Are there any specific libraries or paths from which loads happen that we should watch out for? We generally use procmon to debug this stuff, and knowing the name of a library that should be loaded can dramatically reduce the amount of stuff that we need to look through."
   },
   {
      "x": "[JIT] terminate called after throwing an instance of 'torch::jit::script::ErrorReport'",
      "z": "I am using C++ version of torch from here ```https://download.pytorch.org/libtorch/cu90/libtorch-shared-with-deps-latest.zip```\n \n \n \n Pytorch version is from master branch.",
      "y": "I am using C++ version of torch from here ```https://download.pytorch.org/libtorch/cu90/libtorch-shared-with-deps-latest.zip```\n \n \n \n Pytorch version is from master branch."
   },
   {
      "x": "Allow DataParallel to wrap CPU modules",
      "z": "> Creating a model on CPU and then wrapping the model with `DataParallel` should automatically replicate the model on destination GPUs. Are there any reason to enforce that `DataParallel`'s input model must be on GPU?\n \n \n \n This is not true. The model is broadcast at the beginning of each forward, not when constructing the `DataParallel` wrapper. The disadvantage of having the model on CPU, of course, is that the gradients are reduced to **CPU** at each iteration, which is slow and undesirable. IMO, automatically convert to one GPU upon construction is also not desirable because:\n \n 1. Users may save a pointer to the wrapped module and reasonably expect it to still be on the original device.\n \n 2. It can initialize a CUDA context, which isn't obvious to users that it should.",
      "y": "> Creating a model on CPU and then wrapping the model with `DataParallel` should automatically replicate the model on destination GPUs. Are there any reason to enforce that `DataParallel`'s input model must be on GPU?\n \n \n \n This is not true. The model is broadcast at the beginning of each forward, not when constructing the `DataParallel` wrapper. The disadvantage of having the model on CPU, of course, is that the gradients are reduced to **CPU** at each iteration, which is slow and undesirable. IMO, automatically convert to one GPU upon construction is also not desirable because:\n \n 1. Users may save a pointer to the wrapped module and reasonably expect it to still be on the original device.\n \n 2. It can initialize a CUDA context, which isn't obvious to users that it should."
   },
   {
      "x": "Saved PyTorch network does not load in libtorch C++",
      "z": "you must've missed it in the tutorials, `torch.save` saves in python pickle format, it cannot be loaded into `libtorch`. `model.save` of a torch.jit.ScriptModule object can save something in a format that libtorch can loadl",
      "y": "you must've missed it in the tutorials, `torch.save` saves in python pickle format, it cannot be loaded into `libtorch`. `model.save` of a torch.jit.ScriptModule object can save something in a format that libtorch can loadl"
   },
   {
      "x": "drop_last (bool, optional) - more options needed",
      "z": "I have following order in my head: idea, notion, concept, definition. Idea can be dumb, notion can be whimsy, concept sounds more serious, definition is precise. We don't even have to agree to continue main discussion.\n \n \n \n 1. I finally get why you said 'if I want to discard the notion of epoch'. Maybe I could forget about epoch in order to not lose any data.\n \n 2. If iterator knows in advance amount of all samples, it can plan accordingly. If you say iterator does not know amount of all samples in advance, that is only for the first epoch. In all following epochs it can plan accordingly.",
      "y": "I have following order in my head: idea, notion, concept, definition. Idea can be dumb, notion can be whimsy, concept sounds more serious, definition is precise. We don't even have to agree to continue main discussion.\n \n \n \n 1. I finally get why you said 'if I want to discard the notion of epoch'. Maybe I could forget about epoch in order to not lose any data.\n \n 2. If iterator knows in advance amount of all samples, it can plan accordingly. If you say iterator does not know amount of all samples in advance, that is only for the first epoch. In all following epochs it can plan accordingly."
   },
   {
      "x": "Numpy-Like RandomState object",
      "z": "torch.Generator() is now documented here: https://pytorch.org/docs/master/torch.html#generators and should behave like numpy `RandomState` now. Following is an example usage. @petered Please feel free to document here if there is something we have missed, otherwise we can close this issue.\n \n \n \n ```\n \n >>> import torch\n \n >>> a = torch.Generator(device='cuda')\n \n >>> b = torch.FloatTensor(10).cuda().uniform_(generator=a)\n \n >>> b\n \n tensor([0.3931, 0.5116, 0.8851, 0.4404, 0.8998, 0.1218, 0.5304, 0.3333, 0.2389,\n \n  0.5041], device='cuda:0')\n \n >>> a = torch.Generator()\n \n >>> b = torch.FloatTensor(10).cuda().uniform_(generator=a)\n \n Traceback (most recent call last):\n \n  File \"<stdin>\", line 1, in <module>\n \n RuntimeError: Expected a 'cuda' device type for generator but found 'cpu'\n \n \n \n >>> a = torch.Generator(device='cuda')\n \n >>> torch.normal(torch.Tensor([0.5]).cuda(), 0.5, generator=a)\n \n tensor([-0.3292], device='cuda:0')\n \n \n \n >>> a = torch.Generator()\n \n >>> torch.normal(torch.Tensor([0.5]).cuda(), 0.5, generator=a)\n \n Traceback (most recent call last):\n \n  File \"<stdin>\", line 1, in <module>\n \n RuntimeError: Expected a 'cuda' device type for generator but found 'cpu'\n \n \n \n ```",
      "y": "torch.Generator() is now documented here: https://pytorch.org/docs/master/torch.html#generators and should behave like numpy `RandomState` now. Following is an example usage. @petered Please feel free to document here if there is something we have missed, otherwise we can close this issue.\n \n \n \n ```\n \n >>> import torch\n \n >>> a = torch.Generator(device='cuda')\n \n >>> b = torch.FloatTensor(10).cuda().uniform_(generator=a)\n \n >>> b\n \n tensor([0.3931, 0.5116, 0.8851, 0.4404, 0.8998, 0.1218, 0.5304, 0.3333, 0.2389,\n \n  0.5041], device='cuda:0')\n \n >>> a = torch.Generator()\n \n >>> b = torch.FloatTensor(10).cuda().uniform_(generator=a)\n \n Traceback (most recent call last):\n \n  File \"<stdin>\", line 1, in <module>\n \n RuntimeError: Expected a 'cuda' device type for generator but found 'cpu'\n \n \n \n >>> a = torch.Generator(device='cuda')\n \n >>> torch.normal(torch.Tensor([0.5]).cuda(), 0.5, generator=a)\n \n tensor([-0.3292], device='cuda:0')\n \n \n \n >>> a = torch.Generator()\n \n >>> torch.normal(torch.Tensor([0.5]).cuda(), 0.5, generator=a)\n \n Traceback (most recent call last):\n \n  File \"<stdin>\", line 1, in <module>\n \n RuntimeError: Expected a 'cuda' device type for generator but found 'cpu'\n \n \n \n ```"
   },
   {
      "x": "libtorch elevated memory usage.",
      "z": "@dawnwch The equivalent to `with torch.no_grad()` in C++ is `torch::NoGradGuard no_grad`. For example:\n \n Python:\n \n ```python\n \n with torch.no_grad():\n \n  module.weight += 1\n \n ```\n \n C++:\n \n ```cpp\n \n {\n \n  torch::NoGradGuard no_grad;\n \n  module->weight += 1;\n \n } // Note that anything out of this scope will still record gradients\n \n ``` \n \n Please try it out and let me know if it resolves the memory usage issue.",
      "y": "equivalent to `with torch.no_grad()` in C++ is `torch::NoGradGuard no_grad`. For example:\n \n Python:\n \n ```python\n \n with torch.no_grad():\n \n  module.weight += 1\n \n ```\n \n C++:\n \n ```cpp\n \n {\n \n  torch::NoGradGuard no_grad;\n \n  module->weight += 1;\n \n } // Note that anything out of this scope will still record gradients\n \n ``` \n \n Please try it out and let me know if it resolves the memory usage issue."
   },
   {
      "x": "[JIT] Scripting arguments for call are not valid",
      "z": "Hi, is there an update on this issue? I also ran into this error. Here's my code snippet\n \n ```\n \n import torch\n \n from torch import nn\n \n class CrossScale(torch.jit.ScriptModule):\n \n  __constants__ = ['xc']\n \n \n \n  def __init__(self):\n \n  super(CrossScale, self).__init__()\n \n  self.xc = nn.ModuleList((nn.Conv2d(2, 2, 2, bias=True), nn.Upsample(scale_factor=2, mode='bilinear')))\n \n \n \n  @torch.jit.script_method\n \n  def forward(self, x):\n \n  cols = []\n \n  i = 0\n \n  for xc in self.xc:\n \n  out = xc(x[i])\n \n  i += 1\n \n  cols.append(out)\n \n  return cols\n \n \n \n \n \n if __name__ == \"__main__\":\n \n  cs = CrossScale()\n \n ```\n \n Changing `scale_factor=2` to `scale_factor=float(2)` fixes the issue.",
      "y": "Hi, is there an update on this issue? I also ran into this error. Here's my code snippet\n \n ```\n \n import torch\n \n from torch import nn\n \n class CrossScale(torch.jit.ScriptModule):\n \n  __constants__ = ['xc']\n \n \n \n  def __init__(self):\n \n  super(CrossScale, self).__init__()\n \n  self.xc = nn.ModuleList((nn.Conv2d(2, 2, 2, bias=True), nn.Upsample(scale_factor=2, mode='bilinear')))\n \n \n \n  @torch.jit.script_method\n \n  def forward(self, x):\n \n  cols = []\n \n  i = 0\n \n  for xc in self.xc:\n \n  out = xc(x[i])\n \n  i += 1\n \n  cols.append(out)\n \n  return cols\n \n \n \n \n \n if __name__ == \"__main__\":\n \n  cs = CrossScale()\n \n ```\n \n Changing `scale_factor=2` to `scale_factor=float(2)` fixes the issue."
   },
   {
      "x": "[Running on windows 10] cuda runtime error (30) : unknown error at ..\\aten\\src\\THC\\THCGeneral.cpp:87",
      "z": "OK, I did some extra tests, and it seems that it is some weird behavior **only when running on an interactive shell**. Here's what I have done (step-by-step)\n \n \n \n 1. Prepare a simple file with the example:\n \n ```\n \n > type torch_test.ipy\n \n import torch\n \n print(\"torch.cuda.is_available() =\", torch.cuda.is_available())\n \n print(\"torch.cuda.device_count() =\", torch.cuda.device_count())\n \n print(\"torch.cuda.device('cuda') =\", torch.cuda.device('cuda'))\n \n print(\"torch.cuda.current_device() =\", torch.cuda.current_device())\n \n ```\n \n \n \n I can run this file with either `Python `or `iPython`, and it all works fine:\n \n ```\n \n > python torch_test.ipy\n \n torch.cuda.is_available() = True\n \n torch.cuda.device_count() = 1\n \n torch.cuda.device('cuda') = <torch.cuda.device object at 0x0000021B331A0160>\n \n torch.cuda.current_device() = 0\n \n \n \n > ipython torch_test.ipy\n \n torch.cuda.is_available() = True\n \n torch.cuda.device_count() = 1\n \n torch.cuda.device('cuda') = <torch.cuda.device object at 0x000002B39C1FD390>\n \n torch.cuda.current_device() = 0\n \n ```\n \n \n \n Now, if I try to use _**exactly the same**_ commands in an _**interactive**_ shell, I get the error:\n \n \n \n With python:\n \n ```\n \n >python\n \n Python 3.7.1 (default, Dec 10 2018, 22:54:23) [MSC v.1915 64 bit (AMD64)] :: Anaconda, Inc. on win32\n \n Type \"help\", \"copyright\", \"credits\" or \"license\" for more information.\n \n >>> import torch\n \n >>> print(\"torch.cuda.is_available() =\", torch.cuda.is_available())\n \n torch.cuda.is_available() = True\n \n >>> print(\"torch.cuda.device_count() =\", torch.cuda.device_count())\n \n torch.cuda.device_count() = 1\n \n >>> print(\"torch.cuda.device('cuda') =\", torch.cuda.device('cuda'))\n \n torch.cuda.device('cuda') = <torch.cuda.device object at 0x0000028CBD034198>\n \n >>> print(\"torch.cuda.current_device() =\", torch.cuda.current_device())\n \n THCudaCheck FAIL file=..\\aten\\src\\THC\\THCGeneral.cpp line=87 error=30 : unknown error\n \n Traceback (most recent call last):\n \n  File \"<stdin>\", line 1, in <module>\n \n  File \"C:\\Anaconda3\\lib\\site-packages\\torch\\cuda\\__init__.py\", line 341, in current_device\n \n  _lazy_init()\n \n  File \"C:\\Anaconda3\\lib\\site-packages\\torch\\cuda\\__init__.py\", line 162, in _lazy_init\n \n  torch._C._cuda_init()\n \n RuntimeError: cuda runtime error (30) : unknown error at ..\\aten\\src\\THC\\THCGeneral.cpp:87\n \n >>> ^Z\n \n ```\n \n \n \n or with ipython:\n \n ```\n \n >ipython\n \n Python 3.7.1 (default, Dec 10 2018, 22:54:23) [MSC v.1915 64 bit (AMD64)]\n \n Type 'copyright', 'credits' or 'license' for more information\n \n IPython 7.2.0 -- An enhanced Interactive Python. Type '?' for help.\n \n \n \n In [1]: import torch\n \n \n \n In [2]: print(\"torch.cuda.is_available() =\", torch.cuda.is_available())\n \n torch.cuda.is_available() = True\n \n \n \n In [3]: print(\"torch.cuda.device_count() =\", torch.cuda.device_count())\n \n torch.cuda.device_count() = 1\n \n \n \n In [4]: print(\"torch.cuda.device('cuda') =\", torch.cuda.device('cuda'))\n \n torch.cuda.device('cuda') = <torch.cuda.device object at 0x0000018A068007F0>\n \n \n \n In [5]: print(\"torch.cuda.current_device() =\", torch.cuda.current_device())\n \n THCudaCheck FAIL file=..\\aten\\src\\THC\\THCGeneral.cpp line=87 error=30 : unknown error\n \n ---------------------------------------------------------------------------\n \n RuntimeError Traceback (most recent call last)\n \n <ipython-input-5-f8c552eb6277> in <module>\n \n ----> 1 print(\"torch.cuda.current_device() =\", torch.cuda.current_device())\n \n \n \n C:\\Anaconda3\\lib\\site-packages\\torch\\cuda\\__init__.py in current_device()\n \n  339 def current_device():\n \n  340 r\"\"\"Returns the index of a currently selected device.\"\"\"\n \n --> 341 _lazy_init()\n \n  342 return torch._C._cuda_getDevice()\n \n  343\n \n \n \n C:\\Anaconda3\\lib\\site-packages\\torch\\cuda\\__init__.py in _lazy_init()\n \n  160 \"Cannot re-initialize CUDA in forked subprocess. \" + msg)\n \n  161 _check_driver()\n \n --> 162 torch._C._cuda_init()\n \n  163 _cudart = _load_cudart()\n \n  164 _cudart.cudaGetErrorName.restype = ctypes.c_char_p\n \n \n \n RuntimeError: cuda runtime error (30) : unknown error at ..\\aten\\src\\THC\\THCGeneral.cpp:87\n \n \n \n In [6]:\n \n ```\n \n \n \n Any hints?",
      "y": "OK, I did some extra tests, and it seems that it is some weird behavior **only when running on an interactive shell**. Here's what I have done (step-by-step)\n \n \n \n 1. Prepare a simple file with the example:\n \n ```\n \n > type torch_test.ipy\n \n import torch\n \n print(\"torch.cuda.is_available() =\", torch.cuda.is_available())\n \n print(\"torch.cuda.device_count() =\", torch.cuda.device_count())\n \n print(\"torch.cuda.device('cuda') =\", torch.cuda.device('cuda'))\n \n print(\"torch.cuda.current_device() =\", torch.cuda.current_device())\n \n ```\n \n \n \n I can run this file with either `Python `or `iPython`, and it all works fine:\n \n ```\n \n > python torch_test.ipy\n \n torch.cuda.is_available() = True\n \n torch.cuda.device_count() = 1\n \n torch.cuda.device('cuda') = <torch.cuda.device object at 0x0000021B331A0160>\n \n torch.cuda.current_device() = 0\n \n \n \n > ipython torch_test.ipy\n \n torch.cuda.is_available() = True\n \n torch.cuda.device_count() = 1\n \n torch.cuda.device('cuda') = <torch.cuda.device object at 0x000002B39C1FD390>\n \n torch.cuda.current_device() = 0\n \n ```\n \n \n \n Now, if I try to use _**exactly the same**_ commands in an _**interactive**_ shell, I get the error:\n \n \n \n With python:\n \n ```\n \n >python\n \n Python 3.7.1 (default, Dec 10 2018, 22:54:23) [MSC v.1915 64 bit (AMD64)] :: Anaconda, Inc. on win32\n \n Type \"help\", \"copyright\", \"credits\" or \"license\" for more information.\n \n >>> import torch\n \n >>> print(\"torch.cuda.is_available() =\", torch.cuda.is_available())\n \n torch.cuda.is_available() = True\n \n >>> print(\"torch.cuda.device_count() =\", torch.cuda.device_count())\n \n torch.cuda.device_count() = 1\n \n >>> print(\"torch.cuda.device('cuda') =\", torch.cuda.device('cuda'))\n \n torch.cuda.device('cuda') = <torch.cuda.device object at 0x0000028CBD034198>\n \n >>> print(\"torch.cuda.current_device() =\", torch.cuda.current_device())\n \n THCudaCheck FAIL file=..\\aten\\src\\THC\\THCGeneral.cpp line=87 error=30 : unknown error\n \n Traceback (most recent call last):\n \n  File \"<stdin>\", line 1, in <module>\n \n  File \"C:\\Anaconda3\\lib\\site-packages\\torch\\cuda\\__init__.py\", line 341, in current_device\n \n  _lazy_init()\n \n  File \"C:\\Anaconda3\\lib\\site-packages\\torch\\cuda\\__init__.py\", line 162, in _lazy_init\n \n  torch._C._cuda_init()\n \n RuntimeError: cuda runtime error (30) : unknown error at ..\\aten\\src\\THC\\THCGeneral.cpp:87\n \n >>> ^Z\n \n ```\n \n \n \n or with ipython:\n \n ```\n \n >ipython\n \n Python 3.7.1 (default, Dec 10 2018, 22:54:23) [MSC v.1915 64 bit (AMD64)]\n \n Type 'copyright', 'credits' or 'license' for more information\n \n IPython 7.2.0 -- An enhanced Interactive Python. Type '?' for help.\n \n \n \n In [1]: import torch\n \n \n \n In [2]: print(\"torch.cuda.is_available() =\", torch.cuda.is_available())\n \n torch.cuda.is_available() = True\n \n \n \n In [3]: print(\"torch.cuda.device_count() =\", torch.cuda.device_count())\n \n torch.cuda.device_count() = 1\n \n \n \n In [4]: print(\"torch.cuda.device('cuda') =\", torch.cuda.device('cuda'))\n \n torch.cuda.device('cuda') = <torch.cuda.device object at 0x0000018A068007F0>\n \n \n \n In [5]: print(\"torch.cuda.current_device() =\", torch.cuda.current_device())\n \n THCudaCheck FAIL file=..\\aten\\src\\THC\\THCGeneral.cpp line=87 error=30 : unknown error\n \n ---------------------------------------------------------------------------\n \n RuntimeError Traceback (most recent call last)\n \n <ipython-input-5-f8c552eb6277> in <module>\n \n ----> 1 print(\"torch.cuda.current_device() =\", torch.cuda.current_device())\n \n \n \n C:\\Anaconda3\\lib\\site-packages\\torch\\cuda\\__init__.py in current_device()\n \n  339 def current_device():\n \n  340 r\"\"\"Returns the index of a currently selected device.\"\"\"\n \n --> 341 _lazy_init()\n \n  342 return torch._C._cuda_getDevice()\n \n  343\n \n \n \n C:\\Anaconda3\\lib\\site-packages\\torch\\cuda\\__init__.py in _lazy_init()\n \n  160 \"Cannot re-initialize CUDA in forked subprocess. \" + msg)\n \n  161 _check_driver()\n \n --> 162 torch._C._cuda_init()\n \n  163 _cudart = _load_cudart()\n \n  164 _cudart.cudaGetErrorName.restype = ctypes.c_char_p\n \n \n \n RuntimeError: cuda runtime error (30) : unknown error at ..\\aten\\src\\THC\\THCGeneral.cpp:87\n \n \n \n In [6]:\n \n ```\n \n \n \n Any hints?"
   },
   {
      "x": "Namedtuples prune the tensor representations too much",
      "z": "@zasdfgbnm Could we overwrite that?",
      "y": "@zasdfgbnm Could we overwrite that?"
   },
   {
      "x": "clamp makes grad None",
      "z": "The reason why in your first snippet the grad is `None` is because you are modifying the leaf variable `a` and you overwrite it with the result of `a.clamp`. This means that `a` now is an intermediate variable, and the gradient gets freed once it's not needed anymore.\n \n You should instead do:\n \n ```python\n \n a = torch.rand(1, requires_grad=True)\n \n b = torch.clamp(a, 0, 1)\n \n z = 2 * b\n \n z.backward()\n \n print(a.requires_grad) # True\n \n print(a.grad) # tensor([ 2.])\n \n ```",
      "y": "The reason why in your first snippet the grad is `None` is because you are modifying the leaf variable `a` and you overwrite it with the result of `a.clamp`. This means that `a` now is an intermediate variable, and the gradient gets freed once it's not needed anymore.\n \n You should instead do:\n \n ```python\n \n a = torch.rand(1, requires_grad=True)\n \n b = torch.clamp(a, 0, 1)\n \n z = 2 * b\n \n z.backward()\n \n print(a.requires_grad) # True\n \n print(a.grad) # tensor([ 2.])\n \n ```"
   },
   {
      "x": "torch.clamp kills gradients at the border",
      "z": "@SsnL if all values were random the probability would be small, but that's not the case generally. The [Equilibrium propagation](https://www.frontiersin.org/articles/10.3389/fncom.2017.00024/full#note1) algorithm by Bengio et al. initializes states to zero which are clamped to 0 and 1 and updated based on gradients of an energy function. It seems to rely on the gradient being 1 when states are 0 to update properly. I will try the suggestion by @zou3519. Thanks.",
      "y": "if all values were random the probability would be small, but that's not the case generally. The [Equilibrium propagation](https://www.frontiersin.org/articles/10.3389/fncom.2017.00024/full#note1) algorithm by Bengio et al. initializes states to zero which are clamped to 0 and 1 and updated based on gradients of an energy function. It seems to rely on the gradient being 1 when states are 0 to update properly. I will try the suggestion by @zou3519. Thanks."
   },
   {
      "x": "multinomial with inf fails in cuda",
      "z": "I actually think that the CUDA behavior is reasonable, we should raise an error in the CPU case as well. You can't have a probability distribution where one of the elements has an infinite probability, so there's nothing we can do. What would you expect to happen in that case?\n \n \n \n Sorry for the long debugging, unfortunately getting meaningful errors from CUDA kernels is a bit hard. In the future you might want to use `CUDA_LAUNCH_BLOCKING=1` which should report the error in the same operation that caused it.",
      "y": "I actually think that the CUDA behavior is reasonable, we should raise an error in the CPU case as well. You can't have a probability distribution where one of the elements has an infinite probability, so there's nothing we can do.\n\n Sorry for the long debugging, unfortunately getting meaningful errors from CUDA kernels is a bit hard. In the future you might want to use `CUDA_LAUNCH_BLOCKING=1` which should report the error in the same operation that caused it."
   },
   {
      "x": "PyTorch 0.4 hangs with nn.DataParallel but PyTorch 0.3.1 does not",
      "z": "cc: @teng-li who knows all NCCL2 caveats\n \n cc: @ngimel who knows everything about NVIDIA-things",
      "y": "who knows all NCCL2 caveats\n \nwho knows everything about NVIDIA-things"
   },
   {
      "x": "Batchnorm's running_var is wrong(I test it use momentum = 1)",
      "z": "Anyway, `numpy.std` and `numpy.var` divide sum of square by `n` (see [numpy document](https://docs.scipy.org/doc/numpy/reference/generated/numpy.std.html)), \n \n otoh, pytorch divide sum of square by `n-1` ([source](https://github.com/pytorch/pytorch/blob/master/aten/src/THNN/generic/BatchNormalization.c#L54-L58)).\n \n So, there should be some differences.",
      "y": "Anyway, `numpy.std` and `numpy.var` divide sum of square by `n` (see [numpy document](https://docs.scipy.org/doc/numpy/reference/generated/numpy.std.html)), \n \n otoh, pytorch divide sum of square by `n-1` ([source](https://github.com/pytorch/pytorch/blob/master/aten/src/THNN/generic/BatchNormalization.c#L54-L58)).\n \n So, there should be some differences."
   },
   {
      "x": "[pytorch 0.4.0] Cuda runtime error (30) after upgrading from 0.3.1",
      "z": "@gmkim90 I came across the same problem after upgrading from 0.3.1 to 0.4. Do you solve this problem now ?",
      "y": " I came across the same problem after upgrading from 0.3.1 to 0.4. Do you solve this problem now ?"
   },
   {
      "x": "PyTorch RNN Tutorials don't work on Windows",
      "z": "https://github.com/pytorch/tutorials/pull/232 will fix this.",
      "y": "https://github.com/pytorch/tutorials/pull/232 will fix this."
   },
   {
      "x": "torch.stft output size is not right.",
      "z": "librosa by default has `center=True` which pads the signal so that returned frequency at index `i` is for signal **centered** at index `i * hop`. So return size = `ceil(224960 / 275) = 819`.\n \n \n \n PyTorch has returned frequency at index `i` beginning at index `i * hop`. So return size = `ceil((224960 - 1102) / 275) = 815`.\n \n \n \n I believe if you set `center=False` for librosa or pad PyTorch tensor properly you will get same outputs.",
      "y": "librosa by default has `center=True` which pads the signal so that returned frequency at index `i` is for signal **centered** at index `i * hop`. So return size = `ceil(224960 / 275) = 819`.\n \n \n \n PyTorch has returned frequency at index `i` beginning at index `i * hop`. So return size = `ceil((224960 - 1102) / 275) = 815`.\n \n \n \n I believe if you set `center=False` for librosa or pad PyTorch tensor properly you will get same outputs."
   },
   {
      "x": "[bug report] (Pytorch 0.4.0) Dropout layer error when input is longtensor with 0",
      "z": "@yu45020 I had a similar error, but it was because I was accidentally feeding a `2D` tensor to the `Dropout2D()` function.\n \n \n \n In my code, the shape of each `batch` is this: `sequence_len x batch_size x embedding_size`. \n \n \n \n You seem to be making the same mistake I did by feeding a `tensor` of `sequence_len x batch_size` where the values are indexes to rows in your embedding matrix. Once you \"lookup\" the indexes and get a shape of `sequenc_len x batch_size x embedding_size`, it should work.\n \n \n \n If you want to do the dropout *before* looking up embeddings, then you just need `Dropout[1D]()`.",
      "y": "I had a similar error, but it was because I was accidentally feeding a `2D` tensor to the `Dropout2D()` function.\n \n \n \n In my code, the shape of each `batch` is this: `sequence_len x batch_size x embedding_size`. \n \n \n \n You seem to be making the same mistake I did by feeding a `tensor` of `sequence_len x batch_size` where the values are indexes to rows in your embedding matrix. Once you \"lookup\" the indexes and get a shape of `sequenc_len x batch_size x embedding_size`, it should work.\n \n \n \n If you want to do the dropout *before* looking up embeddings, then you just need `Dropout[1D]()`."
   },
   {
      "x": "module 'torch' has no attribute 'no_grad'",
      "z": "The tutorial is based on PyTorch 0.4 which introduces torch.no_grad. You can consider updating your PyTorch installation.",
      "y": "The tutorial is based on PyTorch 0.4 which introduces torch.no_grad. You can consider updating your PyTorch installation."
   },
   {
      "x": "can't reproduce results even set all random seeds",
      "z": "Here are some points to check:\n \n * https://pytorch.org/docs/stable/notes/randomness.html : Pytorch is not reproducible between CPU and GPU, between different platforms, and different commits.\n \n * Call this before running any of your functions: (you can put it in the beginning of your main code, right after importing your modules. If you do not use a module among these in your main code, it is ok, just import it and seed it so when you import it a second time, Python will find that it has already been imported so there will be nothing to do. Example: `torch`):\n \n ```python\n \n  torch.manual_seed(seed)\n \n  torch.cuda.manual_seed(seed)\n \n  torch.cuda.manual_seed_all(seed) # if you are using multi-GPU.\n \n  np.random.seed(seed) # Numpy module.\n \n  random.seed(seed) # Python random module.\n \n  torch.manual_seed(seed)\n \n  torch.backends.cudnn.benchmark = False\n \n  torch.backends.cudnn.deterministic = True\n \n ```\n \n * Following the above, you need to seed EVERY external module (outside Pytorch) that may introduce randomness in your entire code.\n \n * You need to set the init function of the worker(s) to be fed to the `DataLoader`:\n \n ```python\n \n def _init_fn(worker_id):\n \n  np.random.seed(int(seed)\n \n ```\n \n Make sure that your dataloader loads samples in the same order every call. If you do cropping, or other preprocessing steps, make sure that they are deterministic. As I remember, such modules can provide a deterministic result (in Pytorch 1.0.0 or even before this version).\n \n * I learned this recently, despite it was written in Pytoch doc the whole time!!!!: Unfortunately, some Pytorch modules follow non-deterministic behavior; and most of the time you can not get rid of it by following the above steps. Example: the BACKWARD of upsampling and interpolation functionals/classes is non-deterministic (see [here](https://discuss.pytorch.org/t/non-deterministic-behavior-of-pytorch-upsample-interpolate/42842?u=sbelharbi)). This means, if you use such modules in the training graph, you will never obtain a deterministic results no matter what you do. `torch.nn.ConvTranspose2d` is not deterministic unless you set `torch.backends.cudnn.deterministic = True` (they said `you can try to make the operation deterministic ... by setting torch.backends.cudnn.deterministic = True`. So I am not sure if by doing so, you certainly obtain a deterministic result). So, make sure you are not using any of the non-deterministic Pytorch modules.\n \n \n \n Print your intermediate results to find out from where the non-deterministic behavior comes from: loaded samples, crops, losses, ...\n \n \n \n In my experience, following the above steps lead to exactly same results.\n \n If it is not your case, you may be using an external library that is a source of randomness, or you are using a non-deterministic Pytorch module (or something else that I am not aware of it). If reproducibility is important for you (and I assume it is), you can check step by step your code to trace the source of randomness (run your code gradually, and inspect the results at every step). Make sure that the part of your code that you think it is deterministic is indeed deterministic.\n \n Let me know how it goes, and please, let me know what was the source of randomness in your case. I am curious. Thanks!",
      "y": "Here are some points to check:\n \n * https://pytorch.org/docs/stable/notes/randomness.html : Pytorch is not reproducible between CPU and GPU, between different platforms, and different commits.\n \n * Call this before running any of your functions: (you can put it in the beginning of your main code, right after importing your modules. If you do not use a module among these in your main code, it is ok, just import it and seed it so when you import it a second time, Python will find that it has already been imported so there will be nothing to do. Example: `torch`):\n \n ```python\n \n  torch.manual_seed(seed)\n \n  torch.cuda.manual_seed(seed)\n \n  torch.cuda.manual_seed_all(seed) # if you are using multi-GPU.\n \n  np.random.seed(seed) # Numpy module.\n \n  random.seed(seed) # Python random module.\n \n  torch.manual_seed(seed)\n \n  torch.backends.cudnn.benchmark = False\n \n  torch.backends.cudnn.deterministic = True\n \n ```\n \n * Following the above, you need to seed EVERY external module (outside Pytorch) that may introduce randomness in your entire code.\n \n * You need to set the init function of the worker(s) to be fed to the `DataLoader`:\n \n ```python\n \n def _init_fn(worker_id):\n \n  np.random.seed(int(seed)\n \n ```\n \n Make sure that your dataloader loads samples in the same order every call. If you do cropping, or other preprocessing steps, make sure that they are deterministic. As I remember, such modules can provide a deterministic result (in Pytorch 1.0.0 or even before this version).\n \n * I learned this recently, despite it was written in Pytoch doc the whole time!!!!: Unfortunately, some Pytorch modules follow non-deterministic behavior; and most of the time you can not get rid of it by following the above steps. Example: the BACKWARD of upsampling and interpolation functionals/classes is non-deterministic (see [here](https://discuss.pytorch.org/t/non-deterministic-behavior-of-pytorch-upsample-interpolate/42842?u=sbelharbi)). This means, if you use such modules in the training graph, you will never obtain a deterministic results no matter what you do. `torch.nn.ConvTranspose2d` is not deterministic unless you set `torch.backends.cudnn.deterministic = True` (they said `you can try to make the operation deterministic ... by setting torch.backends.cudnn.deterministic = True`. So I am not sure if by doing so, you certainly obtain a deterministic result). So, make sure you are not using any of the non-deterministic Pytorch modules.\n \n \n \n Print your intermediate results to find out from where the non-deterministic behavior comes from: loaded samples, crops, losses, ...\n \n \n \n In my experience, following the above steps lead to exactly same results.\n \n If it is not your case, you may be using an external library that is a source of randomness, or you are using a non-deterministic Pytorch module (or something else that I am not aware of it). If reproducibility is important for you (and I assume it is), you can check step by step your code to trace the source of randomness (run your code gradually, and inspect the results at every step). Make sure that the part of your code that you think it is deterministic is indeed deterministic.\n \n Let me know how it goes, and please, let me know what was the source of randomness in your case. I am curious. Thanks!"
   },
   {
      "x": "[BUG Report] torch.from_numpy()",
      "z": "@JimLee1996 Yes, I know. When I say that I can print I meant that I can print it quickly. I just manually patched #6863 in 0.4 and verified that it works. So this is fixed.",
      "y": "@JimLee1996 Yes, I know. When I say that I can print I meant that I can print it quickly. I just manually patched #6863 in 0.4 and verified that it works. So this is fixed."
   },
   {
      "x": "import error: from torch._C import * RuntimeError: stoi",
      "z": "I had the same error. I fixed it using: system('env -i /usr/bin/python3 -c \"import torch\"').\n \n Some environment variable set by matlab is messing with pytorch, but I don't know which.",
      "y": "I had the same error. I fixed it using: system('env -i /usr/bin/python3 -c \"import torch\"').\n \n Some environment variable set by matlab is messing with pytorch, but I don't know which."
   },
   {
      "x": "Pytorch build finds local mkl instead of conda version",
      "z": "@meder411 I think a simpler way might be to simply `export CMAKE_PREFIX_PATH=...` before running the script.",
      "y": "think a simpler way might be to simply `export CMAKE_PREFIX_PATH=...` before running the script."
   },
   {
      "x": "Multi-GPU autograd error with Pytorch 0.4",
      "z": "@erogol Thanks Eren. This is very helpful. We'll look into it.",
      "y": " This is very helpful. We'll look into it."
   },
   {
      "x": "Crash when creating a tensor from a tensor on a different device",
      "z": "@albanD Its unrelated to `nn.Parameter` and `torch.cuda.FloatTensor(torch.ones(3))` alreay crashes.",
      "y": "Its unrelated to `nn.Parameter` and `torch.cuda.FloatTensor(torch.ones(3))` alreay crashes."
   },
   {
      "x": "IndexError: Dimension out of range (expected to be in range of [-3, 2], but got -4)",
      "z": "It's this line https://github.com/pytorch/pytorch/blob/82d58ed484eb04f894475de0551055f7e070f481/aten/src/ATen/native/cuda/DilatedMaxPool2d.cu#L361 introduced by #34519. cc @xwang233, maxpool2d can accept 3d tensors, in this case batch dimension is implicitly assumed to be 1. For convenience, a slightly shorter repro\n \n ```\n \n import torch\n \n \n \n device = \"cuda\"\n \n \n \n prev_layer = torch.randn(1, 84, 84, device=device)\n \n prev_layer = torch.nn.functional.max_pool2d(prev_layer, kernel_size=(3, 3))\n \n torch.cuda.synchronize()\n \n ```",
      "y": "It's this line https://github.com/pytorch/pytorch/blob/82d58ed484eb04f894475de0551055f7e070f481/aten/src/ATen/native/cuda/DilatedMaxPool2d.cu#L361 introduced by #34519.can accept 3d tensors, in this case batch dimension is implicitly assumed to be 1. For convenience, a slightly shorter repro\n \n ```\n \n import torch\n \n \n \n device = \"cuda\"\n \n \n \n prev_layer = torch.randn(1, 84, 84, device=device)\n \n prev_layer = torch.nn.functional.max_pool2d(prev_layer, kernel_size=(3, 3))\n \n torch.cuda.synchronize()\n \n ```"
   },
   {
      "x": "Add moveaxis function",
      "z": "single dim version\n \n \n \n ```py\n \n def _moveaxis(tensor: torch.Tensor, source: int, destination: int) -> torch.Tensor:\n \n  dim = tensor.dim()\n \n  perm = list(range(dim))\n \n  if destination < 0:\n \n  destination += dim\n \n  perm.pop(source)\n \n  perm.insert(destination, source)\n \n  return tensor.permute(*perm)\n \n ```",
      "y": "single dim version\n \n \n \n ```py\n \n def _moveaxis(tensor: torch.Tensor, source: int, destination: int) -> torch.Tensor:\n \n  dim = tensor.dim()\n \n  perm = list(range(dim))\n \n  if destination < 0:\n \n  destination += dim\n \n  perm.pop(source)\n \n  perm.insert(destination, source)\n \n  return tensor.permute(*perm)\n \n ```"
   },
   {
      "x": "C++ Adam optimizer",
      "z": "I don't think the changed _adam_ implementation is to blame here.\n \n I was tripping over the new betas option specification (from two separate scalars to a 2-element tuple), so ended up with a misspecified set of options.\n \n I'm seeing similar convergence now on some simple GANs & conditional GANs.\n \n \n \n I'll try to submit a PR to fix the check messages later this evening.\n \n Thanks",
      "y": "I don't think the changed _adam_ implementation is to blame here.\n \n I was tripping over the new betas option specification (from two separate scalars to a 2-element tuple), so ended up with a misspecified set of options.\n \n I'm seeing similar convergence now on some simple GANs & conditional GANs.\n \n \n \n I'll try to submit a PR to fix the check messages later this evening.\n \n Thanks"
   },
   {
      "x": "[RFC] Async User Function for RPC",
      "z": "> Currently, every RPC request occupies an RPC thread on the server side until done.\n \n \n \n fwiw, for async TorchScript that uses jit.fork/wait underneath, this was actually addressed last week, but plumbing through interpreter runAsync() call into the request_callback_impl handler.\n \n \n \n Definitely true about pure Python though.",
      "y": "> Currently, every RPC request occupies an RPC thread on the server side until done.\n \n \n \n fwiw, for async TorchScript that uses jit.fork/wait underneath, this was actually addressed last week, but plumbing through interpreter runAsync() call into the request_callback_impl handler.\n \n \n \n Definitely true about pure Python though."
   },
   {
      "x": "[question] Is amp CUDA-specific?",
      "z": "Right now Amp is Cuda-specific. The FP32 and FP16 oplists are based on the hardware capabilities of Volta and Turing GPUs and the properties of Pytorch's own Cuda kernels.\n \n \n \n For different precisions/backends, I expect you could use the same Python-side API, but you'd probably need different sets of backend wrappers: The \"Smaller/Faster Type\" and FP32 oplists would need to be different, because the hardware capabilities and kernel implementations would be different.\n \n \n \n bfloat16 has (almost) the same dynamic range as FP32, so it shouldn't require gradient scaling, but definitely needs the optimizer to act on FP32 weights to capture small updates. An autocasting-style API where model leaves remain in default precision, and tensors are casted as they enter certain functions, is one way to ensure FP32 weight updates.",
      "y": "Right now Amp is Cuda-specific. The FP32 and FP16 oplists are based on the hardware capabilities of Volta and Turing GPUs and the properties of Pytorch's own Cuda kernels.\n \n \n \n For different precisions/backends, I expect you could use the same Python-side API, but you'd probably need different sets of backend wrappers: The \"Smaller/Faster Type\" and FP32 oplists would need to be different, because the hardware capabilities and kernel implementations would be different.\n \n \n \n bfloat16 has (almost) the same dynamic range as FP32, so it shouldn't require gradient scaling, but definitely needs the optimizer to act on FP32 weights to capture small updates. An autocasting-style API where model leaves remain in default precision, and tensors are casted as they enter certain functions, is one way to ensure FP32 weight updates."
   },
   {
      "x": "len(DataLoader) doesn't take into account DataLoader.batch_size when using IterableDataset",
      "z": "Thanks for reporting! Yes, this is a bug and should be fixed.",
      "y": "Thanks for reporting! Yes, this is a bug and should be fixed."
   },
   {
      "x": "Calling onnx export hangs using multiprocessing",
      "z": "@marksibrahim call `torch.set_num_threads(1)` before multiprocessing is started\n \n \n \n ```bash\n \n python torch_export_bug.py\n \n Threads before: 4\n \n Threads after: 1\n \n [+] Start\n \n [+] Got model\n \n [+] Starting process\n \n [+] Waiting process\n \n  Getting model inside proc\n \n  Got model inside proc\n \n [+] End\n \n ```\n \n \n \n Another option is `export OMP_NUM_THREADS=1` on your Linux terminal",
      "y": " call `torch.set_num_threads(1)` before multiprocessing is started\n \n \n \n ```bash\n \n python torch_export_bug.py\n \n Threads before: 4\n \n Threads after: 1\n \n [+] Start\n \n [+] Got model\n \n [+] Starting process\n \n [+] Waiting process\n \n  Getting model inside proc\n \n  Got model inside proc\n \n [+] End\n \n ```\n \n \n \n Another option is `export OMP_NUM_THREADS=1` on your Linux terminal"
   },
   {
      "x": "Pruning doesn't affect speed nor memory usage",
      "z": "As far as I know, for memory concerns, you can use `.to_sparse()`.\n \n Example:\n \n ```python\n \n import torch\n \n import torch.nn.utils.prune as prune\n \n \n \n t = torch.randn(1000, 1000)\n \n torch.save(t, 'original.pth')\n \n \n \n p = prune.L1Unstructured(amount=0.99)\n \n pruned_t = p.prune(t)\n \n torch.save(pruned_t, 'pruned.pth')\n \n \n \n sparse_t = pruned_t.to_sparse()\n \n torch.save(sparse_t, 'sparse.pth')\n \n ```\n \n Then `ls -lht *.pth` should return:\n \n ```\n \n 196K sparse.pth\n \n 3.9M pruned.pth\n \n 3.9M original.pth\n \n ```\n \n \n \n This only works if you prune your tensor by a large enough amount that representing it in coordinate space is actually memory efficient.\n \n \n \n You can look at `torch.sparse` to see what ops are then supported on these sparse tensors. Note that this is marked as experimental and the API might change in the future: https://pytorch.org/docs/stable/sparse.html\n \n \n \n I'm not sure about the computational speed-up concerns... @bwasti, maybe?",
      "y": "As far as I know, for memory concerns, you can use `.to_sparse()`.\n \n Example:\n \n ```python\n \n import torch\n \n import torch.nn.utils.prune as prune\n \n \n \n t = torch.randn(1000, 1000)\n \n torch.save(t, 'original.pth')\n \n \n \n p = prune.L1Unstructured(amount=0.99)\n \n pruned_t = p.prune(t)\n \n torch.save(pruned_t, 'pruned.pth')\n \n \n \n sparse_t = pruned_t.to_sparse()\n \n torch.save(sparse_t, 'sparse.pth')\n \n ```\n \n Then `ls -lht *.pth` should return:\n \n ```\n \n 196K sparse.pth\n \n 3.9M pruned.pth\n \n 3.9M original.pth\n \n ```\n \n \n \n This only works if you prune your tensor by a large enough amount that representing it in coordinate space is actually memory efficient.\n \n \n \n You can look at `torch.sparse` to see what ops are then supported on these sparse tensors. Note that this is marked as experimental and the API might change in the future: https://pytorch.org/docs/stable/sparse.html\n \n \n \n I'm not sure about the computational speed-up concerns... @bwasti, maybe?"
   },
   {
      "x": "DistributedDataParallel don't work at nightly build(1.6.0.dev20200408+cu101)",
      "z": "cc @osalpekar",
      "y": "cc @osalpekar"
   },
   {
      "x": "nn.Dropout layer TypeError: \u00e2\u20ac\u02dc<\u00e2\u20ac\u2122 not supported between instances of \u00e2\u20ac\u02dcDropout\u00e2\u20ac\u2122 and \u00e2\u20ac\u02dcint\u00e2\u20ac\u2122",
      "z": "This makes sense since you are doing this:\n \n ```python\n \n self.embedding_dropout = nn.Dropout(p = self.embedding_dropout)\n \n ```\n \n \n \n If you assign a different name to the `p` argument, you should be fine.",
      "y": "This makes sense since you are doing this:\n \n ```python\n \n self.embedding_dropout = nn.Dropout(p = self.embedding_dropout)\n \n ```\n \n \n \n If you assign a different name to the `p` argument, you should be fine."
   },
   {
      "x": "Extra whitespace issue when printing complex tensors",
      "z": "I will work on this issue, but in the meantime, if there are any other suggestions on any additional changes we'd like to see in the expected print behavior, I would love your input!",
      "y": "I will work on this issue, but in the meantime, if there are any other suggestions on any additional changes we'd like to see in the expected print behavior, I would love your input!"
   },
   {
      "x": "NCCL Connection Failed Using PyTorch Distributed with Error ProcessGroupNCCL.cpp:290, unhandled system error",
      "z": "> > Thanks for your help! I found that this problem is caused by the network connection problem. I will close this issue.\n \n > \n \n > I have exactly the same problem with that random port number. Can you tell me how you solved it? Thanks!\n \n \n \n I found that the two servers that I used were not under the same VPC. So I used other servers and it worked. Maybe you can consider using other servers.",
      "y": "> > Thanks for your help! I found that this problem is caused by the network connection problem. I will close this issue.\n \n > \n \n > I have exactly the same problem with that random port number. Can you tell me how you solved it? Thanks!\n \n \n \n I found that the two servers that I used were not under the same VPC. So I used other servers and it worked. Maybe you can consider using other servers."
   },
   {
      "x": "DistributedDataParallel on multiple GPU nodes slower than one GPU node.",
      "z": "The network bandwidth is indeed the bottleneck when synchronizing large models. NCCL folks provide some numbers in the other thread. Basically, the bandwidth for all reduce operation on intra-gpu communication would be much higher than that of inter-node communication. (NCCl folks mentioned 120 GB/s vs 10 GB/s, https://github.com/NVIDIA/nccl/issues/318).\n \n \n \n Thanks for the help. I am closing this issue here.",
      "y": "The network bandwidth is indeed the bottleneck when synchronizing large models. NCCL folks provide some numbers in the other thread. Basically, the bandwidth for all reduce operation on intra-gpu communication would be much higher than that of inter-node communication. (NCCl folks mentioned 120 GB/s vs 10 GB/s, https://github.com/NVIDIA/nccl/issues/318).\n \n \n \n Thanks for the help. I am closing this issue here."
   },
   {
      "x": "Batched torch.solve on GPU fails with: invalid device function",
      "z": "Yes, thanks for the help.",
      "y": "Yes, thanks for the help."
   },
   {
      "x": "Build system cannot find MKL libraries",
      "z": "@adamjstewart here's a simple workaround: `export CMAKE_PREFIX_PATH=\"/opt/intel/compilers_and_libraries_2019.4.233/mac/mkl:$PATH\"`",
      "y": "a simple workaround: `export CMAKE_PREFIX_PATH=\"/opt/intel/compilers_and_libraries_2019.4.233/mac/mkl:$PATH\"`"
   },
   {
      "x": "cdist allocates a huge amount of memory in the bachward pass (pytorch 1.2.0)",
      "z": "I also have this issue.\n \n \n \n ```\n \n PyTorch version: 1.2.0\n \n Is debug build: No\n \n CUDA used to build PyTorch: 10.0.130\n \n \n \n OS: Manjaro Linux\n \n GCC version: (GCC) 9.1.0\n \n CMake version: version 3.15.1\n \n \n \n Python version: 3.7\n \n Is CUDA available: Yes\n \n CUDA runtime version: 10.1.168\n \n GPU models and configuration: GPU 0: GeForce RTX 2080 Ti\n \n Nvidia driver version: 430.26\n \n cuDNN version: /usr/lib/libcudnn.so.7.6.1\n \n \n \n Versions of relevant libraries:\n \n [pip3] numpy==1.16.4\n \n [pip3] torch==1.2.0\n \n [pip3] torch-cluster==1.4.2\n \n [pip3] torch-geometric==1.3.0\n \n [pip3] torch-scatter==1.3.1\n \n [pip3] torch-sparse==0.4.0\n \n [pip3] torch-spline-conv==1.1.0\n \n [pip3] torchvision==0.4.0\n \n [conda] Could not collect\n \n ```\n \n \n \n For me it was a bit tricky to reach this bug report, since the error it gives is the following:\n \n \n \n ```\n \n ---------------------------------------------------------------------------\n \n RuntimeError Traceback (most recent call last)\n \n <ipython-input-4-281f7d587d04> in <module>\n \n  71 torch.cuda.synchronize()\n \n  72 res = loss_func_vec(y_pred, batch)\n \n ---> 73 res.backward()\n \n  74 opt.step()\n \n  75 opt.zero_grad()\n \n \n \n ~/venv/lib/python3.7/site-packages/torch/tensor.py in backward(self, gradient, retain_graph, create_graph)\n \n  116 products. Defaults to ``False``.\n \n  117 \"\"\"\n \n --> 118 torch.autograd.backward(self, gradient, retain_graph, create_graph)\n \n  119 \n \n  120 def register_hook(self, hook):\n \n \n \n ~/venv/lib/python3.7/site-packages/torch/autograd/__init__.py in backward(tensors, grad_tensors, retain_graph, create_graph, grad_variables)\n \n  91 Variable._execution_engine.run_backward(\n \n  92 tensors, grad_tensors, retain_graph, create_graph,\n \n ---> 93 allow_unreachable=True) # allow_unreachable flag\n \n  94 \n \n  95 \n \n \n \n RuntimeError: CUDA error: invalid configuration argument\n \n ```",
      "y": "I also have this issue.\n \n \n \n ```\n \n PyTorch version: 1.2.0\n \n Is debug build: No\n \n CUDA used to build PyTorch: 10.0.130\n \n \n \n OS: Manjaro Linux\n \n GCC version: (GCC) 9.1.0\n \n CMake version: version 3.15.1\n \n \n \n Python version: 3.7\n \n Is CUDA available: Yes\n \n CUDA runtime version: 10.1.168\n \n GPU models and configuration: GPU 0: GeForce RTX 2080 Ti\n \n Nvidia driver version: 430.26\n \n cuDNN version: /usr/lib/libcudnn.so.7.6.1\n \n \n \n Versions of relevant libraries:\n \n [pip3] numpy==1.16.4\n \n [pip3] torch==1.2.0\n \n [pip3] torch-cluster==1.4.2\n \n [pip3] torch-geometric==1.3.0\n \n [pip3] torch-scatter==1.3.1\n \n [pip3] torch-sparse==0.4.0\n \n [pip3] torch-spline-conv==1.1.0\n \n [pip3] torchvision==0.4.0\n \n [conda] Could not collect\n \n ```\n \n \n \n For me it was a bit tricky to reach this bug report, since the error it gives is the following:\n \n \n \n ```\n \n ---------------------------------------------------------------------------\n \n RuntimeError Traceback (most recent call last)\n \n <ipython-input-4-281f7d587d04> in <module>\n \n  71 torch.cuda.synchronize()\n \n  72 res = loss_func_vec(y_pred, batch)\n \n ---> 73 res.backward()\n \n  74 opt.step()\n \n  75 opt.zero_grad()\n \n \n \n ~/venv/lib/python3.7/site-packages/torch/tensor.py in backward(self, gradient, retain_graph, create_graph)\n \n  116 products. Defaults to ``False``.\n \n  117 \"\"\"\n \n --> 118 torch.autograd.backward(self, gradient, retain_graph, create_graph)\n \n  119 \n \n  120 def register_hook(self, hook):\n \n \n \n ~/venv/lib/python3.7/site-packages/torch/autograd/__init__.py in backward(tensors, grad_tensors, retain_graph, create_graph, grad_variables)\n \n  91 Variable._execution_engine.run_backward(\n \n  92 tensors, grad_tensors, retain_graph, create_graph,\n \n ---> 93 allow_unreachable=True) # allow_unreachable flag\n \n  94 \n \n  95 \n \n \n \n RuntimeError: CUDA error: invalid configuration argument\n \n ```"
   },
   {
      "x": "Build fails on FreeBSD with the unclear cause, see the attached log",
      "z": "Now I've succeeded building. -)\n \n \n \n Thanks!",
      "y": "Now I've succeeded building. -)\n \n \n \n Thanks!"
   },
   {
      "x": "Issue on installation from source",
      "z": "@Cganzorig \n \n > Also, the reason why I am trying to install pytorch from source is to modify affine_grid functon to make it work for 3D data. Is there any way to modify this function without instally pytorch from source?\n \n > \n \n > Any help will be much appreciated.\n \n \n \n This is not documented, but `affine_grid` actually _already works for 3D data_, as with the following example:\n \n ```\n \n affine_matrix = torch.tensor([[[1,0,0,0],[0,1,0,0],[0,0,1,0]]], dtype=torch.float)\n \n grid = torch.nn.functional.affine_grid(affine_matrix, (1,1,3,3,3))\n \n ```\n \n You can then transform your data using `torch.nn.functional.grid_sample(image_3d, grid)`.\n \n \n \n Of course, this doesn't help you with the build issue, but at least it does get around having to build it from source. I hope this helps.",
      "y": "\n \n > Also, the reason why I am trying to install pytorch from source is to modify affine_grid functon to make it work for 3D data. Is there any way to modify this function without instally pytorch from source?\n \n > \n \n > Any help will be much appreciated.\n \n \n \n This is not documented, but `affine_grid` actually _already works for 3D data_, as with the following example:\n \n ```\n \n affine_matrix = torch.tensor([[[1,0,0,0],[0,1,0,0],[0,0,1,0]]], dtype=torch.float)\n \n grid = torch.nn.functional.affine_grid(affine_matrix, (1,1,3,3,3))\n \n ```\n \n You can then transform your data using `torch.nn.functional.grid_sample(image_3d, grid)`.\n \n \n \n Of course, this doesn't help you with the build issue, but at least it does get around having to build it from source. I hope this helps."
   },
   {
      "x": "Incorrect LibTorch download links on pytorch.org",
      "z": "@mdlockyer This is the full list of download links:\n \n Linux CPU:\n \n https://download.pytorch.org/libtorch/cpu/libtorch-shared-with-deps-1.2.0.zip\n \n https://download.pytorch.org/libtorch/cpu/libtorch-shared-without-deps-1.2.0.zip\n \n https://download.pytorch.org/libtorch/cpu/libtorch-static-with-deps-1.2.0.zip\n \n https://download.pytorch.org/libtorch/cpu/libtorch-static-without-deps-1.2.0.zip\n \n \n \n Linux CUDA 9.2:\n \n https://download.pytorch.org/libtorch/cu92/libtorch-shared-with-deps-1.2.0.zip\n \n https://download.pytorch.org/libtorch/cu92/libtorch-shared-without-deps-1.2.0.zip\n \n https://download.pytorch.org/libtorch/cu92/libtorch-static-with-deps-1.2.0.zip\n \n https://download.pytorch.org/libtorch/cu92/libtorch-static-without-deps-1.2.0.zip\n \n \n \n Linux CUDA 10.0:\n \n https://download.pytorch.org/libtorch/cu100/libtorch-shared-with-deps-1.2.0.zip\n \n https://download.pytorch.org/libtorch/cu100/libtorch-shared-without-deps-1.2.0.zip\n \n https://download.pytorch.org/libtorch/cu100/libtorch-static-with-deps-1.2.0.zip\n \n https://download.pytorch.org/libtorch/cu100/libtorch-static-without-deps-1.2.0.zip\n \n \n \n macOS CPU:\n \n https://download.pytorch.org/libtorch/cpu/libtorch-macos-1.2.0.zip",
      "y": "@mdlockyer This is the full list of download links:\n \n Linux CPU:\n \n https://download.pytorch.org/libtorch/cpu/libtorch-shared-with-deps-1.2.0.zip\n \n https://download.pytorch.org/libtorch/cpu/libtorch-shared-without-deps-1.2.0.zip\n \n https://download.pytorch.org/libtorch/cpu/libtorch-static-with-deps-1.2.0.zip\n \n https://download.pytorch.org/libtorch/cpu/libtorch-static-without-deps-1.2.0.zip\n \n \n \n Linux CUDA 9.2:\n \n https://download.pytorch.org/libtorch/cu92/libtorch-shared-with-deps-1.2.0.zip\n \n https://download.pytorch.org/libtorch/cu92/libtorch-shared-without-deps-1.2.0.zip\n \n https://download.pytorch.org/libtorch/cu92/libtorch-static-with-deps-1.2.0.zip\n \n https://download.pytorch.org/libtorch/cu92/libtorch-static-without-deps-1.2.0.zip\n \n \n \n Linux CUDA 10.0:\n \n https://download.pytorch.org/libtorch/cu100/libtorch-shared-with-deps-1.2.0.zip\n \n https://download.pytorch.org/libtorch/cu100/libtorch-shared-without-deps-1.2.0.zip\n \n https://download.pytorch.org/libtorch/cu100/libtorch-static-with-deps-1.2.0.zip\n \n https://download.pytorch.org/libtorch/cu100/libtorch-static-without-deps-1.2.0.zip\n \n \n \n macOS CPU:\n \n https://download.pytorch.org/libtorch/cpu/libtorch-macos-1.2.0.zip"
   },
   {
      "x": "In-place modification detection does not work if a Tensor does not own its storage",
      "z": "this and other autograd tasks (especially high pri ones) would be great for Alban to tackle once he's back into the world next week :)",
      "y": "this and other autograd tasks (especially high pri ones) would be great for Alban to tackle once he's back into the world next week :)"
   },
   {
      "x": "Example in torchscript documentation does not work",
      "z": "This is fixed in the [master docs](https://pytorch.org/docs/master/jit.html#torch.jit.trace), but we still need to push that to the stable docs so it's what people actually see. We're also in the process of adding our doc examples to our CI so this won't happen in the future.",
      "y": "This is fixed in the [master docs](https://pytorch.org/docs/master/jit.html#torch.jit.trace), but we still need to push that to the stable docs so it's what people actually see. We're also in the process of adding our doc examples to our CI so this won't happen in the future."
   },
   {
      "x": "Default initial hidden states for recurrent layers",
      "z": "Can we add this to the RNN documentation page as well? Took me a while to find this information here..",
      "y": "Can we add this to the RNN documentation page as well? Took me a while to find this information here.."
   },
   {
      "x": "Reduce package size",
      "z": "You can remove 3.7 - I don't think you use K80's large shared memory anywhere, and code generated for 3.5 should just run on 3.7. I'm also not sure how valuable and/or common 5.0 is. Finally, try adding -Xfatbin -compress-all to compiler args.",
      "y": "You can remove 3.7 - I don't think you use K80's large shared memory anywhere, and code generated for 3.5 should just run on 3.7. I'm also not sure how valuable and/or common 5.0 is. Finally, try adding -Xfatbin -compress-all to compiler args."
   },
   {
      "x": "add transforms to DataLoader",
      "z": "@colesbury batching along 2nd dimension is common, since the standard input format for all recurrent layers in `nn` is `seq_length x batch_size x dim`.",
      "y": "batching along 2nd dimension is common, since the standard input format for all recurrent layers in `nn` is `seq_length x batch_size x dim`."
   },
   {
      "x": "Implement remaining Variable functions",
      "z": "@soumith @SsnL The two references for Cholesky gradients that I know of are \n \n * https://people.maths.ox.ac.uk/gilesm/files/NA-08-01.pdf\n \n * https://arxiv.org/abs/1602.07527\n \n \n \n It's implemented in tensorflow and autograd here:\n \n * https://github.com/tensorflow/tensorflow/blob/master/tensorflow/python/ops/linalg_grad.py#L58\n \n * https://github.com/HIPS/autograd/blob/master/autograd/numpy/linalg.py#L110\n \n \n \n I think it's also implemented in stan somewhere as well but I'm not sure where exactly. \n \n \n \n I haven't thought too much about triangular solve gradients yet but I know that they're implemented in tensorflow here: https://github.com/tensorflow/tensorflow/blob/master/tensorflow/python/ops/linalg_grad.py#L207.",
      "y": "The two references for Cholesky gradients that I know of are \n \n * https://people.maths.ox.ac.uk/gilesm/files/NA-08-01.pdf\n \n * https://arxiv.org/abs/1602.07527\n \n \n \n It's implemented in tensorflow and autograd here:\n \n * https://github.com/tensorflow/tensorflow/blob/master/tensorflow/python/ops/linalg_grad.py#L58\n \n * https://github.com/HIPS/autograd/blob/master/autograd/numpy/linalg.py#L110\n\n I haven't thought too much about triangular solve gradients yet but I know that they're implemented in tensorflow here: https://github.com/tensorflow/tensorflow/blob/master/tensorflow/python/ops/linalg_grad.py#L207."
   },
   {
      "x": "Making Variable.grad a Variable breaks dropout backprop",
      "z": "Ok, everything is correct. I forgot I didn't do one last thing, that will change user-facing parts of the autograd API - `Function.backward` still accepts Tensors (and that's why dropout code is correct), but it will have to accept `Variable`s. I left it out for now, because it's a huge change that I didn't want to make hastily before the release, because it requires rewriting most of the code in autograd an a few other places, and will likely introduce new bugs... Only the code of users that define custom functions will need some changes in the future, but I think it's not going to be very common, and I'm willing to accept this, just to be sure that I'm not breaking it for everyone.",
      "y": "Ok, everything is correct. I forgot I didn't do one last thing, that will change user-facing parts of the autograd API - `Function.backward` still accepts Tensors (and that's why dropout code is correct), but it will have to accept `Variable`s. I left it out for now, because it's a huge change that I didn't want to make hastily before the release, because it requires rewriting most of the code in autograd an a few other places, and will likely introduce new bugs... Only the code of users that define custom functions will need some changes in the future, but I think it's not going to be very common, and I'm willing to accept this, just to be sure that I'm not breaking it for everyone."
   },
   {
      "x": "Dropout mode",
      "z": "I guess `y = F.dropout(x,p,mode) * (1-p)` should do the trick?",
      "y": "I guess `y = F.dropout(x,p,mode) * (1-p)` should do the trick?"
   },
   {
      "x": "wheels for python3.5 fail to install: .dist-info directory not found",
      "z": "this is fixed now",
      "y": "this is fixed now"
   },
   {
      "x": "OpenCL Support",
      "z": "Porting code to OpenCL by hand is not very maintainable. I think a more automated approach could be good.\n \n \n \n Here is a table of how I see things:\n \n \n \n | What | Who | Input | Backend | Comments |\n \n |----|----|---|---|----|\n \n |[coriander](https://github.com/hughperkins/coriander) | Me :-) | NVIDIA\u00c2\u00ae CUDA\u00e2\u201e\u00a2 | OpenCL 1.2 | Works on Mac :-) Opensource |\n \n | [HIP](https://github.com/GPUOpen-ProfessionalCompute-Tools/HIP) | AMD | HIP | AMD | |\n \n | [ComputeCpp](https://www.codeplay.com/products/computesuite/computecpp)\u00e2\u201e\u00a2 | Codeplay\u00c2\u00ae | SYCL | SPIR 1.2 | Official Tensorflow approach to OpenCL. |\n \n | [triSYCL](https://github.com/Xilinx/triSYCL) | Keryell | SYCL | \"SPIR 2.0\" | Opensource |\n \n | [OpenCL\u00e2\u201e\u00a2](https://www.khronos.org/opencl/) by hand | | OpenCL | OpenCL | High maintenance, unportable, means forking the code ... |\n \n | [NVIDIA\u00c2\u00ae CUDA\u00e2\u201e\u00a2](https://www.nvidia.com/object/cuda_home_new.html) | NVIDIA | NVIDIA\u00c2\u00ae CUDA\u00e2\u201e\u00a2 | CUDA/PTX/SASS | Reference implementation for most/all projects |\n \n \n \n Note: quick introduction to 'SPIR', well I will just quote https://www.khronos.org/spir:\n \n \n \n \"SPIR (Standard Portable Intermediate Representation) was initially developed for use by OpenCL and SPIR versions 1.2 and 2.0 were based on LLVM. SPIR has now evolved into a true cross-API standard that is fully defined by Khronos with native support for shader and kernel features \u00e2\u20ac\u201c called SPIR-V. [...]\n \n \n \n \"For developers, using SPIR-V means that kernel source code no longer has to be directly exposed, kernel load times can be accelerated and developers can choose the use of a common language front-end, improving kernel reliability and portability across multiple hardware implementations.\"",
      "y": "Porting code to OpenCL by hand is not very maintainable. I think a more automated approach could be good.\n \n \n \n Here is a table of how I see things:\n \n \n \n | What | Who | Input | Backend | Comments |\n \n |----|----|---|---|----|\n \n |[coriander](https://github.com/hughperkins/coriander) | Me :-) | NVIDIA\u00c2\u00ae CUDA\u00e2\u201e\u00a2 | OpenCL 1.2 | Works on Mac :-) Opensource |\n \n | [HIP](https://github.com/GPUOpen-ProfessionalCompute-Tools/HIP) | AMD | HIP | AMD | |\n \n | [ComputeCpp](https://www.codeplay.com/products/computesuite/computecpp)\u00e2\u201e\u00a2 | Codeplay\u00c2\u00ae | SYCL | SPIR 1.2 | Official Tensorflow approach to OpenCL. |\n \n | [triSYCL](https://github.com/Xilinx/triSYCL) | Keryell | SYCL | \"SPIR 2.0\" | Opensource |\n \n | [OpenCL\u00e2\u201e\u00a2](https://www.khronos.org/opencl/) by hand | | OpenCL | OpenCL | High maintenance, unportable, means forking the code ... |\n \n | [NVIDIA\u00c2\u00ae CUDA\u00e2\u201e\u00a2](https://www.nvidia.com/object/cuda_home_new.html) | NVIDIA | NVIDIA\u00c2\u00ae CUDA\u00e2\u201e\u00a2 | CUDA/PTX/SASS | Reference implementation for most/all projects |\n \n \n \n Note: quick introduction to 'SPIR', well I will just quote https://www.khronos.org/spir:\n \n \n \n \"SPIR (Standard Portable Intermediate Representation) was initially developed for use by OpenCL and SPIR versions 1.2 and 2.0 were based on LLVM. SPIR has now evolved into a true cross-API standard that is fully defined by Khronos with native support for shader and kernel features \u00e2\u20ac\u201c called SPIR-V. [...]\n \n \n \n \"For developers, using SPIR-V means that kernel source code no longer has to be directly exposed, kernel load times can be accelerated and developers can choose the use of a common language front-end, improving kernel reliability and portability across multiple hardware implementations.\""
   },
   {
      "x": "Implement Broadcasting with exact semantics as NumPy",
      "z": "Actually, it's more efficient to use `b.unqueeze(0).expand(X.size(0), b.size(0))` (`repeat` makes a new tensor, while `expand` does strides). I'll add `b.broadcast` that does it as a single op",
      "y": "Actually, it's more efficient to use `b.unqueeze(0).expand(X.size(0), b.size(0))` (`repeat` makes a new tensor, while `expand` does strides). I'll add `b.broadcast` that does it as a single op"
   },
   {
      "x": "wheels for python2.7 install failed: cuda 8.0",
      "z": "I think this is related to https://github.com/pytorch/pytorch/issues/482\n \n Could you try seeing if `import torch` works for you?",
      "y": "I think this is related to https://github.com/pytorch/pytorch/issues/482\n \n Could you try seeing if `import torch` works for you?"
   },
   {
      "x": "Add windows support please",
      "z": "@soumith I guess we could close this issue now.",
      "y": " I guess we could close this issue now."
   },
   {
      "x": "DOCS: Links to Source Code Broken",
      "z": "This seems to be fixed now.",
      "y": "This seems to be fixed now."
   },
   {
      "x": "Feature Request: Spatial Transformer Network",
      "z": "i'm working on this today.",
      "y": "i'm working on this today."
   },
   {
      "x": "shoud we support play pytorch without Anaconda?",
      "z": "we have wheels ready, but cant upload them until PyPI switches to their new infrastructure. See: https://github.com/pypa/packaging-problems/issues/120#issuecomment-366515498\n \n \n \n They said the move is in the coming weeks.",
      "y": "we have wheels ready, but cant upload them until PyPI switches to their new infrastructure. See: https://github.com/pypa/packaging-problems/issues/120#issuecomment-366515498\n \n \n \n They said the move is in the coming weeks."
   },
   {
      "x": "DOCS: Search feature hangs",
      "z": "now fixed :)",
      "y": "now fixed :)"
   },
   {
      "x": "How can I use CTC loss function ?",
      "z": "I'll be opening it soon, but @apaszke is right. ATM I've just wrapped a numpy API for quick use, but I'll be working on an actual C extension to use the C api directly",
      "y": "I'll be opening it soon, but @apaszke is right. ATM I've just wrapped a numpy API for quick use, but I'll be working on an actual C extension to use the C api directly"
   },
   {
      "x": "undefined symbol: sormqr_",
      "z": "I'm running on an up-to-date Ubuntu 16.04.01 LTS, kernel 4.4.0-59 @ Lenovo Y50.\n \n Cuda 8.0.44; Nvidia drivers 367.57 @ GeForce GTX 860M\n \n gcc 4.8.5\n \n \n \n You were right, the math and pthread libraries were missing and they were not linked during installation. On my machine the two libraries are in this folder `/usr/lib/x86_64-linux-gnu/`. This folder is among those listed by `gcc --print-search-dirs`, but it's not in `$LIBRARY_PATH`.\n \n \n \n Executing `export LIBRARY_PATH=$LIBRARY_PATH:/usr/lib/x86_64-linux-gnu/` before installing pytorch solved the problem on my machine.",
      "y": "I'm running on an up-to-date Ubuntu 16.04.01 LTS, kernel 4.4.0-59 @ Lenovo Y50.\n \n Cuda 8.0.44; Nvidia drivers 367.57 @ GeForce GTX 860M\n \n gcc 4.8.5\n \n \n \n You were right, the math and pthread libraries were missing and they were not linked during installation. On my machine the two libraries are in this folder `/usr/lib/x86_64-linux-gnu/`. This folder is among those listed by `gcc --print-search-dirs`, but it's not in `$LIBRARY_PATH`.\n \n \n \n Executing `export LIBRARY_PATH=$LIBRARY_PATH:/usr/lib/x86_64-linux-gnu/` before installing pytorch solved the problem on my machine."
   },
   {
      "x": "AttributeError: type object 'torch._C.FloatTensorBase' has no attribute 'numpy'",
      "z": "You need to open python from a different folder than `~/pytorch`, as can be seen in https://github.com/pytorch/pytorch/issues/7 .",
      "y": "You need to open python from a different folder than `~/pytorch`, as can be seen in https://github.com/pytorch/pytorch/issues/7 ."
   },
   {
      "x": "look into why the NVIDIA CUDA Dockerfile doesn't build pytorch",
      "z": "@ngimel fixed now",
      "y": "fixed now"
   },
   {
      "x": "Feature Request: Length Masking for RNNs",
      "z": "But sometimes we have more than one input. We can't sort both seq1 and seq2 at the same time. Like machine comprehension model, our inputs is: question-passage.",
      "y": "But sometimes we have more than one input. We can't sort both seq1 and seq2 at the same time. Like machine comprehension model, our inputs is: question-passage."
   },
   {
      "x": "Half precision convolution not working",
      "z": "In cudnn bindings convolution math type is inferred from input data type (and inferred to be equal to input data type):\n \n https://github.com/pytorch/pytorch/blob/master/torch/csrc/cudnn/Conv.cpp#L353-L361\n \n https://github.com/pytorch/pytorch/blob/master/tools/cwrap/plugins/CuDNNPlugin.py#L51\n \n Math type for convolutions with half inputs and weights should be CUDNN_DATA_FLOAT because\n \n 1) computations with half math are not supported anywhere except Pascal and TX1\n \n 2) Even on Pascal, 6.1 cards have very low performance fp16 math\n \n 3) fp16 math comes with a set of accuracy problems\n \n @colesbury",
      "y": "In cudnn bindings convolution math type is inferred from input data type (and inferred to be equal to input data type):\n \n https://github.com/pytorch/pytorch/blob/master/torch/csrc/cudnn/Conv.cpp#L353-L361\n \n https://github.com/pytorch/pytorch/blob/master/tools/cwrap/plugins/CuDNNPlugin.py#L51\n \n Math type for convolutions with half inputs and weights should be CUDNN_DATA_FLOAT because\n \n 1) computations with half math are not supported anywhere except Pascal and TX1\n \n 2) Even on Pascal, 6.1 cards have very low performance fp16 math\n \n 3) fp16 math comes with a set of accuracy problems\n "
   },
   {
      "x": "cuda runtime error (8) : invalid device function during ops between cuda tensors",
      "z": "if you reinstall pytorch from conda, this should now be fixed.\n \n I am also going to commit refreshed pip wheels in ~2 hours",
      "y": "if you reinstall pytorch from conda, this should now be fixed.\n \n I am also going to commit refreshed pip wheels in ~2 hours"
   },
   {
      "x": "Error installing from source on Mac(10.12),CUDA=8.0, CUDNN=5.1.5",
      "z": "Try this if you have added the Anaconda directory to your bash shell PATH environment variable:\n \n \n \n ```shell\n \n CC=clang CXX=clang++ python setup.py install\n \n ```",
      "y": "Try this if you have added the Anaconda directory to your bash shell PATH environment variable:\n \n \n \n ```shell\n \n CC=clang CXX=clang++ python setup.py install\n \n ```"
   },
   {
      "x": "Initial call to .cuda() very slow with Titan X",
      "z": "you installed with Conda.\n \n Your Titan-X is Titan-X Pascal.\n \n \n \n I am aware of the issue, i am building Conda binaries which have pre-compiled code fore 6.0 and 6.1 architectures :)",
      "y": "you installed with Conda.\n \n Your Titan-X is Titan-X Pascal.\n \n \n \n I am aware of the issue, i am building Conda binaries which have pre-compiled code fore 6.0 and 6.1 architectures :)"
   },
   {
      "x": "Inferring NumPy array type when using `from_numpy`",
      "z": "It will always create a tensor of a corresponding type. It doesn't do a memory copy, and only views onto `ndarray`'s memory, so it can't use a different data type, or you'd see complete garbage there.\n \n \n \n `y.dtype` is `dtype('float64')` i.e. `double`. E.g. this:\n \n \n \n ```python\n \n y = np.array([[1., 2., 3.],\n \n  [4., 5., 6.]], dtype=np.int32)\n \n \n \n torch.from_numpy(y)\n \n ```\n \n \n \n gives\n \n ```\n \n  1 2 3\n \n  4 5 6\n \n [torch.IntTensor of size 2x3]\n \n ```",
      "y": "It will always create a tensor of a corresponding type. It doesn't do a memory copy, and only views onto `ndarray`'s memory, so it can't use a different data type, or you'd see complete garbage there.\n \n \n \n `y.dtype` is `dtype('float64')` i.e. `double`. E.g. this:\n \n \n \n ```python\n \n y = np.array([[1., 2., 3.],\n \n  [4., 5., 6.]], dtype=np.int32)\n \n \n \n torch.from_numpy(y)\n \n ```\n \n \n \n gives\n \n ```\n \n  1 2 3\n \n  4 5 6\n \n [torch.IntTensor of size 2x3]\n \n ```"
   },
   {
      "x": "API suggestion: \".gpu()\" instead of \".cuda()\"",
      "z": "As a Torch/PyTorch beginner, I found the naming a little confusing at first and requiring education what exactly is going on when I call this. I thought it was configuration-only at first, so it's not as self-explanatory as it could be. Fortunately, this can be addressed entirely with an extra helper method \u00e2\u20ac\u201d hence backward-compatible and Torch-friendly too.\n \n \n \n I'd personally suggest naming to reflect the operation actually done under the hood. I tend to name functions as verbs, so something like `transfer(gpu=0)` or some combination of `upload(device='gpu0')` / `download(...)`.",
      "y": "As a Torch/PyTorch beginner, I found the naming a little confusing at first and requiring education what exactly is going on when I call this. I thought it was configuration-only at first, so it's not as self-explanatory as it could be. Fortunately, this can be addressed entirely with an extra helper method \u00e2\u20ac\u201d hence backward-compatible and Torch-friendly too.\n \n \n \n I'd personally suggest naming to reflect the operation actually done under the hood. I tend to name functions as verbs, so something like `transfer(gpu=0)` or some combination of `upload(device='gpu0')` / `download(...)`."
   },
   {
      "x": "NO_DISTRIBUTED environment variable documentation needs to be updated",
      "z": "I think we standardize on USE_... now, so the first variant would be it. ~~Maybe I'll tack this onto #17295.~~\n \n On second thought, maybe it's best to fix all of the variables in the description that aren't up to date in one go.",
      "y": "I think we standardize on USE_... now, so the first variant would be it. ~~Maybe I'll tack this onto #17295.~~\n \n On second thought, maybe it's best to fix all of the variables in the description that aren't up to date in one go."
   },
   {
      "x": "torch.exp on CPU is incorrect for tensors with more than 2^31 or more elements",
      "z": "This is due to the fact that we link against MKL LP64 and not MKL ILP64. As such the MKL VML calls only accept 32bit types. I'll write something that will call into VML repeatedly if this is the case.",
      "y": "This is due to the fact that we link against MKL LP64 and not MKL ILP64. As such the MKL VML calls only accept 32bit types. I'll write something that will call into VML repeatedly if this is the case."
   },
   {
      "x": "Maximum value of torch.rand",
      "z": "@vsub21 It will work if you use low precision.\n \n \n \n PyTorch supports very high precision like 1e-15 or more.\n \n \n \n That\u00e2\u20ac\u2122s why I confused.\n \n \n \n For very precisely, I recommend you to use my method. It\u00e2\u20ac\u2122s simple and fast in terms of probablistic points :)\n \n \n \n If you don\u00e2\u20ac\u2122t mind about sampling 0.0000001, then use your method.\n \n \n \n In my experience, sometimes very small value is important.\n \n \n \n But it is your choice. :)",
      "y": "It will work if you use low precision.\n \n \n \n PyTorch supports very high precision like 1e-15 or more.\n \n \n \n That\u00e2\u20ac\u2122s why I confused.\n \n \n \n For very precisely, I recommend you to use my method. It\u00e2\u20ac\u2122s simple and fast in terms of probablistic points :)\n \n \n \n If you don\u00e2\u20ac\u2122t mind about sampling 0.0000001, then use your method.\n \n \n \n In my experience, sometimes very small value is important.\n \n \n \n But it is your choice. :)"
   },
   {
      "x": "RuntimeError: cublas runtime error : the GPU program failed to execute at at pytorch/work/aten/src/THC/THCBlas.cu:258",
      "z": "For RTX 2080, you need to use CUDA10, not CUDA 9.2",
      "y": "For RTX 2080, you need to use CUDA10, not CUDA 9.2"
   },
   {
      "x": "Multiprocessing's spawn says \"Only Tensors of floating point dtype can require gradients\" while it isn't",
      "z": "Bug is here:\n \n \n \n https://github.com/pytorch/pytorch/blob/3145f46a2287619e79add11a8d543729b2590a14/torch/multiprocessing/reductions.py#L83-L84\n \n \n \n nn.Parameter() needs `requires_grad=False` in constructor in this case",
      "y": "Bug is here:\n \n \n \n https://github.com/pytorch/pytorch/blob/3145f46a2287619e79add11a8d543729b2590a14/torch/multiprocessing/reductions.py#L83-L84\n \n \n \n nn.Parameter() needs `requires_grad=False` in constructor in this case"
   },
   {
      "x": "ProcessGroupNCCL.cpp:260, unhandled cuda error",
      "z": "I've met a similar issue on my machine with pytorch 1.1, CUDA 10.0 and NCCL 2.4.2. The only difference is that my error message is \"RuntimeError: ...... ProcessGroupNCCL.cpp:272, unhandled system error\". I finally solved it with setting NCCL_SOCKET_IFNAME=ib0 and NCCL_IB_DISABLE=1. \n \n \n \n Hope it will help!",
      "y": "I've met a similar issue on my machine with pytorch 1.1, CUDA 10.0 and NCCL 2.4.2. The only difference is that my error message is \"RuntimeError: ...... ProcessGroupNCCL.cpp:272, unhandled system error\". I finally solved it with setting NCCL_SOCKET_IFNAME=ib0 and NCCL_IB_DISABLE=1. \n \n \n \n Hope it will help!"
   },
   {
      "x": "cuda runtime error (3): we're not detecting bad forks",
      "z": "@colesbury @mrshenli If description of this issue is going to be changed to fixing the \"bad fork\" error detection, we can close https://github.com/pytorch/pytorch/issues/17359 as duplicate.",
      "y": "@colesbury @mrshenli If description of this issue is going to be changed to fixing the \"bad fork\" error detection, we can close https://github.com/pytorch/pytorch/issues/17359 as duplicate."
   },
   {
      "x": "What is a version/git-hash of nightly build?",
      "z": "Fwiwi t's not much work at all. When we built the linux wheel and libtorch packages we unzip them, mess around with them, and then rezip them back up. If you just want a txt file called build_hash or something like that you could just add that file before the wheel gets zipped up here https://github.com/pytorch/builder/blob/master/manywheel/build_common.sh#L300 . A line like\n \n ```\n \n echo \"$(pushd $pytorch_rootdir && git rev-list --max-count 1 HEAD)\" > $PREFIX/build_hash\n \n ```\n \n would probably do the trick. There is more work to get the same file in the python packages or on non-linux.",
      "y": "Fwiwi t's not much work at all. When we built the linux wheel and libtorch packages we unzip them, mess around with them, and then rezip them back up. If you just want a txt file called build_hash or something like that you could just add that file before the wheel gets zipped up here https://github.com/pytorch/builder/blob/master/manywheel/build_common.sh#L300 . A line like\n \n ```\n \n echo \"$(pushd $pytorch_rootdir && git rev-list --max-count 1 HEAD)\" > $PREFIX/build_hash\n \n ```\n \n would probably do the trick. There is more work to get the same file in the python packages or on non-linux."
   },
   {
      "x": "Support ... indexing in JIT",
      "z": "@suo I was thinking to give this one a try if you don't mind.",
      "y": "I was thinking to give this one a try if you don't mind."
   },
   {
      "x": "Missing (dynamic) libraries in current libtorch (1.0.1) - macOS",
      "z": "As of 23rd June, the archive at https://download.pytorch.org/libtorch/nightly/cpu/libtorch-macos-latest.zip still doesn't contain libmklml.dylib\n \n \n \n ```\n \n wilmot_p@Pierres-MacBook-Air \u00e2\u017e\u0153 build git:(AddModel) \u00e2\u0153\u2014 ll ../lib/libtorch/lib/\n \n total 283904\n \n -rw-r--r-- 1 wilmot_p staff 7.5K Jun 23 07:27 libCaffe2_perfkernels_avx.a\n \n -rw-r--r-- 1 wilmot_p staff 183K Jun 23 07:27 libCaffe2_perfkernels_avx2.a\n \n -rw-r--r-- 1 wilmot_p staff 384B Jun 23 07:27 libCaffe2_perfkernels_avx512.a\n \n -rw-r--r-- 1 wilmot_p staff 398K Jun 23 07:27 libTHD.a\n \n -rw-r--r-- 1 wilmot_p staff 480K Jun 23 07:27 libbenchmark.a\n \n -rw-r--r-- 1 wilmot_p staff 1.4K Jun 23 07:27 libbenchmark_main.a\n \n -rwxr-xr-x 1 wilmot_p staff 293K Jun 23 07:27 libc10.dylib\n \n -rwxr-xr-x 1 wilmot_p staff 799K Jun 23 07:27 libcaffe2_detectron_ops.dylib\n \n -rwxr-xr-x 1 wilmot_p staff 130K Jun 23 07:27 libcaffe2_module_test_dynamic.dylib\n \n -rwxr-xr-x 1 wilmot_p staff 145K Jun 23 07:27 libcaffe2_observers.dylib\n \n -rw-r--r-- 1 wilmot_p staff 914K Jun 23 07:27 libcaffe2_protos.a\n \n -rw-r--r-- 1 wilmot_p staff 5.0K Jun 23 07:27 libclog.a\n \n -rw-r--r-- 1 wilmot_p staff 43K Jun 23 07:27 libcpuinfo.a\n \n -rw-r--r-- 1 wilmot_p staff 40K Jun 23 07:27 libcpuinfo_internals.a\n \n -rwxr-xr-x 1 wilmot_p staff 14K Jun 23 07:27 libfoxi.dylib\n \n -rwxr-xr-x 1 wilmot_p staff 13K Jun 23 07:27 libfoxi_dummy.dylib\n \n -rw-r--r-- 1 wilmot_p staff 2.0K Jun 23 07:27 libfoxi_loader.a\n \n -rw-r--r-- 1 wilmot_p staff 161K Jun 23 07:27 libgmock.a\n \n -rw-r--r-- 1 wilmot_p staff 3.9K Jun 23 07:27 libgmock_main.a\n \n -rw-r--r-- 1 wilmot_p staff 492K Jun 23 07:27 libgtest.a\n \n -rw-r--r-- 1 wilmot_p staff 1.2K Jun 23 07:27 libgtest_main.a\n \n -rw-r--r-- 1 wilmot_p staff 20M Jun 23 07:27 libmkldnn.a\n \n -rw-r--r-- 1 wilmot_p staff 206K Jun 23 07:27 libnnpack.a\n \n -rw-r--r-- 1 wilmot_p staff 16K Jun 23 07:27 libnnpack_reference_layers.a\n \n -rw-r--r-- 1 wilmot_p staff 4.2M Jun 23 07:27 libonnx.a\n \n -rw-r--r-- 1 wilmot_p staff 363K Jun 23 07:27 libonnx_proto.a\n \n -rwxr-xr-x 1 wilmot_p staff 14K Jun 23 07:27 libonnxifi.dylib\n \n -rwxr-xr-x 1 wilmot_p staff 13K Jun 23 07:27 libonnxifi_dummy.dylib\n \n -rw-r--r-- 1 wilmot_p staff 2.0K Jun 23 07:27 libonnxifi_loader.a\n \n -rw-r--r-- 1 wilmot_p staff 529K Jun 23 07:27 libprotobuf-lite.a\n \n -rw-r--r-- 1 wilmot_p staff 3.9M Jun 23 07:27 libprotobuf.a\n \n -rw-r--r-- 1 wilmot_p staff 3.5M Jun 23 07:27 libprotoc.a\n \n -rw-r--r-- 1 wilmot_p staff 9.5K Jun 23 07:27 libpthreadpool.a\n \n -rw-r--r-- 1 wilmot_p staff 122K Jun 23 07:27 libqnnpack.a\n \n -rwxr-xr-x 1 wilmot_p staff 54K Jun 23 07:27 libshm.dylib\n \n -rwxr-xr-x 1 wilmot_p staff 83M Jun 23 07:27 libtorch.dylib\n \n -rwxr-xr-x 1 wilmot_p staff 10M Jun 23 07:27 libtorch_python.dylib\n \n ```",
      "y": "As of 23rd June, the archive at https://download.pytorch.org/libtorch/nightly/cpu/libtorch-macos-latest.zip still doesn't contain libmklml.dylib\n \n \n \n ```\n \n\u00e2\u017e\u0153 build git:(AddModel) \u00e2\u0153\u2014 ll ../lib/libtorch/lib/\n \n total 283904\n \n -rw-r--r-- 1 wilmot_p staff 7.5K Jun 23 07:27 libCaffe2_perfkernels_avx.a\n \n -rw-r--r-- 1 wilmot_p staff 183K Jun 23 07:27 libCaffe2_perfkernels_avx2.a\n \n -rw-r--r-- 1 wilmot_p staff 384B Jun 23 07:27 libCaffe2_perfkernels_avx512.a\n \n -rw-r--r-- 1 wilmot_p staff 398K Jun 23 07:27 libTHD.a\n \n -rw-r--r-- 1 wilmot_p staff 480K Jun 23 07:27 libbenchmark.a\n \n -rw-r--r-- 1 wilmot_p staff 1.4K Jun 23 07:27 libbenchmark_main.a\n \n -rwxr-xr-x 1 wilmot_p staff 293K Jun 23 07:27 libc10.dylib\n \n -rwxr-xr-x 1 wilmot_p staff 799K Jun 23 07:27 libcaffe2_detectron_ops.dylib\n \n -rwxr-xr-x 1 wilmot_p staff 130K Jun 23 07:27 libcaffe2_module_test_dynamic.dylib\n \n -rwxr-xr-x 1 wilmot_p staff 145K Jun 23 07:27 libcaffe2_observers.dylib\n \n -rw-r--r-- 1 wilmot_p staff 914K Jun 23 07:27 libcaffe2_protos.a\n \n -rw-r--r-- 1 wilmot_p staff 5.0K Jun 23 07:27 libclog.a\n \n -rw-r--r-- 1 wilmot_p staff 43K Jun 23 07:27 libcpuinfo.a\n \n -rw-r--r-- 1 wilmot_p staff 40K Jun 23 07:27 libcpuinfo_internals.a\n \n -rwxr-xr-x 1 wilmot_p staff 14K Jun 23 07:27 libfoxi.dylib\n \n -rwxr-xr-x 1 wilmot_p staff 13K Jun 23 07:27 libfoxi_dummy.dylib\n \n -rw-r--r-- 1 wilmot_p staff 2.0K Jun 23 07:27 libfoxi_loader.a\n \n -rw-r--r-- 1 wilmot_p staff 161K Jun 23 07:27 libgmock.a\n \n -rw-r--r-- 1 wilmot_p staff 3.9K Jun 23 07:27 libgmock_main.a\n \n -rw-r--r-- 1 wilmot_p staff 492K Jun 23 07:27 libgtest.a\n \n -rw-r--r-- 1 wilmot_p staff 1.2K Jun 23 07:27 libgtest_main.a\n \n -rw-r--r-- 1 wilmot_p staff 20M Jun 23 07:27 libmkldnn.a\n \n -rw-r--r-- 1 wilmot_p staff 206K Jun 23 07:27 libnnpack.a\n \n -rw-r--r-- 1 wilmot_p staff 16K Jun 23 07:27 libnnpack_reference_layers.a\n \n -rw-r--r-- 1 wilmot_p staff 4.2M Jun 23 07:27 libonnx.a\n \n -rw-r--r-- 1 wilmot_p staff 363K Jun 23 07:27 libonnx_proto.a\n \n -rwxr-xr-x 1 wilmot_p staff 14K Jun 23 07:27 libonnxifi.dylib\n \n -rwxr-xr-x 1 wilmot_p staff 13K Jun 23 07:27 libonnxifi_dummy.dylib\n \n -rw-r--r-- 1 wilmot_p staff 2.0K Jun 23 07:27 libonnxifi_loader.a\n \n -rw-r--r-- 1 wilmot_p staff 529K Jun 23 07:27 libprotobuf-lite.a\n \n -rw-r--r-- 1 wilmot_p staff 3.9M Jun 23 07:27 libprotobuf.a\n \n -rw-r--r-- 1 wilmot_p staff 3.5M Jun 23 07:27 libprotoc.a\n \n -rw-r--r-- 1 wilmot_p staff 9.5K Jun 23 07:27 libpthreadpool.a\n \n -rw-r--r-- 1 wilmot_p staff 122K Jun 23 07:27 libqnnpack.a\n \n -rwxr-xr-x 1 wilmot_p staff 54K Jun 23 07:27 libshm.dylib\n \n -rwxr-xr-x 1 wilmot_p staff 83M Jun 23 07:27 libtorch.dylib\n \n -rwxr-xr-x 1 wilmot_p staff 10M Jun 23 07:27 libtorch_python.dylib\n \n ```"
   },
   {
      "x": "CuDNN detected while compile; returns false on compiledWithCuDNN() check",
      "z": "this log is not complete, as it's the log of a second invocation.\n \n \n \n First run `git clean -xfd` to clean all build files, and then run\n \n \n \n ```\n \n CUDNN_INCLUDE_DIR=~/cuda/include CUDNN_LIBRARY=~/cuda/lib64 python setup.py install\n \n ```\n \n \n \n Then paste that full log.\n \n \n \n thanks.",
      "y": "this log is not complete, as it's the log of a second invocation.\n \n \n \n First run `git clean -xfd` to clean all build files, and then run\n \n \n \n ```\n \n CUDNN_INCLUDE_DIR=~/cuda/include CUDNN_LIBRARY=~/cuda/lib64 python setup.py install\n \n ```\n \n \n \n Then paste that full log.\n \n \n \n thanks."
   },
   {
      "x": "The GPU memory of tensor will not release in libtorch",
      "z": "@soumith Thanks a lot! `cudaDeviceReset()` can release the resource associated with the current process.",
      "y": "@soumith Thanks a lot! `cudaDeviceReset()` can release the resource associated with the current process."
   },
   {
      "x": "RuntimeError: cur_offset == offset ASSERT FAILED at ..\\aten\\src\\ATen\\native\\cudnn\\RNN.cpp:471, please report a bug to PyTorch. cur_offset = 36864; offset = 18432",
      "z": "Hi, you should check if the data type is suitable; in my case, the embedding weight needs float64 type, then I fix it.\n \n Cheers,",
      "y": "Hi, you should check if the data type is suitable; in my case, the embedding weight needs float64 type, then I fix it.\n \n Cheers,"
   },
   {
      "x": "Why do I have to install CUDA and CUDNN first before installing pytorch GPU version ?",
      "z": "No, `conda install` will include the necessary cuda and cudnn binaries, you don't have to install them separately. However you do have to specify the cuda version you want to use, e.g. `conda install pytorch cudatoolkit=9.0 -c pytorch`.\n \n See PyTorch's [Get started](https://pytorch.org/get-started/locally/) guide for more info and detailed installation instructions :smile:",
      "y": "No, `conda install` will include the necessary cuda and cudnn binaries, you don't have to install them separately. However you do have to specify the cuda version you want to use, e.g. `conda install pytorch cudatoolkit=9.0 -c pytorch`.\n \n See PyTorch's [Get started](https://pytorch.org/get-started/locally/) guide for more info and detailed installation instructions :smile:"
   },
   {
      "x": "inaccurate description of torch.mul(tensor, tensor)",
      "z": "yea, I agree, the wording there is a bit off.\n \n Would you mind sending a Pull Request improving the doc?",
      "y": "yea, I agree, the wording there is a bit off.\n \n Would you mind sending a Pull Request improving the doc?"
   },
   {
      "x": "torch.utils.ffi is deprecated. How do I use cpp extensions instead?",
      "z": "damn, weeks of work to the trash...\n \n \n \n do you plan on deprecating such low-level stuff on a regular basis or is it really worth it trying to make a cpp extension now ?",
      "y": "damn, weeks of work to the trash...\n \n \n \n do you plan on deprecating such low-level stuff on a regular basis or is it really worth it trying to make a cpp extension now ?"
   },
   {
      "x": "bce(sigmoid(a), b) and bce_with_logits(a, b) return different values",
      "z": "Yes, that's the problem of numerical stability. Sigmoid of 17 and 15 is very close to 1, and loses precision. Convert a and b to double and see results become the same.",
      "y": "Yes, that's the problem of numerical stability. Sigmoid of 17 and 15 is very close to 1, and loses precision. Convert a and b to double and see results become the same."
   },
   {
      "x": "[jit] Python functions can't take list argument",
      "z": "@zou3519 Is this the same issue? https://github.com/pytorch/pytorch/issues/15809",
      "y": " Is this the same issue? https://github.com/pytorch/pytorch/issues/15809"
   },
   {
      "x": "torch.einsum equation works in NumPy but not in Pytorch",
      "z": "Ah, sorry, I meant to post the workaround but forgot. Just replace the letter in size-1-dimension with anything not appearing in the equation, e.g.:\n \n ```python\n \n torch.einsum(\"xijk, bilm -> bjklm\", torch.randn(1, 3, 24, 20), torch.randn(5, 3, 24, 20))\n \n ```\n \n This works because you can split out the constant (after broadcasting) from the product.\n \n \n \n Best regards\n \n \n \n Thomas",
      "y": "Ah, sorry, I meant to post the workaround but forgot. Just replace the letter in size-1-dimension with anything not appearing in the equation, e.g.:\n \n ```python\n \n torch.einsum(\"xijk, bilm -> bjklm\", torch.randn(1, 3, 24, 20), torch.randn(5, 3, 24, 20))\n \n ```\n \n This works because you can split out the constant (after broadcasting) from the product.\n "
   },
   {
      "x": "Tests for Vec256 classes",
      "z": "@quickwritereader I think it would be a very useful activity for unit tests to come into existence as part of the POWER work, if you can put that into your work scope.",
      "y": " I think it would be a very useful activity for unit tests to come into existence as part of the POWER work, if you can put that into your work scope."
   },
   {
      "x": "jit_type.h Type Class's cast function",
      "z": "Thank you!",
      "y": "Thank you!"
   },
   {
      "x": "Consolidate flag/environment variable handling in setup.py",
      "z": "one step towards this that I'm working on is removing the middle layer (build_pytorch_libs.sh)",
      "y": "one step towards this that I'm working on is removing the middle layer (build_pytorch_libs.sh)"
   },
   {
      "x": "cuda10 version of 1.0 doesn't work on linux",
      "z": "@solomatov this is a bug in our packaging i think, I'll try to get it fixed for 1.0.1 release. cc: @pjh5 for visibility.\n \n \n \n In the meanwhile, you can use the `anaconda` version via `conda install` and it works because we also pull the anaconda `cudatoolkit` dependency.",
      "y": "this is a bug in our packaging i think, I'll try to get it fixed for 1.0.1 release. cc: @pjh5 for visibility.\n \n \n \n In the meanwhile, you can use the `anaconda` version via `conda install` and it works because we also pull the anaconda `cudatoolkit` dependency."
   },
   {
      "x": "torch.Tensor.bernoulli_ gives different results in mac and linux",
      "z": "Determinism is not guaranteed across version/hardware/platforms.\n \n On Fri, Jan 4, 2019 at 06:46 Alasdair Tran <notifications@github.com> wrote:\n \n > \u00f0\u0178\u0090\u203a Bug\n >\n > Running the following commands\n >\n > import torch\n >\n > torch.manual_seed(0)\n >\n > a = torch.zeros(5).long()\n >\n > a.bernoulli_()\n >\n > give different samples in linux and mac. On my Ubuntu 18.04, I get tensor([1,\n > 0, 0, 1, 1]). On my MacOS Mojave, I get tensor([0, 0, 1, 0, 0]). On the\n > same machine, it always gives the same sample, but it's not reproducible\n > across machines.\n >\n > I sampled from other distributions such as normal_(), uniform_() and\n > random_(), and they all work fine (i.e. give the same sample across\n > machines). The non-inplace version of Bernoulli bernoulli() works fine\n > too.\n >\n > This non-reproducible problem seems to only occur with the in-place\n > bernoulli_() function.\n > Environment\n >\n > - PyTorch Version (e.g., 1.0): 1.0.0\n > - How you installed Pytorch: conda\n > - Python version: 3.7\n >\n > \u00e2\u20ac\u201d\n > You are receiving this because you are subscribed to this thread.\n > Reply to this email directly, view it on GitHub\n > <https://github.com/pytorch/pytorch/issues/15714>, or mute the thread\n > <https://github.com/notifications/unsubscribe-auth/AFaWZfY55m7eJEzapcURzGbHrMJM2bOLks5u_ogugaJpZM4Zo4_i>\n > .\n >",
      "y": "Determinism is not guaranteed across version/hardware/platforms.\n \n On Fri, Jan 4, 2019 at 06:46 Alasdair Tran <notifications@github.com> wrote:\n \n > \u00f0\u0178\u0090\u203a Bug\n >\n > Running the following commands\n >\n > import torch\n >\n > torch.manual_seed(0)\n >\n > a = torch.zeros(5).long()\n >\n > a.bernoulli_()\n >\n > give different samples in linux and mac. On my Ubuntu 18.04, I get tensor([1,\n > 0, 0, 1, 1]). On my MacOS Mojave, I get tensor([0, 0, 1, 0, 0]). On the\n > same machine, it always gives the same sample, but it's not reproducible\n > across machines.\n >\n > I sampled from other distributions such as normal_(), uniform_() and\n > random_(), and they all work fine (i.e. give the same sample across\n > machines). The non-inplace version of Bernoulli bernoulli() works fine\n > too.\n >\n > This non-reproducible problem seems to only occur with the in-place\n > bernoulli_() function.\n > Environment\n >\n > - PyTorch Version (e.g., 1.0): 1.0.0\n > - How you installed Pytorch: conda\n > - Python version: 3.7\n >\n > \u00e2\u20ac\u201d\n > You are receiving this because you are subscribed to this thread.\n > Reply to this email directly, view it on GitHub\n > <https://github.com/pytorch/pytorch/issues/15714>, or mute the thread\n > <https://github.com/notifications/unsubscribe-auth/AFaWZfY55m7eJEzapcURzGbHrMJM2bOLks5u_ogugaJpZM4Zo4_i>\n > .\n >"
   },
   {
      "x": "PyTorch doesn't report useful error when forking after CUDA initialization (originally: Unknown cuda error with multiprocessing)",
      "z": "We can't fix this directly, because CUDA API must not be initialized before you fork (this is a well known limitation), but I was pretty sure we had a good error message for this case, which seems to have regressed.\n \n \n \n You can use the spawn method instead. But you also have to make sure you wrap the parent script in an `if __name__ == '__main__'` block or you'll get an oblique error message about `freeze_support`:\n \n \n \n ```\n \n import torch.multiprocessing as multiprocessing\n \n import torch\n \n \n \n def mp_worker(gpu):\n \n  print(torch.cuda.get_device_properties(gpu))\n \n \n \n if __name__ == '__main__':\n \n  gpus = list(range(torch.cuda.device_count()))\n \n \n \n  ctx = multiprocessing.get_context('spawn')\n \n \n \n  processes = [ctx.Process(target=mp_worker, args=(gpui,)) for gpui in gpus]\n \n  for process in processes:\n \n  process.start()\n \n  for process in processes:\n \n  process.join() \n \n ```",
      "y": "We can't fix this directly, because CUDA API must not be initialized before you fork (this is a well known limitation), but I was pretty sure we had a good error message for this case, which seems to have regressed.\n \n \n \n You can use the spawn method instead. But you also have to make sure you wrap the parent script in an `if __name__ == '__main__'` block or you'll get an oblique error message about `freeze_support`:\n \n \n \n ```\n \n import torch.multiprocessing as multiprocessing\n \n import torch\n \n \n \n def mp_worker(gpu):\n \n  print(torch.cuda.get_device_properties(gpu))\n \n \n \n if __name__ == '__main__':\n \n  gpus = list(range(torch.cuda.device_count()))\n \n \n \n  ctx = multiprocessing.get_context('spawn')\n \n \n \n  processes = [ctx.Process(target=mp_worker, args=(gpui,)) for gpui in gpus]\n \n  for process in processes:\n \n  process.start()\n \n  for process in processes:\n \n  process.join() \n \n ```"
   },
   {
      "x": "torch.bincount() seems problematic",
      "z": "Got it, I tried on a mac and I can reproduce this. We will look into it, thank you for the report",
      "y": "Got it, I tried on a mac and I can reproduce this. We will look into it, thank you for the report"
   },
   {
      "x": "torch.argsort descends wrongly",
      "z": "Okay I think the problem is caused by `NaN`. Here's a simpler repro:\n \n ```\n \n In [33]: a = torch.tensor([3, float('NaN'), 2])\n \n \n \n In [34]: torch.argsort(a, dim=0)\n \n Out[34]: tensor([0, 1, 2]) # wrong!\n \n \n \n In [36]: a = torch.tensor([1, float('NaN'), 2])\n \n \n \n In [37]: torch.argsort(a, dim=0, descending=True)\n \n Out[37]: tensor([0, 1, 2]) # wrong!\n \n ```\n \n Similarly if we remove the leading `Nan` in the array you provided, the problem is gone. \n \n ```\n \n In [38]: a = numpy.load(\"log\")\n \n \n \n In [39]: a = torch.tensor(a)\n \n \n \n In [40]: b = a[1:]\n \n \n \n In [41]: b\n \n Out[41]: tensor([ 0.0791, -0.0522, 0.0226, ..., -0.0267, 0.1027, 0.0064])\n \n \n \n In [49]: print(torch.argsort(b, dim=0, descending=True).narrow(0,0,8))\n \n tensor([ 49, 117, 46, 12, 143, 16, 19, 327])\n \n \n \n In [50]: print(torch.argsort(b, dim=0).narrow(0,b.shape[0]-8,8))\n \n tensor([327, 19, 16, 143, 12, 46, 117, 49])\n \n ```\n \n Actually the same problem exists for `torch.sort` as well, that `nan` breaks the monotonic increasing in the middle, but it was hidden in the printed sequence. \n \n ```\n \n In [51]: torch.sort(a)\n \n Out[51]:\n \n (tensor([-0.2427, -0.2274, -0.2272, ..., 0.5829, 0.7206, 1.0000]),\n \n  tensor([26606, 25901, 32186, ..., 47, 118, 50]))\n \n In [57]: torch.sort(a)[0][16360:16365]\n \n Out[57]: tensor([ 0.2460, 0.2769, nan, -0.2238, -0.2159])\n \n ```",
      "y": "Okay I think the problem is caused by `NaN`. Here's a simpler repro:\n \n ```\n \n In [33]: a = torch.tensor([3, float('NaN'), 2])\n \n \n \n In [34]: torch.argsort(a, dim=0)\n \n Out[34]: tensor([0, 1, 2]) # wrong!\n \n \n \n In [36]: a = torch.tensor([1, float('NaN'), 2])\n \n \n \n In [37]: torch.argsort(a, dim=0, descending=True)\n \n Out[37]: tensor([0, 1, 2]) # wrong!\n \n ```\n \n Similarly if we remove the leading `Nan` in the array you provided, the problem is gone. \n \n ```\n \n In [38]: a = numpy.load(\"log\")\n \n \n \n In [39]: a = torch.tensor(a)\n \n \n \n In [40]: b = a[1:]\n \n \n \n In [41]: b\n \n Out[41]: tensor([ 0.0791, -0.0522, 0.0226, ..., -0.0267, 0.1027, 0.0064])\n \n \n \n In [49]: print(torch.argsort(b, dim=0, descending=True).narrow(0,0,8))\n \n tensor([ 49, 117, 46, 12, 143, 16, 19, 327])\n \n \n \n In [50]: print(torch.argsort(b, dim=0).narrow(0,b.shape[0]-8,8))\n \n tensor([327, 19, 16, 143, 12, 46, 117, 49])\n \n ```\n \n Actually the same problem exists for `torch.sort` as well, that `nan` breaks the monotonic increasing in the middle, but it was hidden in the printed sequence. \n \n ```\n \n In [51]: torch.sort(a)\n \n Out[51]:\n \n (tensor([-0.2427, -0.2274, -0.2272, ..., 0.5829, 0.7206, 1.0000]),\n \n  tensor([26606, 25901, 32186, ..., 47, 118, 50]))\n \n In [57]: torch.sort(a)[0][16360:16365]\n \n Out[57]: tensor([ 0.2460, 0.2769, nan, -0.2238, -0.2159])\n \n ```"
   },
   {
      "x": "`torch.linalg.eigh` is much slower than `torch.linalg.svd` on GPU",
      "z": "> It seems that torch.linalg.svd is also optimized.\n \n \n \n That's right. For the problem size of 1000 x 4x4 matrices, an efficient batched algorithm is used for SVD. However, for symmetric eigenvalue decomposition, we currently have to use a for-loop based algorithm for all single matrix because there was a known numerical issue on the batched algorithm. \n \n \n \n We plan to update the code and use the more efficient batched algorithm for symmetric eigenvalue decomposition soon. Stay tuned! \u00f0\u0178\u02dc\u201e",
      "y": "> It seems that torch.linalg.svd is also optimized.\n \n \n \n That's right. For the problem size of 1000 x 4x4 matrices, an efficient batched algorithm is used for SVD. However, for symmetric eigenvalue decomposition, we currently have to use a for-loop based algorithm for all single matrix because there was a known numerical issue on the batched algorithm. \n \n \n \n We plan to update the code and use the more efficient batched algorithm for symmetric eigenvalue decomposition soon. Stay tuned! \u00f0\u0178\u02dc\u201e"
   },
   {
      "x": "What does \"build from source\" mean?",
      "z": "I think [this](https://github.com/pytorch/pytorch#from-source) might be helpful.",
      "y": "I think [this](https://github.com/pytorch/pytorch#from-source) might be helpful."
   },
   {
      "x": "nn.FeatureAlphaDropout is missing from the docs",
      "z": "@jbschlosser I am not totally sure how to add someone as a reviewer, but I have filed a pull request. #60590",
      "y": " I have filed a pull request. #60590"
   },
   {
      "x": "init_rpc fails after setting CUDA_VISIBLE_DEVICES env var to \"\"",
      "z": "Also tracked in https://github.com/pytorch/tensorpipe/issues/393.",
      "y": "Also tracked in https://github.com/pytorch/tensorpipe/issues/393."
   },
   {
      "x": "Exporting normalization modules introduces numerical errors",
      "z": "There may be some formal or informal expectations between different implentations of a given ONNX operator (I'm not sure). For that you could ask in the ONNX GitHub or Slack. But between ONNX and PyTorch I don't think there's any standard other than basic correctness.",
      "y": "There may be some formal or informal expectations between different implentations of a given ONNX operator (I'm not sure). For that you could ask in the ONNX GitHub or Slack. But between ONNX and PyTorch I don't think there's any standard other than basic correctness."
   },
   {
      "x": "[CI] Report wall-time and execution time stats from build/test stages",
      "z": "If possible, you should get fine grained time for individual gcc invocations, because the timings will vary wildly depending on if you get a hit on sccache or not, and the easiest way to reduce noise here is to record these separately.",
      "y": "If possible, you should get fine grained time for individual gcc invocations, because the timings will vary wildly depending on if you get a hit on sccache or not, and the easiest way to reduce noise here is to record these separately."
   },
   {
      "x": "CUDA unknown error - this may be due to an incorrectly set up environment, e.g. changing env variable CUDA_VISIBLE_DEVICES after program start. Setting the available devices to be zero.",
      "z": "I ran into the same problem with cuda 11.1 with debian 10 on GPC \n \n First uninstalling the cuda driver and then installing cuda 11.3 helped me solve it\n \n \n \n ```bash\n \n # uninstall cuda 11.1\n \n sudo nvidia-installer --uninstall\n \n \n \n # install cuda 11.3 for debian 10\n \n # you might need to match your distribution https://developer.nvidia.com/cuda-downloads?target_os=Linux&target_arch=x86_64&Distribution=Debian&target_version=10&target_type=deb_local\n \n \n \n wget https://developer.download.nvidia.com/compute/cuda/11.3.1/local_installers/cuda-repo-debian10-11-3-local_11.3.1-465.19.01-1_amd64.deb\n \n sudo dpkg -i cuda-repo-debian10-11-3-local_11.3.1-465.19.01-1_amd64.deb\n \n sudo apt-key add /var/cuda-repo-debian10-11-3-local/7fa2af80.pub\n \n sudo add-apt-repository contrib\n \n sudo apt-get update\n \n sudo apt-get -y install cuda\n \n ```",
      "y": "I ran into the same problem with cuda 11.1 with debian 10 on GPC \n \n First uninstalling the cuda driver and then installing cuda 11.3 helped me solve it\n \n \n \n ```bash\n \n # uninstall cuda 11.1\n \n sudo nvidia-installer --uninstall\n \n \n \n # install cuda 11.3 for debian 10\n \n # you might need to match your distribution https://developer.nvidia.com/cuda-downloads?target_os=Linux&target_arch=x86_64&Distribution=Debian&target_version=10&target_type=deb_local\n \n \n \n wget https://developer.download.nvidia.com/compute/cuda/11.3.1/local_installers/cuda-repo-debian10-11-3-local_11.3.1-465.19.01-1_amd64.deb\n \n sudo dpkg -i cuda-repo-debian10-11-3-local_11.3.1-465.19.01-1_amd64.deb\n \n sudo apt-key add /var/cuda-repo-debian10-11-3-local/7fa2af80.pub\n \n sudo add-apt-repository contrib\n \n sudo apt-get update\n \n sudo apt-get -y install cuda\n \n ```"
   },
   {
      "x": "FileNotFoundError: [Errno 2] No such file or directory when putting tensor in multiprocessing queue",
      "z": "Seems that you are not following the best practices of sending Tensor in Queue. In short, you need to setup an event to ask sub process to wait for main process to read the queue.\n \n \n \n Please refer to the following link for more information:\n \n https://discuss.pytorch.org/t/tensors-as-items-in-multiprocessing-queue/411\n \n \n \n And the following is the sample code I modified, which works well:\n \n \n \n  from torch import multiprocessing as mp\n \n  import torch\n \n  \n \n  def f(q, evt):\n \n  x = torch.rand(5, 5)\n \n  q.put(x) # <---- Change this to `q.put(x.numpy())` to fix\n \n  evt.wait()\n \n  # q.close()\n \n  # q.join_thread()\n \n  \n \n  \n \n  if __name__ == '__main__':\n \n  mp.set_start_method('spawn')\n \n  nworkers = 5\n \n  \n \n  evt = mp.Event()\n \n  \n \n  q = mp.Queue()\n \n  workers = [mp.Process(target=f, args=(q,evt,)) for _ in range(nworkers)]\n \n  [w.start() for w in workers]\n \n  \n \n  for _ in range(nworkers):\n \n  x = q.get()\n \n  print(x)\n \n  \n \n  evt.set()\n \n  \n \n  [w.join() for w in workers]",
      "y": "Seems that you are not following the best practices of sending Tensor in Queue. In short, you need to setup an event to ask sub process to wait for main process to read the queue.\n \n \n \n Please refer to the following link for more information:\n \n https://discuss.pytorch.org/t/tensors-as-items-in-multiprocessing-queue/411\n \n \n \n And the following is the sample code I modified, which works well:\n \n \n \n  from torch import multiprocessing as mp\n \n  import torch\n \n  \n \n  def f(q, evt):\n \n  x = torch.rand(5, 5)\n \n  q.put(x) # <---- Change this to `q.put(x.numpy())` to fix\n \n  evt.wait()\n \n  # q.close()\n \n  # q.join_thread()\n \n  \n \n  \n \n  if __name__ == '__main__':\n \n  mp.set_start_method('spawn')\n \n  nworkers = 5\n \n  \n \n  evt = mp.Event()\n \n  \n \n  q = mp.Queue()\n \n  workers = [mp.Process(target=f, args=(q,evt,)) for _ in range(nworkers)]\n \n  [w.start() for w in workers]\n \n  \n \n  for _ in range(nworkers):\n \n  x = q.get()\n \n  print(x)\n \n  \n \n  evt.set()\n \n  \n \n  [w.join() for w in workers]"
   },
   {
      "x": "DISABLED test_ddp_model_diff_across_ranks (__main__.TestDistBackendWithFork)",
      "z": "@rohan-varma",
      "y": "@rohan-varma"
   },
   {
      "x": "Windows binary wheels are broken on master",
      "z": "Failures started with the inclusion of `pyproject.toml`\n \n \n \n ![image](https://user-images.githubusercontent.com/1700823/123308102-1e38de80-d4d8-11eb-85cb-1385ab010698.png)",
      "y": "Failures started with the inclusion of `pyproject.toml`\n \n \n \n ![image](https://user-images.githubusercontent.com/1700823/123308102-1e38de80-d4d8-11eb-85cb-1385ab010698.png)"
   },
   {
      "x": "torch.nn.utils.clip_grad_norm_: bad GPU utilization due to GPU-data-dependent control-flow",
      "z": "It should be possible to get rid of `clip_coef` synchronization either like you suggest, or creating some custom op that would skip pointless multiplication.",
      "y": "It should be possible to get rid of `clip_coef` synchronization either like you suggest, or creating some custom op that would skip pointless multiplication."
   },
   {
      "x": "`TensorPipeJitRpcTestWithSpawn.test_call_fork_in_jit_with_profiling` is failing on the master branch",
      "z": "Thanks for letting us know! The engineers who maintain these tests are looking into the failure, so I'm going to close the issue for now",
      "y": "Thanks for letting us know! The engineers who maintain these tests are looking into the failure, so I'm going to close the issue for now"
   },
   {
      "x": "torch.distributed runs under INFO log level in 1.9.0",
      "z": "Apologies for the many mishaps with `torch.distributed.run/launch` and thanks for your patience with the migration. \n \n \n \n The warning message stating the future deprecation of `--use_env` flag is definitely misleading and has been corrected in this PR: https://github.com/pytorch/pytorch/pull/60808 (we will make sure to add this into the next patch release).\n \n \n \n As for the log level. This requires a bit of code search in our internal repo. Long story is that `torch.distributed.launch` has historically not been used in our internal launches of distributed torch, but `torch.distributed.run` (formally known as `torchelastic.distributed.launch`) actually is used in production at Facebook. Since `torch.distributed.launch` is a proxy to `torch.distributed.run` we've essentially put forth the same launcher we use for FB production use-cases for our OSS customers. Since our prod stack runs off the HEAD of master, we believe that long term we can offer our OSS users a \"production\" stable version of pytorch on each release.\n \n \n \n Issue with the log level is that our internal log analyzers expect INFO level logging by default. While I do some research to see if we can make the default log level WARNING and override it to INFO for our internal production jobs, in the interim you could set the `LOGLEVEL=WARNING` env var when invoking the launcher (see: https://github.com/pytorch/pytorch/blob/6ff0002b12a07b66e65600de0028cf0ad902e503/torch/distributed/elastic/utils/logging.py#L35)\n \n \n \n ```\n \n LOGLEVEL=WARNING python -m torch.distributed.launch ...\n \n ```",
      "y": "Apologies for the many mishaps with `torch.distributed.run/launch` and thanks for your patience with the migration. \n \n \n \n The warning message stating the future deprecation of `--use_env` flag is definitely misleading and has been corrected in this PR: https://github.com/pytorch/pytorch/pull/60808 (we will make sure to add this into the next patch release).\n \n \n \n As for the log level. This requires a bit of code search in our internal repo. Long story is that `torch.distributed.launch` has historically not been used in our internal launches of distributed torch, but `torch.distributed.run` (formally known as `torchelastic.distributed.launch`) actually is used in production at Facebook. Since `torch.distributed.launch` is a proxy to `torch.distributed.run` we've essentially put forth the same launcher we use for FB production use-cases for our OSS customers. Since our prod stack runs off the HEAD of master, we believe that long term we can offer our OSS users a \"production\" stable version of pytorch on each release.\n \n \n \n Issue with the log level is that our internal log analyzers expect INFO level logging by default. While I do some research to see if we can make the default log level WARNING and override it to INFO for our internal production jobs, in the interim you could set the `LOGLEVEL=WARNING` env var when invoking the launcher (see: https://github.com/pytorch/pytorch/blob/6ff0002b12a07b66e65600de0028cf0ad902e503/torch/distributed/elastic/utils/logging.py#L35)\n \n \n \n ```\n \n LOGLEVEL=WARNING python -m torch.distributed.launch ...\n \n ```"
   },
   {
      "x": "torch.distributed.run future warning in 1.9.0",
      "z": "Thank you for the report. This warning message should have been removed. I've gone ahead and submitted a PR to address this. https://github.com/pytorch/pytorch/pull/60807",
      "y": "Thank you for the report. This warning message should have been removed. I've gone ahead and submitted a PR to address this. https://github.com/pytorch/pytorch/pull/60807"
   },
   {
      "x": "`test_reference_numerics_hard_polygamma_polygamma_n_2_cpu_bfloat16` is failing on `pytorch_linux_xenial_py3_clang5_asan_test2`",
      "z": "Should we close this issue then?\n \n We just need to make sure we don;t break master when relanding 60444 ?",
      "y": "Should we close this issue then?\n \n We just need to make sure we don;t break master when relanding 60444 ?"
   },
   {
      "x": "[autograd-custom-function] output variables' number check in backward path",
      "z": "@albanD @ezyang \n \n Thanks for your suggestions.\n \n \n \n We understood the risk of varargs in custom Function, and will follow your suggestion.\n \n \n \n Thanks.",
      "y": "\n \n We understood the risk of varargs in custom Function, and will follow your suggestion.\n"
   },
   {
      "x": "more `torch.distributed.launch` issues in 1.9.0",
      "z": "Thanks for reporting this issue! I can indeed repro this on my local dev desktop. But I have a reason to believe the issue is not in the launcher but rather somewhere in the NCCL process group. Here's the reason why: I tried running the script as a single worker without the launcher as such:\n \n \n \n ```\n \n CUDA_VISIBLE_DEVICES=0 RANK=0 WORLD_SIZE=1 MASTER_ADDR=localhost MASTER_PORT=8081 python test.py --local_rank 0\n \n ```\n \n \n \n and get \n \n ```\n \n [W ProcessGroupNCCL.cpp:1569] Rank 0 using best-guess GPU 0 to perform barrier as devices used by this process are currently unknown. This can potentially cause a hang if this rank to GPU mapping is incorrect.Specify device_ids in barrier() to force use of a particular device.\n \n ```\n \n I'll investigate further and report back. In the meanwhile please do not hesitate to point out glaring misses on my hypothesis or my repro steps!",
      "y": "Thanks for reporting this issue! I can indeed repro this on my local dev desktop. But I have a reason to believe the issue is not in the launcher but rather somewhere in the NCCL process group. Here's the reason why: I tried running the script as a single worker without the launcher as such:\n \n \n \n ```\n \n CUDA_VISIBLE_DEVICES=0 RANK=0 WORLD_SIZE=1 MASTER_ADDR=localhost MASTER_PORT=8081 python test.py --local_rank 0\n \n ```\n \n \n \n and get \n \n ```\n \n [W ProcessGroupNCCL.cpp:1569] Rank 0 using best-guess GPU 0 to perform barrier as devices used by this process are currently unknown. This can potentially cause a hang if this rank to GPU mapping is incorrect.Specify device_ids in barrier() to force use of a particular device.\n \n ```\n \n I'll investigate further and report back. In the meanwhile please do not hesitate to point out glaring misses on my hypothesis or my repro steps!"
   },
   {
      "x": "Exception when DataLoader's num_workers parameter is specified",
      "z": "Well it appeared again and I believe it's PyCharm problem as I could not reproduce it elsewhere. Putting `del dataiter` at the end of the code seems to solve it.",
      "y": "Well it appeared again and I believe it's PyCharm problem as I could not reproduce it elsewhere. Putting `del dataiter` at the end of the code seems to solve it."
   },
   {
      "x": "[caffe2]Compiled error with CUDA9.0",
      "z": "It's work for me\n \n  \n \n Eigen with CUDA9: fatal error\u00ef\u00bc\u0161math_functions.hpp\u00ef\u00bc\u0161No such file or directory.\n \n \n \n Was fixed in eigen at https://bitbucket.org/eigen/eigen/commits/034b6c3e101792a3cc3ccabd9bfaddcabe85bb58?at=default",
      "y": "It's work for me\n \n  \n \n Eigen with CUDA9: fatal error\u00ef\u00bc\u0161math_functions.hpp\u00ef\u00bc\u0161No such file or directory.\n \n \n \n Was fixed in eigen at https://bitbucket.org/eigen/eigen/commits/034b6c3e101792a3cc3ccabd9bfaddcabe85bb58?at=default"
   },
   {
      "x": "Pytorch in java",
      "z": "Hello. Just to clarify things, there is no plan to add Java support (not Android only) to pytorch in a near future correct? I am considering switching from TF to pytorch but the lack of Java support is currently a limitation in my case.",
      "y": "there is no plan to add Java support (not Android only) to pytorch in a near future correct? I am considering switching from TF to pytorch but the lack of Java support is currently a limitation in my case."
   },
   {
      "x": "[feature request] smarter module matching for model.load_state_dict()",
      "z": "I think that's a bit too automagical, will regularly guess wrong, and you won't even realize because debugging ML models is almost impossible. We haven't implemented any kind of auto-matching for state dicts on purpose. If you have multiple models, you should know how they are structured, and you should implement the matching yourself.",
      "y": "I think that's a bit too automagical, will regularly guess wrong, and you won't even realize because debugging ML models is almost impossible. We haven't implemented any kind of auto-matching for state dicts on purpose. If you have multiple models, you should know how they are structured, and you should implement the matching yourself."
   },
   {
      "x": "compile from source error",
      "z": "You have `WITH_DISTRIBUTED_MW` enabled, but this is not a flag we support at the moment. Disable it and it should build just fine.",
      "y": "You have `WITH_DISTRIBUTED_MW` enabled, but this is not a flag we support at the moment. Disable it and it should build just fine."
   },
   {
      "x": "[pytorch] Incorrect error message in NLL_Loss",
      "z": "Closed by #6617. Thanks for the report, @pilou-K75VJ!",
      "y": "Closed by #6617. "
   },
   {
      "x": "glibc error while importing torch",
      "z": "I am also having this issue. I know that the version of GLIBC in my conda environment is up to date (version 2.55) but pytorch seems to be using the global GLIBC on my machine (version 2.12). how can I configure pytorch so that it uses the correct GLIBC installation?",
      "y": "I am also having this issue. I know that the version of GLIBC in my conda environment is up to date (version 2.55) but pytorch seems to be using the global GLIBC on my machine (version 2.12). how can I configure pytorch so that it uses the correct GLIBC installation?"
   },
   {
      "x": "How to export pytorch to ONNX alternative forward path",
      "z": "You could try making a module that wraps the module, but its forward calls incremental_forward instead.",
      "y": "You could try making a module that wraps the module, but its forward calls incremental_forward instead."
   },
   {
      "x": "Seg Fault when using more than one layer in torch.nn.GRU",
      "z": "@Wahou thank you for your report and the reproduction script. I have reproduced the segfault on 0.3.1 and on master.",
      "y": " I have reproduced the segfault on 0.3.1 and on master."
   },
   {
      "x": "[Caffe2] Windows CI not building with CUDA enabled",
      "z": "Fixed https://ci.pytorch.org/jenkins/job/caffe2-builds/job/py2-cuda9.0-cudnn7-windows-build/557/console now builds with CUDA",
      "y": "Fixed https://ci.pytorch.org/jenkins/job/caffe2-builds/job/py2-cuda9.0-cudnn7-windows-build/557/console now builds with CUDA"
   },
   {
      "x": "libcaffe2.a for Android is too large",
      "z": "It doesn't make much sense to look at the size of `libcaffe2.a`, because it is just a collection of object files. They contain duplicates of the same symbols, and much debug information. The size you should be looking it is increase in the size of your shared library after you link `libcaffe2.a`: the linker will remove duplicate symbols, and can strip debug symbols.",
      "y": "It doesn't make much sense to look at the size of `libcaffe2.a`, because it is just a collection of object files. They contain duplicates of the same symbols, and much debug information. The size you should be looking it is increase in the size of your shared library after you link `libcaffe2.a`: the linker will remove duplicate symbols, and can strip debug symbols."
   },
   {
      "x": "Same program much slower in 0.4.0 than in 0.3.0",
      "z": "@fmassa not any more. it seems roughly at the same speed (not 100% sure cuz of randomness in the program). Thanks very much!",
      "y": "it seems roughly at the same speed (not 100% sure cuz of randomness in the program). Thanks very much!"
   },
   {
      "x": "`torch.Tensor.new_empty((3, 2))` gives weird error",
      "z": "Right now, it's\n \n ```\n \n TypeError: descriptor 'new' requires a 'torch._C._TensorBase' object but received a 'tuple'\n \n ```\n \n This is what I think it should be:\n \n TypeError: descriptor 'new' requires a tensor but received a 'tuple'.\n \n \n \n Or even better:\n \n TypeError: descriptor 'new': Incorrect argument #1: expected a tensor but received a 'tuple'.",
      "y": "Right now, it's\n \n ```\n \n TypeError: descriptor 'new' requires a 'torch._C._TensorBase' object but received a 'tuple'\n \n ```\n \n This is what I think it should be:\n \n TypeError: descriptor 'new' requires a tensor but received a 'tuple'.\n \n \n \n Or even better:\n \n TypeError: descriptor 'new': Incorrect argument #1: expected a tensor but received a 'tuple'."
   },
   {
      "x": "dataloader issue python3.6",
      "z": "I met the same problem with pytorch docker image provided by NVIDIA, by setting `--ipc=host` in the container starting script the problem is solved.",
      "y": "I met the same problem with pytorch docker image provided by NVIDIA, by setting `--ipc=host` in the container starting script the problem is solved."
   },
   {
      "x": "in-place operation error thrown after mutation instead of before",
      "z": "It can, but it would work differently than all other methods and would have to be moved to C. Also, we would need to go over all of our functions and make sure it is called before the tensors are actually mutated. But yes, as long as we do it this way it should actually be possible to raise errors before mutation",
      "y": "It can, but it would work differently than all other methods and would have to be moved to C. Also, we would need to go over all of our functions and make sure it is called before the tensors are actually mutated. But yes, as long as we do it this way it should actually be possible to raise errors before mutation"
   },
   {
      "x": "`parameters()` should be a list, not an iterator",
      "z": "I don't think I agree. A lot of things like this are generators/iterators in Python3 (think `map`, `filter`, `zip`, `range`, ...) and you can shoot yourself in the same way. If you want to iterate over the thing multiple times just do `list(model.parameters())` and if you just want the first one do `next(model.parameters())` (which is also going to be much faster, because it won't iterate over all of your model at all).",
      "y": "I don't think I agree. A lot of things like this are generators/iterators in Python3 (think `map`, `filter`, `zip`, `range`, ...) and you can shoot yourself in the same way. If you want to iterate over the thing multiple times just do `list(model.parameters())` and if you just want the first one do `next(model.parameters())` (which is also going to be much faster, because it won't iterate over all of your model at all)."
   },
   {
      "x": "Warning when compiling on ppc64le",
      "z": "I found the line responsible line\n \n ```\n \n static inline __host__ __device__ char max() { return CHAR_MAX; }\n \n ```\n \n https://github.com/torch/cutorch/blob/5e9d86cb982a6048d3077aeb0e0cee19847b4c08/lib/THC/THCNumerics.cuh#L38\n \n \n \n It is (relatively) harmless I would say.",
      "y": "I found the line responsible line\n \n ```\n \n static inline __host__ __device__ char max() { return CHAR_MAX; }\n \n ```\n \n https://github.com/torch/cutorch/blob/5e9d86cb982a6048d3077aeb0e0cee19847b4c08/lib/THC/THCNumerics.cuh#L38\n \n \n \n It is (relatively) harmless I would say."
   },
   {
      "x": "DataLoader and HalfTensors",
      "z": "I'm not sure why this issue is closed. I fully understand that it makes sense in the context of data loading. Creating a shared storage is not math.",
      "y": "I'm not sure why this issue is closed. I fully understand that it makes sense in the context of data loading. Creating a shared storage is not math."
   },
   {
      "x": "Add compiled CUDA version in torch.version",
      "z": "Makes sense. Let's do it",
      "y": "Makes sense. Let's do it"
   },
   {
      "x": "Can't compile with gcc 6.4 and cuda 9",
      "z": "this is likely fixes with\n \n python setup.py clean",
      "y": "this is likely fixes with\n \n python setup.py clean"
   },
   {
      "x": "broadcasting inconsistency?",
      "z": "i meant the following scenario:\n \n \n \n loss = torch.mean((x - y) ** 2)\n \n \n \n loss is a scalar regardless of whether (x-y).size() is (10,10) or (10,1). this makes debugging pretty much impossible :(",
      "y": "i meant the following scenario:\n \n \n \n loss = torch.mean((x - y) ** 2)\n \n \n \n loss is a scalar regardless of whether (x-y).size() is (10,10) or (10,1). this makes debugging pretty much impossible :("
   },
   {
      "x": "DataParallel Gather works only with iterable outputs",
      "z": "Has anyone ever taken a look at this? Thanks!",
      "y": "Has anyone ever taken a look at this? Thanks!"
   },
   {
      "x": "Trying to do advanced indexing using a Variable causes a hang",
      "z": "Hi @futurulus - would it be possible for you to try this on `master`? I just built a clean copy of PyTorch and the code snippet succeeds for me. You may not have a version of PyTorch that includes https://github.com/pytorch/pytorch/pull/2590, which addresses this issue.",
      "y": " try this on `master`? I just built a clean copy of PyTorch and the code snippet succeeds for me. You may not have a version of PyTorch that includes https://github.com/pytorch/pytorch/pull/2590, which addresses this issue."
   },
   {
      "x": "SVD MAGMA gesdd : the updating process of SBDSDC did not converge",
      "z": "Ran into the same issue. This seems to be a feature of underlying algorithm (GESDD) and can be fixed by improving condition number to be better than 8000. GESVD seems to not have this problem, so if you care about tiny singular values, a workaround is to use scipy with lapack_driver `gesvd` \n \n https://github.com/pytorch/pytorch/issues/25978",
      "y": "Ran into the same issue. This seems to be a feature of underlying algorithm (GESDD) and can be fixed by improving condition number to be better than 8000. GESVD seems to not have this problem, so if you care about tiny singular values, a workaround is to use scipy with lapack_driver `gesvd` \n \n https://github.com/pytorch/pytorch/issues/25978"
   },
   {
      "x": "CUDA initialization hangs on CUDA 9",
      "z": "Magma is not used to boost the performance. It's only that without it some linear algebra functions (linear system solvers, Cholesky decomposition, and similar) will not work. You won't find any difference unless you use those.",
      "y": "Magma is not used to boost the performance. It's only that without it some linear algebra functions (linear system solvers, Cholesky decomposition, and similar) will not work. You won't find any difference unless you use those."
   },
   {
      "x": "[windows] driver shut down",
      "z": "@peterjc123 , I noticed I have installed two different packages(0.1.12 and 0.2.1) . I deleted and uninstalled all (Anaconda + pycharm+pytorch) and then reinstalled them(Python 3.5.2+pytorch 0.1.12). And I think this problem has something to do with multiprocess. So I set the worker 0(I find the training becomes a little slower). For now, everything works well. \n \n Thanks!",
      "y": "I noticed I have installed two different packages(0.1.12 and 0.2.1) . I deleted and uninstalled all (Anaconda + pycharm+pytorch) and then reinstalled them(Python 3.5.2+pytorch 0.1.12). And I think this problem has something to do with multiprocess. So I set the worker 0(I find the training becomes a little slower). For now, everything works well. \n \n Thanks!"
   },
   {
      "x": "Reliably repeating pytorch system crash/reboot when using imagenet examples",
      "z": "Had the same issue with _GTX1070_ but reboots were not random. \n \n I had a code that was able to make my PC reboot every time i run it after at most 1 epoch. \n \n At first i thought it can be PSU since mine has only 500W. However after closer investigation and even setting max power consumption to lower values with `nvidia-smi` i realized the issue is somewhere else. \n \n It was not an overheating problem as well so i started to think that it might be because of _I7-7820x_ Turbo mode. After disabling Turbo mode in BIOS settings of my _Asus X299-A_ and changing Ubuntu's configuration as stated [here](https://askubuntu.com/questions/619875/disabling-intel-turbo-boost-in-ubuntu/620114) the issue seems to be gone. \n \n \n \n What did **NOT** work:\n \n - Changing `pin_memory` for dataloaders.\n \n - Playing with batch size.\n \n - Increasing system shared memory limits.\n \n - Setting `nvidia-smi -pl 150` out of 195 possible for my system.\n \n \n \n Not sure if this is related to native BIOS issues. I am running 1203 version while the latest is 3 releases ahead -- 1503 and they put \n \n \n \n > improved stability\n \n \n \n  into the description of each of those 3. [Asus X299-A BIOS versions](https://www.asus.com/us/Motherboards/PRIME-X299-A/HelpDesk_BIOS/) One of those releases had also \n \n \n \n > Updated Intel CPU microcode.\n \n \n \n So there is a chance this is fixed.",
      "y": "Had the same issue with _GTX1070_ but reboots were not random. \n \n I had a code that was able to make my PC reboot every time i run it after at most 1 epoch. \n \n At first i thought it can be PSU since mine has only 500W. However after closer investigation and even setting max power consumption to lower values with `nvidia-smi` i realized the issue is somewhere else. \n \n It was not an overheating problem as well so i started to think that it might be because of _I7-7820x_ Turbo mode. After disabling Turbo mode in BIOS settings of my _Asus X299-A_ and changing Ubuntu's configuration as stated [here](https://askubuntu.com/questions/619875/disabling-intel-turbo-boost-in-ubuntu/620114) the issue seems to be gone. \n \n \n \n What did **NOT** work:\n \n - Changing `pin_memory` for dataloaders.\n \n - Playing with batch size.\n \n - Increasing system shared memory limits.\n \n - Setting `nvidia-smi -pl 150` out of 195 possible for my system.\n \n \n \n Not sure if this is related to native BIOS issues. I am running 1203 version while the latest is 3 releases ahead -- 1503 and they put \n \n \n \n > improved stability\n \n \n \n  into the description of each of those 3. [Asus X299-A BIOS versions](https://www.asus.com/us/Motherboards/PRIME-X299-A/HelpDesk_BIOS/) One of those releases had also \n \n \n \n > Updated Intel CPU microcode.\n "
   },
   {
      "x": "Function request: np.isin",
      "z": "Sure! Also, instead of implementing `in1d`, I think we should implement the more general `isin`, as recommended by [the numpy documentation](https://docs.scipy.org/doc/numpy/reference/generated/numpy.in1d.html)\n \n Here is a slightly more runtime efficient (but memory-hungry and suboptimal) implementation (requires PyTorch compiled from master):\n \n ```python\n \n def isin(ar1, ar2):\n \n  return (ar1[..., None] == ar2).any(-1)\n \n ```",
      "y": "Sure! Also, instead of implementing `in1d`, I think we should implement the more general `isin`, as recommended by [the numpy documentation](https://docs.scipy.org/doc/numpy/reference/generated/numpy.in1d.html)\n \n Here is a slightly more runtime efficient (but memory-hungry and suboptimal) implementation (requires PyTorch compiled from master):\n \n ```python\n \n def isin(ar1, ar2):\n \n  return (ar1[..., None] == ar2).any(-1)\n \n ```"
   },
   {
      "x": "Feature Request: Scheduler should be able to load and save state dictionaries",
      "z": "Definitely a +1 on this one from me.\n \n \n \n However, a simple workaround might be to simply keep track of the epoch index during training (as many probably do anyway). If training is halted only to be resumed later, the lr_scheduler can be loaded into its previous state simply by reinstatiating it and then setting the last_epoch property. \n \n \n \n It appears that `self.last_epoch` is the only \"state\" of most learning rate schedulers, and setting the property should correctly restore the learning rate scheduler to its previous state.\n \n \n \n ```\n \n last_epoch_trained_upon = 42\n \n lr_scheduler = optim.ExponentialLRS(optimizer, gamma=0.99)\n \n lr_scheduler.last_epoch = last_epoch_trained_upon\n \n ```",
      "y": "Definitely a +1 on this one from me.\n \n \n \n However, a simple workaround might be to simply keep track of the epoch index during training (as many probably do anyway). If training is halted only to be resumed later, the lr_scheduler can be loaded into its previous state simply by reinstatiating it and then setting the last_epoch property. \n \n \n \n It appears that `self.last_epoch` is the only \"state\" of most learning rate schedulers, and setting the property should correctly restore the learning rate scheduler to its previous state.\n \n \n \n ```\n \n last_epoch_trained_upon = 42\n \n lr_scheduler = optim.ExponentialLRS(optimizer, gamma=0.99)\n \n lr_scheduler.last_epoch = last_epoch_trained_upon\n \n ```"
   },
   {
      "x": "JIT scripted indexing with ellipsis and None is not working correctly",
      "z": "@One-sixth Thank you for reporting this and providing a concise example. That really helps a lot tracking down these bugs!",
      "y": "@One-sixth Thank you for reporting this and providing a concise example. That really helps a lot tracking down these bugs!"
   },
   {
      "x": "libtorch does not initialize OpenMP/MKL by default",
      "z": "I build the C++ API example as the tutorials on website, using cmake to build.\n \n CMakeLists:\n \n ```\n \n cmake_minimum_required(VERSION 3.0 FATAL_ERROR)\n \n project(pytorch-cpp-example)\n \n find_package(Torch REQUIRED)\n \n add_executable(pytorch-cpp-example pytorch-cpp-example.cpp)\n \n target_link_libraries(pytorch-cpp-example \"${TORCH_LIBRARIES}\")\n \n set_property(TARGET pytorch-cpp-example PROPERTY CXX_STANDARD 11)\n \n ```\n \n I get libtorch from url on website : \n \n ```\n \n wget https://download.pytorch.org/libtorch/cpu/libtorch-shared-with-deps-latest.zip\n \n ```\n \n I install pytorch using conda:\n \n ```\n \n conda install pytorch-cpu torchvision-cpu -c pytorch\n \n ```\n \n \n \n Both are CPU-only.\n \n \n \n How to rerun the benchmark with OMP_NUM_THREADS=1? I'm new to pytorch so I don't know much about running parameters.",
      "y": "I build the C++ API example as the tutorials on website, using cmake to build.\n \n CMakeLists:\n \n ```\n \n cmake_minimum_required(VERSION 3.0 FATAL_ERROR)\n \n project(pytorch-cpp-example)\n \n find_package(Torch REQUIRED)\n \n add_executable(pytorch-cpp-example pytorch-cpp-example.cpp)\n \n target_link_libraries(pytorch-cpp-example \"${TORCH_LIBRARIES}\")\n \n set_property(TARGET pytorch-cpp-example PROPERTY CXX_STANDARD 11)\n \n ```\n \n I get libtorch from url on website : \n \n ```\n \n wget https://download.pytorch.org/libtorch/cpu/libtorch-shared-with-deps-latest.zip\n \n ```\n \n I install pytorch using conda:\n \n ```\n \n conda install pytorch-cpu torchvision-cpu -c pytorch\n \n ```\n \n \n \n Both are CPU-only.\n \n \n \n How to rerun the benchmark with OMP_NUM_THREADS=1? I'm new to pytorch so I don't know much about running parameters."
   },
   {
      "x": "test_namedtuple_return is flakey",
      "z": "@zou3519 Yes, this test was added by me, I will take a look.",
      "y": "@zou3519 Yes, this test was added by me, I will take a look."
   },
   {
      "x": "In-place modification with double slicing (using boolean mask)",
      "z": "This behavior is expected and is the same as in numpy.\n \n \n \n There are a few subtleties in here:\n \n - indexing with a boolean tensor always returns a new tensor, and not a view to the existing tensor\n \n - this means that operations on the result of boolean indexing do not modify the indexed tensor\n \n ```python\n \n a = torch.randn(5)\n \n b = a[a > 0]\n \n b.fill_(0) # doesn't modify `a`\n \n # BUT\n \n a[a > 0] = 0 # changes `a`\n \n ```\n \n Now, why does `a[a > 0] = 0` modify `a`? Because this dispatches to `__setitem__` internally, which calls the appropriate masking functions.\n \n \n \n Now, when you do `a[a > 0.5][a < -0.5] = 0`, what this is in fact doing is that it is first creating a new (temporary) tensor `a[a > 0.5]`, and then on this temporary tensor it applies `t[a < -0.5] = 0` by calling into `__setitem__`. Which means that the result of the operation gets discarded with the temporary tensor.\n \n \n \n Now you can say: but why does `x[0][1] = 5` works then? Because `x[0]` returns a view to the underlying tensor, which shares storage.\n \n \n \n ```python\n \n a = torch.rand(5, 5)\n \n b = a[0]\n \n b.fill_(0)\n \n print(a) # modified!\n \n ```\n \n \n \n If you want to compose together boolean indexing, you should first compose the boolean mask (using `&` and `|`), and then apply the indexing. Also, using `torch.where` can be helpful here.\n \n \n \n ---\n \n \n \n Given that this is not a bug and the behavior is expected and follow numpy semantics, I'm closing this issue, but please feel free to reopen it if you feel otherwise.",
      "y": "This behavior is expected and is the same as in numpy.\n \n \n \n There are a few subtleties in here:\n \n - indexing with a boolean tensor always returns a new tensor, and not a view to the existing tensor\n \n - this means that operations on the result of boolean indexing do not modify the indexed tensor\n \n ```python\n \n a = torch.randn(5)\n \n b = a[a > 0]\n \n b.fill_(0) # doesn't modify `a`\n \n # BUT\n \n a[a > 0] = 0 # changes `a`\n \n ```\n \n Now, why does `a[a > 0] = 0` modify `a`? Because this dispatches to `__setitem__` internally, which calls the appropriate masking functions.\n \n \n \n Now, when you do `a[a > 0.5][a < -0.5] = 0`, what this is in fact doing is that it is first creating a new (temporary) tensor `a[a > 0.5]`, and then on this temporary tensor it applies `t[a < -0.5] = 0` by calling into `__setitem__`. Which means that the result of the operation gets discarded with the temporary tensor.\n \n \n \n Now you can say: but why does `x[0][1] = 5` works then? Because `x[0]` returns a view to the underlying tensor, which shares storage.\n \n \n \n ```python\n \n a = torch.rand(5, 5)\n \n b = a[0]\n \n b.fill_(0)\n \n print(a) # modified!\n \n ```\n \n \n \n If you want to compose together boolean indexing, you should first compose the boolean mask (using `&` and `|`), and then apply the indexing. Also, using `torch.where` can be helpful here.\n \n \n \n ---\n \n \n \n Given that this is not a bug and the behavior is expected and follow numpy semantics, I'm closing this issue, but please feel free to reopen it if you feel otherwise."
   },
   {
      "x": "cuDNN arch mismatch - software or hardware error?",
      "z": "Wait, I'll fix it in the related code. Looks like we can do better in choosing the right cudnn library during runtime.",
      "y": "Wait, I'll fix it in the related code. Looks like we can do better in choosing the right cudnn library during runtime."
   },
   {
      "x": "[jit] add support for tuple unpacking in for loops",
      "z": "@wanchaol is this case going to be supported with your latest `Iterable` changes?",
      "y": "is this case going to be supported with your latest `Iterable` changes?"
   },
   {
      "x": "Cumulative Maximum",
      "z": "@pandeykartikey I think you can, last message from previous worker was in November, it is unlikely that he is still working",
      "y": "I think you can, last message from previous worker was in November, it is unlikely that he is still working"
   },
   {
      "x": "Problem running custom operator test, pytorch 1.1.0",
      "z": "As libtorch for cuda 10.0 has just a nightly build, I also tried installing torch-nightly (1.1.0.dev20190507) (via pip), and again save and load, but the same error exist. It seems that save has changed the way it creates .pt file, but load is not aligned with it, as load can load models which are saved by Pytorch 1.0 and not the ones saved by 1.1.0.\n \n \n \n gdb gives the same stack trace:\n \n ```bash\n \n Reading symbols from ./test_custom_ops...(no debugging symbols found)...done.\n \n (gdb) r\n \n Starting program: /home/shakiba/Programs/pytorch/test/custom_operator/build/test_custom_ops ../test_op_nighty.pt\n \n [Thread debugging using libthread_db enabled]\n \n Using host libthread_db library \"/lib/x86_64-linux-gnu/libthread_db.so.1\".\n \n terminate called after throwing an instance of 'c10::Error'\n \n  what(): isGenericList() ASSERT FAILED at /pytorch/aten/src/ATen/core/ivalue.h:385, please report a bug to PyTorch. (toGenericList at /pytorch/aten/src/ATen/core/ivalue.h:385)\n \n frame #0: std::function<std::string ()>::operator()() const + 0x11 (0x7fffb3450441 in /home/shakiba/Downloads/libtorch/lib/libc10.so)\n \n frame #1: c10::Error::Error(c10::SourceLocation, std::string const&) + 0x2a (0x7fffb344fd7a in /home/shakiba/Downloads/libtorch/lib/libc10.so)\n \n frame #2: <unknown function> + 0x9728f2 (0x7ffff6e8b8f2 in /home/shakiba/Downloads/libtorch/lib/libtorch.so.1)\n \n frame #3: torch::jit::Unpickler::parse_ivalue_list() + 0x41 (0x7ffff6e88f31 in /home/shakiba/Downloads/libtorch/lib/libtorch.so.1)\n \n frame #4: <unknown function> + 0xa6f91b (0x7ffff6f8891b in /home/shakiba/Downloads/libtorch/lib/libtorch.so.1)\n \n frame #5: torch::jit::load(std::unique_ptr<caffe2::serialize::ReadAdapterInterface, std::default_delete<caffe2::serialize::ReadAdapterInterface> >, c10::optional<c10::Device>, std::unordered_map<std::string, std::string, std::hash<std::string>, std::equal_to<std::string>, std::allocator<std::pair<std::string const, std::string> > >&) + 0x10d (0x7ffff6f89f5d in /home/shakiba/Downloads/libtorch/lib/libtorch.so.1)\n \n frame #6: torch::jit::load(std::string const&, c10::optional<c10::Device>, std::unordered_map<std::string, std::string, std::hash<std::string>, std::equal_to<std::string>, std::allocator<std::pair<std::string const, std::string> > >&) + 0x68 (0x7ffff6f8a088 in /home/shakiba/Downloads/libtorch/lib/libtorch.so.1)\n \n frame #7: load_serialized_module_with_custom_op_and_execute(std::string const&) + 0x58 (0x44413f in /home/shakiba/Programs/pytorch/test/custom_operator/build/test_custom_ops)\n \n frame #8: main + 0x85 (0x445787 in /home/shakiba/Programs/pytorch/test/custom_operator/build/test_custom_ops)\n \n frame #9: __libc_start_main + 0xf0 (0x7fffb2ae5830 in /lib/x86_64-linux-gnu/libc.so.6)\n \n frame #10: _start + 0x29 (0x442969 in /home/shakiba/Programs/pytorch/test/custom_operator/build/test_custom_ops)\n \n \n \n \n \n Program received signal SIGABRT, Aborted.\n \n 0x00007fffb2afa428 in __GI_raise (sig=sig@entry=6) at ../sysdeps/unix/sysv/linux/raise.c:54\n \n 54 ../sysdeps/unix/sysv/linux/raise.c: No such file or directory.\n \n ```\n \n \n \n p.s: I updated my post (cmake should have -DCMAKE_PRIFIX_PATH=/path/to/libtorch/)",
      "y": "As libtorch for cuda 10.0 has just a nightly build, I also tried installing torch-nightly (1.1.0.dev20190507) (via pip), and again save and load, but the same error exist. It seems that save has changed the way it creates .pt file, but load is not aligned with it, as load can load models which are saved by Pytorch 1.0 and not the ones saved by 1.1.0.\n \n \n \n gdb gives the same stack trace:\n \n ```bash\n \n Reading symbols from ./test_custom_ops...(no debugging symbols found)...done.\n \n (gdb) r\n \n Starting program: /home/shakiba/Programs/pytorch/test/custom_operator/build/test_custom_ops ../test_op_nighty.pt\n \n [Thread debugging using libthread_db enabled]\n \n Using host libthread_db library \"/lib/x86_64-linux-gnu/libthread_db.so.1\".\n \n terminate called after throwing an instance of 'c10::Error'\n \n  what(): isGenericList() ASSERT FAILED at /pytorch/aten/src/ATen/core/ivalue.h:385, please report a bug to PyTorch. (toGenericList at /pytorch/aten/src/ATen/core/ivalue.h:385)\n \n frame #0: std::function<std::string ()>::operator()() const + 0x11 (0x7fffb3450441 in /home/shakiba/Downloads/libtorch/lib/libc10.so)\n \n frame #1: c10::Error::Error(c10::SourceLocation, std::string const&) + 0x2a (0x7fffb344fd7a in /home/shakiba/Downloads/libtorch/lib/libc10.so)\n \n frame #2: <unknown function> + 0x9728f2 (0x7ffff6e8b8f2 in /home/shakiba/Downloads/libtorch/lib/libtorch.so.1)\n \n frame #3: torch::jit::Unpickler::parse_ivalue_list() + 0x41 (0x7ffff6e88f31 in /home/shakiba/Downloads/libtorch/lib/libtorch.so.1)\n \n frame #4: <unknown function> + 0xa6f91b (0x7ffff6f8891b in /home/shakiba/Downloads/libtorch/lib/libtorch.so.1)\n \n frame #5: torch::jit::load(std::unique_ptr<caffe2::serialize::ReadAdapterInterface, std::default_delete<caffe2::serialize::ReadAdapterInterface> >, c10::optional<c10::Device>, std::unordered_map<std::string, std::string, std::hash<std::string>, std::equal_to<std::string>, std::allocator<std::pair<std::string const, std::string> > >&) + 0x10d (0x7ffff6f89f5d in /home/shakiba/Downloads/libtorch/lib/libtorch.so.1)\n \n frame #6: torch::jit::load(std::string const&, c10::optional<c10::Device>, std::unordered_map<std::string, std::string, std::hash<std::string>, std::equal_to<std::string>, std::allocator<std::pair<std::string const, std::string> > >&) + 0x68 (0x7ffff6f8a088 in /home/shakiba/Downloads/libtorch/lib/libtorch.so.1)\n \n frame #7: load_serialized_module_with_custom_op_and_execute(std::string const&) + 0x58 (0x44413f in /home/shakiba/Programs/pytorch/test/custom_operator/build/test_custom_ops)\n \n frame #8: main + 0x85 (0x445787 in /home/shakiba/Programs/pytorch/test/custom_operator/build/test_custom_ops)\n \n frame #9: __libc_start_main + 0xf0 (0x7fffb2ae5830 in /lib/x86_64-linux-gnu/libc.so.6)\n \n frame #10: _start + 0x29 (0x442969 in /home/shakiba/Programs/pytorch/test/custom_operator/build/test_custom_ops)\n \n \n \n \n \n Program received signal SIGABRT, Aborted.\n \n 0x00007fffb2afa428 in __GI_raise (sig=sig@entry=6) at ../sysdeps/unix/sysv/linux/raise.c:54\n \n 54 ../sysdeps/unix/sysv/linux/raise.c: No such file or directory.\n \n ```\n \n \n \n p.s: I updated my post (cmake should have -DCMAKE_PRIFIX_PATH=/path/to/libtorch/)"
   },
   {
      "x": "C++ Can only index with tensors that are scalars (zero-dim)",
      "z": "This feature is available in libtorch v1.5. Following the docs in https://pytorch.org/cppdocs/notes/tensor_indexing.html, we can write\n \n ```python\n \n data=torch.tensor([[[1],[1]],[[1],[1]]]);\n \n index=torch.tensor([[[1],[1]],[[1],[1]]]);\n \n print(data[index])\n \n ```\n \n in C++ as\n \n ```cpp\n \n auto data = torch::tensor({{{1}, {1}}, {{1}, {1}}});\n \n auto index = torch::tensor({{{1}, {1}}, {{1}, {1}}});\n \n std::cout << data.index({index}) <<std::endl;\n \n ```",
      "y": "This feature is available in libtorch v1.5. Following the docs in https://pytorch.org/cppdocs/notes/tensor_indexing.html, we can write\n \n ```python\n \n data=torch.tensor([[[1],[1]],[[1],[1]]]);\n \n index=torch.tensor([[[1],[1]],[[1],[1]]]);\n \n print(data[index])\n \n ```\n \n in C++ as\n \n ```cpp\n \n auto data = torch::tensor({{{1}, {1}}, {{1}, {1}}});\n \n auto index = torch::tensor({{{1}, {1}}, {{1}, {1}}});\n \n std::cout << data.index({index}) <<std::endl;\n \n ```"
   },
   {
      "x": "Memory (CPU/sys) leak with custom batch norm layer",
      "z": "Yeah, that `running_mean` update is exactly your problem. You keep extending the graph.",
      "y": "Yeah, that `running_mean` update is exactly your problem. You keep extending the graph."
   },
   {
      "x": "Get people to stop improperly using AT_ASSERT",
      "z": "@Krovatkin The `ASSERT` macro, for internal implementation reasons, currently requires you use to a separate `ASSERTM` for messages.\n \n \n \n ```\n \n AT_ASSERTM(vec.size(), \"Vector size should never be equal to zero context ( node = \", *node, \" ) some more useful information \");\n \n ```\n \n \n \n I plan to eliminate this inconsistency. (I'll have to construct two strings on error case because of variadic macro comma pasting shenanigans, but that's a small price to pay).",
      "y": "@Krovatkin The `ASSERT` macro, for internal implementation reasons, currently requires you use to a separate `ASSERTM` for messages.\n \n \n \n ```\n \n AT_ASSERTM(vec.size(), \"Vector size should never be equal to zero context ( node = \", *node, \" ) some more useful information \");\n \n ```\n \n \n \n I plan to eliminate this inconsistency. (I'll have to construct two strings on error case because of variadic macro comma pasting shenanigans, but that's a small price to pay)."
   },
   {
      "x": "Softmax Docs Example Deprecated",
      "z": "Closing as the PR is merged. @vadimkantorov please make a separate issue for running examples code. (I'm not sure how it's implemented, but I guess it's manually pasted there :P",
      "y": "Closing as the PR is merged. @vadimkantorov please make a separate issue for running examples code. (I'm not sure how it's implemented, but I guess it's manually pasted there :P"
   },
   {
      "x": "pytorch1.1 training speed slower than pytorch1.0",
      "z": "It is not only `DataLoader` threads, OMP by default can set up to 24 threads per process. Having `--nproc_per_node=8` you potentially ending up with 192 threads, which is way higher than your available threads. I recommend to run everything with `OMP_NUM_THREADS=10` and `OMP_NUM_THREADS=5` accordingly and compare results.",
      "y": "It is not only `DataLoader` threads, OMP by default can set up to 24 threads per process. Having `--nproc_per_node=8` you potentially ending up with 192 threads, which is way higher than your available threads. I recommend to run everything with `OMP_NUM_THREADS=10` and `OMP_NUM_THREADS=5` accordingly and compare results."
   },
   {
      "x": "nccl runtime error: unhandled system error",
      "z": "I think I fixed the issue. It's my fault not to set `NCCL_SOCKET_IFNAME`, `MASTER_ADDR` and `MASTER_PORT` correctly. Debug with `NCCL_DEBUG=INFO` is very useful!",
      "y": "I think I fixed the issue. It's my fault not to set `NCCL_SOCKET_IFNAME`, `MASTER_ADDR` and `MASTER_PORT` correctly. Debug with `NCCL_DEBUG=INFO` is very useful!"
   },
   {
      "x": "cat()/stack() calls in JIT do not accept lists of tensors",
      "z": "@driazati can you take a look? it appears whatever type promotion we have for lists isn't working",
      "y": " it appears whatever type promotion we have for lists isn't working"
   },
   {
      "x": "Backwards hangs",
      "z": "In my case I solved it by disabling **PCIe Active State Power Management**. No more hanging and no more frenetic logging from the OS.\n \n \n \n Heres how to do it: [https://askubuntu.com/questions/863150/pcie-bus-error-severity-corrected-type-physical-layer-id-00e5receiver-id](https://askubuntu.com/questions/863150/pcie-bus-error-severity-corrected-type-physical-layer-id-00e5receiver-id)\n \n \n \n Hope it helps",
      "y": "In my case I solved it by disabling **PCIe Active State Power Management**. No more hanging and no more frenetic logging from the OS.\n \n \n \n Heres how to do it: [https://askubuntu.com/questions/863150/pcie-bus-error-severity-corrected-type-physical-layer-id-00e5receiver-id](https://askubuntu.com/questions/863150/pcie-bus-error-severity-corrected-type-physical-layer-id-00e5receiver-id)\n \n \n \n Hope it helps"
   },
   {
      "x": "Load tensor from file in C++ fails",
      "z": "It's still hidden behind a flag on the Python `torch.save` side that will eventually be flipped. Right now on nightly you can do this (copied from #22954):\n \n \n \n Python -> C++\n \n ```python\n \n import io\n \n f = io.BytesIO()\n \n torch.save(x, f, _use_new_zipfile_serialization=True)\n \n # send f wherever\n \n ```\n \n \n \n ```cpp\n \n // receive bytes in a `std::vector<char>`\n \n std::vector<char> f = get_the_bytes();\n \n torch::IValue x = torch::pickle_load(f);\n \n ```\n \n \n \n and the reverse (C++ -> Python)\n \n \n \n ```cpp\n \n torch::Tensor x = torch::ones({1, 2, 3});\n \n std::vector<char> f = torch::pickle_save(x);\n \n ```\n \n \n \n ```python\n \n x = torch.load(f)\n \n ```",
      "y": "It's still hidden behind a flag on the Python `torch.save` side that will eventually be flipped. Right now on nightly you can do this (copied from #22954):\n \n \n \n Python -> C++\n \n ```python\n \n import io\n \n f = io.BytesIO()\n \n torch.save(x, f, _use_new_zipfile_serialization=True)\n \n # send f wherever\n \n ```\n \n \n \n ```cpp\n \n // receive bytes in a `std::vector<char>`\n \n std::vector<char> f = get_the_bytes();\n \n torch::IValue x = torch::pickle_load(f);\n \n ```\n \n \n \n and the reverse (C++ -> Python)\n \n \n \n ```cpp\n \n torch::Tensor x = torch::ones({1, 2, 3});\n \n std::vector<char> f = torch::pickle_save(x);\n \n ```\n \n \n \n ```python\n \n x = torch.load(f)\n \n ```"
   },
   {
      "x": "load_state_dict: KeyError: 'unexpected key in state_dict'",
      "z": "I don't really know what's your plan. As @jekbradbury noticed, you're reinitializing the layer at every forward. You probably want to do sth like that:\n \n ```python\n \n class Maxout(nn.Module):\n \n  def __init__(self, num_pieces, input_shape):\n \n  super(Maxout, self).__init__()\n \n  self.num_pieces = num_pieces\n \n  self.linear = nn.Linear(input_shape, input_shape * num_pieces, bias=False)\n \n \n \n  def forwad(self, input):\n \n  # here you can use self.linear\n \n ```",
      "y": "I don't really know what's your plan. As @jekbradbury noticed, you're reinitializing the layer at every forward. You probably want to do sth like that:\n \n ```python\n \n class Maxout(nn.Module):\n \n  def __init__(self, num_pieces, input_shape):\n \n  super(Maxout, self).__init__()\n \n  self.num_pieces = num_pieces\n \n  self.linear = nn.Linear(input_shape, input_shape * num_pieces, bias=False)\n \n \n \n  def forwad(self, input):\n \n  # here you can use self.linear\n \n ```"
   },
   {
      "x": "pytorch does not build on ppc64le",
      "z": "@gchanan is going to look into it, we got access to minicloud.",
      "y": "@gchanan is going to look into it, we got access to minicloud."
   },
   {
      "x": "Segmentation fault (core dumped) on Ubuntu 14.04.5",
      "z": "can you do:\n \n \n \n ```\n \n gdb python\n \n ```\n \n \n \n and in gdb:\n \n \n \n ```\n \n r main.py\n \n ```\n \n \n \n And when segfault happens, you can type:\n \n \n \n ```\n \n where\n \n ```\n \n \n \n Then it will show a stack-trace. Can you paste that here. It'll be helpful to figure out the issue.\n \n \n \n Thanks.",
      "y": "can you do:\n \n \n \n ```\n \n gdb python\n \n ```\n \n \n \n and in gdb:\n \n \n \n ```\n \n r main.py\n \n ```\n \n \n \n And when segfault happens, you can type:\n \n \n \n ```\n \n where\n \n ```\n \n \n \n Then it will show a stack-trace. Can you paste that here. It'll be helpful to figure out the issue.\n \n \n \n Thanks."
   },
   {
      "x": "ImportError: libiomp5.so: cannot open shared object file",
      "z": "I got the same problem. My environment is CentOS 7, cuda 8.0, python 2.7. I installed it by pip as well.",
      "y": "I got the same problem. My environment is CentOS 7, cuda 8.0, python 2.7. I installed it by pip as well."
   },
   {
      "x": "Placeholder source builds for PyPI",
      "z": "Thanks Luke.\n \n I've done this for `pip install torch`.\n \n I'm not the owner/maintainer of pytorch, @graingert is, and he said he'll transfer it to me / give commit rights. Once he gives me those rights, will do for `pip install pytorch` too.",
      "y": "Thanks Luke.\n \n I've done this for `pip install torch`.\n \n I'm not the owner/maintainer of pytorch, @graingert is, and he said he'll transfer it to me / give commit rights. Once he gives me those rights, will do for `pip install pytorch` too."
   },
   {
      "x": "In-place sub with shared memory, wrong result",
      "z": "I think this is a duplicate of #906",
      "y": "I think this is a duplicate of #906"
   },
   {
      "x": "issue when using LSTM with Dropout",
      "z": "Could it be that you are operating with a very constrained memory? From the error messages, it looks like you are erroring out in cudnnSetDropoutDescriptor. The kernel initializing dropout states requires a lot of stack memory (grrrrrrrrr, XORWOW!), and it might return the error you are seeing if it is not able to allocate it.",
      "y": "Could it be that you are operating with a very constrained memory? From the error messages, it looks like you are erroring out in cudnnSetDropoutDescriptor. The kernel initializing dropout states requires a lot of stack memory (grrrrrrrrr, XORWOW!), and it might return the error you are seeing if it is not able to allocate it."
   },
   {
      "x": "load_lua yields \"object has no attribute 'running_var'\" for BatchNormalization",
      "z": "The whole code looks like this:\n \n \n \n Torch:\n \n ```\n \n require 'nn'\n \n require 'dpnn'\n \n require 'cunn'\n \n model = torch.load('n4.small2.v1.t7')\n \n \n \n -- Forward pass a random tensor\n \n x = torch.FloatTensor()\n \n model:forward(x:resize(1,3,96,96))\n \n \n \n torch.save('nn4.small2.v1.resaved.t7',model)\n \n ```\n \n \n \n Then in pytorch:\n \n ```\n \n from torch.utils.serialization import load_lua\n \n model = load_lua('nn4.small2.v1.resaved.t7',unknown_classes=True)\n \n ```",
      "y": "The whole code looks like this:\n \n \n \n Torch:\n \n ```\n \n require 'nn'\n \n require 'dpnn'\n \n require 'cunn'\n \n model = torch.load('n4.small2.v1.t7')\n \n \n \n -- Forward pass a random tensor\n \n x = torch.FloatTensor()\n \n model:forward(x:resize(1,3,96,96))\n \n \n \n torch.save('nn4.small2.v1.resaved.t7',model)\n \n ```\n \n \n \n Then in pytorch:\n \n ```\n \n from torch.utils.serialization import load_lua\n \n model = load_lua('nn4.small2.v1.resaved.t7',unknown_classes=True)\n \n ```"
   },
   {
      "x": "RuntimeError: cuda runtime error (2) : out of memory at /data/users/soumith/miniconda2/conda-bld/pytorch-0.1.9_1487346124464/work/torch/lib/THC/generic/THCStorage.cu:66",
      "z": "If you only consider the weights of a single Linear layer from that model. You get \n \n ```\n \n 49200^2 = 2\u00c2 420\u00c2 640\u00c2 000\n \n ```\n \n elements + each element takes 4 bytes, which gives you\n \n ```\n \n 2\u00c2 420\u00c2 640\u00c2 000 * 4 / 1024^3 = 9,01GB\n \n ```\n \n for the weights alone. Then, you need another memory chunk of this size to store gradients. You also need to save the intermediate results so you can compute the gradient.",
      "y": "If you only consider the weights of a single Linear layer from that model. You get \n \n ```\n \n 49200^2 = 2\u00c2 420\u00c2 640\u00c2 000\n \n ```\n \n elements + each element takes 4 bytes, which gives you\n \n ```\n \n 2\u00c2 420\u00c2 640\u00c2 000 * 4 / 1024^3 = 9,01GB\n \n ```\n \n for the weights alone. Then, you need another memory chunk of this size to store gradients. You also need to save the intermediate results so you can compute the gradient."
   },
   {
      "x": "Sparse and CosineLoss only support Float on CUDA",
      "z": "If nobody is working on fixing it, I'll send a PR later today",
      "y": "If nobody is working on fixing it, I'll send a PR later today"
   },
   {
      "x": "backward() in Autograd Index Function is broken when masking with ByteTensor",
      "z": "Yes, just tested with last master, it seems to have been fixed, sorry!",
      "y": "Yes, just tested with last master, it seems to have been fixed, sorry!"
   },
   {
      "x": "file_descriptor sharing strategy may be leaking FDs, resulting in DataLoader causing `RuntimeError: received 0 items of ancdata`",
      "z": "This does seem to be an issue in pytorch as best as we can tell - FDs aren't being released when they should be. As @sampathweb mentioned, it's reproducible using the above code. The workaround for now is:\n \n \n \n ```\n \n import resource\n \n rlimit = resource.getrlimit(resource.RLIMIT_NOFILE)\n \n resource.setrlimit(resource.RLIMIT_NOFILE, (2048, rlimit[1]))\n \n ```\n \n (You may need to set 2048 to a higher number.)",
      "y": "This does seem to be an issue in pytorch as best as we can tell - FDs aren't being released when they should be. As @sampathweb mentioned, it's reproducible using the above code. The workaround for now is:\n \n \n \n ```\n \n import resource\n \n rlimit = resource.getrlimit(resource.RLIMIT_NOFILE)\n \n resource.setrlimit(resource.RLIMIT_NOFILE, (2048, rlimit[1]))\n \n ```\n \n (You may need to set 2048 to a higher number.)"
   },
   {
      "x": "pytorch not respecting torch.set_num_threads",
      "z": "To answer @mattmacy 's high level question, I've bought an additional machine to setup perf-regression contbuilds, I'll get that setup in the next week or two. we will additionally be setting up OSX contbuilds as well.",
      "y": "To answer @mattmacy 's high level question, I've bought an additional machine to setup perf-regression contbuilds, I'll get that setup in the next week or two. we will additionally be setting up OSX contbuilds as well."
   },
   {
      "x": "Inconsistent argument names in Linear and Conv layers",
      "z": "It could be said that Linear layers operate on 2D Tensor of N x Channels, and that ConvXd operates on consumes a produces a set of feature maps.",
      "y": "It could be said that Linear layers operate on 2D Tensor of N x Channels, and that ConvXd operates on consumes a produces a set of feature maps."
   },
   {
      "x": "Download speed is too slow in China.",
      "z": "pip install -i https://pypi.tuna.tsinghua.edu.cn/simple [some-package]\n \n you can use it to speed up.",
      "y": "pip install -i https://pypi.tuna.tsinghua.edu.cn/simple [some-package]\n \n you can use it to speed up."
   },
   {
      "x": "ImportError: libpython3.6m.so.1.0",
      "z": "I solved the same issue by installing a `libpython3.6-dev` package on Ubuntu system.",
      "y": "I solved the same issue by installing a `libpython3.6-dev` package on Ubuntu system."
   },
   {
      "x": "[FEATURE REQUEST] Java extensions for PyTorch",
      "z": "Hi Loreto,\n \n We dont have an explicit plan on this for Java.\n \n Our razor-sharp focus as core developers is Python / C / C++.\n \n If anyone in the community will develop other language extensions we are looking forward to how it turns out.",
      "y": "\n We dont have an explicit plan on this for Java.\n \n Our razor-sharp focus as core developers is Python / C / C++.\n \n If anyone in the community will develop other language extensions we are looking forward to how it turns out."
   },
   {
      "x": "GPU goes away after an error occurs",
      "z": "Another common reason for this error could be if the number of classes in the labels does not match with the number of units in the final softmaxed Linear Layer for a classification problem. I recently encountered this accidentally.",
      "y": "Another common reason for this error could be if the number of classes in the labels does not match with the number of units in the final softmaxed Linear Layer for a classification problem. I recently encountered this accidentally."
   },
   {
      "x": "[PyTorch] Unary Operator Vectorization",
      "z": "@jamesr66a as you do this you should consider moving the CUDA implementations as well following the pattern in [BinaryOpsKernel.cu](https://github.com/pytorch/pytorch/blob/master/aten/src/ATen/native/cuda/BinaryOpsKernel.cu). My guess is that it's not much more work and will result in simpler dispatch and more opportunities to delete dead code. If it turns out to be a significant amount of extra work, don't bother. \n \n \n \n (I don't expect many perf gains for CUDA, except maybe for column-major tensors or other \"compact\" but not \"contiguous\" tensors)",
      "y": "consider moving the CUDA implementations as well following the pattern in [BinaryOpsKernel.cu](https://github.com/pytorch/pytorch/blob/master/aten/src/ATen/native/cuda/BinaryOpsKernel.cu). My guess is that it's not much more work and will result in simpler dispatch and more opportunities to delete dead code. If it turns out to be a significant amount of extra work, don't bother. \n \n \n \n (I don't expect many perf gains for CUDA, except maybe for column-major tensors or other \"compact\" but not \"contiguous\" tensors)"
   },
   {
      "x": "`torch.irfft` and `np.fft.irfft` disagree",
      "z": "https://pytorch.org/docs/master/torch.html#torch.irfft\n \n \n \n > Due to the conjugate symmetry, input do not need to contain the full complex frequency values. Roughly half of the values will be sufficient, as is the case when input is given by rfft() with rfft(signal, onesided=True). In such case, set the onesided argument of this method to True. Moreover, the original signal shape information can sometimes be lost, optionally set signal_sizes to be the size of the original signal (without the batch dimensions if in batched mode) to recover it with correct shape.",
      "y": "https://pytorch.org/docs/master/torch.html#torch.irfft\n \n \n \n > Due to the conjugate symmetry, input do not need to contain the full complex frequency values. Roughly half of the values will be sufficient, as is the case when input is given by rfft() with rfft(signal, onesided=True). In such case, set the onesided argument of this method to True. Moreover, the original signal shape information can sometimes be lost, optionally set signal_sizes to be the size of the original signal (without the batch dimensions if in batched mode) to recover it with correct shape."
   },
   {
      "x": "[feature request] torch.trapz or some other method for numerical integration similar to numpy",
      "z": "Is anyone working on this, I would be happy take this up with some guidance.",
      "y": "Is anyone working on this, I would be happy take this up with some guidance."
   },
   {
      "x": "[feature request] torch.pi",
      "z": "We generally run on less of a bar than consensus, but I opened a more general feature request (again( in #19172",
      "y": "We generally run on less of a bar than consensus, but I opened a more general feature request (again( in #19172"
   },
   {
      "x": "flat_hash_map.h error C3203 'templated_iterator' unspecialized class template",
      "z": "Would you please update your VS 2017 to the latest version? If you don't want to update, then you can try to wrap these code with `#ifndef __CUDACC__ ` and `#endif`. cc @ezyang",
      "y": "Would you please update your VS 2017 to the latest version? If you don't want to update, then you can try to wrap these code with `#ifndef __CUDACC__ ` and `#endif`. cc @ezyang"
   },
   {
      "x": "\"undefined symbol: PySlice_Unpack\" of pytorch 1.0.0 with Python 3.6.0",
      "z": "Upgrading to Python 3.6.1 was suggested in https://github.com/pytorch/pytorch/issues/14931#issuecomment-445448141.\n \n \n \n Duplicate of #14931.",
      "y": "Upgrading to Python 3.6.1 was suggested in https://github.com/pytorch/pytorch/issues/14931#issuecomment-445448141.\n \n \n \n Duplicate of #14931."
   },
   {
      "x": "Torch.tensor.sum() is not accurate enough",
      "z": "One thing that is worth investigating is why numpy, for float32 arrays, produces more accurate results than pytorch, when forcing the reduction to be float32 as well.",
      "y": "One thing that is worth investigating is why numpy, for float32 arrays, produces more accurate results than pytorch, when forcing the reduction to be float32 as well."
   },
   {
      "x": "better CUDA optimization using read-only cache and __restrict__",
      "z": "Thanks :)\n \n \n \n There was also some dead code so I'm going to clean that up too and commit in a couple days.",
      "y": "Thanks :)\n \n \n \n There was also some dead code so I'm going to clean that up too and commit in a couple days."
   },
   {
      "x": "Building from source failed",
      "z": "@soumith \n \n @ezyang\n \n @qmpzzpmq \n \n The issue is still unsolved",
      "y": "\n The issue is still unsolved"
   },
   {
      "x": "Poisson NLL loss in libtorch",
      "z": "yea you're right. just put it in aten/native",
      "y": "yea you're right. just put it in aten/native"
   },
   {
      "x": "`with torch.enable_grad` also works outside a `no_grad` context",
      "z": "\"Enables gradient calculation, if it has been disabled via `no_grad` or `torch.set_grad_enabled`\" <- I think something like that would make it clearer. But yes, we would accept a PR, thank you!",
      "y": "\"Enables gradient calculation, if it has been disabled via `no_grad` or `torch.set_grad_enabled`\" <- I think something like that would make it clearer. But yes, we would accept a PR, thank you!"
   },
   {
      "x": "Serving Trained model for pytorch",
      "z": "We run PyTorch in production at Facebook, at enormous scale, so we've actually invested a lot in production serving capabilities and will only continue to improve things. It really just depends on what your use case is.\n \n \n \n Running inference on a trained model is as simple as `module(input)`, whether that module is traced using the JIT. The TorchScript JIT is one of the most exciting new features focused on this production use case. It allows for compilation of your high-level Python program. Here's a good example of how to use the hybrid frontend of the JIT: https://pytorch.org/tutorials/beginner/hybrid_frontend/learning_hybrid_frontend_through_example_tutorial.html I also like this tutorial showing you how to take advantage of LibTorch for Python-free C++ serving: https://pytorch.org/tutorials/beginner/saving_loading_models.html \n \n \n \n Of course, you can also take advantage of ONNX export as @Amiralae suggests. This can be useful in combination with mobile runtimes or other runtimes that support ONNX on the server side, like NVIDIA's TensorRT or AWS Elastic Inference.\n \n \n \n If you want the model management features of a model server, you can get some of that from the cloud services that run PyTorch and ONNX. There are also several groups trying to build PyTorch compatible OSS model servers. As @Amiralae mentions, there's Kubeflow, which I would characterize as in development. Similarly, [MLFlow](https://mlflow.org/) is also working on a level PyTorch support within their model server. And the are PyTorch community projects, like [EuclidesDB](https://euclidesdb.readthedocs.io/en/latest/). Lots of innovation going on in open source model serving these days.\n \n \n \n Hope that helps.",
      "y": "We run PyTorch in production at Facebook, at enormous scale, so we've actually invested a lot in production serving capabilities and will only continue to improve things. It really just depends on what your use case is.\n \n \n \n Running inference on a trained model is as simple as `module(input)`, whether that module is traced using the JIT. The TorchScript JIT is one of the most exciting new features focused on this production use case. It allows for compilation of your high-level Python program. Here's a good example of how to use the hybrid frontend of the JIT: https://pytorch.org/tutorials/beginner/hybrid_frontend/learning_hybrid_frontend_through_example_tutorial.html I also like this tutorial showing you how to take advantage of LibTorch for Python-free C++ serving: https://pytorch.org/tutorials/beginner/saving_loading_models.html \n \n \n \n Of course, you can also take advantage of ONNX export as @Amiralae suggests. This can be useful in combination with mobile runtimes or other runtimes that support ONNX on the server side, like NVIDIA's TensorRT or AWS Elastic Inference.\n \n \n \n If you want the model management features of a model server, you can get some of that from the cloud services that run PyTorch and ONNX. There are also several groups trying to build PyTorch compatible OSS model servers. As @Amiralae mentions, there's Kubeflow, which I would characterize as in development. Similarly, [MLFlow](https://mlflow.org/) is also working on a level PyTorch support within their model server. And the are PyTorch community projects, like [EuclidesDB](https://euclidesdb.readthedocs.io/en/latest/). Lots of innovation going on in open source model serving these days.\n \n \n \n Hope that helps."
   },
   {
      "x": "torch::set_num_threads does not work",
      "z": "@kishwarshafin looks fine to me. we dont globally set a config -- you set the number of threads in your process' lifetime.",
      "y": "@kishwarshafin looks fine to me. we dont globally set a config -- you set the number of threads in your process' lifetime."
   },
   {
      "x": "The cuFFT plan cache is not CUDA context-aware",
      "z": "It\u00e2\u20ac\u2122s alright. I figured out a way to trigger it.",
      "y": "It\u00e2\u20ac\u2122s alright. I figured out a way to trigger it."
   },
   {
      "x": "InstanceNorm, GroupNorm doesn't crash with # of elements == 1",
      "z": "Yep the issue and proposal make sense, would be a sensible thing to catch :+1:",
      "y": "Yep the issue and proposal make sense, would be a sensible thing to catch :+1:"
   },
   {
      "x": "[CUDA] torch::stack / torch::cat fails on a single element",
      "z": "Hi, I am from @quansight team. I would like to work on this issue.",
      "y": "Hi, I am from @quansight team. I would like to work on this issue."
   },
   {
      "x": "nn.functional.binary_cross_entropy_with_logits got error when work with 'weight'",
      "z": "@ht-alchera your `weights` variable has requires_grad which is not supported: `binary_cross_entropy_with_logits` doesn't support back-propagating through the weights attribute.\n \n \n \n If you don't need the derivative w.r.t. `weights` then you can use `weights.detach()` instead of `weights`. If you need the derivative, then you'll having to implement `binary_cross_entropy_with_logits` yourself.",
      "y": "your `weights` variable has requires_grad which is not supported: `binary_cross_entropy_with_logits` doesn't support back-propagating through the weights attribute.\n \n \n \n If you don't need the derivative w.r.t. `weights` then you can use `weights.detach()` instead of `weights`. If you need the derivative, then you'll having to implement `binary_cross_entropy_with_logits` yourself."
   },
   {
      "x": "checkpoint_sequential doesn't respect nn.Sequential's behavior",
      "z": "@sublee i agree with this approach",
      "y": " i agree with this approach"
   },
   {
      "x": "DataLoader with sparse matrices adds a dimension to the batch",
      "z": "How do i establish a custom collate function? is there a way around this?\n \n I am facing a similar issue",
      "y": "How do i establish a custom collate function? is there a way around this?\n \n I am facing a similar issue"
   },
   {
      "x": "import failure (macOS)",
      "z": "My workaround\n \n ` $ conda activate [MYENV]`\n \n `$ pip uninstall torch `\n \n `$ pip install torch `",
      "y": "My workaround\n \n ` $ conda activate [MYENV]`\n \n `$ pip uninstall torch `\n \n `$ pip install torch `"
   },
   {
      "x": "is it possible to add c++ api to load .pth model directly?",
      "z": "we dont plan to provide an interface other than the one via TorchScript.\n \n \n \n > I really puzzled why pytorch DO NOT want to provide unified interface for both .pth and .pt, which is not difficult to implement\n \n \n \n Without re-implementing Python, it's simply not possible. So, no you are wrong, it **is** difficult to implement.",
      "y": "we dont plan to provide an interface other than the one via TorchScript.\n \n \n \n > I really puzzled why pytorch DO NOT want to provide unified interface for both .pth and .pt, which is not difficult to implement\n \n \n \n Without re-implementing Python, it's simply not possible. So, no you are wrong, it **is** difficult to implement."
   },
   {
      "x": "Inconsistent dimension argument numbering between function",
      "z": "@vishwakftw well, there's some overlap. The issue you mentioned didn't address torch.diag_embed.",
      "y": "there's some overlap. The issue you mentioned didn't address torch.diag_embed."
   },
   {
      "x": "mobilenet_v2 not working with the newest version(1.1.0)",
      "z": "great, thanks. this will be parts of the upcoming release next week.\n \n \n \n @gujinghui and team, nothing to fix, looks like already fixed in a newer MKL-DNN release.",
      "y": "great, thanks. this will be parts of the upcoming release next week.\n \n \n \nnothing to fix, looks like already fixed in a newer MKL-DNN release."
   },
   {
      "x": "PyTorch->Onnx conversion incorrect",
      "z": "My mistake! I am using the correct PyTorch nightly build (1.2.0.dev20190731+cpu\n \n ), and I got this output from PyTorch nightly over dynamic_axes, \n \n \n \n /home/charles/anaconda2/lib/python2.7/site-packages/torch/onnx/utils.py:718: UserWarning: Provided key output for dynamic axes is not a valid input/output name\n \n  warnings.warn(\"Provided key {} for dynamic axes is not a valid input/output name\".format(key))\n \n \n \n but it turns out that a super_resolution.onnx file was created anyways, and after exporting the model and after testing it in other NN frameworks I do indeed get the correct output image. Thanks for all the help!",
      "y": "My mistake! I am using the correct PyTorch nightly build (1.2.0.dev20190731+cpu\n \n ), and I got this output from PyTorch nightly over dynamic_axes, \n \n \n \n /home/charles/anaconda2/lib/python2.7/site-packages/torch/onnx/utils.py:718: UserWarning: Provided key output for dynamic axes is not a valid input/output name\n \n  warnings.warn(\"Provided key {} for dynamic axes is not a valid input/output name\".format(key))\n \n \n \n but it turns out that a super_resolution.onnx file was created anyways, and after exporting the model and after testing it in other NN frameworks I do indeed get the correct output image. Thanks for all the help!"
   },
   {
      "x": "Add nn.functional.interpolate slow down backward process more than 10 times",
      "z": "Hi, thank you so much, after updating to nightly version, time cost for 2-layer model with interpolate is about 0.013s/iter and 2-layer model without interpolate is 0.009s/iter. Much better now.",
      "y": "Hi, thank you so much, after updating to nightly version, time cost for 2-layer model with interpolate is about 0.013s/iter and 2-layer model without interpolate is 0.009s/iter. Much better now."
   },
   {
      "x": "Not able to install after building successfully from source",
      "z": "I followed readme, used conda, etc. Still get this error. The libs it's looking for are in build/lib but for some reason, it can;t find them when it links. Done so many clean builds, all day.\n \n v1.3.0, but I saw it with latest version as well before.",
      "y": "I followed readme, used conda, etc. Still get this error. The libs it's looking for are in build/lib but for some reason, it can;t find them when it links. Done so many clean builds, all day.\n \n v1.3.0, but I saw it with latest version as well before."
   },
   {
      "x": "[v1.2.0] Release Tracker",
      "z": "@Nikronic Yeah, there was some trouble with the builds. Hopefully they'll be up today.",
      "y": "@Nikronic Yeah, there was some trouble with the builds. Hopefully they'll be up today."
   },
   {
      "x": "Compilation error with CUDA 10.0/ninja on Linux",
      "z": "enable the environment variable `export TORCH_CUDA_ARCH_LIST=7.0`, and this should be fixed.\n \n \n \n Basically, what's happening here is that you are building in a docker container that has CUDA, but doesn't have access to a GPU. So PyTorch is defaulting to building for all CUDA architectures, and I suspect it's including some old architectures. If you paste the full build log, we can tell what it's attempting to do.",
      "y": "enable the environment variable `export TORCH_CUDA_ARCH_LIST=7.0`, and this should be fixed.\n \n \n \n Basically, what's happening here is that you are building in a docker container that has CUDA, but doesn't have access to a GPU. So PyTorch is defaulting to building for all CUDA architectures, and I suspect it's including some old architectures. If you paste the full build log, we can tell what it's attempting to do."
   },
   {
      "x": "Speed Regression in torch.qr",
      "z": "QR batching PR was done after 1.1, please allow me to investigate.",
      "y": "QR batching PR was done after 1.1, please allow me to investigate."
   },
   {
      "x": "torchvision0.3.0 is incompatible with pytorch1.2(nightly)",
      "z": "@ganleiboy If you use PyTorch 1.2, then you need torchvision 0.4.0. You cannot use torchvision 0.3.0 as it relies on the C++ API of PyTorch 1.1.",
      "y": "@ganleiboy If you use PyTorch 1.2, then you need torchvision 0.4.0. You cannot use torchvision 0.3.0 as it relies on the C++ API of PyTorch 1.1."
   },
   {
      "x": "torch.Tensor.repeat() fails for 0 repeats",
      "z": "Hi, I'd like to try this. Expect a PR soon :+1:",
      "y": "Hi, I'd like to try this. Expect a PR soon :+1:"
   },
   {
      "x": "[FR][hub] load_state_dict_from_url should avoid using tempfile or allow specifying tmp dir",
      "z": "@ailzhang That sounds a pretty elegant fix. Thanks so much for the quick reply! :) \n \n There is no rush. I live on the edge and run my code on master builds anyways :D",
      "y": "That sounds a pretty elegant fix. Thanks so much for the quick reply! :) \n \n There is no rush. I live on the edge and run my code on master builds anyways :D"
   },
   {
      "x": "Unnecessary extra memory allocation when using circular padding",
      "z": "Resolved by https://github.com/pytorch/pytorch/pull/39273.",
      "y": "Resolved by https://github.com/pytorch/pytorch/pull/39273."
   },
   {
      "x": "[RFC] DDP Communication Hook",
      "z": "Hi @sinannasir \n \n \n \n Thank you for your reply.\n \n \n \n That's some really good work out there. I experiment with different gradient compression techniques. I am eagerly waiting to try out some compression algorithms using this new development.",
      "y": "\n Thank you for your reply.\n \n \n \n That's some really good work out there. I experiment with different gradient compression techniques. I am eagerly waiting to try out some compression algorithms using this new development."
   },
   {
      "x": "[jit] `@staticmethod`s retrieved from `self` don't work on normal class",
      "z": "Oh, maybe you mean to say that the arguments should be inferred to be `Tensor` instead of `MyCell`?",
      "y": "Oh, maybe you mean to say that the arguments should be inferred to be `Tensor` instead of `MyCell`?"
   },
   {
      "x": "Use nn.spectral_norm in LSTM",
      "z": "fwiw, the following script works on master (and likely in 1.5)\n \n ```\n \n import torch\n \n \n \n def lstm_spectral_norm(input_size, hidden_size, n_layers=1):\n \n  lstm = torch.nn.LSTM(input_size, hidden_size, n_layers)\n \n  name_pre = 'weight'\n \n  for i in range(n_layers):\n \n  name = name_pre+'_hh_l'+str(i)\n \n  torch.nn.utils.spectral_norm(lstm, name)\n \n  name = name_pre+'_ih_l'+str(i)\n \n  torch.nn.utils.spectral_norm(lstm, name)\n \n  return lstm\n \n \n \n ninp = 128\n \n m = lstm_spectral_norm(ninp, ninp)\n \n name = 'weight_hh_l0'\n \n m = m.cuda()\n \n print([(name, p.device) for name, p in m.named_parameters()])\n \n inp = torch.randn(3, 32, 128, device=\"cuda\")\n \n ```\n \n There was a bug that was fixed couple months back.",
      "y": "fwiw, the following script works on master (and likely in 1.5)\n \n ```\n \n import torch\n \n \n \n def lstm_spectral_norm(input_size, hidden_size, n_layers=1):\n \n  lstm = torch.nn.LSTM(input_size, hidden_size, n_layers)\n \n  name_pre = 'weight'\n \n  for i in range(n_layers):\n \n  name = name_pre+'_hh_l'+str(i)\n \n  torch.nn.utils.spectral_norm(lstm, name)\n \n  name = name_pre+'_ih_l'+str(i)\n \n  torch.nn.utils.spectral_norm(lstm, name)\n \n  return lstm\n \n \n \n ninp = 128\n \n m = lstm_spectral_norm(ninp, ninp)\n \n name = 'weight_hh_l0'\n \n m = m.cuda()\n \n print([(name, p.device) for name, p in m.named_parameters()])\n \n inp = torch.randn(3, 32, 128, device=\"cuda\")\n \n ```\n \n There was a bug that was fixed couple months back."
   },
   {
      "x": "Pytorch could save the model in FP16(Half) mode and reload it to FP32.",
      "z": "@vadimkantorov This is confusing, while i saevd fp16 model in pth, when exporting it to onnx, it return a bug to me",
      "y": "@vadimkantorov This is confusing, while i saevd fp16 model in pth, when exporting it to onnx, it return a bug to me"
   },
   {
      "x": "verify_ninja_availability does not return True",
      "z": "@0phoff \n \n It seems this doc line is for _is_ninja_available instead of verify_ninja_availability, if you need a true or false return value, you can use _is_ninja_available. I will update the doc and will change _is_ninja_available to is_ninja_available since this should be a public api. cc @yf225",
      "y": "\n \n It seems this doc line is for _is_ninja_available instead of verify_ninja_availability, if you need a true or false return value, you can use _is_ninja_available. I will update the doc and will change _is_ninja_available to is_ninja_available since this should be a public api. "
   },
   {
      "x": "[JIT] Scripted model second run cost 5 mins, after that 1-2 secs",
      "z": "> Did you run the same script I did? Do you mind sharing the output of that script? I will try with your torch configuration\n \n \n \n I tried the exported models from some days ago. So, I think that this was the problem. \n \n \n \n I have tested your code on new conda venv and is working again. I am going to try to export them again now and see if they works correctly now.",
      "y": "> Did you run the same script I did? Do you mind sharing the output of that script? I will try with your torch configuration\n \n \n \n I tried the exported models from some days ago. So, I think that this was the problem. \n \n \n \n I have tested your code on new conda venv and is working again. I am going to try to export them again now and see if they works correctly now."
   },
   {
      "x": "DISABLED test_backward_node_failure (__main__.TensorPipeAgentDistAutogradTestWithSpawn)",
      "z": "For now it is expected. There's a fixme in the code, but I haven't gotten to address it yet:\n \n https://github.com/pytorch/pytorch/blob/ec5d579929b2c56418aacaec0874b92937d095a4/torch/csrc/distributed/rpc/tensorpipe_agent.cpp#L308-L316\n \n \n \n It's harmless though.",
      "y": "For now it is expected. There's a fixme in the code, but I haven't gotten to address it yet:\n \n https://github.com/pytorch/pytorch/blob/ec5d579929b2c56418aacaec0874b92937d095a4/torch/csrc/distributed/rpc/tensorpipe_agent.cpp#L308-L316\n \n \n \n It's harmless though."
   },
   {
      "x": "pytorch README build table cleanup for ppc64le",
      "z": "PR https://github.com/pytorch/pytorch/pull/39475/files submitted.\n \n \n \n The build has now been renamed, so the link to the old name is briefly incorrect (until this file is merged).",
      "y": "PR https://github.com/pytorch/pytorch/pull/39475/files submitted.\n \n \n \n The build has now been renamed, so the link to the old name is briefly incorrect (until this file is merged)."
   },
   {
      "x": "DISABLED test_backward_node_failure_python_udf (__main__.TensorPipeAgentDistAutogradTestWithSpawn)",
      "z": "Yes, I wanted to get a few more commits in but actually let's first get this fixed and worry about those later. I'll send out a PR right away.",
      "y": "Yes, I wanted to get a few more commits in but actually let's first get this fixed and worry about those later. I'll send out a PR right away."
   },
   {
      "x": "When the name of running file is \"dis.py\", the pytorch environment will be crashed.",
      "z": "You have a file that shadows a builtin library of python https://docs.python.org/3/library/dis.html , which should not happen unless you are messing with Python's import path or PATH, neither of which is recommended. It's not really PyTorch's fault. Any code that uses `inspect` would be broken.",
      "y": "You have a file that shadows a builtin library of python https://docs.python.org/3/library/dis.html , which should not happen unless you are messing with Python's import path or PATH, neither of which is recommended. It's not really PyTorch's fault. Any code that uses `inspect` would be broken."
   },
   {
      "x": "iOS predict got different result in simulator and device",
      "z": "@yyang9887 This issue is duplicated with https://github.com/pytorch/pytorch/issues/38186. Root cause found, working on the solution.",
      "y": "@yyang9887 This issue is duplicated with https://github.com/pytorch/pytorch/issues/38186. Root cause found, working on the solution."
   },
   {
      "x": "CTCLoss documentation is unclear",
      "z": "@rlorigro This is highly appreciated, as would your PR! I'm sure the documentation can be much improved over what I put in there. (Maybe one could consider linking to the [distill.pub article](https://distill.pub/2017/ctc/) on CTC, too - but I haven't looked at the docs lately to see this.)\n \n Some of your points are not yet actionable (e.g. that the targets start at one because 0 is blank in the example is written as a question).\n \n I'd be very happy to review your PR, too, so please don't hesitate to tag me on it.",
      "y": "Raise your PR! I'm sure the documentation can be much improved over what I put in there. (Maybe one could consider linking to the [distill.pub article](https://distill.pub/2017/ctc/) on CTC, too - but I haven't looked at the docs lately to see this.)\n \n Some of your points are not yet actionable (e.g. that the targets start at one because 0 is blank in the example is written as a question).\n \n I'd be very happy to review your PR, too, so please don't hesitate to tag me on it."
   },
   {
      "x": "[ONNX] Enable argmax and argmin with no dim and no keepdims",
      "z": "@houseroad Sure, I will",
      "y": "Sure, I will"
   },
   {
      "x": "DistributedDataParallel",
      "z": "@soumith I set up GPU just as the doc you posted, however I still noticed that DistributedDataParallel wrapper cost more GPU memory:\n \n the model cost around 7300 MB when loaded into one GPU, and 21129 when wrapped in DistributedDataParallel.",
      "y": "set up GPU ,however I still noticed that DistributedDataParallel wrapper cost more GPU memory:\n \n the model cost around 7300 MB when loaded into one GPU, and 21129 when wrapped in DistributedDataParallel."
   },
   {
      "x": "cmake build stuck",
      "z": "Would you please clean the build cache `python setup.py clean` and try again?",
      "y": "Would you please clean the build cache `python setup.py clean` and try again?"
   },
   {
      "x": "Mac OS X build is broken: ld: warning: ignoring file lib/libonnxifi_loader.a, file was built for archive which is not the architecture being linked (x86_64): lib/libonnxifi_loader.a",
      "z": "`brew unlink binutils` solved my problem.",
      "y": "`brew unlink binutils` solved my problem."
   },
   {
      "x": "Gradient of torch.where not handling nan correctly",
      "z": "the good old mask problem with the autograd looking only at each step..",
      "y": "the good old mask problem with the autograd looking only at each step.."
   },
   {
      "x": "Model Quantization for PyTorch (Proposal)",
      "z": "> How to export to mobile\n \n \n \n Ha!",
      "y": "> How to export to mobile\n \n \n \n Ha!"
   },
   {
      "x": "Add Windows tests for multiple configurations",
      "z": "Now that we have enabled Windows CI on Circle. Due to its large parallelism, we could proceed much further this time. Below are the things that we could do first.\n \n 1. Separate CPU and CUDA builds. It will make the build script more readable. https://github.com/pytorch/pytorch/blob/master/.jenkins/pytorch/win-test-helpers/build_pytorch.bat#L79\n \n 2. Build and test job for more python versions, at least python latest should be included. By doing this, we could avoid errors like this to happen. https://github.com/pytorch/pytorch/pull/30809\n \n 3. VS 2019 latest builds. (~Usually we don't need this, unless MS made some significant changes to their compiler toolchain~ But actually, there are far more bugs than we might think.)\n \n 4. VS 2017 latest builds (Same reason with the above one)\n \n \n \n The following are all the available options that are of low priority.\n \n 1. Debug LibTorch builds. (Already tested in nightly jobs)\n \n 2. Build and test job for more cuda versions. (Code changes related to CUDA are usually platform-agnostic)\n \n 3. 32-bit CPU builds. (Users are limited)\n \n 4. Static builds. (Not tested yet)\n \n 5. Legacy OS test builds. (Not possible in CircleCI)\n \n 7. Builds with other compilers, such as GCC, Intel CC, Clang. (Not easy to fix or maintain)\n \n 8. Test with Python in Microsoft Store (Windows 10 only) (Not possible in CircleCI)\n \n \n \n cc @mingbowan",
      "y": "Now that we have enabled Windows CI on Circle. Due to its large parallelism, we could proceed much further this time. Below are the things that we could do first.\n \n 1. Separate CPU and CUDA builds. It will make the build script more readable. https://github.com/pytorch/pytorch/blob/master/.jenkins/pytorch/win-test-helpers/build_pytorch.bat#L79\n \n 2. Build and test job for more python versions, at least python latest should be included. By doing this, we could avoid errors like this to happen. https://github.com/pytorch/pytorch/pull/30809\n \n 3. VS 2019 latest builds. (~Usually we don't need this, unless MS made some significant changes to their compiler toolchain~ But actually, there are far more bugs than we might think.)\n \n 4. VS 2017 latest builds (Same reason with the above one)\n \n \n \n The following are all the available options that are of low priority.\n \n 1. Debug LibTorch builds. (Already tested in nightly jobs)\n \n 2. Build and test job for more cuda versions. (Code changes related to CUDA are usually platform-agnostic)\n \n 3. 32-bit CPU builds. (Users are limited)\n \n 4. Static builds. (Not tested yet)\n \n 5. Legacy OS test builds. (Not possible in CircleCI)\n \n 7. Builds with other compilers, such as GCC, Intel CC, Clang. (Not easy to fix or maintain)\n \n 8. Test with Python in Microsoft Store (Windows 10 only) (Not possible in CircleCI)\n \n \n \n cc @mingbowan"
   },
   {
      "x": "JITed program is getting stuck",
      "z": "Great, thanks a lot for the explanation. I did try with `torch.no_grad()` but apparently, we can't do that inside the JIT yet. I haven't tried `.required_grad=True`. Will give it a shot.",
      "y": "Great, thanks a lot for the explanation. I did try with `torch.no_grad()` but apparently, we can't do that inside the JIT yet. I haven't tried `.required_grad=True`. Will give it a shot."
   },
   {
      "x": "ModuleNotFoundError: No module named 'models'",
      "z": "**torch.load() requires model module in the same folder** pytorch/pytorch#3678",
      "y": "**torch.load() requires model module in the same folder** pytorch/pytorch#3678"
   },
   {
      "x": "Unserialize autograd",
      "z": "Hi,\n \n May I ask if there is any progress?",
      "y": "Hi,\n \n May I ask if there is any progress?"
   },
   {
      "x": "Pass Multiple Tensors in CPP Module Forward",
      "z": "[solved] Turned out it works to push multiple input tensors to the IValue like so:\n \n \n \n ```cpp\n \n std::vector<torch::jit::IValue> i;\n \n std::vector<float> x = {2.0, 1.0, 0.5, 3.0, 4.0};\n \n std::vector<float> y = {3.0, 2.0};\n \n i.push_back(torch::tensor(x));\n \n i.push_back(torch::tensor(y));\n \n \n \n torch::Tensor output = module->forward(i).toTensor();\n \n ```",
      "y": "[solved] Turned out it works to push multiple input tensors to the IValue like so:\n \n \n \n ```cpp\n \n std::vector<torch::jit::IValue> i;\n \n std::vector<float> x = {2.0, 1.0, 0.5, 3.0, 4.0};\n \n std::vector<float> y = {3.0, 2.0};\n \n i.push_back(torch::tensor(x));\n \n i.push_back(torch::tensor(y));\n \n \n \n torch::Tensor output = module->forward(i).toTensor();\n \n ```"
   },
   {
      "x": "Port TemporalReplicationPadding to ATen",
      "z": "I filed a ticket to give you rights.",
      "y": "I filed a ticket to give you rights."
   },
   {
      "x": "mkl-dnn build breaks with mkl 2019.3",
      "z": "MKL-DNN version breaks on ClearLinux 28320 as well for mkl 2019.3\n \n \n \n I can confirm that Pytorch builds successfully with mkl=2019.1 and mkl-include=2019.1 for Python 3.7.1",
      "y": "MKL-DNN version breaks on ClearLinux 28320 as well for mkl 2019.3\n \n \n \n I can confirm that Pytorch builds successfully with mkl=2019.1 and mkl-include=2019.1 for Python 3.7.1"
   },
   {
      "x": "[jit] be more permissive with bool casting.",
      "z": "Yeah, that would be good to see the behavior depicted in the docs, I am sure I am not the first one to stumble on this issue",
      "y": "Yeah, that would be good to see the behavior depicted in the docs, I am sure I am not the first one to stumble on this issue"
   },
   {
      "x": "[FR] make torch.nn.utils.convert_sync_batchnorm a classmethod of SyncBatchNorm",
      "z": "Just want to comment. If we do this, we need to do this before next release.",
      "y": "Just want to comment. If we do this, we need to do this before next release."
   },
   {
      "x": "pip install torch==1.0.1.post2",
      "z": "Sorry, pytorch for Windows is currently not available through PYPI. You have to enter the commands in https://pytorch.org.",
      "y": "Sorry, pytorch for Windows is currently not available through PYPI. You have to enter the commands in https://pytorch.org."
   },
   {
      "x": "Can't find `Python.h` when include `torch/extension.h`",
      "z": "Soumith solved the OP's problem already, but if anyone else arrives here from Google just looking for `Python.h`, the solution is to get the Python include path from\n \n ```python\n \n import sysconfig\n \n print(sysconfig.get_paths()['include'])\n \n ```\n \n and add it to your includes.",
      "y": "Soumith solved the OP's problem already, but if anyone else arrives here from Google just looking for `Python.h`, the solution is to get the Python include path from\n \n ```python\n \n import sysconfig\n \n print(sysconfig.get_paths()['include'])\n \n ```\n \n and add it to your includes."
   },
   {
      "x": "update from source",
      "z": "@stas00 sent a PR here: https://github.com/pytorch/pytorch/pull/18409",
      "y": "a PR here: https://github.com/pytorch/pytorch/pull/18409"
   },
   {
      "x": "Build from source in virtualenv causes libiomp5.so load failure",
      "z": "(2) sounds simple and easy. I don't want mkl-dnn downloading stuff in the build.",
      "y": "(2) sounds simple and easy. I don't want mkl-dnn downloading stuff in the build."
   },
   {
      "x": "Unable to locate SpatialAdaptiveAveragePooling.c",
      "z": "SpatialAdaptiveAveragePooling was removed in [this commit](https://github.com/pytorch/pytorch/commit/64b336420918d31ee3da623bbcb7afadc73edb6d).\n \n \n \n Please close the issue if you think it is resolved.",
      "y": "SpatialAdaptiveAveragePooling was removed in [this commit](https://github.com/pytorch/pytorch/commit/64b336420918d31ee3da623bbcb7afadc73edb6d).\n \n \n \n Please close the issue if you think it is resolved."
   },
   {
      "x": "Regarding porting of functions in ATen directory",
      "z": "some backend code is simply not used by any function in python. it's old code from a while ago.",
      "y": "some backend code is simply not used by any function in python. it's old code from a while ago."
   },
   {
      "x": "AttributeError: Can't get attribute '_rebuild_parameter' on",
      "z": "ty \n \n i found solution \n \n i created my model with version 1.0\n \n when i upload it in machine ,pytorch version is 0.4",
      "y": "ty \n \n i found solution \n \n i created my model with version 1.0\n \n when i upload it in machine ,pytorch version is 0.4"
   },
   {
      "x": "Are there any difference of autograd between these two ways to get the gradient of FFT?",
      "z": "pytorch native fft works on cuda, and should be preferred as we have extensive tests on it. the tutorial is just an example of how to write extensions.",
      "y": "pytorch native fft works on cuda, and should be preferred as we have extensive tests on it. the tutorial is just an example of how to write extensions."
   },
   {
      "x": "Cannot create operator of type 'ReduceL2' on the device 'CUDA' when use caffe2 to run export model",
      "z": "Sorry for this mistake. Keep me informed if you find any bug please.\n \n `// simple implementation of ReduceL2\n \n template <typename T>\n \n __device__ void warpReduce(volatile T* sdata, int tid) {\n \n  sdata[tid] += sdata[tid + 32];\n \n  sdata[tid] += sdata[tid + 16];\n \n  sdata[tid] += sdata[tid + 8];\n \n  sdata[tid] += sdata[tid + 4];\n \n  sdata[tid] += sdata[tid + 2];\n \n  sdata[tid] += sdata[tid + 1];\n \n }\n \n template <typename T>\n \n __global__ void\n \n kRowwiseL2Reduce(const T* data, T* output, int nrows, int ncols) {\n \n  extern __shared__ T sh[];\n \n  int tid = threadIdx.x;\n \n  // int rid = blockIdx.x; // each block process one row of the matrix\n \n  for (int rid = blockIdx.x; rid < nrows; rid += gridDim.x) {\n \n  const T* x = data + rid * ncols;\n \n  //T* y = output + rid * ncols;\n \n  sh[tid] = 0.0f;\n \n  __syncthreads();\n \n  float tmp = 0.0f;\n \n  for (int j = tid; j < ncols; j += blockDim.x) {\n \n  tmp += x[j] * x[j];\n \n  }\n \n  sh[tid] = tmp;\n \n  __syncthreads();\n \n  for (int s = blockDim.x / 2; s > 32; s >>= 1) {\n \n  if (tid < s) {\n \n  sh[tid] += sh[tid + s];\n \n  }\n \n  __syncthreads();\n \n  }\n \n  if (tid < 32) {\n \n  warpReduce(sh, tid);\n \n  }\n \n  if (tid == 0) {\n \n  output[rid] = sqrtf(sh[tid]);\n \n  // sh[tid] = sqrtf(sh[tid]);\n \n  }\n \n  //__syncthreads();\n \n  }\n \n }\n \n template <typename T>\n \n __global__ void\n \n kColwiseL2Reduce(const T* data, T* output, int nrows, int ncols) {\n \n  extern __shared__ T sh[];\n \n  int tid = threadIdx.x;\n \n  // int rid = blockIdx.x; // each block process one row of the matrix\n \n  for (int cid = blockIdx.x; cid < ncols; cid += gridDim.x) {\n \n  //const T* x = data + rid * ncols;\n \n  //T* y = output + cid * ncols;\n \n  sh[tid] = 0.0f;\n \n  __syncthreads();\n \n  T tmp = 0.0f;\n \n  for (int j = tid; j < nrows; j += blockDim.x) {\n \n  tmp += data[j * ncols + cid] * data[j * ncols + cid];\n \n  }\n \n  sh[tid] = tmp;\n \n  __syncthreads();\n \n  for (int s = blockDim.x / 2; s > 32; s >>= 1) {\n \n  if (tid < s) {\n \n  sh[tid] += sh[tid + s];\n \n  }\n \n  __syncthreads();\n \n  }\n \n  if (tid < 32) {\n \n  warpReduce(sh, tid);\n \n  }\n \n  if (tid == 0) {\n \n  output[cid] = sqrtf(sh[tid]);\n \n  // sh[tid] = sqrtf(sh[tid]);\n \n  }\n \n  //__syncthreads();\n \n  }\n \n }\n \n \n \n \n \n #define CAFFE2_SPECIALIZED_CUDA_REDUCE_L2(T) \\\n \n  template <> \\\n \n  C10_EXPORT void ReduceL2<T, CUDAContext>( \\\n \n  const int num_dims, \\\n \n  const int* dims, \\\n \n  const int num_axes, \\\n \n  const int* axes, \\\n \n  const T alpha, \\\n \n  const T* X, \\\n \n  T* Y, \\\n \n  CUDAContext* context) { \\\n \n  CAFFE_ENFORCE_LE(num_axes, num_dims); \\\n \n  std::vector<int> Y_dims_vector(dims, dims + num_dims); \\\n \n  for (int i = 0; i < num_axes; ++i) { \\\n \n  Y_dims_vector[axes[i]] = 1; \\\n \n  } \\\n \n  const int* X_dims = dims; \\\n \n  const int* Y_dims = Y_dims_vector.data(); \\\n \n  const int X_size = \\\n \n  std::accumulate(X_dims, X_dims + num_dims, 1, std::multiplies<int>()); \\\n \n  const int Y_size = \\\n \n  std::accumulate(Y_dims, Y_dims + num_dims, 1, std::multiplies<int>()); \\\n \n  if (X_size == 0) { \\\n \n  Set<T, CUDAContext>(Y_size, 0, Y, context); \\\n \n  return; \\\n \n  } \\\n \n  if (alpha == T(0)) { \\\n \n  Set<T, CUDAContext>(Y_size, 0, Y, context); \\\n \n  return; \\\n \n  } \\\n \n  if (std::equal(X_dims, X_dims + num_dims, Y_dims)) { \\\n \n  Abs<T, CUDAContext>(X_size, X, Y, context); \\\n \n  Scale<T, T, CUDAContext>(Y_size, alpha, Y, Y, context); \\\n \n  return; \\\n \n  } \\\n \n  int rows; \\\n \n  int cols; \\\n \n  if (utils::IsRowwiseReduce(num_dims, X_dims, Y_dims, &rows, &cols)) { \\\n \n  kRowwiseL2Reduce<T> \\\n \n  <<<CAFFE_GET_BLOCKS(rows), \\\n \n  CAFFE_CUDA_NUM_THREADS, \\\n \n  sizeof(T) * CAFFE_CUDA_NUM_THREADS, \\\n \n  context->cuda_stream()>>>(X, Y, rows, cols); \\\n \n  return; \\\n \n  } \\\n \n  if (utils::IsColwiseReduce(num_dims, X_dims, Y_dims, &rows, &cols)) { \\\n \n  kColwiseL2Reduce<T> \\\n \n  <<<CAFFE_GET_BLOCKS(cols), \\\n \n  CAFFE_CUDA_NUM_THREADS, \\\n \n  sizeof(T) * CAFFE_CUDA_NUM_THREADS, \\\n \n  context->cuda_stream()>>>(X, Y, rows, cols); \\\n \n  return; \\\n \n  } \\\n \n  int pre; \\\n \n  int mid; \\\n \n  int nxt; \\\n \n  if (utils::IsBothEndsReduce(num_dims, X_dims, Y_dims, &pre, &mid, &nxt)) { \\\n \n  LOG(FATAL) << \"Currently ReduceL2 only supports row/col wise reduce\"; \\\n \n  return; \\\n \n  } \\\n \n  LOG(FATAL) << \"Currently ReduceL2 only supports row/col wise reduce\"; \\\n \n  }\n \n CAFFE2_SPECIALIZED_CUDA_REDUCE_L2(float)`",
      "y": "Sorry for this mistake. Keep me informed if you find any bug please.\n \n `// simple implementation of ReduceL2\n \n template <typename T>\n \n __device__ void warpReduce(volatile T* sdata, int tid) {\n \n  sdata[tid] += sdata[tid + 32];\n \n  sdata[tid] += sdata[tid + 16];\n \n  sdata[tid] += sdata[tid + 8];\n \n  sdata[tid] += sdata[tid + 4];\n \n  sdata[tid] += sdata[tid + 2];\n \n  sdata[tid] += sdata[tid + 1];\n \n }\n \n template <typename T>\n \n __global__ void\n \n kRowwiseL2Reduce(const T* data, T* output, int nrows, int ncols) {\n \n  extern __shared__ T sh[];\n \n  int tid = threadIdx.x;\n \n  // int rid = blockIdx.x; // each block process one row of the matrix\n \n  for (int rid = blockIdx.x; rid < nrows; rid += gridDim.x) {\n \n  const T* x = data + rid * ncols;\n \n  //T* y = output + rid * ncols;\n \n  sh[tid] = 0.0f;\n \n  __syncthreads();\n \n  float tmp = 0.0f;\n \n  for (int j = tid; j < ncols; j += blockDim.x) {\n \n  tmp += x[j] * x[j];\n \n  }\n \n  sh[tid] = tmp;\n \n  __syncthreads();\n \n  for (int s = blockDim.x / 2; s > 32; s >>= 1) {\n \n  if (tid < s) {\n \n  sh[tid] += sh[tid + s];\n \n  }\n \n  __syncthreads();\n \n  }\n \n  if (tid < 32) {\n \n  warpReduce(sh, tid);\n \n  }\n \n  if (tid == 0) {\n \n  output[rid] = sqrtf(sh[tid]);\n \n  // sh[tid] = sqrtf(sh[tid]);\n \n  }\n \n  //__syncthreads();\n \n  }\n \n }\n \n template <typename T>\n \n __global__ void\n \n kColwiseL2Reduce(const T* data, T* output, int nrows, int ncols) {\n \n  extern __shared__ T sh[];\n \n  int tid = threadIdx.x;\n \n  // int rid = blockIdx.x; // each block process one row of the matrix\n \n  for (int cid = blockIdx.x; cid < ncols; cid += gridDim.x) {\n \n  //const T* x = data + rid * ncols;\n \n  //T* y = output + cid * ncols;\n \n  sh[tid] = 0.0f;\n \n  __syncthreads();\n \n  T tmp = 0.0f;\n \n  for (int j = tid; j < nrows; j += blockDim.x) {\n \n  tmp += data[j * ncols + cid] * data[j * ncols + cid];\n \n  }\n \n  sh[tid] = tmp;\n \n  __syncthreads();\n \n  for (int s = blockDim.x / 2; s > 32; s >>= 1) {\n \n  if (tid < s) {\n \n  sh[tid] += sh[tid + s];\n \n  }\n \n  __syncthreads();\n \n  }\n \n  if (tid < 32) {\n \n  warpReduce(sh, tid);\n \n  }\n \n  if (tid == 0) {\n \n  output[cid] = sqrtf(sh[tid]);\n \n  // sh[tid] = sqrtf(sh[tid]);\n \n  }\n \n  //__syncthreads();\n \n  }\n \n }\n \n \n \n \n \n #define CAFFE2_SPECIALIZED_CUDA_REDUCE_L2(T) \\\n \n  template <> \\\n \n  C10_EXPORT void ReduceL2<T, CUDAContext>( \\\n \n  const int num_dims, \\\n \n  const int* dims, \\\n \n  const int num_axes, \\\n \n  const int* axes, \\\n \n  const T alpha, \\\n \n  const T* X, \\\n \n  T* Y, \\\n \n  CUDAContext* context) { \\\n \n  CAFFE_ENFORCE_LE(num_axes, num_dims); \\\n \n  std::vector<int> Y_dims_vector(dims, dims + num_dims); \\\n \n  for (int i = 0; i < num_axes; ++i) { \\\n \n  Y_dims_vector[axes[i]] = 1; \\\n \n  } \\\n \n  const int* X_dims = dims; \\\n \n  const int* Y_dims = Y_dims_vector.data(); \\\n \n  const int X_size = \\\n \n  std::accumulate(X_dims, X_dims + num_dims, 1, std::multiplies<int>()); \\\n \n  const int Y_size = \\\n \n  std::accumulate(Y_dims, Y_dims + num_dims, 1, std::multiplies<int>()); \\\n \n  if (X_size == 0) { \\\n \n  Set<T, CUDAContext>(Y_size, 0, Y, context); \\\n \n  return; \\\n \n  } \\\n \n  if (alpha == T(0)) { \\\n \n  Set<T, CUDAContext>(Y_size, 0, Y, context); \\\n \n  return; \\\n \n  } \\\n \n  if (std::equal(X_dims, X_dims + num_dims, Y_dims)) { \\\n \n  Abs<T, CUDAContext>(X_size, X, Y, context); \\\n \n  Scale<T, T, CUDAContext>(Y_size, alpha, Y, Y, context); \\\n \n  return; \\\n \n  } \\\n \n  int rows; \\\n \n  int cols; \\\n \n  if (utils::IsRowwiseReduce(num_dims, X_dims, Y_dims, &rows, &cols)) { \\\n \n  kRowwiseL2Reduce<T> \\\n \n  <<<CAFFE_GET_BLOCKS(rows), \\\n \n  CAFFE_CUDA_NUM_THREADS, \\\n \n  sizeof(T) * CAFFE_CUDA_NUM_THREADS, \\\n \n  context->cuda_stream()>>>(X, Y, rows, cols); \\\n \n  return; \\\n \n  } \\\n \n  if (utils::IsColwiseReduce(num_dims, X_dims, Y_dims, &rows, &cols)) { \\\n \n  kColwiseL2Reduce<T> \\\n \n  <<<CAFFE_GET_BLOCKS(cols), \\\n \n  CAFFE_CUDA_NUM_THREADS, \\\n \n  sizeof(T) * CAFFE_CUDA_NUM_THREADS, \\\n \n  context->cuda_stream()>>>(X, Y, rows, cols); \\\n \n  return; \\\n \n  } \\\n \n  int pre; \\\n \n  int mid; \\\n \n  int nxt; \\\n \n  if (utils::IsBothEndsReduce(num_dims, X_dims, Y_dims, &pre, &mid, &nxt)) { \\\n \n  LOG(FATAL) << \"Currently ReduceL2 only supports row/col wise reduce\"; \\\n \n  return; \\\n \n  } \\\n \n  LOG(FATAL) << \"Currently ReduceL2 only supports row/col wise reduce\"; \\\n \n  }\n \n CAFFE2_SPECIALIZED_CUDA_REDUCE_L2(float)`"
   },
   {
      "x": "Compile fail on HEAD, `autodiff.cpp:156:20: error: \u00e2\u20ac\u02dcinlineCallTo\u00e2\u20ac\u2122 is not a member of \u00e2\u20ac\u02dctorch::jit::script\u00e2\u20ac\u2122` and others",
      "z": "Please reopen if there's any further problems. Thanks!",
      "y": "Please reopen if there's any further problems. Thanks!"
   },
   {
      "x": "Bad error message when creating a class instance in script functions",
      "z": "@apaszke This issue can be closed. This [PR](https://github.com/pytorch/pytorch/pull/16416) resolves it.",
      "y": "@apaszke This issue can be closed. This [PR](https://github.com/pytorch/pytorch/pull/16416) resolves it."
   },
   {
      "x": "Core dump on threadripper/Ryzen",
      "z": "OK, found the issue. I had an old version of MKL-DNN installed as a dynamic library:\n \n \n \n `#6 0x00007fffe3f873bf in _GLOBAL__sub_I_jit_avx512_common_conv_kernel.cpp () from /usr/local/lib/libmkldnn.so.0`\n \n \n \n ...oversaw this as pytorch source includes mkl-dnn repo so I (wrongly) assumed it would be linked statically, but it looks it's linked dynamically and if you have another version in your system it picks the dynamic library.\n \n \n \n Solution: I reinstalled last official release of mkl-dnn from Intel repo and works now.",
      "y": "OK, found the issue. I had an old version of MKL-DNN installed as a dynamic library:\n \n \n \n `#6 0x00007fffe3f873bf in _GLOBAL__sub_I_jit_avx512_common_conv_kernel.cpp () from /usr/local/lib/libmkldnn.so.0`\n \n \n \n ...oversaw this as pytorch source includes mkl-dnn repo so I (wrongly) assumed it would be linked statically, but it looks it's linked dynamically and if you have another version in your system it picks the dynamic library.\n \n \n \n Solution: I reinstalled last official release of mkl-dnn from Intel repo and works now."
   },
   {
      "x": "libtorch API params error with outdated documentation",
      "z": "In\n \n \n \n `auto img_tensor = torch::CUDA(torch::kFloat32).tensorFromBlob(img_float.data, {1, 224, 224, 3});`\n \n \n \n You are creating a CUDA tensor and then assigning it a blob that is on CPU. This is not allowed. You have to create a CPU tensor first and then *copy* the data to CUDA memory. You should write `torch::CPU(torch::kFloat32).tensorFromBlob(...).to(torch::kCUDA)`.\n \n \n \n Also note that `torch::CPU` and `torch::CUDA` are deprecated in PyTorch 1.0. You should write this as `torch::from_blob(img_float.data, {1, 224, 224, 3}).to(torch::kCUDA)`.",
      "y": "In\n \n \n \n `auto img_tensor = torch::CUDA(torch::kFloat32).tensorFromBlob(img_float.data, {1, 224, 224, 3});`\n \n \n \n You are creating a CUDA tensor and then assigning it a blob that is on CPU. This is not allowed. You have to create a CPU tensor first and then *copy* the data to CUDA memory. You should write `torch::CPU(torch::kFloat32).tensorFromBlob(...).to(torch::kCUDA)`.\n \n \n \n Also note that `torch::CPU` and `torch::CUDA` are deprecated in PyTorch 1.0. You should write this as `torch::from_blob(img_float.data, {1, 224, 224, 3}).to(torch::kCUDA)`."
   },
   {
      "x": "Pytorch Build: Caffe2 libcaffe2_gpu.so undefined reference to '__cudaPopCallConfiguration'",
      "z": "I noticed that I had installed a potentially incorrect version of 'magma-cuda'.\n \n \n \n I'd installed magma-cuda92 rather than magma-cuda90.\n \n \n \n Might be worth checking.\n \n \n \n @fwillo This appears to have resolved this for me!",
      "y": "I noticed that I had installed a potentially incorrect version of 'magma-cuda'.\n \n \n \n I'd installed magma-cuda92 rather than magma-cuda90.\n \n \n \n Might be worth checking.\n \n \n \nThis appears to have resolved this for me!"
   },
   {
      "x": "Loading a PyTorch Model in C++ tutorial fails with No rule to make target '/usr/local/cuda/lib64/libculibos.a'",
      "z": "Bandaid solution:\n \n \n \n In the file `/path/to/libtorch/share/cmake/Caffe2/Caffe2Targets.cmake`\n \n \n \n Replace the references to `/usr/local/cuda/...` with what ever path your CUDA is installed to",
      "y": "Bandaid solution:\n \n \n \n In the file `/path/to/libtorch/share/cmake/Caffe2/Caffe2Targets.cmake`\n \n \n \n Replace the references to `/usr/local/cuda/...` with what ever path your CUDA is installed to"
   },
   {
      "x": "nan propagates through backward pass even when not accessed",
      "z": "Why is selection handled through multiplication by 0, though? And doesn\u00e2\u20ac\u2122t the multiplication incur an overhead too? Shouldn\u00e2\u20ac\u2122t it be handled by a conditional like torch.where?",
      "y": "Why is selection handled through multiplication by 0, though? And doesn\u00e2\u20ac\u2122t the multiplication incur an overhead too? Shouldn\u00e2\u20ac\u2122t it be handled by a conditional like torch.where?"
   },
   {
      "x": "pdist with large inputs is giving illegal memory exception on CUDA",
      "z": "E.g. here https://github.com/pytorch/pytorch/blob/master/aten/src/ATen/native/cuda/Embedding.cu#L264, add cudaGetLastError call after kernel launch. \n \n If launch parameters are invalid (too many blocks, too many threads etc) it will error out immediately.\n \n 2. Yes, I was talking about number of vectors. Rather than adding a loop, it may be easier to set gridDim.x to the number of output elements, because max value of gridDim.x is 2^31.",
      "y": "E.g. here https://github.com/pytorch/pytorch/blob/master/aten/src/ATen/native/cuda/Embedding.cu#L264, add cudaGetLastError call after kernel launch. \n \n If launch parameters are invalid (too many blocks, too many threads etc) it will error out immediately.\n \n 2. Yes, I was talking about number of vectors. Rather than adding a loop, it may be easier to set gridDim.x to the number of output elements, because max value of gridDim.x is 2^31."
   },
   {
      "x": "Can't open discuss.pytorch.org",
      "z": "I can not get discuss.pytorch.org via chrome on my mac pro but it is reachable for example by opera",
      "y": "I can not get discuss.pytorch.org via chrome on my mac pro but it is reachable for example by opera"
   },
   {
      "x": "[feature request] torch.cuda.is_supported()",
      "z": "torch.cuda._check_capability()",
      "y": "torch.cuda._check_capability()"
   },
   {
      "x": "`torch.hub` cannot download if branch name starts with `v`",
      "z": "cc @ailzhang",
      "y": "cc @ailzhang"
   },
   {
      "x": "c++ Frontend: example mnist hits nan",
      "z": "The NaN/instability issue is now fixed. The model converges successfully.",
      "y": "The NaN/instability issue is now fixed. The model converges successfully."
   },
   {
      "x": "[jit] the torch script and c++ api using",
      "z": "@szupzp indeed concatenation works. Otherwise, you could do the following to handle multiple outputs:\n \n ```\n \n ...\n \n auto outputs = module->forward(inputs).toTuple();\n \n torch::Tensor out1 = outputs->elements()[0].toTensor();\n \n torch::Tensor out2 = outputs->elements()[1].toTensor();\n \n ...\n \n ```",
      "y": "concatenation works. Otherwise, you could do the following to handle multiple outputs:\n \n ```\n \n ...\n \n auto outputs = module->forward(inputs).toTuple();\n \n torch::Tensor out1 = outputs->elements()[0].toTensor();\n \n torch::Tensor out2 = outputs->elements()[1].toTensor();\n \n ...\n \n ```"
   },
   {
      "x": "Feature Request: Nested state_dict (OrderedDict) for nested modules",
      "z": "We discussed this in the early days, when implementing `state_dict`. The conclusion was that we dont want to complicate the `state_dict` API by adding nesting (or other schemes that were under discussion).\n \n The nesting is already given in the keys of the dict, such as `layer1.layer2.layer3.conv1.weight`, and it's trivial to filter the keys according to the nesting you want.\n \n We wanted to keep it simple and stupid, and let users refilter / reorder as they want.",
      "y": "We discussed this in the early days, when implementing `state_dict`. The conclusion was that we dont want to complicate the `state_dict` API by adding nesting (or other schemes that were under discussion).\n \n The nesting is already given in the keys of the dict, such as `layer1.layer2.layer3.conv1.weight`, and it's trivial to filter the keys according to the nesting you want.\n \n We wanted to keep it simple and stupid, and let users refilter / reorder as they want."
   },
   {
      "x": "Migrate `gather` from the TH to Aten (CUDA)",
      "z": "You can do it with TensorIterator, using an approach similar to https://github.com/pytorch/pytorch/blob/master/aten/src/ATen/native/cuda/IndexKernel.cu. I'm concerned that it'll lead to regressions compared to THC, but you can experiment with maybe simplifying indexing computations. Directly copying approach that you used for cpu, with separating the indexed dimension into a loop is not going to work here, both because the remaining exposed parallelism won't be enough on the gpu, and because memory accesses from a single thread are going to be very inefficient. So my suggestion is, experiment with TensorIterator-like approaches for some time (trying to reproduce general THC access pattern with TI), but if that does not work, just port THC code. \n \n As for determinism, let's ignore that for now and just use atomic operations.",
      "y": "You can do it with TensorIterator, using an approach similar to https://github.com/pytorch/pytorch/blob/master/aten/src/ATen/native/cuda/IndexKernel.cu. I'm concerned that it'll lead to regressions compared to THC, but you can experiment with maybe simplifying indexing computations. Directly copying approach that you used for cpu, with separating the indexed dimension into a loop is not going to work here, both because the remaining exposed parallelism won't be enough on the gpu, and because memory accesses from a single thread are going to be very inefficient. So my suggestion is, experiment with TensorIterator-like approaches for some time (trying to reproduce general THC access pattern with TI), but if that does not work, just port THC code. \n \n As for determinism, let's ignore that for now and just use atomic operations."
   },
   {
      "x": "Migrate `index_select` from the TH to Aten (CUDA)",
      "z": "Actually, I'm going to start working on this now, since I'm now on idle in all my other tasks and it looks like kshitij12345 has not committed himself to this issue yet.",
      "y": "Actually, I'm going to start working on this now, since I'm now on idle in all my other tasks and it looks like kshitij12345 has not committed himself to this issue yet."
   },
   {
      "x": "PyTorch breaks Matplotlib",
      "z": "The issue could be reproduced on my system too.\n \n Ubuntu 16.04.2 LTS, Python version 2.7.12, PyTorch version 0.1.12_2, Matplotlib version 2.0.2\n \n ```\n \n Python 2.7.12 (default, Nov 19 2016, 06:48:10) \n \n [GCC 5.4.0 20160609] on linux2\n \n Type \"help\", \"copyright\", \"credits\" or \"license\" for more information.\n \n >>> import matplotlib\n \n >>> matplotlib.use('Agg')\n \n >>> import matplotlib.pyplot as plt\n \n >>> import torch\n \n >>> \n \n >>> fig = plt.figure()\n \n >>> plt.plot([1,2])\n \n [<matplotlib.lines.Line2D object at 0x7f160844bd50>]\n \n >>> fig.savefig('test.pdf')\n \n *** Error in `python': free(): invalid pointer: 0x00007f16361d6ac0 ***\n \n ======= Backtrace: =========\n \n /lib/x86_64-linux-gnu/libc.so.6(+0x777e5)[0x7f165ca1b7e5]\n \n /lib/x86_64-linux-gnu/libc.so.6(+0x8037a)[0x7f165ca2437a]\n \n /lib/x86_64-linux-gnu/libc.so.6(cfree+0x4c)[0x7f165ca2853c]\n \n /usr/lib/x86_64-linux-gnu/libstdc++.so.6(_ZNSt15basic_stringbufIcSt11char_traitsIcESaIcEE8overflowEi+0x181)[0x7f163a193fa1]\n \n /usr/lib/x86_64-linux-gnu/libstdc++.so.6(_ZNSt15basic_streambufIcSt11char_traitsIcEE6xsputnEPKcl+0x89)[0x7f163a1eae79]\n \n ```",
      "y": "The issue could be reproduced on my system too.\n \n Ubuntu 16.04.2 LTS, Python version 2.7.12, PyTorch version 0.1.12_2, Matplotlib version 2.0.2\n \n ```\n \n Python 2.7.12 (default, Nov 19 2016, 06:48:10) \n \n [GCC 5.4.0 20160609] on linux2\n \n Type \"help\", \"copyright\", \"credits\" or \"license\" for more information.\n \n >>> import matplotlib\n \n >>> matplotlib.use('Agg')\n \n >>> import matplotlib.pyplot as plt\n \n >>> import torch\n \n >>> \n \n >>> fig = plt.figure()\n \n >>> plt.plot([1,2])\n \n [<matplotlib.lines.Line2D object at 0x7f160844bd50>]\n \n >>> fig.savefig('test.pdf')\n \n *** Error in `python': free(): invalid pointer: 0x00007f16361d6ac0 ***\n \n ======= Backtrace: =========\n \n /lib/x86_64-linux-gnu/libc.so.6(+0x777e5)[0x7f165ca1b7e5]\n \n /lib/x86_64-linux-gnu/libc.so.6(+0x8037a)[0x7f165ca2437a]\n \n /lib/x86_64-linux-gnu/libc.so.6(cfree+0x4c)[0x7f165ca2853c]\n \n /usr/lib/x86_64-linux-gnu/libstdc++.so.6(_ZNSt15basic_stringbufIcSt11char_traitsIcESaIcEE8overflowEi+0x181)[0x7f163a193fa1]\n \n /usr/lib/x86_64-linux-gnu/libstdc++.so.6(_ZNSt15basic_streambufIcSt11char_traitsIcEE6xsputnEPKcl+0x89)[0x7f163a1eae79]\n \n ```"
   },
   {
      "x": "[Feature Request] PyTorch RNN State Dropout",
      "z": "what about now ?",
      "y": "what about now ?"
   },
   {
      "x": "libAten undefined symbol",
      "z": "Yeah, I found that removing the build and torch/lib/build folders and rebuilding fixed it. I do not know why the normal install procedure doesn't pick up the change.",
      "y": "Yeah, I found that removing the build and torch/lib/build folders and rebuilding fixed it. I do not know why the normal install procedure doesn't pick up the change."
   },
   {
      "x": "support slicing on nn.Sequential",
      "z": "I attempted getting a `slice` to work. Is this fine?\n \n ```\n \n def __getitem__(self, inp):\n \n  if isinstance(inp, slice):\n \n  ret_seq = Sequential()\n \n  seq_list = list(self._modules.items())\n \n  for i in range(*inp.indices(len(self._modules))):\n \n  ret_seq.add_module(seq_list[i][0], seq_list[i][1])\n \n  return ret_seq\n \n ```\n \n If this is fine, I can issue a PR too. Let me know what you think. I believe this modification needs to be done in the `Sequential` class.",
      "y": "I attempted getting a `slice` to work. Is this fine?\n \n ```\n \n def __getitem__(self, inp):\n \n  if isinstance(inp, slice):\n \n  ret_seq = Sequential()\n \n  seq_list = list(self._modules.items())\n \n  for i in range(*inp.indices(len(self._modules))):\n \n  ret_seq.add_module(seq_list[i][0], seq_list[i][1])\n \n  return ret_seq\n \n ```\n \n If this is fine, I can issue a PR too. Let me know what you think. I believe this modification needs to be done in the `Sequential` class."
   },
   {
      "x": "should weight norm only recompute weights at the beginning and after each call to backward?",
      "z": "Right, so in recurrent nets we're recomputing the `weight` multiple times, which is unnecessary and leads to extra memory usage.\n \n \n \n The backward hook solution isn't robust: `weight` needs to be re-computed if `weight_v` or `weight_g` changes, not after they're gradients are computed. For example:\n \n \n \n ```python\n \n output = module(input) # 1\n \n output.backward() \n \n output2 = module(input) # 2\n \n optimizer.step()\n \n output3 = module(input) # 3\n \n ```\n \n \n \n With the backward hook, weight would be recomputed at (2) when it should be recomputed at (3).",
      "y": "Right, so in recurrent nets we're recomputing the `weight` multiple times, which is unnecessary and leads to extra memory usage.\n \n \n \n The backward hook solution isn't robust: `weight` needs to be re-computed if `weight_v` or `weight_g` changes, not after they're gradients are computed. For example:\n \n \n \n ```python\n \n output = module(input) # 1\n \n output.backward() \n \n output2 = module(input) # 2\n \n optimizer.step()\n \n output3 = module(input) # 3\n \n ```\n \n \n \n With the backward hook, weight would be recomputed at (2) when it should be recomputed at (3)."
   },
   {
      "x": "Problem when using DataLoader with a 1-dimensional FloatTensor",
      "z": "We can solve this once we get scalars",
      "y": "We can solve this once we get scalars"
   },
   {
      "x": "Missing numpy dependency at installation",
      "z": "I've fixed this on master and it'll be correctly installed with the wheel files of the next release",
      "y": "I've fixed this on master and it'll be correctly installed with the wheel files of the next release"
   },
   {
      "x": "Memory leak problem in LSTM and RNN",
      "z": "You may refer to this example to get precise usage of `detach` in a long sequence:\n \n https://github.com/pytorch/examples/blob/master/word_language_model/main.py\n \n line132 `hidden = repackage_hidden(hidden)` does the same thing as `detach`.",
      "y": "You may refer to this example to get precise usage of `detach` in a long sequence:\n \n https://github.com/pytorch/examples/blob/master/word_language_model/main.py\n \n line132 `hidden = repackage_hidden(hidden)` does the same thing as `detach`."
   },
   {
      "x": "Issues with Loss1 + 0*Loss2 and graph computation",
      "z": "BCELoss accepts only inputs that have all elements in range [0; 1] but this condition doesn't hold in your case",
      "y": "BCELoss accepts only inputs that have all elements in range [0; 1] but this condition doesn't hold in your case"
   },
   {
      "x": "\"LogSoftmax can only be differentiated once\"",
      "z": "I'll take a look.",
      "y": "I'll take a look."
   },
   {
      "x": "when i use the max_unpool2d ,i got some error",
      "z": "Have a look a the [full documentation of MaxUnpool](http://pytorch.org/docs/master/nn.html#torch.nn.MaxUnpool2d). You first need to get the indices from `MaxPool`, and then pass those indices to `MaxUnpool`. For example:\n \n ```python\n \n out1, indices = F.max_pool2d(x, kernel_size=2, return_indices=True)\n \n result = F.max_unpool2d(out1, indices, kernel_size=2, stride=2, output_size=x.size())\n \n ```\n \n Note that you need to pass the `stride` parameter manually to `max_unpool2d` for the moment.\n \n We should change a bit how we handle the default in `max_unpool` so that we don't have to pass the `stride` manually. I'll send a fix for it.",
      "y": "Have a look a the [full documentation of MaxUnpool](http://pytorch.org/docs/master/nn.html#torch.nn.MaxUnpool2d). You first need to get the indices from `MaxPool`, and then pass those indices to `MaxUnpool`. For example:\n \n ```python\n \n out1, indices = F.max_pool2d(x, kernel_size=2, return_indices=True)\n \n result = F.max_unpool2d(out1, indices, kernel_size=2, stride=2, output_size=x.size())\n \n ```\n \n Note that you need to pass the `stride` parameter manually to `max_unpool2d` for the moment.\n \n We should change a bit how we handle the default in `max_unpool` so that we don't have to pass the `stride` manually. I'll send a fix for it."
   },
   {
      "x": "CUDA OOM for tiny Variable broadcast-add",
      "z": "It makes sense. Broadcasting will make the output be of size (500000, 500000, 1) because it aligns dims to the right (not to the left)\n \n ```\n \n 500000 1 1\n \n  500000 1\n \n ---------------------\n \n 500000 500000 1\n \n ```",
      "y": "It makes sense. Broadcasting will make the output be of size (500000, 500000, 1) because it aligns dims to the right (not to the left)\n \n ```\n \n 500000 1 1\n \n  500000 1\n \n ---------------------\n \n 500000 500000 1\n \n ```"
   },
   {
      "x": "where is the torch.nn.NLLLoss ?",
      "z": "`torch.nn.NLLLoss` is here:\n \n \n \n https://github.com/pytorch/pytorch/blob/master/torch/nn/modules/loss.py?hl=64#L64\n \n \n \n Like many modules, this module is just a thin wrapper around the corresponding function, in this case `nll_loss`:\n \n \n \n line 131: `return F.nll_loss(...)`\n \n \n \n Tracing the F import, we find:\n \n \n \n https://github.com/pytorch/pytorch/blob/master/torch/nn/functional.py#L644\n \n \n \n This function, in turn, calls:\n \n \n \n `return _functions.thnn.NLLLoss(...)`\n \n \n \n `_functions` is here:\n \n https://github.com/pytorch/pytorch/blob/master/torch/nn/_functions/thnn/loss.py\n \n \n \n But this is kind of a dead end, as there is no Python code implementing `NLLLoss`.\n \n \n \n Like many low-level functions, `NLLLoss` is implemented in C. The PyTorch C code is here:\n \n \n \n https://github.com/pytorch/pytorch/blob/master/torch/lib\n \n \n \n `torch.nn` implementations are in THNN, which leads to the actual implementation:\n \n \n \n https://github.com/pytorch/pytorch/blob/master/torch/lib/THNN/generic/ClassNLLCriterion.c#L5",
      "y": "`torch.nn.NLLLoss` is here:\n \n \n \n https://github.com/pytorch/pytorch/blob/master/torch/nn/modules/loss.py?hl=64#L64\n \n \n \n Like many modules, this module is just a thin wrapper around the corresponding function, in this case `nll_loss`:\n \n \n \n line 131: `return F.nll_loss(...)`\n \n \n \n Tracing the F import, we find:\n \n \n \n https://github.com/pytorch/pytorch/blob/master/torch/nn/functional.py#L644\n \n \n \n This function, in turn, calls:\n \n \n \n `return _functions.thnn.NLLLoss(...)`\n \n \n \n `_functions` is here:\n \n https://github.com/pytorch/pytorch/blob/master/torch/nn/_functions/thnn/loss.py\n \n \n \n But this is kind of a dead end, as there is no Python code implementing `NLLLoss`.\n \n \n \n Like many low-level functions, `NLLLoss` is implemented in C. The PyTorch C code is here:\n \n \n \n https://github.com/pytorch/pytorch/blob/master/torch/lib\n \n \n \n `torch.nn` implementations are in THNN, which leads to the actual implementation:\n \n \n \n https://github.com/pytorch/pytorch/blob/master/torch/lib/THNN/generic/ClassNLLCriterion.c#L5"
   },
   {
      "x": "PyTorch with numpy syntax?",
      "z": "> The Tensor API matches numpy's exactly\n \n \n \n This is, IMHO, the biggest stopper in teaching (otherwise close to perfect) pytorch as a first deep learning framework.\n \n Really looking forward to the day pytorch will match numpy API.",
      "y": "> The Tensor API matches numpy's exactly\n \n \n \n This is, IMHO, the biggest stopper in teaching (otherwise close to perfect) pytorch as a first deep learning framework.\n \n Really looking forward to the day pytorch will match numpy API."
   },
   {
      "x": "tensorflow conflicts with nn.DataParallel",
      "z": "It's too bad that Tensorflow changes the current device, but this is the expected PyTorch behavior. The model must be on `device_ids[0]`. `device_ids` defaults to `0, 1, 2, ...`. If you're model is on device 7, you must manually specify `device_ids`.\n \n \n \n Or, just set the current device after the TensorFlow call:\n \n \n \n ```python\n \n config = tf.ConfigProto()\n \n config.gpu_options.allow_growth = True\n \n with tf.Session(config=config) as sess:\n \n  sess.run(emb.initializer)\n \n \n \n torch.cuda.set_device(0) # set the device back to 0\n \n \n \n model = torch.nn.Linear(128, 1).cuda()\n \n model = torch.nn.DataParallel(model).cuda()\n \n \n \n data = Variable(torch.Tensor(8,128)).cuda()\n \n x = model(data)\n \n ```",
      "y": "It's too bad that Tensorflow changes the current device, but this is the expected PyTorch behavior. The model must be on `device_ids[0]`. `device_ids` defaults to `0, 1, 2, ...`. If you're model is on device 7, you must manually specify `device_ids`.\n \n \n \n Or, just set the current device after the TensorFlow call:\n \n \n \n ```python\n \n config = tf.ConfigProto()\n \n config.gpu_options.allow_growth = True\n \n with tf.Session(config=config) as sess:\n \n  sess.run(emb.initializer)\n \n \n \n torch.cuda.set_device(0) # set the device back to 0\n \n \n \n model = torch.nn.Linear(128, 1).cuda()\n \n model = torch.nn.DataParallel(model).cuda()\n \n \n \n data = Variable(torch.Tensor(8,128)).cuda()\n \n x = model(data)\n \n ```"
   },
   {
      "x": "Error installing pytorch vision",
      "z": "Try pip install with --no-deps \n \n Courtsey: https://discuss.pytorch.org/t/failed-instalation-with-pip3-for-torchvision/12667/6",
      "y": "Try pip install with --no-deps \n \n Courtsey: https://discuss.pytorch.org/t/failed-instalation-with-pip3-for-torchvision/12667/6"
   },
   {
      "x": "Error building torch from source for python 2.7 on macOS",
      "z": "going back to f1fd4ac7edeb2477404883b21c3c4fe8420c0757 makes the build work",
      "y": "going back to f1fd4ac7edeb2477404883b21c3c4fe8420c0757 makes the build work"
   },
   {
      "x": "Implementing hashing (2016 paper) to improve computational performance by 95%",
      "z": "questions and discussions are usually better at https://discuss.pytorch.org",
      "y": "questions and discussions are usually better at https://discuss.pytorch.org"
   },
   {
      "x": "linux python3.6 pip install command wrong",
      "z": "@sshuair we provide one package that has everything and supports GPU and CPU. This is better and consistent packaging in our opinion. This is not a typo/mistake.",
      "y": "we provide one package that has everything and supports GPU and CPU. This is better and consistent packaging in our opinion. This is not a typo/mistake."
   },
   {
      "x": "Bus error (core dumped) model share memory",
      "z": "Okay. I think I solved it. Looks like the shared memory of the docker container wasn't set high enough. Setting a higher amount by adding `--shm-size 8G` to the `docker run` command seems to be the trick as [mentioned here](https://github.com/pytorch/pytorch/issues/1355#issuecomment-308587289). Let me fully test it, if solved I'll close issue.",
      "y": "Okay. I think I solved it. Looks like the shared memory of the docker container wasn't set high enough. Setting a higher amount by adding `--shm-size 8G` to the `docker run` command seems to be the trick as [mentioned here](https://github.com/pytorch/pytorch/issues/1355#issuecomment-308587289). Let me fully test it, if solved I'll close issue."
   },
   {
      "x": "Fork start method is susceptible to deadlocks",
      "z": "`mp.set_start_method('spawn')` seems to resolve the issue.",
      "y": "`mp.set_start_method('spawn')` seems to resolve the issue."
   },
   {
      "x": "what is exactly batch_size in pytorch?",
      "z": "sample == example\n \n \n \n If you have 10 samples or examples in a batch, then the batch size is 10. Maximum batch_size is limited by the memory that your system has -- main memory in case of CPU and GPU memory if you are using the GPU. \n \n \n \n Also, please use the PyTorch discussion forum for questions like these, rather than opening an issue.",
      "y": "sample == example\n \n \n \n If you have 10 samples or examples in a batch, then the batch size is 10. Maximum batch_size is limited by the memory that your system has -- main memory in case of CPU and GPU memory if you are using the GPU. \n \n \n \n Also, please use the PyTorch discussion forum for questions like these, rather than opening an issue."
   },
   {
      "x": "Unable to use multiple GPUs",
      "z": "Please use forum (discuss.pytorch.org) for questions. DataParallel uses multiple GPUs only for forward method, if you want to use multiple GPUs for other methods, you have to manually do it.",
      "y": "Please use forum (discuss.pytorch.org) for questions. DataParallel uses multiple GPUs only for forward method, if you want to use multiple GPUs for other methods, you have to manually do it."
   },
   {
      "x": "Should I expect a performance difference between jit and native c++ models?",
      "z": "Yes, you should expect a performance difference. The C++ API has the same execution model as PyTorch eager, and re-uses the same infrastructure. It will execute your model code op by op, as you wrote it.\n \n \n \n The JIT has a domain-specific optimizing runtime, so in general you should expect your code to be faster than the C++ frontend. However, we're still early on in delivering perf wins using compilation techniques (it's a big focus for us for 2020), so I'd encourage you to benchmark different approaches for your workload.\n \n \n \n Going to close this issue to help with our triage process, but feel free to comment if you have followup questions!",
      "y": "Yes, you should expect a performance difference. The C++ API has the same execution model as PyTorch eager, and re-uses the same infrastructure. It will execute your model code op by op, as you wrote it.\n \n \n \n The JIT has a domain-specific optimizing runtime, so in general you should expect your code to be faster than the C++ frontend. However, we're still early on in delivering perf wins using compilation techniques (it's a big focus for us for 2020), so I'd encourage you to benchmark different approaches for your workload.\n \n \n \n Going to close this issue to help with our triage process, but feel free to comment if you have followup questions!"
   },
   {
      "x": "log cdf and log survival function of normal distribution",
      "z": "We have `.cdf` for most distributions, you can get what you want (e.g. for normal distribution) via `log_cdf = normal_dist.cdf(x).log()` and `log_sf = torch.log1p(-normal_dist.cdf(x))`.",
      "y": "We have `.cdf` for most distributions, you can get what you want (e.g. for normal distribution) via `log_cdf = normal_dist.cdf(x).log()` and `log_sf = torch.log1p(-normal_dist.cdf(x))`."
   },
   {
      "x": "Unbound local variable in LR scheduler",
      "z": "Indeed, this should remain opened for 1.",
      "y": "Indeed, this should remain opened for 1."
   },
   {
      "x": "LambdaLR type bug",
      "z": "I agree that the type should be updated. `lr_lambda` returns a float though. Did you mean the following?\n \n \n \n ```python\n \n LRLambdaType = Callable[[int], float]\n \n \n \n class LambdaLR(_LRScheduler):\n \n  def __init__(self, optimizer: Optimizer, lr_lambda: Union[LRLambdaType, List[float]], last_epoch: int=...) -> None: ...\n \n ```\n \n \n \n Please feel free to open a PR for this.",
      "y": "I agree that the type should be updated. `lr_lambda` returns a float though. Did you mean the following?\n \n \n \n ```python\n \n LRLambdaType = Callable[[int], float]\n \n \n \n class LambdaLR(_LRScheduler):\n \n  def __init__(self, optimizer: Optimizer, lr_lambda: Union[LRLambdaType, List[float]], last_epoch: int=...) -> None: ...\n \n ```\n \n \n \n Please feel free to open a PR for this."
   },
   {
      "x": "Missing default value in torch/optim/lr_scheduler.pyi",
      "z": "> Thanks for pointing this out. Please feel free to open a PR for this.\n \n \n \n @vincentqb I created one. #32647 Would you mind if you review my PR?",
      "y": "> Thanks for pointing this out. Please feel free to open a PR for this.\n "
   },
   {
      "x": "[Question]Pipeline batches inferencing",
      "z": "Hey @adizhol \n \n \n \n > The idea is to start the calculations of the next batch while the previous batch is still calculating\n \n \n \n If you are looking for GPipe-like parallelism, yes, it is possible and it's quite simple. Checkout this [tutorial](https://pytorch.org/tutorials/intermediate/model_parallel_tutorial.html).\n \n \n \n The paper you cited seems to do it differently though:\n \n \n \n > we overwrite network activations whenever new ones, i.e., from new frames, become available.\n \n Such a more gradual accumulation of information from both passes breaks the precise correspondence between gradients and activations, leading to theoretically more noisy weight updates.\n \n \n \n I haven't read the full paper, but looks like they could use last batch's gradients and this batch's activations to update parameters? Is that correct?",
      "y": " \n > The idea is to start the calculations of the next batch while the previous batch is still calculating\n \n \n \n If you are looking for GPipe-like parallelism, yes, it is possible and it's quite simple. Checkout this [tutorial](https://pytorch.org/tutorials/intermediate/model_parallel_tutorial.html).\n \n \n \n The paper you cited seems to do it differently though:\n \n \n \n > we overwrite network activations whenever new ones, i.e., from new frames, become available.\n \n Such a more gradual accumulation of information from both passes breaks the precise correspondence between gradients and activations, leading to theoretically more noisy weight updates.\n \n \n \n I haven't read the full paper, but looks like they could use last batch's gradients and this batch's activations to update parameters? Is that correct?"
   },
   {
      "x": "RuntimeError: iter.device(arg).is_cuda() INTERNAL ASSERT FAILED at /pytorch/aten/src/ATen/native/cuda/Loops.cuh:197",
      "z": "Same problem if QAT is performed with 'fbgemm' parameters\n \n ```\n \n model.qconfig = torch.quantization.get_default_qat_qconfig('fbgemm')\n \n torch.backends.quantized.engine = 'fbgemm'\n \n ```\n \n But it works with 'qnnpack' parameters\n \n \n \n The error is following:\n \n ```\n \n Traceback (most recent call last):\n \n  File \"train_net.py\", line 120, in <module>\n \n  args=(args,),\n \n  File \"/root/some_detectron2/detectron2/engine/launch.py\", line 52, in launch\n \n  main_func(*args)\n \n  File \"train_net.py\", line 78, in main\n \n  return trainer.train()\n \n  File \"/root/some_detectron2/detectron2/engine/defaults.py\", line 380, in train\n \n  super().train(self.start_iter, self.max_iter)\n \n  File \"/root/some_detectron2/detectron2/engine/train_loop.py\", line 132, in train\n \n  self.run_step()\n \n  File \"/root/some_detectron2/detectron2/engine/train_loop.py\", line 215, in run_step\n \n  loss_dict = self.model(data)\n \n  File \"/root/anaconda2/envs/pytorch-gpu/lib/python3.7/site-packages/torch/nn/modules/module.py\", line 550, in __call__\n \n  result = self.forward(*input, **kwargs)\n \n  File \"/root/some_detectron2/detectron2/modeling/meta_arch/rcnn.py\", line 121, in forward\n \n  features = self.backbone(images.tensor)\n \n  File \"/root/anaconda2/envs/pytorch-gpu/lib/python3.7/site-packages/torch/nn/modules/module.py\", line 550, in __call__\n \n  result = self.forward(*input, **kwargs)\n \n  File \"/root/DensePose_ADASE/densepose/modeling/quantize.py\", line 177, in new_forward\n \n  p5, p4, p3, p2 = self.bottom_up(x) # top->down\n \n  File \"/root/anaconda2/envs/pytorch-gpu/lib/python3.7/site-packages/torch/nn/modules/module.py\", line 550, in __call__\n \n  result = self.forward(*input, **kwargs)\n \n  File \"/root/DensePose_ADASE/densepose/modeling/quantize.py\", line 130, in new_forward\n \n  return old_forward(self, x)\n \n  File \"/root/anaconda2/envs/pytorch-gpu/lib/python3.7/site-packages/timm/models/efficientnet.py\", line 350, in forward\n \n  x = self.conv_stem(x)\n \n  File \"/root/anaconda2/envs/pytorch-gpu/lib/python3.7/site-packages/torch/nn/modules/module.py\", line 550, in __call__\n \n  result = self.forward(*input, **kwargs)\n \n  File \"/root/anaconda2/envs/pytorch-gpu/lib/python3.7/site-packages/torch/nn/intrinsic/qat/modules/conv_fused.py\", line 243, in forward\n \n  return self.activation_post_process(F.relu(ConvBn2d._forward(self, input)))\n \n  File \"/root/anaconda2/envs/pytorch-gpu/lib/python3.7/site-packages/torch/nn/intrinsic/qat/modules/conv_fused.py\", line 95, in _forward\n \n  conv = self._conv_forward(input, self.weight_fake_quant(scaled_weight))\n \n  File \"/root/anaconda2/envs/pytorch-gpu/lib/python3.7/site-packages/torch/nn/modules/module.py\", line 550, in __call__\n \n  result = self.forward(*input, **kwargs)\n \n  File \"/root/anaconda2/envs/pytorch-gpu/lib/python3.7/site-packages/torch/quantization/fake_quantize.py\", line 86, in forward\n \n  self.ch_axis, self.quant_min, self.quant_max)\n \n RuntimeError: iter.device(arg).is_cuda() INTERNAL ASSERT FAILED at /opt/conda/conda-bld/pytorch_1587428266983/work/aten/src/ATen/native/cuda/Loops.cuh:56, please report a bug to PyTorch.\n \n ```\n \n \n \n CUDA: 10.2\n \n PyTorch: py3.7_cuda10.2.89_cudnn7.6.5_0\n \n OS: Ubuntu 18",
      "y": "Same problem if QAT is performed with 'fbgemm' parameters\n \n ```\n \n model.qconfig = torch.quantization.get_default_qat_qconfig('fbgemm')\n \n torch.backends.quantized.engine = 'fbgemm'\n \n ```\n \n But it works with 'qnnpack' parameters\n \n \n \n The error is following:\n \n ```\n \n Traceback (most recent call last):\n \n  File \"train_net.py\", line 120, in <module>\n \n  args=(args,),\n \n  File \"/root/some_detectron2/detectron2/engine/launch.py\", line 52, in launch\n \n  main_func(*args)\n \n  File \"train_net.py\", line 78, in main\n \n  return trainer.train()\n \n  File \"/root/some_detectron2/detectron2/engine/defaults.py\", line 380, in train\n \n  super().train(self.start_iter, self.max_iter)\n \n  File \"/root/some_detectron2/detectron2/engine/train_loop.py\", line 132, in train\n \n  self.run_step()\n \n  File \"/root/some_detectron2/detectron2/engine/train_loop.py\", line 215, in run_step\n \n  loss_dict = self.model(data)\n \n  File \"/root/anaconda2/envs/pytorch-gpu/lib/python3.7/site-packages/torch/nn/modules/module.py\", line 550, in __call__\n \n  result = self.forward(*input, **kwargs)\n \n  File \"/root/some_detectron2/detectron2/modeling/meta_arch/rcnn.py\", line 121, in forward\n \n  features = self.backbone(images.tensor)\n \n  File \"/root/anaconda2/envs/pytorch-gpu/lib/python3.7/site-packages/torch/nn/modules/module.py\", line 550, in __call__\n \n  result = self.forward(*input, **kwargs)\n \n  File \"/root/DensePose_ADASE/densepose/modeling/quantize.py\", line 177, in new_forward\n \n  p5, p4, p3, p2 = self.bottom_up(x) # top->down\n \n  File \"/root/anaconda2/envs/pytorch-gpu/lib/python3.7/site-packages/torch/nn/modules/module.py\", line 550, in __call__\n \n  result = self.forward(*input, **kwargs)\n \n  File \"/root/DensePose_ADASE/densepose/modeling/quantize.py\", line 130, in new_forward\n \n  return old_forward(self, x)\n \n  File \"/root/anaconda2/envs/pytorch-gpu/lib/python3.7/site-packages/timm/models/efficientnet.py\", line 350, in forward\n \n  x = self.conv_stem(x)\n \n  File \"/root/anaconda2/envs/pytorch-gpu/lib/python3.7/site-packages/torch/nn/modules/module.py\", line 550, in __call__\n \n  result = self.forward(*input, **kwargs)\n \n  File \"/root/anaconda2/envs/pytorch-gpu/lib/python3.7/site-packages/torch/nn/intrinsic/qat/modules/conv_fused.py\", line 243, in forward\n \n  return self.activation_post_process(F.relu(ConvBn2d._forward(self, input)))\n \n  File \"/root/anaconda2/envs/pytorch-gpu/lib/python3.7/site-packages/torch/nn/intrinsic/qat/modules/conv_fused.py\", line 95, in _forward\n \n  conv = self._conv_forward(input, self.weight_fake_quant(scaled_weight))\n \n  File \"/root/anaconda2/envs/pytorch-gpu/lib/python3.7/site-packages/torch/nn/modules/module.py\", line 550, in __call__\n \n  result = self.forward(*input, **kwargs)\n \n  File \"/root/anaconda2/envs/pytorch-gpu/lib/python3.7/site-packages/torch/quantization/fake_quantize.py\", line 86, in forward\n \n  self.ch_axis, self.quant_min, self.quant_max)\n \n RuntimeError: iter.device(arg).is_cuda() INTERNAL ASSERT FAILED at /opt/conda/conda-bld/pytorch_1587428266983/work/aten/src/ATen/native/cuda/Loops.cuh:56, please report a bug to PyTorch.\n \n ```\n \n \n \n CUDA: 10.2\n \n PyTorch: py3.7_cuda10.2.89_cudnn7.6.5_0\n \n OS: Ubuntu 18"
   },
   {
      "x": "Quantized weights cant be loaded, when tried, generate \"copy_\" not implemented for \\'QInt8' exception",
      "z": "@Coderx7 seems like when you load the quantized state dict you are trying to load it into the float model. You will first have to call `prepare_qat(m,..)` followed by `m = convert(m, ..)` and then call `m.load_state_dict('quantized_state_dict')`\n \n \n \n Closing this issue since it isn't a bug, if there is a further issue please post in the discussion forum.",
      "y": "@Coderx7 seems like when you load the quantized state dict you are trying to load it into the float model. You will first have to call `prepare_qat(m,..)` followed by `m = convert(m, ..)` and then call `m.load_state_dict('quantized_state_dict')`\n \n \n \n Closing this issue since it isn't a bug, if there is a further issue please post in the discussion forum."
   },
   {
      "x": "Can't build documentation on master",
      "z": "It looks like [that feature](https://www.sphinx-doc.org/en/master/usage/extensions/autodoc.html#skipping-members) already exists. I found a [simple example](https://stackoverflow.com/a/21449475) of how to include this, although it would need to check if distributed is available.",
      "y": "It looks like [that feature](https://www.sphinx-doc.org/en/master/usage/extensions/autodoc.html#skipping-members) already exists. I found a [simple example](https://stackoverflow.com/a/21449475) of how to include this, although it would need to check if distributed is available."
   },
   {
      "x": "Failure on non-contiguous gradients for F.pad/permute combination",
      "z": "I'm optimistically closing this because it seems fixed on master. Please feel free to reopen if this is not the case.",
      "y": "I'm optimistically closing this because it seems fixed on master. Please feel free to reopen if this is not the case."
   },
   {
      "x": "torch.prod with internal upcasting (fp16 input, dtype=torch.float32 output) produces garbage",
      "z": "Putting high priority because of silent wrong answer. \n \n The root cause is likely incorrect `prod_kernel_impl`, note there's no out_t, compared to working `sum_kernel_impl`\n \n ```\n \n template <typename scalar_t, typename acc_t=scalar_t, typename out_t=scalar_t>\n \n void sum_kernel_impl(TensorIterator& iter) {\n \n  gpu_reduce_kernel<scalar_t, out_t>(iter, func_wrapper<out_t> ([]GPU_LAMBDA(acc_t a, acc_t b) -> acc_t {\n \n  return a + b;\n \n  }));\n \n }\n \n template <typename scalar_t, typename acc_t=scalar_t>\n \n void prod_kernel_impl(TensorIterator& iter) {\n \n  gpu_reduce_kernel<scalar_t, scalar_t>(iter, func_wrapper<scalar_t> ([]GPU_LAMBDA(acc_t a, acc_t b) -> acc_t {\n \n  return a * b;\n \n  }), 1);\n \n }\n \n ```",
      "y": "Putting high priority because of silent wrong answer. \n \n The root cause is likely incorrect `prod_kernel_impl`, note there's no out_t, compared to working `sum_kernel_impl`\n \n ```\n \n template <typename scalar_t, typename acc_t=scalar_t, typename out_t=scalar_t>\n \n void sum_kernel_impl(TensorIterator& iter) {\n \n  gpu_reduce_kernel<scalar_t, out_t>(iter, func_wrapper<out_t> ([]GPU_LAMBDA(acc_t a, acc_t b) -> acc_t {\n \n  return a + b;\n \n  }));\n \n }\n \n template <typename scalar_t, typename acc_t=scalar_t>\n \n void prod_kernel_impl(TensorIterator& iter) {\n \n  gpu_reduce_kernel<scalar_t, scalar_t>(iter, func_wrapper<scalar_t> ([]GPU_LAMBDA(acc_t a, acc_t b) -> acc_t {\n \n  return a * b;\n \n  }), 1);\n \n }\n \n ```"
   },
   {
      "x": "Bizarre \"no kernel image\" error for pytorch built from source",
      "z": "det is implemented in MAGMA, so this might be related to how you compiled against magma. Could you please run the [environment collection script](https://raw.githubusercontent.com/pytorch/pytorch/master/torch/utils/collect_env.py)",
      "y": "det is implemented in MAGMA, so this might be related to how you compiled against magma. Could you please run the [environment collection script](https://raw.githubusercontent.com/pytorch/pytorch/master/torch/utils/collect_env.py)"
   },
   {
      "x": "Pytorch 1.4.0 ARM build failing",
      "z": "The problem was the docker container running low on memory and gcc was crashing. Increasing memory in docker desktop on mac to 8GB solved it.",
      "y": "The problem was the docker container running low on memory and gcc was crashing. Increasing memory in docker desktop on mac to 8GB solved it."
   },
   {
      "x": "Windows nightly CPU build failed",
      "z": "Seems to be resolved by #32116, #32114 and #32112 according to https://ezyang.github.io/pytorch-ci-hud/build/pytorch-master. Could you please confirm, cc @ezyang?",
      "y": "Seems to be resolved by #32116, #32114 and #32112 according to https://ezyang.github.io/pytorch-ci-hud/build/pytorch-master."
   },
   {
      "x": "Incorrect typing errors",
      "z": "This is all fixed (3 is a user code snippet that needs changing as in the comment above), so closing.\n \n \n \n gh-38062 replaces `is_tensor` usages in this repo in anticipation of adding mypy typechecking on that code, to avoid potential issues.",
      "y": "This is all fixed (3 is a user code snippet that needs changing as in the comment above), so closing.\n \n \n \n gh-38062 replaces `is_tensor` usages in this repo in anticipation of adding mypy typechecking on that code, to avoid potential issues."
   },
   {
      "x": "Autograd fails if used before multiprocessing Pool",
      "z": "I am not too familiar with C++ so better someone else takes a look, sorry.\n \n Definitely worth adding some kind of error message as you suggested.",
      "y": "I am not too familiar with C++ so better someone else takes a look, sorry.\n \n Definitely worth adding some kind of error message as you suggested."
   },
   {
      "x": "Improve docs search engine indexing",
      "z": "The \"single page per module\" strategy makes it VERY HARD to search for Pytorch documentation since Google will just redirect you to the top of the page, then you have to hit `Ctrl + f` and type the term again (try finding `nn.Linear` which appears 23 times).\n \n \n \n Why not break it into a page per function / class like most other libraries (numpy, pandas, sklearn, etc)? It would reduce the load time problem discussed in #20984 by virtue of just having to process less content and it would make its content easier to find.",
      "y": "The \"single page per module\" strategy makes it VERY HARD to search for Pytorch documentation since Google will just redirect you to the top of the page, then you have to hit `Ctrl + f` and type the term again (try finding `nn.Linear` which appears 23 times).\n \n \n \n Why not break it into a page per function / class like most other libraries (numpy, pandas, sklearn, etc)? It would reduce the load time problem discussed in #20984 by virtue of just having to process less content and it would make its content easier to find."
   },
   {
      "x": "Failed to run 'bash ../tools/build_pytorch_libs.sh --use-cuda --use-fbgemm --use-nnpack --use-mkldnn --use-qnnpack caffe2'",
      "z": "@CF2220160244, could you change this line \n \n https://github.com/pytorch/pytorch/blob/master/caffe2/quantization/server/activation_distribution_observer.cc#L500\n \n to \n \n ```\n \n ist.str(first_line);\n \n ```\n \n And see if it works for you?",
      "y": "could you change this line \n \n https://github.com/pytorch/pytorch/blob/master/caffe2/quantization/server/activation_distribution_observer.cc#L500\n \n to \n \n ```\n \n ist.str(first_line);\n \n ```\n \n And see if it works for you?"
   },
   {
      "x": "Libtorch C++ load model error on windows10",
      "z": "@peterjc123 Thanks Peter! A prebuild pdb version would save many peoples time for quick debugging. \n \n \n \n For my case running on Window (which might be similar to @hset911), it cashes at top level (the sample example on your site) on this line:\n \n \n \n  `at::Tensor output = module->forward(inputs).toTensor();`\n \n \n \n Internally crashed at constants.cpp:\n \n \n \n ```cpp\n \n c10::optional<IValue> toIValue(const Value* v) {\n \n  if (v->node()->kind() != prim::Constant) {\n \n  return c10::nullopt;\n \n  }\n \n  // use implemenation of prim::Constant to compute the output IValue\n \n  auto op = getOperation(v->node());\n \n  Stack stack;\n \n  op(stack);\n \n  return stack.back(); <------------ here (due to the stack is empty)\n \n }\n \n ```\n \n \n \n Callstack:\n \n > torch.dll!torch::jit::toIValue(const torch::jit::Value * v) Line 164 C++\n \n   torch.dll!torch::jit::Node::get(c10::Symbol name) Line 640 C++\n \n   torch.dll!torch::jit::Node::is_constant(c10::Symbol name) Line 368 C++\n \n   torch.dll!torch::jit::Node::matches(const char * signature_literal, c10::ArrayRef<c10::Symbol> const_inputs) Line 653 C++\n \n   torch.dll!torch::jit::PeepholeOptimizeImpl(torch::jit::Block * block, bool addmm_fusion_enabled) Line 65 C++\n \n   torch.dll!torch::jit::PeepholeOptimize(torch::jit::Block * block, bool addmm_fusion_enabled) Line 175 C++\n \n   torch.dll!torch::jit::PeepholeOptimize(const std::shared_ptr<torch::jit::Graph> & graph, bool addmm_fusion_enabled) Line 182 C++\n \n   torch.dll!torch::jit::GraphExecutorImpl::runOptimization(std::shared_ptr<torch::jit::Graph> & graph, const torch::jit::ArgumentSpec & spec) Line 503 C++\n \n   torch.dll!torch::jit::GraphExecutorImpl::compileSpec(const torch::jit::ArgumentSpec & spec) Line 473 C++\n \n   torch.dll!torch::jit::GraphExecutorImpl::getOrCompile(const std::vector<c10::IValue,std::allocator<c10::IValue> > & stack) Line 440 C++\n \n   torch.dll!torch::jit::GraphExecutorImpl::run(std::vector<c10::IValue,std::allocator<c10::IValue> > & stack) Line 374 C++\n \n   torch.dll!torch::jit::GraphExecutor::run(std::vector<c10::IValue,std::allocator<c10::IValue> > & inputs) Line 610 C++\n \n   example-app.exe!torch::jit::script::Method::run(std::vector<c10::IValue,std::allocator<c10::IValue> > & stack) Line 72 C++\n \n   example-app.exe!torch::jit::script::Method::operator()(std::vector<c10::IValue,std::allocator<c10::IValue> > stack) Line 77 C++\n \n   example-app.exe!torch::jit::script::Module::forward(std::vector<c10::IValue,std::allocator<c10::IValue> > inputs) Line 384 C++\n \n   example-app.exe!main(int argc, const char * * argv) Line 45 C++\n \n \n \n It's real cause is due to the line above it op(stack). \n \n The lambda operator returned which supposed to do a push onto the stack but failed (in constant.cpp):\n \n \n \n  return [i](Stack& stack) {\n \n  push(stack, i);\n \n  return 0;\n \n  };\n \n \n \n The push function doesn't work for the above case where only pushing one element on the stack. Pushing more elements like push(stack, 1, 2) worked fine. But not push(stack, 1). Probably due to a combination of this particular implementation and VS compiler (VS 15.6.6).\n \n \n \n In jit\\stack.h:\n \n \n \n ```cpp\n \n template <typename... Types>\n \n static inline void push(Stack& stack, Types&&... args) {\n \n  constexpr size_t N = sizeof...(args);\n \n  int result[N] = {(stack.emplace_back(std::forward<Types>(args)), 0)...};\n \n  (void)result;\n \n }\n \n ```\n \n \n \n My colleague Evan who's an expert in c11 suggested trying to change the implementation to the following:\n \n \n \n ```cpp\n \n template <typename... Types>\n \n static inline void push(Stack& stack, Types&&... args) {\n \n  std::initializer_list<int>{(stack.emplace_back(std::forward<Types>(args)), 0)...};\n \n }\n \n ```\n \n \n \n Which looks cleaner and worked great on Window! \n \n So posted here, in case, it's a potential fix for others.",
      "y": "A prebuild pdb version would save many peoples time for quick debugging. \n \n \n \nrunning on Window (which might be similar to @hset911), it cashes at top level (the sample example on your site) on this line:\n \n \n \n  `at::Tensor output = module->forward(inputs).toTensor();`\n \n \n \n Internally crashed at constants.cpp:\n \n \n \n ```cpp\n \n c10::optional<IValue> toIValue(const Value* v) {\n \n  if (v->node()->kind() != prim::Constant) {\n \n  return c10::nullopt;\n \n  }\n \n  // use implemenation of prim::Constant to compute the output IValue\n \n  auto op = getOperation(v->node());\n \n  Stack stack;\n \n  op(stack);\n \n  return stack.back(); <------------ here (due to the stack is empty)\n \n }\n \n ```\n \n \n \n Callstack:\n \n > torch.dll!torch::jit::toIValue(const torch::jit::Value * v) Line 164 C++\n \n   torch.dll!torch::jit::Node::get(c10::Symbol name) Line 640 C++\n \n   torch.dll!torch::jit::Node::is_constant(c10::Symbol name) Line 368 C++\n \n   torch.dll!torch::jit::Node::matches(const char * signature_literal, c10::ArrayRef<c10::Symbol> const_inputs) Line 653 C++\n \n   torch.dll!torch::jit::PeepholeOptimizeImpl(torch::jit::Block * block, bool addmm_fusion_enabled) Line 65 C++\n \n   torch.dll!torch::jit::PeepholeOptimize(torch::jit::Block * block, bool addmm_fusion_enabled) Line 175 C++\n \n   torch.dll!torch::jit::PeepholeOptimize(const std::shared_ptr<torch::jit::Graph> & graph, bool addmm_fusion_enabled) Line 182 C++\n \n   torch.dll!torch::jit::GraphExecutorImpl::runOptimization(std::shared_ptr<torch::jit::Graph> & graph, const torch::jit::ArgumentSpec & spec) Line 503 C++\n \n   torch.dll!torch::jit::GraphExecutorImpl::compileSpec(const torch::jit::ArgumentSpec & spec) Line 473 C++\n \n   torch.dll!torch::jit::GraphExecutorImpl::getOrCompile(const std::vector<c10::IValue,std::allocator<c10::IValue> > & stack) Line 440 C++\n \n   torch.dll!torch::jit::GraphExecutorImpl::run(std::vector<c10::IValue,std::allocator<c10::IValue> > & stack) Line 374 C++\n \n   torch.dll!torch::jit::GraphExecutor::run(std::vector<c10::IValue,std::allocator<c10::IValue> > & inputs) Line 610 C++\n \n   example-app.exe!torch::jit::script::Method::run(std::vector<c10::IValue,std::allocator<c10::IValue> > & stack) Line 72 C++\n \n   example-app.exe!torch::jit::script::Method::operator()(std::vector<c10::IValue,std::allocator<c10::IValue> > stack) Line 77 C++\n \n   example-app.exe!torch::jit::script::Module::forward(std::vector<c10::IValue,std::allocator<c10::IValue> > inputs) Line 384 C++\n \n   example-app.exe!main(int argc, const char * * argv) Line 45 C++\n \n \n \n It's real cause is due to the line above it op(stack). \n \n The lambda operator returned which supposed to do a push onto the stack but failed (in constant.cpp):\n \n \n \n  return [i](Stack& stack) {\n \n  push(stack, i);\n \n  return 0;\n \n  };\n \n \n \n The push function doesn't work for the above case where only pushing one element on the stack. Pushing more elements like push(stack, 1, 2) worked fine. But not push(stack, 1). Probably due to a combination of this particular implementation and VS compiler (VS 15.6.6).\n \n \n \n In jit\\stack.h:\n \n \n \n ```cpp\n \n template <typename... Types>\n \n static inline void push(Stack& stack, Types&&... args) {\n \n  constexpr size_t N = sizeof...(args);\n \n  int result[N] = {(stack.emplace_back(std::forward<Types>(args)), 0)...};\n \n  (void)result;\n \n }\n \n ```\n \n \n \n My colleague Evan who's an expert in c11 suggested trying to change the implementation to the following:\n \n \n \n ```cpp\n \n template <typename... Types>\n \n static inline void push(Stack& stack, Types&&... args) {\n \n  std::initializer_list<int>{(stack.emplace_back(std::forward<Types>(args)), 0)...};\n \n }\n \n ```\n \n \n \n Which looks cleaner and worked great on Window! \n \n So posted here, in case, it's a potential fix for others."
   },
   {
      "x": "Inconsistent behaviour of torch.fmod on CPU and GPU",
      "z": "Seems like using % for ints is reasonable and matches numpy. For example:\n \n ```\n \n # 2**54+1 can't be represented as a double without loss of precision:\n \n >>> 2**54+1\n \n 18014398509481985\n \n >>> int(float(2**54+1))\n \n 18014398509481984\n \n \n \n # but np calculates the \"fmod\" correctly without converting to double.\n \n >>> a=np.array([2**54+1, 2**54+1])\n \n >>> np.fmod(a,2)\n \n array([1, 1])\n \n >>> np.fmod(a.astype(np.double),2)\n \n array([0., 0.])\n \n ```",
      "y": "Seems like using % for ints is reasonable and matches numpy. For example:\n \n ```\n \n # 2**54+1 can't be represented as a double without loss of precision:\n \n >>> 2**54+1\n \n 18014398509481985\n \n >>> int(float(2**54+1))\n \n 18014398509481984\n \n \n \n # but np calculates the \"fmod\" correctly without converting to double.\n \n >>> a=np.array([2**54+1, 2**54+1])\n \n >>> np.fmod(a,2)\n \n array([1, 1])\n \n >>> np.fmod(a.astype(np.double),2)\n \n array([0., 0.])\n \n ```"
   },
   {
      "x": "Cannot use LBFGS in C++ (clang, torch 1.0.0)",
      "z": "I fixed this problem using a `NoGradGuard` in LBFGS before the parameter update, I'll submit a pull request soon",
      "y": "I fixed this problem using a `NoGradGuard` in LBFGS before the parameter update, I'll submit a pull request soon"
   },
   {
      "x": "torch.utils.data.dataloader doesn't support multiprocessing with multiple workers",
      "z": "It\u00e2\u20ac\u2122s impossible to set your subprocess as daemonic and also ask to use multiprocessing workers. You should either not set your subprocess as daemonic or use num_workers=0. This isn\u00e2\u20ac\u2122t a bug at all.",
      "y": "It\u00e2\u20ac\u2122s impossible to set your subprocess as daemonic and also ask to use multiprocessing workers. You should either not set your subprocess as daemonic or use num_workers=0. This isn\u00e2\u20ac\u2122t a bug at all."
   },
   {
      "x": "[jit] `for in` for lists",
      "z": "It would also be nice to support `enumerate` and `zip`, they make working with python lists a better experience",
      "y": "It would also be nice to support `enumerate` and `zip`, they make working with python lists a better experience"
   },
   {
      "x": "Product operation along some axis",
      "z": "Thank you for the suggestion. We do have `torch.prod`. Is there anything that you find lacking with it?",
      "y": "Thank you for the suggestion. We do have `torch.prod`. Is there anything that you find lacking with it?"
   },
   {
      "x": "[Caffe2] Enable Windows build with MKL-DNN",
      "z": "I opened an issue at ideep: https://github.com/intel/ideep/issues/35. Hope that they will support ideep on Windows.",
      "y": "I opened an issue at ideep: https://github.com/intel/ideep/issues/35. Hope that they will support ideep on Windows."
   },
   {
      "x": "Recurrent dropout",
      "z": "Similar to https://github.com/pytorch/pytorch/issues/9572",
      "y": "Similar to https://github.com/pytorch/pytorch/issues/9572"
   },
   {
      "x": "Issues with tutorial \u00e2\u20ac\u0153Installing C++ Distributions of PyTorch\u00e2\u20ac\u009d",
      "z": "If you use MSVC, then the alternative command for make is `msbuild` (or `ninja` if you use Ninja-build). The complete command is `msbuild INSTALL.vcxproj /p:Configuration=Release`, which is used in the script https://github.com/pytorch/pytorch/blob/master/tools/build_pytorch_libs.bat#L118.",
      "y": "If you use MSVC, then the alternative command for make is `msbuild` (or `ninja` if you use Ninja-build). The complete command is `msbuild INSTALL.vcxproj /p:Configuration=Release`, which is used in the script https://github.com/pytorch/pytorch/blob/master/tools/build_pytorch_libs.bat#L118."
   },
   {
      "x": "No error when selecting a stream that belongs to a different device",
      "z": "As a data point, in the current C++ API for CUDAStreamGuard, if you pass a stream that lives on a different device, we change *both* the device and the stream.",
      "y": "As a data point, in the current C++ API for CUDAStreamGuard, if you pass a stream that lives on a different device, we change *both* the device and the stream."
   },
   {
      "x": "More readable error message for index error of nn.Embedding in CUDA",
      "z": "It's complicated; if we do specific error checks then the code becomes much slower because we'd have to launch a kernel just to do the error checking.",
      "y": "It's complicated; if we do specific error checks then the code becomes much slower because we'd have to launch a kernel just to do the error checking."
   },
   {
      "x": "torch.nn.functional.glu is wrong ?",
      "z": "I came back to this GLU many years after I implemented it. The figure in the original paper is confusing, and there is no \"split\" for the inputs. Essentially you do two independent convolutions for the same input, get two outputs of the same shape, do sigmoid activation for one of the outputs, then multiply these two together. See [my explanations on GLU](https://leimao.github.io/blog/Gated-Linear-Units/).",
      "y": "I came back to this GLU many years after I implemented it. The figure in the original paper is confusing, and there is no \"split\" for the inputs. Essentially you do two independent convolutions for the same input, get two outputs of the same shape, do sigmoid activation for one of the outputs, then multiply these two together. See [my explanations on GLU](https://leimao.github.io/blog/Gated-Linear-Units/)."
   },
   {
      "x": "Cublas run time error with RTX 2080Ti with Cuda 9.0",
      "z": "RTX 2080Ti needs CUDA10 version of PyTorch to be installed, not CUDA9.\n \n That's likely the reason for the error.",
      "y": "RTX 2080Ti needs CUDA10 version of PyTorch to be installed, not CUDA9.\n \n That's likely the reason for the error."
   },
   {
      "x": "F.conv2d throws C++ side error when shape of weights and input mismatch",
      "z": "this is fixed on master, and will be part of the next release.\n \n \n \n The cause of this issue is a libmkldnn.so on your machine that is of a different version than what PyTorch expects.",
      "y": "this is fixed on master, and will be part of the next release.\n \n \n \n The cause of this issue is a libmkldnn.so on your machine that is of a different version than what PyTorch expects."
   },
   {
      "x": "[RFC] Integration with MKL-DNN",
      "z": "We (Intel Pytorch team) have a couple of discussions with @bddppq on whether MKL-DNN op should be visible to user and how MKL-DNN op should be implemented in Pytorch C10 backend. We summarize the discussion below to capture the motivation and directions on the high level design. Your feedback is welcome. \n \n \n \n ====\n \n The Pytorch C10 design would like to make MKL-DNN operation visible to user. So if user wants to use MKL-DNN operations, user needs to explicitly convert cpu tensor to MKL-DNN tensor, and vice versa. The motivation of explicit user conversion is to support performance debugging. If the conversion is done implicitly in the C10 ops/runtime, the model designer may produce a model with hidden conversion cost which it is hard to debug the conversion related performance issues during the deploy time. When the pytorch program is statically compiled to a computation graph, C10 would like to able to differentiate MKL-DNN op from cpu op in the computation graph, and the tensor conversion is represented as a node in the graph. This facilitates the graph optimization and also ease the performance debugging. This involves changes in both Pytorch front-end and back-end. Pytorch front-end needs to expose MKL-DNN to user through certain mechanism, and Pytorch back-end needs to treat MKL-DNN as a backend device so the MKL-DNN OP is represented as Aten OP\u00e2\u20ac\u2122s implementation on MKL-DNN device. \n \n \u00c2 \n \n Intel Pytroch team would like to have the MKL-DNN optimization enabled without user changing code. With the computation graph, an optimization pass may be done to promote CPU op to MKL-DNN op, and add or delete the format conversion op for best performance. The optimization pass ensures that the graph path can enjoy the performance benefit from the latest MKL-DNN version without user changing the program. \n \n \u00c2 \n \n FB Pytorch team understands that MKL-DNN op decides the input/output tensor format at runtime. The user specified to_mkl-dnn() may not trigger the actual format conversion, which may actually happen at the entry of the following MKL-DNN op. The user specified to_cpu() usually triggers a conversion since the MKL-DNN tensor is typically not the plain format. The format conversion may happen between two consecutive MKL-DNN ops. \n \n \u00c2 \n \n Intel Pytorch team would like to not confuse Pytroch user taking MKL-DNN as a HW device. As MKL-DNN is essentially an optimization library on the base device (CPU), calling MKL-DNN a device on Pytorch front-end may cause confusion. For example, the MKL-DNN library may extended to cover another Intel device in the future, in that case, user may be confused by a MKL-DNN device on top of the new Intel device. So, if the MKL-DNN OP has to be exposed in Pytorch front-end, it may be exposed as OPs working with special format tensors, not a device. It is like how sparse tensor is presented to user using torch.to_sparse(), so it could be tensor.to_mkl-dnn(), not tensor.to(device='mkl-dnn-x86'). \n \n \u00c2 \n \n In early experiment stage, if Pytorch user (model designer) complains such usage model, Pytorch team may consider a Python compatible layer, which enlarges MKL-DNN operation scope by simply implementing the non-MKL-DNN Ops by a conversion to cpu tensor and cpu op implementation. This simplifies MKL-DNN op usage since user only needs to specify the conversion to MKL-DNN format at the beginning of the program, but it also defeat the purpose of having user aware of the conversion and an accurate computation graph.",
      "y": "We (Intel Pytorch team) have a couple of discussions with @bddppq on whether MKL-DNN op should be visible to user and how MKL-DNN op should be implemented in Pytorch C10 backend. We summarize the discussion below to capture the motivation and directions on the high level design. Your feedback is welcome. \n \n \n \n ====\n \n The Pytorch C10 design would like to make MKL-DNN operation visible to user. So if user wants to use MKL-DNN operations, user needs to explicitly convert cpu tensor to MKL-DNN tensor, and vice versa. The motivation of explicit user conversion is to support performance debugging. If the conversion is done implicitly in the C10 ops/runtime, the model designer may produce a model with hidden conversion cost which it is hard to debug the conversion related performance issues during the deploy time. When the pytorch program is statically compiled to a computation graph, C10 would like to able to differentiate MKL-DNN op from cpu op in the computation graph, and the tensor conversion is represented as a node in the graph. This facilitates the graph optimization and also ease the performance debugging. This involves changes in both Pytorch front-end and back-end. Pytorch front-end needs to expose MKL-DNN to user through certain mechanism, and Pytorch back-end needs to treat MKL-DNN as a backend device so the MKL-DNN OP is represented as Aten OP\u00e2\u20ac\u2122s implementation on MKL-DNN device. \n \n \u00c2 \n \n Intel Pytroch team would like to have the MKL-DNN optimization enabled without user changing code. With the computation graph, an optimization pass may be done to promote CPU op to MKL-DNN op, and add or delete the format conversion op for best performance. The optimization pass ensures that the graph path can enjoy the performance benefit from the latest MKL-DNN version without user changing the program. \n \n \u00c2 \n \n FB Pytorch team understands that MKL-DNN op decides the input/output tensor format at runtime. The user specified to_mkl-dnn() may not trigger the actual format conversion, which may actually happen at the entry of the following MKL-DNN op. The user specified to_cpu() usually triggers a conversion since the MKL-DNN tensor is typically not the plain format. The format conversion may happen between two consecutive MKL-DNN ops. \n \n \u00c2 \n \n Intel Pytorch team would like to not confuse Pytroch user taking MKL-DNN as a HW device. As MKL-DNN is essentially an optimization library on the base device (CPU), calling MKL-DNN a device on Pytorch front-end may cause confusion. For example, the MKL-DNN library may extended to cover another Intel device in the future, in that case, user may be confused by a MKL-DNN device on top of the new Intel device. So, if the MKL-DNN OP has to be exposed in Pytorch front-end, it may be exposed as OPs working with special format tensors, not a device. It is like how sparse tensor is presented to user using torch.to_sparse(), so it could be tensor.to_mkl-dnn(), not tensor.to(device='mkl-dnn-x86'). \n \n \u00c2 \n \n In early experiment stage, if Pytorch user (model designer) complains such usage model, Pytorch team may consider a Python compatible layer, which enlarges MKL-DNN operation scope by simply implementing the non-MKL-DNN Ops by a conversion to cpu tensor and cpu op implementation. This simplifies MKL-DNN op usage since user only needs to specify the conversion to MKL-DNN format at the beginning of the program, but it also defeat the purpose of having user aware of the conversion and an accurate computation graph."
   },
   {
      "x": "grad_fn missing?",
      "z": "I think this was flagged as a bug, and a fix is in master. See https://github.com/pytorch/pytorch/issues/15353\n \n \n \n It'll be part of the 1.0.1 release this week.",
      "y": "I think this was flagged as a bug, and a fix is in master. See https://github.com/pytorch/pytorch/issues/15353\n \n \n \n It'll be part of the 1.0.1 release this week."
   },
   {
      "x": "Bug: fail to throw error when computing loss between tensors with shapes [n, 1] and [n]",
      "z": "#ERROR!",
      "y": "#ERROR!"
   },
   {
      "x": "torch.histc doesn't work if input tensor on gpu",
      "z": "Let's investigate supporting this once #6688 is merged.\n \n \n \n @mario98 histogram on GPU is really difficult. If you want this get prioritized, please just say so. There is no need to blame devs on working on other changes that, while may be BC breaking, are also and may be more important.",
      "y": "Let's investigate supporting this once #6688 is merged.\n \n \n \nhistogram on GPU is really difficult. If you want this get prioritized, please just say so. There is no need to blame devs on working on other changes that, while may be BC breaking, are also and may be more important."
   },
   {
      "x": "conda 3.6 installation currently broken due to 3.6.1 ABI breakage",
      "z": "Definitely a conda 3.6 thing, fresh install into a 3.5 environment works perfectly :+1:",
      "y": "Definitely a conda 3.6 thing, fresh install into a 3.5 environment works perfectly :+1:"
   },
   {
      "x": "Perform autograd directly on Tensor, not Variable",
      "z": "I'd agree for keeping them separate. One of the things I like about PyTorch in general is the separation of concerns between Tensors, Variables/autograd, and Modules; Tensor is just a generic ndarray that knows nothing about deep learning, or computational graphs, or gradients; it's just a numpy replacement that runs on GPU.",
      "y": "I'd agree for keeping them separate. One of the things I like about PyTorch in general is the separation of concerns between Tensors, Variables/autograd, and Modules; Tensor is just a generic ndarray that knows nothing about deep learning, or computational graphs, or gradients; it's just a numpy replacement that runs on GPU."
   },
   {
      "x": "Inconsistent behavior between ByteTensor and Variable(ByteTensor): bug or feature?",
      "z": "This happens because Variables always return a 1-element Tensor for functions that return scalars.\n \n And when we try to construct a 1-element ByteTensor with the value 256, it overflows to 0.\n \n \n \n This is expected until we introduce a scalar type into autograd \n \n ( https://github.com/pytorch/pytorch/issues/1433 )",
      "y": "This happens because Variables always return a 1-element Tensor for functions that return scalars.\n \n And when we try to construct a 1-element ByteTensor with the value 256, it overflows to 0.\n \n \n \n This is expected until we introduce a scalar type into autograd \n \n ( https://github.com/pytorch/pytorch/issues/1433 )"
   },
   {
      "x": "Allow to set 0 weight decay for biases and params in batch norm",
      "z": "For this issue, I basically do something like this and send the output to the optimizer:\n \n ```python\n \n def group_weight(module):\n \n  group_decay = []\n \n  group_no_decay = []\n \n  for m in module.modules():\n \n  if isinstance(m, nn.Linear):\n \n  group_decay.append(m.weight)\n \n  if m.bias is not None:\n \n  group_no_decay.append(m.bias)\n \n  elif isinstance(m, _ConvNd):\n \n  group_decay.append(m.weight)\n \n  if m.bias is not None:\n \n  group_no_decay.append(m.bias)\n \n  elif isinstance(m, _BatchNorm):\n \n  if m.bias is not None:\n \n  group_no_decay.append(m.weight)\n \n  if m.bias is not None:\n \n  group_no_decay.append(m.bias)\n \n \n \n  assert len(list(module.parameters())) == len(group_decay) + len(group_no_decay)\n \n  groups = [dict(params=group_decay), dict(params=group_no_decay, weight_decay=.0)]\n \n  return groups\n \n ```",
      "y": "For this issue, I basically do something like this and send the output to the optimizer:\n \n ```python\n \n def group_weight(module):\n \n  group_decay = []\n \n  group_no_decay = []\n \n  for m in module.modules():\n \n  if isinstance(m, nn.Linear):\n \n  group_decay.append(m.weight)\n \n  if m.bias is not None:\n \n  group_no_decay.append(m.bias)\n \n  elif isinstance(m, _ConvNd):\n \n  group_decay.append(m.weight)\n \n  if m.bias is not None:\n \n  group_no_decay.append(m.bias)\n \n  elif isinstance(m, _BatchNorm):\n \n  if m.bias is not None:\n \n  group_no_decay.append(m.weight)\n \n  if m.bias is not None:\n \n  group_no_decay.append(m.bias)\n \n \n \n  assert len(list(module.parameters())) == len(group_decay) + len(group_no_decay)\n \n  groups = [dict(params=group_decay), dict(params=group_no_decay, weight_decay=.0)]\n \n  return groups\n \n ```"
   },
   {
      "x": "How to implement 'Signed Sqrt' under the Tensor level",
      "z": "How about the following codes?\n \n ```\n \n # signed sqrt\n \n  x = torch.mul(torch.sign(x),torch.sqrt(torch.abs(x)+1e-12)) \n \n ```\n \n BTW, I am implementing bilinear CNN. Do you know anything about it? Can you help me about the problem? [click here](https://discuss.pytorch.org/t/unkonwn-probelm-of-bilinear-cnn-not-error-but-problem-in-my-implemention-of-algorithm/11704) @KaiyuYue",
      "y": "How about the following codes?\n \n ```\n \n # signed sqrt\n \n  x = torch.mul(torch.sign(x),torch.sqrt(torch.abs(x)+1e-12)) \n \n ```\n \n BTW, I am implementing bilinear CNN. Do you know anything about it? Can you help me about the problem? [click here](https://discuss.pytorch.org/t/unkonwn-probelm-of-bilinear-cnn-not-error-but-problem-in-my-implemention-of-algorithm/11704) @KaiyuYue"
   },
   {
      "x": "torch.norm named arguments do not work as expected",
      "z": "edit: @SsnL is looking into this",
      "y": "edit: @SsnL is looking into this"
   },
   {
      "x": "introduce torch.Scalar to represent scalars in autograd",
      "z": "I had a discussion with @colesbury, @apaszke, @ezyang about this. Here are some notes from that discussion followed by my own proposal. The below discussion is limited to \u00e2\u20ac\u0153Scalars\u00e2\u20ac\u009d in autograd; we probably want to move the Tensor and Variable API together in some some sensible way (see https://github.com/pytorch/pytorch/issues/2633), but I don\u00e2\u20ac\u2122t want to muddy the discussion of autograd Scalars with the details of that process as much as is feasible.\n \n \n \n **Scalars vs 0-dimensional objects**\n \n \n \n NumPy has both scalars and 0-dimensional arrays. Scalars are immutable numeric types that (at least for cases where the type is compatible) are instances of the relevant python numeric type, e.g.:\n \n ```\n \n >>> isinstance(np.double(5), float)\n \n True\n \n >>> isinstance(np.float(5),float)\n \n True\n \n ```\n \n They also \u00e2\u20ac\u0153quack\u00e2\u20ac\u009d like 0-dimensional arrays, i.e. they have `shape` and `ndim` attributes:\n \n ```\n \n >>> np.double(5).shape\n \n ()\n \n >>> np.double(5).ndim\n \n 0\n \n ```\n \n These properties allow you to pass scalar types to libraries outside of NumPy (even ones that do explicit type checking via isinstance or e.g. PyFloat_Check) and to mix NumPy scalars and nd-arrays in code (as long as you don\u00e2\u20ac\u2122t try to mutate a scalar).\n \n \n \n On the other handle, 0-dimensional arrays are mutable types that are not instances of python numeric types:\n \n ```\n \n >>> a=np.array(0.5)\n \n >>> a[()]=2\n \n >>> a\n \n array(2.0)\n \n >>> isinstance(np.array(0.5), float)\n \n False\n \n >>> isinstance(np.array(0.5), np.ndarray)\n \n True\n \n ```\n \n Thus, at least for libraries that do explicit type checking, one would need to cast the 0-dimensional array to the correct python or NumPy scalar type before converting it (np.asscalar will do this)\n \n \n \n **Do we want Scalars or 0-dimensional Variables or both in PyTorch?**\n \n \n \n I\u00e2\u20ac\u2122m going to argue that we only want 0-dimensional Variables.\n \n \n \n First, we want at least 0-dimensional Variables in PyTorch. Consider if we had only (immutable) Scalars and your loss is a .sum() over some variable. Now, if you treat this as an immutable scalar, either it has a grad and can be optimized (which is weird for an immutable value), or it doesn\u00e2\u20ac\u2122t (and you have to hack around that in some way to still be able to do training). You could also represent it as a 1-dimensional Variable (which is what we do today), but then the dimensions are inconsistent (i.e. reduction functions reduce the dimension by 1 except when the dimension is 1). Representing this quantity as a 0-dimensional Variable is just much more straightforward and mathematically consistent.\n \n \n \n Now, if we have 0-dimensional Variables, do we need immutable scalars in autograd? I\u00e2\u20ac\u2122d argue no:\n \n 1) The ability to use Scalars in external libraries seem like not a strong reason for the extra complexity: many operations should just work on 0-dimensional arrays (because we define a number of numerical operators on Variables) and if we provide conversions, in the worst case users can just call the conversions.\n \n 2) Immutability in-and-of-itself may be useful, but its usefulness is not just restricted to scalar types; you may want immutability on all tensor/Variable types.\n \n 3) A related question is what to return when indexing the last dimension of a Variable. The normal python, numpy, and current Tensor semantics are that it gives you an (immutable) scalar type, while the current Variable semantics are that it gives you a Variable that shares storage (of the wrong dimensionality). Given that we already return Variables in this case, returning a 0-dimensional one seems acceptable.\n \n \n \n I\u00e2\u20ac\u2122ll also note here that TensorFlow only supports 0-dimensional tensors, although it\u00e2\u20ac\u2122s quite different because the execution doesn\u00e2\u20ac\u2122t define the graph, as in PyTorch.\n \n \n \n Many of the other arguments for scalars are not about immutability, but convenience, e.g. it\u00e2\u20ac\u2122s nice to get pretty prints (i.e. \u00e2\u20ac\u01530\u00e2\u20ac\u009d or \u00e2\u20ac\u0153torch.FloatScalar(0)\u00e2\u20ac\u009d instead of something like:\n \n  ```\n \n  5\n \n [torch.FloatTensor of size ()]\n \n ```\n \n \n \n Those kinds of convenience problems are solvable without adding scalar types.\n \n \n \n **A Proposal for adding 0-dimensional support to autograd**\n \n \n \n \n \n As mentioned above, the goal here is to get to a sensible Variable API; this doesn\u00e2\u20ac\u2122t purport to solve all issues with scalars (i.e. forced synchronization), but that we can work on those issues separately while maintaining the API as is.\n \n \n \n 1) Autograd forward functions can now return a python number to represent that a 0-dimensional Variable should be returned to the .apply caller. Note that they can\u00e2\u20ac\u2122t return 0-dimensional objects directly because the forward functions are written in terms of tensors, which don\u00e2\u20ac\u2122t support 0-dimensions yet. Note also that this will now match what the equivalent tensor functions do (i.e. `max()` returns a python number), and when they support 0-dimensional tensors, the code will just work as written. This won\u00e2\u20ac\u2122t immediately solve all dimensionality problems, because for example `max(dim=0)` on a 1-dimensional tensor will still return a 1-dimensional tensor, but again, this will automatically be fixed when the tensor APIs are changed.\n \n \n \n 2) Variable is now 0-dimensional aware (via its underlying ATen implementation). For example:\n \n `.dim()` will return 0\n \n `.size()` will return ()\n \n `.shape` will return ()\n \n indexing with a dimension will throw an error (i.e. `[()]` and `[...]` will work, everything else will throw an error)\n \n All other functions will behave as before. Note that accessing .data will return a 1-dimensional tensor, but again, that will automatically change when tensor changes.\n \n \n \n 3) We will provide constructors for 0-dimensional Variables. Unfortunately, a python number passed to a tensor constructor is interpreted as a size: i.e. `torch.FloatTensor(5)` creates a 1-dimensional tensor of size 5, so we eventually need a different name when we support 0-dimensions on tensors. I propose torch.*Scalar (e.g. `torch.FloatScalar`); this has a more object-looking name than say, torch.float(...), which will make it more obvious it\u00e2\u20ac\u2122s a mutable type. One question is how this should interact with the work in (https://github.com/pytorch/pytorch/issues/2633). A full \u00e2\u20ac\u0153leap-of-faith\u00e2\u20ac\u009d approach would be to have this \u00e2\u20ac\u0153constructor\u00e2\u20ac\u009d always return a Variable and have \u00e2\u20ac\u02dcrequires_grad\u00e2\u20ac\u2122 and \u00e2\u20ac\u02dcvolatile\u00e2\u20ac\u2122 parameters. This name will probably be confusing in the short term because you would expect `torch.FloatScalar` to return a tensor type, but constructing these objects directly will probably be rare in the short term anyway.\n \n \n \n 4) `__str__` on a 0-dimensional tensor will return the underlying number, `__repr__` will return how it is constructed, i.e. `torch.FloatScalar(5)`. This is consistent with numpy 0-dimensional arrrays:\n \n ```\n \n >>> print(np.array(0))\n \n 0\n \n >>> np.array(0)\n \n array(0)\n \n ```\n \n \n \n 5) We will provide conversion functions (`__int__`, `__float__`, more?) for 0-dimensional Variables for ease of use with existing python libraries. Calling these functions on non-0-dimensional Variables will throw an error.\n \n \n \n Thoughts?",
      "y": "I had a discussion with @colesbury, @apaszke, @ezyang about this. Here are some notes from that discussion followed by my own proposal. The below discussion is limited to \u00e2\u20ac\u0153Scalars\u00e2\u20ac\u009d in autograd; we probably want to move the Tensor and Variable API together in some some sensible way (see https://github.com/pytorch/pytorch/issues/2633), but I don\u00e2\u20ac\u2122t want to muddy the discussion of autograd Scalars with the details of that process as much as is feasible.\n \n \n \n **Scalars vs 0-dimensional objects**\n \n \n \n NumPy has both scalars and 0-dimensional arrays. Scalars are immutable numeric types that (at least for cases where the type is compatible) are instances of the relevant python numeric type, e.g.:\n \n ```\n \n >>> isinstance(np.double(5), float)\n \n True\n \n >>> isinstance(np.float(5),float)\n \n True\n \n ```\n \n They also \u00e2\u20ac\u0153quack\u00e2\u20ac\u009d like 0-dimensional arrays, i.e. they have `shape` and `ndim` attributes:\n \n ```\n \n >>> np.double(5).shape\n \n ()\n \n >>> np.double(5).ndim\n \n 0\n \n ```\n \n These properties allow you to pass scalar types to libraries outside of NumPy (even ones that do explicit type checking via isinstance or e.g. PyFloat_Check) and to mix NumPy scalars and nd-arrays in code (as long as you don\u00e2\u20ac\u2122t try to mutate a scalar).\n \n \n \n On the other handle, 0-dimensional arrays are mutable types that are not instances of python numeric types:\n \n ```\n \n >>> a=np.array(0.5)\n \n >>> a[()]=2\n \n >>> a\n \n array(2.0)\n \n >>> isinstance(np.array(0.5), float)\n \n False\n \n >>> isinstance(np.array(0.5), np.ndarray)\n \n True\n \n ```\n \n Thus, at least for libraries that do explicit type checking, one would need to cast the 0-dimensional array to the correct python or NumPy scalar type before converting it (np.asscalar will do this)\n \n \n \n **Do we want Scalars or 0-dimensional Variables or both in PyTorch?**\n \n \n \n I\u00e2\u20ac\u2122m going to argue that we only want 0-dimensional Variables.\n \n \n \n First, we want at least 0-dimensional Variables in PyTorch. Consider if we had only (immutable) Scalars and your loss is a .sum() over some variable. Now, if you treat this as an immutable scalar, either it has a grad and can be optimized (which is weird for an immutable value), or it doesn\u00e2\u20ac\u2122t (and you have to hack around that in some way to still be able to do training). You could also represent it as a 1-dimensional Variable (which is what we do today), but then the dimensions are inconsistent (i.e. reduction functions reduce the dimension by 1 except when the dimension is 1). Representing this quantity as a 0-dimensional Variable is just much more straightforward and mathematically consistent.\n \n \n \n Now, if we have 0-dimensional Variables, do we need immutable scalars in autograd? I\u00e2\u20ac\u2122d argue no:\n \n 1) The ability to use Scalars in external libraries seem like not a strong reason for the extra complexity: many operations should just work on 0-dimensional arrays (because we define a number of numerical operators on Variables) and if we provide conversions, in the worst case users can just call the conversions.\n \n 2) Immutability in-and-of-itself may be useful, but its usefulness is not just restricted to scalar types; you may want immutability on all tensor/Variable types.\n \n 3) A related question is what to return when indexing the last dimension of a Variable. The normal python, numpy, and current Tensor semantics are that it gives you an (immutable) scalar type, while the current Variable semantics are that it gives you a Variable that shares storage (of the wrong dimensionality). Given that we already return Variables in this case, returning a 0-dimensional one seems acceptable.\n \n \n \n I\u00e2\u20ac\u2122ll also note here that TensorFlow only supports 0-dimensional tensors, although it\u00e2\u20ac\u2122s quite different because the execution doesn\u00e2\u20ac\u2122t define the graph, as in PyTorch.\n \n \n \n Many of the other arguments for scalars are not about immutability, but convenience, e.g. it\u00e2\u20ac\u2122s nice to get pretty prints (i.e. \u00e2\u20ac\u01530\u00e2\u20ac\u009d or \u00e2\u20ac\u0153torch.FloatScalar(0)\u00e2\u20ac\u009d instead of something like:\n \n  ```\n \n  5\n \n [torch.FloatTensor of size ()]\n \n ```\n \n \n \n Those kinds of convenience problems are solvable without adding scalar types.\n \n \n \n **A Proposal for adding 0-dimensional support to autograd**\n \n \n \n \n \n As mentioned above, the goal here is to get to a sensible Variable API; this doesn\u00e2\u20ac\u2122t purport to solve all issues with scalars (i.e. forced synchronization), but that we can work on those issues separately while maintaining the API as is.\n \n \n \n 1) Autograd forward functions can now return a python number to represent that a 0-dimensional Variable should be returned to the .apply caller. Note that they can\u00e2\u20ac\u2122t return 0-dimensional objects directly because the forward functions are written in terms of tensors, which don\u00e2\u20ac\u2122t support 0-dimensions yet. Note also that this will now match what the equivalent tensor functions do (i.e. `max()` returns a python number), and when they support 0-dimensional tensors, the code will just work as written. This won\u00e2\u20ac\u2122t immediately solve all dimensionality problems, because for example `max(dim=0)` on a 1-dimensional tensor will still return a 1-dimensional tensor, but again, this will automatically be fixed when the tensor APIs are changed.\n \n \n \n 2) Variable is now 0-dimensional aware (via its underlying ATen implementation). For example:\n \n `.dim()` will return 0\n \n `.size()` will return ()\n \n `.shape` will return ()\n \n indexing with a dimension will throw an error (i.e. `[()]` and `[...]` will work, everything else will throw an error)\n \n All other functions will behave as before. Note that accessing .data will return a 1-dimensional tensor, but again, that will automatically change when tensor changes.\n \n \n \n 3) We will provide constructors for 0-dimensional Variables. Unfortunately, a python number passed to a tensor constructor is interpreted as a size: i.e. `torch.FloatTensor(5)` creates a 1-dimensional tensor of size 5, so we eventually need a different name when we support 0-dimensions on tensors. I propose torch.*Scalar (e.g. `torch.FloatScalar`); this has a more object-looking name than say, torch.float(...), which will make it more obvious it\u00e2\u20ac\u2122s a mutable type. One question is how this should interact with the work in (https://github.com/pytorch/pytorch/issues/2633). A full \u00e2\u20ac\u0153leap-of-faith\u00e2\u20ac\u009d approach would be to have this \u00e2\u20ac\u0153constructor\u00e2\u20ac\u009d always return a Variable and have \u00e2\u20ac\u02dcrequires_grad\u00e2\u20ac\u2122 and \u00e2\u20ac\u02dcvolatile\u00e2\u20ac\u2122 parameters. This name will probably be confusing in the short term because you would expect `torch.FloatScalar` to return a tensor type, but constructing these objects directly will probably be rare in the short term anyway.\n \n \n \n 4) `__str__` on a 0-dimensional tensor will return the underlying number, `__repr__` will return how it is constructed, i.e. `torch.FloatScalar(5)`. This is consistent with numpy 0-dimensional arrrays:\n \n ```\n \n >>> print(np.array(0))\n \n 0\n \n >>> np.array(0)\n \n array(0)\n \n ```\n \n \n \n 5) We will provide conversion functions (`__int__`, `__float__`, more?) for 0-dimensional Variables for ease of use with existing python libraries. Calling these functions on non-0-dimensional Variables will throw an error.\n \n \n \n Thoughts?"
   },
   {
      "x": "no member named 'shared_ptr' in namespace 'std'",
      "z": "actually, that file already has memory included.\n \n \n \n Are you following instructions from https://github.com/pytorch/pytorch#from-source\n \n Especially: \n \n ```\n \n MACOSX_DEPLOYMENT_TARGET=10.9 CC=clang CXX=clang++ python setup.py install\n \n ```\n \n \n \n This part is important: `MACOSX_DEPLOYMENT_TARGET=10.9 `",
      "y": "actually, that file already has memory included.\n \n \n \n Are you following instructions from https://github.com/pytorch/pytorch#from-source\n \n Especially: \n \n ```\n \n MACOSX_DEPLOYMENT_TARGET=10.9 CC=clang CXX=clang++ python setup.py install\n \n ```\n \n \n \n This part is important: `MACOSX_DEPLOYMENT_TARGET=10.9 `"
   },
   {
      "x": "Conv2d checks raise an error with confusing message",
      "z": "@bkj @apaszke This error seems to appear when you call `.cuda` on the module and the input tensor, but forget to use the value returned by the call on the tensor (rather than the original tensor, which stays on the cpu). \n \n \n \n It seems that the error message is getting something wrong: should it say \"expected CUDA tensor (got CPU tensor)\"?\n \n \n \n I find it somewhat confusing that calling `.cuda` on a module moves it to gpu wheareas calling `.cuda` on a tensor returns a new tensor but the original one stays on the cpu.",
      "y": "This error seems to appear when you call `.cuda` on the module and the input tensor, but forget to use the value returned by the call on the tensor (rather than the original tensor, which stays on the cpu). \n \n \n \n It seems that the error message is getting something wrong: should it say \"expected CUDA tensor (got CPU tensor)\"?\n \n \n \n I find it somewhat confusing that calling `.cuda` on a module moves it to gpu wheareas calling `.cuda` on a tensor returns a new tensor but the original one stays on the cpu."
   },
   {
      "x": "Handle bugs in cudnnGet* better",
      "z": "Looks like cudnnGet* is returning the wrong algorithm. A workaround is to use ```torch.backends.cudnn.benchmark=True```",
      "y": "Looks like cudnnGet* is returning the wrong algorithm. A workaround is to use ```torch.backends.cudnn.benchmark=True```"
   },
   {
      "x": "Improper error msg when calling Variable.cuda() in forked subprocess",
      "z": "@keven425 Generally you will have to run in Python 3,and include \n \n ``` Python \n \n from multiprocessing import set_start_method\n \n try:\n \n  set_start_method('spawn')\n \n except RuntimeError:\n \n  pass\n \n ```\n \n To avoid fork()",
      "y": " Generally you will have to run in Python 3,and include \n \n ``` Python \n \n from multiprocessing import set_start_method\n \n try:\n \n  set_start_method('spawn')\n \n except RuntimeError:\n \n  pass\n \n ```\n \n To avoid fork()"
   },
   {
      "x": "import error",
      "z": "Same here, fixed by upgrading Python from 3.6.0 to 3.6.1.",
      "y": "Same here, fixed by upgrading Python from 3.6.0 to 3.6.1."
   },
   {
      "x": "Conv1d only accepts FloatTensor",
      "z": "If you look at the printed tensors, you'll see that the input is a DoubleTensor, but the weights and biases are FloatTensors. Adding `conv_1.double()` line before using the module fixes it.",
      "y": "If you look at the printed tensors, you'll see that the input is a DoubleTensor, but the weights and biases are FloatTensors. Adding `conv_1.double()` line before using the module fixes it."
   },
   {
      "x": "Inconsistency with .numpy()",
      "z": "I am using pytorch v0.3.1 which [doesn't has this feature](https://pytorch.org/docs/0.3.1/torch.html?highlight=zeros#torch.zeros).",
      "y": "I am using pytorch v0.3.1 which [doesn't has this feature](https://pytorch.org/docs/0.3.1/torch.html?highlight=zeros#torch.zeros)."
   },
   {
      "x": "torch.save does not work with _LRSchedulers",
      "z": "The same thing happens if you serialize a model and its optimizer in two separate checkpoints, so I'm not very concerned about that.",
      "y": "The same thing happens if you serialize a model and its optimizer in two separate checkpoints, so I'm not very concerned about that."
   },
   {
      "x": "Grid sampler fails with CUDNN_STATUS_EXECUTION_FAILED for large images (not due to memory exhaustion)",
      "z": "I was running this script until I hit an OOM error, not an illegal memory access. This issue is pretty old and likely to be fixed. I'm going to close it. @emitch, if you're still seeing this issue would you update and reopen it?",
      "y": "I was running this script until I hit an OOM error, not an illegal memory access. This issue is pretty old and likely to be fixed. I'm going to close it. @emitch, if you're still seeing this issue would you update and reopen it?"
   },
   {
      "x": "Possible memory leak in spectral_norm",
      "z": "Please, find the script below. The discriminator network is the same that I'm using in my project. \n \n \n \n On the same machine described at the beginning of the issue, at each epoch the GPU memory increases by ~2Mb (Nvidia-smi visual stats). If you let this script run for a long time, then it gives you out of memory.\n \n \n \n Removing the spectral_norm() calls in _ResidualDownSamplingBlock stabilizes memory consumption.\n \n \n \n Let me know if I can do anything else.\n \n Morgan\n \n \n \n ```python\n \n from argparse import ArgumentParser\n \n \n \n import torch\n \n import torch.backends.cudnn as cudnn\n \n import torch.cuda as cuda\n \n \n \n from torch.nn import Sequential, Conv2d, ReLU, Linear, Module, AvgPool2d, BatchNorm2d\n \n from torch.nn.functional import binary_cross_entropy_with_logits, avg_pool2d\n \n from torch.nn.utils import spectral_norm\n \n from torch.optim import Adam\n \n \n \n __author__ = 'Morgan Funtowicz'\n \n \n \n \n \n class Flatten(Module):\n \n  def forward(self, x):\n \n  x = x.view(x.size(0), -1)\n \n  return x\n \n \n \n \n \n class _ResidualDownSamplingBlock(Module):\n \n \n \n  def __init__(self, n_in, n_out, ksize, stride=1, padding=1):\n \n  super().__init__()\n \n  self._f = Sequential(\n \n  ReLU(),\n \n  spectral_norm(Conv2d(n_in, n_out, ksize, stride, padding)),\n \n  ReLU(True),\n \n  spectral_norm(Conv2d(n_out, n_out, ksize, stride, padding)),\n \n  AvgPool2d(2, 2)\n \n  )\n \n  self._sc = spectral_norm(Conv2d(n_in, n_out, 1, padding=0))\n \n \n \n  def forward(self, x):\n \n  return avg_pool2d(self._sc(x), 2, 2) + self._f(x)\n \n \n \n \n \n if __name__ == '__main__':\n \n  # Ensure Tensor are allocated as FloatTensor\n \n  cudnn.benchmark = True\n \n  torch.set_default_tensor_type('torch.FloatTensor')\n \n  torch.set_default_dtype(torch.float32)\n \n \n \n  # Parse provided arguments\n \n  args_parser = ArgumentParser()\n \n  args_parser.add_argument('-d', type=int, default=-1, dest='device', help='Device to use for training (-1 = CPU)')\n \n  args_parser.add_argument('-b', type=int, default=-16, dest='batch', help='Size of the minibatch')\n \n \n \n  args = args_parser.parse_args()\n \n  args.gpu = args.device >= 0 and cuda.is_available()\n \n  device = torch.device(\"cuda:%d\" % args.device if args.gpu else \"cpu\")\n \n \n \n  # Define the model & Optimizer\n \n  model = Sequential(\n \n  _ResidualDownSamplingBlock(3, 64, ksize=3),\n \n  _ResidualDownSamplingBlock(64, 64, ksize=3),\n \n  _ResidualDownSamplingBlock(64, 128, ksize=3),\n \n  _ResidualDownSamplingBlock(128, 128, ksize=3),\n \n  _ResidualDownSamplingBlock(128, 128, ksize=3),\n \n  _ResidualDownSamplingBlock(128, 64, ksize=3),\n \n  BatchNorm2d(64), ReLU(True), Flatten(),\n \n  spectral_norm(Linear(256, 1))\n \n  ).to(device)\n \n \n \n  opt = Adam(model.parameters())\n \n \n \n  # Train\n \n  for epoch in range(20000):\n \n  print('Starting epoch %d' % epoch)\n \n \n \n  x, y = torch.randn((args.batch, 3, 128, 128), device=device), torch.rand((args.batch, 1), device=device)\n \n  y_hat = model(x)\n \n \n \n  opt.zero_grad()\n \n  loss = binary_cross_entropy_with_logits(y_hat, y)\n \n  loss.backward()\n \n  opt.step()\n \n ```",
      "y": "Please, find the script below. The discriminator network is the same that I'm using in my project. \n \n \n \n On the same machine described at the beginning of the issue, at each epoch the GPU memory increases by ~2Mb (Nvidia-smi visual stats). If you let this script run for a long time, then it gives you out of memory.\n \n \n \n Removing the spectral_norm() calls in _ResidualDownSamplingBlock stabilizes memory consumption.\n \n \n \n Let me know if I can do anything else.\n \n Morgan\n \n \n \n ```python\n \n from argparse import ArgumentParser\n \n \n \n import torch\n \n import torch.backends.cudnn as cudnn\n \n import torch.cuda as cuda\n \n \n \n from torch.nn import Sequential, Conv2d, ReLU, Linear, Module, AvgPool2d, BatchNorm2d\n \n from torch.nn.functional import binary_cross_entropy_with_logits, avg_pool2d\n \n from torch.nn.utils import spectral_norm\n \n from torch.optim import Adam\n \n \n \n __author__ = 'Morgan Funtowicz'\n \n \n \n \n \n class Flatten(Module):\n \n  def forward(self, x):\n \n  x = x.view(x.size(0), -1)\n \n  return x\n \n \n \n \n \n class _ResidualDownSamplingBlock(Module):\n \n \n \n  def __init__(self, n_in, n_out, ksize, stride=1, padding=1):\n \n  super().__init__()\n \n  self._f = Sequential(\n \n  ReLU(),\n \n  spectral_norm(Conv2d(n_in, n_out, ksize, stride, padding)),\n \n  ReLU(True),\n \n  spectral_norm(Conv2d(n_out, n_out, ksize, stride, padding)),\n \n  AvgPool2d(2, 2)\n \n  )\n \n  self._sc = spectral_norm(Conv2d(n_in, n_out, 1, padding=0))\n \n \n \n  def forward(self, x):\n \n  return avg_pool2d(self._sc(x), 2, 2) + self._f(x)\n \n \n \n \n \n if __name__ == '__main__':\n \n  # Ensure Tensor are allocated as FloatTensor\n \n  cudnn.benchmark = True\n \n  torch.set_default_tensor_type('torch.FloatTensor')\n \n  torch.set_default_dtype(torch.float32)\n \n \n \n  # Parse provided arguments\n \n  args_parser = ArgumentParser()\n \n  args_parser.add_argument('-d', type=int, default=-1, dest='device', help='Device to use for training (-1 = CPU)')\n \n  args_parser.add_argument('-b', type=int, default=-16, dest='batch', help='Size of the minibatch')\n \n \n \n  args = args_parser.parse_args()\n \n  args.gpu = args.device >= 0 and cuda.is_available()\n \n  device = torch.device(\"cuda:%d\" % args.device if args.gpu else \"cpu\")\n \n \n \n  # Define the model & Optimizer\n \n  model = Sequential(\n \n  _ResidualDownSamplingBlock(3, 64, ksize=3),\n \n  _ResidualDownSamplingBlock(64, 64, ksize=3),\n \n  _ResidualDownSamplingBlock(64, 128, ksize=3),\n \n  _ResidualDownSamplingBlock(128, 128, ksize=3),\n \n  _ResidualDownSamplingBlock(128, 128, ksize=3),\n \n  _ResidualDownSamplingBlock(128, 64, ksize=3),\n \n  BatchNorm2d(64), ReLU(True), Flatten(),\n \n  spectral_norm(Linear(256, 1))\n \n  ).to(device)\n \n \n \n  opt = Adam(model.parameters())\n \n \n \n  # Train\n \n  for epoch in range(20000):\n \n  print('Starting epoch %d' % epoch)\n \n \n \n  x, y = torch.randn((args.batch, 3, 128, 128), device=device), torch.rand((args.batch, 1), device=device)\n \n  y_hat = model(x)\n \n \n \n  opt.zero_grad()\n \n  loss = binary_cross_entropy_with_logits(y_hat, y)\n \n  loss.backward()\n \n  opt.step()\n \n ```"
   },
   {
      "x": "FloatTensor constructor bug",
      "z": "That's a question to @ezyang. This issue might actually be an argument to change this convention (with the huge downside being that a lot of our current code will need to be updated).",
      "y": "That's a question to @ezyang. This issue might actually be an argument to change this convention (with the huge downside being that a lot of our current code will need to be updated)."
   },
   {
      "x": "torch.tensor not copying to correct device if data is a Tensor with correct dtype",
      "z": "Note, this also occurs when creating the array from a NumPy array (that took me a while to figure out what was causing the problem). I would definitely expect a non-PyTorch datatype to take the device setting into account:\n \n \n \n ```\n \n import torch\n \n import numpy as np\n \n \n \n x = torch.tensor(np.array(1), device='cuda:0')\n \n print(x.device) # Prints `cpu`\n \n x = torch.tensor(1, device='cuda:0')\n \n print(x.device) # Prints `cuda:0`\n \n ```",
      "y": "Note, this also occurs when creating the array from a NumPy array (that took me a while to figure out what was causing the problem). I would definitely expect a non-PyTorch datatype to take the device setting into account:\n \n \n \n ```\n \n import torch\n \n import numpy as np\n \n \n \n x = torch.tensor(np.array(1), device='cuda:0')\n \n print(x.device) # Prints `cpu`\n \n x = torch.tensor(1, device='cuda:0')\n \n print(x.device) # Prints `cuda:0`\n \n ```"
   },
   {
      "x": "[BUG] use dropout in multi-LSTM(GRU) when GPU index is not '0', will cause \"cublas runtime error\" [pytorch0.4.0]",
      "z": "As a workaround, you can add\n \n ```\n \n if cuda:\n \n  torch.cuda.set_device(device)\n \n ```\n \n to the beginning of your script.",
      "y": "As a workaround, you can add\n \n ```\n \n if cuda:\n \n  torch.cuda.set_device(device)\n \n ```\n \n to the beginning of your script."
   },
   {
      "x": "[feature request] Bucketization",
      "z": "I updated my searchsorted implementation to work with pytorch v1.0\n \n [https://github.com/aliutkus/torchsearchsorted](https://github.com/aliutkus/torchsearchsorted)",
      "y": "I updated my searchsorted implementation to work with pytorch v1.0\n \n [https://github.com/aliutkus/torchsearchsorted](https://github.com/aliutkus/torchsearchsorted)"
   },
   {
      "x": "cpp API Adam optimizer test is flaky",
      "z": "I was about to file the same issue.\n \n \n \n Some random draws will cause these optim tests to fail, and because they use std::rand() whether they pass or fail is dependent not only on the seed given but also how many prior calls to std::rand() are made. Further, std::rand() does not appear to be the only source of pseudorandomness in these tests, as setting different random seeds at the start of them will still result in intermittent failures/success. (Perhaps the network initializations are pseudorandom, too?)\n \n \n \n While controlling the pseudorandomness is one option, the fact that these tests are so fragile calls them into question. Maybe there are better tests for these optimizers?",
      "y": "I was about to file the same issue.\n \n \n \n Some random draws will cause these optim tests to fail, and because they use std::rand() whether they pass or fail is dependent not only on the seed given but also how many prior calls to std::rand() are made. Further, std::rand() does not appear to be the only source of pseudorandomness in these tests, as setting different random seeds at the start of them will still result in intermittent failures/success. (Perhaps the network initializations are pseudorandom, too?)\n \n \n \n While controlling the pseudorandomness is one option, the fact that these tests are so fragile calls them into question. Maybe there are better tests for these optimizers?"
   },
   {
      "x": "[BUG/Feature request] Missing documentation of torch.jit ?",
      "z": "it's intentional. we'll add docs when it's ready to use.",
      "y": "it's intentional. we'll add docs when it's ready to use."
   },
   {
      "x": "Broken `Type Hints` in PyTorch 0.4.0, related to IDEs(eq. PyCharm)",
      "z": "Going to bump the priority on this because a lot of users have been requesting this... we'll try our best to look into it more.",
      "y": "Going to bump the priority on this because a lot of users have been requesting this... we'll try our best to look into it more."
   },
   {
      "x": "Adagrad not working with GPU",
      "z": "Thanks for pointing that out, it makes sense. But I noticed that other optimizers (e.g. Adam) somehow keep track of the parameters even if they are moved to the gpu after initializing the optimizer. So I thought all optimizers would work correctly in that case. I'll stick with what suggested in the documentation for now.",
      "y": "Thanks for pointing that out, it makes sense. But I noticed that other optimizers (e.g. Adam) somehow keep track of the parameters even if they are moved to the gpu after initializing the optimizer. So I thought all optimizers would work correctly in that case. I'll stick with what suggested in the documentation for now."
   },
   {
      "x": "Possible bug with .ge gradient flow?",
      "z": "\u00e2\u20ac\u0153greater than or equal\u00e2\u20ac\u009d has zero gradient almost everywhere, and nondifferentiable at other points. It\u00e2\u20ac\u2122s not a bug.",
      "y": "\u00e2\u20ac\u0153greater than or equal\u00e2\u20ac\u009d has zero gradient almost everywhere, and nondifferentiable at other points. It\u00e2\u20ac\u2122s not a bug."
   },
   {
      "x": "ATen: How to get cusparseHandle_t handle?",
      "z": "Once you have an ATen context object, you can just call `lazyInitCUDA()` and get a `THCState*`. Then, you can pass that into the THCS functions.\n \n \n \n Also, please remember that we're using GitHub issues for bug reports only, and all questions should be posted on [our forums](https://discuss.pytorch.org).",
      "y": "Once you have an ATen context object, you can just call `lazyInitCUDA()` and get a `THCState*`. Then, you can pass that into the THCS functions.\n \n \n \n Also, please remember that we're using GitHub issues for bug reports only, and all questions should be posted on [our forums](https://discuss.pytorch.org)."
   },
   {
      "x": "Error msg with conv2D in pytorch 0.4",
      "z": "Hmm that's not great. We might want to pass the expected dimensionality of the convolution to the generic implementation, so that we can improve the error messages.",
      "y": "Hmm that's not great. We might want to pass the expected dimensionality of the convolution to the generic implementation, so that we can improve the error messages."
   },
   {
      "x": "no gradients for torch.tensor(np.array(...)) on cuda",
      "z": "You're hiding the reference to the original tensor you created:\n \n ```\n \n a = torch.tensor(np.array([1.,2.,3.]), dtype=torch.float, requires_grad=True).cuda()\n \n ```\n \n This creates a tensor that gradients accumulates into, and then you call `.cuda()` on it, which creates a second tensor. \n \n \n \n Try \n \n ```\n \n a = torch.tensor(np.array([1.,2.,3.]), dtype=torch.float, device=\"cuda\", requires_grad=True)\n \n ```\n \n instead.",
      "y": "You're hiding the reference to the original tensor you created:\n \n ```\n \n a = torch.tensor(np.array([1.,2.,3.]), dtype=torch.float, requires_grad=True).cuda()\n \n ```\n \n This creates a tensor that gradients accumulates into, and then you call `.cuda()` on it, which creates a second tensor. \n \n \n \n Try \n \n ```\n \n a = torch.tensor(np.array([1.,2.,3.]), dtype=torch.float, device=\"cuda\", requires_grad=True)\n \n ```\n \n instead."
   },
   {
      "x": "[feature request] Possibility of returning only require_grad parameters in Module.parameters()",
      "z": "Yeah that makes sense. There's really nothing that prevents people from setting `requires_grad` to `False` after they construct the optimizer, and it won't even complain. We should just remove the check.",
      "y": "Yeah that makes sense. There's really nothing that prevents people from setting `requires_grad` to `False` after they construct the optimizer, and it won't even complain. We should just remove the check."
   },
   {
      "x": "Rebuilding Pytorch after modifying C++ code in aten library",
      "z": "No, you need to run `build_deps`. When I'm compiling PyTorch from source I tend to run `python setup.py build_deps develop` (make sure you haven't installed it previously; if you have, `pip uninstall torch` twice will fix it.)\n \n \n \n See also https://github.com/pytorch/pytorch/blob/master/CONTRIBUTING.md",
      "y": "No, you need to run `build_deps`. When I'm compiling PyTorch from source I tend to run `python setup.py build_deps develop` (make sure you haven't installed it previously; if you have, `pip uninstall torch` twice will fix it.)\n \n \n \n See also https://github.com/pytorch/pytorch/blob/master/CONTRIBUTING.md"
   },
   {
      "x": "[Caffe 2] common_gpu.cc:55] Found an unknown error - this may be due to an incorrectly set up environment, e.g. changing env variable CUDA_VISIBLE_DEVICES after program start.",
      "z": "The problem was solved by restarting the computer :).",
      "y": "The problem was solved by restarting the computer :)."
   },
   {
      "x": "Difference between dilation=1 and dilation=2 convolution outputs",
      "z": "it's because they are computed with different convolution algorithms. normal convolution possibly uses winograd or fft to compute convolution. dilated convolution uses spatial convolution algorithm",
      "y": "it's because they are computed with different convolution algorithms. normal convolution possibly uses winograd or fft to compute convolution. dilated convolution uses spatial convolution algorithm"
   },
   {
      "x": "Bad link to a tutorial about Distributed Overview",
      "z": "Hey @vfdev-5, this is expected. The page you accessed is the master instead of the stable doc release. The distributed overview page is at https://github.com/pytorch/tutorials/blob/release/1.6/beginner_source/dist_overview.rst. @jlin27 will flush contents in `release/1.6` branch into the master branch when we announce v1.6 release. After that, the link would work.",
      "y": "this is expected. The page you accessed is the master instead of the stable doc release. The distributed overview page is at https://github.com/pytorch/tutorials/blob/release/1.6/beginner_source/dist_overview.rst. We will flush contents in `release/1.6` branch into the master branch when we announce v1.6 release. After that, the link would work."
   },
   {
      "x": "Building offline documentation does not work",
      "z": "I like that katex is faster to render and doesn't reflow the page. I think that justifies the slightly more complicated build process for the docs.",
      "y": "I like that katex is faster to render and doesn't reflow the page. I think that justifies the slightly more complicated build process for the docs."
   },
   {
      "x": "libtorch: torch::cuda::is_available() returns zero",
      "z": "I have tracked down the issue. In my CMakelist.txt after building the pytorch libs I need to link it with -Wl,--no-as-needed if not they only link to the CPU libs. \n \n \n \n You could add this line to your CMakelist to link properly:\n \n \n \n set(CMAKE_LINK_WHAT_YOU_USE TRUE)\n \n \n \n Closing.",
      "y": "I have tracked down the issue. In my CMakelist.txt after building the pytorch libs I need to link it with -Wl,--no-as-needed if not they only link to the CPU libs. \n \n \n \n You could add this line to your CMakelist to link properly:\n \n \n \n set(CMAKE_LINK_WHAT_YOU_USE TRUE)\n \n \n \n Closing."
   },
   {
      "x": "[jit] [transformer] [8-bit quantize] Error when trying to quantize pytorch build-in transformer model and then export it with libtorch",
      "z": "I realized that this bug has been fixed in the latest master code (I am using pytorch 1.5.1) in \n \n https://github.com/pytorch/pytorch/blob/890b52e09ff1a758a2ac814f130a1dc1f1a46e9d/torch/nn/modules/activation.py#L869\n \n by changing the Linear class to _LinearWithBias class, which won't be passed to dynamic quantization function to pack the out_proj.\n \n I add this fix in my code and it works well.\n \n Please simply ignore this above error description then\n \n Thanks",
      "y": "I realized that this bug has been fixed in the latest master code (I am using pytorch 1.5.1) in \n \n https://github.com/pytorch/pytorch/blob/890b52e09ff1a758a2ac814f130a1dc1f1a46e9d/torch/nn/modules/activation.py#L869\n \n by changing the Linear class to _LinearWithBias class, which won't be passed to dynamic quantization function to pack the out_proj.\n \n I add this fix in my code and it works well.\n \n Please simply ignore this above error description then\n \n Thanks"
   },
   {
      "x": "support `fftshift` and `ifftshift` in pytorch",
      "z": "Thanks @arthdh, I'd like to point out that torch has `torch.roll` function that provides similar funcitonality to `roll_n` with likely better performance.",
      "y": "I'd like to point out that torch has `torch.roll` function that provides similar funcitonality to `roll_n` with likely better performance."
   },
   {
      "x": "AttributeError: module 'torch.jit' has no attribute '_script_if_tracing'",
      "z": "Try downgrading ` torchversion ` to ` 0.4.0 `\n \n pip uninstall torchvision\n \n pip install torchvision==0.4.0 -f https://download.pytorch.org/whl/torch_stable.html",
      "y": "Try downgrading ` torchversion ` to ` 0.4.0 `\n \n pip uninstall torchvision\n \n pip install torchvision==0.4.0 -f https://download.pytorch.org/whl/torch_stable.html"
   },
   {
      "x": "Import error from torch.distributed",
      "z": "@oke-aditya you are right, `torch.distributed` does not support Windows yet. I am closing this issue and let's move discussions to #42095. Thanks.",
      "y": "`torch.distributed` does not support Windows yet. I am closing this issue and let's move discussions to #42095. Thanks."
   },
   {
      "x": "torch.distributed.launch: despite errors, training continues on some GPUs without printing any logs",
      "z": "@mrshenli Great. As long as one can set a timeout that's fine. I'll try that. Thanks a lot!",
      "y": "As long as one can set a timeout that's fine. I'll try that. Thanks a lot!"
   },
   {
      "x": "The cuda version of torch.det is much slower than cpu version, why?",
      "z": "It's correct that the first cuda run is slower because it includes Magma library initialization. The script in the second comment is not performing correct synchronizations either. Please use Timer utility to properly benchmark code, see [README](https://github.com/pytorch/pytorch/tree/master/torch/utils/_benchmark) and simple_timeit in the examples. You'd need a recent pytorch build for that. With proper timing, cuda det is much faster than cpu det.",
      "y": "It's correct that the first cuda run is slower because it includes Magma library initialization. The script in the second comment is not performing correct synchronizations either. Please use Timer utility to properly benchmark code, see [README](https://github.com/pytorch/pytorch/tree/master/torch/utils/_benchmark) and simple_timeit in the examples. You'd need a recent pytorch build for that. With proper timing, cuda det is much faster than cpu det."
   },
   {
      "x": "[Build Failure] Wrong copy path for shared DNNL",
      "z": "LGTM, will test",
      "y": "LGTM, will test"
   },
   {
      "x": "Error, Compiled pytorch from source on power9 machine(cannot import name '_add_docstr' from 'torch._C' (unknown location)..)",
      "z": "Are you starting your python interpreter from pytorch root directory? Depending on how you installed pytorch that might lead to errors, try changing directory.",
      "y": "Are you starting your python interpreter from pytorch root directory? Depending on how you installed pytorch that might lead to errors, try changing directory."
   },
   {
      "x": "[feature request] support for division in quantization",
      "z": "hi @kodonnell , we don't have a kernel yet for quantized division (it can be considered for future additions). A workaround you could use in the meanwhile is surround division with `DeQuantStub` and `QuantStub` to let the computation happen in floating point.",
      "y": "we don't have a kernel yet for quantized division (it can be considered for future additions). A workaround you could use in the meanwhile is surround division with `DeQuantStub` and `QuantStub` to let the computation happen in floating point."
   },
   {
      "x": "Whether PixelShuffle could add support for 3D data\u00ef\u00bc\u0178",
      "z": "@jbschlosser Was this fixed?",
      "y": "Was this fixed?"
   },
   {
      "x": "[bug] Binomial distribution has small chance of returning -1",
      "z": "I doubt log(u) being rounded to 0 is the culprit. log of 1-2**(-24), (the largest representable fp32 number less than 1) is -5.9605e-8, so unless uniform returns 1. (which it should not) the log term should be non-zero. So my suspicion is, standard_uniform is not 1-exclusive.",
      "y": "I doubt log(u) being rounded to 0 is the culprit. log of 1-2**(-24), (the largest representable fp32 number less than 1) is -5.9605e-8, so unless uniform returns 1. (which it should not) the log term should be non-zero. So my suspicion is, standard_uniform is not 1-exclusive."
   },
   {
      "x": "Weird result multiplying a cpu tensor by a cuda:1 tensor",
      "z": "Thanks for the bug report. I can repro the problem on 1.5.1. It appears to be fixed on master but I am not sure which PR fixed it.\n \n ```\n \n [ins] In [2]: import torch\n \n  ...: a = torch.tensor([3., 4.]).to(torch.device('cuda:1'))\n \n  ...: b: torch.Tensor = torch.tensor(2.)\n \n  ...: b * a\n \n Out[2]: tensor([6., 8.], device='cuda:1')\n \n ```",
      "y": "Thanks for the bug report. I can repro the problem on 1.5.1. It appears to be fixed on master but I am not sure which PR fixed it.\n \n ```\n \n [ins] In [2]: import torch\n \n  ...: a = torch.tensor([3., 4.]).to(torch.device('cuda:1'))\n \n  ...: b: torch.Tensor = torch.tensor(2.)\n \n  ...: b * a\n \n Out[2]: tensor([6., 8.], device='cuda:1')\n \n ```"
   },
   {
      "x": "torch.save produces inconsistent results between multiple runs",
      "z": "@pmeier I was wrong, PR[29232] (https://github.com/pytorch/pytorch/pull/29232) doesn't Introduce this problem.\n \n Torch 0.4.1 has the same logic.\n \n https://github.com/pytorch/pytorch/blob/a24163a95edb193ff7b06e98cd69bf7cfd4c0d2f/torch/serialization.py#L249-L251",
      "y": "PR[29232] (https://github.com/pytorch/pytorch/pull/29232) doesn't Introduce this problem.\n \n Torch 0.4.1 has the same logic.\n \n https://github.com/pytorch/pytorch/blob/a24163a95edb193ff7b06e98cd69bf7cfd4c0d2f/torch/serialization.py#L249-L251"
   },
   {
      "x": "torch.fft tracking issue",
      "z": "I've run some simple benchmarks comparing `torch.fft` transforms against `mkl_fft`, `numpy`, `scipy` and `cupy`. I haven't sampled a huge number of shapes because the benchmarks take a fairly long time to run, but do cover transforming over different dimensions which is interesting because the resulting performance depends a lot more on how batching is handled.\n \n \n \n <details>\n \n <summary>Simple benchmark code</summary>\n \n \n \n ```python\n \n import torch\n \n import numpy as np\n \n import scipy.fft\n \n import cupy\n \n import mkl_fft._numpy_fft\n \n import mkl\n \n import itertools\n \n \n \n shape = (40, 40, 100)\n \n #shape = (42, 14, 18)\n \n c = torch.rand(*shape, dtype=torch.cdouble)\n \n cn = c.numpy()\n \n cc = c.cuda()\n \n r = torch.rand(*shape, dtype=torch.double)\n \n rn = r.numpy()\n \n rc = r.cuda()\n \n \n \n operators = ['fft', 'ifft', 'rfft', 'irfft', 'fftn', 'ifftn', 'rfftn', 'irfftn', 'hfft', 'ihfft']\n \n results = []\n \n def add_result(name, operator, dim, times):\n \n  results.append((name, operator, dim, times.best, times.average, times.stdev))\n \n \n \n for op, dim in itertools.product(operators, (0, 1, 2)):\n \n  torch_fn = getattr(torch.fft, op)\n \n  numpy_fn = getattr(np.fft, op)\n \n  scipy_fn = getattr(scipy.fft, op)\n \n  mkl_fn = getattr(mkl_fft._numpy_fft, op, None)\n \n  cupy_fn = getattr(cupy.fft, op)\n \n  name = f'{op} dim={dim}'\n \n  if op.startswith('rfft') or op.startswith('ihfft'):\n \n  x, xn, xc = r, rn, rc\n \n  else:\n \n  x, xn, xc = c, cn, cc\n \n \n \n  xcp = cupy.array(xn)\n \n  if op.endswith('n'):\n \n  x = x.movedim(dim, -1)\n \n  xn = np.moveaxis(xn, dim, -1)\n \n  xc = xc.movedim(dim, -1)\n \n  xcp = cupy.moveaxis(xcp, dim, -1)\n \n  dim_kwargs = axis_kwargs = dict()\n \n  else:\n \n  axis_kwargs = {'axis': dim}\n \n  dim_kwargs = {'dim': dim}\n \n \n \n  print(name, 'multi-threaded')\n \n  torch.set_num_threads(8)\n \n  t = %timeit -o torch_fn(x, **dim_kwargs)\n \n  add_result('torch mulithreaded', op, dim, t)\n \n  if mkl_fn is not None:\n \n  mkl.set_num_threads(8)\n \n  t = %timeit -o mkl_fn(xn, **axis_kwargs)\n \n  add_result('mkl_fft multi threaded', op, dim, t)\n \n  t = %timeit -o scipy_fn(xn, workers=8, **axis_kwargs)\n \n  add_result('scipy multi threaded', op, dim, t)\n \n \n \n  print(name, 'single threaded')\n \n  torch.set_num_threads(1)\n \n  t = %timeit -o torch_fn(x, **dim_kwargs)\n \n  add_result('torch single threaded', op, dim, t)\n \n  if mkl_fn is not None:\n \n  mkl.set_num_threads(1)\n \n  t = %timeit -o mkl_fn(xn, **axis_kwargs)\n \n  add_result('mkl_fft single threaded', op, dim, t)\n \n  t = %timeit -o numpy_fn(xn, **axis_kwargs)\n \n  add_result('numpy', op, dim, t)\n \n  t = %timeit -o scipy_fn(xn, **axis_kwargs)\n \n  add_result('scipy single threaded', op, dim, t)\n \n \n \n  print(name, 'cufft')\n \n  t = %timeit -o torch_fn(xc, **dim_kwargs); torch.cuda.synchronize()\n \n  add_result('torch cuda', op, dim, t)\n \n  t = %timeit -o cupy_fn(xcp, **axis_kwargs); cupy.cuda.runtime.deviceSynchronize()\n \n  add_result('cupy', op, dim, t)\n \n \n \n ```\n \n \n \n </details>\n \n \n \n \n \n \n \n The full results are available in some spread sheets but I'll sumarise here.\n \n [fft-comparison-benchmarks.zip](https://github.com/pytorch/pytorch/files/5721529/fft-comparison-benchmarks.zip)\n \n \n \n \n \n For cuda, `torch.fft.*` either performed similarly or as much as 2x faster than `cupy.fft.*`. That's pretty good considering we're both just calling cuFFT under the hood.\n \n \n \n For single threaded CPU performance I compared against `mkl_fft`, `numpy` and `scipy`.\n \n - `numpy` was slower in all cases, at best around 1.5x slower than pytorch and at worst 3.9x slower. \n \n - `scipy` hovered around 1-1.5x slower in most cases. For the larger shaped tensor, `scipy` is around 15-20% faster at `ihfft` on all but the last dimension. This is perhaps not surprising though, as `scipy.fft` has custom kernels for `hfft` and `ihfft` whereas intel mkl doesn't.\n \n - Perhaps most interesting is `mkl_fft`. `mkl_fft` is often a few microseconds (around 10% for the smaller tensor) faster for `fft` and `ifft` with `dim=0` or `dim=-1`. I would attribute this to `mkl_fft` caching the transform descriptor between calls whereas `torch.fft` recreates them each time. This significantly simplifies the code though, so may not be worth the extra microseconds.\n \n  On the other hand, many `dim=1` transforms were 2-4x slower on `mkl_fft` than `torch.fft` so the way I'm handling batching is clearly good.\n \n \n \n For multithreaded CPU performance, I only compare against `mkl_fft` and `scipy` since `numpy` doesn't support multithreading.\n \n - `scipy` was up to 30% faster for the smaller single dimensional transform on `dim=1`. Probably because It doesn't require contiguous batch dimensions, so avoids an extra copy. However, this difference becomes negligible for multi-dimensional transforms or transforms over larger tensors. In most other cases, `torch.fft` was 2-3x faster.\n \n - `mkl_fft` is again unsurprisingly similar in the easy cases. It does still get a slight advantage from descriptor caching though. I tested with 8 threads and at that level plan creation took up to 40% of the transform time for the smaller tensor, but still was only a few microseconds of overhead. \n \n  However, on the harder cases `torch.fft` can actually be much faster than `mkl_fft`. I see 4-6x faster for the larger tensor on `rfftn` and `irfftn` e.g. in one case `mkl_fft` takes 1.9 ms and `torch.fft` takes only 0.3 ms to do the same transform.",
      "y": "I've run some simple benchmarks comparing `torch.fft` transforms against `mkl_fft`, `numpy`, `scipy` and `cupy`. I haven't sampled a huge number of shapes because the benchmarks take a fairly long time to run, but do cover transforming over different dimensions which is interesting because the resulting performance depends a lot more on how batching is handled.\n \n \n \n <details>\n \n <summary>Simple benchmark code</summary>\n \n \n \n ```python\n \n import torch\n \n import numpy as np\n \n import scipy.fft\n \n import cupy\n \n import mkl_fft._numpy_fft\n \n import mkl\n \n import itertools\n \n \n \n shape = (40, 40, 100)\n \n #shape = (42, 14, 18)\n \n c = torch.rand(*shape, dtype=torch.cdouble)\n \n cn = c.numpy()\n \n cc = c.cuda()\n \n r = torch.rand(*shape, dtype=torch.double)\n \n rn = r.numpy()\n \n rc = r.cuda()\n \n \n \n operators = ['fft', 'ifft', 'rfft', 'irfft', 'fftn', 'ifftn', 'rfftn', 'irfftn', 'hfft', 'ihfft']\n \n results = []\n \n def add_result(name, operator, dim, times):\n \n  results.append((name, operator, dim, times.best, times.average, times.stdev))\n \n \n \n for op, dim in itertools.product(operators, (0, 1, 2)):\n \n  torch_fn = getattr(torch.fft, op)\n \n  numpy_fn = getattr(np.fft, op)\n \n  scipy_fn = getattr(scipy.fft, op)\n \n  mkl_fn = getattr(mkl_fft._numpy_fft, op, None)\n \n  cupy_fn = getattr(cupy.fft, op)\n \n  name = f'{op} dim={dim}'\n \n  if op.startswith('rfft') or op.startswith('ihfft'):\n \n  x, xn, xc = r, rn, rc\n \n  else:\n \n  x, xn, xc = c, cn, cc\n \n \n \n  xcp = cupy.array(xn)\n \n  if op.endswith('n'):\n \n  x = x.movedim(dim, -1)\n \n  xn = np.moveaxis(xn, dim, -1)\n \n  xc = xc.movedim(dim, -1)\n \n  xcp = cupy.moveaxis(xcp, dim, -1)\n \n  dim_kwargs = axis_kwargs = dict()\n \n  else:\n \n  axis_kwargs = {'axis': dim}\n \n  dim_kwargs = {'dim': dim}\n \n \n \n  print(name, 'multi-threaded')\n \n  torch.set_num_threads(8)\n \n  t = %timeit -o torch_fn(x, **dim_kwargs)\n \n  add_result('torch mulithreaded', op, dim, t)\n \n  if mkl_fn is not None:\n \n  mkl.set_num_threads(8)\n \n  t = %timeit -o mkl_fn(xn, **axis_kwargs)\n \n  add_result('mkl_fft multi threaded', op, dim, t)\n \n  t = %timeit -o scipy_fn(xn, workers=8, **axis_kwargs)\n \n  add_result('scipy multi threaded', op, dim, t)\n \n \n \n  print(name, 'single threaded')\n \n  torch.set_num_threads(1)\n \n  t = %timeit -o torch_fn(x, **dim_kwargs)\n \n  add_result('torch single threaded', op, dim, t)\n \n  if mkl_fn is not None:\n \n  mkl.set_num_threads(1)\n \n  t = %timeit -o mkl_fn(xn, **axis_kwargs)\n \n  add_result('mkl_fft single threaded', op, dim, t)\n \n  t = %timeit -o numpy_fn(xn, **axis_kwargs)\n \n  add_result('numpy', op, dim, t)\n \n  t = %timeit -o scipy_fn(xn, **axis_kwargs)\n \n  add_result('scipy single threaded', op, dim, t)\n \n \n \n  print(name, 'cufft')\n \n  t = %timeit -o torch_fn(xc, **dim_kwargs); torch.cuda.synchronize()\n \n  add_result('torch cuda', op, dim, t)\n \n  t = %timeit -o cupy_fn(xcp, **axis_kwargs); cupy.cuda.runtime.deviceSynchronize()\n \n  add_result('cupy', op, dim, t)\n \n \n \n ```\n \n \n \n </details>\n \n \n \n \n \n \n \n The full results are available in some spread sheets but I'll sumarise here.\n \n [fft-comparison-benchmarks.zip](https://github.com/pytorch/pytorch/files/5721529/fft-comparison-benchmarks.zip)\n \n \n \n \n \n For cuda, `torch.fft.*` either performed similarly or as much as 2x faster than `cupy.fft.*`. That's pretty good considering we're both just calling cuFFT under the hood.\n \n \n \n For single threaded CPU performance I compared against `mkl_fft`, `numpy` and `scipy`.\n \n - `numpy` was slower in all cases, at best around 1.5x slower than pytorch and at worst 3.9x slower. \n \n - `scipy` hovered around 1-1.5x slower in most cases. For the larger shaped tensor, `scipy` is around 15-20% faster at `ihfft` on all but the last dimension. This is perhaps not surprising though, as `scipy.fft` has custom kernels for `hfft` and `ihfft` whereas intel mkl doesn't.\n \n - Perhaps most interesting is `mkl_fft`. `mkl_fft` is often a few microseconds (around 10% for the smaller tensor) faster for `fft` and `ifft` with `dim=0` or `dim=-1`. I would attribute this to `mkl_fft` caching the transform descriptor between calls whereas `torch.fft` recreates them each time. This significantly simplifies the code though, so may not be worth the extra microseconds.\n \n  On the other hand, many `dim=1` transforms were 2-4x slower on `mkl_fft` than `torch.fft` so the way I'm handling batching is clearly good.\n \n \n \n For multithreaded CPU performance, I only compare against `mkl_fft` and `scipy` since `numpy` doesn't support multithreading.\n \n - `scipy` was up to 30% faster for the smaller single dimensional transform on `dim=1`. Probably because It doesn't require contiguous batch dimensions, so avoids an extra copy. However, this difference becomes negligible for multi-dimensional transforms or transforms over larger tensors. In most other cases, `torch.fft` was 2-3x faster.\n \n - `mkl_fft` is again unsurprisingly similar in the easy cases. It does still get a slight advantage from descriptor caching though. I tested with 8 threads and at that level plan creation took up to 40% of the transform time for the smaller tensor, but still was only a few microseconds of overhead. \n \n  However, on the harder cases `torch.fft` can actually be much faster than `mkl_fft`. I see 4-6x faster for the larger tensor on `rfftn` and `irfftn` e.g. in one case `mkl_fft` takes 1.9 ms and `torch.fft` takes only 0.3 ms to do the same transform."
   },
   {
      "x": "pytorch istft runs slower than torchaudio istft especially at higher n_fft",
      "z": "I confirm that I could reproduce the issue.\n \n \n \n | | Total | Par Call |\n \n |-----------------|:--------:|:------:|\n \n | PyTorch 1.6.0 | 9.0404 | 0.904 |\n \n | torchaudio 0.5.0 | 0.1052 | 0.010 |\n \n \n \n <Details><Summary>script</Summary>\n \n \n \n ```python\n \n import time\n \n \n \n import torch\n \n import torchaudio\n \n \n \n n_fft = 2**15\n \n x = torch.rand(size=[1, n_fft//2 + 1, 10, 2])\n \n \n \n start = time.monotonic()\n \n n = 10\n \n for _ in range(n):\n \n  # y = torchaudio.functional.istft(x,n_fft)\n \n  y = torch.functional.istft(x,n_fft)\n \n elapsed = time.monotonic() - start\n \n \n \n print(elapsed, elapsed / n)\n \n ```\n \n \n \n </Details>\n \n \n \n <Details><Summary>Env 1: PyTorch 1.6.0</Summary>\n \n \n \n ```\n \n $ python -m torch.utils.collect_env\n \n Collecting environment information...\n \n PyTorch version: 1.5.0\n \n Is debug build: No\n \n CUDA used to build PyTorch: 10.2\n \n \n \n OS: Ubuntu 18.04.3 LTS\n \n GCC version: (Ubuntu 7.4.0-1ubuntu1~18.04.1) 7.4.0\n \n CMake version: version 3.10.2\n \n \n \n Python version: 3.6\n \n Is CUDA available: No\n \n CUDA runtime version: Could not collect\n \n GPU models and configuration:\n \n GPU 0: Quadro GP100\n \n GPU 1: Quadro GP100\n \n \n \n Nvidia driver version: 418.116.00\n \n cuDNN version: Could not collect\n \n \n \n Versions of relevant libraries:\n \n [pip3] numpy==1.18.5\n \n [pip3] torch==1.5.0\n \n [pip3] torchaudio==0.5.0a0+3305d5c\n \n [conda] blas 1.0 mkl\n \n [conda] mkl 2020.1 217\n \n [conda] mkl-service 2.3.0 py36he904b0f_0\n \n [conda] mkl_fft 1.1.0 py36h23d657b_0\n \n [conda] mkl_random 1.1.1 py36h0573a6f_0\n \n [conda] pytorch 1.5.0 py3.6_cuda10.2.89_cudnn7.6.5_0 pytorch\n \n [conda] torchaudio 0.5.0 py36 pytorch\n \n ```\n \n \n \n </Details>\n \n \n \n <Details><Summary>Env 2: torchaudio 0.5.0</Summary>\n \n \n \n ```\n \n $ python -m torch.utils.collect_env\n \n Collecting environment information...\n \n PyTorch version: 1.6.0\n \n Is debug build: No\n \n CUDA used to build PyTorch: 10.2\n \n \n \n OS: Ubuntu 18.04.3 LTS\n \n GCC version: (Ubuntu 7.4.0-1ubuntu1~18.04.1) 7.4.0\n \n CMake version: version 3.10.2\n \n \n \n Python version: 3.6\n \n Is CUDA available: No\n \n CUDA runtime version: Could not collect\n \n GPU models and configuration:\n \n GPU 0: Quadro GP100\n \n GPU 1: Quadro GP100\n \n \n \n Nvidia driver version: 418.116.00\n \n cuDNN version: Could not collect\n \n \n \n Versions of relevant libraries:\n \n [pip3] numpy==1.18.5\n \n [pip3] torch==1.6.0\n \n [pip3] torchaudio==0.6.0a0+f17ae39\n \n [conda] blas 1.0 mkl\n \n [conda] cudatoolkit 10.2.89 hfd86e86_1\n \n [conda] mkl 2020.1 217\n \n [conda] mkl-service 2.3.0 py36he904b0f_0\n \n [conda] mkl_fft 1.1.0 py36h23d657b_0\n \n [conda] mkl_random 1.1.1 py36h0573a6f_0\n \n [conda] numpy 1.18.5 py36ha1c710e_0\n \n [conda] numpy-base 1.18.5 py36hde5b4d6_0\n \n [conda] pytorch 1.6.0 py3.6_cuda10.2.89_cudnn7.6.5_0 pytorch\n \n [conda] torchaudio 0.6.0 py36 pytorch\n \n ```\n \n \n \n </Details>",
      "y": "I confirm that I could reproduce the issue.\n \n \n \n | | Total | Par Call |\n \n |-----------------|:--------:|:------:|\n \n | PyTorch 1.6.0 | 9.0404 | 0.904 |\n \n | torchaudio 0.5.0 | 0.1052 | 0.010 |\n \n \n \n <Details><Summary>script</Summary>\n \n \n \n ```python\n \n import time\n \n \n \n import torch\n \n import torchaudio\n \n \n \n n_fft = 2**15\n \n x = torch.rand(size=[1, n_fft//2 + 1, 10, 2])\n \n \n \n start = time.monotonic()\n \n n = 10\n \n for _ in range(n):\n \n  # y = torchaudio.functional.istft(x,n_fft)\n \n  y = torch.functional.istft(x,n_fft)\n \n elapsed = time.monotonic() - start\n \n \n \n print(elapsed, elapsed / n)\n \n ```\n \n \n \n </Details>\n \n \n \n <Details><Summary>Env 1: PyTorch 1.6.0</Summary>\n \n \n \n ```\n \n $ python -m torch.utils.collect_env\n \n Collecting environment information...\n \n PyTorch version: 1.5.0\n \n Is debug build: No\n \n CUDA used to build PyTorch: 10.2\n \n \n \n OS: Ubuntu 18.04.3 LTS\n \n GCC version: (Ubuntu 7.4.0-1ubuntu1~18.04.1) 7.4.0\n \n CMake version: version 3.10.2\n \n \n \n Python version: 3.6\n \n Is CUDA available: No\n \n CUDA runtime version: Could not collect\n \n GPU models and configuration:\n \n GPU 0: Quadro GP100\n \n GPU 1: Quadro GP100\n \n \n \n Nvidia driver version: 418.116.00\n \n cuDNN version: Could not collect\n \n \n \n Versions of relevant libraries:\n \n [pip3] numpy==1.18.5\n \n [pip3] torch==1.5.0\n \n [pip3] torchaudio==0.5.0a0+3305d5c\n \n [conda] blas 1.0 mkl\n \n [conda] mkl 2020.1 217\n \n [conda] mkl-service 2.3.0 py36he904b0f_0\n \n [conda] mkl_fft 1.1.0 py36h23d657b_0\n \n [conda] mkl_random 1.1.1 py36h0573a6f_0\n \n [conda] pytorch 1.5.0 py3.6_cuda10.2.89_cudnn7.6.5_0 pytorch\n \n [conda] torchaudio 0.5.0 py36 pytorch\n \n ```\n \n \n \n </Details>\n \n \n \n <Details><Summary>Env 2: torchaudio 0.5.0</Summary>\n \n \n \n ```\n \n $ python -m torch.utils.collect_env\n \n Collecting environment information...\n \n PyTorch version: 1.6.0\n \n Is debug build: No\n \n CUDA used to build PyTorch: 10.2\n \n \n \n OS: Ubuntu 18.04.3 LTS\n \n GCC version: (Ubuntu 7.4.0-1ubuntu1~18.04.1) 7.4.0\n \n CMake version: version 3.10.2\n \n \n \n Python version: 3.6\n \n Is CUDA available: No\n \n CUDA runtime version: Could not collect\n \n GPU models and configuration:\n \n GPU 0: Quadro GP100\n \n GPU 1: Quadro GP100\n \n \n \n Nvidia driver version: 418.116.00\n \n cuDNN version: Could not collect\n \n \n \n Versions of relevant libraries:\n \n [pip3] numpy==1.18.5\n \n [pip3] torch==1.6.0\n \n [pip3] torchaudio==0.6.0a0+f17ae39\n \n [conda] blas 1.0 mkl\n \n [conda] cudatoolkit 10.2.89 hfd86e86_1\n \n [conda] mkl 2020.1 217\n \n [conda] mkl-service 2.3.0 py36he904b0f_0\n \n [conda] mkl_fft 1.1.0 py36h23d657b_0\n \n [conda] mkl_random 1.1.1 py36h0573a6f_0\n \n [conda] numpy 1.18.5 py36ha1c710e_0\n \n [conda] numpy-base 1.18.5 py36hde5b4d6_0\n \n [conda] pytorch 1.6.0 py3.6_cuda10.2.89_cudnn7.6.5_0 pytorch\n \n [conda] torchaudio 0.6.0 py36 pytorch\n \n ```\n \n \n \n </Details>"
   },
   {
      "x": "Add torch.nn.Unflatten module into C++ Frontend",
      "z": "@glaringlee no worries then. Thanks for letting me know!",
      "y": "@glaringlee no worries then. Thanks for letting me know!"
   },
   {
      "x": "Type mismatch error with torch.nn.functional.grid_sample() under AMP",
      "z": "Is this fix? I still get same error in pytorch 1.7.1",
      "y": "Is this fix? I still get same error in pytorch 1.7.1"
   },
   {
      "x": "Compilation error, Performing Test SUPPORT_GLIBCXX_USE_C99 Failed.",
      "z": "Pytorch-1.5 needs a C++14 compatible compiler. You are trying to compile PyTorch-1.6.0 with gcc-4.8.5, which is not fully C++14 compatible. \n \n Is there a reason why you can not try to compile it with at least gcc-5.4?",
      "y": "Pytorch-1.5 needs a C++14 compatible compiler. You are trying to compile PyTorch-1.6.0 with gcc-4.8.5, which is not fully C++14 compatible. \n \n Is there a reason why you can not try to compile it with at least gcc-5.4?"
   },
   {
      "x": "when i use amp on LSTM network,errro",
      "z": "Closing as duplicate of https://github.com/pytorch/pytorch/issues/36428.\n \n \n \n If you're using `nn.LSTM`, this is a known issue, amp doesn't work with cudnn RNNs in 1.6. they're more complicated than other ops because autocast needs to flatten weights in a particular format, in addition to casting them to FP16. I didn't have time to fix them before the release.\n \n \n \n This past week I wrote a fix that works on my machine. The PR should be up in the next few days.\n \n \n \n The cell-based RNN API is expected to work with Amp already. If you observed the error while using `nn.LSTMCell`, please reopen the issue and submit a minimal repro.",
      "y": "Closing as duplicate of https://github.com/pytorch/pytorch/issues/36428.\n \n \n \n If you're using `nn.LSTM`, this is a known issue, amp doesn't work with cudnn RNNs in 1.6. they're more complicated than other ops because autocast needs to flatten weights in a particular format, in addition to casting them to FP16. I didn't have time to fix them before the release.\n \n \n \n This past week I wrote a fix that works on my machine. The PR should be up in the next few days.\n \n \n \n The cell-based RNN API is expected to work with Amp already. If you observed the error while using `nn.LSTMCell`, please reopen the issue and submit a minimal repro."
   },
   {
      "x": "torch.inverse() performing very poorly on GPU vs CPU",
      "z": "@xwang233 could you try to grab the nsight profiles and check the call trace for these methods, please?",
      "y": "try to grab the nsight profiles and check the call trace for these methods, please?"
   },
   {
      "x": "https://github.com/pytorch/pytorch/issues/42271#issue-668293527",
      "z": "https://github.com/pytorch/pytorch/issues/42271#issuecomment-666055175",
      "y": "https://github.com/pytorch/pytorch/issues/42271#issuecomment-666055175"
   },
   {
      "x": "cudaErrorIllegalAddress printing result of torch.nn.Linear(1, 1).cuda()(torch.Tensor([[0.5]]))",
      "z": "btw, the problem is that your input tensor isn't on cuda, but the error is definitely a bug.",
      "y": "btw, the problem is that your input tensor isn't on cuda, but the error is definitely a bug."
   },
   {
      "x": "elementMap_.count(v) ASSERT FAILED at csrc/jit/passes/alias_analysis.cpp:536",
      "z": "I simply ran: torch.jit.trace(my_model.pth).\n \n \n \n Previous nightlies had worked fine.\n \n \n \n Network from this repository with the prebuilt model https://github.com/zisianw/FaceBoxes.PyTorch\n \n \n \n \n \n Model: https://drive.google.com/file/d/1aRdcf692S2-oxNdoc6oOjIVvoyvSHl52/view?usp=sharing\n \n \n \n To test you can run within test.py which has model loading code.",
      "y": "I simply ran: torch.jit.trace(my_model.pth).\n \n \n \n Previous nightlies had worked fine.\n \n \n \n Network from this repository with the prebuilt model https://github.com/zisianw/FaceBoxes.PyTorch\n \n \n \n \n \n Model: https://drive.google.com/file/d/1aRdcf692S2-oxNdoc6oOjIVvoyvSHl52/view?usp=sharing\n \n \n \n To test you can run within test.py which has model loading code."
   },
   {
      "x": "[cuda] randn on a non-default stream doesn't work",
      "z": "I think a better solution is to move all random generator APIs to use philox, that's easily both stream safe (each thread has to compute only its counter, and is not using global state that would need to be synchronized across streams) and thread safe https://github.com/pytorch/pytorch/blob/cf094d4edcc7928f9a4a368b3e2eeb22579b29b0/aten/src/ATen/native/cuda/Distributions.cu#L31-L35 (global counters are stored on the cpu and atomically incremented). Bernoulli is already using philox. \n \n May be we should postpone major random refactors and just have a targeted PR eliminating mtgp in favor of philox, either from curand or with a standalone implementation that's currently in the fuser.",
      "y": "I think a better solution is to move all random generator APIs to use philox, that's easily both stream safe (each thread has to compute only its counter, and is not using global state that would need to be synchronized across streams) and thread safe https://github.com/pytorch/pytorch/blob/cf094d4edcc7928f9a4a368b3e2eeb22579b29b0/aten/src/ATen/native/cuda/Distributions.cu#L31-L35 (global counters are stored on the cpu and atomically incremented). Bernoulli is already using philox. \n \n May be we should postpone major random refactors and just have a targeted PR eliminating mtgp in favor of philox, either from curand or with a standalone implementation that's currently in the fuser."
   },
   {
      "x": "[FR] torch.cuda.synchronize takes in device objects",
      "z": "We will accept a PR that makes `torch.cuda.synchronize(device=None)`. Make sure it checks that the input device is actually CUDA.\n \n \n \n @gchanan is not sure what the point of the stream version of this function is, since `stream.synchronize()` is pretty easy.",
      "y": "We will accept a PR that makes `torch.cuda.synchronize(device=None)`. Make sure it checks that the input device is actually CUDA.\n \n \n \n not sure what the point of the stream version of this function is, since `stream.synchronize()` is pretty easy."
   },
   {
      "x": "[jit] jit.trace segfault on variable slicing using `torch.narrow`",
      "z": "@eellison Yes that's the line segfault occurs: `torch.onnx.export(model, torch_in, onnx_path, verbose=True)`",
      "y": "that's the line segfault occurs: `torch.onnx.export(model, torch_in, onnx_path, verbose=True)`"
   },
   {
      "x": "C++ Frontend data_parallel Does Not Update Weights",
      "z": "It looks like the [replicated module copies](https://github.com/pytorch/pytorch/blob/9fbce974c96a7768ce38f860bebcd9f17a66c9ec/torch/csrc/api/include/torch/nn/parallel/data_parallel.h#L41) are not [accumulating grads](https://github.com/pytorch/pytorch/blob/9fbce974c96a7768ce38f860bebcd9f17a66c9ec/torch/csrc/api/include/torch/nn/cloneable.h#L36) back to the original module (confirmed with @goldsborough ). I will implement `Broadcast` and `ReduceAddCoalesced` in C++ (as we did in Python). I feel eventually we might want to call C++ `data_parallel` from Python and drop Python's implementation.",
      "y": "It looks like the [replicated module copies](https://github.com/pytorch/pytorch/blob/9fbce974c96a7768ce38f860bebcd9f17a66c9ec/torch/csrc/api/include/torch/nn/parallel/data_parallel.h#L41) are not [accumulating grads](https://github.com/pytorch/pytorch/blob/9fbce974c96a7768ce38f860bebcd9f17a66c9ec/torch/csrc/api/include/torch/nn/cloneable.h#L36) back to the original module (confirmed with @goldsborough ). I will implement `Broadcast` and `ReduceAddCoalesced` in C++ (as we did in Python). I feel eventually we might want to call C++ `data_parallel` from Python and drop Python's implementation."
   },
   {
      "x": "using occupancy calculator instead of fixed constant for threads per block",
      "z": "@ThisIsIsaac it's a constructor for a Tensor. It's generated by C macros, what you probably want to look for is `THCTensor_(new)` or `THCudaTensor_(new)`",
      "y": "it's a constructor for a Tensor. It's generated by C macros, what you probably want to look for is `THCTensor_(new)` or `THCudaTensor_(new)`"
   },
   {
      "x": "TestJit.test_input_dict_unify is flaky",
      "z": "Should be fixed in master",
      "y": "Should be fixed in master"
   },
   {
      "x": "Multiprocessping-distributed ERROR",
      "z": "After almost one year later, now, I know why.\n \n If we are using multi-GPU to train models, it would like to start multi-thread for different GPUs. And each thread has to rerun the script. If we change the code during this time, the other threads may load the modified code, which caused this problem.",
      "y": "After almost one year later, now, I know why.\n \n If we are using multi-GPU to train models, it would like to start multi-thread for different GPUs. And each thread has to rerun the script. If we change the code during this time, the other threads may load the modified code, which caused this problem."
   },
   {
      "x": "How to do prediction/inference for a batch of images at a time with libtorch?",
      "z": "The input tensor is usually in the shape of (images, channels, width, height). \n \n \n \n Use torch::cat or torch::stack to add tensors along dimension 0 and then run forward.",
      "y": "The input tensor is usually in the shape of (images, channels, width, height). \n \n \n \n Use torch::cat or torch::stack to add tensors along dimension 0 and then run forward."
   },
   {
      "x": "Norm operator broken when used with `p=1` and `dim=(1, 2)`",
      "z": "I think this is already fixed on master. Closing, please feel free to reopen if you see further issues.\n \n ```\n \n In [1]: import torch\n \n  ...:\n \n  ...: x = torch.rand(4, 3, 3)\n \n  ...: torch.norm(x, dim=(1,2)) # Works\n \n  ...: torch.norm(x, p=1) # Works\n \n  ...: torch.norm(x, p=1, dim=(1,2)) # Breaks\n \n  ...:\n \n Out[1]: tensor([4.8355, 4.1928, 4.2854, 4.0480])\n \n ```",
      "y": "I think this is already fixed on master. Closing, please feel free to reopen if you see further issues.\n \n ```\n \n In [1]: import torch\n \n  ...:\n \n  ...: x = torch.rand(4, 3, 3)\n \n  ...: torch.norm(x, dim=(1,2)) # Works\n \n  ...: torch.norm(x, p=1) # Works\n \n  ...: torch.norm(x, p=1, dim=(1,2)) # Breaks\n \n  ...:\n \n Out[1]: tensor([4.8355, 4.1928, 4.2854, 4.0480])\n \n ```"
   },
   {
      "x": "TestJit.test_cpp broken on master",
      "z": "Seems to be this patch https://github.com/pytorch/pytorch/pull/19445 cc @eellison",
      "y": "Seems to be this patch https://github.com/pytorch/pytorch/pull/19445 cc @eellison"
   },
   {
      "x": "interpolate bicubic should follow opencv result",
      "z": "@jonmorton FYI the PR above should fix the issue. \n \n I didn't put exact same repro script as testcase as I feel it's not good to install cv2 just for this baseline so I added a \"expected result\" check there. \n \n For reference, after the patch the script (node with `align_corners=False`) matches opencv result. \n \n ```\n \n ('max abs diff:', tensor(7.4208e-06))\n \n ('max rel diff:', tensor(80458.5469))\n \n ('MSE:', tensor(9.1130e-13))\n \n ```",
      "y": "@jonmorton FYI the PR above should fix the issue. \n \n I didn't put exact same repro script as testcase as I feel it's not good to install cv2 just for this baseline so I added a \"expected result\" check there. \n \n For reference, after the patch the script (node with `align_corners=False`) matches opencv result. \n \n ```\n \n ('max abs diff:', tensor(7.4208e-06))\n \n ('max rel diff:', tensor(80458.5469))\n \n ('MSE:', tensor(9.1130e-13))\n \n ```"
   },
   {
      "x": "`torch.nn.functional.conv2d` (CPU) is very slow on a specific trained weight",
      "z": "maybe you're hitting a problem with denormalized weights? you may try setting `torch.set_flush_denormal(True)` and see if that helps",
      "y": "maybe you're hitting a problem with denormalized weights? you may try setting `torch.set_flush_denormal(True)` and see if that helps"
   },
   {
      "x": "Unexpected numeric limit for CUDA integral types",
      "z": "See @gchanan's comment here: https://github.com/pytorch/pytorch/issues/17750#issuecomment-486302496",
      "y": "See comment here: https://github.com/pytorch/pytorch/issues/17750#issuecomment-486302496"
   },
   {
      "x": "LSTM batch_first flag does not produce ouputs with the batch as the first dimension.",
      "z": "This isn't a bug: (batch, seq, feature) corresponds to the output tensor of lstm, not the hidden states.\n \n ```\n \n output, (hx, cx) = lstm(input)\n \n ```\n \n We could improve the documentation around this, but I'm not sure that the hidden state should also have its batch transposed to be first. Do you have a use case for that, @al093?",
      "y": "This isn't a bug: (batch, seq, feature) corresponds to the output tensor of lstm, not the hidden states.\n \n ```\n \n output, (hx, cx) = lstm(input)\n \n ```\n \n We could improve the documentation around this, but I'm not sure that the hidden state should also have its batch transposed to be first. Do you have a use case for that, @al093?"
   },
   {
      "x": "1.2.0.dev20190723 Conv1D Illegal instruction (core dumped)",
      "z": "fixed in #23292 \n \n \n \n @soumith\n \n @vpirogov\n \n @Jianhui-Li\n \n @jgong5\n \n @uyongw",
      "y": "fixed in #23292 \n"
   },
   {
      "x": "Add collective communication APIs for Python objects.",
      "z": "Another example of where `all_gather` is used with arbitrary Python objects is in [`maskrcnn_benchmark`](https://github.com/facebookresearch/maskrcnn-benchmark/blob/24c8c90efdb7cc51381af5ce0205b23567c3cd21/maskrcnn_benchmark/utils/comm.py#L48-L88)",
      "y": "Another example of where `all_gather` is used with arbitrary Python objects is in [`maskrcnn_benchmark`](https://github.com/facebookresearch/maskrcnn-benchmark/blob/24c8c90efdb7cc51381af5ce0205b23567c3cd21/maskrcnn_benchmark/utils/comm.py#L48-L88)"
   },
   {
      "x": "JIT inputs does not support collections.OrderedDict",
      "z": "@lara-hdr Thanks, it worked.",
      "y": "@lara-hdr Thanks, it worked."
   },
   {
      "x": "Equations in the document do not display properly: Their LaTeX code is shown instead",
      "z": "Yeah, given how nicely prerendered katex is shaping up, I think I'm going to just revert the imgmath change and then wait until we can deploy using prerendered katex.",
      "y": "Yeah, given how nicely prerendered katex is shaping up, I think I'm going to just revert the imgmath change and then wait until we can deploy using prerendered katex."
   },
   {
      "x": "Potentially missing else",
      "z": "@pietern Done: #23429",
      "y": "@pietern Done: #23429"
   },
   {
      "x": "bitwise_not documentation is not rendering correctly",
      "z": "cc @brianjo - Please add this to the list for 1.2 release.",
      "y": "cc @brianjo - Please add this to the list for 1.2 release."
   },
   {
      "x": "[Feature Request] inferred module dimensions",
      "z": "This has come up several times in the past, and I spent the last 20 mins searching for the proposal I wrote on a previous issue, but I wasn't able to (the fun of having thousands of issues).\n \n Hence, I'm re-iterating my previous thoughts both from memory and after re-thinking.\n \n \n \n I think this is a feature worth designing, but the following design principles have to be upheld:\n \n - no meta-programming or added magic to support it\n \n - the module hard-errors on all cases where it is not fully initialized, with a clear message stating to the user what is expected of them\n \n - the Auto-Infer mechanism has to be explicitly requested by the user, not via constants such as `-1` or `None`, but something like `torch.nn.Linear(torch.nn.INFER, 32)`.\n \n \n \n To flesh out the implementation details,\n \n - something like `nn.Linear`, when it gets as constructor input a `torch.nn.INFER`, will explicitly set it's weight to `nn._UninitializedParameter()`.\n \n - the downstream tasks that handle parameters have to now explicitly handle and throw exceptions appropriately on getting `nn._UninitializedParameter`, with a clear error message\n \n  - for example `.parameters()` should throw, `.apply()` should throw, `jit` entrypoints should throw, `.to()` doesn't need to throw.\n \n - when a user does `model(input)`, `nn.Linear` will flesh out it's uninitalized parameters\n \n - the performance hit of this deferred initialization logic should not affect the non-deferred initialization at all",
      "y": "This has come up several times in the past, and I spent the last 20 mins searching for the proposal I wrote on a previous issue, but I wasn't able to (the fun of having thousands of issues).\n \n Hence, I'm re-iterating my previous thoughts both from memory and after re-thinking.\n \n \n \n I think this is a feature worth designing, but the following design principles have to be upheld:\n \n - no meta-programming or added magic to support it\n \n - the module hard-errors on all cases where it is not fully initialized, with a clear message stating to the user what is expected of them\n \n - the Auto-Infer mechanism has to be explicitly requested by the user, not via constants such as `-1` or `None`, but something like `torch.nn.Linear(torch.nn.INFER, 32)`.\n \n \n \n To flesh out the implementation details,\n \n - something like `nn.Linear`, when it gets as constructor input a `torch.nn.INFER`, will explicitly set it's weight to `nn._UninitializedParameter()`.\n \n - the downstream tasks that handle parameters have to now explicitly handle and throw exceptions appropriately on getting `nn._UninitializedParameter`, with a clear error message\n \n  - for example `.parameters()` should throw, `.apply()` should throw, `jit` entrypoints should throw, `.to()` doesn't need to throw.\n \n - when a user does `model(input)`, `nn.Linear` will flesh out it's uninitalized parameters\n \n - the performance hit of this deferred initialization logic should not affect the non-deferred initialization at all"
   },
   {
      "x": "error in multi node",
      "z": "@mrshenli After updating pytorch from 1.1 to 1.2(nightly), the default nccl is updated from 2.4.2 to 2.4.8, then the nccl runtime error is fixed under multi-nodes communication. By the way, when will pytorch 1.2 be released?",
      "y": "@mrshenli After updating pytorch from 1.1 to 1.2(nightly), the default nccl is updated from 2.4.2 to 2.4.8, then the nccl runtime error is fixed under multi-nodes communication. By the way, when will pytorch 1.2 be released?"
   },
   {
      "x": "[PyTorch][Feature Request]Lookahead Optimizer",
      "z": "We don't think this method should be in pytorch core, as opposed to your own personal repository or something like https://github.com/pytorch/contrib , at least not yet in time.\n \n \n \n Our reservation is that we want to include methods that the community uses as a standard, or else the code maintenance problem balloons up for us.\n \n We do show discretion based on what the paper shows as evidence, for example BatchNorm was included within weeks of it's publish date (in Torch).\n \n In terms of rejected methods, we've rejected newly minted papers such as Swish ( #3260 , #3182 ), Yellowfin ( https://github.com/pytorch/pytorch/issues/1960 ) and many others, and rightly so, these haven't become standardized in the community (like LSTM / Transformer / BatchNorm).\n \n \n \n If you have a differing opinion, let us know why, and we can re-think.\n \n \n \n tl;dr: The paper doesn't show evidence that makes it a method that has obvious long-term success. If the paper does have long-term success in the field we will include it",
      "y": "We don't think this method should be in pytorch core, as opposed to your own personal repository or something like https://github.com/pytorch/contrib , at least not yet in time.\n \n \n \n Our reservation is that we want to include methods that the community uses as a standard, or else the code maintenance problem balloons up for us.\n \n We do show discretion based on what the paper shows as evidence, for example BatchNorm was included within weeks of it's publish date (in Torch).\n \n In terms of rejected methods, we've rejected newly minted papers such as Swish ( #3260 , #3182 ), Yellowfin ( https://github.com/pytorch/pytorch/issues/1960 ) and many others, and rightly so, these haven't become standardized in the community (like LSTM / Transformer / BatchNorm).\n \n \n \n If you have a differing opinion, let us know why, and we can re-think.\n \n \n \n tl;dr: The paper doesn't show evidence that makes it a method that has obvious long-term success. If the paper does have long-term success in the field we will include it"
   },
   {
      "x": "Bug of LongTensor/IntTensor ?",
      "z": "fyi, @nairbv",
      "y": "fyi, @nairbv"
   },
   {
      "x": "Fatal error gloo/transport/tcp/device.h: no such file or directory build from source",
      "z": "USE_DISTRIBUTED=0 is definitely a flag you'd want to set. Generally, look at the Android mobile build and what flags are set in that case. @ljk53 might be able to help here.",
      "y": "USE_DISTRIBUTED=0 is definitely a flag you'd want to set. Generally, look at the Android mobile build and what flags are set in that case. @ljk53 might be able to help here."
   },
   {
      "x": "cpp_extension should use ninja for setup.py builds too",
      "z": "This is fixed by @zou3519 !",
      "y": "This is fixed "
   },
   {
      "x": "torch.mean() calculations are not consistent across CPU/GPU",
      "z": "Sum itself seems to behave a bit different for gpu & cpu for your array of values.\n \n \n \n This is what I see:\n \n \n \n ```\n \n a=torch.linspace(10000,1.7,10000)\n \n b=a.cuda()\n \n a.sum()\n \n # tensor(50008496.)\n \n b.sum()\n \n # tensor(50008504., device='cuda:0')\n \n ```\n \n I am not sure if this really is to be classified as a bug or as an acceptable artifact of parallel floating point summation on the GPU. The order of fp summation operations generally matters since a summand with larger exponent will ignore bits from a smaller summand's mantissa. Different algorithms exist to counter this, e.g. [Kahan_summation_algorithm](https://en.wikipedia.org/wiki/Kahan_summation_algorithm). Also one should keep in mind that internally different register width might be used, even on CPU e.g. 80 bits for the x87 while SSE uses stictly 64/32 bit float registers etc.\n \n \n \n A GPU summation precision related discussion (with some papers) is in the NVIDIA dev forums [here](https://devtalk.nvidia.com/default/topic/1044661/how-to-improve-float-array-summation-precision-and-stability-/).",
      "y": "Sum itself seems to behave a bit different for gpu & cpu for your array of values.\n \n \n \n This is what I see:\n \n \n \n ```\n \n a=torch.linspace(10000,1.7,10000)\n \n b=a.cuda()\n \n a.sum()\n \n # tensor(50008496.)\n \n b.sum()\n \n # tensor(50008504., device='cuda:0')\n \n ```\n \n I am not sure if this really is to be classified as a bug or as an acceptable artifact of parallel floating point summation on the GPU. The order of fp summation operations generally matters since a summand with larger exponent will ignore bits from a smaller summand's mantissa. Different algorithms exist to counter this, e.g. [Kahan_summation_algorithm](https://en.wikipedia.org/wiki/Kahan_summation_algorithm). Also one should keep in mind that internally different register width might be used, even on CPU e.g. 80 bits for the x87 while SSE uses stictly 64/32 bit float registers etc.\n \n \n \n A GPU summation precision related discussion (with some papers) is in the NVIDIA dev forums [here](https://devtalk.nvidia.com/default/topic/1044661/how-to-improve-float-array-summation-precision-and-stability-/)."
   },
   {
      "x": "RuntimeError: set_storage is not allowed on Tensor created from .data or .detach()",
      "z": "Hi Imad,\n \n Here is a quick fix:\n \n 1. Replace weight.data.set_ with weight.set_.\n \n 2. Add with th.no_grad(): block.\n \n \n \n Revised version: \n \n \n \n ```python\n \n with th.no_grad():\n \n  model.weight.set_(((alice_model.weight.data + bob_model.weight.data) / 2).get())\n \n ```\n \n \n \n The line after the colon (:) needs to be indented. \n \n \n \n Details: https://discuss.pytorch.org/t/api-change-for-tensor-data-set-in-torch-nightly/33310/6\n \n Hope this helps.\n \n Agata",
      "y": "Hi Imad,\n \n Here is a quick fix:\n \n 1. Replace weight.data.set_ with weight.set_.\n \n 2. Add with th.no_grad(): block.\n \n \n \n Revised version: \n \n \n \n ```python\n \n with th.no_grad():\n \n  model.weight.set_(((alice_model.weight.data + bob_model.weight.data) / 2).get())\n \n ```\n \n \n \n The line after the colon (:) needs to be indented. \n \n \n \n Details: https://discuss.pytorch.org/t/api-change-for-tensor-data-set-in-torch-nightly/33310/6\n \n Hope this helps.\n \n Agata"
   },
   {
      "x": "Incorrect gradients for torch.where when one of the target tensors contains inf/nan",
      "z": "Why is this line using `z` (instead of w)?\n \n \n \n ```\n \n dwdx = torch.autograd.grad(z.sum(), x, retain_graph=True)[0] \n \n ```",
      "y": "Why is this line using `z` (instead of w)?\n \n \n \n ```\n \n dwdx = torch.autograd.grad(z.sum(), x, retain_graph=True)[0] \n \n ```"
   },
   {
      "x": "RuntimeError: cuda runtime error (3) : initialization error AFTER daemonization",
      "z": "> @zhangguanheng66 can hopefully comment on why it was left commented out in #23030. Did it break something?\n \n \n \n For the full history, read #15782, #15734, #17357.\n \n \n \n But I believe the true constraint is that it is not permissible to initialize CUDA context when device count is called. This is because internally, we have a hack called \"lazy cuda library stubs\", where you can run programs that are dynamically linked against CUDA even on systems that don't have CUDA. In this situation, calls to `cudaGetDeviceCount` will (properly) return zero, but all other calls will raise an exception. Various CPU-only code relies on this to call `device_count()` even when CUDA is not available. If you initialize CUDA when device count is invoked, you break these stubs.",
      "y": " Hopefully, I'll be able to remark on why it was commented out in #23030. Did it cause any damage?\n\nRead #15782, #15734, and #17357 for the complete narrative.\nHowever, I believe the underlying constraint is that initialising CUDA context when device count is called is not permitted. This is because we have a hack called \"lazy cuda library stubs\" that allows you to run applications that aren't actually cuda libraries."
   },
   {
      "x": "error: no viable conversion from 'script::Module' to 'std::shared_ptr<torch::jit::script::Module>'",
      "z": "Are you using a nightly/master build of libtorch? This was changed recently and we haven't updated the tutorial on the stable docs yet (which apply to v1.1), you can see an updated version [here](https://github.com/pytorch/tutorials/pull/560).\n \n \n \n TLDR: `script::Module` was changed to a value type, so you can just do `script::Module myModule = torch::jit::load(argv[1]);`\n \n \n \n Closing since we already have a PR out to fix the tutorial, but feel free to re-open if you still have problems",
      "y": "Are you using a nightly/master build of libtorch? This was changed recently and we haven't updated the tutorial on the stable docs yet (which apply to v1.1), you can see an updated version [here](https://github.com/pytorch/tutorials/pull/560).\n \n \n \n TLDR: `script::Module` was changed to a value type, so you can just do `script::Module myModule = torch::jit::load(argv[1]);`\n \n \n \n Closing since we already have a PR out to fix the tutorial, but feel free to re-open if you still have problems"
   },
   {
      "x": "Update RPC examples/docs/tutorials to use RRef helper",
      "z": "> Then I would try to pass PyTorch computation to remote machine with as little modifications as possible. The best is to specify the address of additional remote worker from command line.\n \n \n \n In this case you might want to start from `DistributedDataParallel`?\n \n \n \n > And ideally I'd like to be able to add worker dynamically without losing the progress I made so far.\n \n \n \n This is not possible in `torch.distributed.rpc` yet. For `DistributedDataParallel`, there is a [`torchelastic`](https://pytorch.org/elastic) project built for this purpose. cc @kiukchung",
      "y": "> Then I would try to pass PyTorch computation to remote machine with as little modifications as possible. The best is to specify the address of additional remote worker from command line.\n \n \n \n In this case you might want to start from `DistributedDataParallel`?\n \n \n \n > And ideally I'd like to be able to add worker dynamically without losing the progress I made so far.\n \n \n \n This is not possible in `torch.distributed.rpc` yet. For `DistributedDataParallel`, there is a [`torchelastic`](https://pytorch.org/elastic) project built for this purpose. cc @kiukchung"
   },
   {
      "x": "Pytorch's jit weight file cannot run on libtorch 1.5release",
      "z": "The problem has been solved. net->forward() return three tensors.\n \n auto ret = model->forward(jitdata).toTuple().elements();",
      "y": "The problem has been solved. net->forward() return three tensors.\n \n auto ret = model->forward(jitdata).toTuple().elements();"
   },
   {
      "x": "JIT-traced module incurs bug: nSubTensors_ > 1 ASSERT FAILED",
      "z": "[solved] The problem was that somewhere in the code `torch.cat` was applied to a single tensor only instead of a list of tensors, i.e., `torch.cat([tensor])`. While this worked fine for normal runs, the JIT tracer threw the above mentioned error without any hint on where it occured. @ailzhang",
      "y": "[solved] The problem was that somewhere in the code `torch.cat` was applied to a single tensor only instead of a list of tensors, i.e., `torch.cat([tensor])`. While this worked fine for normal runs, the JIT tracer threw the above mentioned error without any hint on where it occured. @ailzhang"
   },
   {
      "x": "torch.relu performance appears slower on a single Tensor vs. a Python outer for-loop",
      "z": "I suspect it's because the second one has to allocate an output of `256 *80 * 80 * 20` (125 MiB) while the first one only has outputs of size ~6.25 MiB (one or two live at a time). \n \n \n \n The glibc allocator will re-use 6.25 MiB allocations but map and unmap fresh pages for the 125 MiB allocation, which is expensive and repeatedly incurs the kernel's zeroing cost. This cost is a lot bigger than the overhead of 20 additional calls to relu.\n \n \n \n jemalloc might be better; it tends to hold on to large allocations for longer than glibc.\n \n \n \n I don't think this is something we can fix in PyTorch other than possibly changing the memory allocator (which might be a good idea).",
      "y": "I suspect it's because the second one has to allocate an output of `256 *80 * 80 * 20` (125 MiB) while the first one only has outputs of size ~6.25 MiB (one or two live at a time). \n \n \n \n The glibc allocator will re-use 6.25 MiB allocations but map and unmap fresh pages for the 125 MiB allocation, which is expensive and repeatedly incurs the kernel's zeroing cost. This cost is a lot bigger than the overhead of 20 additional calls to relu.\n \n \n \n jemalloc might be better; it tends to hold on to large allocations for longer than glibc.\n \n \n \n I don't think this is something we can fix in PyTorch other than possibly changing the memory allocator (which might be a good idea)."
   },
   {
      "x": "DISABLED test_profiler_with_async_rpc_builtin (__main__.RpcTestWithSpawn)",
      "z": "btw, if we simply moved that future's condvar.notify_all() to the end, it still won't get a guarantee that all the callbacks have been executed in the fut.wait() code snippet above.\n \n \n \n Typically in a future-style api, the main contract is simply that the future's value has been set, rather than all the subscribers being notified and having processed the event.",
      "y": "btw, if we simply moved that future's condvar.notify_all() to the end, it still won't get a guarantee that all the callbacks have been executed in the fut.wait() code snippet above.\n \n \n \n Typically in a future-style api, the main contract is simply that the future's value has been set, rather than all the subscribers being notified and having processed the event."
   },
   {
      "x": "Windows Libtorch Ver1.5 did't operate at the GPU mode , it operated at the CPU mode.",
      "z": "https://github.com/pytorch/pytorch/issues/37124",
      "y": "https://github.com/pytorch/pytorch/issues/37124"
   },
   {
      "x": "Runtime error while calculating gradients , Version 1.5.0 specific",
      "z": "Oh, I see, that's why its a problem faced while having 'retail graph = True' . Thanks a lot.",
      "y": "Oh, I see, that's why its a problem faced while having 'retail graph = True' . Thanks a lot."
   },
   {
      "x": "SIGILL from libtorch_cpu.so at import with a CPU without AVX",
      "z": "@ezyang these lines look suspicious to me:\n \n \n \n https://github.com/pytorch/pytorch/blob/69e2f1aaff9614bd801b34b0c9b4cc8305ee9a61/aten/src/ATen/cpu/vec256/vec256_float.h#L19\n \n \n \n https://github.com/pytorch/pytorch/blob/69e2f1aaff9614bd801b34b0c9b4cc8305ee9a61/aten/src/ATen/cpu/vec256/vec256_float.h#L328\n \n \n \n and similar in other files (e.g. complex).\n \n \n \n We should stop accepting patches that declare data in Vec256 files. There isn't a good reason for these \"1\" constants and they cause trouble -- use `Vec256<float>(1.0f)` instead.",
      "y": " these lines look suspicious to me:\n \n \n \n https://github.com/pytorch/pytorch/blob/69e2f1aaff9614bd801b34b0c9b4cc8305ee9a61/aten/src/ATen/cpu/vec256/vec256_float.h#L19\n \n \n \n https://github.com/pytorch/pytorch/blob/69e2f1aaff9614bd801b34b0c9b4cc8305ee9a61/aten/src/ATen/cpu/vec256/vec256_float.h#L328\n \n \n \n and similar in other files (e.g. complex).\n \n \n \n We should stop accepting patches that declare data in Vec256 files. There isn't a good reason for these \"1\" constants and they cause trouble -- use `Vec256<float>(1.0f)` instead."
   },
   {
      "x": "[dataloader] Multiple warnings printed when torch.as_tensor applied to readonly NumPy tensor",
      "z": "Often these non-writable NumPy tensors appear as wrapper of some contig memory from an external source (like read file). An optional no-copy behavior is often desirable even in these cases: e.g. audio after decompression can be huge, and copy 1) asks to have a second free big chunk of memory, 2) introduces copying overhead",
      "y": "Often these non-writable NumPy tensors appear as wrapper of some contig memory from an external source (like read file). An optional no-copy behavior is often desirable even in these cases: e.g. audio after decompression can be huge, and copy 1) asks to have a second free big chunk of memory, 2) introduces copying overhead"
   },
   {
      "x": "Add a CI job using conda compilers",
      "z": "> The question I have is if it should be an extra CI job entry, or if we should modify an existing one (and if so, which one)?\n \n \n \n Extra CI seems right",
      "y": "> The question I have is if it should be an extra CI job entry, or if we should modify an existing one (and if so, which one)?\n \n \n \n Extra CI seems right"
   },
   {
      "x": "einsum shape zero",
      "z": "## zeros in the output indices\n \n If I run `torch.einsum('anything->ijkl', ...)` I expect the output to have dimension 4 and the shape corresponding to the indices `i`, `j`, `k` and `l`.\n \n \n \n Example\n \n ```\n \n torch.einsum('ijk,k->ij', torch.randn(3, 0, 6), torch.randn(6)) == torch.empty(3, 0)\n \n ```\n \n \n \n ## zeros in the contracted indices\n \n If one of the contracted indices is shape zero it means that you sum over nothing, therefore the result is well defined and is a zero tensor.\n \n ```\n \n torch.einsum('ij,j->i', torch.randn(4, 0), torch.randn(0)) == torch.zeros(4)\n \n ```",
      "y": "## zeros in the output indices\n \n If I run `torch.einsum('anything->ijkl', ...)` I expect the output to have dimension 4 and the shape corresponding to the indices `i`, `j`, `k` and `l`.\n \n \n \n Example\n \n ```\n \n torch.einsum('ijk,k->ij', torch.randn(3, 0, 6), torch.randn(6)) == torch.empty(3, 0)\n \n ```\n \n \n \n ## zeros in the contracted indices\n \n If one of the contracted indices is shape zero it means that you sum over nothing, therefore the result is well defined and is a zero tensor.\n \n ```\n \n torch.einsum('ij,j->i', torch.randn(4, 0), torch.randn(0)) == torch.zeros(4)\n \n ```"
   },
   {
      "x": "Lack of AVX2 not detected correctly in build",
      "z": "Probably related (or similar to) #37577",
      "y": "Probably related (or similar to) #37577"
   },
   {
      "x": "[JIT] Error accessing NamedTuple field by name in module's forward",
      "z": "Your second example works fine actually; I think you referenced an attribute called `name` that doesn't exist as per your definition of `Params`.",
      "y": "Your second example works fine actually; I think you referenced an attribute called `name` that doesn't exist as per your definition of `Params`."
   },
   {
      "x": "Remove torch.max/min warning",
      "z": "We do verify that behaviour in #42004.\n \n \n \n Attaching screen-shot as the github doesn't render the file post 11k lines.\n \n ![image](https://user-images.githubusercontent.com/19503980/92435070-15805180-f1bf-11ea-924e-e7199a187262.png)",
      "y": "We do verify that behaviour in #42004.\n \n \n \n Attaching screen-shot as the github doesn't render the file post 11k lines.\n \n ![image](https://user-images.githubusercontent.com/19503980/92435070-15805180-f1bf-11ea-924e-e7199a187262.png)"
   },
   {
      "x": "Generalized Helper for Parsing Environment Variables from Process Group",
      "z": "Hi @Rubix982, I can let you know if any of the FB internal tests are failing and how can we address those. Feel free to put up a PR and we can coordinate from there!",
      "y": "I can let you know if any of the FB internal tests are failing and how can we address those. Feel free to put up a PR and we can coordinate from there!"
   },
   {
      "x": "Using DataLoader with num_workers>0 causes re-run of script",
      "z": "This is expected. Windows uses `spawn` by default (because of Python), which literally reruns the script, which is why you should wrap actual executing code in `if __name__ == '__main__':`. https://pytorch.org/docs/stable/notes/windows.html#multiprocessing-error-without-if-clause-protection",
      "y": "This is expected. Windows uses `spawn` by default (because of Python), which literally reruns the script, which is why you should wrap actual executing code in `if __name__ == '__main__':`. https://pytorch.org/docs/stable/notes/windows.html#multiprocessing-error-without-if-clause-protection"
   },
   {
      "x": "\"exp_cuda\" not implemented for 'ComplexDouble'",
      "z": "It is fixed. Works on latest master.\n \n ```python\n \n >>> import torch\n \n >>> torch.set_default_tensor_type(torch.cuda.DoubleTensor)\n \n >>> tt = torch.Tensor([1])\n \n >>> torch.exp(1j*tt)\n \n tensor([0.5403+0.8415j])\n \n ```",
      "y": "It is fixed. Works on latest master.\n \n ```python\n \n >>> import torch\n \n >>> torch.set_default_tensor_type(torch.cuda.DoubleTensor)\n \n >>> tt = torch.Tensor([1])\n \n >>> torch.exp(1j*tt)\n \n tensor([0.5403+0.8415j])\n \n ```"
   },
   {
      "x": "Current behavior of `as_tuple` argument is inconsistent in `nonzero`",
      "z": "This is not yet fixed for `tensor.nonzero()` due to https://github.com/pytorch/pytorch/pull/45413#issuecomment-703947414. Could you reopen or open a new issue?",
      "y": "This is not yet fixed for `tensor.nonzero()` due to https://github.com/pytorch/pytorch/pull/45413#issuecomment-703947414. Could you reopen or open a new issue?"
   },
   {
      "x": "Performance issue with FFTs, numpy vs pytorch",
      "z": "Timing with gh-43011 still shows a performance drop with pytorch:\n \n ```\n \n zero elapsed time: 3.07e-05 seconds\n \n rot elapsed time: 5.59e-05 seconds\n \n loop elapsed time: 0.001341 seconds\n \n NUMPY TIME elapsed time: 0.0015816 seconds\n \n zero elapsed time: 8.81e-05 seconds\n \n rot elapsed time: 0.0002193 seconds\n \n loop elapsed time: 0.0037979 seconds\n \n PYTORCH TIME elapsed time: 0.00421 seconds\n \n ```\n \n \n \n However, I don't think this is a very good benchmark. It does many small non-vectorized 1D transforms which is not what tensor libraries are best at. It's also conflating `diag`, `fft`, `abs` and copy-assign into one single benchmark.\n \n \n \n If I use `timeit` to isolate each line:\n \n * diag is ~3 us for numpy and ~5-6 us for pytorch\n \n * fft is ~10 us for numpy and 20 us for pytorch\n \n * abs is ~9 us for numpy and 12 us for pytorch\n \n * assignment is ~1us in numpy and 11 us for pytorch.\n \n \n \n Every operator has at least 3 us overhead with pytorch. That's pretty bad, but for large tensors I'm sure it balances out a bit better. FFT is about 2x slower for this small 1D tensor, but for a `256 X 512` FFT pytorch is 2x faster than NumPy, even single threaded.\n \n \n \n The biggest standout is copying from one slice to another which is an order of magnitude slower in pytorch.",
      "y": "Timing with gh-43011 still shows a performance drop with pytorch:\n \n ```\n \n zero elapsed time: 3.07e-05 seconds\n \n rot elapsed time: 5.59e-05 seconds\n \n loop elapsed time: 0.001341 seconds\n \n NUMPY TIME elapsed time: 0.0015816 seconds\n \n zero elapsed time: 8.81e-05 seconds\n \n rot elapsed time: 0.0002193 seconds\n \n loop elapsed time: 0.0037979 seconds\n \n PYTORCH TIME elapsed time: 0.00421 seconds\n \n ```\n \n \n \n However, I don't think this is a very good benchmark. It does many small non-vectorized 1D transforms which is not what tensor libraries are best at. It's also conflating `diag`, `fft`, `abs` and copy-assign into one single benchmark.\n \n \n \n If I use `timeit` to isolate each line:\n \n * diag is ~3 us for numpy and ~5-6 us for pytorch\n \n * fft is ~10 us for numpy and 20 us for pytorch\n \n * abs is ~9 us for numpy and 12 us for pytorch\n \n * assignment is ~1us in numpy and 11 us for pytorch.\n \n \n \n Every operator has at least 3 us overhead with pytorch. That's pretty bad, but for large tensors I'm sure it balances out a bit better. FFT is about 2x slower for this small 1D tensor, but for a `256 X 512` FFT pytorch is 2x faster than NumPy, even single threaded.\n \n \n \n The biggest standout is copying from one slice to another which is an order of magnitude slower in pytorch."
   },
   {
      "x": "Bug in pytorch\\aten\\src\\ATen/native/Resize.h",
      "z": "This was fixed in PyTorch 1.6. The error message is now \"RuntimeError: Index tensor must have the same number of dimensions as self tensor\"",
      "y": "This was fixed in PyTorch 1.6. The error message is now \"RuntimeError: Index tensor must have the same number of dimensions as self tensor\""
   },
   {
      "x": "getting RuntimeError: CUDA error: device-side assert triggered after using cross enthropy (jupyter nbotebook)",
      "z": "Is a device-side assert the expected behaviour here? \n \n \n \n It would be nice if a more user-friendly error were produced",
      "y": "Is a device-side assert the expected behaviour here? \n \n \n \n It would be nice if a more user-friendly error were produced"
   },
   {
      "x": "Can't solve torch.lstsq() with specific values",
      "z": "Thanks for the simpler reproduction, @chrisby. \n \n \n \n Our team is currently working through linear algebra updates, but I've changed this issue to high priority. It should be fixed before the next release.",
      "y": "Thanks for the simpler reproduction, @chrisby. \n \n \n \n Our team is currently working through linear algebra updates, but I've changed this issue to high priority. It should be fixed before the next release."
   },
   {
      "x": "How to print optimized IR?",
      "z": "Does using `export PYTORCH_JIT_LOG_LEVEL=\">profiling_graph_executor_impl\"` work for you?",
      "y": "Does using `export PYTORCH_JIT_LOG_LEVEL=\">profiling_graph_executor_impl\"` work"
   },
   {
      "x": "Distributions are not nn.Modules",
      "z": "I'd add JIT support to the constraints.\n \n I'm still very vaguely hopeful that introducing a calculated param extension like #7313 might help with the constraints and caching. (Interaction of spectral norm / weight norm with hooks and JIT came up recently, too.)\n \n Unfortunately, barring unforeseen interest, I'm not going to be working on it.",
      "y": "I'd add JIT support to the constraints.\n \n I'm still very vaguely hopeful that introducing a calculated param extension like #7313 might help with the constraints and caching. (Interaction of spectral norm / weight norm with hooks and JIT came up recently, too.)\n  I'm not going to be working on it."
   },
   {
      "x": "RuntimeError: has_marked_unused_parameters_ INTERNAL ASSERT FAILED at /opt/conda/conda-bld/pytorch_1570910687650/work/torch/csrc/distributed/c10d/reducer.cpp:290, please report a bug to PyTorch.",
      "z": "Excellent! Thanks for the update.\n \n \n \n This means you have either a data dependent control flow in your model, or you simply define some parameters and never use them. If it's the former, then great. If it's the latter, I would look for the parameters that you're not using and remove them. There is a small perf cost to finding unused parameters that you can remove if this is the case.",
      "y": "\n This means you have either a data dependent control flow in your model, or you simply define some parameters and never use them. If it's the former, then great. If it's the latter, I would look for the parameters that you're not using and remove them. There is a small perf cost to finding unused parameters that you can remove if this is the case."
   },
   {
      "x": "NCCL Error 4: invalid argument",
      "z": "This was fixed in #29014\n \n \n \n Should probably request a cherry pick for 1.4.0",
      "y": "This was fixed in #29014\n \n \n \n Should probably request a cherry pick for 1.4.0"
   },
   {
      "x": "ONNX exporter: issues with F.interpolate",
      "z": "@kossnick this issue is fixed in master, I tested your script with the latest pytorch-nighlty and it ran succesfully :)\n \n \n \n Yes, the graph is different when scale_factor and output_size are given; we are also working on some improvements and optimizations to the graph in https://github.com/pytorch/pytorch/pull/28324 that should be merged soon.",
      "y": " this issue is fixed in master, I tested your script with the latest pytorch-nighlty and it ran succesfully :)\n \n \n \n Yes, the graph is different when scale_factor and output_size are given; we are also working on some improvements and optimizations to the graph in https://github.com/pytorch/pytorch/pull/28324 that should be merged soon."
   },
   {
      "x": "torch.logsumexp only works on a single dimension, but doc says tuple of dims should work",
      "z": "I think this is fixed as of version 1.5:\n \n \n \n ```python\n \n >>> A = torch.ones(3, 3) \n \n >>> torch.logsumexp(A, dim=(0, 1)) \n \n Out[]: tensor(3.1972)\n \n ```",
      "y": "I think this is fixed as of version 1.5:\n \n \n \n ```python\n \n >>> A = torch.ones(3, 3) \n \n >>> torch.logsumexp(A, dim=(0, 1)) \n \n Out[]: tensor(3.1972)\n \n ```"
   },
   {
      "x": "pin_memory stuck in DDP/Reducer constructor",
      "z": "Right before the point where you suggest adding a `time.sleep(3)` the model parameters are broadcast from rank 0 to all other ranks. This runs NCCL kernels (see `_broadcast_coalesced`).\n \n \n \n I won't be surprised that this is what's interfering with pinned memory allocation.\n \n \n \n Can you run this with `NCCL_DEBUG=INFO` to see if anything stands out?",
      "y": "Right before the point where you suggest adding a `time.sleep(3)` the model parameters are broadcast from rank 0 to all other ranks. This runs NCCL kernels (see `_broadcast_coalesced`).\n \n \n \n I won't be surprised that this is what's interfering with pinned memory allocation.\n \n \n \n Can you run this with `NCCL_DEBUG=INFO` to see if anything stands out?"
   },
   {
      "x": "TestAutogradDeviceTypeCUDA.test_logdet_1x1_cuda is flaky",
      "z": "I don't think it has, @kurtamohler. Let's close it.",
      "y": "I don't think it has, @kurtamohler. Let's close it."
   },
   {
      "x": "macOS 10.15 on Ryzen get Intel MKL ERROR: CPU 0 is not supported.",
      "z": "@liygzting \n \n export MKL_DEBUG_CPU_TYPE=5\n \n this seems to work well for me,mac os 15.1 on AMD,\u00e8\u20ac\u0081\u00e5\u201c\u00a5\u00e7\u2030\u203a\u00e7\u0161\u00ae",
      "y": "\n export MKL_DEBUG_CPU_TYPE=5\n \n this seems to work well for me,mac os 15.1 on AMD,\u00e8\u20ac\u0081\u00e5\u201c\u00a5\u00e7\u2030\u203a\u00e7\u0161\u00ae"
   },
   {
      "x": "test_int_pow_cuda failed on Windows",
      "z": "It is both reproducible on circleci and jenkins once you use vs2019 as compiler.",
      "y": "It is both reproducible on circleci and jenkins once you use vs2019 as compiler."
   },
   {
      "x": "cannot find /usr/local/cuda-* and no nvcc inside pytorch docker container",
      "z": "@marcoleewow you are using runtime images that don't contain cuda toolkit and nvcc. If you need cuda toolkit, you should use *devel pytorch image that, as @ezyang says, uses `nvidia/cuda:10.1-cudnn7-devel-ubuntu16.04` as a base.",
      "y": " you are using runtime images that don't contain cuda toolkit and nvcc. If you need cuda toolkit, you should use *devel pytorch image that, as @ezyang says, uses `nvidia/cuda:10.1-cudnn7-devel-ubuntu16.04` as a base."
   },
   {
      "x": "Cannot init cuda under cuda10.1 & pytorch 1.3.1",
      "z": "jsut for future googlers: I found following solution - https://discuss.pytorch.org/t/cuda-runtime-error-999/69658/11\n \n \n \n > You could try to reload the nvidia kernel module via:\n \n > \n \n > sudo rmmod nvidia_uvm\n \n > sudo modprobe nvidia_uvm\n \n > Ubuntu seems to have some issues with sleep/suspend (or maybe Linux in general?).\n \n >",
      "y": "jsut for future googlers: I found following solution - https://discuss.pytorch.org/t/cuda-runtime-error-999/69658/11\n \n \n \n > You could try to reload the nvidia kernel module via:\n \n > \n \n > sudo rmmod nvidia_uvm\n \n > sudo modprobe nvidia_uvm\n \n > Ubuntu seems to have some issues with sleep/suspend (or maybe Linux in general?).\n \n >"
   },
   {
      "x": "CUDNN and NCCL paths are not detected while they are set",
      "z": "I think you want to set the environment variable `CUDNN_LIBRARY` instead of `CUDNN_LIBRARY_PATH`. I believe `CUDNN_LIBRARY_PATH` is set based on `CUDNN_LIBRARY`. (Alternatively set `CUDNN_ROOT`)\n \n \n \n https://github.com/pytorch/pytorch/blob/92750acb88ec5539658b1c51ee27a648ce92a33a/cmake/Modules_CUDA_fix/FindCUDNN.cmake#L4-L7\n \n \n \n Also see the NCCL variables:\n \n \n \n https://github.com/pytorch/pytorch/blob/92750acb88ec5539658b1c51ee27a648ce92a33a/cmake/Modules/FindNCCL.cmake#L4-L6",
      "y": "I think you want to set the environment variable `CUDNN_LIBRARY` instead of `CUDNN_LIBRARY_PATH`. I believe `CUDNN_LIBRARY_PATH` is set based on `CUDNN_LIBRARY`. (Alternatively set `CUDNN_ROOT`)\n \n \n \n https://github.com/pytorch/pytorch/blob/92750acb88ec5539658b1c51ee27a648ce92a33a/cmake/Modules_CUDA_fix/FindCUDNN.cmake#L4-L7\n \n \n \n Also see the NCCL variables:\n \n \n \n https://github.com/pytorch/pytorch/blob/92750acb88ec5539658b1c51ee27a648ce92a33a/cmake/Modules/FindNCCL.cmake#L4-L6"
   },
   {
      "x": "torch.utils.data.dataloader doesn't support multiprocessing with multiple workers",
      "z": "It\u00e2\u20ac\u2122s impossible to set your subprocess as daemonic and also ask to use multiprocessing workers. You should either not set your subprocess as daemonic or use num_workers=0. This isn\u00e2\u20ac\u2122t a bug at all. \n",
      "y": "It's impossible to set your subprocess as daemonic and also ask to use multiprocessing workers. You should either not set your subprocess as daemonic or use num_workers=0."
   },
   {
      "x": "Trouble building for ROCm",
      "z": "Closing ticket. Cannot reproduce with Ubuntu 20.04, ROCm 3.9, and reproduction step of USE_ROCM=1 USE_LMDB=1 USE_OPENCV=1 MAX_JOBS=8 python3 setup.py install --user.\n\nAlso note that our ROCm PyTorch CI jobs have been successful for a long while now, for example most of the ROCm 3.x release series.\n\nPlease reopen if you continue to face issues with ROCm PyTorch builds. Thanks.",
      "y": "Note that our ROCm PyTorch CI jobs have been successful for a long while now, for example most of the ROCm 3.x release series. Please check latest docs."
   },
   {
      "x": "torch.load issue on loading file created by torch.save",
      "z": "I've had the same ussie, until I put the model on CPU then saved . torch.load works in this case\nIt seems the issue was in tensor.cuda and tensor",
      "y": "Put the model on CPU and then save.I t may be an issue with tensor.cuda"
   },
   {
      "x": "The mkldnn multiplication goes wrong when compiled with -march=native and the tensors are sufficiently large (>=1024 items) on both Haswell and Cascadelake architecture CPUs.\n\n",
      "z": "I built pytorch master with LDFLAGS=\"$LDFLAGS -ldl\" USE_FFMPEG=ON USE_GFLAGS=ON USE_GLOG=ON CFLAGS=\"-O2 -ftree-vectorize -march=native -fno-math-errno\" CXXFLAGS=\"-O2 -ftree-vectorize -march=native -fno-math-errno\" BUILD_TEST=0 python setup.py develop and\n\ndef test_mul(self):\n        size = 1024\n        x = torch.randn(size, dtype=torch.float32)\n        mx = x.to_mkldnn()\n        self.assertEqual((x * x), (mx * mx).to_dense())\nrun successfully.\nTested on Intel(R) Xeon(R) CPU E5-2680 v3 @ 2.50GHz, according to wikipedia Xeon E5 v3 has Haswell architecture.",
      "y": "Pytorch master has been updated with the required configuration. Tested on Intel(R) Xeon(R) CPU E5-2680 v3 @ 2.50GHz, according to wikipedia Xeon E5 v3 has Haswell architecture."
   },
   {
      "x": "What is the purpose of fp16 training? faster training? or better accuracy?",
      "z": "From some experiments I've run on MNIST with LeNet for 20 epochs, the 1st method will lead to some loss in accuracy of the model\n\nFP32: Acc: 96.790% (9679/10000)\nSame weight converted to FP16: Acc: 96.690% (9669/10000)\nTrained with FP16: Acc: 96.090% (9609/10000)\nAlthough the accuracy gap is not much for dataset like MNIST using method (1) it should be more for real world dataset. Training on FP16 adapts the weight for FP16 inference so it should be able to match the accuracy with sufficient training.\n\nTraining on FP32 and converting it to FP16 will also throw off the weights as per my hunch. This can be checked by training using both methods and comparing the weight distribution.\n\nAlso training with FP16 on supported GPUs drastically cuts off training time while preserving accuracy as shown in studies and leads to smaller memory footprint.\n\nI did a detailed comparison of the training on different GPUs, summarised in suvojit-0x55aa/mixed-precision-pytorch. Feel free to play around with the code and share your results.",
      "y": "Training with FP16 on supported GPUs drastically cuts off training time while preserving accuracy as shown in studies and leads to smaller memory footprint."
   },
   {
      "x": "torch.Tensor([True]) returns different result than torch.tensor([True])",
      "z": "@izdeby in your example you used the lower-case torch.tensor() method. This does already now produce a BoolTensor.\n\nIf you write torch.Tensor([True]) or torch.Tensor([1]) ... it will invoke the ctor of the default tensor type (by default FloatTensor) in both cases even though a list of boolean or an integer values is passed. The default tensor type can be set by torch.set_default_tensor_type(). To explicitly select the type you can use the corresponding tensor class, e.g. torch.BoolTensor([True]).\n\nOnly the torch.tensor() (lower case 't') factory method does tensor-type deduction.",
      "y": "If you write torch.Tensor([True]) or torch.Tensor([1]) ... it will invoke the ctor of the default tensor type (by default FloatTensor) in both cases even though a list of boolean or an integer values is passed. The default tensor type can be set by torch.set_default_tensor_type(). To explicitly select the type you can use the corresponding tensor class, e.g. torch.BoolTensor([True])."
   },
   {
      "x": "Conda install gives 1.0.0 with CUDA 9.0",
      "z": "@LLNLanLeN upgrade your NVIDIA driver to latest, and that will likely fix the issue. cudatoolkit 10.1 requires much newer driver than 10.0",
      "y": "upgrade your NVIDIA driver to latest"
   },
   {
      "x": "[Feature Request]: Batchwise torch.lstsq",
      "z": "If you are only looking to compute the `solution` output from [`torch.lstsq`](https://pytorch.org/docs/stable/generated/torch.lstsq.html), but batch-wise, you could make use of [`torch.pinverse`](https://pytorch.org/docs/stable/generated/torch.pinverse.html) which is implemented to support batch-wise computation.\n\n> More about the relationship between the Moore-Penrose pseudo-inverse and ordinary least-squares can be found here: https://en.wikipedia.org/wiki/Moore%E2%80%93Penrose_inverse#Linear_least-squares\n\nSo that, if the take the example from the [`torch.lstsq`](https://pytorch.org/docs/stable/generated/torch.lstsq.html) documentation:\n\n# No batch support\n\n```python\nimport torch\n\nA = torch.tensor([[1., 1, 1],\n                  [2, 3, 4],\n                  [3, 5, 2],\n                  [4, 2, 5],\n                  [5, 4, 3]])\n\nB = torch.tensor([[-10., -3],\n                  [ 12, 14],\n                  [ 14, 12],\n                  [ 16, 16],\n                  [ 18, 16]])\n\nX, _ = torch.lstsq(B, A)\n\nX[:A.shape[1]]  # According to the doc: \"The first n rows of X contains the solution.\"\n\n# tensor([[2.0000, 1.0000],\n#         [1.0000, 1.0000],\n#         [1.0000, 2.0000]])\n```\n\n# With batch support\n\n```python\ndef batch_lstsq(\n    input: torch.Tensor,  # matrix B of shape (batch * m * k) \n    A: torch.Tensor  # matrix A of shape (batch * m * n) \n):\n   \n    X = torch.bmm(\n        torch.pinverse(A),\n        input\n    )\n \n    return X\n```\n\n## Tests\n\n- with the same tensors as the ones defined above, but repeated\n\n```python\nn_batch = 3\n\nA_batch = torch.cat(n_batch * [A.unsqueeze(0)])\nB_batch = torch.cat(n_batch * [B.unsqueeze(0)])\n\nsolution = batch_lstsq(B_batch, A_batch)\n\nfor i in range(len(solution)):    \n    \n    X, _ = torch.lstsq(B_batch[i], A_batch[i])\n    X = X[:A_batch[i].shape[1]]\n    \n    assert torch.allclose(solution[i], X[:A.shape[1]])\n```\n\n- with random tensors, works for both (m >= n) and (m < n)\n\n```python\nn_batch = 3\nm = 10  # 2\nn = 3\nk = 2\n\nA_batch = torch.rand(n_batch, m, n)\nB_batch = torch.rand(n_batch, m, k)\n\nsolution = batch_lstsq(B_batch, A_batch)\n\nfor i in range(len(solution)):    \n    \n    X, _ = torch.lstsq(B_batch[i], A_batch[i])\n    X = X[:A_batch[i].shape[1]]\n    \n    assert torch.allclose(solution[i], X[:A.shape[1]])\n```\n\n## Wall clock performance test\n\n- Setup\n```\nn_batch = 1000\n\nA_batch = torch.cat(n_batch * [A.unsqueeze(0)])\nB_batch = torch.cat(n_batch * [B.unsqueeze(0)])\n\nm = A_batch.shape[2]\n```\n\n- with a for loop as suggested by @Electric-Turtle \n\n```\n%%timeit\nX = torch.stack([torch.lstsq(B_batch[i], A_batch[i])[0][:m] for i in range(n_batch)],dim=0)\n\n# on my machine\n# 16.7 ms \u00b1 320 \u00b5s per loop (mean \u00b1 std. dev. of 7 runs, 100 loops each)\n```\n\n- when using batch-wise `torch.pinverse`\n```\n%%timeit\nX = batch_lstsq(B_batch, A_batch)\n\n# on my machine\n# 3.02 ms \u00b1 60.7 \u00b5s per loop (mean \u00b1 std. dev. of 7 runs, 100 loops each)\n```\n\n\n\n\n\n",
      "y": "you could make use of [`torch.pinverse`](https://pytorch.org/docs/stable/generated/torch.pinverse.html) which is implemented to support batch-wise computation.\n\n> More about the relationship between the Moore-Penrose pseudo-inverse and ordinary least-squares can be found here: https://en.wikipedia.org/wiki/Moore%E2%80%93Penrose_inverse#Linear_least-squares"
   },
   {
      "x": "[Android] Unknown builtin op: aten::mul",
      "z": "> Oh man, I went through the bother of reproducing the error by creating a simple app, then added `torch::autograd::AutoGradMode guard(false);` and that fixed everything (also in my real app)!\n\nGlad to know it helps. Did it fix the \"_adaptive_avg_pool2d_backward\" error or not?\n\nIn case you don't know, we also created a sample android app that you can play with: https://github.com/pytorch/android-demo-app\n\nMore information can be found at: https://pytorch.org/mobile/home/",
      "y": "add `torch::autograd::AutoGradMode guard(false);` "
   },
   {
      "x": "quantizationed model cannot inference with cuda?",
      "z": "That is correct, we don't support cuda currently.",
      "y": "That is correct, we don't support cuda currently."
   },
   {
      "x": "Can not use .cuda() function to load the model into GPU using Pytorch 1.3",
      "z": "@sayakpaul as the error message says, upgrade your CUDA driver, you installed the CUDA 10.1 compatible pytorch package which is the default.",
      "y": "upgrade your CUDA driver, you installed the CUDA 10.1 compatible pytorch package which is the default."
   },
   {
      "x": "Try torch.nn.quantized.functional.conv2d failed",
      "z": "Finally, it works and thanks for your help!\n```\nqF.conv2d(q_inputs, q_filters, bias)\n```\nWhat's more, the data type of convolutional kernals seems to be quint8, and data type of intput tensor is qint8.",
      "y": "```\nqF.conv2d(q_inputs, q_filters, bias)\n```\nthe data type of convolutional kernals seems to be quint8, and data type of intput tensor is qint8."
   },
   {
      "x": "Support dictionary outputs in TorchScript tracer",
      "z": "@mohammedayub44 this is a feature after 1.5, you have to install master or nightly builds to get this feature, or wait for 1.6 :)",
      "y": "this is a feature after 1.5, you have to install master or nightly builds to get this feature, or wait for 1.6"
   },
   {
      "x": "[feature request] Batched (n-1)-D to n-D matrix_diag",
      "z": "@chrelli check my comment in https://github.com/pytorch/pytorch/issues/5198#issuecomment-425069863\nHere is a proposed implementation\n```python\ndef matrix_diag(diagonal):\n    N = diagonal.shape[-1]\n    shape = diagonal.shape[:-1] + (N, N)\n    device, dtype = diagonal.device, diagonal.dtype\n    result = torch.zeros(shape, dtype=dtype, device=device)\n    indices = torch.arange(result.numel(), device=device).reshape(shape)\n    indices = indices.diagonal(dim1=-2, dim2=-1)\n    result.view(-1)[indices] = diagonal\n    return result\n```",
      "y": "Here is a proposed implementation\n```python\ndef matrix_diag(diagonal):\n    N = diagonal.shape[-1]\n    shape = diagonal.shape[:-1] + (N, N)\n    device, dtype = diagonal.device, diagonal.dtype\n    result = torch.zeros(shape, dtype=dtype, device=device)\n    indices = torch.arange(result.numel(), device=device).reshape(shape)\n    indices = indices.diagonal(dim1=-2, dim2=-1)\n    result.view(-1)[indices] = diagonal\n    return result\n```"
   },
   {
      "x": "[caffe2] VS2017 compiler error for ATen",
      "z": "I hit the same problem and updating to VS 2017 15.9.0 also fixed it for me. I'm not trying to build with CUDA, but have CUDA 10.0 installed.",
      "y": "updating to VS 2017 15.9.0 fixes it. I'm not trying to build with CUDA, but have CUDA 10.0 installed."
   },
   {
      "x": "GPU hangs after killing the program using DistributedDataparallel Model",
      "z": "hi, i  meet the same issue, my solution is :\ntry\n`ps aux|grep python` \nand then \n`kill -9 PID`\nbe sure that kill process **in order**",
      "y": "`ps aux|grep python` \nand then \n`kill -9 PID`\nbe sure that kill process **in order**"
   },
   {
      "x": "Error building a custom PyTorch CUDA extension with 1.0 on macOS High Sierra",
      "z": "We deprecated and removed FFI two days ago. Please try to build our new C++ extension API: https://pytorch.org/tutorials/advanced/cpp_extension.html",
      "y": "Please try to build our new C++ extension API: https://pytorch.org/tutorials/advanced/cpp_extension.html"
   },
   {
      "x": "get_cudnn_version in collect_env is flaky",
      "z": "That's because LD_LIBRARY_PATH is not the only path used for lookup.",
      "y": "That's because LD_LIBRARY_PATH is not the only path used for lookup."
   },
   {
      "x": "Slowdown in distributions log_prob methods",
      "z": "Yes some of my benchmarks showed that the main slowdown in `log_prob`was in `pow` but that slowdown doesn't scale with input shapes, which might be related to some tensor overhead we added after 0.4. \nSorry didn't have time to get back to this last week, will do this week. ",
      "y": "Yes some of my benchmarks showed that the main slowdown in `log_prob`was in `pow`"
   },
   {
      "x": "[Feature Request]Synchronized batch norm",
      "z": "If anyone happens to be struggling to get the `DistributedDataParallel` to use `SyncBatchnorm`, take a look at this small step-by-step a colleague of mine wrote: [github/dougsouza/pytorch-sync-batchnorm-example](https://github.com/dougsouza/pytorch-sync-batchnorm-example). Hope it helps. :)",
      "y": " take a look at this: [github/dougsouza/pytorch-sync-batchnorm-example](https://github.com/dougsouza/pytorch-sync-batchnorm-example)."
   },
   {
      "x": "Non Deterministic Behaviour even after cudnn.deterministic = True and cudnn.benchmark=False",
      "z": "I don't think there is much to do, and FGSA is intentionally discontinuous (due to the sign). What you could do is determine the digit where it is unstable empirically and round that away before taking the sign.\n\nThat said, the reason is likely that the backward uses the cuda function `atomicAdd`, which is non-deterministic. For a factor of two, it would seem that\n```python\nclass MyUpsample2(nn.Module):\n    def forward(self, x):\n        return x[:, :, :, None, :, None].expand(-1, -1, -1, 2, -1, 2).reshape(x.size(0), x.size(1), x.size(2)*2, x.size(3)*2)\n```\nmakes this deterministic.\n\nI'll probably add a note to https://github.com/pytorch/pytorch/blob/master/docs/source/notes/randomness.rst about atomic add (Users include: index_add, scatter_add, bincount, lossctc in forward, embedding/embedding_bag, all sorts of pooling, padding, sampling in backward, possibly also sparse to dense or coalescing, but I didn't check).\n",
      "y": "```python\nclass MyUpsample2(nn.Module):\n    def forward(self, x):\n        return x[:, :, :, None, :, None].expand(-1, -1, -1, 2, -1, 2).reshape(x.size(0), x.size(1), x.size(2)*2, x.size(3)*2)\n```\nmakes this deterministic.\n"
   },
   {
      "x": "PackagesNotFoundError: unable to install torchvision in anaconda prompt on windows 10",
      "z": "Use `pip install torchvision` instead. It is not yet available in Anaconda Cloud.",
      "y": "Use `pip install torchvision` instead. It is not yet available in Anaconda Cloud."
   },
   {
      "x": "Weight decay modifies grad for SGD but not for Adam",
      "z": "no adam modifies it in place see:\n\nhttps://github.com/pytorch/pytorch/blob/master/torch/optim/adam.py#L90\n",
      "y": "no adam modifies it in place see:\n\nhttps://github.com/pytorch/pytorch/blob/master/torch/optim/adam.py#L90\n"
   },
   {
      "x": "Bug in masked_fill_ for non contiguous tensors",
      "z": "> `expanded_tensor * scalar` now copies it into a non contiguous tensor on linux\n\nGood to know! We're relying on it triggering a `.contiguous()` in a number of places in Pyro.",
      "y": "> `expanded_tensor * scalar` now copies it into a non contiguous tensor on linux"
   },
   {
      "x": "[Conv3D][cudnn] CUDNN_STATUS_NOT_INITIALIZED error when using batch size > 1 with conv layer",
      "z": "@zou3519  Thank you for the reply. Unfortunately due to a non disclosure agreement, I am not allowed to upload code or parts of the code. But I found the solution. I was using a custom collate function for dataloader and one of the variables was initialized wrong. I was copying a tensor (index wise) to another, based on the indices in the wrongly initialized variable right before the conv3d layer. So it was more like an \"out of range\" error or something and I was confused by the CUDNN_STATUS_NOT_INITIALIZED error statement. ",
      "y": "It is more like an \"out of range\" error or something and confused by the CUDNN_STATUS_NOT_INITIALIZED error statement. "
   },
   {
      "x": "Documentation not clear on torch.expand() alternatives when performing torch.autograd.gradcheck",
      "z": "The note only talks about input though. It should have nothing to do with how your function is implemented. All you need to make sure is that the input used in gradcheck don't have overlapping storage.",
      "y": "All you need to make sure is that the input used in gradcheck don't have overlapping storage."
   },
   {
      "x": "Build failed when linking bin/test_parallel with multiple openmp functions defined",
      "z": "I managed to complete the build+install process with a quick and dirty workaround:\n\n> find . -type f -name 'build.make' -exec sed -i 's:^.*\\: /usr/lib/x86_64-linux-gnu/libiomp5.so::g' {} \\;\n> find . -type f -name 'link.txt' -exec sed -i 's:/usr/lib/x86_64-linux-gnu/libiomp5.so::g' {} \\;\n\nI think I've run these commands mainly in `build/caffe2/`, `build/test_api/` and `build/test_jit/` one can simply run them in `build` and everything should build fine.\n",
      "y": "\n> find . -type f -name 'build.make' -exec sed -i 's:^.*\\: /usr/lib/x86_64-linux-gnu/libiomp5.so::g' {} \\;\n> find . -type f -name 'link.txt' -exec sed -i 's:/usr/lib/x86_64-linux-gnu/libiomp5.so::g' {} \\;\n\nI think I've run these commands mainly in `build/caffe2/`, `build/test_api/` and `build/test_jit/` one can simply run them in `build` and everything should build fine.\n"
   },
   {
      "x": "CTCLoss CUDA backward throws setStorage: storage size mismatch error",
      "z": "The canonical way is to build torchvision from source, it's really easy (much more so than PyTorch itself).\n",
      "y": "The canonical way is to build torchvision from source, it's really easy (much more so than PyTorch itself).\n"
   },
   {
      "x": "In-place operation on differentiable view leaks memory",
      "z": "This is a ref-cycle in cpp that goes as `x -> CopySlices -> AccumulateGrad -> x`.\nThis only happens because `x` here is a leaf Tensor.\nNote that after doing such thing, trying to run backward will fail with \"leaf variable has been moved into the graph interior\" so we should fail earlier before the cycle is created.\nThis can be done by making [this check](https://github.com/pytorch/pytorch/blob/6249d7302b7277864ed0ade93f58d88ee0cd3aa8/torch/csrc/autograd/VariableTypeUtils.h#L44) detect views of leafs in addition to only leafs.",
      "y": "This is a ref-cycle in cpp that goes as `x -> CopySlices -> AccumulateGrad -> x`.\nThis only happens because `x` here is a leaf Tensor.\nNote that after doing such thing, trying to run backward will fail with \"leaf variable has been moved into the graph interior\" so we should fail earlier before the cycle is created."
   },
   {
      "x": "Converting from IValue to double gives INTERNAL ASSERT failure",
      "z": "The `.to...` methods don't do any conversion, you'll have to cast the IValue to a tensor `.toTensor()` then get the value out. The `toDouble()` method only works if the `IValue` contained is a C++ `double` type",
      "y": "The `.to...` methods don't do any conversion, you'll have to cast the IValue to a tensor `.toTensor()` then get the value out. The `toDouble()` method only works if the `IValue` contained is a C++ `double` type"
   },
   {
      "x": "cmake error build failed",
      "z": "Two options available: \n1. unset `CMAKE_GENERATOR_TOOLSET_VERSION` \n```cmd\nset CMAKE_GENERATOR_TOOLSET_VERSION=\n```\n2. activate the env.\n```cmd\nset DISTUTILS_USE_SDK=1\nfor /f \"usebackq tokens=*\" %i in (`\"%ProgramFiles(x86)%\\Microsoft Visual Studio\\Installer\\vswhere.exe\" -version [15^,16^) -products * -latest -property installationPath`) do call \"%i\\VC\\Auxiliary\\Build\\vcvarsall.bat\" x64 -vcvars_ver=%CMAKE_GENERATOR_TOOLSET_VERSION%\n```\nIt is clearly written in the doc through the previous link. If it is still broken, then please check your VS installation.",
      "y": "Two options available: \n1. unset `CMAKE_GENERATOR_TOOLSET_VERSION` \n```cmd\nset CMAKE_GENERATOR_TOOLSET_VERSION=\n```\n2. activate the env.\n```cmd\nset DISTUTILS_USE_SDK=1\nfor /f \"usebackq tokens=*\" %i in (`\"%ProgramFiles(x86)%\\Microsoft Visual Studio\\Installer\\vswhere.exe\" -version [15^,16^) -products * -latest -property installationPath`) do call \"%i\\VC\\Auxiliary\\Build\\vcvarsall.bat\" x64 -vcvars_ver=%CMAKE_GENERATOR_TOOLSET_VERSION%\n```\nIt is clearly written in the doc through the previous link. If it is still broken, then please check your VS installation."
   },
   {
      "x": "Multiprocessing and tensor problem",
      "z": "To be more precise it still happens if I use the no grad function, hence no backprop involved. So, the problem is returning a pytorch tensor, I think. If I return the pytorch tensor as a list using the related tolist() method everything is fine. I don't know if this is expected or not.",
      "y": "To be more precise it still happens if I use the no grad function, hence no backprop involved. So, the problem is returning a pytorch tensor, I think. If I return the pytorch tensor as a list using the related tolist() method everything is fine. I don't know if this is expected or not."
   },
   {
      "x": "Bug in saving indexed torch tensors makes it much slower than numpy.",
      "z": "this is not a bug, it's actually by design.\n\nwhen you index individual elements into a Tensor, then you are holding a view into the original Tensor.\n\n```\na = torch.randn(10, 20)\nb = a[0]\nb.add_(10) # changes `a`\n```\n\nIf you actually do `torch.save([a, b], 'foo.pt')` then PyTorch makes sure that this \"view\" property is actually preserved across serialization / deserialization.\nSo, \n\n```\na = torch.randn(10, 20)\nb = a[0]\nb.add_(10) # changes `a`\n\ntorch.save((a, b), 'foo.pt')\na, b = torch.save('foo.pt')\n\nb.add_(10) # still changes `a`\n```\n\nSo, what you are seeing is that even when you are indexing, the whole original Storage is saved.\n\nYou can make pytorch only save the particular element with `torch.save(a[0].clone()`",
      "y": "this is not a bug, it's actually by design.\n\nwhen you index individual elements into a Tensor, then you are holding a view into the original Tensor.\n\n```\na = torch.randn(10, 20)\nb = a[0]\nb.add_(10) # changes `a`\n```\n\nIf you actually do `torch.save([a, b], 'foo.pt')` then PyTorch makes sure that this \"view\" property is actually preserved across serialization / deserialization."
   },
   {
      "x": "Unexpected output size for Maxpool",
      "z": "\nThe implementation should allow for the output size to be 0 if the kernel is larger than input.\n\nFor example, if ** i=2, p=0, d=1, s=2, k=3**, the kernel is 1 pixel larger than the input.\n\nThe formula says output size =  **(2 - 3 )/ 2 + 1** =  **-1/2 + 1**\n\nHowever, in  the C++ implementation above, because of truncation (and not flooring) we get output size = 1. In Python we get 0.\n\nIf k=4, then the C++ behavior becomes correct again, and we get output size = 0. It seems like a corner case in the C++ implementation. The  floor function seems correct to use, versus rounding towards zero.\n",
      "y": "\nThe implementation should allow for the output size to be 0 if the kernel is larger than input.\n\nFor example, if ** i=2, p=0, d=1, s=2, k=3**, the kernel is 1 pixel larger than the input.\n\nThe formula says output size =  **(2 - 3 )/ 2 + 1** =  **-1/2 + 1**\n\nHowever, in  the C++ implementation above, because of truncation (and not flooring) we get output size = 1. In Python we get 0.\n\nIf k=4, then the C++ behavior becomes correct again, and we get output size = 0. It seems like a corner case in the C++ implementation. The  floor function seems correct to use, versus rounding towards zero.\n"
   },
   {
      "x": "base_lrs in torch.optim.lr_scheduler.CyclicLR gets overriden by parent class if parameter groups have 'initial_lr' set",
      "z": "~~Schedulers are not currently chainable #13022, and so switching from one to another overrides the past. This should be considered for #24352.~~",
      "y": "Schedulers are not currently chainable "
   },
   {
      "x": "Implement covariance_matrix for Independent distributions",
      "z": "@fehiepsi showed me this trick for batch-compatible `torch.diag()` (I think that's what you're asking for, @justindomke )\n```py\ndef batch_diag(batched_variance):\n    batch_shape = batched_variance.shape[:-1]\n    event_size = batched_variance.size(-1)\n    cov = batched_variance.new_zeros(batch_shape + (event_size * event_size,))\n    cov[..., ::1 + event_size] = batched_variance\n    return cov.reshape(batch_shape + (event_size, event_size))\n```",
      "y": "```py\ndef batch_diag(batched_variance):\n    batch_shape = batched_variance.shape[:-1]\n    event_size = batched_variance.size(-1)\n    cov = batched_variance.new_zeros(batch_shape + (event_size * event_size,))\n    cov[..., ::1 + event_size] = batched_variance\n    return cov.reshape(batch_shape + (event_size, event_size))\n```"
   },
   {
      "x": "About CMAKE_PREFIX_PATH",
      "z": "OK I fixed that.\n\n`CMAKE_PREFIX_PATH` should be a folder containing `TorchConfig.cmake` which is `torch/share/cmake/Torch` \n\nmahmood@m2000:build$ cmake -DCMAKE_PREFIX_PATH=/home/mahmood/pytorch/torch/share/cmake/Torch/ ..\n-- The C compiler identification is GNU 7.4.0\n-- The CXX compiler identification is GNU 7.4.0\n-- Check for working C compiler: /usr/bin/cc\n-- Check for working C compiler: /usr/bin/cc -- works\n-- Detecting C compiler ABI info\n-- Detecting C compiler ABI info - done\n-- Detecting C compile features\n-- Detecting C compile features - done\n-- Check for working CXX compiler: /usr/bin/c++\n-- Check for working CXX compiler: /usr/bin/c++ -- works\n-- Detecting CXX compiler ABI info\n-- Detecting CXX compiler ABI info - done\n-- Detecting CXX compile features\n-- Detecting CXX compile features - done\n-- Looking for pthread.h\n-- Looking for pthread.h - found\n-- Looking for pthread_create\n-- Looking for pthread_create - not found\n-- Looking for pthread_create in pthreads\n-- Looking for pthread_create in pthreads - not found\n-- Looking for pthread_create in pthread\n-- Looking for pthread_create in pthread - found\n-- Found Threads: TRUE\n-- Found CUDA: /usr/local/cuda-10.0 (found version \"10.0\")\n-- Caffe2: CUDA detected: 10.0\n-- Caffe2: CUDA nvcc is: /usr/local/cuda-10.0/bin/nvcc\n-- Caffe2: CUDA toolkit directory: /usr/local/cuda-10.0\n-- Caffe2: Header version is: 10.0\n-- Found CUDNN: /usr/local/cuda-10.0/include\n-- Found cuDNN: v7.4.2  (include: /usr/local/cuda-10.0/include, library: /usr/local/cuda-10.0/lib64/libcudnn.so)\n-- Autodetected CUDA architecture(s):  5.2\n-- Added CUDA NVCC flags for: -gencode;arch=compute_52,code=sm_52\n-- Found torch: /home/mahmood/pytorch/torch/lib/libtorch.so\n-- Downloading MNIST dataset\n-- Configuring done\n-- Generating done\n-- Build files have been written to: /home/mahmood/pytorch/examples/cpp/dcgan/build\n```",
      "y": "`CMAKE_PREFIX_PATH` should be a folder containing `TorchConfig.cmake` which is `torch/share/cmake/Torch` "
   },
   {
      "x": "ReduceLROnPlateau parent class is not _LRScheduler",
      "z": "`ReduceLROnPlateau` overrides every `_LRScheduler` function. We never had need to have `_LRScheduler` as parent. \n\nPlus, as already mentioned, usage of any internal API names is a terrible idea. \n\nI will close the issue as non-bug. However feel free to send us PR which will introduce good method of checking if object is Scheduler",
      "y": "`ReduceLROnPlateau` overrides every `_LRScheduler` function. We never had need to have `_LRScheduler` as parent. "
   },
   {
      "x": "\"Trying to resize storage that is not resizable\" when calling pin_memory() on some zero-dimensional tensors",
      "z": "@qbx2 This bug seems to be fixed on master, could you try installing PyTorch via `pip install torch_nightly -f https://download.pytorch.org/whl/nightly/cu100/torch_nightly.html` and then run the code again?",
      "y": "This bug seems to be fixed on master, could you try installing PyTorch via `pip install torch_nightly -f https://download.pytorch.org/whl/nightly/cu100/torch_nightly.html` and then run the code again?"
   },
   {
      "x": "Need GPU implementation of dirichlet_grad (originally: Reparameterized gradient on GPU for beta / Dirichlet)",
      "z": "I find it surprising that there is no implementation of the Dirichlet gradient because there is already an implementation of the Gamma gradient. The standard way to generate a Beta variable is to generate two Gamma variables (see for instance [Wikipedia](https://en.wikipedia.org/wiki/Beta_distribution#Generating_beta-distributed_random_variates)) and compute their fractional ratio. For instance, the following fix on the code of @vmoens seems to work:\n\n    a,b = torch.ones(3,4,1,5,requires_grad=True,device='cuda'),torch.ones(3,4,1,5,requires_grad=True,device='cuda')\n    s1 = torch.distributions.Gamma(a,1).rsample(torch.Size((10,)))\n    s2 = torch.distributions.Gamma(b,1).rsample(torch.Size((10,)))\n    s = s1 / (s1+s2)\n    torch.sum(s).backward()\n    a.grad\n\nSame goes for sampling Dirichlet variables, one actually samples independent Gamma variables. So, right now users should be able to use this fix, but it seems to me that it should not be difficult to implement native support for Beta gradients with CUDA because all the functions are already there.\n\n@ezyang and @vishwakftw I posted a similar comment on related issue https://github.com/pytorch/pytorch/issues/11030.",
      "y": "I find it surprising that there is no implementation of the Dirichlet gradient because there is already an implementation of the Gamma gradient. The standard way to generate a Beta variable is to generate two Gamma variables (see for instance [Wikipedia](https://en.wikipedia.org/wiki/Beta_distribution#Generating_beta-distributed_random_variates)) and compute their fractional ratio. For instance, the following fix on the code of @vmoens seems to work:\n\n    a,b = torch.ones(3,4,1,5,requires_grad=True,device='cuda'),torch.ones(3,4,1,5,requires_grad=True,device='cuda')\n    s1 = torch.distributions.Gamma(a,1).rsample(torch.Size((10,)))\n    s2 = torch.distributions.Gamma(b,1).rsample(torch.Size((10,)))\n    s = s1 / (s1+s2)\n    torch.sum(s).backward()\n    a.grad\n"
   },
   {
      "x": "Seems there's memory issue in pytorch 1.0.0",
      "z": "I don't know what exactly would be the issue, but I experienced something somewhat similar, which I actually managed to pinpoint. The reason for my memory increase was using DataLoader with custom Dataset (subclassing from torch.utils.data.Dataset) where I was storing loading data in a dict instead of a simple list to accommodate shuffling. However, the indices going to method `__getitem__` (generated by DataLoader) were not simple integers as I anticipated (and as the doc would suggest, https://pytorch.org/docs/stable/data.html#torch.utils.data.Dataset) but 0-D dimensional integer tensors. And since these don't have the `__hash__` depending on a value, rather on memory location, the dict would grow and grow and grow, because it didn't know there was something in it already. Hope I made it somewhat clear. To illustrate it more:\n\n    class LiDARDataset(torch.utils.data.Dataset):\n        def __init__(self, folder, keep_ram=False):\n            self.names = sorted(glob.glob(os.path.join(folder, '*.npy')))\n            self.keep_ram = keep_ram\n            if self.keep_ram:\n                self.loaded = dict()\n            else:\n                self.loaded = weakref.WeakValueDictionary()\n\n        def __len__(self):\n            return len(self.names)\n\n        def __getitem__(self, key):\n            if not isinstance(key, int): # without these two lines, it would grow\n                key = key.item() \n            result = self.loaded.get(key) # because the result would always be None\n            if result is None:\n                result = self.transform(np.load(self.names[key]), key)\n                self.loaded[key] = result\n            return result\n\nSo, check you don't have any dictionary, which is indexed by torch.tensor, because that is what is changed since 0.4.1. Or you don't rely on hashing of torch.tensor by its value rather than the memory location",
      "y": " check you don't have any dictionary, which is indexed by torch.tensor, because that is what is changed since 0.4.1. Or you don't rely on hashing of torch.tensor by its value rather than the memory location"
   },
   {
      "x": "torch::save causes serialization error on mnist example",
      "z": "You need to wrap the torch::nn::Module object (i.e., model) with torch::nn::ModuleHolder",
      "y": "You need to wrap the torch::nn::Module object (i.e., model) with torch::nn::ModuleHolder"
   },
   {
      "x": "Implementation of ISTFT",
      "z": "@SsnL @vincentqb Seems that ISTFT has been recently contributed to torchaudio: https://pytorch.org/audio/functional.html#istft\n\nIt would be great if it was relocated to the core",
      "y": "Seems that ISTFT has been recently contributed to torchaudio: https://pytorch.org/audio/functional.html#istft"
   },
   {
      "x": "Dropout on integer tensor types dies with SIGFPE",
      "z": "@mruberry Think this issue should be closed since the signal is no longer raised.\n\n~~Instead we get this errors:~~\n\n~~```RuntimeError: Integer division of tensors using div or / is no longer supported, and in a future release div will perform true division as in Python 3. Use true_divide or floor_divide (// in Python) instead.```~~\n\n~~in both cases (the one with Dropout and the second one from @colesbury).~~ \n\nRan the code with the nightly build.\n\nFor the first example this error is raised:\n`RuntimeError: result type Float can't be cast to the desired output type Long`\n\nand for the second one (the one from @colesbury) there is no error, instead we get: \n`tensor([inf, nan, nan, nan, inf])`\n\nI think now it's more clear to the user what's the problem.",
      "y": "@mruberry Think this issue should be closed since the signal is no longer raised.\n\n~~Instead we get this errors:~~\n\n~~```RuntimeError: Integer division of tensors using div or / is no longer supported, and in a future release div will perform true division as in Python 3. Use true_divide or floor_divide (// in Python) instead.```~~\n\n~~in both cases (the one with Dropout and the second one from @colesbury).~~ \n\nRan the code with the nightly build.\n\nFor the first example this error is raised:\n`RuntimeError: result type Float can't be cast to the desired output type Long`\n\nand for the second one (the one from @colesbury) there is no error, instead we get: \n`tensor([inf, nan, nan, nan, inf])`\n\nI think now it's more clear to the user what's the problem."
   },
   {
      "x": "THCudaCheck FAIL file=/pytorch/aten/src/THC/THCGeneral.cpp line=405 error=11 : invalid argument",
      "z": "I have successfully solved this problem by updating the cuda8.0 to cuda10.0.",
      "y": "I have successfully solved this problem by updating the cuda8.0 to cuda10.0."
   },
   {
      "x": "Memory leak during backprop() in PyTorch 1.0.0",
      "z": "tl;dr After investigation I determined that the reinforcement learning tutorial doesn't leak memory on pytorch 1.2. However, we are committed to making sure there are no more memory leaks so please create new issues with individual new examples of memory leaks and we will investigate with high priority.\n\nI tried to repro this on my mac. I don't have a linux box with a monitor (which is a requirement to run the tutorial) but I suspect that the result should be the same.\n\nMy conclusion was that our reinforcement learning tutorial does NOT leak memory. However, it does grow in memory consumption. The reason why it grows in memory consumption is due to the model storing replay memory: https://github.com/pytorch/tutorials/blob/94cb6a3635b9b5ccb8a59c5346addba6901cd15d/intermediate_source/reinforcement_q_learning.py#L336\n\nOnce the replay memory limit is hit, the RAM consumption stays constant. This can be verified by setting the replay number to something low (i.e. 10) and watching RAM.\n\nI know that this wasn't completely satisfactory because I'm sure that some of you have encountered memory leaks while training in PyTorch, but we are committed to fixing them so please submit additional bug reports about memory leaks for your individual examples and we can investigate them separately. \n\n",
      "y": "tl;dr After investigation I determined that the reinforcement learning tutorial doesn't leak memory on pytorch 1.2. However, we are committed to making sure there are no more memory leaks so please create new issues with individual new examples of memory leaks and we will investigate with high priority.\n\nI tried to repro this on my mac. I don't have a linux box with a monitor (which is a requirement to run the tutorial) but I suspect that the result should be the same.\n\nMy conclusion was that our reinforcement learning tutorial does NOT leak memory. However, it does grow in memory consumption. The reason why it grows in memory consumption is due to the model storing replay memory: https://github.com/pytorch/tutorials/blob/94cb6a3635b9b5ccb8a59c5346addba6901cd15d/intermediate_source/reinforcement_q_learning.py#L336\n\nOnce the replay memory limit is hit, the RAM consumption stays constant. This can be verified by setting the replay number to something low (i.e. 10) and watching RAM.\n\nI know that this wasn't completely satisfactory because I'm sure that some of you have encountered memory leaks while training in PyTorch, but we are committed to fixing them so please submit additional bug reports about memory leaks for your individual examples and we can investigate them separately. \n\n"
   },
   {
      "x": "Debugging feature for \"modified by an inplace operation\" errors",
      "z": "> which points directly to `b.exp_()`, and indeed, if you change that line to be `b.exp()`, it all works fine.\n\nTo clarify for other readers, the anomaly detection will *not* necessarily point you at the inplace operation that caused the failure. Instead, it will point you at the operation that could not compute its gradient in the backward pass. The inplace operation to blame may occur anywhere after that, modifying one of the tensors that participated in the line found by the anomaly detection.\n\nExample:\n```\nx = torch.rand(10, 20, requires_grad=True)\ny = torch.rand(10)\nz = (x / y[:, np.newaxis])  # anomaly detection will point here\nc = y.abs_()  # but the problem is here\nz.sum().backward()\n```\nThe last line will cause a `RuntimeError`. With anomaly detection enabled, it will point at the line performing the division, but the inplace operation came later.",
      "y": "> which points directly to `b.exp_()`, and indeed, if you change that line to be `b.exp()`, it all works fine.\n\nTo clarify for other readers, the anomaly detection will *not* necessarily point you at the inplace operation that caused the failure. Instead, it will point you at the operation that could not compute its gradient in the backward pass. The inplace operation to blame may occur anywhere after that, modifying one of the tensors that participated in the line found by the anomaly detection.\n\nExample:\n```\nx = torch.rand(10, 20, requires_grad=True)\ny = torch.rand(10)\nz = (x / y[:, np.newaxis])  # anomaly detection will point here\nc = y.abs_()  # but the problem is here\nz.sum().backward()\n```\nThe last line will cause a `RuntimeError`. With anomaly detection enabled, it will point at the line performing the division, but the inplace operation came later."
   },
   {
      "x": "Keyboard interrupt and saving the last state of a model",
      "z": "This is all you need to implement this. I don't see why would this have to be part of the library.\n```python\ntry:\n    # training code here\nexcept KeyboardInterrupt:\n    # save model here\n```",
      "y": "This is all you need to implement this. I don't see why would this have to be part of the library.\n```python\ntry:\n    # training code here\nexcept KeyboardInterrupt:\n    # save model here\n```"
   },
   {
      "x": "DataLoader freezes randomly when num_workers > 0 (Multiple threads train models on different GPUs in separate threads)",
      "z": "I have solved the problem by using processes instead of threads. So in my main python file where I generate all possible hyperparameter combinations and then train each combination on a different GPU, I do not create a new thread and train a model in each thread. Instead, I fork new processes (one for each GPU) and create a `multiprocessing.Queue` that is passed to each subprocess. This queue contains hyperparameters to be trained. Each process is working on the same queue and training models on a dedicated GPU. \n\nIn my tests, this works even with num_workers=32. There was no freezing for 3 hours now. Usually it freezed much earlier (after a few minutes), so I hope that it is really fixed.\n\nSince it is working with `Process` and not with `Thread`, I assume that there is a bug in PyTorch's `DataLoader` class that causes the freezes when multiple `DataLoader`s are used in different threads. This problem does not occur with processes, because they have their private memory.",
      "y": "I have solved the problem by using processes instead of threads. So in my main python file where I generate all possible hyperparameter combinations and then train each combination on a different GPU, I do not create a new thread and train a model in each thread. Instead, I fork new processes (one for each GPU) and create a `multiprocessing.Queue` that is passed to each subprocess. This queue contains hyperparameters to be trained. Each process is working on the same queue and training models on a dedicated GPU. \n\nIn my tests, this works even with num_workers=32. There was no freezing for 3 hours now. Usually it freezed much earlier (after a few minutes), so I hope that it is really fixed.\n\nSince it is working with `Process` and not with `Thread`, I assume that there is a bug in PyTorch's `DataLoader` class that causes the freezes when multiple `DataLoader`s are used in different threads. This problem does not occur with processes, because they have their private memory."
   },
   {
      "x": "[JIT] Support Meta programming on If self.training to conditionally NOT compile training only code",
      "z": "Hi @sidazhang, thanks for reporting this. Actually `self.training` is already supported in JIT, training and inference subgraph should be both exists in the generated IR, and will depend on the actual value of self.training to execute the corresponding if branch. So we don't need to meta program it. \n\nThe problem or workaround on your example is that, you should also annotate the function `training_code_only` with `@torch.jit.script_method`, because although we can run it without making it a script_method, we cannot serialize a python function call if you want to save the graph. \n\nEdited: if `training_code_only` is a TorchScript incompatible logic and cannot be rewrite to make it compatible, can you describe what is actually not compatible so that we could fix it instead? A simple for loop like the above can be simply rewrite to make it compatible. We don't want to add additional meta programming conditions as those logics are kinda hacks to get around some special syntax, also in many cases we could not know the actual value of `self.training` until runtime, so meta programming it might not be a generic solution. ",
      "y": "Hi @sidazhang, thanks for reporting this. Actually `self.training` is already supported in JIT, training and inference subgraph should be both exists in the generated IR, and will depend on the actual value of self.training to execute the corresponding if branch. So we don't need to meta program it. \n\nThe problem or workaround on your example is that, you should also annotate the function `training_code_only` with `@torch.jit.script_method`, because although we can run it without making it a script_method, we cannot serialize a python function call if you want to save the graph. \n\nEdited: if `training_code_only` is a TorchScript incompatible logic and cannot be rewrite to make it compatible, can you describe what is actually not compatible so that we could fix it instead? A simple for loop like the above can be simply rewrite to make it compatible. We don't want to add additional meta programming conditions as those logics are kinda hacks to get around some special syntax, also in many cases we could not know the actual value of `self.training` until runtime, so meta programming it might not be a generic solution. "
   },
   {
      "x": "[QUESTION] Should _dummy_type use name instead of storage_name?",
      "z": "Are you sure the 50% failure rate has anything to do with your change? That's a Caffe2 test and it shouldn't be exercising `torch/cuda/__init__.py` at all",
      "y": "Are you sure the 50% failure rate has anything to do with your change? That's a Caffe2 test and it shouldn't be exercising `torch/cuda/__init__.py` at all"
   },
   {
      "x": "Non-monotonic execution times for increasing kernel sizes",
      "z": "Maybe cudnn is probably selecting different algorithms.\nCould you try setting `torch.backends.cudnn.benchmark = True` in the beginning of the script and compare the values again?",
      "y": "Maybe cudnn is probably selecting different algorithms.\nCould you try setting `torch.backends.cudnn.benchmark = True` in the beginning of the script and compare the values again?"
   },
   {
      "x": "How to load a model trained on GPU 0 (cuda: 0) to GPU 1 (cuda:1) for inference?",
      "z": "In all likelihood your `device` value is nonsense. Print it out and see what the problem is.",
      "y": "In all likelihood your `device` value is nonsense. Print it out and see what the problem is."
   },
   {
      "x": "Feature request : Profiler",
      "z": "MXnet\u2019s profiler is almost certainly based on (or at least inspired by) the PyTorch profiler https://pytorch.org/docs/stable/autograd.html#torch.autograd.profiler.profile",
      "y": "MXnet\u2019s profiler is almost certainly based on (or at least inspired by) the PyTorch profiler https://pytorch.org/docs/stable/autograd.html#torch.autograd.profiler.profile"
   },
   {
      "x": "LibTorch Windows binaries appear to not be built with CuDNN",
      "z": "Okay, I know the reason. The PATHs of cuDNN is not passed through arguments like in setup.py when building libtorch.",
      "y": "Okay, I know the reason. The PATHs of cuDNN is not passed through arguments like in setup.py when building libtorch."
   },
   {
      "x": "RuntimeError: Expected a 'N2at13CUDAGeneratorE' but found 'PN2at9GeneratorE'",
      "z": "Note that the error: `RuntimeError: Expected a 'N2at13CUDAGeneratorE' but found 'PN2at9GeneratorE'` originated from this line. https://github.com/pytorch/pytorch/blob/master/aten/src/ATen/CheckGenerator.h#L15. Before my PR, the generated dispatch code utilized this function: https://github.com/pytorch/pytorch/blob/master/aten/src/ATen/function_wrapper.py#L327, and put it on all random distribution function dispatch code. For CUDA side,`THGenerator*` is set to null: https://github.com/pytorch/pytorch/blob/master/aten/src/ATen/function_wrapper.py#L1247. Hence, with the THGenerator* being null, and the `check_generator` function resolving to `check_generator<CUDAGenerator>(null, &globalContext().defaultGenerator(at::CUDA))`, we were getting that error. In my PR, `check_generator` (now called `check_generator_with_default`) is removed from `function_wrapper.py` and is currently only being called for the CPU functions and hence \"solves\" the bug. \n\nThe torch.manual_seed returning a CPU Generator is still there and I agree with @ezyang that it's weird. IMO, torch.manual_seed should return nothing, like how it's in torch.cuda.manual_seed. That is, [Py_RETURN_NONE](https://github.com/pytorch/pytorch/blob/53bb739b675a0dd2af2fcb2b02e37f64a432bd2f/torch/csrc/cuda/Module.cpp#L165) vs [(PyObject*)self](https://github.com/pytorch/pytorch/blob/53bb739b675a0dd2af2fcb2b02e37f64a432bd2f/torch/csrc/Generator.cpp#L104)",
      "y": "Note that the error: `RuntimeError: Expected a 'N2at13CUDAGeneratorE' but found 'PN2at9GeneratorE'` originated from this line. https://github.com/pytorch/pytorch/blob/master/aten/src/ATen/CheckGenerator.h#L15. Before my PR, the generated dispatch code utilized this function: https://github.com/pytorch/pytorch/blob/master/aten/src/ATen/function_wrapper.py#L327, and put it on all random distribution function dispatch code. For CUDA side,`THGenerator*` is set to null: https://github.com/pytorch/pytorch/blob/master/aten/src/ATen/function_wrapper.py#L1247. Hence, with the THGenerator* being null, and the `check_generator` function resolving to `check_generator<CUDAGenerator>(null, &globalContext().defaultGenerator(at::CUDA))`, we were getting that error. In my PR, `check_generator` (now called `check_generator_with_default`) is removed from `function_wrapper.py` and is currently only being called for the CPU functions and hence \"solves\" the bug. \n\nThe torch.manual_seed returning a CPU Generator is still there and I agree with @ezyang that it's weird. IMO, torch.manual_seed should return nothing, like how it's in torch.cuda.manual_seed. That is, [Py_RETURN_NONE](https://github.com/pytorch/pytorch/blob/53bb739b675a0dd2af2fcb2b02e37f64a432bd2f/torch/csrc/cuda/Module.cpp#L165) vs [(PyObject*)self](https://github.com/pytorch/pytorch/blob/53bb739b675a0dd2af2fcb2b02e37f64a432bd2f/torch/csrc/Generator.cpp#L104)"
   },
   {
      "x": ".size() vs .shape, which one should be used?",
      "z": ".size() method returns total elements in a dataframe , for eg shape of a tensor might be (10,3) , here total elements in tensor would be returned by .size() = 10X3 = 30 elements!!",
      "y": ".size() method returns total elements in a dataframe , for eg shape of a tensor might be (10,3) , here total elements in tensor would be returned by .size() = 10X3 = 30 elements!!"
   },
   {
      "x": "[feature request] ATen Documentation and Tutorial",
      "z": "See https://pytorch.org/cppdocs/\nSee https://pytorch.org/cppdocs/notes/tensor_creation.html",
      "y": "See https://pytorch.org/cppdocs/\nSee https://pytorch.org/cppdocs/notes/tensor_creation.html"
   },
   {
      "x": "RuntimeError: dimension specified as 0 but tensor has no dimensions",
      "z": "Did you try `z = z.unsqueeze(0)`? \n\nThe reference is [here](https://github.com/hunkim/PyTorchZeroToAll/issues/24).\n",
      "y": "Did you try `z = z.unsqueeze(0)`? \n\nThe reference is [here](https://github.com/hunkim/PyTorchZeroToAll/issues/24).\n"
   },
   {
      "x": "NCCL Error 1 when using torch.nn.DataParallel",
      "z": "I can help -  @aleksod \nLooking at their official build guide: https://github.com/NVIDIA/nccl\n\nyou need to clone a different repo for the tests:\n`git clone https://github.com/NVIDIA/nccl-tests.git`\nThen, in order to build the tests, enter that repo and run:\nCUDA_HOME=[path to your cuda main install dir] NCCL_HOME=[path to your nccl build dir] make\n\nfor example\nCUDA_HOME=/usr/local/cuda-10.0 NCCL_HOME=/path/to/nccl/build make\n\nthen, an example test from their official doc is:\n`./build/all_reduce_perf -b 8 -e 256M -f 2 -g <ngpus>`\n\nif you get missing .so libraries errors, you can use LD_LIBRARY_PATH to run the test. for example:\n\n`LD_LIBRARY_PATH=/usr/local/cuda-10.0/lib64:/some/path/to/nccl/nccl/build/lib:$LD_LIBRARY_PATH ./build/all_reduce_perf -b 8 -e 256M -f 2 -g 2`\n\n\n\n\n",
      "y": "I can help -  @aleksod \nLooking at their official build guide: https://github.com/NVIDIA/nccl\n\nyou need to clone a different repo for the tests:\n`git clone https://github.com/NVIDIA/nccl-tests.git`\nThen, in order to build the tests, enter that repo and run:\nCUDA_HOME=[path to your cuda main install dir] NCCL_HOME=[path to your nccl build dir] make\n\nfor example\nCUDA_HOME=/usr/local/cuda-10.0 NCCL_HOME=/path/to/nccl/build make\n\nthen, an example test from their official doc is:\n`./build/all_reduce_perf -b 8 -e 256M -f 2 -g <ngpus>`\n\nif you get missing .so libraries errors, you can use LD_LIBRARY_PATH to run the test. for example:\n\n`LD_LIBRARY_PATH=/usr/local/cuda-10.0/lib64:/some/path/to/nccl/nccl/build/lib:$LD_LIBRARY_PATH ./build/all_reduce_perf -b 8 -e 256M -f 2 -g 2`\n\n\n\n\n"
   },
   {
      "x": "dimension out of range (expected to be in range of [-1, 0], but got 1)",
      "z": "ok, i figured it out. So the key point which i didn't understand from the documentation was that the TARGET should be just one entry saying which class it belongs to Eg:[2] instead of a one hot vector like [0,0,1]. I mean, frankly, I  imagined the input and target being of similar shape (which is more intuitive). i.e Now that the input is a vector of [0,0,1] i imagined the TARGET also should be in same shape. Anyway, am glad it worked out. I would love to have a better worded documentation though, imho. \n\n Below is the code/shapes without dimension error.\n\n```\nlen(pred_y):\ntorch.Size([1, 3])\ntensor([[ 0.0000,  0.0000,  0.1527]])\nlen(x):\ntorch.Size([1])\ntensor([ 2])\nloss_training = loss_fn(pred_y, x)\n\n\n```",
      "y": "ok, i figured it out. So the key point which i didn't understand from the documentation was that the TARGET should be just one entry saying which class it belongs to Eg:[2] instead of a one hot vector like [0,0,1]. I mean, frankly, I  imagined the input and target being of similar shape (which is more intuitive). i.e Now that the input is a vector of [0,0,1] i imagined the TARGET also should be in same shape. Anyway, am glad it worked out. I would love to have a better worded documentation though, imho. \n\n Below is the code/shapes without dimension error.\n\n```\nlen(pred_y):\ntorch.Size([1, 3])\ntensor([[ 0.0000,  0.0000,  0.1527]])\nlen(x):\ntorch.Size([1])\ntensor([ 2])\nloss_training = loss_fn(pred_y, x)\n\n\n```"
   },
   {
      "x": "Incorrect error message for advanced indexing cuda tensor",
      "z": "This has been fixed in #5583. Now one can index tensors with both CPU and CUDA tensors so this particular error message doesn't apply anymore :)",
      "y": "This has been fixed in #5583. Now one can index tensors with both CPU and CUDA tensors so this particular error message doesn't apply anymore :)"
   },
   {
      "x": "\"Reduce Failed to Synchronise\" in F.binary_cross_entropy ",
      "z": "> See also: #2209\n> \n> BCELoss accepts only inputs that have all elements in range [0; 1] but this condition doesn't hold in your case\n\nI got [the same error](https://discuss.pytorch.org/t/cuda-out-of-memory-when-optimizer-step/55942?u=shirui-japina)\n\nand I tried to use `nn.BCELoss()` like:\n\n```\noptimizer = optim.SGD(model.parameters(), lr=0.0001)\ncriterion = nn.BCELoss()\n```\n\n_loop epoch train part:_\n\n```\nprediction = model(batch_input)\nloss = criterion(torch.sigmoid(prediction), label)\n\noptimizer.zero_grad()\nloss.backward()\noptimizer.step()\n```\n\nThen I solved the problem. Thank you for your comment!\n(But I don't know why optim.Adam() can't work well. It still errors: CUDA out of memory.)\n",
      "y": "> See also: #2209\n> \n> BCELoss accepts only inputs that have all elements in range [0; 1] but this condition doesn't hold in your case\n\nI got [the same error](https://discuss.pytorch.org/t/cuda-out-of-memory-when-optimizer-step/55942?u=shirui-japina)\n\nand I tried to use `nn.BCELoss()` like:\n\n```\noptimizer = optim.SGD(model.parameters(), lr=0.0001)\ncriterion = nn.BCELoss()\n```\n\n_loop epoch train part:_\n\n```\nprediction = model(batch_input)\nloss = criterion(torch.sigmoid(prediction), label)\n\noptimizer.zero_grad()\nloss.backward()\noptimizer.step()\n```\n\nThen I solved the problem. Thank you for your comment!\n(But I don't know why optim.Adam() can't work well. It still errors: CUDA out of memory.)\n"
   },
   {
      "x": "Device-side Assert in `THCReduceAll.cuh:339`",
      "z": "You have out-of-bound indices when you create the one-hot vector. The error will be raised on the responsible line if you use `CUDA_LAUNCH_BLOCKING=1`",
      "y": "You have out-of-bound indices when you create the one-hot vector. The error will be raised on the responsible line if you use `CUDA_LAUNCH_BLOCKING=1`"
   },
   {
      "x": "Import Error : no module  named torch",
      "z": "Add the path by: export PATH=~/anaconda3/bin:$PATH  \nbefore opening the python.",
      "y": "Add the path by: export PATH=~/anaconda3/bin:$PATH  \nbefore opening the python."
   },
   {
      "x": "torch.autograd.Function memory leak",
      "z": "Yes. The leaks happen only if you hold on to outputs via a mechanism different than `save_for_backward`",
      "y": "Yes. The leaks happen only if you hold on to outputs via a mechanism different than `save_for_backward`"
   },
   {
      "x": "Missing torch.* docs",
      "z": "#5443 addresses all but `torch.default_generator`. :)",
      "y": "#5443 addresses all but `torch.default_generator`. :)"
   },
   {
      "x": "Add Scale Factor To SGD",
      "z": "1. because it's so much more common than rescaling gradients by a constant that doesn't depend on the iteration\n2. it needs to be performed on weights and not gradients, and this requires an extra `no_grad` block\n3. ideally it wouldn't be part of the optimizer, but we're stuck with it for now because of backward compatibility",
      "y": "1. because it's so much more common than rescaling gradients by a constant that doesn't depend on the iteration\n2. it needs to be performed on weights and not gradients, and this requires an extra `no_grad` block\n3. ideally it wouldn't be part of the optimizer, but we're stuck with it for now because of backward compatibility"
   },
   {
      "x": "[bug] Expected GLOO_USE_IBVERBS to be defined",
      "z": "Looks like the reproduced failure was due to a staled gloo library.  After starting everything from fresh, everything works fine. So as I said, @alexholdenmiller, could you try building from scratch by git clone --recursive the latest PyTorch master. \n\nOr if you don't want to clone a new pytorch, at least, please make sure torch/lib/gloo folder's git status is clean.  Also in Pytorch folder, please do git submodule update to get gloo lib updated.\n\nIf this works, please close this issue.",
      "y": "Looks like the reproduced failure was due to a staled gloo library.  After starting everything from fresh, everything works fine. So as I said, @alexholdenmiller, could you try building from scratch by git clone --recursive the latest PyTorch master. \n\nOr if you don't want to clone a new pytorch, at least, please make sure torch/lib/gloo folder's git status is clean.  Also in Pytorch folder, please do git submodule update to get gloo lib updated.\n\nIf this works, please close this issue."
   },
   {
      "x": "[bug]  assert len(modules) == len(inputs) when use torch.distributed to compute last batch",
      "z": "As a quick fix you can pass `drop_last=True` when creating the data loader, but it's a bug that we'll need to fix",
      "y": "As a quick fix you can pass `drop_last=True` when creating the data loader, but it's a bug that we'll need to fix"
   },
   {
      "x": "[bug?] Problem with load_state_dict after installing latest pytorch by source",
      "z": "The `running_*` are disabled by default for InstanceNorm* layers after #4922 . You can add `track_running_stats=True` to the InstanceNorm* layer constructors.",
      "y": "The `running_*` are disabled by default for InstanceNorm* layers after #4922 . You can add `track_running_stats=True` to the InstanceNorm* layer constructors."
   },
   {
      "x": "when pytorch should add Windows support.",
      "z": "Starting from the next release. Should be out within weeks",
      "y": "Starting from the next release. Should be out within weeks"
   },
   {
      "x": "In IPython torch.dot always returns 0",
      "z": "please do:\n\n```\npip uninstall -y numpy\nconda install -y numpy \n```\n\nThe pip version of numpy is linked against OpenBLAS, which has a `dot` that conflicts in ABI with MKL's `dot`",
      "y": "please do:\n\n```\npip uninstall -y numpy\nconda install -y numpy \n```\n\nThe pip version of numpy is linked against OpenBLAS, which has a `dot` that conflicts in ABI with MKL's `dot`"
   },
   {
      "x": "Feature request: named dimensions",
      "z": "See `xarray` for prior work https://github.com/pydata/xarray",
      "y": "See `xarray` for prior work https://github.com/pydata/xarray"
   },
   {
      "x": "[feature request] torch.argmax / torch.argmin",
      "z": "@thecortex note that you can currently obtain the argmax as the second return value of `torch.max` when a dimension is specified. Same for argmin",
      "y": "@thecortex note that you can currently obtain the argmax as the second return value of `torch.max` when a dimension is specified. Same for argmin"
   },
   {
      "x": "Pytorch 0.4.0 documentation typo for batchnorm's momentum parameter",
      "z": "closed via https://github.com/pytorch/pytorch/pull/5450/",
      "y": "closed via https://github.com/pytorch/pytorch/pull/5450/"
   },
   {
      "x": "Can not restart the training to obtain the same results",
      "z": "Many things can matter, e.g. random number generator, cudnn non-derministic algorithms, dataloader worker task scheduling, optimizer state, etc.",
      "y": "Many things can matter, e.g. random number generator, cudnn non-derministic algorithms, dataloader worker task scheduling, optimizer state, etc."
   },
   {
      "x": "[feature request] Add C++ test framework for pure C++ autograd+jit tests",
      "z": "The Catch2 devs recommend to just vendor Catch2 in repositories\nhttps://github.com/catchorg/Catch2/blob/master/docs/build-systems.md#cmake\n\nThe alternative, also mentioned on that page, is to use CMake's ability to git clone dependencies\u00a0using the ExternalProject machinery, but I honestly wouldn't go there.",
      "y": "The Catch2 devs recommend to just vendor Catch2 in repositories\nhttps://github.com/catchorg/Catch2/blob/master/docs/build-systems.md#cmake\n\nThe alternative, also mentioned on that page, is to use CMake's ability to git clone dependencies\u00a0using the ExternalProject machinery, but I honestly wouldn't go there."
   },
   {
      "x": "Handle None gradients in nn.utils.clip_grad_norm",
      "z": "What this says is `p.grad` is None. It's possible that `p` (whatever it is) wasn't used in the gradient computation, or there was no backwards pass applied.",
      "y": "What this says is `p.grad` is None. It's possible that `p` (whatever it is) wasn't used in the gradient computation, or there was no backwards pass applied."
   },
   {
      "x": "Variables are behaving strangely for indexing",
      "z": "One problem is the slice parsing code didn't properly handle the error return. That caused the weird error messages.\n\nThe other problem is that `Variable` doesn't implement `__index__` so you can't use Variables as slice indices.",
      "y": "One problem is the slice parsing code didn't properly handle the error return. That caused the weird error messages.\n\nThe other problem is that `Variable` doesn't implement `__index__` so you can't use Variables as slice indices."
   },
   {
      "x": "Softmax using multiple threads inhibiting parallel execution of forward passes (?)",
      "z": "Using `torch.set_num_threads(1)` solves the problem.\n\nI figure the problem was a deadlock arising as a result of creating a huge number of threads:\n\nI create a number of threads where each thread should execute a series of forward passes. However, when the thread encounters the softmax, it spawns a number of child threads. As every thread did this, a huge number of threads was being spawned in total, which caused a deadlock. \n\nIt is funny though that this occurs only on Linux and not Mac OS.",
      "y": "Using `torch.set_num_threads(1)` solves the problem.\n\nI figure the problem was a deadlock arising as a result of creating a huge number of threads:\n\nI create a number of threads where each thread should execute a series of forward passes. However, when the thread encounters the softmax, it spawns a number of child threads. As every thread did this, a huge number of threads was being spawned in total, which caused a deadlock. \n\nIt is funny though that this occurs only on Linux and not Mac OS."
   },
   {
      "x": "0.3.0 availability on conda",
      "z": "conda install pytorch=0.3.0 torchvision -c pytorch\n\nI test this command is right to install pytorch3.0.0 in MacOS!",
      "y": "conda install pytorch=0.3.0 torchvision -c pytorch\n\nI test this command is right to install pytorch3.0.0 in MacOS!"
   },
   {
      "x": "Only nn.Parameters defined directly within nn.Module are listed in module.parameters()",
      "z": "There are good reason why we don't want to traverse arbitrary Python data structures, with performance being the most important consideration. That's why we have `nn.ParameterList` and `nn.ModuleList`. `nn.Module()` can also be used as a dictionary-like object for both parameters and submodules.",
      "y": "There are good reason why we don't want to traverse arbitrary Python data structures, with performance being the most important consideration. That's why we have `nn.ParameterList` and `nn.ModuleList`. `nn.Module()` can also be used as a dictionary-like object for both parameters and submodules."
   },
   {
      "x": "Issues about symeig and svd on GPU",
      "z": "Do you mean svd? We use magma bindings to many linalg functions. Magma doesn't implement everything in GPU so it sometimes still makes some lapack calls.",
      "y": "Do you mean svd? We use magma bindings to many linalg functions. Magma doesn't implement everything in GPU so it sometimes still makes some lapack calls."
   },
   {
      "x": "ModuleNotFoundError: No module named 'torch.version'",
      "z": "It looks like you are. Change the directory and you should be good.",
      "y": "It looks like you are. Change the directory and you should be good."
   },
   {
      "x": "the derivative for 'svd' is not implemented",
      "z": "the derivative for torch.svd is implemented in the master branch. It hasn't yet been incorporated into a release yet. If you would like to use this feature immediately, you can install pytorch from source: https://github.com/pytorch/pytorch#from-source",
      "y": "the derivative for torch.svd is implemented in the master branch. It hasn't yet been incorporated into a release yet. If you would like to use this feature immediately, you can install pytorch from source: https://github.com/pytorch/pytorch#from-source"
   },
   {
      "x": "backward(create_graph=True) should raise a warning for potential memory leak",
      "z": "My 2 cents. I've done higher-order differentiation many times. I've always used `autograd.grad` instead of `.backward` or `autograd.backward` because \n1. `autograd.grad` doesn't accumulate gradient with previous values, which can lead to obscure errors.\n2. it is almost always clearer in code than accessing `.grad` of leaves.\n3. it avoids ref cycle.\n\nIn fact, most DL uses of higher-order gradients I've seen use `autograd.grad`. So could we add a warning (either in doc or in code) to `backward(create_graph)` and advocate `autograd.grad` instead?",
      "y": "My 2 cents. I've done higher-order differentiation many times. I've always used `autograd.grad` instead of `.backward` or `autograd.backward` because \n1. `autograd.grad` doesn't accumulate gradient with previous values, which can lead to obscure errors.\n2. it is almost always clearer in code than accessing `.grad` of leaves.\n3. it avoids ref cycle.\n\nIn fact, most DL uses of higher-order gradients I've seen use `autograd.grad`. So could we add a warning (either in doc or in code) to `backward(create_graph)` and advocate `autograd.grad` instead?"
   },
   {
      "x": "Bug in inplace operation after expansion",
      "z": "Yes, and that's the expected behavior. Don't do in-place on expanded tensors.",
      "y": "Yes, and that's the expected behavior. Don't do in-place on expanded tensors."
   },
   {
      "x": "[Feature request] Gradient of cholesky_inverse, cholesky_solve",
      "z": "I use the following workarounds for lack of batching in `cholesky_inverse` and lack of gradients in `cholesky_solve`. I'm guessing it should be easy to move this logic into PyTorch.\n```py\ndef cholesky_solve(b, u):\n    \"Like :func:`torch.cholesky_solve` but supports gradients.\"\n    if not b.requires_grad and not u.requires_grad:\n        return b.cholesky_solve(u)\n    x = b.triangular_solve(u, upper=False).solution\n    return x.triangular_solve(u, upper=False, transpose=True).solution\n\ndef cholesky_inverse(u):\n    \"Like :func:`torch.cholesky_inverse` but supports batching and gradients.\"\n    if u.dim() == 2 and not u.requires_grad:\n        return u.cholesky_inverse()\n    return cholesky_solve(torch.eye(u.size(-1)).expand(u.size()), u)\n```",
      "y": "I use the following workarounds for lack of batching in `cholesky_inverse` and lack of gradients in `cholesky_solve`. I'm guessing it should be easy to move this logic into PyTorch.\n```py\ndef cholesky_solve(b, u):\n    \"Like :func:`torch.cholesky_solve` but supports gradients.\"\n    if not b.requires_grad and not u.requires_grad:\n        return b.cholesky_solve(u)\n    x = b.triangular_solve(u, upper=False).solution\n    return x.triangular_solve(u, upper=False, transpose=True).solution\n\ndef cholesky_inverse(u):\n    \"Like :func:`torch.cholesky_inverse` but supports batching and gradients.\"\n    if u.dim() == 2 and not u.requires_grad:\n        return u.cholesky_inverse()\n    return cholesky_solve(torch.eye(u.size(-1)).expand(u.size()), u)\n```"
   },
   {
      "x": "No module named torchvision",
      "z": "If you followed the instructions here: http://pytorch.org/ and install with conda it will install both torch and torchvision together.",
      "y": "If you followed the instructions here: http://pytorch.org/ and install with conda it will install both torch and torchvision together."
   },
   {
      "x": "Failed to load model",
      "z": "@apaszke I solved it by using the recommended way instead, it's my fault : )\n```python\ntorch.save(the_model.state_dict(), PATH)\n\nthe_model = TheModelClass(*args, **kwargs)\nthe_model.load_state_dict(torch.load(PATH))\n```",
      "y": "@apaszke I solved it by using the recommended way instead, it's my fault : )\n```python\ntorch.save(the_model.state_dict(), PATH)\n\nthe_model = TheModelClass(*args, **kwargs)\nthe_model.load_state_dict(torch.load(PATH))\n```"
   },
   {
      "x": "Serialization of tensors with pickle.dumps seems to be inconsistent, leading to inconsistent redis cache hit/miss",
      "z": "Can you provide a minimum example that reproduces this behavior and that indicate that this is a PyTorch problem?\n\nFrom your description, it might imply that `pickle.dumps(tensor)` is not always the same. But I couldn't reproduce this locally.\n\nIt could also be that the algorithm that Redis uses to cache large objects is not perfect and is subject to missing it a few times. Hard to say without a repro.",
      "y": "Can you provide a minimum example that reproduces this behavior and that indicate that this is a PyTorch problem?\n\nFrom your description, it might imply that `pickle.dumps(tensor)` is not always the same. But I couldn't reproduce this locally.\n\nIt could also be that the algorithm that Redis uses to cache large objects is not perfect and is subject to missing it a few times. Hard to say without a repro."
   },
   {
      "x": "Grad is None after using view",
      "z": "So this isn't a bug per se, but it is definitely a source of confusion. The issue with the above code is that the gradient information is attached to the initial tensor before the `view`, but not the viewed tensor. Performing the initialization and view operation before assigning the tensor to the variable results in losing the access to the gradient information. Splitting out the view works fine. It would be useful to call this out in the docs (maybe I missed this).\n```\nX0 = torch.tensor([0.25, 0.75], requires_grad=True,)\nX_view = X0.view(2, 1, 1)\nprint(f\"X_view.shape: {X_view.shape}\")\nX_view.sum().backward()\nprint(f\"X_view.grad: {X_view.grad}\")\nprint(f\"X_view.grad is None: {X_view.grad is None}\")\nprint(f\"X0.grad: {X0.grad}\")\n```\nOutput:\n```\nX_view.shape: torch.Size([2, 1, 1])\nX_view.grad: None\nX_view.grad is None: True\nX0.grad: tensor([1., 1.])\n```\n",
      "y": "So this isn't a bug per se, but it is definitely a source of confusion. The issue with the above code is that the gradient information is attached to the initial tensor before the `view`, but not the viewed tensor. Performing the initialization and view operation before assigning the tensor to the variable results in losing the access to the gradient information. Splitting out the view works fine. It would be useful to call this out in the docs (maybe I missed this).\n```\nX0 = torch.tensor([0.25, 0.75], requires_grad=True,)\nX_view = X0.view(2, 1, 1)\nprint(f\"X_view.shape: {X_view.shape}\")\nX_view.sum().backward()\nprint(f\"X_view.grad: {X_view.grad}\")\nprint(f\"X_view.grad is None: {X_view.grad is None}\")\nprint(f\"X0.grad: {X0.grad}\")\n```\nOutput:\n```\nX_view.shape: torch.Size([2, 1, 1])\nX_view.grad: None\nX_view.grad is None: True\nX0.grad: tensor([1., 1.])\n```\n"
   },
   {
      "x": "Could Pytorch C++ API load tensor from gpu memory directly?",
      "z": "Example code that converts a cv::cuda::GpuMat of type 32F to a torch::Tensor:\n\n```C++\n    void deleter(void *arg){};\n    torch::Tensor matToTensor(const cv::cuda::GpuMat &image)\n    {\n        std::vector<int64_t> dims = {image.rows, image.cols, image.channels()};\n        long long step = image.step / sizeof(float);\n        std::vector<int64_t> strides = {step, image.channels(), 1};\n        return torch::from_blob(image.data, dims, strides, deleter, torch::kCUDA)\n    }\n```\n\nIf you want to send in bytes, you would update step size and add torch::kByte/kChar to options.\n\nOnly tricky part for your GL image may be step size otherwise the code should look almost identical.",
      "y": "Example code that converts a cv::cuda::GpuMat of type 32F to a torch::Tensor:\n\n```C++\n    void deleter(void *arg){};\n    torch::Tensor matToTensor(const cv::cuda::GpuMat &image)\n    {\n        std::vector<int64_t> dims = {image.rows, image.cols, image.channels()};\n        long long step = image.step / sizeof(float);\n        std::vector<int64_t> strides = {step, image.channels(), 1};\n        return torch::from_blob(image.data, dims, strides, deleter, torch::kCUDA)\n    }\n```\n\nIf you want to send in bytes, you would update step size and add torch::kByte/kChar to options.\n\nOnly tricky part for your GL image may be step size otherwise the code should look almost identical."
   },
   {
      "x": "[Caffe2] Compile error: onnxTensorDescriptorV1 has no member named quantizationParams",
      "z": "Should be fixed by https://github.com/pytorch/pytorch/pull/19793",
      "y": "Should be fixed by https://github.com/pytorch/pytorch/pull/19793"
   },
   {
      "x": "AverageUnpooling layer for PyTorch (Proposal)",
      "z": "Isn't `AverageUnpooling` very similar to `F.interpolate`?",
      "y": "Isn't `AverageUnpooling` very similar to `F.interpolate`?"
   },
   {
      "x": "Why nn.Sequential can't handle multiple input?",
      "z": "@soumith Hi, I changed `nn.Sequential` to this\n```python\nclass mySequential(nn.Sequential):\n    def forward(self, *input):\n        for module in self._modules.values():\n            input = module(*input)\n        return input\n```\nAnd it could handle multiple inputs/outputs only need the number of outputs from the previous layer equals the number of inputs from the next layer.\n```python\nclass n_to_n(nn.Module):\n    def __init__(self):\n        super().__init__()\n        self.conv1 = nn.Conv2d(3, 3, 1, 1, bias=False)\n        self.conv2 = nn.Conv2d(3, 3, 1, 1, bias=False)\n\n    def forward(self, x1, x2):\n        y1 = self.conv1(x1)\n        y2 = self.conv2(x2)\n        return y1, y2\n\n\nclass n_to_one(nn.Module):\n    def __init__(self):\n        super().__init__()\n        self.conv1 = nn.Conv2d(3, 3, 1, 1, bias=False)\n        self.conv2 = nn.Conv2d(3, 3, 1, 1, bias=False)\n\n    def forward(self, x1, x2):\n        y1 = self.conv1(x1)\n        y2 = self.conv2(x2)\n        return y1 + y2\n\n\nclass one_to_n(nn.Module):\n    def __init__(self):\n        super().__init__()\n        self.conv1 = nn.Conv2d(3, 3, 1, 1, bias=False)\n        self.conv2 = nn.Conv2d(3, 3, 1, 1, bias=False)\n\n    def forward(self, x):\n        y1 = self.conv1(x)\n        y2 = self.conv2(x)\n        return y1, y2\n\nseq = mySequential(one_to_n(), n_to_n(), n_to_one()).cuda()\ntd = torch.rand(1, 3, 32, 32).cuda()\n\nout = seq(td)\nprint(out.size())\n```\n```shell\ntorch.Size([1, 3, 32, 32])\n```\nWhat do you think?",
      "y": "@soumith Hi, I changed `nn.Sequential` to this\n```python\nclass mySequential(nn.Sequential):\n    def forward(self, *input):\n        for module in self._modules.values():\n            input = module(*input)\n        return input\n```\nAnd it could handle multiple inputs/outputs only need the number of outputs from the previous layer equals the number of inputs from the next layer.\n```python\nclass n_to_n(nn.Module):\n    def __init__(self):\n        super().__init__()\n        self.conv1 = nn.Conv2d(3, 3, 1, 1, bias=False)\n        self.conv2 = nn.Conv2d(3, 3, 1, 1, bias=False)\n\n    def forward(self, x1, x2):\n        y1 = self.conv1(x1)\n        y2 = self.conv2(x2)\n        return y1, y2\n\n\nclass n_to_one(nn.Module):\n    def __init__(self):\n        super().__init__()\n        self.conv1 = nn.Conv2d(3, 3, 1, 1, bias=False)\n        self.conv2 = nn.Conv2d(3, 3, 1, 1, bias=False)\n\n    def forward(self, x1, x2):\n        y1 = self.conv1(x1)\n        y2 = self.conv2(x2)\n        return y1 + y2\n\n\nclass one_to_n(nn.Module):\n    def __init__(self):\n        super().__init__()\n        self.conv1 = nn.Conv2d(3, 3, 1, 1, bias=False)\n        self.conv2 = nn.Conv2d(3, 3, 1, 1, bias=False)\n\n    def forward(self, x):\n        y1 = self.conv1(x)\n        y2 = self.conv2(x)\n        return y1, y2\n\nseq = mySequential(one_to_n(), n_to_n(), n_to_one()).cuda()\ntd = torch.rand(1, 3, 32, 32).cuda()\n\nout = seq(td)\nprint(out.size())\n```\n```shell\ntorch.Size([1, 3, 32, 32])\n```\nWhat do you think?"
   },
   {
      "x": "CTCLoss reduction=\u2019none\u2018 and calculate average in hand is not equal to 'mean'",
      "z": "@carry-xz this is expected, see the implementation of `CTCLoss` in https://github.com/pytorch/pytorch/blob/39b885cbbfc8c115069d49f5a6d27ea622bd05dc/aten/src/ATen/native/LossCTC.cpp#L366-L371\n\nFrom [the documentation](https://pytorch.org/docs/stable/nn.html#torch.nn.CTCLoss)\n> \u2018mean\u2019: the output losses will be divided by the target lengths and then the mean over the batch is taken",
      "y": "@carry-xz this is expected, see the implementation of `CTCLoss` in https://github.com/pytorch/pytorch/blob/39b885cbbfc8c115069d49f5a6d27ea622bd05dc/aten/src/ATen/native/LossCTC.cpp#L366-L371\n\nFrom [the documentation](https://pytorch.org/docs/stable/nn.html#torch.nn.CTCLoss)\n> \u2018mean\u2019: the output losses will be divided by the target lengths and then the mean over the batch is taken"
   },
   {
      "x": "Using DistributedDataParallel through NCCL throws RuntimeError",
      "z": "@Ze-Yang Pytorch DDP has a serious flaw which is not documented: the computational graphs for all the nodes must be the same or you will get this error or training sometimes just hangs at 100% CPU (even when training on GPU) on backward() call. You can debug your problem by visualising the computational graphs on each node and see if they are the same. If not, make them (or fix the pytorch source).\n\nAn example: if your loss function contains an IF condition with additional computation, and one node goes into IF and other does not, this will be enough to crash DDP training. Instead of IF try to solve the same thing some other way and/or trick pytorch to make the graph look the same on each node. Of course, there might be other causes for different autograd graph too.\n\nMy case was this. Buggy code (crashed):\n```\nif positive_class.shape[0] > 0:\n   loss += torch.somefunction(positive_class)\n```\nFix:\n```\n# concat with tensor that has no effect on loss calculation, but positive_class always has content\npositive_class = torch.cat(positive_class, dummy)\nloss += torch.somefunction(positive_class)\n```\n\nI don't know if your problem is similar, I do not see any obvious divergence of graphs here, but it's worth to look at graphs if you are stuck :)",
      "y": "@Ze-Yang Pytorch DDP has a serious flaw which is not documented: the computational graphs for all the nodes must be the same or you will get this error or training sometimes just hangs at 100% CPU (even when training on GPU) on backward() call. You can debug your problem by visualising the computational graphs on each node and see if they are the same. If not, make them (or fix the pytorch source).\n\nAn example: if your loss function contains an IF condition with additional computation, and one node goes into IF and other does not, this will be enough to crash DDP training. Instead of IF try to solve the same thing some other way and/or trick pytorch to make the graph look the same on each node. Of course, there might be other causes for different autograd graph too.\n\nMy case was this. Buggy code (crashed):\n```\nif positive_class.shape[0] > 0:\n   loss += torch.somefunction(positive_class)\n```\nFix:\n```\n# concat with tensor that has no effect on loss calculation, but positive_class always has content\npositive_class = torch.cat(positive_class, dummy)\nloss += torch.somefunction(positive_class)\n```\n\nI don't know if your problem is similar, I do not see any obvious divergence of graphs here, but it's worth to look at graphs if you are stuck :)"
   },
   {
      "x": "At training time, pytorch batchnorm use biased batch var to normalize input, but running var is updated by unbiased batch var",
      "z": "Finally,I find the problem.\nAt training time, pytorch batchnorm use biased batch var to normalize input, but running var is updated by unbiased batch var.So after model convergence and switch to eval model,the running var gives unbiased prediction, but this is inconsistency with train mode with biased prediction.\nChange the torch.var unbiased option to True or False for seeing the result.\n```\nimport torch\nimport torch.nn as nn\nfrom torch.nn import Parameter, init\n\nclass MyBatchNorm(nn.Module):\n    _version = 2\n    __constants__ = ['track_running_stats', 'momentum', 'eps', 'weight', 'bias',\n                     'running_mean', 'running_var', 'num_batches_tracked']\n\n    def __init__(self, num_features, eps=1e-5, momentum=0.1, affine=True,\n                 track_running_stats=True):\n        super(MyBatchNorm, self).__init__()\n        self.num_features = num_features\n        self.eps = eps\n        self.momentum = momentum\n        self.affine = affine\n        self.track_running_stats = track_running_stats\n        if self.affine:\n            self.weight = Parameter(torch.Tensor(num_features))\n            self.bias = Parameter(torch.Tensor(num_features))\n        else:\n            self.register_parameter('weight', None)\n            self.register_parameter('bias', None)\n        if self.track_running_stats:\n            self.register_buffer('running_mean', torch.zeros(num_features))\n            self.register_buffer('running_var', torch.ones(num_features))\n            self.register_buffer('num_batches_tracked', torch.tensor(0, dtype=torch.long))\n        else:\n            self.register_parameter('running_mean', None)\n            self.register_parameter('running_var', None)\n            self.register_parameter('num_batches_tracked', None)\n        self.reset_parameters()\n\n    def reset_running_stats(self):\n        if self.track_running_stats:\n            self.running_mean.zero_()\n            self.running_var.fill_(1)\n            self.num_batches_tracked.zero_()\n\n    def reset_parameters(self):\n        self.reset_running_stats()\n        if self.affine:\n            init.uniform_(self.weight)\n            init.zeros_(self.bias)\n        \n    def forward(self, input):        \n        input_size = input.size()\n        input = input.transpose(1,0)\n        input = input.view(input.size(0), -1)\n\n        if self.training:\n            mean = input.mean(dim=1)\n            var = torch.var(input,dim=1, unbiased=True)\n            self.running_mean[:] = (1. - self.momentum) * self.running_mean + self.momentum * mean\n            self.running_var[:] = (1. - self.momentum) * self.running_var + self.momentum * var\n        else:\n            mean = self.running_mean\n            var = self.running_var\n\n        input = input - mean.view(-1,1)\n        input = input / (torch.sqrt(var+self.eps).view(-1,1))\n       \n        input = self.weight.view(-1, 1) * input + self.bias.view(-1, 1)\n        input = input.transpose(1,0)\n        input = input.view(*input_size)\n        return input\n\n    def extra_repr(self):\n        return '{num_features}, eps={eps}, momentum={momentum}, affine={affine}, ' \\\n               'track_running_stats={track_running_stats}'.format(**self.__dict__)\n\n    def _load_from_state_dict(self, state_dict, prefix, local_metadata, strict,\n                              missing_keys, unexpected_keys, error_msgs):\n        version = local_metadata.get('version', None)\n\n        if (version is None or version < 2) and self.track_running_stats:\n            # at version 2: added num_batches_tracked buffer\n            #               this should have a default value of 0\n            num_batches_tracked_key = prefix + 'num_batches_tracked'\n            if num_batches_tracked_key not in state_dict:\n                state_dict[num_batches_tracked_key] = torch.tensor(0, dtype=torch.long)\n\n        super(MyBatchNorm, self)._load_from_state_dict(\n            state_dict, prefix, local_metadata, strict,\n            missing_keys, unexpected_keys, error_msgs)\n\ndef test_batch_norm():\n    momentum = 1.0\n    torch.manual_seed(1234)\n\n    batch_norm = MyBatchNorm(3,eps=1e-5,momentum=momentum,affine=True,track_running_stats=True)\n    n1 = batch_norm\n    torch.save(n1.state_dict(),'n1.pth')\n    \n    torch_batch_norm = nn.BatchNorm1d(3,eps=1e-5,momentum=momentum,affine=True,track_running_stats=True)\n    n2 = torch_batch_norm\n    n2.load_state_dict(torch.load('n1.pth'))\n    \n    x = torch.FloatTensor([[1,2,3], [3,4,0], [3,3,1]])\n    y = torch.FloatTensor([[2], [3], [1]])\n    criterion = nn.MSELoss()\n\n    x = x.cuda()\n    y = y.cuda()\n    batch_norm.cuda()\n    torch_batch_norm.cuda()\n\n    print('Switch to eval mode.')\n    batch_norm.eval()\n    torch_batch_norm.eval()\n    out1 = n1(x)\n    out2 = n2(x)\n    eval1 = (torch.abs(out2-out1).sum().item() < 1e-4)\n\n    print('Swtich to train mode.')\n    batch_norm.train()\n    torch_batch_norm.train()\n\n    out1 = n1(x)\n    out2 = n2(x)\n    train2 = (torch.abs(out2-out1).sum().item() < 1e-4)\n\n    print('Switch to eval mode.')\n    n1.eval()\n    n2.eval()\n    print('MyBatchNorm:')\n    print('running_mean:',batch_norm.running_mean.cpu().numpy())\n    print('running_var:',batch_norm.running_var.cpu().numpy())\n    print('weight:',batch_norm.weight.data.cpu().numpy())\n    print('bias:',batch_norm.bias.data.cpu().numpy())\n    print()\n    \n    print('TorchBatchNorm:')\n    print('running_mean:',torch_batch_norm.running_mean.cpu().numpy())\n    print('running_var:',torch_batch_norm.running_var.cpu().numpy())\n    print('weight:',torch_batch_norm.weight.data.cpu().numpy())\n    print('bias:',torch_batch_norm.bias.data.cpu().numpy())\n    out1 = n1(x)\n    out2 = n2(x)\n    eval3 = (torch.abs(out2-out1).sum().item() < 1e-4)\n    print('eval1,train2,eval3:',eval1,train2,eval3)\n    assert eval1 and train2 and eval3\n\nif __name__ == '__main__':\n    test_batch_norm()\n```",
      "y": "Finally,I find the problem.\nAt training time, pytorch batchnorm use biased batch var to normalize input, but running var is updated by unbiased batch var.So after model convergence and switch to eval model,the running var gives unbiased prediction, but this is inconsistency with train mode with biased prediction.\nChange the torch.var unbiased option to True or False for seeing the result.\n```\nimport torch\nimport torch.nn as nn\nfrom torch.nn import Parameter, init\n\nclass MyBatchNorm(nn.Module):\n    _version = 2\n    __constants__ = ['track_running_stats', 'momentum', 'eps', 'weight', 'bias',\n                     'running_mean', 'running_var', 'num_batches_tracked']\n\n    def __init__(self, num_features, eps=1e-5, momentum=0.1, affine=True,\n                 track_running_stats=True):\n        super(MyBatchNorm, self).__init__()\n        self.num_features = num_features\n        self.eps = eps\n        self.momentum = momentum\n        self.affine = affine\n        self.track_running_stats = track_running_stats\n        if self.affine:\n            self.weight = Parameter(torch.Tensor(num_features))\n            self.bias = Parameter(torch.Tensor(num_features))\n        else:\n            self.register_parameter('weight', None)\n            self.register_parameter('bias', None)\n        if self.track_running_stats:\n            self.register_buffer('running_mean', torch.zeros(num_features))\n            self.register_buffer('running_var', torch.ones(num_features))\n            self.register_buffer('num_batches_tracked', torch.tensor(0, dtype=torch.long))\n        else:\n            self.register_parameter('running_mean', None)\n            self.register_parameter('running_var', None)\n            self.register_parameter('num_batches_tracked', None)\n        self.reset_parameters()\n\n    def reset_running_stats(self):\n        if self.track_running_stats:\n            self.running_mean.zero_()\n            self.running_var.fill_(1)\n            self.num_batches_tracked.zero_()\n\n    def reset_parameters(self):\n        self.reset_running_stats()\n        if self.affine:\n            init.uniform_(self.weight)\n            init.zeros_(self.bias)\n        \n    def forward(self, input):        \n        input_size = input.size()\n        input = input.transpose(1,0)\n        input = input.view(input.size(0), -1)\n\n        if self.training:\n            mean = input.mean(dim=1)\n            var = torch.var(input,dim=1, unbiased=True)\n            self.running_mean[:] = (1. - self.momentum) * self.running_mean + self.momentum * mean\n            self.running_var[:] = (1. - self.momentum) * self.running_var + self.momentum * var\n        else:\n            mean = self.running_mean\n            var = self.running_var\n\n        input = input - mean.view(-1,1)\n        input = input / (torch.sqrt(var+self.eps).view(-1,1))\n       \n        input = self.weight.view(-1, 1) * input + self.bias.view(-1, 1)\n        input = input.transpose(1,0)\n        input = input.view(*input_size)\n        return input\n\n    def extra_repr(self):\n        return '{num_features}, eps={eps}, momentum={momentum}, affine={affine}, ' \\\n               'track_running_stats={track_running_stats}'.format(**self.__dict__)\n\n    def _load_from_state_dict(self, state_dict, prefix, local_metadata, strict,\n                              missing_keys, unexpected_keys, error_msgs):\n        version = local_metadata.get('version', None)\n\n        if (version is None or version < 2) and self.track_running_stats:\n            # at version 2: added num_batches_tracked buffer\n            #               this should have a default value of 0\n            num_batches_tracked_key = prefix + 'num_batches_tracked'\n            if num_batches_tracked_key not in state_dict:\n                state_dict[num_batches_tracked_key] = torch.tensor(0, dtype=torch.long)\n\n        super(MyBatchNorm, self)._load_from_state_dict(\n            state_dict, prefix, local_metadata, strict,\n            missing_keys, unexpected_keys, error_msgs)\n\ndef test_batch_norm():\n    momentum = 1.0\n    torch.manual_seed(1234)\n\n    batch_norm = MyBatchNorm(3,eps=1e-5,momentum=momentum,affine=True,track_running_stats=True)\n    n1 = batch_norm\n    torch.save(n1.state_dict(),'n1.pth')\n    \n    torch_batch_norm = nn.BatchNorm1d(3,eps=1e-5,momentum=momentum,affine=True,track_running_stats=True)\n    n2 = torch_batch_norm\n    n2.load_state_dict(torch.load('n1.pth'))\n    \n    x = torch.FloatTensor([[1,2,3], [3,4,0], [3,3,1]])\n    y = torch.FloatTensor([[2], [3], [1]])\n    criterion = nn.MSELoss()\n\n    x = x.cuda()\n    y = y.cuda()\n    batch_norm.cuda()\n    torch_batch_norm.cuda()\n\n    print('Switch to eval mode.')\n    batch_norm.eval()\n    torch_batch_norm.eval()\n    out1 = n1(x)\n    out2 = n2(x)\n    eval1 = (torch.abs(out2-out1).sum().item() < 1e-4)\n\n    print('Swtich to train mode.')\n    batch_norm.train()\n    torch_batch_norm.train()\n\n    out1 = n1(x)\n    out2 = n2(x)\n    train2 = (torch.abs(out2-out1).sum().item() < 1e-4)\n\n    print('Switch to eval mode.')\n    n1.eval()\n    n2.eval()\n    print('MyBatchNorm:')\n    print('running_mean:',batch_norm.running_mean.cpu().numpy())\n    print('running_var:',batch_norm.running_var.cpu().numpy())\n    print('weight:',batch_norm.weight.data.cpu().numpy())\n    print('bias:',batch_norm.bias.data.cpu().numpy())\n    print()\n    \n    print('TorchBatchNorm:')\n    print('running_mean:',torch_batch_norm.running_mean.cpu().numpy())\n    print('running_var:',torch_batch_norm.running_var.cpu().numpy())\n    print('weight:',torch_batch_norm.weight.data.cpu().numpy())\n    print('bias:',torch_batch_norm.bias.data.cpu().numpy())\n    out1 = n1(x)\n    out2 = n2(x)\n    eval3 = (torch.abs(out2-out1).sum().item() < 1e-4)\n    print('eval1,train2,eval3:',eval1,train2,eval3)\n    assert eval1 and train2 and eval3\n\nif __name__ == '__main__':\n    test_batch_norm()\n```"
   },
   {
      "x": "Is CudnnRNN thread-safe?",
      "z": "can you reproduce this on 1.1?  A number of thread-safe fixes have gone in since 0.4.1.  I'm going to close for now, please reopen if you can reproduce this on 1.1.",
      "y": "can you reproduce this on 1.1?  A number of thread-safe fixes have gone in since 0.4.1.  I'm going to close for now, please reopen if you can reproduce this on 1.1."
   },
   {
      "x": "ProcessGroupNCCL.cpp:260, unhandled cuda error, when using 2 nodes with 4 GPUs each",
      "z": "@SerailHydra Hi, I finally solved it with setting NCCL_SOCKET_IFNAME=ib0 and NCCL_IB_DISABLE=1, where ib0 is my ip interface.\n\nHope it will help!",
      "y": "@SerailHydra Hi, I finally solved it with setting NCCL_SOCKET_IFNAME=ib0 and NCCL_IB_DISABLE=1, where ib0 is my ip interface.\n\nHope it will help!"
   },
   {
      "x": "DataLoader for Large Corpus File",
      "z": "This is definitely a valid feature request. In fact, I implemented something called `IterableDataset` that will be used as an iterable (e.g., generator, data stream) in PyTorch. It is currently being reviewed at #19228 .",
      "y": "This is definitely a valid feature request. In fact, I implemented something called `IterableDataset` that will be used as an iterable (e.g., generator, data stream) in PyTorch. It is currently being reviewed at #19228 ."
   },
   {
      "x": "[jit] Slice assignment is completely elided in Onnx graph",
      "z": "This is an expected behavior, and appropriate warning have sent out. So close the issue.",
      "y": "This is an expected behavior, and appropriate warning have sent out. So close the issue."
   },
   {
      "x": "torch.nn.LogSoftmax.__repr__() does not include dim argument",
      "z": "@SdgJlbl  reminder that you can override `extra_repr` instead of  `__repr__`.",
      "y": "@SdgJlbl  reminder that you can override `extra_repr` instead of  `__repr__`."
   },
   {
      "x": "cuda-runtime error(4) on PyTorch 0.4.1",
      "z": "> Is there some solution can solve it without upgrade?\n\nI was able to solve this problem on 0.4.1 by reinstalling NVIDIA drivers. ",
      "y": "> Is there some solution can solve it without upgrade?\n\nI was able to solve this problem on 0.4.1 by reinstalling NVIDIA drivers. "
   },
   {
      "x": "[ONNX] The shape of PReLU weight is wrong",
      "z": "> @ezyang Thanks! Could you please give directions on how to fix it? The weight [here](https://github.com/pytorch/pytorch/blob/master/torch/onnx/symbolic_opset9.py#L396) has the type `prim::Param`. I haven't found a way to modify it.\n\nYou can try inserting ```onnx::unsqueeze``` for the weight\n```\nweight = g.op(\"Unsqueeze\", axes_i=[1, 2])\nreturn g.op(\"PRelu\", self, weight)\n```\nIf the weight is ```prim::Param``` or Constant, this unsqueeze will be optimized away if ```do_constant_folding``` is turned on while exporting. \n\nEdit: you'll also need to construct the ```axes_i``` parameter for ```Unsqueeze``` based on actual rank of ```self``` tensor, like here\nhttps://github.com/pytorch/pytorch/blob/0f58d20fe43e89138ffcbbf64fb48569539f2e4e/torch/onnx/symbolic_opset9.py#L316",
      "y": "> @ezyang Thanks! Could you please give directions on how to fix it? The weight [here](https://github.com/pytorch/pytorch/blob/master/torch/onnx/symbolic_opset9.py#L396) has the type `prim::Param`. I haven't found a way to modify it.\n\nYou can try inserting ```onnx::unsqueeze``` for the weight\n```\nweight = g.op(\"Unsqueeze\", axes_i=[1, 2])\nreturn g.op(\"PRelu\", self, weight)\n```\nIf the weight is ```prim::Param``` or Constant, this unsqueeze will be optimized away if ```do_constant_folding``` is turned on while exporting. \n\nEdit: you'll also need to construct the ```axes_i``` parameter for ```Unsqueeze``` based on actual rank of ```self``` tensor, like here\nhttps://github.com/pytorch/pytorch/blob/0f58d20fe43e89138ffcbbf64fb48569539f2e4e/torch/onnx/symbolic_opset9.py#L316"
   },
   {
      "x": "Make DDP failure recoverable",
      "z": "## Trying Solution 2\n\n#21534 seems addressed the problem but in quite a dirty way. A better solution might need to satisfy the following requirements:\n\n1. As mentioned by @pietern, the hook deletion function should be implemented in `torch/csrc/autograd/function.h`, as it owns the data. \n2. We should not slow down existing use cases of `add_post_hook` and `post_hooks()`.\n\nI initially thought about using an `OrderedDict` to store named hooks, as what we did for params, buffers, and children in `nn/Module.h`, but that would violate the second requirement.\n\n~Another possibility is that, instead of using the default deleter, we create a special deleter for the hook unique ptr in DDP, e.g., `ReducerHookDeleter`, that wraps the default deleter.  The `add_post_hook` and `post_hooks()` APIs would stay the same, then we add one `delete_post_hook<DeleterType>()` API to `torch/csrc/autograd/function.h`, which loops over all post hooks, and only delete the ones with matching deleter type, i.e., `ReducerHookDeleter`. This would be slow, but is OK, as we only need this on failures, where timeout delay will dominate. Any comments?~ Should be able to directly check hook pointer type.",
      "y": "## Trying Solution 2\n\n#21534 seems addressed the problem but in quite a dirty way. A better solution might need to satisfy the following requirements:\n\n1. As mentioned by @pietern, the hook deletion function should be implemented in `torch/csrc/autograd/function.h`, as it owns the data. \n2. We should not slow down existing use cases of `add_post_hook` and `post_hooks()`.\n\nI initially thought about using an `OrderedDict` to store named hooks, as what we did for params, buffers, and children in `nn/Module.h`, but that would violate the second requirement.\n\n~Another possibility is that, instead of using the default deleter, we create a special deleter for the hook unique ptr in DDP, e.g., `ReducerHookDeleter`, that wraps the default deleter.  The `add_post_hook` and `post_hooks()` APIs would stay the same, then we add one `delete_post_hook<DeleterType>()` API to `torch/csrc/autograd/function.h`, which loops over all post hooks, and only delete the ones with matching deleter type, i.e., `ReducerHookDeleter`. This would be slow, but is OK, as we only need this on failures, where timeout delay will dominate. Any comments?~ Should be able to directly check hook pointer type."
   },
   {
      "x": "pytorch 1.1.0 fails to load on windows (python3.6, 3.7)",
      "z": "I extracted the build scripts from your log, that is, the code below.\n```cmd\nset PYTHON=Python36\nset ARCH=-x64 \nset PYTORCH=1.1.0-cp36-cp36m\n\ngit clone -q --depth=25 --branch=travis-pytorch-avy https://github.com/jvesely/PsyNeuLink.git C:\\projects\\psyneulink-wuxsn\ncd C:\\projects\\psyneulink-wuxsn\ngit checkout -qf ae5a4dcbe1f72bf83c199ce8e95b21828b90219d\n\nchoco upgrade graphviz.portable -y\npip --version\npip install --user -U pip\npip --version\npip install --user -U certifi \"numpy<1.16\"\npip install --user git+https://github.com/benureau/leabra.git@master\nif not \"%PYTORCH%\" == \"\" pip install --user http://download.pytorch.org/whl/cpu/torch-%PYTORCH%-win_amd64.whl\nif \"%PYTORCH%\" == \"\" (findstr /V torch < dev_requirements.txt > tmp_req && move /Y tmp_req dev_requirements.txt)\npip install --user -e .[dev]\npytest --junit-xml=tests_out.xml -n auto --strict-markers tests/ %EXTRA_ARGS%\ncurl -X POST -F \"file=@tests_out.xml\" https://ci.appveyor.com/api/testresults/junit/%APPVEYOR_JOB_ID%\n```\nAnd then I tried to reproduce it locally, so I started to contrust the env according to your commands and did the smoke testing after that. And I found that it is throwing out an error.\n```cmd\nPython 3.6.2 (v3.6.2:5fd33b5, Jul  8 2017, 04:57:36) [MSC v.1900 64 bit (AMD64)] on win32\nType \"help\", \"copyright\", \"credits\" or \"license\" for more information.\n>>> import torch\nTraceback (most recent call last):\n  File \"<stdin>\", line 1, in <module>\n  File \"C:\\Python36\\lib\\site-packages\\torch\\__init__.py\", line 79, in <module>\n    from torch._C import *\nModuleNotFoundError: No module named 'numpy.core._multiarray_umath'\n>>> quit()\n```\nAfter that, I tried to upgrade numpy from 3.15 to 3.16, and it worked. I also tried to downgrade torch to 1.0.1 and it also worked. So you could actually remove the version constraint on numpy and try again.",
      "y": "I extracted the build scripts from your log, that is, the code below.\n```cmd\nset PYTHON=Python36\nset ARCH=-x64 \nset PYTORCH=1.1.0-cp36-cp36m\n\ngit clone -q --depth=25 --branch=travis-pytorch-avy https://github.com/jvesely/PsyNeuLink.git C:\\projects\\psyneulink-wuxsn\ncd C:\\projects\\psyneulink-wuxsn\ngit checkout -qf ae5a4dcbe1f72bf83c199ce8e95b21828b90219d\n\nchoco upgrade graphviz.portable -y\npip --version\npip install --user -U pip\npip --version\npip install --user -U certifi \"numpy<1.16\"\npip install --user git+https://github.com/benureau/leabra.git@master\nif not \"%PYTORCH%\" == \"\" pip install --user http://download.pytorch.org/whl/cpu/torch-%PYTORCH%-win_amd64.whl\nif \"%PYTORCH%\" == \"\" (findstr /V torch < dev_requirements.txt > tmp_req && move /Y tmp_req dev_requirements.txt)\npip install --user -e .[dev]\npytest --junit-xml=tests_out.xml -n auto --strict-markers tests/ %EXTRA_ARGS%\ncurl -X POST -F \"file=@tests_out.xml\" https://ci.appveyor.com/api/testresults/junit/%APPVEYOR_JOB_ID%\n```\nAnd then I tried to reproduce it locally, so I started to contrust the env according to your commands and did the smoke testing after that. And I found that it is throwing out an error.\n```cmd\nPython 3.6.2 (v3.6.2:5fd33b5, Jul  8 2017, 04:57:36) [MSC v.1900 64 bit (AMD64)] on win32\nType \"help\", \"copyright\", \"credits\" or \"license\" for more information.\n>>> import torch\nTraceback (most recent call last):\n  File \"<stdin>\", line 1, in <module>\n  File \"C:\\Python36\\lib\\site-packages\\torch\\__init__.py\", line 79, in <module>\n    from torch._C import *\nModuleNotFoundError: No module named 'numpy.core._multiarray_umath'\n>>> quit()\n```\nAfter that, I tried to upgrade numpy from 3.15 to 3.16, and it worked. I also tried to downgrade torch to 1.0.1 and it also worked. So you could actually remove the version constraint on numpy and try again."
   },
   {
      "x": "Why module->eval() doesn't work in C++",
      "z": "nvm, got it to work in annotation method as well.. Just had to add `model.eval();` after loading the model in C++. ",
      "y": "nvm, got it to work in annotation method as well.. Just had to add `model.eval();` after loading the model in C++. "
   },
   {
      "x": "Support sublist arguments for torch.einsum",
      "z": "Similar to numpy functionality `torch.einsum(([[0,1],[0,2],[0,3],[0,4]],[1,2,3,4]), comp_list)` can be implemented to transform first parameter to string and then call already implemented `torch.einsum('..', comp_list)`\n\nOne way to do this:\n1. implement additional `einsum` in  `ATen/native/Linear.cpp`\n```\nTensor einsum(Tensor lhs_t, Tensor rhs_t, TensorList tensors) {\n      // convert lhs_t and rhs_t to eqn\n     std::string eqn =  ... ;\n     return at::einsum(eqn, tensors);\n}\n``` \n2. add definition to `native_funcions.yaml`\n```\n- func: einsum.Tensor(Tensor lhs_t, Tensor rhs_t, Tensor[] tensors) -> Tensor\n  use_c10_dispatcher: unboxed_only\n```\n3. `torch/functional.py`  dispatch call in `def einsum(equation, *operands):` by checking if the first parameter is a string \n\nAnother:\n \n1. just transform the first argument to string directly in `torch/functional.py`, in that case, there won't be any cpp implementation for that. \n\nI have the following questions :\n1. Which approach suits best to PyTorch?\n2. Using only-python support we can treat python `ellipsis` , but I don't see how ellipsis can be supported in cpp implementation of subscripts.\n3. If it should be a cpp version, which types for subscripts arrays are better to use?  ",
      "y": "Similar to numpy functionality `torch.einsum(([[0,1],[0,2],[0,3],[0,4]],[1,2,3,4]), comp_list)` can be implemented to transform first parameter to string and then call already implemented `torch.einsum('..', comp_list)`\n\nOne way to do this:\n1. implement additional `einsum` in  `ATen/native/Linear.cpp`\n```\nTensor einsum(Tensor lhs_t, Tensor rhs_t, TensorList tensors) {\n      // convert lhs_t and rhs_t to eqn\n     std::string eqn =  ... ;\n     return at::einsum(eqn, tensors);\n}\n``` \n2. add definition to `native_funcions.yaml`\n```\n- func: einsum.Tensor(Tensor lhs_t, Tensor rhs_t, Tensor[] tensors) -> Tensor\n  use_c10_dispatcher: unboxed_only\n```\n3. `torch/functional.py`  dispatch call in `def einsum(equation, *operands):` by checking if the first parameter is a string \n\nAnother:\n \n1. just transform the first argument to string directly in `torch/functional.py`, in that case, there won't be any cpp implementation for that. \n\nI have the following questions :\n1. Which approach suits best to PyTorch?\n2. Using only-python support we can treat python `ellipsis` , but I don't see how ellipsis can be supported in cpp implementation of subscripts.\n3. If it should be a cpp version, which types for subscripts arrays are better to use?  "
   },
   {
      "x": "NCCL process groups don't support `.group_ranks()`",
      "z": "In that case you should be able to use `torch.distributed.get_world_size(group=pg)` directly.\n\nThe `pg` is still to be considered an abstract object. In reality it's an instance of the C++ `c10d::ProcessGroup` class, and you can also call `pg.size` to get the same information. This is not considered public API so please prefer going through the `torch.distributed` function instead.",
      "y": "In that case you should be able to use `torch.distributed.get_world_size(group=pg)` directly.\n\nThe `pg` is still to be considered an abstract object. In reality it's an instance of the C++ `c10d::ProcessGroup` class, and you can also call `pg.size` to get the same information. This is not considered public API so please prefer going through the `torch.distributed` function instead."
   },
   {
      "x": "JIT RuntimeError: isTensor() ASSERT FAILED",
      "z": "@wanchaol thanks for providing a solution, but I don't feel quite convinced about it. I just see one thing here: from the maintenance side of the library if we want to provide jit ready functions to be traced, does it mean that we have to wrap every single function to a ScriptModule, cast all to tensors and so ? Also from user side, in case we don't provide such wrapping could be really a pain to the user and convert the whole thing to the worst user friendly lib ever.\n\nA \"quick\" fix we figured out would be converting the non-tensors parameters from all signatures to tensor, meaning that this will break all our current api and its backward compatibility. We have also been tracking this issue: https://github.com/pytorch/pytorch/issues/20939 Do you think that this will make things easier from jit perspective side ?",
      "y": "@wanchaol thanks for providing a solution, but I don't feel quite convinced about it. I just see one thing here: from the maintenance side of the library if we want to provide jit ready functions to be traced, does it mean that we have to wrap every single function to a ScriptModule, cast all to tensors and so ? Also from user side, in case we don't provide such wrapping could be really a pain to the user and convert the whole thing to the worst user friendly lib ever.\n\nA \"quick\" fix we figured out would be converting the non-tensors parameters from all signatures to tensor, meaning that this will break all our current api and its backward compatibility. We have also been tracking this issue: https://github.com/pytorch/pytorch/issues/20939 Do you think that this will make things easier from jit perspective side ?"
   },
   {
      "x": "MKLDNN convolution leaks memory",
      "z": "Thanks for reporting this issue. We've been able to identify the issue and come up with a workaround as below. A formal fix will be submitted soon. \n\nA simple workaround in mkldnn src/common/stream.cpp.\nhttps://github.com/intel/mkl-dnn/blob/rls-v0.18/src/common/stream.cpp\n\n```\ndiff --git a/src/common/stream.cpp b/src/common/stream.cpp\nindex 054fbb9..be11cf0 100644\n--- a/src/common/stream.cpp\n+++ b/src/common/stream.cpp\n@@ -46,7 +46,9 @@ status_t stream_t::submit(const nstl::vector<primitive_t *> &prims,\n\n     const size_t start = stream_.size();\n     stream_.insert(stream_.end(), prims.begin(), prims.end());\n-    return submit_impl(start, stream_.size(), error_prim);\n+    auto res = submit_impl(start, stream_.size(), error_prim);\n+    stream_.clear();\n+    return res;\n }\n\n bool stream_t::closed() const { return true; }\n```\n\n",
      "y": "Thanks for reporting this issue. We've been able to identify the issue and come up with a workaround as below. A formal fix will be submitted soon. \n\nA simple workaround in mkldnn src/common/stream.cpp.\nhttps://github.com/intel/mkl-dnn/blob/rls-v0.18/src/common/stream.cpp\n\n```\ndiff --git a/src/common/stream.cpp b/src/common/stream.cpp\nindex 054fbb9..be11cf0 100644\n--- a/src/common/stream.cpp\n+++ b/src/common/stream.cpp\n@@ -46,7 +46,9 @@ status_t stream_t::submit(const nstl::vector<primitive_t *> &prims,\n\n     const size_t start = stream_.size();\n     stream_.insert(stream_.end(), prims.begin(), prims.end());\n-    return submit_impl(start, stream_.size(), error_prim);\n+    auto res = submit_impl(start, stream_.size(), error_prim);\n+    stream_.clear();\n+    return res;\n }\n\n bool stream_t::closed() const { return true; }\n```\n\n"
   },
   {
      "x": "How to disable MKL-DNN 64-bit compilation?",
      "z": "It may be caused by the use of `sudo` here, because some configuration of sudo would drop or only keep some current environment variables. Normally you shouldn't run the build command with `sudo`; but if you have to do so, you can try `sudo -E` to force `sudo` to preserve environment variables.",
      "y": "It may be caused by the use of `sudo` here, because some configuration of sudo would drop or only keep some current environment variables. Normally you shouldn't run the build command with `sudo`; but if you have to do so, you can try `sudo -E` to force `sudo` to preserve environment variables."
   },
   {
      "x": "Using Ninja instead of Make results in inability to find correct headers when building from source on Linux",
      "z": "Yes it seems to all work normally with USE_NINJA=OFF so far",
      "y": "Yes it seems to all work normally with USE_NINJA=OFF so far"
   },
   {
      "x": "dist.new_group() failed. BUG or I misunderstood something?",
      "z": "Hey @lecoan, the doc says:\n\n> This function requires that all processes in the main group (i.e. all processes that are part of the distributed job) enter this function, even if they are not going to be members of the group. \n\nIn the code snippet above, you have:\n\n```python\n    start = 0\n    while rank not in perm[start: start + 2]:\n        start += 2\n    group = perm.tolist()[start: start + 2]\n    pg = dist.new_group(group, timeout=timedelta(seconds=30))\n```\n\nSo all processes will enter `dist.new_group` but with different `group` ranks. As a result, all of them will enter `_new_process_group_helper` together with a smaller world size and duplicated group ranks. \n\nhttps://github.com/pytorch/pytorch/blob/590619ab8c2d237d4e0b55c8cc3552932afe7da5/torch/distributed/distributed_c10d.py#L1471-L1486\n\nCan you try making multiple `dist.new_group` calls on all processes, one call per new group. Say if you would like to create two new groups [0, 1] and [2, 3] out of 4 processes, then each process should do sth like the following, even if they are not in the group.\n\n```python\ndis.new_group([0, 1], ...)\ndis.new_group([2, 3], ...)\n```",
      "y": "Hey @lecoan, the doc says:\n\n> This function requires that all processes in the main group (i.e. all processes that are part of the distributed job) enter this function, even if they are not going to be members of the group. \n\nIn the code snippet above, you have:\n\n```python\n    start = 0\n    while rank not in perm[start: start + 2]:\n        start += 2\n    group = perm.tolist()[start: start + 2]\n    pg = dist.new_group(group, timeout=timedelta(seconds=30))\n```\n\nSo all processes will enter `dist.new_group` but with different `group` ranks. As a result, all of them will enter `_new_process_group_helper` together with a smaller world size and duplicated group ranks. \n\nhttps://github.com/pytorch/pytorch/blob/590619ab8c2d237d4e0b55c8cc3552932afe7da5/torch/distributed/distributed_c10d.py#L1471-L1486\n\nCan you try making multiple `dist.new_group` calls on all processes, one call per new group. Say if you would like to create two new groups [0, 1] and [2, 3] out of 4 processes, then each process should do sth like the following, even if they are not in the group.\n\n```python\ndis.new_group([0, 1], ...)\ndis.new_group([2, 3], ...)\n```"
   },
   {
      "x": "No type hints on nn.Parameter",
      "z": "In general, you should replicate the module import structure of the original py files, which definitionally don't have a cycle. It's possible we messed up some imports in the pyi files; in that case, you'd have to fix it.",
      "y": "In general, you should replicate the module import structure of the original py files, which definitionally don't have a cycle. It's possible we messed up some imports in the pyi files; in that case, you'd have to fix it."
   },
   {
      "x": "[jit] Can't script .type()",
      "z": "Thanks for the report! We should fix this but as a workaround until then you can use `x.to(torch.int8)`",
      "y": "Thanks for the report! We should fix this but as a workaround until then you can use `x.to(torch.int8)`"
   },
   {
      "x": "torch.distributed.gather(): the type of gather_list parameter must be list[Tensor]?",
      "z": "This works for me:\n\n```python\nimport torch\nimport torch.distributed as dist\n\ndist.init_process_group(\"gloo\")\n\ntensor = torch.tensor([dist.get_rank()], dtype=torch.int32)\nif dist.get_rank() == 0:\n    output = [tensor.clone() for _ in range(dist.get_world_size())]\n    dist.gather(tensor=tensor, gather_list=output, dst=0)\n    print(output)\nelse:\n    dist.gather(tensor=tensor, gather_list=[], dst=0)\n```\n\nThat said, we can improve this such that the non-dst doesn't have to specify `gather_list`.",
      "y": "This works for me:\n\n```python\nimport torch\nimport torch.distributed as dist\n\ndist.init_process_group(\"gloo\")\n\ntensor = torch.tensor([dist.get_rank()], dtype=torch.int32)\nif dist.get_rank() == 0:\n    output = [tensor.clone() for _ in range(dist.get_world_size())]\n    dist.gather(tensor=tensor, gather_list=output, dst=0)\n    print(output)\nelse:\n    dist.gather(tensor=tensor, gather_list=[], dst=0)\n```\n\nThat said, we can improve this such that the non-dst doesn't have to specify `gather_list`."
   },
   {
      "x": "[jit] torch.jit.script range() input type raises isInt() when used with int tensor values",
      "z": "You can find more info on https://pytorch.org/ with the Quick Start selector (change it to \"Preview (Nightly)\" at the top), but this should work for your env:\n\n```bash\npip install --pre torch torchvision -f https://download.pytorch.org/whl/nightly/cu92/torch_nightly.html\n```\n",
      "y": "You can find more info on https://pytorch.org/ with the Quick Start selector (change it to \"Preview (Nightly)\" at the top), but this should work for your env:\n\n```bash\npip install --pre torch torchvision -f https://download.pytorch.org/whl/nightly/cu92/torch_nightly.html\n```\n"
   },
   {
      "x": "RuntimeError: CUDA error: device-side assert triggered - yesterday it worked",
      "z": "I met the same error, then I set 'device = torch.device(\"cpu\")' to run codes on cpu, the error statements were much more clear.\nAnyone has a similar error can have a try.",
      "y": "I met the same error, then I set 'device = torch.device(\"cpu\")' to run codes on cpu, the error statements were much more clear.\nAnyone has a similar error can have a try."
   },
   {
      "x": "nn.functional.conv2d is a factor 5 slower when using specific weight tensor on CPU.",
      "z": "It seems your performances gets a hit by handling denormal values.\nTry to set [torch.set_flush_denormal(True)](https://pytorch.org/docs/stable/torch.html#torch.set_flush_denormal) and profile the code again.",
      "y": "It seems your performances gets a hit by handling denormal values.\nTry to set [torch.set_flush_denormal(True)](https://pytorch.org/docs/stable/torch.html#torch.set_flush_denormal) and profile the code again."
   },
   {
      "x": "interfaces of many schedulers in lr_scheduler.py are missing in lr_scheduler.pyi",
      "z": "I checked the lr_scheduler.pyi file and only found the missing type checking for two classes. I have submitted a PR #23934 . If you find other files in which there is a missing type check, I am willing to help.",
      "y": "I checked the lr_scheduler.pyi file and only found the missing type checking for two classes. I have submitted a PR #23934 . If you find other files in which there is a missing type check, I am willing to help."
   },
   {
      "x": "Memory leak when using itertools.cycle",
      "z": "Ok upon further investigation it seems that `itertools.cycle` attempts to save all outputs in order to re-cycle through them. Replacing it with something like this:\n\n```python\ndef cycle(iterable):\n    iterator = iter(iterable)\n    while True:\n        try:\n            yield next(iterator)\n        except StopIteration:\n            iterator = iter(iterable)\n```\n\nsolves the issue. Is there a canonical way of creating an infinite iterator in pytorch? It is unfortunate that `itertools.cycle` is implemented in this way, although I suppose it makes sense in the average case where the amount of data is very low.\n\nI will close this issue since it's not a pytorch issue.",
      "y": "Ok upon further investigation it seems that `itertools.cycle` attempts to save all outputs in order to re-cycle through them. Replacing it with something like this:\n\n```python\ndef cycle(iterable):\n    iterator = iter(iterable)\n    while True:\n        try:\n            yield next(iterator)\n        except StopIteration:\n            iterator = iter(iterable)\n```\n\nsolves the issue. Is there a canonical way of creating an infinite iterator in pytorch? It is unfortunate that `itertools.cycle` is implemented in this way, although I suppose it makes sense in the average case where the amount of data is very low.\n\nI will close this issue since it's not a pytorch issue."
   },
   {
      "x": "count_nonzero",
      "z": "We have added torch.count_nonzero, which I believe addresses this issue. See: https://pytorch.org/docs/master/generated/torch.count_nonzero.html?highlight=count_nonzero#torch.count_nonzero.",
      "y": "We have added torch.count_nonzero, which I believe addresses this issue. See: https://pytorch.org/docs/master/generated/torch.count_nonzero.html?highlight=count_nonzero#torch.count_nonzero."
   },
   {
      "x": "New _batch_mahalanobis slower than in previous commit",
      "z": "Yes, it is undesirable. For such shapes of A, and b, I would like to make `b` have shape `(10, 1000)` and apply triangular solve for A and new b (so no broadcasting is triggered).\n\nBtw, the regression here is caused by a performance issue of triangular_solve in GPU with batch_size=1. Consider\n```\nimport torch\nx = torch.eye(2).cuda()\ny = torch.eye(2).cuda()\nbx = x.reshape(1, 2, 2)\nby = y.reshape(1, 2, 2)\n\n%%timeit\ntorch.cuda.synchronize()\ntorch.triangular_solve(x, y)\n\n%%timeit\ntorch.cuda.synchronize()\ntorch.triangular_solve(bx, by)\n```\nwhich will return\n```\n45.4 \u00b5s \u00b1 318 ns per loop (mean \u00b1 std. dev. of 7 runs, 10000 loops each)\n```\nfor the first case and\n```\n243 \u00b5s \u00b1 3.81 \u00b5s per loop (mean \u00b1 std. dev. of 7 runs, 1000 loops each)\n```\nfor the second case.\n\nI guess we can fix it in `triangular_solve` implementation. Or I can add an if/else check to squeeze the input when `batch_shape=(1,)`. What do you think? Fixing it in `triangular_solve` seems more reasonable to me.",
      "y": "Yes, it is undesirable. For such shapes of A, and b, I would like to make `b` have shape `(10, 1000)` and apply triangular solve for A and new b (so no broadcasting is triggered).\n\nBtw, the regression here is caused by a performance issue of triangular_solve in GPU with batch_size=1. Consider\n```\nimport torch\nx = torch.eye(2).cuda()\ny = torch.eye(2).cuda()\nbx = x.reshape(1, 2, 2)\nby = y.reshape(1, 2, 2)\n\n%%timeit\ntorch.cuda.synchronize()\ntorch.triangular_solve(x, y)\n\n%%timeit\ntorch.cuda.synchronize()\ntorch.triangular_solve(bx, by)\n```\nwhich will return\n```\n45.4 \u00b5s \u00b1 318 ns per loop (mean \u00b1 std. dev. of 7 runs, 10000 loops each)\n```\nfor the first case and\n```\n243 \u00b5s \u00b1 3.81 \u00b5s per loop (mean \u00b1 std. dev. of 7 runs, 1000 loops each)\n```\nfor the second case.\n\nI guess we can fix it in `triangular_solve` implementation. Or I can add an if/else check to squeeze the input when `batch_shape=(1,)`. What do you think? Fixing it in `triangular_solve` seems more reasonable to me."
   },
   {
      "x": "Quantized Linear does not work for bias=False",
      "z": "The issue is that we don't have the direct test coverage for the `from_float` function with `bias = NULL`. The current unit test with \"indirect\" test coverage is to test `convert` and `quantize` function from here:\nhttps://github.com/pytorch/pytorch/blob/master/test/test_quantization.py#L21\n\nPreviously we have fixed the general `bias = nullptr` here: https://github.com/pytorch/pytorch/pull/22403",
      "y": "The issue is that we don't have the direct test coverage for the `from_float` function with `bias = NULL`. The current unit test with \"indirect\" test coverage is to test `convert` and `quantize` function from here:\nhttps://github.com/pytorch/pytorch/blob/master/test/test_quantization.py#L21\n\nPreviously we have fixed the general `bias = nullptr` here: https://github.com/pytorch/pytorch/pull/22403"
   },
   {
      "x": "forward_packed operator in LSTM not supported by jit scriptmodule",
      "z": "I think this is expected for your example, the hidden state is supposed to be of `Optional[Tuple[Tensor, Tensor]]` instead of just `Tensor`. Your example fails in eager mode as well\n\n```\nRuntimeError: Expected hidden[0] size (1, 1, 512), got (1, 512)\n```\n\nYour example runs fine if you use `h1` for the hidden state instead. Closing since this looks like user error, feel free to re-open if you are still running into errors.",
      "y": "I think this is expected for your example, the hidden state is supposed to be of `Optional[Tuple[Tensor, Tensor]]` instead of just `Tensor`. Your example fails in eager mode as well\n\n```\nRuntimeError: Expected hidden[0] size (1, 1, 512), got (1, 512)\n```\n\nYour example runs fine if you use `h1` for the hidden state instead. Closing since this looks like user error, feel free to re-open if you are still running into errors."
   },
   {
      "x": "Getting gradient of element of tensor wrt the element itself",
      "z": "```\nprint(torch.autograd.grad(b[0][0], b[0], allow_unused=True)\n```\n\nThis doesn't do what you want because the autograd graph for `b[0][0]` is not related to `b[0]` (no CSE happens here.) The second code is correct and you should do it that way.",
      "y": "```\nprint(torch.autograd.grad(b[0][0], b[0], allow_unused=True)\n```\n\nThis doesn't do what you want because the autograd graph for `b[0][0]` is not related to `b[0]` (no CSE happens here.) The second code is correct and you should do it that way."
   },
   {
      "x": "[naming] Promote _LRScheduler to LRScheduler",
      "z": "Since this could encourage people to experiment with the learning rate scheduler class, I'm ok if @vadimkantorov wants to open a pull request promoting `_LRScheduler` to `LRScheduler`, and keeping the old name for backward compatibility, potentially with a deprecation warning. ",
      "y": "Since this could encourage people to experiment with the learning rate scheduler class, I'm ok if @vadimkantorov wants to open a pull request promoting `_LRScheduler` to `LRScheduler`, and keeping the old name for backward compatibility, potentially with a deprecation warning. "
   },
   {
      "x": "[docs] Update and momentum formulas in SGD docs",
      "z": "IMO, this is a no brainer. I'll put up a PR.\n\nShould it be\n`v_{t+1} = p_{t}*v_{t} + g_{t+1}`? (Added a subscript for `p`)",
      "y": "IMO, this is a no brainer. I'll put up a PR.\n\nShould it be\n`v_{t+1} = p_{t}*v_{t} + g_{t+1}`? (Added a subscript for `p`)"
   },
   {
      "x": "torch.jit.trace() does not work without check_trace =False",
      "z": "> hoe to slove it ?\nJust don't pass the same tensor 3 times as the same var. See the next my comment.\n\n",
      "y": "> hoe to slove it ?\nJust don't pass the same tensor 3 times as the same var. See the next my comment.\n\n"
   },
   {
      "x": "TracerWarning: Output nr 1. of the traced function does not match the corresponding output of the Python function. Detailed error: Not within tolerance",
      "z": "I solved the problem by adding model.eval() and deleting the model.cuda(), aka, map the model to cpu",
      "y": "I solved the problem by adding model.eval() and deleting the model.cuda(), aka, map the model to cpu"
   },
   {
      "x": "[JIT] Making JIT work with Pyro's VAE example",
      "z": "> Do the examples do a lot of operations with scalar tensors?\n\nThe arguments passed to the traced functions are not scalars in many cases, but there may be other places within the traced function call (e.g. in distributions or our own internals) that have scalar operations. Is there a way to get a more detailed JIT log to localize where the issue might be?",
      "y": "> Do the examples do a lot of operations with scalar tensors?\n\nThe arguments passed to the traced functions are not scalars in many cases, but there may be other places within the traced function call (e.g. in distributions or our own internals) that have scalar operations. Is there a way to get a more detailed JIT log to localize where the issue might be?"
   },
   {
      "x": "[feature request][pytorch] finfo as in numpy and finfo for default dtype",
      "z": "If you don't care about where the code lives: https://github.com/pytorch/pytorch/blob/ddf187c198f8e249e78351ba94e773bf9d21de3a/torch/distributions/utils.py#L20",
      "y": "If you don't care about where the code lives: https://github.com/pytorch/pytorch/blob/ddf187c198f8e249e78351ba94e773bf9d21de3a/torch/distributions/utils.py#L20"
   },
   {
      "x": "Performance improvement on sparse CUDA coalesce()",
      "z": "Closing this because when nnz is large enough, CUDA kernel actually performance reasonably well:\n```\n>>> from random import *\n>>> n = 100000\n>>> I = torch.tensor([[randint(0, 99) for _ in range(3)] for _ in range(n)])\n>>> V = torch.randn(n)\n>>> size = torch.Size([1000, 1000, 1000])\n>>> S = torch.sparse_coo_tensor(I.t(), V, size)\n\n>>> %timeit S.coalesce()\n10 loops, best of 3: 30.7 ms per loop\n\n>>> S = torch.sparse_coo_tensor(I.t(), V.cuda(), size)\n>>> %timeit torch.cuda.synchronize(); S.coalesce(); torch.cuda.synchronize();\n100 loops, best of 3: 9.59 ms per loop\n```",
      "y": "Closing this because when nnz is large enough, CUDA kernel actually performance reasonably well:\n```\n>>> from random import *\n>>> n = 100000\n>>> I = torch.tensor([[randint(0, 99) for _ in range(3)] for _ in range(n)])\n>>> V = torch.randn(n)\n>>> size = torch.Size([1000, 1000, 1000])\n>>> S = torch.sparse_coo_tensor(I.t(), V, size)\n\n>>> %timeit S.coalesce()\n10 loops, best of 3: 30.7 ms per loop\n\n>>> S = torch.sparse_coo_tensor(I.t(), V.cuda(), size)\n>>> %timeit torch.cuda.synchronize(); S.coalesce(); torch.cuda.synchronize();\n100 loops, best of 3: 9.59 ms per loop\n```"
   },
   {
      "x": "autograd's elu_backward usage seems not correct",
      "z": "Closed it thinking I made a mistake but actually indeed seems wrong.\n\nThis line is on ATen's Type.h:\n`virtual Tensor elu_backward(const Tensor & grad_output, Scalar alpha, Scalar scale, const Tensor & output) const;`\n\n`elu_forward` declaration in derivatives.yaml seems wrong too, no input_scale in ATen",
      "y": "Closed it thinking I made a mistake but actually indeed seems wrong.\n\nThis line is on ATen's Type.h:\n`virtual Tensor elu_backward(const Tensor & grad_output, Scalar alpha, Scalar scale, const Tensor & output) const;`\n\n`elu_forward` declaration in derivatives.yaml seems wrong too, no input_scale in ATen"
   },
   {
      "x": "Parameters in dict not registered",
      "z": "I believe you have to use [`register_parameter`](https://pytorch.org/docs/stable/nn.html#torch.nn.Module.register_parameter) for this purpose.",
      "y": "I believe you have to use [`register_parameter`](https://pytorch.org/docs/stable/nn.html#torch.nn.Module.register_parameter) for this purpose."
   },
   {
      "x": "Cuda runtime error : the launch timed out and was terminated",
      "z": "Problem cause: https://devtalk.nvidia.com/default/topic/1043126/linux/xid-8-in-various-cuda-deep-learning-applications-for-nvidia-gtx-1080-ti/",
      "y": "Problem cause: https://devtalk.nvidia.com/default/topic/1043126/linux/xid-8-in-various-cuda-deep-learning-applications-for-nvidia-gtx-1080-ti/"
   },
   {
      "x": "[Outdated documentation] Previous Versions Installation with Conda",
      "z": "pointing `previous versions` to use `soumith` channel was a mistake. We switch from `soumith` to `pytorch` channel (i think in 0.3.1), but forgot to update the page. I just fixed the link via https://github.com/pytorch/pytorch.github.io/commit/398a3b8f6eb53ac3928cc544018e46744afcf5fa\n\nPyTorch Windows support officially released from v0.4.0, and hence we only have 0.4.0 available from the `pytorch` channel since then. For unofficial / previous releases, see `peterjc123`'s channel: https://anaconda.org/peterjc123\n\nClosed via https://github.com/pytorch/pytorch.github.io/commit/398a3b8f6eb53ac3928cc544018e46744afcf5fa\n",
      "y": "pointing `previous versions` to use `soumith` channel was a mistake. We switch from `soumith` to `pytorch` channel (i think in 0.3.1), but forgot to update the page. I just fixed the link via https://github.com/pytorch/pytorch.github.io/commit/398a3b8f6eb53ac3928cc544018e46744afcf5fa\n\nPyTorch Windows support officially released from v0.4.0, and hence we only have 0.4.0 available from the `pytorch` channel since then. For unofficial / previous releases, see `peterjc123`'s channel: https://anaconda.org/peterjc123\n\nClosed via https://github.com/pytorch/pytorch.github.io/commit/398a3b8f6eb53ac3928cc544018e46744afcf5fa\n"
   },
   {
      "x": "flip a Tensor",
      "z": "Here's @dmarnerides code but with cuda support\n\n```py\n# https://github.com/pytorch/pytorch/issues/229\ndef flip(x, dim):\n    dim = x.dim() + dim if dim < 0 else dim\n    inds = tuple(slice(None, None) if i != dim\n             else x.new(torch.arange(x.size(i)-1, -1, -1).tolist()).long()\n             for i in range(x.dim()))\n    return x[inds]\n\n# Code to test it with cpu\na = torch.Tensor([range(1, 25)]).view(1, 2, 3, 4)\nprint(a)\nprint(flip(a, 0)) # Or -4\nprint(flip(a, 1)) # Or -3\nprint(flip(a, 2)) # Or -2\nprint(flip(a, 3)) # Or -1\n\n# Code to test it with cuda\na = torch.Tensor([range(1, 25)]).view(1, 2, 3, 4).cuda()\nprint(a)\nprint(flip(a, 0)) # Or -4\nprint(flip(a, 1)) # Or -3\nprint(flip(a, 2)) # Or -2\nprint(flip(a, 3)) # Or -1\n```",
      "y": "Here's @dmarnerides code but with cuda support\n\n```py\n# https://github.com/pytorch/pytorch/issues/229\ndef flip(x, dim):\n    dim = x.dim() + dim if dim < 0 else dim\n    inds = tuple(slice(None, None) if i != dim\n             else x.new(torch.arange(x.size(i)-1, -1, -1).tolist()).long()\n             for i in range(x.dim()))\n    return x[inds]\n\n# Code to test it with cpu\na = torch.Tensor([range(1, 25)]).view(1, 2, 3, 4)\nprint(a)\nprint(flip(a, 0)) # Or -4\nprint(flip(a, 1)) # Or -3\nprint(flip(a, 2)) # Or -2\nprint(flip(a, 3)) # Or -1\n\n# Code to test it with cuda\na = torch.Tensor([range(1, 25)]).view(1, 2, 3, 4).cuda()\nprint(a)\nprint(flip(a, 0)) # Or -4\nprint(flip(a, 1)) # Or -3\nprint(flip(a, 2)) # Or -2\nprint(flip(a, 3)) # Or -1\n```"
   },
   {
      "x": "PyTorch goes distributed",
      "z": "Shubho here from SVAIL @ Baidu\n\nOne long-term thing to keep in mind is interfacing with a job scheduler - SLURM is pretty standard. I think using `salloc` with `pytorch_exec` should be fairly easy.\n\nThe framework that I helped architect at Baidu follows a peer-to-peer model - all workers are peers - no master and no slave - each peer has access to 1 GPU - and has a synchronous view of the parameter space that is completely replicated on each worker. Workers use MPI to communicate - and the MPI byte-transport layer deals with IBVerb and CUDA shared mem transport (if GPUs are on the same PCI-E root complex and many are). This peer-to-peer architecture is relatively simple and scales really well - at least to 256 GPUs and possibly more (at this point people started losing friends for hogging the cluster). It can also sustain about 70% of InfiniBand's peak bandwidth (for FDR Infiniband) but I had to reimplement the MPI collective operations since they are not optimized. This simple architecture has served us well for anything we train - various recurrent nets with or without attention, wavenet and I don't see why convnets should be an issue. This setup is however not fault tolerant - if a peer dies - the training comes to a halt - and SLURM will time it out and reschedule and this is fairly painless since we save checkpoints and it restarts from the last checkpoint. People rarely notice these failures in practice.\n\nPossibly `torch.distributed` is more full featured than this... I haven't started looking at the code yet.. but will soon...\n\nHappy to test on our cluster since we have FDR InfiniBand backplane with OpenMPI and SLURM and also contribute our learnings and code",
      "y": "Shubho here from SVAIL @ Baidu\n\nOne long-term thing to keep in mind is interfacing with a job scheduler - SLURM is pretty standard. I think using `salloc` with `pytorch_exec` should be fairly easy.\n\nThe framework that I helped architect at Baidu follows a peer-to-peer model - all workers are peers - no master and no slave - each peer has access to 1 GPU - and has a synchronous view of the parameter space that is completely replicated on each worker. Workers use MPI to communicate - and the MPI byte-transport layer deals with IBVerb and CUDA shared mem transport (if GPUs are on the same PCI-E root complex and many are). This peer-to-peer architecture is relatively simple and scales really well - at least to 256 GPUs and possibly more (at this point people started losing friends for hogging the cluster). It can also sustain about 70% of InfiniBand's peak bandwidth (for FDR Infiniband) but I had to reimplement the MPI collective operations since they are not optimized. This simple architecture has served us well for anything we train - various recurrent nets with or without attention, wavenet and I don't see why convnets should be an issue. This setup is however not fault tolerant - if a peer dies - the training comes to a halt - and SLURM will time it out and reschedule and this is fairly painless since we save checkpoints and it restarts from the last checkpoint. People rarely notice these failures in practice.\n\nPossibly `torch.distributed` is more full featured than this... I haven't started looking at the code yet.. but will soon...\n\nHappy to test on our cluster since we have FDR InfiniBand backplane with OpenMPI and SLURM and also contribute our learnings and code"
   },
   {
      "x": "UnboundLocalError when importing torch.cuda",
      "z": "In pytorch you can import torch.cuda always. cuda is lazy loaded only when actually first used",
      "y": "In pytorch you can import torch.cuda always. cuda is lazy loaded only when actually first used"
   },
   {
      "x": "ubuntu 16.04, CUDA8 and pytorch have compile issues, investigate",
      "z": "Hi @soumith \n\nI had this problem. Mine was because of the thrust library in Cuda.  (my version was 8.0.27)\n\nI updated to 8.0.44 (recent version) and it was solved. \n\n",
      "y": "Hi @soumith \n\nI had this problem. Mine was because of the thrust library in Cuda.  (my version was 8.0.27)\n\nI updated to 8.0.44 (recent version) and it was solved. \n\n"
   },
   {
      "x": "define default GPU device",
      "z": "Yeah I think `set_device` would be better. It has much clearer semantics. With `set_default_device` it might sometimes work inside a nested function, and sometimes silently have no effect.\n\n@colesbury I also don't like this very much, but as @soumith says, I think it's useful for notebooks. However, we should clearly discourage its usage in the docs, except for these situations.",
      "y": "Yeah I think `set_device` would be better. It has much clearer semantics. With `set_default_device` it might sometimes work inside a nested function, and sometimes silently have no effect.\n\n@colesbury I also don't like this very much, but as @soumith says, I think it's useful for notebooks. However, we should clearly discourage its usage in the docs, except for these situations."
   },
   {
      "x": "allow forward / backward hooks to rewrite outputs and gradients",
      "z": "You can overcame the need of the `input`/`output` by using upvalues to the function, and keeping record of them in list/dictionaries.",
      "y": "You can overcame the need of the `input`/`output` by using upvalues to the function, and keeping record of them in list/dictionaries."
   },
   {
      "x": "auto-wrap tensors as inputs to autograd",
      "z": "It's pretty much all about making the flag viral or not. The output of the function doesn't require grad if **all** inputs don't require it. This allows you to efficiently use pretrained models and never backprop through them. The output is volatile if **any** of the inputs is volatile. This is convenient for inference because you don't need to modify your parameters, but simply create a volatile input.\n\nAnother difference is that at the moment variables with requires_grad=False are still constructing the graph, while volatile ones don't (creator is None for all of them)",
      "y": "It's pretty much all about making the flag viral or not. The output of the function doesn't require grad if **all** inputs don't require it. This allows you to efficiently use pretrained models and never backprop through them. The output is volatile if **any** of the inputs is volatile. This is convenient for inference because you don't need to modify your parameters, but simply create a volatile input.\n\nAnother difference is that at the moment variables with requires_grad=False are still constructing the graph, while volatile ones don't (creator is None for all of them)"
   },
   {
      "x": "squeeze dimension after mean / sum",
      "z": "I've been working on broadcasting, and it probably makes sense to introduce broadcasting and squeeze dimension together (i.e. \"Broadcasting itself can be added sooner as it doesn't need any thought on backward-compatibility\" isn't quite true).\n\nFor example, consider test_nn.test_InstanceNorm1d:\n```\n        output = IN(input_var)\n        \n        input_reshaped = input_var.transpose(1, 0).contiguous().view(c, -1)\n        mean = input_reshaped.mean(1)\n\n        # do some calculation based on mean.data - IN.running_mean\n```\n\nHere, mean.data is (4,1) (because the dimension is not squeezed) and In.running_mean is (4).  The broadcast causes the result of the calculation to be (4,4) instead of (4,1), which changes the calculation (e.g. imagine a sum).  If mean is changed to squeeze the dimension, this calculation \"just works\" as written.  Clearly there are cases where this won't hold, but it seems like introducing them together may cause less disruption.",
      "y": "I've been working on broadcasting, and it probably makes sense to introduce broadcasting and squeeze dimension together (i.e. \"Broadcasting itself can be added sooner as it doesn't need any thought on backward-compatibility\" isn't quite true).\n\nFor example, consider test_nn.test_InstanceNorm1d:\n```\n        output = IN(input_var)\n        \n        input_reshaped = input_var.transpose(1, 0).contiguous().view(c, -1)\n        mean = input_reshaped.mean(1)\n\n        # do some calculation based on mean.data - IN.running_mean\n```\n\nHere, mean.data is (4,1) (because the dimension is not squeezed) and In.running_mean is (4).  The broadcast causes the result of the calculation to be (4,4) instead of (4,1), which changes the calculation (e.g. imagine a sum).  If mean is changed to squeeze the dimension, this calculation \"just works\" as written.  Clearly there are cases where this won't hold, but it seems like introducing them together may cause less disruption."
   },
   {
      "x": "gradient clip for optimizer",
      "z": "For those reading this thread, note that clip_grad_norm is now deprecated for clip_grad_norm_",
      "y": "For those reading this thread, note that clip_grad_norm is now deprecated for clip_grad_norm_"
   },
   {
      "x": "Segmentation fault when dividing by zero with integer tensors",
      "z": "integer division by zero getting a FP exception is something we cannot avoid, it's a hardware exception. Numpy wraps it's integer division code to literally check the denominators with conditionals and instead generates fpectl exceptions. I dont think this can be a hi-pri issue for us just looking at the amount of work involved in refactoring TH.\n\nYour second issue looks more like a bug.",
      "y": "integer division by zero getting a FP exception is something we cannot avoid, it's a hardware exception. Numpy wraps it's integer division code to literally check the denominators with conditionals and instead generates fpectl exceptions. I dont think this can be a hi-pri issue for us just looking at the amount of work involved in refactoring TH.\n\nYour second issue looks more like a bug."
   },
   {
      "x": "Softmax2d and LogSoftmax don't work for 2d and higher",
      "z": "It looks correct to me. `Softmax2d` does a softmax over channels, in each point of the 3rd and 4th dimension independently. You have only a single channel, so all values get normalized to ones.",
      "y": "It looks correct to me. `Softmax2d` does a softmax over channels, in each point of the 3rd and 4th dimension independently. You have only a single channel, so all values get normalized to ones."
   },
   {
      "x": "CUDA initialization fails fatally in multiprocessing",
      "z": "Seems that multithreading works in this example. I will first try this line and see how it works. \n\nThanks all for the help! ",
      "y": "Seems that multithreading works in this example. I will first try this line and see how it works. \n\nThanks all for the help! "
   },
   {
      "x": "cudnn backend needs contiguity checks to report better error messages",
      "z": "Ok, so it appears that cuDNN conv modules could give invalid results, but that could happen only if one had non-contiguous weights. **As long as you sticked to nn modules this bug did not affect you in any way.** For RNN modules, we haven't seen any issues like these with conv, but we'll go over the code to make sure there are no more errors like these.\n\nAnother other error is that sometimes non-contuguous inputs gave `CUDNN_STATUS_INTERNAL_ERROR` for some unknown reason. But it was harmless in a sense, that it would quietly mess up your results.\n\n@ngimel do you know of any other places where we should be more careful with contiguity checks? any hints on why non-contig inputs can give internal errors?",
      "y": "Ok, so it appears that cuDNN conv modules could give invalid results, but that could happen only if one had non-contiguous weights. **As long as you sticked to nn modules this bug did not affect you in any way.** For RNN modules, we haven't seen any issues like these with conv, but we'll go over the code to make sure there are no more errors like these.\n\nAnother other error is that sometimes non-contuguous inputs gave `CUDNN_STATUS_INTERNAL_ERROR` for some unknown reason. But it was harmless in a sense, that it would quietly mess up your results.\n\n@ngimel do you know of any other places where we should be more careful with contiguity checks? any hints on why non-contig inputs can give internal errors?"
   },
   {
      "x": "nn.Sequential should have an add_module(module) instead of add_module(name, module)",
      "z": "If you want to just change the stride/padding/dilation, you can just directly change the [property](https://github.com/pytorch/pytorch/blob/master/torch/nn/modules/conv.py#L22) on the `ConvNd` object.\n\nIf you want to reuse the pretrained weight, just do `new_layer.weight = orig_layer.weight` and then use the `new_layer` when constructing the new `Sequential`.",
      "y": "If you want to just change the stride/padding/dilation, you can just directly change the [property](https://github.com/pytorch/pytorch/blob/master/torch/nn/modules/conv.py#L22) on the `ConvNd` object.\n\nIf you want to reuse the pretrained weight, just do `new_layer.weight = orig_layer.weight` and then use the `new_layer` when constructing the new `Sequential`."
   },
   {
      "x": "Some function don't implement \"out=result\" convention",
      "z": "`out` has to be a `ByteTensor`.",
      "y": "`out` has to be a `ByteTensor`."
   },
   {
      "x": "THNN unnecessary .zero() calls on gradients",
      "z": "this is now fixed in master: https://github.com/pytorch/pytorch/blob/master/torch/csrc/autograd/functions/batch_normalization.cpp#L119",
      "y": "this is now fixed in master: https://github.com/pytorch/pytorch/blob/master/torch/csrc/autograd/functions/batch_normalization.cpp#L119"
   },
   {
      "x": "binaries compiled against newer numpy, but executed with older numpy error out",
      "z": "upgrade your numpy",
      "y": "upgrade your numpy"
   },
   {
      "x": "simpler explode and join",
      "z": "Just tried `torch.stack`, it's great, thanks!\n\nRe #289, numpy-like broadcast would be awesome (even if it was an explicit broadcast, like `t.broadcast(dim, num_repeat)`).",
      "y": "Just tried `torch.stack`, it's great, thanks!\n\nRe #289, numpy-like broadcast would be awesome (even if it was an explicit broadcast, like `t.broadcast(dim, num_repeat)`)."
   },
   {
      "x": "How to package pytorch with the file build from source.",
      "z": "Use `python setup.py bdist_wheel` to build a wheel file. The wheel file will be in the `dist` sub-directory.",
      "y": "Use `python setup.py bdist_wheel` to build a wheel file. The wheel file will be in the `dist` sub-directory."
   },
   {
      "x": "Missing tests for most torch.*(out=...) tensor operators",
      "z": "I think https://github.com/pytorch/pytorch/pull/53259 addresses this issue. Let me know if you agree @antocuni. If it doesn't, let's maybe open a new issue that explains what else should be done.",
      "y": "I think https://github.com/pytorch/pytorch/pull/53259 addresses this issue. Let me know if you agree @antocuni. If it doesn't, let's maybe open a new issue that explains what else should be done."
   },
   {
      "x": "BatchSampler & PEP 479",
      "z": "try making the list [1,2,3] into an iterator and try again. That iterator would raise StopIteration at the end and it would be converted to RuntimeError in this parent iterator according to the PEP.",
      "y": "try making the list [1,2,3] into an iterator and try again. That iterator would raise StopIteration at the end and it would be converted to RuntimeError in this parent iterator according to the PEP."
   },
   {
      "x": "`CatTransform` should work with `event_dim > 0` transforms",
      "z": "@vikigenius hmm I think I see your point now, and I think my previous comment was wrong. Does this look right to you:\n1. The concatenation `dim` can be either a batch dim or an event dim.\n2. If `dim` is a batch dim then the current logic is correct, i.e. concatenate the log-det-jacobians:\n    ```py\n    logdeghacs = []\n    for trans, length in zip(self.transforms, self.lengths):\n        ...\n        logdetjacs.append(trans.log_abs_det_jacobian(...))\n    return torch.cat(logdetjacs, dim=self.dim)\n    ```\n3. If `dim` is an event dim then we need new logic to instead *add* the component log-det-jacobians:\n    ```py\n    ...\n    return sum(logdetjacs)\n    ```\n\nIf this sounds correct, then I think there's a pretty simple fix:\n\n<details>\n\n```diff\ndiff --git a/torch/distributions/transforms.py b/torch/distributions/transforms.py\nindex 09e00d55e8..eddce21c25 100644\n--- a/torch/distributions/transforms.py\n+++ b/torch/distributions/transforms.py\n@@ -633,6 +633,7 @@ class CatTransform(Transform):\n     \"\"\"\n     def __init__(self, tseq, dim=0, lengths=None, cache_size=0):\n         assert all(isinstance(t, Transform) for t in tseq)\n+        assert len({t.event_dim for t in tseq}) == 1\n         if cache_size:\n             tseq = [t.with_cache(cache_size) for t in tseq]\n         super(CatTransform, self).__init__(cache_size=cache_size)\n@@ -686,7 +687,15 @@ class CatTransform(Transform):\n             yslice = y.narrow(self.dim, start, length)\n             logdetjacs.append(trans.log_abs_det_jacobian(xslice, yslice))\n             start = start + length  # avoid += for jit compat\n-        return torch.cat(logdetjacs, dim=self.dim)\n+        dim = self.event_dim + (self.dim if self.dim < 0 else self.dim - x.dim())\n+        if dim < 0:  # concatenate along a batch_dim\n+            return torch.cat(logdetjacs, dim=dim)\n+        else:  # concatenate along an event_dim\n+            return sum(logdetjacs)\n+\n+    @property\n+    def event_dim(self):\n+        return self.transforms[0].event_dim\n\n     @property\n     def bijective(self):\n```\n\n</details>",
      "y": "@vikigenius hmm I think I see your point now, and I think my previous comment was wrong. Does this look right to you:\n1. The concatenation `dim` can be either a batch dim or an event dim.\n2. If `dim` is a batch dim then the current logic is correct, i.e. concatenate the log-det-jacobians:\n    ```py\n    logdeghacs = []\n    for trans, length in zip(self.transforms, self.lengths):\n        ...\n        logdetjacs.append(trans.log_abs_det_jacobian(...))\n    return torch.cat(logdetjacs, dim=self.dim)\n    ```\n3. If `dim` is an event dim then we need new logic to instead *add* the component log-det-jacobians:\n    ```py\n    ...\n    return sum(logdetjacs)\n    ```\n\nIf this sounds correct, then I think there's a pretty simple fix:\n\n<details>\n\n```diff\ndiff --git a/torch/distributions/transforms.py b/torch/distributions/transforms.py\nindex 09e00d55e8..eddce21c25 100644\n--- a/torch/distributions/transforms.py\n+++ b/torch/distributions/transforms.py\n@@ -633,6 +633,7 @@ class CatTransform(Transform):\n     \"\"\"\n     def __init__(self, tseq, dim=0, lengths=None, cache_size=0):\n         assert all(isinstance(t, Transform) for t in tseq)\n+        assert len({t.event_dim for t in tseq}) == 1\n         if cache_size:\n             tseq = [t.with_cache(cache_size) for t in tseq]\n         super(CatTransform, self).__init__(cache_size=cache_size)\n@@ -686,7 +687,15 @@ class CatTransform(Transform):\n             yslice = y.narrow(self.dim, start, length)\n             logdetjacs.append(trans.log_abs_det_jacobian(xslice, yslice))\n             start = start + length  # avoid += for jit compat\n-        return torch.cat(logdetjacs, dim=self.dim)\n+        dim = self.event_dim + (self.dim if self.dim < 0 else self.dim - x.dim())\n+        if dim < 0:  # concatenate along a batch_dim\n+            return torch.cat(logdetjacs, dim=dim)\n+        else:  # concatenate along an event_dim\n+            return sum(logdetjacs)\n+\n+    @property\n+    def event_dim(self):\n+        return self.transforms[0].event_dim\n\n     @property\n     def bijective(self):\n```\n\n</details>"
   },
   {
      "x": "Build-from-source Failed on Mac OS",
      "z": "Hi,\n\nThis is simple to fix. All you have to do is to change the `%ld` to `%lld` at `torch/csrc/jit/codegen/cuda/kernel_cache.cpp:29` and `torch/csrc/jit/codegen/cuda/kernel_cache.cpp:31`. Can you make that change and try to build?",
      "y": "Hi,\n\nThis is simple to fix. All you have to do is to change the `%ld` to `%lld` at `torch/csrc/jit/codegen/cuda/kernel_cache.cpp:29` and `torch/csrc/jit/codegen/cuda/kernel_cache.cpp:31`. Can you make that change and try to build?"
   },
   {
      "x": "[Windows CUDA Build] Unknown option '-Xcompiler /w -w'",
      "z": "Seems it's similar but not exactly that issue (and that's already fixed, probably)\n\nFound 2 ways around:\n1. Turn off cuda separable compilation.\n2. msbuild is probably OK. But it's way too slow (no concurrency for nvcc, will run for hours) so I didn't do a full test.",
      "y": "Seems it's similar but not exactly that issue (and that's already fixed, probably)\n\nFound 2 ways around:\n1. Turn off cuda separable compilation.\n2. msbuild is probably OK. But it's way too slow (no concurrency for nvcc, will run for hours) so I didn't do a full test."
   },
   {
      "x": "Implementation of 2d bicubic grid sampler",
      "z": "Answer to myself. If the forward is implemented, the numerical method could be used to get the ground truth of backward gradient. i.e. forward a small amount dh, then get the output dH. dH/dh is the partial differential for the specific pixel output and pixel input",
      "y": "Answer to myself. If the forward is implemented, the numerical method could be used to get the ground truth of backward gradient. i.e. forward a small amount dh, then get the output dH. dH/dh is the partial differential for the specific pixel output and pixel input"
   },
   {
      "x": "Something goes wrong with pytorch build from source,",
      "z": "Hi,\n\nWe use github issues only for bugs or feature requests.\nPlease use the forum to ask questions: https://discuss.pytorch.org/\n\nIn your case it looks like some issues with the GLOO detection. Are you planning on using distributed? If not you can try to set `USE_DISTRIBUTED=0`.",
      "y": "Hi,\n\nWe use github issues only for bugs or feature requests.\nPlease use the forum to ask questions: https://discuss.pytorch.org/\n\nIn your case it looks like some issues with the GLOO detection. Are you planning on using distributed? If not you can try to set `USE_DISTRIBUTED=0`."
   },
   {
      "x": "OneCycleLR Scheduler does not have argument for verbose",
      "z": "Fixed by #41580",
      "y": "Fixed by #41580"
   },
   {
      "x": "rewrite the torch.sparse main doc page",
      "z": "For me it was super-confusing multitude of different matrix multiply ops:\ntorch.mm, torch.matmul torch.sparse.mm, torch.sparse.FloatTensor.spmm, torch.sparse.FloatTensor.hspmm, torch.sparse.FloatTensor.sspmm\n\nMy practical usecase was sketching: dense-sparse multiply with +1/-1 sparse matrix (requires no gradient to the sparse tensor): https://gist.github.com/vadimkantorov/d9b56f9b85f1f4ce59ffecf893a1581a#file-compact_bilinear_pooling-py-L15 ",
      "y": "For me it was super-confusing multitude of different matrix multiply ops:\ntorch.mm, torch.matmul torch.sparse.mm, torch.sparse.FloatTensor.spmm, torch.sparse.FloatTensor.hspmm, torch.sparse.FloatTensor.sspmm\n\nMy practical usecase was sketching: dense-sparse multiply with +1/-1 sparse matrix (requires no gradient to the sparse tensor): https://gist.github.com/vadimkantorov/d9b56f9b85f1f4ce59ffecf893a1581a#file-compact_bilinear_pooling-py-L15 "
   },
   {
      "x": "[feature request] ONNX export for torch.std_mean / torch.var_mean",
      "z": "We recently merged a PR (https://github.com/pytorch/pytorch/pull/45678) adding support for `var_mean` and `std_mean`. Please try it out and see if we can close this item.",
      "y": "We recently merged a PR (https://github.com/pytorch/pytorch/pull/45678) adding support for `var_mean` and `std_mean`. Please try it out and see if we can close this item."
   },
   {
      "x": "[JIT] RuntimeError:  ill formed octal specifier for List[str]",
      "z": "Thanks for the help @SplitInfinity. I can confirm that 1.7 solved the problem :D",
      "y": "Thanks for the help @SplitInfinity. I can confirm that 1.7 solved the problem :D"
   },
   {
      "x": "torch.utils.data.random_split crashes without an error message with non CPU Generator object",
      "z": "> > @Kae1101\n> > I got the same problem 2 day ago in colab, my code also ran find before. I think the problem happen when your DataLoader for training set attribute shuffle=True, you can try with your test DataLoader, which shuffle attribute set to False, the problem won't happen.\n> > I find the way to make my code run again, hope it will help you\n> > add attribute generator to your DataLoader and set it like this:\n> > train_loader = DataLoader(trainset,batch_size=batch_size,shuffle=True,num_workers=0,pin_memory=False, generator=torch.Generator(device='cuda'))\n> > Hope it gonna help :)))\n> \n> @NLQVan\n> Thanks for your solution~But I already fixed the problem by commenting out the following command:\n> \n> # torch.set_default_tensor_type(torch.cuda.FloatTensor)\n> does dataiter.next() return cpu.floatTensor by default? If it does, I think that is why the error was reported... ...\n> \n> But I still confused because I ran the same code may be more than 50 times a week before 2021/06/19 and didn't get any errors like this.\n\nThanks. I was having the same issue and downgrading to 1.8.1 (March release) from 1.9.0 (June release, current latest) fixes this. ",
      "y": "> > @Kae1101\n> > I got the same problem 2 day ago in colab, my code also ran find before. I think the problem happen when your DataLoader for training set attribute shuffle=True, you can try with your test DataLoader, which shuffle attribute set to False, the problem won't happen.\n> > I find the way to make my code run again, hope it will help you\n> > add attribute generator to your DataLoader and set it like this:\n> > train_loader = DataLoader(trainset,batch_size=batch_size,shuffle=True,num_workers=0,pin_memory=False, generator=torch.Generator(device='cuda'))\n> > Hope it gonna help :)))\n> \n> @NLQVan\n> Thanks for your solution~But I already fixed the problem by commenting out the following command:\n> \n> # torch.set_default_tensor_type(torch.cuda.FloatTensor)\n> does dataiter.next() return cpu.floatTensor by default? If it does, I think that is why the error was reported... ...\n> \n> But I still confused because I ran the same code may be more than 50 times a week before 2021/06/19 and didn't get any errors like this.\n\nThanks. I was having the same issue and downgrading to 1.8.1 (March release) from 1.9.0 (June release, current latest) fixes this. "
   },
   {
      "x": "Memory surges when loading models",
      "z": "Came across this while training on a GPU with very limited memory. Training worked fine but upon restoring from a previous checkpoint, got OOM on the GPU. Thanks so much for the hint to \"load the checkpoint to cpu first and then move onto GPU\" and the note in the torch.load() function! This easily worked for me:\n```\n# Bad gives OOM on GPU\n# params = torch.load(model_save_path, map_location=lambda storage, loc: storage)\n# Good. Load to cpu first, then to GPU\nparams = torch.load(model_save_path, map_location='cpu')\nmodel.load_state_dict(params['state_dict'])\nmodel = model.to(device)\n```\nSubsequently, I ran into them same problem with the optimizer, where I got OOM while the GPU tensors where copied. The same strategy was successful here too, namely\n```\n# Bad. Gives OOM\n# optimizer.load_state_dict(torch.load(model_save_path + '.optim')\n# Good. Works.\noptimizer.load_state_dict(torch.load(model_save_path + '.optim', map_location='cpu'))\noptimizer_to(optimizer,device)\n```\nHere, optimizer_to is the code snippet from @0phoff posted in #8741 \n```\ndef optimizer_to(optim, device):\n    for param in optim.state.values():\n        # Not sure there are any global tensors in the state dict\n        if isinstance(param, torch.Tensor):\n            param.data = param.data.to(device)\n            if param._grad is not None:\n                param._grad.data = param._grad.data.to(device)\n        elif isinstance(param, dict):\n            for subparam in param.values():\n                if isinstance(subparam, torch.Tensor):\n                    subparam.data = subparam.data.to(device)\n                    if subparam._grad is not None:\n                        subparam._grad.data = subparam._grad.data.to(device)\n```\n(This issue came up prominently on google when searching my error, so just would like to share how one can solve.)",
      "y": "Came across this while training on a GPU with very limited memory. Training worked fine but upon restoring from a previous checkpoint, got OOM on the GPU. Thanks so much for the hint to \"load the checkpoint to cpu first and then move onto GPU\" and the note in the torch.load() function! This easily worked for me:\n```\n# Bad gives OOM on GPU\n# params = torch.load(model_save_path, map_location=lambda storage, loc: storage)\n# Good. Load to cpu first, then to GPU\nparams = torch.load(model_save_path, map_location='cpu')\nmodel.load_state_dict(params['state_dict'])\nmodel = model.to(device)\n```\nSubsequently, I ran into them same problem with the optimizer, where I got OOM while the GPU tensors where copied. The same strategy was successful here too, namely\n```\n# Bad. Gives OOM\n# optimizer.load_state_dict(torch.load(model_save_path + '.optim')\n# Good. Works.\noptimizer.load_state_dict(torch.load(model_save_path + '.optim', map_location='cpu'))\noptimizer_to(optimizer,device)\n```\nHere, optimizer_to is the code snippet from @0phoff posted in #8741 \n```\ndef optimizer_to(optim, device):\n    for param in optim.state.values():\n        # Not sure there are any global tensors in the state dict\n        if isinstance(param, torch.Tensor):\n            param.data = param.data.to(device)\n            if param._grad is not None:\n                param._grad.data = param._grad.data.to(device)\n        elif isinstance(param, dict):\n            for subparam in param.values():\n                if isinstance(subparam, torch.Tensor):\n                    subparam.data = subparam.data.to(device)\n                    if subparam._grad is not None:\n                        subparam._grad.data = subparam._grad.data.to(device)\n```\n(This issue came up prominently on google when searching my error, so just would like to share how one can solve.)"
   },
   {
      "x": "[feature request] `select_index` for sparse tensors",
      "z": "Thanks. I have solved the problem. I just need to get one-hot embedding for the diag sparse matrix. So I do not need to select the specific row. Instead, I directly generate the one-hot embeddings in the training process rather than select from the sparse matrix.",
      "y": "Thanks. I have solved the problem. I just need to get one-hot embedding for the diag sparse matrix. So I do not need to select the specific row. Instead, I directly generate the one-hot embeddings in the training process rather than select from the sparse matrix."
   },
   {
      "x": "grad of output with respect to inputs  functions on cpu but not gpu",
      "z": "Your code had a bug, I fixed it for you:\n\n```\ndef test_gpu():\n    mod = testModule().cuda()\n    t = torch.ones([1, 10], requires_grad=True, device=\"cuda:0\")\n    output = mod(t)\n    output[0].backward()\n    test = t.grad\n```",
      "y": "Your code had a bug, I fixed it for you:\n\n```\ndef test_gpu():\n    mod = testModule().cuda()\n    t = torch.ones([1, 10], requires_grad=True, device=\"cuda:0\")\n    output = mod(t)\n    output[0].backward()\n    test = t.grad\n```"
   },
   {
      "x": "Caffe2 install failure",
      "z": "Serendipitously, I was able to eventually find the solution to my problem. I will post my solution here, in case someone else stumbles upon this same problem, then I'll close this issue. \n\n1. Someone else had a similar issue on the caffe2 repo: https://github.com/caffe2/caffe2/issues/2487 and the solution was to re-install the eigen3 library from the master branch at the [github mirror](https://github.com/eigenteam/eigen-git-mirror.git). So that solved this particular issue.\n2. Next, I had another problem with cuda/cudnn somehow (that I did not screenshot) but it was similar to [this problem](https://discuss.pytorch.org/t/solved-source-compile-error/9490). Incredibly, as this link suggests, the solution was to change a line in the file: /usr/include/cudnn.h from\n```c++\n#include \u201cdriver_types.h\u201d\n```\nto\n```c++\n#include <driver_types.h>\n```\nSo that it looks in the correct location for the header file `driver_types.h`...Now everything is installed properly and hopefully this helps someone else down the road.",
      "y": "Serendipitously, I was able to eventually find the solution to my problem. I will post my solution here, in case someone else stumbles upon this same problem, then I'll close this issue. \n\n1. Someone else had a similar issue on the caffe2 repo: https://github.com/caffe2/caffe2/issues/2487 and the solution was to re-install the eigen3 library from the master branch at the [github mirror](https://github.com/eigenteam/eigen-git-mirror.git). So that solved this particular issue.\n2. Next, I had another problem with cuda/cudnn somehow (that I did not screenshot) but it was similar to [this problem](https://discuss.pytorch.org/t/solved-source-compile-error/9490). Incredibly, as this link suggests, the solution was to change a line in the file: /usr/include/cudnn.h from\n```c++\n#include \u201cdriver_types.h\u201d\n```\nto\n```c++\n#include <driver_types.h>\n```\nSo that it looks in the correct location for the header file `driver_types.h`...Now everything is installed properly and hopefully this helps someone else down the road."
   },
   {
      "x": "THD refactoring",
      "z": "Don't think we need this issue from now on.",
      "y": "Don't think we need this issue from now on."
   },
   {
      "x": "[Caffe2] Are you still maintaining Caffe2 docker?",
      "z": "@jgong5 @pjh5\nI think it's better to update the Caffe2 images. Because the Detectron Dockerfile is based on Caffe2 base. \nIt's inconvenient to build our own Caffe2 while I just want to use Detectron.",
      "y": "@jgong5 @pjh5\nI think it's better to update the Caffe2 images. Because the Detectron Dockerfile is based on Caffe2 base. \nIt's inconvenient to build our own Caffe2 while I just want to use Detectron."
   },
   {
      "x": "[Feature Request] nn.Module should also get a `device` attribute",
      "z": "That\u2019s not possible. Modules can hold parameters of different types on different devices, and so it\u2019s not always possible to unambiguously determine the device.",
      "y": "That\u2019s not possible. Modules can hold parameters of different types on different devices, and so it\u2019s not always possible to unambiguously determine the device."
   },
   {
      "x": "magma in pytorch",
      "z": "because we statically link magma, the magma package is not needed at runtime.",
      "y": "because we statically link magma, the magma package is not needed at runtime."
   },
   {
      "x": "Cannot deepcopy torch.(int/float/...)*",
      "z": "That's because `numpy.float32` isn't a numpy dtype:\n```\ntype(numpy.float32)\n<class 'type'>\n\n\n>>> type(numpy.array(0).dtype)\n<class 'numpy.dtype'>\n\n>>> isinstance(numpy.array(0).dtype, type)\nFalse\n```",
      "y": "That's because `numpy.float32` isn't a numpy dtype:\n```\ntype(numpy.float32)\n<class 'type'>\n\n\n>>> type(numpy.array(0).dtype)\n<class 'numpy.dtype'>\n\n>>> isinstance(numpy.array(0).dtype, type)\nFalse\n```"
   },
   {
      "x": "Multiprocessing runtime error freeze_support() in Windows 64 bit",
      "z": "Read the windows [doc](https://pytorch.org/docs/stable/notes/windows.html#multiprocessing-error-without-if-clause-protection) please.",
      "y": "Read the windows [doc](https://pytorch.org/docs/stable/notes/windows.html#multiprocessing-error-without-if-clause-protection) please."
   },
   {
      "x": "AttributeError: module 'torch.nn' has no attribute 'BCEWithLogitsLoss'",
      "z": "That is too old. You should upgrade to the latest version of pytorch (which is 0.4). Installation instructions can be found at our [main website](https://pytorch.org/). Hope that helps!\n\nIf you don't want to upgrade you can probably copy and paste the BCELossWithLogits code and make your own Loss function.",
      "y": "That is too old. You should upgrade to the latest version of pytorch (which is 0.4). Installation instructions can be found at our [main website](https://pytorch.org/). Hope that helps!\n\nIf you don't want to upgrade you can probably copy and paste the BCELossWithLogits code and make your own Loss function."
   },
   {
      "x": "[PyTorch] KeyError: 'momentum' while using SGD optimizer",
      "z": "Because I loaded checkpoints from an Adam optimizer.",
      "y": "Because I loaded checkpoints from an Adam optimizer."
   },
   {
      "x": "Drop support for magma v1 (compilation with it is broken right now)",
      "z": "Remove all the `MAGMA_V2` ifdefs",
      "y": "Remove all the `MAGMA_V2` ifdefs"
   },
   {
      "x": "Can't get attribute 'Net' on <module '__main__' from 'D:/demo/cnn/test1.py'>",
      "z": "You should save & load the statedict instead. :)\nSee https://pytorch.org/docs/master/notes/serialization.html",
      "y": "You should save & load the statedict instead. :)\nSee https://pytorch.org/docs/master/notes/serialization.html"
   },
   {
      "x": "Failure to install caffe2 builded from source",
      "z": "Closing this issue due to age and because it is now recommended to use PyTorch, not Caffe2. If this is still relevant please file a new issue. ",
      "y": "Closing this issue due to age and because it is now recommended to use PyTorch, not Caffe2. If this is still relevant please file a new issue. "
   },
   {
      "x": "Compiling Pytorch 0.4 from source for Tegra (arm processor) fails",
      "z": "I'm closing this since the original problem was solved with commit `fb5cc630f6f4cbdebde08e0e82a9679431afa9d2 `.\n\n@juanmed, if you're running into build issues on Tegra can you open up a new issue with the relevant details (build logs, etc.)? Although, to be honest, I'm not sure how much effort we will put into to supporting PyTorch 0.4. ",
      "y": "I'm closing this since the original problem was solved with commit `fb5cc630f6f4cbdebde08e0e82a9679431afa9d2 `.\n\n@juanmed, if you're running into build issues on Tegra can you open up a new issue with the relevant details (build logs, etc.)? Although, to be honest, I'm not sure how much effort we will put into to supporting PyTorch 0.4. "
   },
   {
      "x": "\"Parameters of a model after .cuda() will be different objects with those before the call.\" is wrong.",
      "z": "> If you need to move a model to GPU via .cuda(), please do so before constructing optimizers for it. Parameters of a model after .cuda() will be different objects with those before the call.\n>\n> In general, you should make sure that optimized parameters live in consistent locations when optimizers are constructed and used.\n\nStarting from https://github.com/pytorch/pytorch/pull/21613, the new behavior we will have in future releases is consistent with this warning, which is that parameters of a model after dtype/device conversion functions such as `.cuda()`/`.cpu()`/`.to()`/`.float()`/`.double()` will be different objects with those before the call (you can enable this new behavior by setting `torch.__future__.set_overwrite_module_params_on_conversion(True)`. Hence we strongly recommend converting the model to a different device / dtype **before** constructing optimizers for it.",
      "y": "> If you need to move a model to GPU via .cuda(), please do so before constructing optimizers for it. Parameters of a model after .cuda() will be different objects with those before the call.\n>\n> In general, you should make sure that optimized parameters live in consistent locations when optimizers are constructed and used.\n\nStarting from https://github.com/pytorch/pytorch/pull/21613, the new behavior we will have in future releases is consistent with this warning, which is that parameters of a model after dtype/device conversion functions such as `.cuda()`/`.cpu()`/`.to()`/`.float()`/`.double()` will be different objects with those before the call (you can enable this new behavior by setting `torch.__future__.set_overwrite_module_params_on_conversion(True)`. Hence we strongly recommend converting the model to a different device / dtype **before** constructing optimizers for it."
   },
   {
      "x": "[PyTorch] [ONNX] Peephole optimizer transpose fusion broken",
      "z": "Try this on for size https://github.com/pytorch/pytorch/pull/7872\n",
      "y": "Try this on for size https://github.com/pytorch/pytorch/pull/7872\n"
   },
   {
      "x": "[PyTorch] torch.stft is slow on cpu compared to numpy",
      "z": "@Rikorose Now the master has a `stft` with a new signature consistent with `librosa` and using `fft`.",
      "y": "@Rikorose Now the master has a `stft` with a new signature consistent with `librosa` and using `fft`."
   },
   {
      "x": "Cannot import onnx_caffe2.backend",
      "z": "I finally resolved it. Seems like I need to use the protobuf that caffe2 depends on instead of the one from conda-forge (they are both 3.5.2 though), to do this just install caffe2 before onnx. And I have to build onnx from source.",
      "y": "I finally resolved it. Seems like I need to use the protobuf that caffe2 depends on instead of the one from conda-forge (they are both 3.5.2 though), to do this just install caffe2 before onnx. And I have to build onnx from source."
   },
   {
      "x": "Serious perf drop on CPU",
      "z": "@mingfeima - Thank you for looking into this further and the update! \n\nI looked into [removing tbb and replacing it with openmp](https://github.com/pytorch/pytorch/pull/8255) and then running the benchmark. I get much better performance (i.e. am able to close the perf gap) when doing so. You could give this a try as well if you like. We generally prefer tbb for many reasons, but might need to (temporarily) go ahead with this if it turns out to be the main source for this and then reintroduce it as part of a larger effort.\n\nGenerally we can't expect the user to set or know about things like `KMP_BLOCKTIME` or \n `KMP_AFFINITY`, so we'd either need to set them statically or change our setup so that they are not necessary.\n\nEDIT: @mingfeima - I assume this is mkldnn through your channel? I've also seen some issues where it won't set the rpath to link against openmp and mklml\n\n```\n/usr/bin/ld: warning: libmklml_intel.so, needed by //private/home/cpuhrsch/miniconda2/lib/libmkldnn.so.0, not found (try using -rpath or -rpath-link)\n/usr/bin/ld: warning: libiomp5.so, needed by //private/home/cpuhrsch/miniconda2/lib/libmkldnn.so.0, not found (try using -rpath or -rpath-link)\n```\n\nIt can be fixed by setting my LD_LIBRARY_PATH to point to `/private/home/cpuhrsch/miniconda2/lib`, but I'm wondering if this is something you could adjust as part of your conda package in general? To give you more context, I'm building a binary against libcaffe2.so within a repository outside of pytorch, which was build using mkldnn using [this cmake setup](https://github.com/pytorch/benchmark/blob/f8fb533f5c62f0766eef2e67d372e94b8be7312f/timing/cpp/CMakeLists.txt).",
      "y": "@mingfeima - Thank you for looking into this further and the update! \n\nI looked into [removing tbb and replacing it with openmp](https://github.com/pytorch/pytorch/pull/8255) and then running the benchmark. I get much better performance (i.e. am able to close the perf gap) when doing so. You could give this a try as well if you like. We generally prefer tbb for many reasons, but might need to (temporarily) go ahead with this if it turns out to be the main source for this and then reintroduce it as part of a larger effort.\n\nGenerally we can't expect the user to set or know about things like `KMP_BLOCKTIME` or \n `KMP_AFFINITY`, so we'd either need to set them statically or change our setup so that they are not necessary.\n\nEDIT: @mingfeima - I assume this is mkldnn through your channel? I've also seen some issues where it won't set the rpath to link against openmp and mklml\n\n```\n/usr/bin/ld: warning: libmklml_intel.so, needed by //private/home/cpuhrsch/miniconda2/lib/libmkldnn.so.0, not found (try using -rpath or -rpath-link)\n/usr/bin/ld: warning: libiomp5.so, needed by //private/home/cpuhrsch/miniconda2/lib/libmkldnn.so.0, not found (try using -rpath or -rpath-link)\n```\n\nIt can be fixed by setting my LD_LIBRARY_PATH to point to `/private/home/cpuhrsch/miniconda2/lib`, but I'm wondering if this is something you could adjust as part of your conda package in general? To give you more context, I'm building a binary against libcaffe2.so within a repository outside of pytorch, which was build using mkldnn using [this cmake setup](https://github.com/pytorch/benchmark/blob/f8fb533f5c62f0766eef2e67d372e94b8be7312f/timing/cpp/CMakeLists.txt)."
   },
   {
      "x": "How to implement the internal zero padding for the feature map?",
      "z": "This works:\n```py\n>>> import torch.nn.functional as F\n>>>\n>>> def pad_within(x, stride=2):\n...   w = x.new_zeros(stride, stride)\n...   w[0, 0] = 1\n...   return F.conv_transpose2d(x, w.expand(x.size(1), 1, stride, stride), stride=stride, groups=x.size(1))\n...\n>>> x = torch.arange(8, dtype=torch.float).view(2, 4).expand(1, 3, 2, 4)\n>>> x\ntensor([[[[ 0.,  1.,  2.,  3.],\n          [ 4.,  5.,  6.,  7.]],\n\n         [[ 0.,  1.,  2.,  3.],\n          [ 4.,  5.,  6.,  7.]],\n\n         [[ 0.,  1.,  2.,  3.],\n          [ 4.,  5.,  6.,  7.]]]])\n>>> pad_within(x)\ntensor([[[[ 0.,  0.,  1.,  0.,  2.,  0.,  3.,  0.],\n          [ 0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.],\n          [ 4.,  0.,  5.,  0.,  6.,  0.,  7.,  0.],\n          [ 0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.]],\n\n         [[ 0.,  0.,  1.,  0.,  2.,  0.,  3.,  0.],\n          [ 0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.],\n          [ 4.,  0.,  5.,  0.,  6.,  0.,  7.,  0.],\n          [ 0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.]],\n\n         [[ 0.,  0.,  1.,  0.,  2.,  0.,  3.,  0.],\n          [ 0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.],\n          [ 4.,  0.,  5.,  0.,  6.,  0.,  7.,  0.],\n          [ 0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.]]]])\n\n```\n\nyou can wrap this in a module to avoid recomputing the weight every time.",
      "y": "This works:\n```py\n>>> import torch.nn.functional as F\n>>>\n>>> def pad_within(x, stride=2):\n...   w = x.new_zeros(stride, stride)\n...   w[0, 0] = 1\n...   return F.conv_transpose2d(x, w.expand(x.size(1), 1, stride, stride), stride=stride, groups=x.size(1))\n...\n>>> x = torch.arange(8, dtype=torch.float).view(2, 4).expand(1, 3, 2, 4)\n>>> x\ntensor([[[[ 0.,  1.,  2.,  3.],\n          [ 4.,  5.,  6.,  7.]],\n\n         [[ 0.,  1.,  2.,  3.],\n          [ 4.,  5.,  6.,  7.]],\n\n         [[ 0.,  1.,  2.,  3.],\n          [ 4.,  5.,  6.,  7.]]]])\n>>> pad_within(x)\ntensor([[[[ 0.,  0.,  1.,  0.,  2.,  0.,  3.,  0.],\n          [ 0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.],\n          [ 4.,  0.,  5.,  0.,  6.,  0.,  7.,  0.],\n          [ 0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.]],\n\n         [[ 0.,  0.,  1.,  0.,  2.,  0.,  3.,  0.],\n          [ 0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.],\n          [ 4.,  0.,  5.,  0.,  6.,  0.,  7.,  0.],\n          [ 0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.]],\n\n         [[ 0.,  0.,  1.,  0.,  2.,  0.,  3.,  0.],\n          [ 0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.],\n          [ 4.,  0.,  5.,  0.,  6.,  0.,  7.,  0.],\n          [ 0.,  0.,  0.,  0.,  0.,  0.,  0.,  0.]]]])\n\n```\n\nyou can wrap this in a module to avoid recomputing the weight every time."
   },
   {
      "x": "Some confusion about the grad of torch.sign()",
      "z": "You should call `x_binary.retain_grad()`",
      "y": "You should call `x_binary.retain_grad()`"
   },
   {
      "x": "[feature request] Official CUDA support for macOS through eGPU",
      "z": "We do support macOS with CUDA GPUs. However atm you have to build from source.",
      "y": "We do support macOS with CUDA GPUs. However atm you have to build from source."
   },
   {
      "x": "Pytorch Model Summary",
      "z": "So something along the lines of:\n\n```\nfrom prettytable import PrettyTable\n\ndef count_parameters(model):\n    table = PrettyTable([\"Modules\", \"Parameters\"])\n    total_params = 0\n    for name, parameter in model.named_parameters():\n        if parameter.requires_grad:\n            param = parameter.numel()\n            table.add_row([name, param])\n            total_params+=param\n    print(table)\n    return f\"{total_params:,}\"\n    \ncount_parameters(net)\n```\n\nwhich outputs:\n\n```\n+---------------------+------------+\n|       Modules       | Parameters |\n+---------------------+------------+\n|  features.0.weight  |    1728    |\n|   features.0.bias   |     64     |\n|  features.3.weight  |   73728    |\n|   features.3.bias   |    128     |\n|  features.6.weight  |   294912   |\n|   features.6.bias   |    256     |\n|  features.8.weight  |   589824   |\n|   features.8.bias   |    256     |\n|  features.11.weight |  1179648   |\n|   features.11.bias  |    512     |\n|  features.13.weight |  2359296   |\n|   features.13.bias  |    512     |\n|  features.16.weight |  2359296   |\n|   features.16.bias  |    512     |\n|  features.18.weight |  2359296   |\n|   features.18.bias  |    512     |\n| classifier.0.weight | 102760448  |\n|  classifier.0.bias  |    4096    |\n| classifier.3.weight |  16777216  |\n|  classifier.3.bias  |    4096    |\n| classifier.6.weight |  4096000   |\n|  classifier.6.bias  |    1000    |\n+---------------------+------------+\n```\n\ncould be done.\n\nWell, this doesn't give shape after each layer.",
      "y": "So something along the lines of:\n\n```\nfrom prettytable import PrettyTable\n\ndef count_parameters(model):\n    table = PrettyTable([\"Modules\", \"Parameters\"])\n    total_params = 0\n    for name, parameter in model.named_parameters():\n        if parameter.requires_grad:\n            param = parameter.numel()\n            table.add_row([name, param])\n            total_params+=param\n    print(table)\n    return f\"{total_params:,}\"\n    \ncount_parameters(net)\n```\n\nwhich outputs:\n\n```\n+---------------------+------------+\n|       Modules       | Parameters |\n+---------------------+------------+\n|  features.0.weight  |    1728    |\n|   features.0.bias   |     64     |\n|  features.3.weight  |   73728    |\n|   features.3.bias   |    128     |\n|  features.6.weight  |   294912   |\n|   features.6.bias   |    256     |\n|  features.8.weight  |   589824   |\n|   features.8.bias   |    256     |\n|  features.11.weight |  1179648   |\n|   features.11.bias  |    512     |\n|  features.13.weight |  2359296   |\n|   features.13.bias  |    512     |\n|  features.16.weight |  2359296   |\n|   features.16.bias  |    512     |\n|  features.18.weight |  2359296   |\n|   features.18.bias  |    512     |\n| classifier.0.weight | 102760448  |\n|  classifier.0.bias  |    4096    |\n| classifier.3.weight |  16777216  |\n|  classifier.3.bias  |    4096    |\n| classifier.6.weight |  4096000   |\n|  classifier.6.bias  |    1000    |\n+---------------------+------------+\n```\n\ncould be done.\n\nWell, this doesn't give shape after each layer."
   },
   {
      "x": "register_full_backward_hook does not consistently fire",
      "z": "The temporary fix you can use is to make the input require gradients.\n\n",
      "y": "The temporary fix you can use is to make the input require gradients.\n\n"
   },
   {
      "x": "[docs] Rendering type hint issues at torchvision",
      "z": "Hello @vadimkantorov \nThis was issued due to Sphinx 3.x. This has been fixed over torchvision master by downgrading Sphinx to 2.4.4\nDocumentations look are fine now over the webpages for master as well as stable!\n\n@vadimkantorov rendering is not unified, for each domain library text, vision and audio docs are built and deployed over GitHub pages from their respective repos!\n\nFeel free to close this as it is fixed!",
      "y": "Hello @vadimkantorov \nThis was issued due to Sphinx 3.x. This has been fixed over torchvision master by downgrading Sphinx to 2.4.4\nDocumentations look are fine now over the webpages for master as well as stable!\n\n@vadimkantorov rendering is not unified, for each domain library text, vision and audio docs are built and deployed over GitHub pages from their respective repos!\n\nFeel free to close this as it is fixed!"
   },
   {
      "x": "Fail to LOADING A TORCHSCRIPT MODEL IN C++",
      "z": "> Here is the other one: https://download.pytorch.org/libtorch/cpu/libtorch-shared-with-deps-1.8.1%2Bcpu.zip. Can you try using this? I used this one and the tutorial is working as expected.\n\nThank you very much! I have succeed to build the example use the libtorch: \n downloaded from `https://download.pytorch.org/libtorch/cpu/libtorch-shared-with-deps-1.8.1%2Bcpu.zip`.",
      "y": "> Here is the other one: https://download.pytorch.org/libtorch/cpu/libtorch-shared-with-deps-1.8.1%2Bcpu.zip. Can you try using this? I used this one and the tutorial is working as expected.\n\nThank you very much! I have succeed to build the example use the libtorch: \n downloaded from `https://download.pytorch.org/libtorch/cpu/libtorch-shared-with-deps-1.8.1%2Bcpu.zip`."
   },
   {
      "x": "Create a context manager to enable InferenceMode in python frontend",
      "z": "You can already use `torch.is_grad_enabled()` (or the corresponding c++ API) to see if you are in a context where you might end up creating order + 1 derivative.",
      "y": "You can already use `torch.is_grad_enabled()` (or the corresponding c++ API) to see if you are in a context where you might end up creating order + 1 derivative."
   },
   {
      "x": "Cleaner mechanism in the source-code to check if multiple tensors are on the same device",
      "z": "We should avoid wrapping Tensor arguments in TensorArg for perf reasons (this bumps ref count), and all the mentioned functions act on TensorArgs. ",
      "y": "We should avoid wrapping Tensor arguments in TensorArg for perf reasons (this bumps ref count), and all the mentioned functions act on TensorArgs. "
   },
   {
      "x": "Incorrect example output in sparse_csr_tensor doc-string",
      "z": "We would definitely accept PRs correcting examples to reflect the current version of PyTorch!",
      "y": "We would definitely accept PRs correcting examples to reflect the current version of PyTorch!"
   },
   {
      "x": "sparse_csr_tensor segfaults when crow_indices or col_indices are non-tensors",
      "z": "This issue waqs solved here: https://github.com/pytorch/pytorch/pull/59010",
      "y": "This issue waqs solved here: https://github.com/pytorch/pytorch/pull/59010"
   },
   {
      "x": "default_pg_timeout in torch/testing/_internal/distributed/distributed_test.py is not sufficient to change system-wide timeouts",
      "z": "@ezyang You are running into the RPC timeout which is different from the process group timeout. The default timeout for RPC is set here: https://github.com/pytorch/pytorch/blob/d83ae5d1b74df1ad6e2b80652392ce0c2b31f3f3/torch/csrc/distributed/rpc/init.cpp#L80. \n\nYou can also modify the rpc timeout by specifying it in rpc_backen_options.rpc_timeout as part of the init_rpc call: https://github.com/pytorch/pytorch/blob/master/torch/testing/_internal/dist_utils.py#L83",
      "y": "@ezyang You are running into the RPC timeout which is different from the process group timeout. The default timeout for RPC is set here: https://github.com/pytorch/pytorch/blob/d83ae5d1b74df1ad6e2b80652392ce0c2b31f3f3/torch/csrc/distributed/rpc/init.cpp#L80. \n\nYou can also modify the rpc timeout by specifying it in rpc_backen_options.rpc_timeout as part of the init_rpc call: https://github.com/pytorch/pytorch/blob/master/torch/testing/_internal/dist_utils.py#L83"
   },
   {
      "x": "quick-check didn't capture error until PR landed",
      "z": "Just checked; the reason is because the Facebook-internal diff [D27774396](https://www.internalfb.com/diff/D27774396) did not match the GitHub pull request.\n\nYou can also see this by comparing the PR diff...\n\n- https://github.com/pytorch/pytorch/pull/55992/files#diff-e7602003c853555c44d805885670cf3500aefe9b8cfec7f8571e2d11f57fd56dR36\n\n... to the commit that landed to `master`:\n\n- https://github.com/pytorch/pytorch/commit/0df239e55082ab947e07161468b61f324eb6bed5#diff-e7602003c853555c44d805885670cf3500aefe9b8cfec7f8571e2d11f57fd56dR36",
      "y": "Just checked; the reason is because the Facebook-internal diff [D27774396](https://www.internalfb.com/diff/D27774396) did not match the GitHub pull request.\n\nYou can also see this by comparing the PR diff...\n\n- https://github.com/pytorch/pytorch/pull/55992/files#diff-e7602003c853555c44d805885670cf3500aefe9b8cfec7f8571e2d11f57fd56dR36\n\n... to the commit that landed to `master`:\n\n- https://github.com/pytorch/pytorch/commit/0df239e55082ab947e07161468b61f324eb6bed5#diff-e7602003c853555c44d805885670cf3500aefe9b8cfec7f8571e2d11f57fd56dR36"
   },
   {
      "x": "Element-wise max of two Tensors computes the wrong gradient in case of equality",
      "z": "Looks like the [`torch.amax()`]() docs are aware of this behavior:\n```\namax/amin evenly distributes gradient between equal values, while max(dim)/min(dim) propagates gradient only to a single index in the source tensor.\n```",
      "y": "Looks like the [`torch.amax()`]() docs are aware of this behavior:\n```\namax/amin evenly distributes gradient between equal values, while max(dim)/min(dim) propagates gradient only to a single index in the source tensor.\n```"
   },
   {
      "x": "[NNC] Vectorization caused wrong results",
      "z": "Fixed by #59423",
      "y": "Fixed by #59423"
   },
   {
      "x": "torch.gather behavior changed from 1.5.1 to master",
      "z": "Fixed by #41672.",
      "y": "Fixed by #41672."
   },
   {
      "x": "Is STFT in torchaudio consistent with librosa?",
      "z": "As far as I know, `stft` was not part of `torchaudio`. (@vincentqb can clarify on this one)\n\n`istft` was originally `torchaudio`, but recently moved to `pytorch`. \n\nSee https://github.com/pytorch/pytorch/issues/3775 for the development around FT.\n\nWe check that `istft` is inverse of `torch.stft`, (see [the tests here](https://github.com/pytorch/audio/blob/master/test/functional_cpu_test.py) the same tests have been ported to PyTorch main repo.) but we do not check the parity against `librosa`.\n\n@LordOfLuck\n\n> consistent with librosa implementations?\n\nIf you can give the configuration you are considering to use, we can provide you better insight. In general, I do not expect PyTorch's `stft` to match `librosa` for all the possible combination of parameters. (at least I did not see a test torch check the parity of `stft`).",
      "y": "As far as I know, `stft` was not part of `torchaudio`. (@vincentqb can clarify on this one)\n\n`istft` was originally `torchaudio`, but recently moved to `pytorch`. \n\nSee https://github.com/pytorch/pytorch/issues/3775 for the development around FT.\n\nWe check that `istft` is inverse of `torch.stft`, (see [the tests here](https://github.com/pytorch/audio/blob/master/test/functional_cpu_test.py) the same tests have been ported to PyTorch main repo.) but we do not check the parity against `librosa`.\n\n@LordOfLuck\n\n> consistent with librosa implementations?\n\nIf you can give the configuration you are considering to use, we can provide you better insight. In general, I do not expect PyTorch's `stft` to match `librosa` for all the possible combination of parameters. (at least I did not see a test torch check the parity of `stft`)."
   },
   {
      "x": "quantization.fuse_modules fails with Conv1d and BatchNorm1d",
      "z": "Here (https://github.com/pytorch/pytorch/blob/0c77bd7c0bbd4d6e50a5f3ce7b4debbee85d7963/torch/quantization/fuse_modules.py#L106) is a list of modules which can be fused.  LeakyRELU fusion does not work because we don't have a fusion implemented for it.",
      "y": "Here (https://github.com/pytorch/pytorch/blob/0c77bd7c0bbd4d6e50a5f3ce7b4debbee85d7963/torch/quantization/fuse_modules.py#L106) is a list of modules which can be fused.  LeakyRELU fusion does not work because we don't have a fusion implemented for it."
   },
   {
      "x": "weird bug of torchscript: it thinks my python bool is a tensor but it's not",
      "z": "I think this is working as intended (other than the confusing error expr range issue).\n\nEMPTY_FLOAT is a mutable tensor outside of the scripted function, TorchScript wouldn't know how to deal with it. Think about the case where you save this model on disk and another program loads it back. This second program doesn't have access to the EMPTY_FLOAT object in the first program. Therefore TorchScript is being conservative here.\n\nWith that being said, I assume your intention is actually to use the value of EMPTY_TENSOR as constant. In that case, I would suggest adding EMPTY_TENSOR as another functional argument.\n\n@suo, any better suggestions?",
      "y": "I think this is working as intended (other than the confusing error expr range issue).\n\nEMPTY_FLOAT is a mutable tensor outside of the scripted function, TorchScript wouldn't know how to deal with it. Think about the case where you save this model on disk and another program loads it back. This second program doesn't have access to the EMPTY_FLOAT object in the first program. Therefore TorchScript is being conservative here.\n\nWith that being said, I assume your intention is actually to use the value of EMPTY_TENSOR as constant. In that case, I would suggest adding EMPTY_TENSOR as another functional argument.\n\n@suo, any better suggestions?"
   },
   {
      "x": "Add SpectralOps CPU implementation for ARM/PowerPC processors (where MKL is not available)",
      "z": "Thanks @malfet. I was able to compute fft on ARM by using CUDA device on waveform:\n```\nimport torchaudio\nimport torch\n\nwaveform, sample_rate = torchaudio.load('test.wav')\nwaveform = waveform.to(\"cuda:0\")\n\nspectrogram = torchaudio.transforms.Spectrogram(sample_rate).to(\"cuda:0\")(waveform)\n```",
      "y": "Thanks @malfet. I was able to compute fft on ARM by using CUDA device on waveform:\n```\nimport torchaudio\nimport torch\n\nwaveform, sample_rate = torchaudio.load('test.wav')\nwaveform = waveform.to(\"cuda:0\")\n\nspectrogram = torchaudio.transforms.Spectrogram(sample_rate).to(\"cuda:0\")(waveform)\n```"
   },
   {
      "x": "cudnn8 version check fails",
      "z": "These lines has already been changed to\n```cmake\n  # Get cuDNN version\n  if(EXISTS ${CUDNN_INCLUDE_PATH}/cudnn_version.h)\n    file(READ ${CUDNN_INCLUDE_PATH}/cudnn_version.h CUDNN_HEADER_CONTENTS)\n  else()\n    file(READ ${CUDNN_INCLUDE_PATH}/cudnn.h CUDNN_HEADER_CONTENTS)\n  endif()\n```\nplease update to latest master branch",
      "y": "These lines has already been changed to\n```cmake\n  # Get cuDNN version\n  if(EXISTS ${CUDNN_INCLUDE_PATH}/cudnn_version.h)\n    file(READ ${CUDNN_INCLUDE_PATH}/cudnn_version.h CUDNN_HEADER_CONTENTS)\n  else()\n    file(READ ${CUDNN_INCLUDE_PATH}/cudnn.h CUDNN_HEADER_CONTENTS)\n  endif()\n```\nplease update to latest master branch"
   },
   {
      "x": "AssertionError: Torch not compiled with CUDA enabled - DETECTRON CPU/LINUX TRAINING ERROR",
      "z": "@svideloc have you reported this to detectron2?  This looks like a detectron2 issue to me, not a pytorch issue.\n\nIf you find that that is not the case, feel free to reopen this issue.",
      "y": "@svideloc have you reported this to detectron2?  This looks like a detectron2 issue to me, not a pytorch issue.\n\nIf you find that that is not the case, feel free to reopen this issue."
   },
   {
      "x": "torch.distributed and RPC cannot both be initialized with the same host:port pair.",
      "z": "@froody and @blefaudeux brought up a good point that, it might be too much hurdle for users to initialize twice (c10d and RPC), it might be easier to use if we support initializing an RPC gang using an initialized ProcessGroup. \n\nI recall we discussed the possibility to have a new `init_rpc` API that takes a Store instead of `rank`, `world_size`, `init_method` etc. Can this be one step further for solving this issue? E.g., we let `ProcessGroup` expose an API to return its store, and then init RPC using that Store. \n\nThere are different options to implement this behavior.\n\nOption 1: As the c10d gang is stable with fixed ranks, the derived RPC gang can also stay that way and share the same rank/id with the ProcessGroup instance. \nOption 2: Let the RPC stay decoupled from the c10d ProcessGroup, and still allows dynamic join/leave. In this case, we cannot match the rank/id between ProcessGroup and RPC agents any more. \n\n",
      "y": "@froody and @blefaudeux brought up a good point that, it might be too much hurdle for users to initialize twice (c10d and RPC), it might be easier to use if we support initializing an RPC gang using an initialized ProcessGroup. \n\nI recall we discussed the possibility to have a new `init_rpc` API that takes a Store instead of `rank`, `world_size`, `init_method` etc. Can this be one step further for solving this issue? E.g., we let `ProcessGroup` expose an API to return its store, and then init RPC using that Store. \n\nThere are different options to implement this behavior.\n\nOption 1: As the c10d gang is stable with fixed ranks, the derived RPC gang can also stay that way and share the same rank/id with the ProcessGroup instance. \nOption 2: Let the RPC stay decoupled from the c10d ProcessGroup, and still allows dynamic join/leave. In this case, we cannot match the rank/id between ProcessGroup and RPC agents any more. \n\n"
   },
   {
      "x": "whitelist keyword to quantization.prepare is implemented incorrectly",
      "z": "the default argument is changed to None in https://github.com/pytorch/pytorch/pull/42576",
      "y": "the default argument is changed to None in https://github.com/pytorch/pytorch/pull/42576"
   },
   {
      "x": "Feature request: torch.isclose should set default atol and rtol based on the dtype of the tensors it's given",
      "z": "Basically if you do any arithmetic on `float32`, it will rarely hit that `1e-8` atol tolerance, so the defaults are almost useless for the default type (unlike `np.isclose` where the default type is usually float64). ",
      "y": "Basically if you do any arithmetic on `float32`, it will rarely hit that `1e-8` atol tolerance, so the defaults are almost useless for the default type (unlike `np.isclose` where the default type is usually float64). "
   },
   {
      "x": "torch.nn.functional.grid_sample segfaults on large inputs",
      "z": "@erdmann Thank you for your perspective, it's fascinating! \nIt's true that as resolution goes up, images become bigger. But in practical terms \n1) GPU memory is usually not big enough to hold multiple large tensors \n2) Even if grid_sample was fixed to properly handle 64-bit indices on the GPU, typically you'd want to call a convolution next, and it won't work because of cudnn limitation. \nThat said, binary size increase from fixing grid_sample will probably be pretty small, so if we can, we should just do it. ",
      "y": "@erdmann Thank you for your perspective, it's fascinating! \nIt's true that as resolution goes up, images become bigger. But in practical terms \n1) GPU memory is usually not big enough to hold multiple large tensors \n2) Even if grid_sample was fixed to properly handle 64-bit indices on the GPU, typically you'd want to call a convolution next, and it won't work because of cudnn limitation. \nThat said, binary size increase from fixing grid_sample will probably be pretty small, so if we can, we should just do it. "
   },
   {
      "x": "CUDA not found when using latest pre-built version (Libtorch 1.5.1  - CUDA 10.1 - Cudnn7.6.4 - VS2017)",
      "z": "Please add the linker option `-INCLUDE:?warp_size@cuda@at@@YAHXZ`.",
      "y": "Please add the linker option `-INCLUDE:?warp_size@cuda@at@@YAHXZ`."
   },
   {
      "x": "Parameter `timeout` in torch.distributed.init_process_group cannot work",
      "z": "@worsecoder the timeout does not include initial socket connection establishment. After initializing process group, the time out will be applied to NCCL collective APIs such as all reduce as etc. \n\nFor your need, do you want to try to use torchelastic for failure handling?",
      "y": "@worsecoder the timeout does not include initial socket connection establishment. After initializing process group, the time out will be applied to NCCL collective APIs such as all reduce as etc. \n\nFor your need, do you want to try to use torchelastic for failure handling?"
   },
   {
      "x": "Provide a convenient way for the user to reset the grad",
      "z": "Having both zero_grad() and reset_grad() might be kind of confusing to the end users. \n\nAre zero-ing all gradients always equivalent to setting all gradients to None? Wonder if there's any case where zero_grad() is preferred over reset_grad().\n\nIf the two are not equivalent, and have pros and cons, we probably should mention those pros and cons in the documentations to articulate the difference in order to prevent confusion. We should probably have some default recommendation (e.g. do we think reset_grad() will be better in most cases, which looks like the case, because most references I found are just calling .backward() right after calling zero_grad()).",
      "y": "Having both zero_grad() and reset_grad() might be kind of confusing to the end users. \n\nAre zero-ing all gradients always equivalent to setting all gradients to None? Wonder if there's any case where zero_grad() is preferred over reset_grad().\n\nIf the two are not equivalent, and have pros and cons, we probably should mention those pros and cons in the documentations to articulate the difference in order to prevent confusion. We should probably have some default recommendation (e.g. do we think reset_grad() will be better in most cases, which looks like the case, because most references I found are just calling .backward() right after calling zero_grad())."
   },
   {
      "x": "Replace blacklist/whitelist in caffe2/opt/tvm_transformer.cc",
      "z": "Simply replacing the names does not help. Assistance is needed.",
      "y": "Simply replacing the names does not help. Assistance is needed."
   },
   {
      "x": "Behavior of torch.mean (and std) compared to numpy mean (std)",
      "z": "A WIP implementation can be found in https://github.com/pytorch/pytorch/pull/2116\nBut it needs someone to complete it.",
      "y": "A WIP implementation can be found in https://github.com/pytorch/pytorch/pull/2116\nBut it needs someone to complete it."
   },
   {
      "x": "Easy way of creating your own custom cuda kernels",
      "z": "we also provide examples of easily interfacing with CUDA code in https://github.com/pytorch/extension-ffi where you dont have to write additional bindings",
      "y": "we also provide examples of easily interfacing with CUDA code in https://github.com/pytorch/extension-ffi where you dont have to write additional bindings"
   },
   {
      "x": "HalfTensor Training Needs non-Stateless Method in F.Linear",
      "z": "If it has an mm method then we should enable `torch.mm` for it too. It should be a one line change in cwrap.",
      "y": "If it has an mm method then we should enable `torch.mm` for it too. It should be a one line change in cwrap."
   },
   {
      "x": "Just one cpu core in use, until I use numpy...",
      "z": "Sometimes mkl caps the number of threads below your system's max. See mkl's cap via `mkl.get_max_threads()`. To increase the cap to your actual max, use `export MKL_DYNAMIC=FALSE` in bash before running Python. Find your actual max via `echo $(nproc)` in bash.\n\nIntel's documentation:\nhttps://software.intel.com/en-us/mkl-linux-developer-guide-mkl-dynamic\n\nI came across this solution in this issue:\nhttps://github.com/ContinuumIO/mkl-service/issues/2\n",
      "y": "Sometimes mkl caps the number of threads below your system's max. See mkl's cap via `mkl.get_max_threads()`. To increase the cap to your actual max, use `export MKL_DYNAMIC=FALSE` in bash before running Python. Find your actual max via `echo $(nproc)` in bash.\n\nIntel's documentation:\nhttps://software.intel.com/en-us/mkl-linux-developer-guide-mkl-dynamic\n\nI came across this solution in this issue:\nhttps://github.com/ContinuumIO/mkl-service/issues/2\n"
   },
   {
      "x": "Support Caffe2 export/pure C(++) inference mode",
      "z": "If I were to suggest something, I think I'd convert all the weights to numpy and dump them in a well established format like HDF5. These should be easily readable from C.\n\nOur main idea is to add a model exporter, that could dump it to a Caffe2 graph, as that framework is very well optimized for production usage. And even though we don't plan to put a great amount of work on packaging production-ready models trained in PyTorch right now, there are some projects in the community that are aiming to do that, and we're going to provide them with support. You can find them in the forum link that @fmassa posted.",
      "y": "If I were to suggest something, I think I'd convert all the weights to numpy and dump them in a well established format like HDF5. These should be easily readable from C.\n\nOur main idea is to add a model exporter, that could dump it to a Caffe2 graph, as that framework is very well optimized for production usage. And even though we don't plan to put a great amount of work on packaging production-ready models trained in PyTorch right now, there are some projects in the community that are aiming to do that, and we're going to provide them with support. You can find them in the forum link that @fmassa posted."
   },
   {
      "x": "GPU torch.multinomial produces an out-of-bounds index",
      "z": "In case this is helpful to anyone, a possible temporary workaround is to use\n```\n_, sample = torch.max(log_dist - torch.log(-torch.log(torch.rand(*log_dist.size()).cuda())), 1)\n```\nwhere `log_dist` is batchwise log probabilities (e.g. output of `F.log_softmax`).",
      "y": "In case this is helpful to anyone, a possible temporary workaround is to use\n```\n_, sample = torch.max(log_dist - torch.log(-torch.log(torch.rand(*log_dist.size()).cuda())), 1)\n```\nwhere `log_dist` is batchwise log probabilities (e.g. output of `F.log_softmax`)."
   },
   {
      "x": "Flexible state dict loading for model (or optimizer)",
      "z": "If you have partial state_dict, which is missing some keys you can do the following:\n\n```python\nstate = model.state_dict()\nstate.update(partial)\nmodel.load_state_dict(state)\n```",
      "y": "If you have partial state_dict, which is missing some keys you can do the following:\n\n```python\nstate = model.state_dict()\nstate.update(partial)\nmodel.load_state_dict(state)\n```"
   },
   {
      "x": "Support int16 numpy conversions",
      "z": "The error message is quite self explanatory. PyTorch doesn't support `int8` tensors at the moment.",
      "y": "The error message is quite self explanatory. PyTorch doesn't support `int8` tensors at the moment."
   },
   {
      "x": "The command '/bin/sh -c' returned a non-zero code: 2 during docker image",
      "z": "As you can see [here](https://github.com/pytorch/pytorch/blob/f2d72ba10fabe6f78e67246031c8b1da48e7ddf1/Dockerfile#L13-L19) the WHOLE command following `RUN` has to be a single line. You should add `\\` when necessary to avoid problem.\nYour sample:\n```\nRUN\nchmod +x ~/miniconda.sh && \n~/miniconda.sh -b -p /opt/conda && \\\nrm ~/miniconda.sh && \n/opt/conda/bin/conda install conda-build && \n/opt/conda/bin/conda create -y --name pytorch-py35 python=3.5.2 numpy scipy ipython mkl&& \n/opt/conda/bin/conda clean -ya\n```\nshould be\n```\nRUN chmod +x ~/miniconda.sh && \\\n~/miniconda.sh -b -p /opt/conda && \\\nrm ~/miniconda.sh && \\\n/opt/conda/bin/conda install conda-build && \\\n/opt/conda/bin/conda create -y --name pytorch-py35 python=3.5.2 numpy scipy ipython mkl && \\\n/opt/conda/bin/conda clean -ya\n```",
      "y": "As you can see [here](https://github.com/pytorch/pytorch/blob/f2d72ba10fabe6f78e67246031c8b1da48e7ddf1/Dockerfile#L13-L19) the WHOLE command following `RUN` has to be a single line. You should add `\\` when necessary to avoid problem.\nYour sample:\n```\nRUN\nchmod +x ~/miniconda.sh && \n~/miniconda.sh -b -p /opt/conda && \\\nrm ~/miniconda.sh && \n/opt/conda/bin/conda install conda-build && \n/opt/conda/bin/conda create -y --name pytorch-py35 python=3.5.2 numpy scipy ipython mkl&& \n/opt/conda/bin/conda clean -ya\n```\nshould be\n```\nRUN chmod +x ~/miniconda.sh && \\\n~/miniconda.sh -b -p /opt/conda && \\\nrm ~/miniconda.sh && \\\n/opt/conda/bin/conda install conda-build && \\\n/opt/conda/bin/conda create -y --name pytorch-py35 python=3.5.2 numpy scipy ipython mkl && \\\n/opt/conda/bin/conda clean -ya\n```"
   },
   {
      "x": "old lua torch model to pytorch, nn.JoinTable(1,3) error",
      "z": "Because the forward looks like that `def forward(self, input):` so it expects two arguments, but it gets self, Z and Y from you. `nn.Sequential` only accepts a single input.",
      "y": "Because the forward looks like that `def forward(self, input):` so it expects two arguments, but it gets self, Z and Y from you. `nn.Sequential` only accepts a single input."
   },
   {
      "x": "torch ModuleNotFoundError in ipython",
      "z": "1. try to uninstall your installed pytorch.\n2. conda install -c peterjc123 pytorch-cpu\n3. on your conda type python\n4. import torch ",
      "y": "1. try to uninstall your installed pytorch.\n2. conda install -c peterjc123 pytorch-cpu\n3. on your conda type python\n4. import torch "
   },
   {
      "x": "Device memory not released",
      "z": "I've seen this before. If the main process segfaults, then the background data loading processes might still be running and holding the cuda context.\n\nRun:\n`killall python` (or `killall python3`)",
      "y": "I've seen this before. If the main process segfaults, then the background data loading processes might still be running and holding the cuda context.\n\nRun:\n`killall python` (or `killall python3`)"
   },
   {
      "x": "Manually unrolling cuDNN RNN OOM",
      "z": "First one is not using cudnn, second one is. Both are unrolled in the same way. You can switch between them by commenting/uncommenting the following line in the script:\n```\ntorch.backends.cudnn.enabled=False\n```",
      "y": "First one is not using cudnn, second one is. Both are unrolled in the same way. You can switch between them by commenting/uncommenting the following line in the script:\n```\ntorch.backends.cudnn.enabled=False\n```"
   },
   {
      "x": "padding for nn.AvgPool3d?",
      "z": "there isn't a particular reason, we haven't implemented it yet in our C backend.",
      "y": "there isn't a particular reason, we haven't implemented it yet in our C backend."
   },
   {
      "x": "Compiling error of gloo when installing pytorch from source",
      "z": "I just added `#define SPEED_UNKNOWN -1` to the file `torch/lib/gloo/gloo/common/linux.cc`.  It appears that this is suppose to be defined by one of the headers from the linux kernel, but I am guessing isn't defined for older versions.",
      "y": "I just added `#define SPEED_UNKNOWN -1` to the file `torch/lib/gloo/gloo/common/linux.cc`.  It appears that this is suppose to be defined by one of the headers from the linux kernel, but I am guessing isn't defined for older versions."
   },
   {
      "x": "RuntimeError: DataLoader worker (pid 23616) is killed by signal: Terminated.",
      "z": "I run into the same problem as @shirishr. It was simply because of not enough memory, you may add more memory / swap space to solve the problem. ",
      "y": "I run into the same problem as @shirishr. It was simply because of not enough memory, you may add more memory / swap space to solve the problem. "
   },
   {
      "x": "from torch._C import *  (ImportError: DLL load failed: The specified module could not be found.",
      "z": "I had the same issue and it was caused by the directory torch which is generated in the same directory by compiling the source. The solution for me was simply changing the directory before open python.\n  ",
      "y": "I had the same issue and it was caused by the directory torch which is generated in the same directory by compiling the source. The solution for me was simply changing the directory before open python.\n  "
   },
   {
      "x": "nn.BatchNorm1d fails with batch size 1 on the new PyTorch 0.3",
      "z": "Like the error message says, you can't use feature-wise batch normalization if you only have 1 element per-feature.\n\nhttps://arxiv.org/abs/1502.03167",
      "y": "Like the error message says, you can't use feature-wise batch normalization if you only have 1 element per-feature.\n\nhttps://arxiv.org/abs/1502.03167"
   },
   {
      "x": "NVIDIA driver too old error",
      "z": "I solved it by downgrading the pytorch. That is less cumbersome compared to updating the driver. \nI followed `https://pytorch.org/get-started/previous-versions/` for compatible pytorch version with cuda toolkit",
      "y": "I solved it by downgrading the pytorch. That is less cumbersome compared to updating the driver. \nI followed `https://pytorch.org/get-started/previous-versions/` for compatible pytorch version with cuda toolkit"
   },
   {
      "x": "Feature request: torch.bincount",
      "z": "@sunshineatnoon, how about:\n```python\n>>> import torch\n>>> help(torch.bincount)\n```\n```rst\nHelp on built-in function bincount:\n\nbincount(...)\n    bincount(self, weights=None, minlength=0) -> Tensor\n\n    Count the frequency of each value in an array of non-negative ints.\n\n    The number of bins (size 1) is one larger than the largest value in\n    :attr:`input`. If :attr:`minlength` is specified, the number of bins is at least\n    :attr:`minlength`. If ``n`` is the value at position ``i``,\n    :math:`out[n] += weights[i]` if :attr:`weights` is specified else\n    :math:`out[n] += 1`.\n\n    Arguments:\n        input (Tensor): 1-d int tensor\n        weights (Tensor): optional, weight for each value in the input tensor.\n            Should be of same size as input tensor.\n        minlength (int): optional, min number of bins. Should be non-negative.\n\n    Shape:\n        output (Tensor): ``Size([max(input) + 1])``\n\n    Example::\n\n        >>> input = torch.randint(0, 8, (5,), dtype=torch.int64)\n        >>> weights = torch.linspace(0, 1, steps=5)\n        >>> input, weights\n        (tensor([4, 3, 6, 3, 4]),\n         tensor([ 0.0000,  0.2500,  0.5000,  0.7500,  1.0000])\n\n        >>> torch.bincount(input)\n        tensor([0, 0, 0, 2, 2, 0, 1])\n\n        >>> input.bincount(weights)\n        tensor([0.0000, 0.0000, 0.0000, 1.0000, 1.0000, 0.0000, 0.5000])\n```",
      "y": "@sunshineatnoon, how about:\n```python\n>>> import torch\n>>> help(torch.bincount)\n```\n```rst\nHelp on built-in function bincount:\n\nbincount(...)\n    bincount(self, weights=None, minlength=0) -> Tensor\n\n    Count the frequency of each value in an array of non-negative ints.\n\n    The number of bins (size 1) is one larger than the largest value in\n    :attr:`input`. If :attr:`minlength` is specified, the number of bins is at least\n    :attr:`minlength`. If ``n`` is the value at position ``i``,\n    :math:`out[n] += weights[i]` if :attr:`weights` is specified else\n    :math:`out[n] += 1`.\n\n    Arguments:\n        input (Tensor): 1-d int tensor\n        weights (Tensor): optional, weight for each value in the input tensor.\n            Should be of same size as input tensor.\n        minlength (int): optional, min number of bins. Should be non-negative.\n\n    Shape:\n        output (Tensor): ``Size([max(input) + 1])``\n\n    Example::\n\n        >>> input = torch.randint(0, 8, (5,), dtype=torch.int64)\n        >>> weights = torch.linspace(0, 1, steps=5)\n        >>> input, weights\n        (tensor([4, 3, 6, 3, 4]),\n         tensor([ 0.0000,  0.2500,  0.5000,  0.7500,  1.0000])\n\n        >>> torch.bincount(input)\n        tensor([0, 0, 0, 2, 2, 0, 1])\n\n        >>> input.bincount(weights)\n        tensor([0.0000, 0.0000, 0.0000, 1.0000, 1.0000, 0.0000, 0.5000])\n```"
   },
   {
      "x": "[feature request] Weight norm option for RNN cells",
      "z": "You can already achieve it now. For instance,\n```\n>>> rnn = nn.RNN(10,10,2)  # build an RNN\n>>> nn.utils.weight_norm(rnn, 'weight_hh_l0')  # apply weight_norm to any particular weight you want\n```",
      "y": "You can already achieve it now. For instance,\n```\n>>> rnn = nn.RNN(10,10,2)  # build an RNN\n>>> nn.utils.weight_norm(rnn, 'weight_hh_l0')  # apply weight_norm to any particular weight you want\n```"
   },
   {
      "x": "[Feature request] PackedSequence with length = 0",
      "z": "I don't think that's a good idea. If you know it can happen in your use case just filter out the empty sequences before you call this function. It would make it more convenient for you, but may silently accept inputs that are incorrect for other uses.",
      "y": "I don't think that's a good idea. If you know it can happen in your use case just filter out the empty sequences before you call this function. It would make it more convenient for you, but may silently accept inputs that are incorrect for other uses."
   },
   {
      "x": "Cannot install pytorch cuda 8.0 using conda",
      "z": "do you already have the cuda90 feature installed, for some reason...\n\nTry first doing:\n\n`conda uninstall cuda90`",
      "y": "do you already have the cuda90 feature installed, for some reason...\n\nTry first doing:\n\n`conda uninstall cuda90`"
   },
   {
      "x": "Warning on infinite acos gradients?",
      "z": "A lot of our ops work this way, and adding these checks would cause massive slowdowns. Thanks for the suggestion, but we can't afford that.",
      "y": "A lot of our ops work this way, and adding these checks would cause massive slowdowns. Thanks for the suggestion, but we can't afford that."
   },
   {
      "x": "Autogradpp issue masterthread",
      "z": "Regarding `tensor.max()` I think a better way would be to auto-generate \"named-tuples\" in C++ (i.e. you can still do `std::get<0>(tensor.max())` OR `tensor.max().values` (we'd generate a struct with two fields that also supports the `get` interface)",
      "y": "Regarding `tensor.max()` I think a better way would be to auto-generate \"named-tuples\" in C++ (i.e. you can still do `std::get<0>(tensor.max())` OR `tensor.max().values` (we'd generate a struct with two fields that also supports the `get` interface)"
   },
   {
      "x": "`torch.normal` accepts Variables but does not propagate gradients",
      "z": "if you are on master, you can do `x = torch.distributions.Normal(mu, sigma).rsample()`",
      "y": "if you are on master, you can do `x = torch.distributions.Normal(mu, sigma).rsample()`"
   },
   {
      "x": "`torch.jit.freeze`'d models cannot be moved to GPU with `.to()`",
      "z": "@Linux-cpp-lisp this is currently expected behavior. We need to be able to inline the attributes as constants in order to do anything useful in optimizing them. There is also nothing preventing the user from having device-specific logic we also bake in.\n\n```\n    def forward(self, x):\n       if self.twos.device.is_cuda():\n              ....\n```\nModels might also contain some CPU & some GPU compute. However, as you've shown, there are many models where it is completely valid to remap devices after freezing. \n\nCan I ask what the specific use case is ?\n\n\n",
      "y": "@Linux-cpp-lisp this is currently expected behavior. We need to be able to inline the attributes as constants in order to do anything useful in optimizing them. There is also nothing preventing the user from having device-specific logic we also bake in.\n\n```\n    def forward(self, x):\n       if self.twos.device.is_cuda():\n              ....\n```\nModels might also contain some CPU & some GPU compute. However, as you've shown, there are many models where it is completely valid to remap devices after freezing. \n\nCan I ask what the specific use case is ?\n\n\n"
   },
   {
      "x": "gradgradcheck fails if the function does not depend on the input",
      "z": "Still fails.\n\n    Traceback (most recent call last):\n      File \"<stdin>\", line 1, in <module>\n      File \"/home/mfkasim/anaconda3/envs/torchdev/lib/python3.8/site-packages/torch/autograd/gradcheck.py\", line 1212, in gradgradcheck\n        return gradcheck(\n      File \"/home/mfkasim/anaconda3/envs/torchdev/lib/python3.8/site-packages/torch/autograd/gradcheck.py\", line 1088, in gradcheck\n        return _gradcheck_helper(**args)\n      File \"/home/mfkasim/anaconda3/envs/torchdev/lib/python3.8/site-packages/torch/autograd/gradcheck.py\", line 1096, in _gradcheck_helper\n        func_out = func(*tupled_inputs)\n      File \"/home/mfkasim/anaconda3/envs/torchdev/lib/python3.8/site-packages/torch/autograd/gradcheck.py\", line 1209, in new_func\n        grad_inputs = torch.autograd.grad(outputs, input_args, grad_outputs, create_graph=True)\n      File \"/home/mfkasim/anaconda3/envs/torchdev/lib/python3.8/site-packages/torch/autograd/__init__.py\", line 226, in grad\n        return Variable._execution_engine.run_backward(\n    RuntimeError: One of the differentiated Tensors appears to not have been used in the graph. Set allow_unused=True if this is the desired behavior.\n\nThe problem is this line:\nhttps://github.com/pytorch/pytorch/blob/807bea1c4e6fdf896570f1fac83897cf42231f49/torch/autograd/gradcheck.py#L1209\n\nProbably the fix is as simple as put `allow_unused=True`",
      "y": "Still fails.\n\n    Traceback (most recent call last):\n      File \"<stdin>\", line 1, in <module>\n      File \"/home/mfkasim/anaconda3/envs/torchdev/lib/python3.8/site-packages/torch/autograd/gradcheck.py\", line 1212, in gradgradcheck\n        return gradcheck(\n      File \"/home/mfkasim/anaconda3/envs/torchdev/lib/python3.8/site-packages/torch/autograd/gradcheck.py\", line 1088, in gradcheck\n        return _gradcheck_helper(**args)\n      File \"/home/mfkasim/anaconda3/envs/torchdev/lib/python3.8/site-packages/torch/autograd/gradcheck.py\", line 1096, in _gradcheck_helper\n        func_out = func(*tupled_inputs)\n      File \"/home/mfkasim/anaconda3/envs/torchdev/lib/python3.8/site-packages/torch/autograd/gradcheck.py\", line 1209, in new_func\n        grad_inputs = torch.autograd.grad(outputs, input_args, grad_outputs, create_graph=True)\n      File \"/home/mfkasim/anaconda3/envs/torchdev/lib/python3.8/site-packages/torch/autograd/__init__.py\", line 226, in grad\n        return Variable._execution_engine.run_backward(\n    RuntimeError: One of the differentiated Tensors appears to not have been used in the graph. Set allow_unused=True if this is the desired behavior.\n\nThe problem is this line:\nhttps://github.com/pytorch/pytorch/blob/807bea1c4e6fdf896570f1fac83897cf42231f49/torch/autograd/gradcheck.py#L1209\n\nProbably the fix is as simple as put `allow_unused=True`"
   },
   {
      "x": "With & without -O2 compilation optimization level, AVX512 Complex multiplication & division results aren't equal",
      "z": "@quickwritereader, this is so embarrassing!!! I had wrongly assumed that `CPU_CAPABILITY_DEFAULT` is always defined, unless a user uses the environment variable `ATEN_CPU_CAPABILITY` to set it to a particular value! That's why I hadn't bothered to change that if clause because it seemed redundant to do so.\n\nSo, the issue was just that the testing for AVX512 was being done against the default implementation? \ud83e\udd23 \nThanks to you, all tests pass now! \ud83d\ude04 ",
      "y": "@quickwritereader, this is so embarrassing!!! I had wrongly assumed that `CPU_CAPABILITY_DEFAULT` is always defined, unless a user uses the environment variable `ATEN_CPU_CAPABILITY` to set it to a particular value! That's why I hadn't bothered to change that if clause because it seemed redundant to do so.\n\nSo, the issue was just that the testing for AVX512 was being done against the default implementation? \ud83e\udd23 \nThanks to you, all tests pass now! \ud83d\ude04 "
   },
   {
      "x": "Linking statically with CUPTI using gold linker disrupts exception handling",
      "z": "The CMake file change at https://github.com/pytorch/pytorch/commit/c71459602785bcc9f0f93a880c642ff61299d3d5#diff-12e8125164bbfc7556b1781a8ed516e333cc0bf058acb7197f7415be44606c72R1859-R1863 now actually honors `USE_CUPTI_SO`. This was previously not the case which meant building PyTorch \"correctly\" i.e. with dynamic cupti, was impossible and the issue unavoidable (we patched that for our builds)\n\nTo me linking to static cupti is the bug which is now fixed: Define USE_CUPTI_SO=1 and all works (testing this right now)\n\nBut yes the (likely) faulty cupti static library is not fixed by this but I think this is rather a cupti bug than a pytorch one. Not sure here, so please reopen if you still want to track this bug here.\nJust from my side I'm happy if USE_CUPTI_SO is now working.",
      "y": "The CMake file change at https://github.com/pytorch/pytorch/commit/c71459602785bcc9f0f93a880c642ff61299d3d5#diff-12e8125164bbfc7556b1781a8ed516e333cc0bf058acb7197f7415be44606c72R1859-R1863 now actually honors `USE_CUPTI_SO`. This was previously not the case which meant building PyTorch \"correctly\" i.e. with dynamic cupti, was impossible and the issue unavoidable (we patched that for our builds)\n\nTo me linking to static cupti is the bug which is now fixed: Define USE_CUPTI_SO=1 and all works (testing this right now)\n\nBut yes the (likely) faulty cupti static library is not fixed by this but I think this is rather a cupti bug than a pytorch one. Not sure here, so please reopen if you still want to track this bug here.\nJust from my side I'm happy if USE_CUPTI_SO is now working."
   },
   {
      "x": "Retiring ONNX Optimizer",
      "z": "Discussed this issue during triage review: let's disable optimizer support as it is no longer supported.",
      "y": "Discussed this issue during triage review: let's disable optimizer support as it is no longer supported."
   },
   {
      "x": "Cannot pass script remote module object to over the RPC",
      "z": "@SciPioneer I wonder if we can employ a method similar to https://github.com/pytorch/pytorch/blob/master/torch/distributed/rpc/internal.py#L110-L112, although I wonder why it isn't already supported out of the box. I think `RecursiveScriptModule` is an instance of `torch.jit.ScriptModule` so not sure why this codepath isn't hit. ",
      "y": "@SciPioneer I wonder if we can employ a method similar to https://github.com/pytorch/pytorch/blob/master/torch/distributed/rpc/internal.py#L110-L112, although I wonder why it isn't already supported out of the box. I think `RecursiveScriptModule` is an instance of `torch.jit.ScriptModule` so not sure why this codepath isn't hit. "
   },
   {
      "x": "torch.cat should not do type promotion when one input is empty tensor",
      "z": "NumPy type promotes in this case and I don't think it makes sense to complicate the type promotion rules even further:\n```\n>>> np.concatenate([np.zeros(5, dtype=np.long), np.array([])]).dtype\ndtype('float64')\n```",
      "y": "NumPy type promotes in this case and I don't think it makes sense to complicate the type promotion rules even further:\n```\n>>> np.concatenate([np.zeros(5, dtype=np.long), np.array([])]).dtype\ndtype('float64')\n```"
   },
   {
      "x": "A bug on the document interpreter in `torch.autograd.profiler` help page",
      "z": "Fixed on master https://pytorch.org/docs/master/generated/torch.autograd.profiler.profile.key_averages.html?highlight=key_averages#torch.autograd.profiler.profile.key_averages",
      "y": "Fixed on master https://pytorch.org/docs/master/generated/torch.autograd.profiler.profile.key_averages.html?highlight=key_averages#torch.autograd.profiler.profile.key_averages"
   },
   {
      "x": "`nan_to_num` produces incorrect output for `BFloat16` on CUDA",
      "z": "Ok, so it looks like `nan_to_num` is actually doing the correct thing, and it's the compare with reference that goes wrong. That's good to know. ",
      "y": "Ok, so it looks like `nan_to_num` is actually doing the correct thing, and it's the compare with reference that goes wrong. That's good to know. "
   },
   {
      "x": "torch.cat fails with torch.jit.script and torch.cuda.amp.autocast",
      "z": "At a wild guess: this patch should fix it:\n\n```\ndiff --git a/aten/src/ATen/autocast_mode.cpp b/aten/src/ATen/autocast_mode.cpp\nindex 7d85211e9c..7809b7d323 100644\n--- a/aten/src/ATen/autocast_mode.cpp\n+++ b/aten/src/ATen/autocast_mode.cpp\n@@ -487,9 +487,9 @@ TORCH_LIBRARY_IMPL(aten, Autocast, m) {\n   KERNEL_UNBOXED_ONLY(ADD_NS(tensordot), \"tensordot\", Tensor (const Tensor &, const Tensor &, IntArrayRef, IntArrayRef), promote)\n   KERNEL_UNBOXED_ONLY(ADD_NS(dot), \"dot\", Tensor (const Tensor &, const Tensor &), promote)\n   KERNEL(ADD_NS(equal), \"equal\", bool (const Tensor &, const Tensor &), promote)\n-  KERNEL_UNBOXED_ONLY(ADD_NS(cat), \"cat\", Tensor (TensorList, int64_t), promote)\n-  KERNEL_UNBOXED_ONLY(ADD_NS(cat), \"cat.names\", Tensor (TensorList, Dimname), promote)\n-  KERNEL_UNBOXED_ONLY(ADD_NS(_cat), \"_cat\", Tensor (TensorList, int64_t), promote)\n+  KERNEL(ADD_NS(cat), \"cat\", Tensor (TensorList, int64_t), promote)\n+  KERNEL(ADD_NS(cat), \"cat.names\", Tensor (TensorList, Dimname), promote)\n+  KERNEL(ADD_NS(_cat), \"_cat\", Tensor (TensorList, int64_t), promote)\n   KERNEL_UNBOXED_ONLY(ADD_NS(stack), \"stack\", Tensor (TensorList, int64_t), promote)\n \n   m.impl_UNBOXED(\"binary_cross_entropy\", &at::autocast::binary_cross_entropy_banned);\n```\n\n@smessmer not sure if we should just rush \"get rid of unboxed only\" or start adding some checks to reject incorrect invocations of unboxed only.",
      "y": "At a wild guess: this patch should fix it:\n\n```\ndiff --git a/aten/src/ATen/autocast_mode.cpp b/aten/src/ATen/autocast_mode.cpp\nindex 7d85211e9c..7809b7d323 100644\n--- a/aten/src/ATen/autocast_mode.cpp\n+++ b/aten/src/ATen/autocast_mode.cpp\n@@ -487,9 +487,9 @@ TORCH_LIBRARY_IMPL(aten, Autocast, m) {\n   KERNEL_UNBOXED_ONLY(ADD_NS(tensordot), \"tensordot\", Tensor (const Tensor &, const Tensor &, IntArrayRef, IntArrayRef), promote)\n   KERNEL_UNBOXED_ONLY(ADD_NS(dot), \"dot\", Tensor (const Tensor &, const Tensor &), promote)\n   KERNEL(ADD_NS(equal), \"equal\", bool (const Tensor &, const Tensor &), promote)\n-  KERNEL_UNBOXED_ONLY(ADD_NS(cat), \"cat\", Tensor (TensorList, int64_t), promote)\n-  KERNEL_UNBOXED_ONLY(ADD_NS(cat), \"cat.names\", Tensor (TensorList, Dimname), promote)\n-  KERNEL_UNBOXED_ONLY(ADD_NS(_cat), \"_cat\", Tensor (TensorList, int64_t), promote)\n+  KERNEL(ADD_NS(cat), \"cat\", Tensor (TensorList, int64_t), promote)\n+  KERNEL(ADD_NS(cat), \"cat.names\", Tensor (TensorList, Dimname), promote)\n+  KERNEL(ADD_NS(_cat), \"_cat\", Tensor (TensorList, int64_t), promote)\n   KERNEL_UNBOXED_ONLY(ADD_NS(stack), \"stack\", Tensor (TensorList, int64_t), promote)\n \n   m.impl_UNBOXED(\"binary_cross_entropy\", &at::autocast::binary_cross_entropy_banned);\n```\n\n@smessmer not sure if we should just rush \"get rid of unboxed only\" or start adding some checks to reject incorrect invocations of unboxed only."
   },
   {
      "x": "[JIT] lack of type support in tensor indexing.",
      "z": "Hi @chenbohua3, thanks for the detailed issue and succinct repro! This was recently added in https://github.com/pytorch/pytorch/pull/38378 and works on master, so I'm closing. ",
      "y": "Hi @chenbohua3, thanks for the detailed issue and succinct repro! This was recently added in https://github.com/pytorch/pytorch/pull/38378 and works on master, so I'm closing. "
   },
   {
      "x": "One PyTorch Upsample op balloons into over 20 ONNX operations",
      "z": "This is expected since by default, interpolate recomputes the given scale_factor using input size. If you try torch.nn.Functional with recompute_scale_factor = False, you'll see a single Resize node in the graph.\nThe default behavior of interpolate is going to change to set recompute_scale_factor = False by default,  as part of the PR: https://github.com/pytorch/pytorch/pull/38362",
      "y": "This is expected since by default, interpolate recomputes the given scale_factor using input size. If you try torch.nn.Functional with recompute_scale_factor = False, you'll see a single Resize node in the graph.\nThe default behavior of interpolate is going to change to set recompute_scale_factor = False by default,  as part of the PR: https://github.com/pytorch/pytorch/pull/38362"
   },
   {
      "x": "Unrecognized attribute: min for operator Clip",
      "z": "seems fixed and can't repro with `pytorch 1.6`",
      "y": "seems fixed and can't repro with `pytorch 1.6`"
   },
   {
      "x": "``ToTensor()`` Exception for ``num_workers`` in ``DataLoader`` when ``torch.set_default_tensor_type(torch.cuda.FloatTensor)``",
      "z": "the root cause is that `fork` inherits the current process state, including default tensor type, while `spawn` doesn't. this is a fundamental difference between the two start methods, and not really pytorch/torchvision specific. ",
      "y": "the root cause is that `fork` inherits the current process state, including default tensor type, while `spawn` doesn't. this is a fundamental difference between the two start methods, and not really pytorch/torchvision specific. "
   },
   {
      "x": "`verbose` unused in `torch.backends.cudnn`",
      "z": "I can fix it by removing this parameter.",
      "y": "I can fix it by removing this parameter."
   },
   {
      "x": "RuntimeError: one of the variables needed for gradient computation has been modified by an inplace operation?",
      "z": "Hi,\n\nThis happens because the `opt_D.step()` modifies the parameters of your discriminator inplace. But these parameters are required to compute the gradient for the generator. Hence the error.\nWe fixed the inplace detection for the optimizers in 1.5, this is why it works in 1.4.\n\nYou should re-organize your code to only do the `steps()` after all the gradients have been computed or make sure you don't modify parameters that are required.\nSomething like that should work.\n```\nfor step in range(10000):\n    artist_paintings = artist_works()  # real painting from artist\n    G_ideas = torch.randn(BATCH_SIZE, N_IDEAS)  # random ideas\n    G_paintings = G(G_ideas)  # fake painting from G (random ideas)\n\n    prob_artist1 = D(G_paintings)  # G tries to fool D\n\n    G_loss = torch.mean(torch.log(1. - prob_artist1))\n    opt_G.zero_grad()\n    G_loss.backward()\n    opt_G.step()\n\n    prob_artist0 = D(artist_paintings)  # D try to increase this prob\n    # detach here to make sure we don't backprop in G that was already changed.\n    prob_artist1 = D(G_paintings.detach())  # D try to reduce this prob\n\n    D_loss = - torch.mean(torch.log(prob_artist0) + torch.log(1. - prob_artist1))\n    opt_D.zero_grad()\n    D_loss.backward(retain_graph=True)  # reusing computational graph\n    opt_D.step()\n```\n\nIn the future, I would recommend to ask these questions on the forum: https://discuss.pytorch.org/\nWe keep github issues for bug and features only. And more people look at the forum so you will get a faster answer.",
      "y": "Hi,\n\nThis happens because the `opt_D.step()` modifies the parameters of your discriminator inplace. But these parameters are required to compute the gradient for the generator. Hence the error.\nWe fixed the inplace detection for the optimizers in 1.5, this is why it works in 1.4.\n\nYou should re-organize your code to only do the `steps()` after all the gradients have been computed or make sure you don't modify parameters that are required.\nSomething like that should work.\n```\nfor step in range(10000):\n    artist_paintings = artist_works()  # real painting from artist\n    G_ideas = torch.randn(BATCH_SIZE, N_IDEAS)  # random ideas\n    G_paintings = G(G_ideas)  # fake painting from G (random ideas)\n\n    prob_artist1 = D(G_paintings)  # G tries to fool D\n\n    G_loss = torch.mean(torch.log(1. - prob_artist1))\n    opt_G.zero_grad()\n    G_loss.backward()\n    opt_G.step()\n\n    prob_artist0 = D(artist_paintings)  # D try to increase this prob\n    # detach here to make sure we don't backprop in G that was already changed.\n    prob_artist1 = D(G_paintings.detach())  # D try to reduce this prob\n\n    D_loss = - torch.mean(torch.log(prob_artist0) + torch.log(1. - prob_artist1))\n    opt_D.zero_grad()\n    D_loss.backward(retain_graph=True)  # reusing computational graph\n    opt_D.step()\n```\n\nIn the future, I would recommend to ask these questions on the forum: https://discuss.pytorch.org/\nWe keep github issues for bug and features only. And more people look at the forum so you will get a faster answer."
   },
   {
      "x": "[JIT] Expected integer literal for index:",
      "z": "@yangsenius changing the forward to look like this:\ndef forward(self, x: List[torch.Tensor]):\nfixed that error for me. Now, I am stuck on this:\n```\nExpected a default value of type Tensor on parameter \"mask\".:\n  File \"/usr/local/lib/python3.8/dist-packages/torchvision/ops/deform_conv.py\", line 152\n    def forward(self, input: Tensor, offset: Tensor, mask: Tensor = None) -> Tensor:\n    ~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n        \"\"\"\n        ~~~\n        Args:\n        ~~~~~\n            input (Tensor[batch_size, in_channels, in_height, in_width]): input tensor\n            ~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n            offset (Tensor[batch_size, 2 * offset_groups * kernel_height * kernel_width,\n            ~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n                out_height, out_width]): offsets to be applied for each position in the\n                ~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n                convolution kernel.\n                ~~~~~~~~~~~~~~~~~~~\n            mask (Tensor[batch_size, offset_groups * kernel_height * kernel_width,\n            ~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n                out_height, out_width]): masks to be applied for each position in the\n                ~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n                convolution kernel.\n                ~~~~~~~~~~~~~~~~~~~\n        \"\"\"\n        ~~~\n        return deform_conv2d(input, offset, self.weight, self.bias, stride=self.stride,\n        ~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n                             padding=self.padding, dilation=self.dilation, mask=mask)\n                             ~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~ <--- HERE\n```\n\nNote: I didn't need to change anything to trace but I got size issues.\nThis is all to fix scripting.",
      "y": "@yangsenius changing the forward to look like this:\ndef forward(self, x: List[torch.Tensor]):\nfixed that error for me. Now, I am stuck on this:\n```\nExpected a default value of type Tensor on parameter \"mask\".:\n  File \"/usr/local/lib/python3.8/dist-packages/torchvision/ops/deform_conv.py\", line 152\n    def forward(self, input: Tensor, offset: Tensor, mask: Tensor = None) -> Tensor:\n    ~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n        \"\"\"\n        ~~~\n        Args:\n        ~~~~~\n            input (Tensor[batch_size, in_channels, in_height, in_width]): input tensor\n            ~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n            offset (Tensor[batch_size, 2 * offset_groups * kernel_height * kernel_width,\n            ~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n                out_height, out_width]): offsets to be applied for each position in the\n                ~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n                convolution kernel.\n                ~~~~~~~~~~~~~~~~~~~\n            mask (Tensor[batch_size, offset_groups * kernel_height * kernel_width,\n            ~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n                out_height, out_width]): masks to be applied for each position in the\n                ~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n                convolution kernel.\n                ~~~~~~~~~~~~~~~~~~~\n        \"\"\"\n        ~~~\n        return deform_conv2d(input, offset, self.weight, self.bias, stride=self.stride,\n        ~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n                             padding=self.padding, dilation=self.dilation, mask=mask)\n                             ~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~ <--- HERE\n```\n\nNote: I didn't need to change anything to trace but I got size issues.\nThis is all to fix scripting."
   },
   {
      "x": "[FR] Add space as delimiter in TORCH_CHECK and other macros",
      "z": "Agree with @ShawnZhong, in most cases error strings already contain the necessary spaces, those that don't can be fixed at the call site, it does not make sense to silently insert spaces in the TORCH_CHECK macro itself. ",
      "y": "Agree with @ShawnZhong, in most cases error strings already contain the necessary spaces, those that don't can be fixed at the call site, it does not make sense to silently insert spaces in the TORCH_CHECK macro itself. "
   },
   {
      "x": "torch.distributed support on MacOS is missing",
      "z": "For Windows support, please check this RFC (#42095)\n\nHey @neggert, yes PyTorch + Gloo works on MacOS, but you will need to compile from source using the following steps:\n\n0. follow the readme in https://github.com/pytorch/pytorch to setup conda and dependencies\n1. then conda install libuv and pkg-config\n2. then run `time env MACOSX_DEPLOYMENT_TARGET=10.9 CC=clang CXX=clang++ BUILD_CAFFE2_OPS=0 USE_CUDA=0 USE_MKLDNN=0 USE_DISTRIBUTED=1 python setup.py develop`",
      "y": "For Windows support, please check this RFC (#42095)\n\nHey @neggert, yes PyTorch + Gloo works on MacOS, but you will need to compile from source using the following steps:\n\n0. follow the readme in https://github.com/pytorch/pytorch to setup conda and dependencies\n1. then conda install libuv and pkg-config\n2. then run `time env MACOSX_DEPLOYMENT_TARGET=10.9 CC=clang CXX=clang++ BUILD_CAFFE2_OPS=0 USE_CUDA=0 USE_MKLDNN=0 USE_DISTRIBUTED=1 python setup.py develop`"
   },
   {
      "x": "Installing PyTorch 1.1 into cloned conda env that contained PyTorch 1.0 gives \"Getting \"module 'torch._C' has no attribute 'BoolStorageBase'\" with PyTorch 1.1\"",
      "z": "This seems to be similar issue to https://github.com/pytorch/pytorch/issues/12031 . It looks like you must do `conda uninstall pytorch -y` before `conda install pytorch` or you end up in unrecoverable bad state",
      "y": "This seems to be similar issue to https://github.com/pytorch/pytorch/issues/12031 . It looks like you must do `conda uninstall pytorch -y` before `conda install pytorch` or you end up in unrecoverable bad state"
   },
   {
      "x": "DLL error on Windows 10",
      "z": "Let me conclude it for you:\n1. Install WDK https://docs.microsoft.com/en-us/windows-hardware/drivers/download-the-wdk\n2. Execute these commands:\n```powershell\ngflags /i ${YOUR EXECUTABLE} +sls # e.g. python.exe # Turn on loader snaps\ncdb ${YOUR COMMAND} # e.g. python -c \"import torch\"\n# Keep typing in `g` and `Enter` until the end. Use `q` and `Enter` to exit.\ngflags /i ${YOUR EXECUTABLE} -sls # e.g. python.exe # Turn off loader snaps\n```",
      "y": "Let me conclude it for you:\n1. Install WDK https://docs.microsoft.com/en-us/windows-hardware/drivers/download-the-wdk\n2. Execute these commands:\n```powershell\ngflags /i ${YOUR EXECUTABLE} +sls # e.g. python.exe # Turn on loader snaps\ncdb ${YOUR COMMAND} # e.g. python -c \"import torch\"\n# Keep typing in `g` and `Enter` until the end. Use `q` and `Enter` to exit.\ngflags /i ${YOUR EXECUTABLE} -sls # e.g. python.exe # Turn off loader snaps\n```"
   },
   {
      "x": "Non blocking tensor copy to GPU not working from torch 1.0",
      "z": "You are not measuring the effect of non_blocking because you are explicitly synchronizing inside your measurement.  You end up measuring some complicated interaction between copy overhead and copy time.\n\nnon_blocking=True doesn't make the copy faster. It just allows the copy_ call to return before the copy is completed. If you call `torch.cuda.synchronize()` immediately after a copy you've added back the synchronization you just tried to remove.\n\nAnyways, `non_blocking=True` works in PyTorch 1.0 (from CPU->CUDA). You can see it by adding a GPU delay before your copy:\n\n```python\nimport time\nimport torch\n\nDELAY = 100000000 \nx = torch.randn((1024, 1024), pin_memory=True)\n\ntorch.cuda.synchronize()\nstart = time.time()\ntorch.cuda._sleep(DELAY)\nx.cuda(non_blocking=True)\nend = time.time()\n\nprint('non_blocking=True', (end - start)*1000.)  # ~7 ms on my GPU\n\ntorch.cuda.synchronize()\nstart = time.time()\ntorch.cuda._sleep(DELAY)\nx.cuda(non_blocking=False)\nend = time.time()\n\n\nprint('non_blocking=False', (end - start)*1000.)  # ~77 ms on my GPU\n```",
      "y": "You are not measuring the effect of non_blocking because you are explicitly synchronizing inside your measurement.  You end up measuring some complicated interaction between copy overhead and copy time.\n\nnon_blocking=True doesn't make the copy faster. It just allows the copy_ call to return before the copy is completed. If you call `torch.cuda.synchronize()` immediately after a copy you've added back the synchronization you just tried to remove.\n\nAnyways, `non_blocking=True` works in PyTorch 1.0 (from CPU->CUDA). You can see it by adding a GPU delay before your copy:\n\n```python\nimport time\nimport torch\n\nDELAY = 100000000 \nx = torch.randn((1024, 1024), pin_memory=True)\n\ntorch.cuda.synchronize()\nstart = time.time()\ntorch.cuda._sleep(DELAY)\nx.cuda(non_blocking=True)\nend = time.time()\n\nprint('non_blocking=True', (end - start)*1000.)  # ~7 ms on my GPU\n\ntorch.cuda.synchronize()\nstart = time.time()\ntorch.cuda._sleep(DELAY)\nx.cuda(non_blocking=False)\nend = time.time()\n\n\nprint('non_blocking=False', (end - start)*1000.)  # ~77 ms on my GPU\n```"
   },
   {
      "x": "The result of  gloo all_gather error",
      "z": "@qijianan777 confirm that I can reproduce, and this is indeed a bug in `ProcessGroupGloo`. More specifically, when you do `chunk` on `dim=0`, the result tensors share the same underlying storage and are contiguous, but with different offset. So, when we do [`flat`](https://github.com/pytorch/pytorch/blob/c5845c44821bb3e9f8847544c122cca42aaeba6d/torch/lib/c10d/ProcessGroupGloo.cpp#L842) the tensors here, it will do nothing. Later, when retrieving [data pointer](https://github.com/pytorch/pytorch/blob/c5845c44821bb3e9f8847544c122cca42aaeba6d/torch/lib/c10d/ProcessGroupGloo.cpp#L110) of the tensors, it will return the same ptr value (this is the bug). As a result, both processes are reading the first 2 elements. \n\nThanks for reporting, I will add a fix for it. ",
      "y": "@qijianan777 confirm that I can reproduce, and this is indeed a bug in `ProcessGroupGloo`. More specifically, when you do `chunk` on `dim=0`, the result tensors share the same underlying storage and are contiguous, but with different offset. So, when we do [`flat`](https://github.com/pytorch/pytorch/blob/c5845c44821bb3e9f8847544c122cca42aaeba6d/torch/lib/c10d/ProcessGroupGloo.cpp#L842) the tensors here, it will do nothing. Later, when retrieving [data pointer](https://github.com/pytorch/pytorch/blob/c5845c44821bb3e9f8847544c122cca42aaeba6d/torch/lib/c10d/ProcessGroupGloo.cpp#L110) of the tensors, it will return the same ptr value (this is the bug). As a result, both processes are reading the first 2 elements. \n\nThanks for reporting, I will add a fix for it. "
   },
   {
      "x": "Error downloading MNIST dataset",
      "z": "I solved my own problem too...\nthere was a mismatch of torchvision I installed using pip...it was 0.2.2 with torch 1.8.0...\nI then built and installed torchvision 0.9.0 from source and it works correctly",
      "y": "I solved my own problem too...\nthere was a mismatch of torchvision I installed using pip...it was 0.2.2 with torch 1.8.0...\nI then built and installed torchvision 0.9.0 from source and it works correctly"
   },
   {
      "x": "\"derivative for _thnn_fused_lstm_cell_backward is not implemented\" while using GPU",
      "z": "Apparently, double backward on gpu has been broken for some time. What's happening is in the forward _thnn_fused_lstm_cell is called https://github.com/pytorch/pytorch/blob/master/aten/src/ATen/native/RNN.cpp#L259-L265, and it has its derivative specified in derivatives.yaml, it's _thnn_fused_lstm_cell_backward. _thnn_fused_lstm_cell_backward does not have derivative, and throws an error. \nThe easiest fix would be to determine that there would be double backward and not use fused cell in forward, but I don't think there's a way to tell in forward if there will be double backward or not. So it looks like it's necessary to define differentiable backward functions for rnn cells, similarly to how it's done for weight norm, and fall back to them if `GradMode::is_enabled()`.",
      "y": "Apparently, double backward on gpu has been broken for some time. What's happening is in the forward _thnn_fused_lstm_cell is called https://github.com/pytorch/pytorch/blob/master/aten/src/ATen/native/RNN.cpp#L259-L265, and it has its derivative specified in derivatives.yaml, it's _thnn_fused_lstm_cell_backward. _thnn_fused_lstm_cell_backward does not have derivative, and throws an error. \nThe easiest fix would be to determine that there would be double backward and not use fused cell in forward, but I don't think there's a way to tell in forward if there will be double backward or not. So it looks like it's necessary to define differentiable backward functions for rnn cells, similarly to how it's done for weight norm, and fall back to them if `GradMode::is_enabled()`."
   },
   {
      "x": "Implementing GELU activation",
      "z": "This is fairly simple (see below for what would need to be added to `functional.py`), but seeing as it is common enough now could be worthwhile. If you're able to submit a PR with this (including a Module that wraps this and tests as necessary) then that would be useful.\n\n```py\ndef gelu(x):\n  return 0.5 * x * (1 + torch.tanh(math.sqrt(math.pi / 2) * (x + 0.044715 * x ** 3)))\n```",
      "y": "This is fairly simple (see below for what would need to be added to `functional.py`), but seeing as it is common enough now could be worthwhile. If you're able to submit a PR with this (including a Module that wraps this and tests as necessary) then that would be useful.\n\n```py\ndef gelu(x):\n  return 0.5 * x * (1 + torch.tanh(math.sqrt(math.pi / 2) * (x + 0.044715 * x ** 3)))\n```"
   },
   {
      "x": "tensorboard not updating",
      "z": "My current workaround:\n\nwhile true; do\n        timeout -sHUP 1m tensorboard --logdir=runs;\ndone",
      "y": "My current workaround:\n\nwhile true; do\n        timeout -sHUP 1m tensorboard --logdir=runs;\ndone"
   },
   {
      "x": "RuntimeError: Creating MTGP constants failed",
      "z": "This is now solved because curandStateMtgp constants are never created as a result of https://github.com/pytorch/pytorch/pull/20886. Closing this issue.",
      "y": "This is now solved because curandStateMtgp constants are never created as a result of https://github.com/pytorch/pytorch/pull/20886. Closing this issue."
   },
   {
      "x": "Install only a specific version via pip",
      "z": "this issue can now be closed, because we use local version identifiers. \nFor example, CPU-only version is:\n```\npip3 install torch==1.2.0+cpu torchvision==0.4.0+cpu -f https://download.pytorch.org/whl/torch_stable.html\n```\n\nSo, in `requirements.txt`, having a constraint such as `1.2.0+cpu` should be good.\n\nWe dont ship the variant wheels via PyPI though, and have no plans to do so (PyPI doesn't allow local version identifiers to be uploaded live)",
      "y": "this issue can now be closed, because we use local version identifiers. \nFor example, CPU-only version is:\n```\npip3 install torch==1.2.0+cpu torchvision==0.4.0+cpu -f https://download.pytorch.org/whl/torch_stable.html\n```\n\nSo, in `requirements.txt`, having a constraint such as `1.2.0+cpu` should be good.\n\nWe dont ship the variant wheels via PyPI though, and have no plans to do so (PyPI doesn't allow local version identifiers to be uploaded live)"
   },
   {
      "x": "MobileNetV2 export to ONNX fails",
      "z": "look into the mobilenet.py in torchvision, change the forward function:\n    def forward(self, x):\n        x = self.features(x)\n        # x = x.mean([2, 3])   # this line will result in bug\n        x = x.mean(3).mean(2)\n        x = self.classifier(x)\n        return x\n\n",
      "y": "look into the mobilenet.py in torchvision, change the forward function:\n    def forward(self, x):\n        x = self.features(x)\n        # x = x.mean([2, 3])   # this line will result in bug\n        x = x.mean(3).mean(2)\n        x = self.classifier(x)\n        return x\n\n"
   },
   {
      "x": "nn.CTCLoss RuntimeError on GPU",
      "z": "Perfect! I compile torchvision from source and it works well. Thanks you @t-vi",
      "y": "Perfect! I compile torchvision from source and it works well. Thanks you @t-vi"
   },
   {
      "x": "StepLR, MultiStepLR, ExponentialLR and CosineAnnealingLR scheduler wrong lr value",
      "z": "Thanks for the example. As mentioned in #26423, `get_lr` should be replaced by `get_last_lr`, see below.\n\n```python\nimport torch\nprint(\"pytorch version\",torch.__version__) \nimport torch.nn as nn\nmodel = nn.Linear(1, 1) # 'Net' is a simple MLP\noptimizer = torch.optim.SGD(model.parameters(), lr=0.1)\nschedular = torch.optim.lr_scheduler.MultiStepLR(optimizer, milestones = [4,7], gamma=0.1)\n\nprint('Initial LR : {0:.8f}'.format(schedular.get_lr()[0]))\nfor e in range(8):\n  optimizer.step()\n  schedular.step()\n  print('Epoch {0}, LR: {1:.8f}, opt LR {2:.8f}'.format(e, schedular.get_last_lr()[0],\n          optimizer.param_groups[0]['lr']))\n```\n\nSince #26423 has been merged, I will close this issue. Please feel free to re-open if the issue persists.",
      "y": "Thanks for the example. As mentioned in #26423, `get_lr` should be replaced by `get_last_lr`, see below.\n\n```python\nimport torch\nprint(\"pytorch version\",torch.__version__) \nimport torch.nn as nn\nmodel = nn.Linear(1, 1) # 'Net' is a simple MLP\noptimizer = torch.optim.SGD(model.parameters(), lr=0.1)\nschedular = torch.optim.lr_scheduler.MultiStepLR(optimizer, milestones = [4,7], gamma=0.1)\n\nprint('Initial LR : {0:.8f}'.format(schedular.get_lr()[0]))\nfor e in range(8):\n  optimizer.step()\n  schedular.step()\n  print('Epoch {0}, LR: {1:.8f}, opt LR {2:.8f}'.format(e, schedular.get_last_lr()[0],\n          optimizer.param_groups[0]['lr']))\n```\n\nSince #26423 has been merged, I will close this issue. Please feel free to re-open if the issue persists."
   },
   {
      "x": "Couple hundred MB are taken just by initializing cuda",
      "z": "> why it's need so many memory on the GPU?\n\nIt's used by the CUDA driver. I think it's used to store the CUDA kernels and PyTorch has a lot of CUDA kernels.\n\n> can i release it?\n\nNo (other than quitting the process)\n\nThe standard practice is to **not** access the same GPU from multiple processes. i.e. use one process per GPU. ",
      "y": "> why it's need so many memory on the GPU?\n\nIt's used by the CUDA driver. I think it's used to store the CUDA kernels and PyTorch has a lot of CUDA kernels.\n\n> can i release it?\n\nNo (other than quitting the process)\n\nThe standard practice is to **not** access the same GPU from multiple processes. i.e. use one process per GPU. "
   },
   {
      "x": "Saving state_dicts should capture shared state",
      "z": "I don't see anything we should change in PyTorch. `load_state_dict` doesn't change the structure of the Module and **should not** change the structure.\n\nIf you create your first version with:\n\n```\na = A()\nb = B(a=a)\nc = C(a=a, b=b)\n```\n\nThen your second version of C needs to be created with the same structure:\n\n```\na2 = A()\nb2 = B(a=a2)\nc2 = C(a=a2, b=b2)\n```\n\nIf you instead do:\n\n```\nc2 = C()\n```\n\nYou've create a different structure and shouldn't expect it to work the same as your first incarnation.\n",
      "y": "I don't see anything we should change in PyTorch. `load_state_dict` doesn't change the structure of the Module and **should not** change the structure.\n\nIf you create your first version with:\n\n```\na = A()\nb = B(a=a)\nc = C(a=a, b=b)\n```\n\nThen your second version of C needs to be created with the same structure:\n\n```\na2 = A()\nb2 = B(a=a2)\nc2 = C(a=a2, b=b2)\n```\n\nIf you instead do:\n\n```\nc2 = C()\n```\n\nYou've create a different structure and shouldn't expect it to work the same as your first incarnation.\n"
   },
   {
      "x": "Multiple GPU, Batch Normalization - RuntimeError: the derivative for 'running_mean' is not implemented",
      "z": "Low chance this applies to your situation, but I got this error when I used a parameter with `requires_grad` set to `True` as the `running_mean` in the `nn.functional.batch_norm` function. Hopefully this helps someone else who finds themselves in the same situation.",
      "y": "Low chance this applies to your situation, but I got this error when I used a parameter with `requires_grad` set to `True` as the `running_mean` in the `nn.functional.batch_norm` function. Hopefully this helps someone else who finds themselves in the same situation."
   },
   {
      "x": "[Feature request] Get cell state from the last layer for each t when using LSTM",
      "z": "considering that using custom RNNs is the only way this is possible (because nn.LSTM's backing CUDNN LSTM does not allow extraction of intermediate states), I am closing this request with @zou3519 's answer above as the way forward.",
      "y": "considering that using custom RNNs is the only way this is possible (because nn.LSTM's backing CUDNN LSTM does not allow extraction of intermediate states), I am closing this request with @zou3519 's answer above as the way forward."
   },
   {
      "x": "Compiling pytorch on MacOSX 10.13.5 with support for CUDA GeForceGT 750M",
      "z": "https://docs.nvidia.com/cuda/cuda-installation-guide-mac-os-x/index.html\n\nYou need to download Command_Line_Tools_macOS_10.13_for_Xcode_9.2.dmg from https://developer.apple.com/download/more/ and install then execute `sudo xcode-select --switch /Library/Developer/CommandLineTools`",
      "y": "https://docs.nvidia.com/cuda/cuda-installation-guide-mac-os-x/index.html\n\nYou need to download Command_Line_Tools_macOS_10.13_for_Xcode_9.2.dmg from https://developer.apple.com/download/more/ and install then execute `sudo xcode-select --switch /Library/Developer/CommandLineTools`"
   },
   {
      "x": "RuntimeError: DataLoader worker is killed by signal: Killed.",
      "z": "It is very likely there was a out-of-memory(OOM) in your system so the data worker got killed by the system. Try to use `dmesg -T` to see the detailed reason.",
      "y": "It is very likely there was a out-of-memory(OOM) in your system so the data worker got killed by the system. Try to use `dmesg -T` to see the detailed reason."
   },
   {
      "x": "[Feature Request] tensordot",
      "z": "I'd do tensordot based on the (private) sumproduct_pair function used by einsum. In fact, if we want this, I'd be glad to send a PR.\nMedium-term, I'd like to incorporate the optimizations into einsum itself. (Numpy, did, too.)",
      "y": "I'd do tensordot based on the (private) sumproduct_pair function used by einsum. In fact, if we want this, I'd be glad to send a PR.\nMedium-term, I'd like to incorporate the optimizations into einsum itself. (Numpy, did, too.)"
   },
   {
      "x": "[proposal] out= doesn't resize storage",
      "z": "We can create memory pool and enforce using it for memory operations. This will allows us to control who can resize and who can't",
      "y": "We can create memory pool and enforce using it for memory operations. This will allows us to control who can resize and who can't"
   },
   {
      "x": "[jit] gen_jit_dispatch generates duplicate \"descriptor\"s",
      "z": "We no longer use descriptors, so this is fixed.",
      "y": "We no longer use descriptors, so this is fixed."
   },
   {
      "x": "[jit] cannot trace tensor factory methods",
      "z": "This is kind of expected. There are two main problems at play here:\n\n1. RNG calls are not functional - they mutate the state of the global PRNG, and therefore we probably shouldn't rearrange them. This however requires us to add the \"world token\" to inhibit optimizations.\n2. We never trace constructors. If we want to do this, we probably need to make the tracing a thread-local property, instead of auto-detecting it as a per-Variable property.",
      "y": "This is kind of expected. There are two main problems at play here:\n\n1. RNG calls are not functional - they mutate the state of the global PRNG, and therefore we probably shouldn't rearrange them. This however requires us to add the \"world token\" to inhibit optimizations.\n2. We never trace constructors. If we want to do this, we probably need to make the tracing a thread-local property, instead of auto-detecting it as a per-Variable property."
   },
   {
      "x": "ImportError: DLL load failed: The specified module could not be found",
      "z": "Downgrade sqlite3 in Anaconda is the solution, as it was the last version I couldn't upgrade it, and no option for install or uninstall is available. after the downgrade it worked fine",
      "y": "Downgrade sqlite3 in Anaconda is the solution, as it was the last version I couldn't upgrade it, and no option for install or uninstall is available. after the downgrade it worked fine"
   },
   {
      "x": "PackedSequence's sorted_indices is not put on cuda when to('cuda') is called.",
      "z": "Issue is still present on `pytorch==1.3.1`\n\nTo fix, replace the following:\n`X = X.to(device)`\nWith this:\n`X = X.to(device=device)`\nProvided that \"X\" is a packed sequence.",
      "y": "Issue is still present on `pytorch==1.3.1`\n\nTo fix, replace the following:\n`X = X.to(device)`\nWith this:\n`X = X.to(device=device)`\nProvided that \"X\" is a packed sequence."
   },
   {
      "x": "Schema not found for node torch::eye",
      "z": "Are you able to extract a smaller repro?\n\n`import geoopt` works fine on master, and re-producing [the code](https://github.com/geoopt/geoopt/blob/master/geoopt/linalg/_expm.py#L29) causing the failure runs fine as well\n\n```python\n@torch.jit.script\ndef test(A):\n    return torch.eye(A.shape[1], dtype=A.dtype, device=A.device)\n```",
      "y": "Are you able to extract a smaller repro?\n\n`import geoopt` works fine on master, and re-producing [the code](https://github.com/geoopt/geoopt/blob/master/geoopt/linalg/_expm.py#L29) causing the failure runs fine as well\n\n```python\n@torch.jit.script\ndef test(A):\n    return torch.eye(A.shape[1], dtype=A.dtype, device=A.device)\n```"
   },
   {
      "x": "element-wise multiplication out of memory",
      "z": "You need ~12.6 GB of memory for your example (20 *75 * 1024* 1024 * 4 bytes * 2 = 12.58 GB). The last 2 is because you need space for `g` and `res`. If you don't care about keeping the value of `g` do:\n\n```python\nimport torch\n\ng = torch.rand([20, 75, 1024, 1024])\nw = torch.rand([1024, 1024])\ng *= w\nres = g\n```\n\nThat will require ~6.3 GB of memory.\n\nAlso, element-wise multiplication is just `g * w` in general. You can use einsum if you want, but it's not necessary.\n\nYou're Tensorflow example isn't actually running anything (you'd need a `session.run()` call) so it doesn't require any memory for storing tensor data.",
      "y": "You need ~12.6 GB of memory for your example (20 *75 * 1024* 1024 * 4 bytes * 2 = 12.58 GB). The last 2 is because you need space for `g` and `res`. If you don't care about keeping the value of `g` do:\n\n```python\nimport torch\n\ng = torch.rand([20, 75, 1024, 1024])\nw = torch.rand([1024, 1024])\ng *= w\nres = g\n```\n\nThat will require ~6.3 GB of memory.\n\nAlso, element-wise multiplication is just `g * w` in general. You can use einsum if you want, but it's not necessary.\n\nYou're Tensorflow example isn't actually running anything (you'd need a `session.run()` call) so it doesn't require any memory for storing tensor data."
   },
   {
      "x": "torch::tensor(std::vector) does not work properly in Microsoft Visual Studio Windows",
      "z": "`int64_t` works fine.",
      "y": "`int64_t` works fine."
   },
   {
      "x": "unable to load istream by using torch::jit::load(istream)",
      "z": "@lantiga\n I have solved it \uff0cthanks\n\n1. read pt model to char buffer by using ifstream\n\n2.change buffer to istream  \nstrstreambuf  buf(pModelData,length);\nstd::istream in(&buf);\n\n3\u3001load istream using torch::jit::load\n ",
      "y": "@lantiga\n I have solved it \uff0cthanks\n\n1. read pt model to char buffer by using ifstream\n\n2.change buffer to istream  \nstrstreambuf  buf(pModelData,length);\nstd::istream in(&buf);\n\n3\u3001load istream using torch::jit::load\n "
   },
   {
      "x": "autodiff for user script functions aka torch.jit.script for autograd.Function",
      "z": "I think the fix should be to allow users to provide derivatives.yaml for the extensions, and generate the autograd tracking code that Tom had to write manually if derivatives.yaml is provided. That would make extensions differentiable without having to wrap them in the custom autograd function, and improve UX in python eager mode and in C++. It will also reduce the need for custom autograd functions - it looks like in 99% {?) of cases custom autograd functions are needed to make extensions differentiable. ",
      "y": "I think the fix should be to allow users to provide derivatives.yaml for the extensions, and generate the autograd tracking code that Tom had to write manually if derivatives.yaml is provided. That would make extensions differentiable without having to wrap them in the custom autograd function, and improve UX in python eager mode and in C++. It will also reduce the need for custom autograd functions - it looks like in 99% {?) of cases custom autograd functions are needed to make extensions differentiable. "
   },
   {
      "x": "Build with MKLDNN broken",
      "z": "@mdreammao\nPls try to apply the patch in #22910 , then run \"git submodule update --init --recursive\".\n\nThanks.",
      "y": "@mdreammao\nPls try to apply the patch in #22910 , then run \"git submodule update --init --recursive\".\n\nThanks."
   },
   {
      "x": "Auto-differentiating torch.cdist is broken on GPU",
      "z": "Interesting. After instantiating a new Python kernel, the first time I run the above code, I receive the \"Backward is not reentrant\" error message. Subsequent reruns of the same code (with new and identical `X` tensor and same Python kernel) result in the original error message and the analytical gradients are very different each call.\n\nNotebook: \n[cdist_issue_repeated.ipynb.txt](https://github.com/pytorch/pytorch/files/3341208/cdist_issue_repeated.ipynb.txt)\n",
      "y": "Interesting. After instantiating a new Python kernel, the first time I run the above code, I receive the \"Backward is not reentrant\" error message. Subsequent reruns of the same code (with new and identical `X` tensor and same Python kernel) result in the original error message and the analytical gradients are very different each call.\n\nNotebook: \n[cdist_issue_repeated.ipynb.txt](https://github.com/pytorch/pytorch/files/3341208/cdist_issue_repeated.ipynb.txt)\n"
   },
   {
      "x": "conversion  to non-scalar type  torch::jit::load(\"model.pt\")",
      "z": "It looks like you are using a nightly build. We recently changed the output type of load. This should work:\n\n```\n torch::jit::script::Module module = torch::jit::load(\"model.pt\");\n```\n\nTutorials/documentation are still for the 1.1 release. It will be updated before we release 1.2.",
      "y": "It looks like you are using a nightly build. We recently changed the output type of load. This should work:\n\n```\n torch::jit::script::Module module = torch::jit::load(\"model.pt\");\n```\n\nTutorials/documentation are still for the 1.1 release. It will be updated before we release 1.2."
   },
   {
      "x": "Dependency issues with torch.utils.tensorboard: \"No module named past\" and \"No module named 'PIL'\"",
      "z": "Note that the right fix for the `past.stringbase` is really to stop using the deprecated API (which is not that hard too...)",
      "y": "Note that the right fix for the `past.stringbase` is really to stop using the deprecated API (which is not that hard too...)"
   },
   {
      "x": "Cuda required when loading a TorchScript with map_location='cpu'",
      "z": "It looks like this model came from a trace, is that correct? When exporting a model for use on CPU, we recommend that you switch it to CPU mode _before_ tracing it and exporting it. Otherwise it will hard-code details of using the GPU into the model, as was done here (`torch.to(CONSTANTS.c0, torch.device(\"cuda:0\"), 6, False, False)`). Telling the parameters to map to the CPU will not change traced details like the `to` call which copies the Tensor to the GPU).",
      "y": "It looks like this model came from a trace, is that correct? When exporting a model for use on CPU, we recommend that you switch it to CPU mode _before_ tracing it and exporting it. Otherwise it will hard-code details of using the GPU into the model, as was done here (`torch.to(CONSTANTS.c0, torch.device(\"cuda:0\"), 6, False, False)`). Telling the parameters to map to the CPU will not change traced details like the `to` call which copies the Tensor to the GPU)."
   },
   {
      "x": "runtime error (7) : too many resources requested for launch at pytorch/aten/src/THC/THCTensorSort.cu:62",
      "z": "The main thing I'm aware of is that the switchover from bitonic sort to thrust as a fallback is different for 64 bit dtypes. These have been lowered for sort but that has not been applied to topk's sorting.\nThe natural questions would be\n- What is the tensor dtype and size and what are dim and k used ins topk?\n- Does the non-sorting topk work for that?\n- Does sort work for the non-sorted values?\n\n(The last two also might offer a workaround until it is fixed in PyTorch).\n",
      "y": "The main thing I'm aware of is that the switchover from bitonic sort to thrust as a fallback is different for 64 bit dtypes. These have been lowered for sort but that has not been applied to topk's sorting.\nThe natural questions would be\n- What is the tensor dtype and size and what are dim and k used ins topk?\n- Does the non-sorting topk work for that?\n- Does sort work for the non-sorted values?\n\n(The last two also might offer a workaround until it is fixed in PyTorch).\n"
   },
   {
      "x": "[Build Error]undefined reference to `__cudaPushCallConfiguration'",
      "z": "This probably means that the inconsistent version of NVCC compile and your conda CUDAToolKit package\n\n```\n# Check the NVCC compile version(e.g.)\n/usr/cuda-9.2/bin/nvcc --version\n# Check the CUDAToolKit version(e.g.)\n~/anaconda3/bin/conda list | grep cuda\n\n# If you need to update your CUDAToolKit\n~/anaconda3/bin/conda install -c anaconda cudatoolkit==9.2\n```\nBoth of them should have the same version. For example, if NVCC==9.2 and CUDAToolKit==9.2, this will be fine while when NVCC==9.2 but CUDAToolKit==9, it fails.",
      "y": "This probably means that the inconsistent version of NVCC compile and your conda CUDAToolKit package\n\n```\n# Check the NVCC compile version(e.g.)\n/usr/cuda-9.2/bin/nvcc --version\n# Check the CUDAToolKit version(e.g.)\n~/anaconda3/bin/conda list | grep cuda\n\n# If you need to update your CUDAToolKit\n~/anaconda3/bin/conda install -c anaconda cudatoolkit==9.2\n```\nBoth of them should have the same version. For example, if NVCC==9.2 and CUDAToolKit==9.2, this will be fine while when NVCC==9.2 but CUDAToolKit==9, it fails."
   },
   {
      "x": "forward function can not insert pdb.trace",
      "z": "Hi @DanlanChen,\n\nWe do not have deep integration of pdb into our runtime. To debug a ScriptModule, you can do a few things:\n\n1) Remove the `@torch.jit.script_method` decorator from the method and debug the method as Python code\n2) Create a free python function, pass in values you're interested to inspect into that function, and within the function call `pdb`. Example:\n\n```\nimport torch\n\ndef debug_fn(a, b, c):\n    print(a, b, c)\n    import pdb; pdb.set_trace()\n\nclass FooMod(torch.jit.ScriptModule):\n    @torch.jit.script_method\n    def forward(self, x, y):\n        z = x + y\n        debug_fn(x, y, z)\n        return z\n\nfm = FooMod()\nfm(torch.rand(3), torch.rand(3))\n```\n\nHope this helps with your debugging!",
      "y": "Hi @DanlanChen,\n\nWe do not have deep integration of pdb into our runtime. To debug a ScriptModule, you can do a few things:\n\n1) Remove the `@torch.jit.script_method` decorator from the method and debug the method as Python code\n2) Create a free python function, pass in values you're interested to inspect into that function, and within the function call `pdb`. Example:\n\n```\nimport torch\n\ndef debug_fn(a, b, c):\n    print(a, b, c)\n    import pdb; pdb.set_trace()\n\nclass FooMod(torch.jit.ScriptModule):\n    @torch.jit.script_method\n    def forward(self, x, y):\n        z = x + y\n        debug_fn(x, y, z)\n        return z\n\nfm = FooMod()\nfm(torch.rand(3), torch.rand(3))\n```\n\nHope this helps with your debugging!"
   },
   {
      "x": "zip not allowed in forward function in torch.jit.ScriptModule",
      "z": "As a workaround, you can do the following:\n\n```\nmod_list = []\nfor mod1, mod2 in zip(module1, module2):\n    mod_list.append(nn.Sequential(mod1, mod2))\nself.module = nn.ModuleList(mod_list)\n```\n",
      "y": "As a workaround, you can do the following:\n\n```\nmod_list = []\nfor mod1, mod2 in zip(module1, module2):\n    mod_list.append(nn.Sequential(mod1, mod2))\nself.module = nn.ModuleList(mod_list)\n```\n"
   },
   {
      "x": "Cannot build libtorch: SLEEF does not allow in-source builds",
      "z": "Sleef doesn't support nesting. I could fix it similarly to https://ceres-solver-review.googlesource.com/c/ceres-solver/+/9780:\n\n- Using sleef_[SOURCE/BINARY]_DIR (which are defined by CMake when\n  project(sleef) is called, in favour of CMAKE_[SOURCE/BINARY]_DIR\n  enables sleef to be nested within (and built by) a larger CMake\n  project (which also contains other projects).\n- CMAKE_[SOURCE/BINARY]_DIR always refers to the top-level source\n  and binary directories (i.e. the first encountered), as a result if\n  sleef is a nested project within a larger project, these would not\n  correctly identify the source/binary directories for sleef (as they\n  would refer to the root project in which sleef is nested).\n- Using sleef_[SOURCE/BINARY]_DIR should ensure that sleef always uses\n  the correct source/binary directories, irrespective of whether sleef\n  is nested or not.\n\nPatch for `aten/src/ATen/CMakeLists.txt`:\n------------------------------------------------------\n```\ndiff --git a/aten/src/ATen/CMakeLists.txt b/aten/src/ATen/CMakeLists.txt\nindex 07b3c106b..6133c583a 100644\n--- a/aten/src/ATen/CMakeLists.txt\n+++ b/aten/src/ATen/CMakeLists.txt\n@@ -172,10 +172,13 @@ if(NOT MSVC AND NOT EMSCRIPTEN)\n   set(BUILD_DFT OFF CACHE BOOL \"Don't build sleef DFT lib\" FORCE)\n   set(BUILD_GNUABI_LIBS OFF CACHE BOOL \"Don't build sleef gnuabi libs\" FORCE)\n   set(BUILD_TESTS OFF CACHE BOOL \"Don't build sleef tests\" FORCE)\n+  set(sleef_SOURCE_DIR \"${CMAKE_CURRENT_SOURCE_DIR}/../../../third_party/sleef\")\n+  set(sleef_BINARY_DIR \"${CMAKE_BINARY_DIR}/sleef\")\n   add_subdirectory(\"${CMAKE_CURRENT_SOURCE_DIR}/../../../third_party/sleef\" ${CMAKE_BINARY_DIR}/sleef)\n   set_property(TARGET sleef PROPERTY FOLDER \"dependencies\")\n-  list(APPEND ATen_THIRD_PARTY_INCLUDE ${CMAKE_BINARY_DIR}/include)\n-  link_directories(${CMAKE_BINARY_DIR}/sleef/lib)\n+  list(APPEND ATen_THIRD_PARTY_INCLUDE ${sleef_BINARY_DIR}/include)\n+  link_directories(${sleef_BINARY_DIR}/lib)\n   list(APPEND ATen_CPU_DEPENDENCY_LIBS sleef)\n \n   set(CMAKE_C_FLAGS_DEBUG ${OLD_CMAKE_C_FLAGS_DEBUG})\n```\n\nPatch for cmake files in sleef:\n---------------------------------------\n[sleef_patch.txt](https://github.com/pytorch/pytorch/files/2979917/sleef_patch.txt)\n",
      "y": "Sleef doesn't support nesting. I could fix it similarly to https://ceres-solver-review.googlesource.com/c/ceres-solver/+/9780:\n\n- Using sleef_[SOURCE/BINARY]_DIR (which are defined by CMake when\n  project(sleef) is called, in favour of CMAKE_[SOURCE/BINARY]_DIR\n  enables sleef to be nested within (and built by) a larger CMake\n  project (which also contains other projects).\n- CMAKE_[SOURCE/BINARY]_DIR always refers to the top-level source\n  and binary directories (i.e. the first encountered), as a result if\n  sleef is a nested project within a larger project, these would not\n  correctly identify the source/binary directories for sleef (as they\n  would refer to the root project in which sleef is nested).\n- Using sleef_[SOURCE/BINARY]_DIR should ensure that sleef always uses\n  the correct source/binary directories, irrespective of whether sleef\n  is nested or not.\n\nPatch for `aten/src/ATen/CMakeLists.txt`:\n------------------------------------------------------\n```\ndiff --git a/aten/src/ATen/CMakeLists.txt b/aten/src/ATen/CMakeLists.txt\nindex 07b3c106b..6133c583a 100644\n--- a/aten/src/ATen/CMakeLists.txt\n+++ b/aten/src/ATen/CMakeLists.txt\n@@ -172,10 +172,13 @@ if(NOT MSVC AND NOT EMSCRIPTEN)\n   set(BUILD_DFT OFF CACHE BOOL \"Don't build sleef DFT lib\" FORCE)\n   set(BUILD_GNUABI_LIBS OFF CACHE BOOL \"Don't build sleef gnuabi libs\" FORCE)\n   set(BUILD_TESTS OFF CACHE BOOL \"Don't build sleef tests\" FORCE)\n+  set(sleef_SOURCE_DIR \"${CMAKE_CURRENT_SOURCE_DIR}/../../../third_party/sleef\")\n+  set(sleef_BINARY_DIR \"${CMAKE_BINARY_DIR}/sleef\")\n   add_subdirectory(\"${CMAKE_CURRENT_SOURCE_DIR}/../../../third_party/sleef\" ${CMAKE_BINARY_DIR}/sleef)\n   set_property(TARGET sleef PROPERTY FOLDER \"dependencies\")\n-  list(APPEND ATen_THIRD_PARTY_INCLUDE ${CMAKE_BINARY_DIR}/include)\n-  link_directories(${CMAKE_BINARY_DIR}/sleef/lib)\n+  list(APPEND ATen_THIRD_PARTY_INCLUDE ${sleef_BINARY_DIR}/include)\n+  link_directories(${sleef_BINARY_DIR}/lib)\n   list(APPEND ATen_CPU_DEPENDENCY_LIBS sleef)\n \n   set(CMAKE_C_FLAGS_DEBUG ${OLD_CMAKE_C_FLAGS_DEBUG})\n```\n\nPatch for cmake files in sleef:\n---------------------------------------\n[sleef_patch.txt](https://github.com/pytorch/pytorch/files/2979917/sleef_patch.txt)\n"
   },
   {
      "x": "Confusing behavior with *= operator with torch.expand",
      "z": "This is a duplicate of https://github.com/pytorch/pytorch/issues/957\n\nI agree this is confusing and has bitten a lot of users in the past, we should discuss about how to circumvent this (maybe by adding a `WRITABLE = False` flag to expanded tensors?)",
      "y": "This is a duplicate of https://github.com/pytorch/pytorch/issues/957\n\nI agree this is confusing and has bitten a lot of users in the past, we should discuss about how to circumvent this (maybe by adding a `WRITABLE = False` flag to expanded tensors?)"
   },
   {
      "x": "from torch._C import * ImportError: DLL load failed: The specified module could not be found.",
      "z": "> the same issue today\n> i've tried python 3.6.x and 3.7.1, didn't work.\n\nfirst, install python3.6.7.(other version such as 3.6.0 and 3.6.8 didn't work)\nand then ,` pip install --upgrade numpy  and pip install intel-openmp` \nit works for me\n",
      "y": "> the same issue today\n> i've tried python 3.6.x and 3.7.1, didn't work.\n\nfirst, install python3.6.7.(other version such as 3.6.0 and 3.6.8 didn't work)\nand then ,` pip install --upgrade numpy  and pip install intel-openmp` \nit works for me\n"
   },
   {
      "x": "multivariate_normal.log_prob is slow",
      "z": "@t-vi Thanks for notifying me! I think that it is my fault when trying to use `trtrs` instead of `inverse`. The slowdown seems lie at [this line](https://github.com/pytorch/pytorch/blob/master/torch/distributions/multivariate_normal.py#L43) where we expand `bL` batch shape to match the later part of `bx` batch shape, then do triangle solve with \"expanded\" bL. If we use `inverse`, then after taking the inverse of `bL`, we just simply do `matmul` with `bx` to get the result. So it will be much faster in the above example.\n\nThe current implementation works well when `x` has additional batch shapes instead. For example, the following version will be fast because we won't expand bL.\n```\nsigma = torch.eye(8).expand(6, 1, 8, 8).contiguous().requires_grad_()\nx_repeat = torch.randn(8000, 6, 1, 8)\n```\n\nI'll sketch out a solution which I have in mind for further discussion.",
      "y": "@t-vi Thanks for notifying me! I think that it is my fault when trying to use `trtrs` instead of `inverse`. The slowdown seems lie at [this line](https://github.com/pytorch/pytorch/blob/master/torch/distributions/multivariate_normal.py#L43) where we expand `bL` batch shape to match the later part of `bx` batch shape, then do triangle solve with \"expanded\" bL. If we use `inverse`, then after taking the inverse of `bL`, we just simply do `matmul` with `bx` to get the result. So it will be much faster in the above example.\n\nThe current implementation works well when `x` has additional batch shapes instead. For example, the following version will be fast because we won't expand bL.\n```\nsigma = torch.eye(8).expand(6, 1, 8, 8).contiguous().requires_grad_()\nx_repeat = torch.randn(8000, 6, 1, 8)\n```\n\nI'll sketch out a solution which I have in mind for further discussion."
   },
   {
      "x": "Training on 360 sequences, validating on 0 sequences. python3: symbol lookup error: /home/ankitakulkarni/anaconda3/lib/python3.6/site-packages/torch/lib/libtorch_python.so: undefined symbol: PySlice_Unpack",
      "z": "Solved ...Update the version of python from 3.6.0 to 3.6.2 \nThanks!!",
      "y": "Solved ...Update the version of python from 3.6.0 to 3.6.2 \nThanks!!"
   },
   {
      "x": "Windows pytorch CUDA 10 error",
      "z": "FWIW, I fixed this by reinstalling torch on a fresh installation of miniconda3 (the one available on their website right now). However, when I do conda update --all, the problem comes back. This means the problem has to do with installing the most recent version of one of these packages: cryptography, pyopenssl, python, setuptools. I've managed to get around this for now by installing torch on a clean new environment.",
      "y": "FWIW, I fixed this by reinstalling torch on a fresh installation of miniconda3 (the one available on their website right now). However, when I do conda update --all, the problem comes back. This means the problem has to do with installing the most recent version of one of these packages: cryptography, pyopenssl, python, setuptools. I've managed to get around this for now by installing torch on a clean new environment."
   },
   {
      "x": "Broken indexing?",
      "z": "Resolved by upgrading to >= Python 3.6.1",
      "y": "Resolved by upgrading to >= Python 3.6.1"
   },
   {
      "x": "\"invalid parameter combination for AltiVec intrinsic\" error with ppc64le, g++ v7.4",
      "z": "Ahh!  I see, I over-corrected.  \nSo I put back vec_sldw on line 297, and the compile completed without error.  Whew!\nGranted, this was a test of the one file that had failed rather than a full build, but it does look good.",
      "y": "Ahh!  I see, I over-corrected.  \nSo I put back vec_sldw on line 297, and the compile completed without error.  Whew!\nGranted, this was a test of the one file that had failed rather than a full build, but it does look good."
   },
   {
      "x": "torch.linalg.cond return dtype inconsistent with doc and np.linalg.cond",
      "z": "`torch.linalg.cond` was changed to always return a real-valued tensor in https://github.com/pytorch/pytorch/pull/48284. We've discussed that with @kurtamohler and @mruberry yesterday and came to the conclusion it's the right thing to be divergent from NumPy in this case. This behavior should be fixed in NumPy and the issue was filed https://github.com/numpy/numpy/issues/18304.\n\nWhen normal `torch.linalg.cond` is called it should return the real number always (float32 for complex64 inputs, float64 for complex128 inputs). When the out variant is called float and complex tensors should be allowed according to the description of the correct \"out=\" behavior https://github.com/pytorch/pytorch/wiki/Developer-FAQ#how-does-out-work-in-pytorch.\n",
      "y": "`torch.linalg.cond` was changed to always return a real-valued tensor in https://github.com/pytorch/pytorch/pull/48284. We've discussed that with @kurtamohler and @mruberry yesterday and came to the conclusion it's the right thing to be divergent from NumPy in this case. This behavior should be fixed in NumPy and the issue was filed https://github.com/numpy/numpy/issues/18304.\n\nWhen normal `torch.linalg.cond` is called it should return the real number always (float32 for complex64 inputs, float64 for complex128 inputs). When the out variant is called float and complex tensors should be allowed according to the description of the correct \"out=\" behavior https://github.com/pytorch/pytorch/wiki/Developer-FAQ#how-does-out-work-in-pytorch.\n"
   },
   {
      "x": "Is `torch.multiprocessing.spawn` compatible with `DataLoader`?",
      "z": "hey @PetrochukM!\n\nI think this is less a PyTorch issue and something the Lightning team should investigate. \n\nI'll hijack this issue for now to make sure my thoughts are documented!\n\nForgive me since I wasn't there when the original DDP docs for Lightning were made and this caveat was introduced, but from working with DDP in a spawn setting vs using `Popen` like `torch.distributed.launch`, the main issue with spawn is the need to serialize all state objects. This becomes an issue when using functions defined at runtime like so:\n\nhttps://gist.github.com/SeanNaren/f4a99235fc736a438637c31ee94f082f\n\nTo fix you just need to move `TestModel` outside of the main script. I also tested with multiple workers and it seems fine. The message in the Lightning Docs may be outdated or incorrect, and if thats the case we should update it ASAP!",
      "y": "hey @PetrochukM!\n\nI think this is less a PyTorch issue and something the Lightning team should investigate. \n\nI'll hijack this issue for now to make sure my thoughts are documented!\n\nForgive me since I wasn't there when the original DDP docs for Lightning were made and this caveat was introduced, but from working with DDP in a spawn setting vs using `Popen` like `torch.distributed.launch`, the main issue with spawn is the need to serialize all state objects. This becomes an issue when using functions defined at runtime like so:\n\nhttps://gist.github.com/SeanNaren/f4a99235fc736a438637c31ee94f082f\n\nTo fix you just need to move `TestModel` outside of the main script. I also tested with multiple workers and it seems fine. The message in the Lightning Docs may be outdated or incorrect, and if thats the case we should update it ASAP!"
   },
   {
      "x": "Using LAPACK on Pi4 (64Bit Raspberry Pi OS RAM 8GB)",
      "z": "I've got a way to work out: \n1. https://github.com/ljk53/pytorch-rpi download torch-1.7.0a0-cp38-cp38-linux_aarch64.whl and pip3 install\n2. apt install libopenblas-dev\n3. success!",
      "y": "I've got a way to work out: \n1. https://github.com/ljk53/pytorch-rpi download torch-1.7.0a0-cp38-cp38-linux_aarch64.whl and pip3 install\n2. apt install libopenblas-dev\n3. success!"
   },
   {
      "x": "Quantized LeakyReLu Bug: Input tensor size determines the output values",
      "z": "Hi @Krosus , thanks again for the report.  After some internal discussion we decided that the discrepancy is acceptable, we are getting higher performance in the vectorized path, and the discrepancy will only be present if we are comparing tensors of different sizes.  Please let us know if this is blocking you, happy to help brainstorm workarounds.",
      "y": "Hi @Krosus , thanks again for the report.  After some internal discussion we decided that the discrepancy is acceptable, we are getting higher performance in the vectorized path, and the discrepancy will only be present if we are comparing tensors of different sizes.  Please let us know if this is blocking you, happy to help brainstorm workarounds."
   },
   {
      "x": "aten::normal_ not handled as a special op in RemoveTensorMutation pass.",
      "z": "The easiest fix is to probably just implement `normal`",
      "y": "The easiest fix is to probably just implement `normal`"
   },
   {
      "x": "test_variant_consistency_jit tests fail on CPU for min & max when dtype is bfloat16 & dim argument is passed",
      "z": "@imaginary-person Thanks for the update! Yes there are known problems with the BFloat16 operator in JIT, it's not fully supported. We have an issue #48978 (I see you've commented on it already) to figure out a way to clean up the test. You should be good to skip the BFloat16 dtype for the test_variant_consistency_jit tests without worry ",
      "y": "@imaginary-person Thanks for the update! Yes there are known problems with the BFloat16 operator in JIT, it's not fully supported. We have an issue #48978 (I see you've commented on it already) to figure out a way to clean up the test. You should be good to skip the BFloat16 dtype for the test_variant_consistency_jit tests without worry "
   },
   {
      "x": "[collect_env] Unable to collect CUDA version anymore",
      "z": "I encountered the same problem some time ago. Please see the description in https://github.com/k2-fsa/k2/pull/584\n\n- For CUDA 11.0\n```\n$ nvcc --version\nnvcc: NVIDIA (R) Cuda compiler driver\nCopyright (c) 2005-2020 NVIDIA Corporation\nBuilt on Thu_Jun_11_22:26:38_PDT_2020\nCuda compilation tools, release 11.0, V11.0.194\nBuild cuda_11.0_bu.TC445_37.28540450_0\n```\n\n- For CUDA 10.1\n```\n$ nvcc --version\nnvcc: NVIDIA (R) Cuda compiler driver\nCopyright (c) 2005-2019 NVIDIA Corporation\nBuilt on Sun_Jul_28_19:07:16_PDT_2019\nCuda compilation tools, release 10.1, V10.1.243\n```\n\nThere is an extra line `Build cuda_11.0_bu.TC445_37.28540450_0` for CUDA 11.0, which cannot be handled\nby `torch.utils.collect_env.get_running_cuda_version` since it uses a pattern `r\"V(.*)$\"` without `re.MULTILINE`.\nTherefore, `torch.utils.collect_env.get_running_cuda_version` returns `None` for CUDA 11.0",
      "y": "I encountered the same problem some time ago. Please see the description in https://github.com/k2-fsa/k2/pull/584\n\n- For CUDA 11.0\n```\n$ nvcc --version\nnvcc: NVIDIA (R) Cuda compiler driver\nCopyright (c) 2005-2020 NVIDIA Corporation\nBuilt on Thu_Jun_11_22:26:38_PDT_2020\nCuda compilation tools, release 11.0, V11.0.194\nBuild cuda_11.0_bu.TC445_37.28540450_0\n```\n\n- For CUDA 10.1\n```\n$ nvcc --version\nnvcc: NVIDIA (R) Cuda compiler driver\nCopyright (c) 2005-2019 NVIDIA Corporation\nBuilt on Sun_Jul_28_19:07:16_PDT_2019\nCuda compilation tools, release 10.1, V10.1.243\n```\n\nThere is an extra line `Build cuda_11.0_bu.TC445_37.28540450_0` for CUDA 11.0, which cannot be handled\nby `torch.utils.collect_env.get_running_cuda_version` since it uses a pattern `r\"V(.*)$\"` without `re.MULTILINE`.\nTherefore, `torch.utils.collect_env.get_running_cuda_version` returns `None` for CUDA 11.0"
   },
   {
      "x": "M1 release and nightly binaries",
      "z": "The compilation should be working just fine now. Note that some tests are failing though (for various reasons, not only M1): https://github.com/pytorch/pytorch/issues/56779\n\nYou can also get binaries from nightly by just following the instructions here: https://pytorch.org/get-started/locally/ (an x86 python will pull the x86 package and an arm64 python will pull the arm64 package).",
      "y": "The compilation should be working just fine now. Note that some tests are failing though (for various reasons, not only M1): https://github.com/pytorch/pytorch/issues/56779\n\nYou can also get binaries from nightly by just following the instructions here: https://pytorch.org/get-started/locally/ (an x86 python will pull the x86 package and an arm64 python will pull the arm64 package)."
   },
   {
      "x": "[FR] Initialize module on specified device",
      "z": "This might be related to the meta-learning paradigm, i.e. to use one network to predict parameters of another network. There are libraries made for this (https://github.com/tristandeleu/pytorch-meta/blob/master/torchmeta/modules/linear.py). It seems to be another potential use case of this feature.\n\nWith this feature and #53144 to allow creating a module on null/meta device, it seems we'll be able to do the following to easily achieve meta learning, reusing nn.Conv2d without having to create custom MetaConv2d:\n\n```python\ndef __init__():\n   self.meta_conv = Conv2d(..., device='meta')\n   self.parameter_predictor = PredictorNet(num_param=sum(k.numel() for k in self.meta_conv.parameters()))\n\ndef forward(x):\n  params = self.parameter_predictor(x)\n  with temporary_set_params(self.meta_conv, params):\n      return self.meta_conv(x)\n```",
      "y": "This might be related to the meta-learning paradigm, i.e. to use one network to predict parameters of another network. There are libraries made for this (https://github.com/tristandeleu/pytorch-meta/blob/master/torchmeta/modules/linear.py). It seems to be another potential use case of this feature.\n\nWith this feature and #53144 to allow creating a module on null/meta device, it seems we'll be able to do the following to easily achieve meta learning, reusing nn.Conv2d without having to create custom MetaConv2d:\n\n```python\ndef __init__():\n   self.meta_conv = Conv2d(..., device='meta')\n   self.parameter_predictor = PredictorNet(num_param=sum(k.numel() for k in self.meta_conv.parameters()))\n\ndef forward(x):\n  params = self.parameter_predictor(x)\n  with temporary_set_params(self.meta_conv, params):\n      return self.meta_conv(x)\n```"
   },
   {
      "x": "Please fix all the related links format from http to https",
      "z": "closed via https://github.com/pytorch/pytorch.github.io/pull/128",
      "y": "closed via https://github.com/pytorch/pytorch.github.io/pull/128"
   },
   {
      "x": "ImportError: libcurand.so.9.0: cannot open shared object file: No such file or directory",
      "z": "@ezyang  thanks for your help. I think it is because that I used to install old caffe2.  The old caffe2 `libcaffe2.so`, `libcaffe2_detectron_ops_gpu.so`,` libcaffe2_gpu.so`, `libcaffe2_module_test_dynamic.so`, `libcaffe2_observers.so` is in `/usr/local/lib`, but now new installed caffe2 they all in `pytorch/build/lib`. I delete all in  `/usr/local/lib`. Now it seems OK. do you think my approach is correct? thanks!",
      "y": "@ezyang  thanks for your help. I think it is because that I used to install old caffe2.  The old caffe2 `libcaffe2.so`, `libcaffe2_detectron_ops_gpu.so`,` libcaffe2_gpu.so`, `libcaffe2_module_test_dynamic.so`, `libcaffe2_observers.so` is in `/usr/local/lib`, but now new installed caffe2 they all in `pytorch/build/lib`. I delete all in  `/usr/local/lib`. Now it seems OK. do you think my approach is correct? thanks!"
   },
   {
      "x": "Exception in Thread: ValueError: signal number 32 out of range",
      "z": "I have solved the problem by updating Python3.5 to Python3.7",
      "y": "I have solved the problem by updating Python3.5 to Python3.7"
   },
   {
      "x": "[JIT] torch.tensor doesn't trace devices correctly",
      "z": "I have a tentative fix, that, unless someone explains me that this is a bad idea, I'll make into a PR.\nThe basic idea is to use a bit more tensor methods (in particular `Tensor.to(...)`) instead of doing this manually.\n\n```\niff --git a/torch/csrc/utils/tensor_new.cpp b/torch/csrc/utils/tensor_new.cpp\nindex f9d6ffc62..d8c939232 100644\n--- a/torch/csrc/utils/tensor_new.cpp\n+++ b/torch/csrc/utils/tensor_new.cpp\n@@ -244,7 +244,9 @@ Tensor internal_new_from_data(\n       (char*)tensor.data_ptr(), tensor.sizes(), tensor.strides(), 0,\n       scalarType, tensor.type().elementSizeInBytes(), data);\n   const auto& type_to_use = type_inference ? type.toScalarType(scalarType) : type;\n-  return new_with_type_conversion(type_to_use, tensor, device_index);\n+  auto device = device_opt.has_value() ? *device_opt : tensor.device();\n+  return tensor.to(device, type_to_use.scalarType(), /*blocking=*/false, /*copy=*/false);\n+  //return new_with_type_conversion(type_to_use, tensor, device_index);\n }\n \n Tensor new_from_data_copy(\n```\n",
      "y": "I have a tentative fix, that, unless someone explains me that this is a bad idea, I'll make into a PR.\nThe basic idea is to use a bit more tensor methods (in particular `Tensor.to(...)`) instead of doing this manually.\n\n```\niff --git a/torch/csrc/utils/tensor_new.cpp b/torch/csrc/utils/tensor_new.cpp\nindex f9d6ffc62..d8c939232 100644\n--- a/torch/csrc/utils/tensor_new.cpp\n+++ b/torch/csrc/utils/tensor_new.cpp\n@@ -244,7 +244,9 @@ Tensor internal_new_from_data(\n       (char*)tensor.data_ptr(), tensor.sizes(), tensor.strides(), 0,\n       scalarType, tensor.type().elementSizeInBytes(), data);\n   const auto& type_to_use = type_inference ? type.toScalarType(scalarType) : type;\n-  return new_with_type_conversion(type_to_use, tensor, device_index);\n+  auto device = device_opt.has_value() ? *device_opt : tensor.device();\n+  return tensor.to(device, type_to_use.scalarType(), /*blocking=*/false, /*copy=*/false);\n+  //return new_with_type_conversion(type_to_use, tensor, device_index);\n }\n \n Tensor new_from_data_copy(\n```\n"
   },
   {
      "x": "gradient difference between single GPU and multi-GPU DataParallel",
      "z": "the order of doing operations accumulates floating point errors in different ways. This is the reason for this difference, and it's expected.",
      "y": "the order of doing operations accumulates floating point errors in different ways. This is the reason for this difference, and it's expected."
   },
   {
      "x": "UserWarning: ONNX export failed on ATen operator _argmax because torch.onnx.symbolic._argmax does not exist",
      "z": "If I add the following function into torch/onnx/symbolic.py\n\n@parse_args('v', 'i', 'i')\ndef _argmax(g, self, dim, keepdim=None):\n    return g.op(\"ArgMax\", self, axis_i=dim, keepdim_i=keepdim)\n\n\nThen the export is able to proceed without error.\n",
      "y": "If I add the following function into torch/onnx/symbolic.py\n\n@parse_args('v', 'i', 'i')\ndef _argmax(g, self, dim, keepdim=None):\n    return g.op(\"ArgMax\", self, axis_i=dim, keepdim_i=keepdim)\n\n\nThen the export is able to proceed without error.\n"
   },
   {
      "x": "cmake error about rocrand",
      "z": "@NIEYALI I see. So to do ROCm build rocblas is a required dependency, so you need to install that. If you don't want to do ROCm build (but just happens you have partially installed ROCm before in your system), I have put up #14261 to allow disabling ROCm build with USE_ROCM=0 environment variable.",
      "y": "@NIEYALI I see. So to do ROCm build rocblas is a required dependency, so you need to install that. If you don't want to do ROCm build (but just happens you have partially installed ROCm before in your system), I have put up #14261 to allow disabling ROCm build with USE_ROCM=0 environment variable."
   },
   {
      "x": "[Aten] at::randint doesn't return a variable",
      "z": "The variable in the case of `at::normal` is because of `torch::zeros`. Functions in the `torch` namespace are by default `Variable`s.",
      "y": "The variable in the case of `at::normal` is because of `torch::zeros`. Functions in the `torch` namespace are by default `Variable`s."
   },
   {
      "x": "libtorch latest-deps cuasing an error: PyTorch script module file is too old",
      "z": "Nevermind -- re-exported model with new nightly & it worked fine.",
      "y": "Nevermind -- re-exported model with new nightly & it worked fine."
   },
   {
      "x": "Unable to pickle torch dtype objects in Python 3.5",
      "z": "Actually torch dtype object is already serializable. Closing....\n```\nIn [6]: b = copy.deepcopy(a)\n\nIn [7]: id(b)\nOut[7]: 139818768678472\n\nIn [8]: id(a)\nOut[8]: 139818768678472\n\nIn [9]: import pickle\n\nIn [10]: with open('/tmp/a', 'wb') as f:\n    ...:     pickle.dump(torch.float32, f)\n    ...:\n```\n",
      "y": "Actually torch dtype object is already serializable. Closing....\n```\nIn [6]: b = copy.deepcopy(a)\n\nIn [7]: id(b)\nOut[7]: 139818768678472\n\nIn [8]: id(a)\nOut[8]: 139818768678472\n\nIn [9]: import pickle\n\nIn [10]: with open('/tmp/a', 'wb') as f:\n    ...:     pickle.dump(torch.float32, f)\n    ...:\n```\n"
   },
   {
      "x": "do pytorch c++ jit trace run model need more gpu memory than python env of the same model?",
      "z": "It is possible, if you model has a bunch of in-place ops like relu_. We are adding mutability to JIT so this will be fixed eventually. cc @zou3519  ",
      "y": "It is possible, if you model has a bunch of in-place ops like relu_. We are adding mutability to JIT so this will be fixed eventually. cc @zou3519  "
   },
   {
      "x": "the device of tensor can not be change",
      "z": "> As I said in your previous issue `.to` returns new tensors, so you need to re-assign results of these two lines\n> \n> ```\n>         lp07.to(device)\n>         lp14.to(device)\n> ```\nthanks for answering my question. i figure out it by modify my code:\n`\n            lp07 = lp07.to(device)\n            lp14 = lp14.to(device)\n`\nbut i don't know why should i do this. In pytorch 0.3.0, the tensor just use:\n`tensor.cuda()`",
      "y": "> As I said in your previous issue `.to` returns new tensors, so you need to re-assign results of these two lines\n> \n> ```\n>         lp07.to(device)\n>         lp14.to(device)\n> ```\nthanks for answering my question. i figure out it by modify my code:\n`\n            lp07 = lp07.to(device)\n            lp14 = lp14.to(device)\n`\nbut i don't know why should i do this. In pytorch 0.3.0, the tensor just use:\n`tensor.cuda()`"
   },
   {
      "x": "torch.full and torch.randint are inconsistent in arg order",
      "z": "I think the intention was probably to keep PyTorch's syntax aligned with NumPy's syntax, because this is how NumPy does it too.",
      "y": "I think the intention was probably to keep PyTorch's syntax aligned with NumPy's syntax, because this is how NumPy does it too."
   },
   {
      "x": "JIT pickler should support both little endian and big endian systems",
      "z": "Endianness is also part of the pickle format itself and should be handled on loading / saving (the binary output from a big endian or little endian system should be the same), so if there are issues with endianness in the pickler that should be considered a bug in our implementation",
      "y": "Endianness is also part of the pickle format itself and should be handled on loading / saving (the binary output from a big endian or little endian system should be the same), so if there are issues with endianness in the pickler that should be considered a bug in our implementation"
   },
   {
      "x": "Improve the way RPC unit tests are skipped for windows",
      "z": "I think `@unittest.skipIf(not dist.is_available())` would be much cleaner and we should probably add it to the test class rather than functions. Regarding the try-catch, why do we need that in the test file? The imports don't throw at the moment.",
      "y": "I think `@unittest.skipIf(not dist.is_available())` would be much cleaner and we should probably add it to the test class rather than functions. Regarding the try-catch, why do we need that in the test file? The imports don't throw at the moment."
   },
   {
      "x": "Build pytorch from source in osx",
      "z": "I've installed Pytorch from source on Mac OS successfully. You can take a look at my tutorial: https://zhaoyu.li/post/install-pytorch-on-mac-with-nvidia-gpu/",
      "y": "I've installed Pytorch from source on Mac OS successfully. You can take a look at my tutorial: https://zhaoyu.li/post/install-pytorch-on-mac-with-nvidia-gpu/"
   },
   {
      "x": "Python crash during backward",
      "z": "Resolved in latest builds.",
      "y": "Resolved in latest builds."
   },
   {
      "x": "Upsampling is not implemented for 1D (temporal) inputs",
      "z": "i think i know this.\n\n```\npip uninstall torch\npip uninstall torch\npython setup.py clean\npython setup.py build develop\n```",
      "y": "i think i know this.\n\n```\npip uninstall torch\npip uninstall torch\npython setup.py clean\npython setup.py build develop\n```"
   },
   {
      "x": "With backward method of CUDA Variables, grad is None if gradient is supplied",
      "z": "try this:\n```\n_x = Variable(torch.ones(10), requires_grad=True)\nx = _x.cuda()\ny = x * Variable(torch.linspace(1, 10, 10), requires_grad=False).cuda()\ny.backward(torch.ones(10).cuda())\nprint(x.grad)\nprint(_x.grad)\n```\nThis is because x is an intermediate node.\nAlternative is:\n```\nx = Variable(torch.ones(10).cuda(), requires_grad=True)\ny = x * Variable(torch.linspace(1, 10, 10), requires_grad=False).cuda()\ny.backward(torch.ones(10).cuda())\nprint(x.grad)\n```",
      "y": "try this:\n```\n_x = Variable(torch.ones(10), requires_grad=True)\nx = _x.cuda()\ny = x * Variable(torch.linspace(1, 10, 10), requires_grad=False).cuda()\ny.backward(torch.ones(10).cuda())\nprint(x.grad)\nprint(_x.grad)\n```\nThis is because x is an intermediate node.\nAlternative is:\n```\nx = Variable(torch.ones(10).cuda(), requires_grad=True)\ny = x * Variable(torch.linspace(1, 10, 10), requires_grad=False).cuda()\ny.backward(torch.ones(10).cuda())\nprint(x.grad)\n```"
   },
   {
      "x": "from torch._C import * ImportError: numpy.core.multiarray failed to import",
      "z": "$ pip install numpy -I \nhas worked on me, I'll close the issue. But I don't know why does $ pip install numpy --upgrade not work.\n(py35) user@user-ASUS:~$ pip install numpy -I  \nCollecting numpy\n  Using cached numpy-1.13.1-cp35-cp35m-manylinux1_x86_64.whl\nInstalling collected packages: numpy\nSuccessfully installed numpy-1.13.1\n\n(py35) user@user-ASUS:~$ python\nPython 3.5.3 |Continuum Analytics, Inc.| (default, Mar  6 2017, 11:58:13) \n[GCC 4.4.7 20120313 (Red Hat 4.4.7-1)] on linux\nType \"help\", \"copyright\", \"credits\" or \"license\" for more information.\n>>> import torch\n>>> x = torch.Tensor(2,3)\n>>> x\n\n 1.6534e+05  4.5673e-41  1.3032e-37\n 0.0000e+00  4.4842e-44  0.0000e+00\n[torch.FloatTensor of size 2x3]\n",
      "y": "$ pip install numpy -I \nhas worked on me, I'll close the issue. But I don't know why does $ pip install numpy --upgrade not work.\n(py35) user@user-ASUS:~$ pip install numpy -I  \nCollecting numpy\n  Using cached numpy-1.13.1-cp35-cp35m-manylinux1_x86_64.whl\nInstalling collected packages: numpy\nSuccessfully installed numpy-1.13.1\n\n(py35) user@user-ASUS:~$ python\nPython 3.5.3 |Continuum Analytics, Inc.| (default, Mar  6 2017, 11:58:13) \n[GCC 4.4.7 20120313 (Red Hat 4.4.7-1)] on linux\nType \"help\", \"copyright\", \"credits\" or \"license\" for more information.\n>>> import torch\n>>> x = torch.Tensor(2,3)\n>>> x\n\n 1.6534e+05  4.5673e-41  1.3032e-37\n 0.0000e+00  4.4842e-44  0.0000e+00\n[torch.FloatTensor of size 2x3]\n"
   },
   {
      "x": "tensor.rand() and uniform_() returns numbers from [0,1] (right-hand inclusive)",
      "z": "Here's a small test, reproducing device- and host-side generation, as implemented in TH/THC:\n```\n#include <curand_globals.h>\n#include <limits.h>\n#include <iostream>\n#include <iomanip>\n\nint main(){\n   uint x = UINT_MAX;\n   double d = x * CURAND_2POW32_INV_DOUBLE + (CURAND_2POW32_INV_DOUBLE/2.0f);\n   std::cout << std::setprecision(14) << \"double max \" << d << \"\\n\";\n   float f = x * CURAND_2POW32_INV + (CURAND_2POW32_INV/2.0f);\n   std::cout << \"float max \" << f << \"\\n\";\n//TH backend\n   double dc = UINT_MAX * (1.0/4294967296.0);   \n   std::cout << std::setprecision(14) << \"double max cpu \" << dc << \"\\n\"; \n   float fc = (float)(UINT_MAX * (1.0/4294967296.0));\n   std::cout << std::setprecision(14) << \"float max cpu \" << fc << \"\\n\"; \n  \n}\n```\nOnly cpu-side double generation is doing what is promised: [0, 1). cpu-side for floats of course rounds max to 1, thus generating [0 1]\ngpu-side for floats generates (0,1], as promised by curand manual, but that's not what torch manual specifies. \ngpu-side for doubles has a bug, thus generates (0,1), contrary to what's promised in the manual. \nReplacing x by (1-x) is expected to fail because of the different density of fp numbers near 0 and near 1 (thus, when 1-x is calculated for a small non-zero x, result will be rounded to 1, so (0,1] range will essentially be mapped to [0 1] range). \n",
      "y": "Here's a small test, reproducing device- and host-side generation, as implemented in TH/THC:\n```\n#include <curand_globals.h>\n#include <limits.h>\n#include <iostream>\n#include <iomanip>\n\nint main(){\n   uint x = UINT_MAX;\n   double d = x * CURAND_2POW32_INV_DOUBLE + (CURAND_2POW32_INV_DOUBLE/2.0f);\n   std::cout << std::setprecision(14) << \"double max \" << d << \"\\n\";\n   float f = x * CURAND_2POW32_INV + (CURAND_2POW32_INV/2.0f);\n   std::cout << \"float max \" << f << \"\\n\";\n//TH backend\n   double dc = UINT_MAX * (1.0/4294967296.0);   \n   std::cout << std::setprecision(14) << \"double max cpu \" << dc << \"\\n\"; \n   float fc = (float)(UINT_MAX * (1.0/4294967296.0));\n   std::cout << std::setprecision(14) << \"float max cpu \" << fc << \"\\n\"; \n  \n}\n```\nOnly cpu-side double generation is doing what is promised: [0, 1). cpu-side for floats of course rounds max to 1, thus generating [0 1]\ngpu-side for floats generates (0,1], as promised by curand manual, but that's not what torch manual specifies. \ngpu-side for doubles has a bug, thus generates (0,1), contrary to what's promised in the manual. \nReplacing x by (1-x) is expected to fail because of the different density of fp numbers near 0 and near 1 (thus, when 1-x is calculated for a small non-zero x, result will be rounded to 1, so (0,1] range will essentially be mapped to [0 1] range). \n"
   },
   {
      "x": "tensor[...,None] adds unit axis to wrong dimension (inconsistent with numpy)",
      "z": "fixed in master. soon to be in nightlies that we're building.",
      "y": "fixed in master. soon to be in nightlies that we're building."
   },
   {
      "x": "How to extract middle layer features",
      "z": "pytorch is built around programs, not graphs. copying/modifying a forward function and returning the particular layer you want is the right way to do things. If you want to return a middle layer more conveniently, write your model `forward` to take a `name` string as well, and return that `name` layer.",
      "y": "pytorch is built around programs, not graphs. copying/modifying a forward function and returning the particular layer you want is the right way to do things. If you want to return a middle layer more conveniently, write your model `forward` to take a `name` string as well, and return that `name` layer."
   },
   {
      "x": "topk cudaerror traceback...",
      "z": "This is not related to topk itself, but because the operation is pretty intense / taking too long on your GPU.\nThe NVIDIA driver realizes that the operation is taking too long, and **because an active display is also attached to this GPU**, it kicks out the CUDA kernel.\n\nThis thread gives more context: https://devtalk.nvidia.com/default/topic/483643/cuda-the-launch-timed-out-and-was-terminated/\n\nThere's not a whole lot we can do from the pytorch end.\n\nWhat GPU do you have? Is it an option to not turn on the display on the GPU (maybe use this GPU purely over SSH). Is it an option to run your code on a second gpu (if you have two GPUs)",
      "y": "This is not related to topk itself, but because the operation is pretty intense / taking too long on your GPU.\nThe NVIDIA driver realizes that the operation is taking too long, and **because an active display is also attached to this GPU**, it kicks out the CUDA kernel.\n\nThis thread gives more context: https://devtalk.nvidia.com/default/topic/483643/cuda-the-launch-timed-out-and-was-terminated/\n\nThere's not a whole lot we can do from the pytorch end.\n\nWhat GPU do you have? Is it an option to not turn on the display on the GPU (maybe use this GPU purely over SSH). Is it an option to run your code on a second gpu (if you have two GPUs)"
   },
   {
      "x": "Variable input size training is slow",
      "z": "do you set `cudnn.benchmark=True` anywhere in your code? that is probably the culprit.",
      "y": "do you set `cudnn.benchmark=True` anywhere in your code? that is probably the culprit."
   },
   {
      "x": "Can't import saved pytorch model",
      "z": "Update:\n\nI finally figure it out the way to make it right.\nsolution for those who want to use SDN pox controller:\nadd the \"hparamDict\" definition before boot() function in the ~/any/pox/path/pox.py\n\nI am wondering if there is a way to avoid this? I mean avoiding copy the definition source code when load model. I save my model with this method \"torch.save(model.state_dict())\".\nThe attribute missing is actually a dict class I define for hyperparameters.\n\n=========================================================\nHi, guys. any solutions?\n\nI got the same problem. In fact, I tried to use pytorch with SDN controller POX. I can not load model because of the similar problem. \nSaying \"'module' object has no attribute 'hparamDict'\".\n\nI am very frustrated. I import and add the code of definition just right before torch.load(*). If this can not be sovled I have to reimplement all my experiments in tensorflow and give up pytorch, which is quite painful.\nThe command I use is \"./pox.py pytorch_model\"\n",
      "y": "Update:\n\nI finally figure it out the way to make it right.\nsolution for those who want to use SDN pox controller:\nadd the \"hparamDict\" definition before boot() function in the ~/any/pox/path/pox.py\n\nI am wondering if there is a way to avoid this? I mean avoiding copy the definition source code when load model. I save my model with this method \"torch.save(model.state_dict())\".\nThe attribute missing is actually a dict class I define for hyperparameters.\n\n=========================================================\nHi, guys. any solutions?\n\nI got the same problem. In fact, I tried to use pytorch with SDN controller POX. I can not load model because of the similar problem. \nSaying \"'module' object has no attribute 'hparamDict'\".\n\nI am very frustrated. I import and add the code of definition just right before torch.load(*). If this can not be sovled I have to reimplement all my experiments in tensorflow and give up pytorch, which is quite painful.\nThe command I use is \"./pox.py pytorch_model\"\n"
   },
   {
      "x": "LSTM architecture has met a explosive growth in the training process",
      "z": "I had this problem just earlier today and I was implementing an RNN as well.  it went away  when i properly detatched my hidden states using h = h.detach() ; c = c.detach(). The model was backpropagating the hidden state further than it should and it was causing massive memory usage. Not sure if thats your problem but I cant see detach or .'repackage_hidden' in your code so that could be it.",
      "y": "I had this problem just earlier today and I was implementing an RNN as well.  it went away  when i properly detatched my hidden states using h = h.detach() ; c = c.detach(). The model was backpropagating the hidden state further than it should and it was causing massive memory usage. Not sure if thats your problem but I cant see detach or .'repackage_hidden' in your code so that could be it."
   },
   {
      "x": "TypeError: Type torch.LongTensor doesn't implement stateless method mean",
      "z": "We dont implement `mean` for LongTensor.\nYou can do: `y=torch.mean(x.float())`",
      "y": "We dont implement `mean` for LongTensor.\nYou can do: `y=torch.mean(x.float())`"
   },
   {
      "x": "None Grad with Custom Loss",
      "z": "This line seems the one which interferes with the gradient:\n```\n            score_final = score_final * (score[..., i] <= 0).float()\n```",
      "y": "This line seems the one which interferes with the gradient:\n```\n            score_final = score_final * (score[..., i] <= 0).float()\n```"
   },
   {
      "x": "[request] Encode/decode variables",
      "z": "I can see two solutions to this issue:\n\n1. Having `encode` and `decode` functions on class Variable. Every time the variable is computed/stored these functions will be called.\n2. Allowing `register_hook` to accept a keyword argument `check`, which will default to `True`. if `check == True`, [python_hook.cpp#L147] will be run.\n\n[python_hook.cpp#L147]:https://github.com/pytorch/pytorch/blob/1290e586fbc3d6266423f3417723d6620267054b/torch/csrc/autograd/python_hook.cpp#L147",
      "y": "I can see two solutions to this issue:\n\n1. Having `encode` and `decode` functions on class Variable. Every time the variable is computed/stored these functions will be called.\n2. Allowing `register_hook` to accept a keyword argument `check`, which will default to `True`. if `check == True`, [python_hook.cpp#L147] will be run.\n\n[python_hook.cpp#L147]:https://github.com/pytorch/pytorch/blob/1290e586fbc3d6266423f3417723d6620267054b/torch/csrc/autograd/python_hook.cpp#L147"
   },
   {
      "x": "no module named reduction",
      "z": "Just in case anyone else stumbles into this issue like I just did:\n\nYou will see this issue if your current working directory is `site-packages/torch`, in which case any `import multiprocessing` will import torch's own conveniently named `multiprocessing` package, and not the default one.\n\n(I saw this, because I was diagnosing LD_LIBRARY_PATH issues with my wheel build of PyTorch 1.0 preview + CUDA 10.)",
      "y": "Just in case anyone else stumbles into this issue like I just did:\n\nYou will see this issue if your current working directory is `site-packages/torch`, in which case any `import multiprocessing` will import torch's own conveniently named `multiprocessing` package, and not the default one.\n\n(I saw this, because I was diagnosing LD_LIBRARY_PATH issues with my wheel build of PyTorch 1.0 preview + CUDA 10.)"
   },
   {
      "x": "Constructing a ParameterDict raises a warning",
      "z": "Thanks, I am ignoring it now with:\n\n```\nimport warnings\nwarnings.filterwarnings(\"ignore\", message=\"Setting attributes on ParameterDict is not supported.\")\n```\n\nI will wait for the new release!!",
      "y": "Thanks, I am ignoring it now with:\n\n```\nimport warnings\nwarnings.filterwarnings(\"ignore\", message=\"Setting attributes on ParameterDict is not supported.\")\n```\n\nI will wait for the new release!!"
   },
   {
      "x": "flake8 errors are not shown by github actions",
      "z": "Good catch, thanks! I'll go ahead and fix that, sorry for introducing the regression",
      "y": "Good catch, thanks! I'll go ahead and fix that, sorry for introducing the regression"
   },
   {
      "x": "RuntimeError: \"mul_cuda\" not implemented for 'Bool'",
      "z": "Maybe just changing, includeBool to `true` will work. https://github.com/pytorch/pytorch/blob/cd26d027b3357cd913412ee13060cdbb28fc178a/aten/src/ATen/native/cuda/BinaryMulDivKernel.cu#L68\n",
      "y": "Maybe just changing, includeBool to `true` will work. https://github.com/pytorch/pytorch/blob/cd26d027b3357cd913412ee13060cdbb28fc178a/aten/src/ATen/native/cuda/BinaryMulDivKernel.cu#L68\n"
   },
   {
      "x": "[POLL][RFC] Can we retire Single-Process Multi-Device Mode from DistributedDataParallel?",
      "z": "The ranking model distributed setting (where the many negatives benefits) has been de-prioritized in ParlAI due to advancements in generative models. It's acceptable for me to lose that functionality now.",
      "y": "The ranking model distributed setting (where the many negatives benefits) has been de-prioritized in ParlAI due to advancements in generative models. It's acceptable for me to lose that functionality now."
   },
   {
      "x": "Convolution operations are extremely slow on RTX 30 series GPU",
      "z": "> The binaries use cudnn8.0.3, which doesn't ship with tuned heuristics for 3090 and cudnn8.0.5 will provide them.\n> Until then performance regressions on these devices are unfortunately expected.\n\nVery thanks for your help.\ud83d\ude03\nSo the solution may be: I should compile PyTorch from source once when cuDNN 8.0.5 is available\uff1f",
      "y": "> The binaries use cudnn8.0.3, which doesn't ship with tuned heuristics for 3090 and cudnn8.0.5 will provide them.\n> Until then performance regressions on these devices are unfortunately expected.\n\nVery thanks for your help.\ud83d\ude03\nSo the solution may be: I should compile PyTorch from source once when cuDNN 8.0.5 is available\uff1f"
   },
   {
      "x": "torch.arange numerics are different after 1.7 update on CPU",
      "z": "I tried using intrinsics to see if it would avoid FMA, and it didn't work at first--but then I found a way to get it working. First, I changed from this\n\n```\n  template<typename step_t>\n  static Vec256<double> arange_index_offset(double base = 0., step_t step = static_cast<step_t>(1), int64_t index_offset = 0) {\n    return Vec256<double>(\n        base + step * index_offset,\n        base + step * (index_offset + 1),\n        base + step * (index_offset + 2),\n        base + step * (index_offset + 3));\n  }\n```\n\nto this:\n```\n  template<typename step_t>\n  static Vec256<double> arange_index_offset(double base = 0., step_t step = static_cast<step_t>(1), int64_t index_offset = 0) {\n    Vec256<double> base_v(base, base, base, base);\n    Vec256<double> step_v(step, step, step, step);\n    Vec256<double> index_v(index_offset, index_offset + 1, index_offset + 2, index_offset + 3); \n    return _mm256_add_pd(base_v, _mm256_mul_pd(step_v, index_v));\n  }\n```\n\nEvidently the `_mm256_add_pd` and `_mm256_mul_pd` calls get combined such that I'm effectively calling `_mm256_fmadd_pd`, so this didn't fix the problem. But then I wondered, what would happen if I just used the multiply intrinsic and then used regular add operations? Like this:\n\n```\n  template<typename step_t>\n  static Vec256<double> arange_index_offset(double base = 0., step_t step = static_cast<step_t>(1), int64_t index_offset = 0) {\n    Vec256<double> index_v(index_offset, index_offset + 1, index_offset + 2, index_offset + 3); \n    Vec256<double> step_v(step, step, step, step);\n    Vec256<double> tmp = _mm256_mul_pd(step_v, index_v);\n    return Vec256<double>(\n      base + tmp.values[0],\n      base + tmp.values[1],\n      base + tmp.values[2],\n      base + tmp.values[3]);\n  }\n```\n\nThis worked! Since I'm using an intrinsic only for the multiply and not the add, the two operations don't get combined, and now we get the proper behavior for the example we've been looking at:\n\n```\n$ ATEN_CPU_CAPABILITY=avx2 python\n>>> import torch\n>>> torch.arange(-5, 5, 1.4, device='cpu', dtype=torch.float64).floor()\ntensor([-5., -4., -3., -1.,  0.,  2.,  3.,  4.], dtype=torch.float64)\n>>> torch.arange(-5, 5, 1.4, device='cpu', dtype=torch.float64).storage()\n -5.0\n -3.6\n -2.2\n -0.8000000000000007\n 0.5999999999999996\n 2.0      <---- NOTE: this is exactly 2 now, and not 1.99999... like before\n 3.3999999999999986\n 4.799999999999999\n[torch.DoubleStorage of size 8]\n```\n\nThis result is exactly the same for `ATEN_CPU_CAPABILITY=avx2`, `ATEN_CPU_CAPABILITY=avx`, and `ATEN_CPU_CAPABILITY=default`. And it agrees with the Pytorch 1.6 result as well.\n\nI'll admit that it's a bit of an odd solution, but it does work. Is it alright if we go ahead with this?",
      "y": "I tried using intrinsics to see if it would avoid FMA, and it didn't work at first--but then I found a way to get it working. First, I changed from this\n\n```\n  template<typename step_t>\n  static Vec256<double> arange_index_offset(double base = 0., step_t step = static_cast<step_t>(1), int64_t index_offset = 0) {\n    return Vec256<double>(\n        base + step * index_offset,\n        base + step * (index_offset + 1),\n        base + step * (index_offset + 2),\n        base + step * (index_offset + 3));\n  }\n```\n\nto this:\n```\n  template<typename step_t>\n  static Vec256<double> arange_index_offset(double base = 0., step_t step = static_cast<step_t>(1), int64_t index_offset = 0) {\n    Vec256<double> base_v(base, base, base, base);\n    Vec256<double> step_v(step, step, step, step);\n    Vec256<double> index_v(index_offset, index_offset + 1, index_offset + 2, index_offset + 3); \n    return _mm256_add_pd(base_v, _mm256_mul_pd(step_v, index_v));\n  }\n```\n\nEvidently the `_mm256_add_pd` and `_mm256_mul_pd` calls get combined such that I'm effectively calling `_mm256_fmadd_pd`, so this didn't fix the problem. But then I wondered, what would happen if I just used the multiply intrinsic and then used regular add operations? Like this:\n\n```\n  template<typename step_t>\n  static Vec256<double> arange_index_offset(double base = 0., step_t step = static_cast<step_t>(1), int64_t index_offset = 0) {\n    Vec256<double> index_v(index_offset, index_offset + 1, index_offset + 2, index_offset + 3); \n    Vec256<double> step_v(step, step, step, step);\n    Vec256<double> tmp = _mm256_mul_pd(step_v, index_v);\n    return Vec256<double>(\n      base + tmp.values[0],\n      base + tmp.values[1],\n      base + tmp.values[2],\n      base + tmp.values[3]);\n  }\n```\n\nThis worked! Since I'm using an intrinsic only for the multiply and not the add, the two operations don't get combined, and now we get the proper behavior for the example we've been looking at:\n\n```\n$ ATEN_CPU_CAPABILITY=avx2 python\n>>> import torch\n>>> torch.arange(-5, 5, 1.4, device='cpu', dtype=torch.float64).floor()\ntensor([-5., -4., -3., -1.,  0.,  2.,  3.,  4.], dtype=torch.float64)\n>>> torch.arange(-5, 5, 1.4, device='cpu', dtype=torch.float64).storage()\n -5.0\n -3.6\n -2.2\n -0.8000000000000007\n 0.5999999999999996\n 2.0      <---- NOTE: this is exactly 2 now, and not 1.99999... like before\n 3.3999999999999986\n 4.799999999999999\n[torch.DoubleStorage of size 8]\n```\n\nThis result is exactly the same for `ATEN_CPU_CAPABILITY=avx2`, `ATEN_CPU_CAPABILITY=avx`, and `ATEN_CPU_CAPABILITY=default`. And it agrees with the Pytorch 1.6 result as well.\n\nI'll admit that it's a bit of an odd solution, but it does work. Is it alright if we go ahead with this?"
   },
   {
      "x": "`F.grid_sample` fails to dispatch correctly when args are of different subclasses",
      "z": "Many thanks for these suggestions @hameerabbasi !\n\nI'm happy to go the wrapping approach, although iirc i'd been warned off it before as being hard to get full compatibility\n\nI've tried this\n\n```\nclass TensorBase:\n    def __init__(self, data, metadata=None, **kwargs):\n        self._fa_tensor = tensor(data)\n        for k,v in kwargs.items(): setattr(self, k, v)\n    def __torch_function__(self, func, types, args=(), kwargs=None):\n        if kwargs is None: kwargs = {}\n        args = [getattr(a,'_fa_tensor',a) for a in args]\n        ret = func(*args, **kwargs)\n        return TensorBase(ret, **self.__dict__)\n    def __getattr__(self, k): return getattr(self._fa_tensor, k)\n    def __getitem__(self, k): return self._fa_tensor[k]\n```\n\nBut I'm not sure how to support stuff like `+` with an int, without doing something a bit hacky",
      "y": "Many thanks for these suggestions @hameerabbasi !\n\nI'm happy to go the wrapping approach, although iirc i'd been warned off it before as being hard to get full compatibility\n\nI've tried this\n\n```\nclass TensorBase:\n    def __init__(self, data, metadata=None, **kwargs):\n        self._fa_tensor = tensor(data)\n        for k,v in kwargs.items(): setattr(self, k, v)\n    def __torch_function__(self, func, types, args=(), kwargs=None):\n        if kwargs is None: kwargs = {}\n        args = [getattr(a,'_fa_tensor',a) for a in args]\n        ret = func(*args, **kwargs)\n        return TensorBase(ret, **self.__dict__)\n    def __getattr__(self, k): return getattr(self._fa_tensor, k)\n    def __getitem__(self, k): return self._fa_tensor[k]\n```\n\nBut I'm not sure how to support stuff like `+` with an int, without doing something a bit hacky"
   },
   {
      "x": "torch.fft does not give the same result as torch.stft",
      "z": "The difference is because `torch.fft`'s second argument isn't the transform axis, instead it's `signal_ndim` or the number of dimensions to transform. So, `torch.fft(x_torch, 3)` is actually equivalent to `scipy.fft.fftn(x_scipy, axes=(-1, -2, -3))`. For a more `numpy`-like interface use the new [`torch.fft.fft`](https://pytorch.org/docs/master/fft.html#torch.fft.fft) function, or if you're stuck with PyTorch 1.6 you can use `transpose(axis, -2)` to move the desired transform axis into the right place:\n\n```\nS_torch = torch.fft(x_torch.transpose(1, -2), signal_ndim=1).transpose(1, -2)\n```\nWhich results in:\n![image](https://user-images.githubusercontent.com/13238737/97711956-abe13d00-1ab5-11eb-9910-7fa99580663f.png)",
      "y": "The difference is because `torch.fft`'s second argument isn't the transform axis, instead it's `signal_ndim` or the number of dimensions to transform. So, `torch.fft(x_torch, 3)` is actually equivalent to `scipy.fft.fftn(x_scipy, axes=(-1, -2, -3))`. For a more `numpy`-like interface use the new [`torch.fft.fft`](https://pytorch.org/docs/master/fft.html#torch.fft.fft) function, or if you're stuck with PyTorch 1.6 you can use `transpose(axis, -2)` to move the desired transform axis into the right place:\n\n```\nS_torch = torch.fft(x_torch.transpose(1, -2), signal_ndim=1).transpose(1, -2)\n```\nWhich results in:\n![image](https://user-images.githubusercontent.com/13238737/97711956-abe13d00-1ab5-11eb-9910-7fa99580663f.png)"
   },
   {
      "x": "Legacy tensor ctor returns uninitialized tensor when input and output device differ",
      "z": "> When the input is a 0-D or 1-size tensor, behavior depends on the dtype: int32: uninitialized tensor\n\nwhat is happening here is that the constructor is interpreting the Tensor as an IntArrayRef because a 1-element integer tensor passed PyLong_Check.",
      "y": "> When the input is a 0-D or 1-size tensor, behavior depends on the dtype: int32: uninitialized tensor\n\nwhat is happening here is that the constructor is interpreting the Tensor as an IntArrayRef because a 1-element integer tensor passed PyLong_Check."
   },
   {
      "x": "Independent Distribution Wrapper Disguises Negative StdDev in Underlying Normal Distribution",
      "z": "Hi @decodyng, I think the best we can guarantee in the `torch.distributions` library is to correctly catch errors *when validation is enabled*. I believe this error would have been caught earlier if you had initially called\n```py\ntorch.distributions.Distribution.set_default_validate_args(True)\n```\nIn fact we recently [enabled validation by default](https://github.com/pyro-ppl/pyro/pull/2701) in Pyro (a downstream library). If this seems useful we could consider enabling validation by default also in PyTorch. What's your opinion?",
      "y": "Hi @decodyng, I think the best we can guarantee in the `torch.distributions` library is to correctly catch errors *when validation is enabled*. I believe this error would have been caught earlier if you had initially called\n```py\ntorch.distributions.Distribution.set_default_validate_args(True)\n```\nIn fact we recently [enabled validation by default](https://github.com/pyro-ppl/pyro/pull/2701) in Pyro (a downstream library). If this seems useful we could consider enabling validation by default also in PyTorch. What's your opinion?"
   },
   {
      "x": "torch.trace type promotion behavior is different on CPU vs CUDA",
      "z": "1.6.0:\n```\n>>> import torch\n>>> torch.__version__\n'1.6.0'\n>>> x = torch.ones(5, 5, dtype=torch.uint8)\n>>> x = x.cuda()\n>>> x.trace()\ntensor(5, device='cuda:0')\n>>> x.trace().dtype\ntorch.int64\n```\n\n1.5.1:\n```\n>>> import torch\n>>> torch.__version__\n'1.5.1'\n>>> x = torch.ones(5, 5, dtype=torch.uint8)\n>>> x = x.cuda()\n>>> x.trace()\ntensor(5, device='cuda:0', dtype=torch.uint8)\n```",
      "y": "1.6.0:\n```\n>>> import torch\n>>> torch.__version__\n'1.6.0'\n>>> x = torch.ones(5, 5, dtype=torch.uint8)\n>>> x = x.cuda()\n>>> x.trace()\ntensor(5, device='cuda:0')\n>>> x.trace().dtype\ntorch.int64\n```\n\n1.5.1:\n```\n>>> import torch\n>>> torch.__version__\n'1.5.1'\n>>> x = torch.ones(5, 5, dtype=torch.uint8)\n>>> x = x.cuda()\n>>> x.trace()\ntensor(5, device='cuda:0', dtype=torch.uint8)\n```"
   },
   {
      "x": "Jit Error with CUDA and FP16 -- identifier \"aten_add_flat__1\" is undefined",
      "z": "The fix for this wasn't cherry picked into 1.7.1.  It should be in nightly tho.",
      "y": "The fix for this wasn't cherry picked into 1.7.1.  It should be in nightly tho."
   },
   {
      "x": "Distributed weight updates",
      "z": "@jianjiandandande \n\nAfter calling `loss.backward()`, can you try the following?\n\n```python\nfor name, param in m.named_parameters():\n  if not param.grad:\n    print(f\"detected unused parameter: {name}\")\n```\n\nThis should tell you what parameters are not used. \n\nBTW, any reason for not using [`DistributedDataParallel`](https://pytorch.org/docs/stable/notes/ddp.html) and set `find_unused_parameters=True`? \n",
      "y": "@jianjiandandande \n\nAfter calling `loss.backward()`, can you try the following?\n\n```python\nfor name, param in m.named_parameters():\n  if not param.grad:\n    print(f\"detected unused parameter: {name}\")\n```\n\nThis should tell you what parameters are not used. \n\nBTW, any reason for not using [`DistributedDataParallel`](https://pytorch.org/docs/stable/notes/ddp.html) and set `find_unused_parameters=True`? \n"
   },
   {
      "x": "How to add PyTorch to requirements.txt",
      "z": "Looks like this works:\n\n```\n--find-links https://download.pytorch.org/whl/torch_stable.html\ntorch==1.3.1+cpu\n```\n\nThanks for the help!",
      "y": "Looks like this works:\n\n```\n--find-links https://download.pytorch.org/whl/torch_stable.html\ntorch==1.3.1+cpu\n```\n\nThanks for the help!"
   },
   {
      "x": "Use non-system cuda path",
      "z": "@mahmoodn I assume you are building from source right? \nTry setting `CUDA_HOME` env?",
      "y": "@mahmoodn I assume you are building from source right? \nTry setting `CUDA_HOME` env?"
   },
   {
      "x": "list of registered buffers does not move to cuda",
      "z": "@xonobo \nSo if you do `print(list(a.buffers()))` you'll see your buffer are on CUDA device. \n`a.params` as an attribute will be copied when `register_buffer` and `params` won't move along with `.to()`. But the buffer does. \nPlease let us know if this doesn't solve your question. Thanks!",
      "y": "@xonobo \nSo if you do `print(list(a.buffers()))` you'll see your buffer are on CUDA device. \n`a.params` as an attribute will be copied when `register_buffer` and `params` won't move along with `.to()`. But the buffer does. \nPlease let us know if this doesn't solve your question. Thanks!"
   },
   {
      "x": "Memory leak with Conv1d on CPU",
      "z": "Wow, indeed, `LRU_CACHE_CAPACITY=1` (from https://github.com/pytorch/pytorch/issues/27971) solves my issue!\n\nThanks a lot for pointing out, @ezyang!",
      "y": "Wow, indeed, `LRU_CACHE_CAPACITY=1` (from https://github.com/pytorch/pytorch/issues/27971) solves my issue!\n\nThanks a lot for pointing out, @ezyang!"
   },
   {
      "x": "nn.Transformer.generate_square_subsequent_mask does not behave as expected",
      "z": "It's an old issue related to type promotion https://github.com/pytorch/pytorch/pull/28231 and has been fixed by v.1.3.1. Please update your pytorch with the latest binary package or master branch. Feel free to re-open the issue if you still have questions.",
      "y": "It's an old issue related to type promotion https://github.com/pytorch/pytorch/pull/28231 and has been fixed by v.1.3.1. Please update your pytorch with the latest binary package or master branch. Feel free to re-open the issue if you still have questions."
   },
   {
      "x": "Vanilla Resnet50 Not Computing on iOS via Libtorch (Pytorch Mobile)",
      "z": "Hi Hussain, \n\nAs we've discussed in the PyTorch forum, this is a known issue, and we've fixed it - #29885  .If you want to try out the fix, feel free to pull the latest code from master and re-compile the static libraries from source code. Note that the fix will be available in the next release of Cocoapods.",
      "y": "Hi Hussain, \n\nAs we've discussed in the PyTorch forum, this is a known issue, and we've fixed it - #29885  .If you want to try out the fix, feel free to pull the latest code from master and re-compile the static libraries from source code. Note that the fix will be available in the next release of Cocoapods."
   },
   {
      "x": "Error when loading model with traced `to` call",
      "z": "@sysuzyq This looks like a forward-compatibility issue, i.e. the version of PyTorch you use to save your model is newer than the libtorch version. Can you check if this works if these versions match?",
      "y": "@sysuzyq This looks like a forward-compatibility issue, i.e. the version of PyTorch you use to save your model is newer than the libtorch version. Can you check if this works if these versions match?"
   },
   {
      "x": "died with <Signals.SIGSEGV: 11>",
      "z": "Try Python 3.6.",
      "y": "Try Python 3.6."
   },
   {
      "x": "Memory leak when evaluating model on CPU with dynamic size tensor input.",
      "z": "os.environ['LRU_CACHE_CAPACITY'] = '1' also work for me, THANKS!",
      "y": "os.environ['LRU_CACHE_CAPACITY'] = '1' also work for me, THANKS!"
   },
   {
      "x": "torch.nn.parallel.DistributedDataParallel is slow on backpropagation",
      "z": "A difference between Apex and PyTorch DDP that comes to mind here is that Apex figures out the order in which gradients are produced at runtime, whereas PyTorch DDP still assumes that gradients are produced in reverse order how they are defined in the `nn.Module`. If the model you link to defines some first encoders / MLPs / etc as the last parameters in the module, you would see head-of-line blocking where all reductions will be sequenced after the final gradients, effectively removing all opportunity for overlapping reduction with gradient computation.",
      "y": "A difference between Apex and PyTorch DDP that comes to mind here is that Apex figures out the order in which gradients are produced at runtime, whereas PyTorch DDP still assumes that gradients are produced in reverse order how they are defined in the `nn.Module`. If the model you link to defines some first encoders / MLPs / etc as the last parameters in the module, you would see head-of-line blocking where all reductions will be sequenced after the final gradients, effectively removing all opportunity for overlapping reduction with gradient computation."
   },
   {
      "x": "Illegal instruction : 4",
      "z": "YESSSSSS !!\nThanks a lot.\n\nHere it is :\n\n(base) iMac27:miniconda3 xxxxxx$ **conda install --update-all pytorch-nightly torchvision -c pytorch**\nCollecting package metadata (current_repodata.json): done\nSolving environment: done\n\n## Package Plan ##\n\n  environment location: /Users/xxxxxx/miniconda3\n\n  added / updated specs:\n    - pytorch-nightly\n    - torchvision\n\n\nThe following packages will be downloaded:\n\n    package                    |            build\n    ---------------------------|-----------------\n    **pytorch-nightly-1.0.0.dev20190328**|          py3.7_0        45.3 MB  pytorch\n    tqdm-4.38.0                |             py_0          51 KB\n    ------------------------------------------------------------\n                                           Total:        45.4 MB\n\nThe following NEW packages will be INSTALLED:\n\n  pytorch-nightly    pytorch/osx-64::pytorch-nightly-1.0.0.dev20190328-py3.7_0\n\nThe following packages will be UPDATED:\n\n  tqdm                                          4.36.1-py_0 --> 4.38.0-py_0\n\n\nProceed ([y]/n)? y\n\n\nDownloading and Extracting Packages\npytorch-nightly-1.0. | 45.3 MB   | #################################################################################### | 100% \ntqdm-4.38.0          | 51 KB     | #################################################################################### | 100% \nPreparing transaction: done\nVerifying transaction: done\nExecuting transaction: done\n(base) iMac27:miniconda3 xxxxxx$ python3\nPython 3.7.5 (default, Oct 25 2019, 10:52:18) \n[Clang 4.0.1 (tags/RELEASE_401/final)] :: Anaconda, Inc. on darwin\nType \"help\", \"copyright\", \"credits\" or \"license\" for more information.\n>>> import torch\n>>> \n(base) iMac27:miniconda3 xxxxxx$ \n",
      "y": "YESSSSSS !!\nThanks a lot.\n\nHere it is :\n\n(base) iMac27:miniconda3 xxxxxx$ **conda install --update-all pytorch-nightly torchvision -c pytorch**\nCollecting package metadata (current_repodata.json): done\nSolving environment: done\n\n## Package Plan ##\n\n  environment location: /Users/xxxxxx/miniconda3\n\n  added / updated specs:\n    - pytorch-nightly\n    - torchvision\n\n\nThe following packages will be downloaded:\n\n    package                    |            build\n    ---------------------------|-----------------\n    **pytorch-nightly-1.0.0.dev20190328**|          py3.7_0        45.3 MB  pytorch\n    tqdm-4.38.0                |             py_0          51 KB\n    ------------------------------------------------------------\n                                           Total:        45.4 MB\n\nThe following NEW packages will be INSTALLED:\n\n  pytorch-nightly    pytorch/osx-64::pytorch-nightly-1.0.0.dev20190328-py3.7_0\n\nThe following packages will be UPDATED:\n\n  tqdm                                          4.36.1-py_0 --> 4.38.0-py_0\n\n\nProceed ([y]/n)? y\n\n\nDownloading and Extracting Packages\npytorch-nightly-1.0. | 45.3 MB   | #################################################################################### | 100% \ntqdm-4.38.0          | 51 KB     | #################################################################################### | 100% \nPreparing transaction: done\nVerifying transaction: done\nExecuting transaction: done\n(base) iMac27:miniconda3 xxxxxx$ python3\nPython 3.7.5 (default, Oct 25 2019, 10:52:18) \n[Clang 4.0.1 (tags/RELEASE_401/final)] :: Anaconda, Inc. on darwin\nType \"help\", \"copyright\", \"credits\" or \"license\" for more information.\n>>> import torch\n>>> \n(base) iMac27:miniconda3 xxxxxx$ \n"
   },
   {
      "x": "ONNX export failed: Couldn't export operator aten::upsample_bilinear2d",
      "z": "Fixed after adding \n\ntorch.onnx.export(..,opset_version=11)\n",
      "y": "Fixed after adding \n\ntorch.onnx.export(..,opset_version=11)\n"
   },
   {
      "x": "Add SWA to PyTorch mainline",
      "z": "In general, we focus on including methods that the community uses as a standard, or else the code maintenance problem balloons up for us. We do show discretion based on what the paper shows as evidence, for example BatchNorm was included within weeks of it's publish date (in Torch). In terms of rejected methods, we've rejected newly minted papers such as Swish (#3260, #3182), [Yellowfin](https://github.com/pytorch/pytorch/issues/1960) and many others that haven't become standardized in the community (like LSTM/Transformer/BatchNorm).\n\nIn this case, since at least a year has passed following the addition in contrib, the github repo and the paper have gathered momentum, I do agree with bringing SWA in pytorch mainline. Would you like to open a pull request with the relevant code, algorithm and tests?",
      "y": "In general, we focus on including methods that the community uses as a standard, or else the code maintenance problem balloons up for us. We do show discretion based on what the paper shows as evidence, for example BatchNorm was included within weeks of it's publish date (in Torch). In terms of rejected methods, we've rejected newly minted papers such as Swish (#3260, #3182), [Yellowfin](https://github.com/pytorch/pytorch/issues/1960) and many others that haven't become standardized in the community (like LSTM/Transformer/BatchNorm).\n\nIn this case, since at least a year has passed following the addition in contrib, the github repo and the paper have gathered momentum, I do agree with bringing SWA in pytorch mainline. Would you like to open a pull request with the relevant code, algorithm and tests?"
   },
   {
      "x": "The Bug of onnx model format exported by torch.onnx.export",
      "z": "For exporting a model to ONNX, you should not set the export type to OperatorExportTypes.ONNX_ATEN.\nIs there a specific reason you're using a different export type?\n\nTo try out the repro code, I added the code below to read the image:\n\n```\ninput_img = osp.join(\n            osp.dirname(__file__), '../tests/data/color.jpg')\noriginal_image = mmcv.imread(input_img)\noriginal_image = torch.from_numpy(original_image)\n```\n\nWith this code, I two errors in exporter regarding missing symbolics:\n1- Unsupported: ONNX export of roi_align with aligned=True\n2-  Exporting the operator new_empty to ONNX opset version 12 is not supported\n\nCan you please confirm if you see a similar behavior with pytorch 1.6 and detectron2 build from source?\n\nAlso, is there a reason you're exporting this model to external data format?\nRunning ONNX checker on models in external data format might be a bit different.\nI tried bypassing the issue with symbolics (1- set aligned=False, and inserted a symbolic for new_empty),\nand then tried export and checker with: onnx.checker.check_model(\"mask_rcnn.onnx\")\nWhich seems to work and pass checker successfully.",
      "y": "For exporting a model to ONNX, you should not set the export type to OperatorExportTypes.ONNX_ATEN.\nIs there a specific reason you're using a different export type?\n\nTo try out the repro code, I added the code below to read the image:\n\n```\ninput_img = osp.join(\n            osp.dirname(__file__), '../tests/data/color.jpg')\noriginal_image = mmcv.imread(input_img)\noriginal_image = torch.from_numpy(original_image)\n```\n\nWith this code, I two errors in exporter regarding missing symbolics:\n1- Unsupported: ONNX export of roi_align with aligned=True\n2-  Exporting the operator new_empty to ONNX opset version 12 is not supported\n\nCan you please confirm if you see a similar behavior with pytorch 1.6 and detectron2 build from source?\n\nAlso, is there a reason you're exporting this model to external data format?\nRunning ONNX checker on models in external data format might be a bit different.\nI tried bypassing the issue with symbolics (1- set aligned=False, and inserted a symbolic for new_empty),\nand then tried export and checker with: onnx.checker.check_model(\"mask_rcnn.onnx\")\nWhich seems to work and pass checker successfully."
   },
   {
      "x": "How to use and debug mixed-precision in 1.6.0 ?",
      "z": "You should check if Tensor cores are used at all, there are some dimension requirements on batch size and number of hidden units. With apex it was done using for example: https://github.com/NVIDIA/apex/tree/master/apex/pyprof. \n\nBtw, while being advertised,  mixed precision is not quite there yet, e.g. RNN modules/cells don't work at all see:\n\n1. https://github.com/pytorch/pytorch/issues/42605\n2. https://github.com/pytorch/pytorch/issues/36428\n\nAlso you mentioned data parallelism and I'm not sure to which parallelism model are you referring to, I think it doesn't work with DataParallel, it only works with DDP (I'm not 100% sure for this, there were some problems, maybe it is fixed now).\n\nFinally, here you have some pointers for debugging at the end of slides: https://nvlabs.github.io/iccv2019-mixed-precision-tutorial/files/dusan_stosic_intro_to_mixed_precision_training.pdf.",
      "y": "You should check if Tensor cores are used at all, there are some dimension requirements on batch size and number of hidden units. With apex it was done using for example: https://github.com/NVIDIA/apex/tree/master/apex/pyprof. \n\nBtw, while being advertised,  mixed precision is not quite there yet, e.g. RNN modules/cells don't work at all see:\n\n1. https://github.com/pytorch/pytorch/issues/42605\n2. https://github.com/pytorch/pytorch/issues/36428\n\nAlso you mentioned data parallelism and I'm not sure to which parallelism model are you referring to, I think it doesn't work with DataParallel, it only works with DDP (I'm not 100% sure for this, there were some problems, maybe it is fixed now).\n\nFinally, here you have some pointers for debugging at the end of slides: https://nvlabs.github.io/iccv2019-mixed-precision-tutorial/files/dusan_stosic_intro_to_mixed_precision_training.pdf."
   },
   {
      "x": "after updating to pytorch 1.6 mypy does not recognise the tensor attributes ndim, nonzero and T ",
      "z": "`ndim` was fixed yesterday (gh-42908), `T` was already present in master for longer, `nonzero` is still missing.",
      "y": "`ndim` was fixed yesterday (gh-42908), `T` was already present in master for longer, `nonzero` is still missing."
   },
   {
      "x": "How to use torch.utils.checkpoint and DistributedDataParallel together",
      "z": "Hey @devilztt if you are manually synchronizing gradients, then you don't need DDP anymore. \n\n```python\ninit_process_group(...)\nmodel = MyModel(...)\nmodel(inputs).sum().backward()\nworks = []\nfor p in model.parameters():\n    # to speed it up, you can also organize grads to larger buckets to make allreduce more efficient\n    works.append(dist.all_reduce(p.grad, async_op=True))\nfor work in works:\n    work.wait()\n...\n```",
      "y": "Hey @devilztt if you are manually synchronizing gradients, then you don't need DDP anymore. \n\n```python\ninit_process_group(...)\nmodel = MyModel(...)\nmodel(inputs).sum().backward()\nworks = []\nfor p in model.parameters():\n    # to speed it up, you can also organize grads to larger buckets to make allreduce more efficient\n    works.append(dist.all_reduce(p.grad, async_op=True))\nfor work in works:\n    work.wait()\n...\n```"
   },
   {
      "x": "complex32 seems to be doing very very weird things on CPU",
      "z": "Currently there is almost no support for `torch.complex32`, so yeah we should disable it.",
      "y": "Currently there is almost no support for `torch.complex32`, so yeah we should disable it."
   },
   {
      "x": "[jit] TorchScript does not work with Python coverage package",
      "z": "Hi @janeyx99, is this resolved by your recent code coverage enhancement for TorchScript? ",
      "y": "Hi @janeyx99, is this resolved by your recent code coverage enhancement for TorchScript? "
   },
   {
      "x": "Trying to get pytorch working for the first time",
      "z": "Looks like you've installed pytorch with cuda 7.5, and it is trying to jit the code for you 1080 card. When you set device to 1, you are running on 610, which has compute capability 3.0 and most likely not supported. Try installing cuda 8 version of pytorch (conda install pytorch torchvision cuda80 -c soumith)",
      "y": "Looks like you've installed pytorch with cuda 7.5, and it is trying to jit the code for you 1080 card. When you set device to 1, you are running on 610, which has compute capability 3.0 and most likely not supported. Try installing cuda 8 version of pytorch (conda install pytorch torchvision cuda80 -c soumith)"
   },
   {
      "x": "Feature Request: Add Pixel Unshuffle",
      "z": "The implementation from the topic starter is quite efficient already. I haven't seen it being used too often and usually, it's only used once at the beginning of the network. So benefits from adding it to the core are questionable. If you need class you could copy-paste this:\n```python\nclass SpaceToDepth(nn.Module):\n    def __init__(self, block_size=4):\n        super().__init__()\n        assert block_size in {2, 4}, \"Space2Depth only supports blocks size = 4 or 2\"\n        self.block_size = block_size\n\n    def forward(self, x):\n        N, C, H, W = x.size()\n        S = self.block_size\n        x = x.view(N, C, H // S, S, W // S, S)  # (N, C, H//bs, bs, W//bs, bs)\n        x = x.permute(0, 3, 5, 1, 2, 4).contiguous()  # (N, bs, bs, C, H//bs, W//bs)\n        x = x.view(N, C * S * S, H // S, W // S)  # (N, C*bs^2, H//bs, W//bs)\n        return x\n\n    def extra_repr(self):\n        return f\"block_size={self.block_size}\"\n```",
      "y": "The implementation from the topic starter is quite efficient already. I haven't seen it being used too often and usually, it's only used once at the beginning of the network. So benefits from adding it to the core are questionable. If you need class you could copy-paste this:\n```python\nclass SpaceToDepth(nn.Module):\n    def __init__(self, block_size=4):\n        super().__init__()\n        assert block_size in {2, 4}, \"Space2Depth only supports blocks size = 4 or 2\"\n        self.block_size = block_size\n\n    def forward(self, x):\n        N, C, H, W = x.size()\n        S = self.block_size\n        x = x.view(N, C, H // S, S, W // S, S)  # (N, C, H//bs, bs, W//bs, bs)\n        x = x.permute(0, 3, 5, 1, 2, 4).contiguous()  # (N, bs, bs, C, H//bs, W//bs)\n        x = x.view(N, C * S * S, H // S, W // S)  # (N, C*bs^2, H//bs, W//bs)\n        return x\n\n    def extra_repr(self):\n        return f\"block_size={self.block_size}\"\n```"
   },
   {
      "x": "Timeout option for parallel DataLoader",
      "z": "We have switched to using `mp.Queue` (not `SimpleQueue`) now. I tested the above code on master. Interestingly, it makes the worker segfault. It works with `num_workers=0`.\n\nI adapted as following to make it run:\n```\nimport torch.utils.data\n\nclass Dataset(object):\n  def __len__(self):\n    return 100\n\n  def __getitem__(self, i):\n    return list(range(100000))\n\n\nclass Sampler(torch.utils.data.Sampler):\n  def __iter__(self):\n    return (range(100000) for batch_ind in range(100))\n\n  def __len__(self):\n    return 100\n\nd = torch.utils.data.DataLoader(dataset = Dataset(), sampler = Sampler(None), num_workers = 0)\nfor i, x in enumerate(d):\n    print(i)\n    \n```",
      "y": "We have switched to using `mp.Queue` (not `SimpleQueue`) now. I tested the above code on master. Interestingly, it makes the worker segfault. It works with `num_workers=0`.\n\nI adapted as following to make it run:\n```\nimport torch.utils.data\n\nclass Dataset(object):\n  def __len__(self):\n    return 100\n\n  def __getitem__(self, i):\n    return list(range(100000))\n\n\nclass Sampler(torch.utils.data.Sampler):\n  def __iter__(self):\n    return (range(100000) for batch_ind in range(100))\n\n  def __len__(self):\n    return 100\n\nd = torch.utils.data.DataLoader(dataset = Dataset(), sampler = Sampler(None), num_workers = 0)\nfor i, x in enumerate(d):\n    print(i)\n    \n```"
   },
   {
      "x": "numpy like tensor.all and tensor.any",
      "z": "As a note, `any` and `all` exist on ByteTensors, but do not appear in online documentation.",
      "y": "As a note, `any` and `all` exist on ByteTensors, but do not appear in online documentation."
   },
   {
      "x": "Bad error message when concatting different type Tensor(Variable)",
      "z": "FYI, the same misleading message happens when trying to concat a Variable with a Tensor.\n\n> TypeError: cat received an invalid combination of arguments - got (list), but expected one of:\n>  * (sequence[torch.cuda.FloatTensor] seq)\n>       didn't match because some of the arguments have invalid types: (list)\n>  * (sequence[torch.cuda.FloatTensor] seq, int dim)\n> ",
      "y": "FYI, the same misleading message happens when trying to concat a Variable with a Tensor.\n\n> TypeError: cat received an invalid combination of arguments - got (list), but expected one of:\n>  * (sequence[torch.cuda.FloatTensor] seq)\n>       didn't match because some of the arguments have invalid types: (list)\n>  * (sequence[torch.cuda.FloatTensor] seq, int dim)\n> "
   },
   {
      "x": "Feature request: nn.View",
      "z": "You could define a module like this:\n```\nclass View(nn.Module):\n       def __init__(self):\n            super(View, self).__init__()\n   \n        def forward(self, x):\n            return x.view(-1) \n```\n\nThis flattens the input but similarly you could provide a size object.\n\nNow you can use this as part of the model.",
      "y": "You could define a module like this:\n```\nclass View(nn.Module):\n       def __init__(self):\n            super(View, self).__init__()\n   \n        def forward(self, x):\n            return x.view(-1) \n```\n\nThis flattens the input but similarly you could provide a size object.\n\nNow you can use this as part of the model."
   },
   {
      "x": "function expand_as() works incorrectly on latest Pytorch 0.2.0_1",
      "z": "Replace sum(1) with sum(1, keepdim=True)\n\nThis is caused by the change of sum in 0.2.0.",
      "y": "Replace sum(1) with sum(1, keepdim=True)\n\nThis is caused by the change of sum in 0.2.0."
   },
   {
      "x": "PyTorch 0.2.0_1 Freezes at nn.Conv2d()",
      "z": "Just to Update:\n\nIt works fine if we use 'spawn' start method.\n\nUpdated Snippet:\n\n```\nimport torch\nimport torch.nn as nn\nimport torch.nn.functional as F\nfrom torch.autograd import Variable\nimport torch.multiprocessing as mp\n\n\nclass Net(nn.Module):\n    def __init__(self, input_size):\n        super(Net, self).__init__()\n        self.conv1 = nn.Conv2d(input_size, 32, 3, stride=2, padding=1)\n\n    def forward(self, input):\n        print('Before Conv1 call')\n        x = F.elu(self.conv1(input))\n        print('After Conv1 call')\n        return x\n\n\ndef train():\n    net = Net(1)\n    net(Variable(torch.randn(1, 80, 80).unsqueeze(0)))\n    print('Passed!')\n\n\nif __name__ == '__main__':\n    mp.set_start_method('spawn')\n\n    # directly calling the method works\n    train()\n\n    # Works fine as well with 'spawn'\n    p = mp.Process(target=train, args=())\n    p.start()\n    p.join()\n```\nOutput:\n\n```\nBefore Conv1 call\nAfter Conv1 call\nPassed!\nBefore Conv1 call\nAfter Conv1 call\nPassed!\n```\n",
      "y": "Just to Update:\n\nIt works fine if we use 'spawn' start method.\n\nUpdated Snippet:\n\n```\nimport torch\nimport torch.nn as nn\nimport torch.nn.functional as F\nfrom torch.autograd import Variable\nimport torch.multiprocessing as mp\n\n\nclass Net(nn.Module):\n    def __init__(self, input_size):\n        super(Net, self).__init__()\n        self.conv1 = nn.Conv2d(input_size, 32, 3, stride=2, padding=1)\n\n    def forward(self, input):\n        print('Before Conv1 call')\n        x = F.elu(self.conv1(input))\n        print('After Conv1 call')\n        return x\n\n\ndef train():\n    net = Net(1)\n    net(Variable(torch.randn(1, 80, 80).unsqueeze(0)))\n    print('Passed!')\n\n\nif __name__ == '__main__':\n    mp.set_start_method('spawn')\n\n    # directly calling the method works\n    train()\n\n    # Works fine as well with 'spawn'\n    p = mp.Process(target=train, args=())\n    p.start()\n    p.join()\n```\nOutput:\n\n```\nBefore Conv1 call\nAfter Conv1 call\nPassed!\nBefore Conv1 call\nAfter Conv1 call\nPassed!\n```\n"
   },
   {
      "x": "Advanced Indexing doesn't work with uniform",
      "z": "This is the expected behavior. Advanced Indexing always returns a copy of the indexed Tensor unless you perform assignment. From NumPy:\n\n> Advanced indexing always returns a copy of the data.",
      "y": "This is the expected behavior. Advanced Indexing always returns a copy of the indexed Tensor unless you perform assignment. From NumPy:\n\n> Advanced indexing always returns a copy of the data."
   },
   {
      "x": "Segfault (free() on invalid pointer)",
      "z": "Ok. So here's an ***_incredibly_*** hacky temorary workaround for Jupyter notebooks:\n\nFind the file called `ipykernel_launcher.py`  (mine is at `~/.local/lib/python3.6/site-packages/ipykernel_launcher.py` for example)\n\nNow, just after `import sys` insert a `import torch`. This gets rid of the notebook crashes, but comes at a \nslight \"cost\" that all the notebooks you start has torch already loaded into memory.\n\nJust remember to remove this hack after this issue is fixed :P ;)\n",
      "y": "Ok. So here's an ***_incredibly_*** hacky temorary workaround for Jupyter notebooks:\n\nFind the file called `ipykernel_launcher.py`  (mine is at `~/.local/lib/python3.6/site-packages/ipykernel_launcher.py` for example)\n\nNow, just after `import sys` insert a `import torch`. This gets rid of the notebook crashes, but comes at a \nslight \"cost\" that all the notebooks you start has torch already loaded into memory.\n\nJust remember to remove this hack after this issue is fixed :P ;)\n"
   },
   {
      "x": "undefined symbol in master",
      "z": "you can compile with `NO_DISTRIBUTED=1 python setup.py install` to avoid this. This might be a consequence of gloo not compiling for pre-3.x cards.",
      "y": "you can compile with `NO_DISTRIBUTED=1 python setup.py install` to avoid this. This might be a consequence of gloo not compiling for pre-3.x cards."
   },
   {
      "x": "weight_norm assertion error when using bias=False and using cuda",
      "z": "This issue is the same issue as raised in https://github.com/pytorch/pytorch/issues/2343 and is due to the way in which RNNs now flatten their weights when used on the GPU (otherwise it's a null op). This also breaks code for the [weight dropped LSTM](https://github.com/salesforce/awd-lstm-lm/blob/master/weight_drop.py).\n\nI realized that weight norm would have the same issue so searched hoping for an elegant solution, but alas :)\n\nI've fixed it by setting `rnn.flatten_parameters = lambda *args, **kwargs: None`, which results in a warning (below) but otherwise running code (except see caveat re: lambda).\n`UserWarning: RNN module weights are not part of single contiguous chunk of memory. This means they need to be compacted at every call, possibly greately increasing memory usage. To compact weights again call flatten_parameters().`\n(Just realized the error has a typo in \"greatly\" but oh well ;))\n\nYou could (and others have) converted the RNN to CUDA early but that would break other parts of my code. Killing the `flatten_parameters` function seems the most consistently backward compatible option. Only issue is you need to create an empty function (i.e. not use lambda) as otherwise you get pickling issues (`AttributeError: Can't pickle local object 'WeightDrop._setup.<locals>.<lambda>'`) on model save.\n\n@apaszke, have you got any insights as to a more elegant solution either temporarily or longer form? I know `flatten_parameters` is already a complicated and low level beast.",
      "y": "This issue is the same issue as raised in https://github.com/pytorch/pytorch/issues/2343 and is due to the way in which RNNs now flatten their weights when used on the GPU (otherwise it's a null op). This also breaks code for the [weight dropped LSTM](https://github.com/salesforce/awd-lstm-lm/blob/master/weight_drop.py).\n\nI realized that weight norm would have the same issue so searched hoping for an elegant solution, but alas :)\n\nI've fixed it by setting `rnn.flatten_parameters = lambda *args, **kwargs: None`, which results in a warning (below) but otherwise running code (except see caveat re: lambda).\n`UserWarning: RNN module weights are not part of single contiguous chunk of memory. This means they need to be compacted at every call, possibly greately increasing memory usage. To compact weights again call flatten_parameters().`\n(Just realized the error has a typo in \"greatly\" but oh well ;))\n\nYou could (and others have) converted the RNN to CUDA early but that would break other parts of my code. Killing the `flatten_parameters` function seems the most consistently backward compatible option. Only issue is you need to create an empty function (i.e. not use lambda) as otherwise you get pickling issues (`AttributeError: Can't pickle local object 'WeightDrop._setup.<locals>.<lambda>'`) on model save.\n\n@apaszke, have you got any insights as to a more elegant solution either temporarily or longer form? I know `flatten_parameters` is already a complicated and low level beast."
   },
   {
      "x": "CUDA error (3): initialization error (multiprocessing)",
      "z": "OK so I narrowed it down to `torch.manual_seed` of all things. Here is a minimal script reproducing the issue.\n\n```python\nimport torch\nimport torch.nn as nn\nimport torch.multiprocessing as mp\nimport torch.nn.functional as F\nfrom torch.autograd import Variable\n\n\ndef task(pid, model):\n    x = Variable(torch.rand(64, 10))\n    y = model(x)\n    t = y.clone() * 0.99\n    loss = F.smooth_l1_loss(y, t)\n\n    # here it breaks\n    loss.backward()\n\n    print(\"Process %d finished\" % pid)\n\n\nif __name__ == \"__main__\":\n\n    # comment manual_seed and the CUDA initialization error is gone.\n    torch.manual_seed(23)\n\n    net = nn.Linear(10, 4)\n    net.share_memory()\n\n    processes = []\n    for pid in range(8):\n        p = mp.Process(target=task, args=(pid, net))\n        p.start()\n\n    for p in processes:\n        p.join()\n\n    print(\"Done.\")\n```\n\nedit: this can be solved by setting `mp.set_start_method('spawn')` before setting the rng seed which in turn calls cuda. Although I am not sure it is ideal.",
      "y": "OK so I narrowed it down to `torch.manual_seed` of all things. Here is a minimal script reproducing the issue.\n\n```python\nimport torch\nimport torch.nn as nn\nimport torch.multiprocessing as mp\nimport torch.nn.functional as F\nfrom torch.autograd import Variable\n\n\ndef task(pid, model):\n    x = Variable(torch.rand(64, 10))\n    y = model(x)\n    t = y.clone() * 0.99\n    loss = F.smooth_l1_loss(y, t)\n\n    # here it breaks\n    loss.backward()\n\n    print(\"Process %d finished\" % pid)\n\n\nif __name__ == \"__main__\":\n\n    # comment manual_seed and the CUDA initialization error is gone.\n    torch.manual_seed(23)\n\n    net = nn.Linear(10, 4)\n    net.share_memory()\n\n    processes = []\n    for pid in range(8):\n        p = mp.Process(target=task, args=(pid, net))\n        p.start()\n\n    for p in processes:\n        p.join()\n\n    print(\"Done.\")\n```\n\nedit: this can be solved by setting `mp.set_start_method('spawn')` before setting the rng seed which in turn calls cuda. Although I am not sure it is ideal."
   },
   {
      "x": "Runtime error when trying to swap first two axes of a four dimensional Tensor with torch.transpose.",
      "z": "This is a printing issue... We fixed it on master. Sorry about it.",
      "y": "This is a printing issue... We fixed it on master. Sorry about it."
   },
   {
      "x": "Transposed Convolution output shape does not take into account dilation",
      "z": "I just noticed this as well because I use the formula provided in by the docs in my [OutputShapeFor](https://github.com/Erotemic/netharn/blob/master/netharn/output_shape_for.py) class.\n\nConsidering the code\n\n```python.\n        module = nn.ConvTranspose2d(1, 1, kernel_size=3, stride=1, dilation=2)\n        input_shape = (1, 1, 10, 10)\n        module(torch.rand(*input_shape)).shape\n```\n\nAccording  to the docs the output shape should be (1, 1, 12, 12), but in reality the output shape is (1, 1, 14, 14).\n\nModifying the formula as suggested by @Coolnesss does result in the correct answer, and the changes matches my intuition of how dilation increases the effective kernel size. ",
      "y": "I just noticed this as well because I use the formula provided in by the docs in my [OutputShapeFor](https://github.com/Erotemic/netharn/blob/master/netharn/output_shape_for.py) class.\n\nConsidering the code\n\n```python.\n        module = nn.ConvTranspose2d(1, 1, kernel_size=3, stride=1, dilation=2)\n        input_shape = (1, 1, 10, 10)\n        module(torch.rand(*input_shape)).shape\n```\n\nAccording  to the docs the output shape should be (1, 1, 12, 12), but in reality the output shape is (1, 1, 14, 14).\n\nModifying the formula as suggested by @Coolnesss does result in the correct answer, and the changes matches my intuition of how dilation increases the effective kernel size. "
   },
   {
      "x": "Caffe2: ONNX building with lite proto failure",
      "z": "Hi @bddppq\nyes, by applying #14140 and #14150 I am able to successfully build Caffe2 with protobuf-lite for Android.\nThank you very much for helping!",
      "y": "Hi @bddppq\nyes, by applying #14140 and #14150 I am able to successfully build Caffe2 with protobuf-lite for Android.\nThank you very much for helping!"
   },
   {
      "x": "[Caffe2] ONNX Caffe2Backend.prepare() initializes input as float64",
      "z": "Due to this bug, I was getting error in a different form. \nI was not using CUDA, but was trying to export the model using -\n```\nfrom caffe2.python.predictor import mobile_exporter\n\nmobile_exporter.Export(prepared_backend.workspace, prepared_backend.predict_net, prepared_backend.predict_net.external_input) \n```\nThis was throwing `KeyError: dtype('float64')` from [here](https://github.com/pytorch/pytorch/blob/d55b25a633b7e2e6122becf6dbdf0528df6e8b13/caffe2/python/predictor/mobile_exporter.py#L41).\n\nThanks for @laggui for the fix given above. I was circumventing this in different way.",
      "y": "Due to this bug, I was getting error in a different form. \nI was not using CUDA, but was trying to export the model using -\n```\nfrom caffe2.python.predictor import mobile_exporter\n\nmobile_exporter.Export(prepared_backend.workspace, prepared_backend.predict_net, prepared_backend.predict_net.external_input) \n```\nThis was throwing `KeyError: dtype('float64')` from [here](https://github.com/pytorch/pytorch/blob/d55b25a633b7e2e6122becf6dbdf0528df6e8b13/caffe2/python/predictor/mobile_exporter.py#L41).\n\nThanks for @laggui for the fix given above. I was circumventing this in different way."
   },
   {
      "x": "FP16 results in \"Floating point exception\"",
      "z": "Unfortunately 9.2 nightlies are built with cudnn 7.1.4 that has a known fpe bug. The solution here would be to build nightlies with more recent cudnn versions.",
      "y": "Unfortunately 9.2 nightlies are built with cudnn 7.1.4 that has a known fpe bug. The solution here would be to build nightlies with more recent cudnn versions."
   },
   {
      "x": "torch.argmin behaves differently on CPU and GPU",
      "z": "@carefree0910 argmin returns the index of the minimum value in the dimension. It doesn't have a guarantee to return the index of the **first** minimum. The GPU result is also correct from what `argmin` is supposed to return",
      "y": "@carefree0910 argmin returns the index of the minimum value in the dimension. It doesn't have a guarantee to return the index of the **first** minimum. The GPU result is also correct from what `argmin` is supposed to return"
   },
   {
      "x": "Library not loaded: libmklml.dylib use c++ front",
      "z": "@yf225 apparently this is fixed in latest MKL-DNN upgrade. It is pending land https://github.com/pytorch/pytorch/pull/22910",
      "y": "@yf225 apparently this is fixed in latest MKL-DNN upgrade. It is pending land https://github.com/pytorch/pytorch/pull/22910"
   },
   {
      "x": "Batched SVD using cuSolver",
      "z": "Closing this since the feature is available on master. The current implementation uses sequential MAGMA calls in a for-loop.",
      "y": "Closing this since the feature is available on master. The current implementation uses sequential MAGMA calls in a for-loop."
   },
   {
      "x": "cross_entropy - class weights is a bit unclear",
      "z": "I believe `elementwise_mean` has been removed from `reduction` options now. https://pytorch.org/docs/stable/nn.html?highlight=cross_entropy#torch.nn.CrossEntropyLoss\nClosing, please feel free to reopen if you have other questions. Thanks!",
      "y": "I believe `elementwise_mean` has been removed from `reduction` options now. https://pytorch.org/docs/stable/nn.html?highlight=cross_entropy#torch.nn.CrossEntropyLoss\nClosing, please feel free to reopen if you have other questions. Thanks!"
   },
   {
      "x": "nn.parallel.DistributedDataParallel raise CUDA error",
      "z": "The problem is not dense vs. sparse, but cpu vs. cuda. Use `.to('cuda')` to put the network on gpu.",
      "y": "The problem is not dense vs. sparse, but cpu vs. cuda. Use `.to('cuda')` to put the network on gpu."
   },
   {
      "x": "\"log_softmax_lastdim_kernel_impl\" not implemented for 'torch.LongTensor'",
      "z": "Softmax doesn't work on a long tensor -- convert it to a float or double tensor first (via `tensor.float()`)",
      "y": "Softmax doesn't work on a long tensor -- convert it to a float or double tensor first (via `tensor.float()`)"
   },
   {
      "x": "RNN have CuDNN error: CUDNN_STATUS_SUCCESS with Tesla T4",
      "z": "Updating to the CUDA 10 compiled version of pytorch along with CUDA 10 runtime resolved this issue for me.",
      "y": "Updating to the CUDA 10 compiled version of pytorch along with CUDA 10 runtime resolved this issue for me."
   },
   {
      "x": "Crash when autograd function returns list instead of tuple",
      "z": "Now it's `TypeError: NewFunctionBackward.forward: expected Variable (got list) for return value 0` which is better, but if it said that a Tuple is expected instead of List or if it cast List to a Tuple automatically, it would be better",
      "y": "Now it's `TypeError: NewFunctionBackward.forward: expected Variable (got list) for return value 0` which is better, but if it said that a Tuple is expected instead of List or if it cast List to a Tuple automatically, it would be better"
   },
   {
      "x": "[feature request] Removing hooks from module",
      "z": "```python\nmodel = ...\nhandle = model.register_forward_hook(...)\nhandle.remove()\n# hook will no longer trigger\n```",
      "y": "```python\nmodel = ...\nhandle = model.register_forward_hook(...)\nhandle.remove()\n# hook will no longer trigger\n```"
   },
   {
      "x": "Unable to build from source",
      "z": "Searching issues for \"PRId64\" shows that this error popped up and has been fixed in various places in the pytorch codebase, e.g. https://github.com/pytorch/pytorch/issues/3571\n\nThe problem seems to be that this format specifier is not defined by default in C++ and/or old gcc versions. A hacky way to fix this is replacing `\"%\" PRId64 \"` with `\"%lld\"` in the two files currently causing this error on master. A better way is to find the proper place for addding\n\n```\n#define __STDC_FORMAT_MACROS\n#include <inttypes.h>\n```\nas described in the issue linked above.",
      "y": "Searching issues for \"PRId64\" shows that this error popped up and has been fixed in various places in the pytorch codebase, e.g. https://github.com/pytorch/pytorch/issues/3571\n\nThe problem seems to be that this format specifier is not defined by default in C++ and/or old gcc versions. A hacky way to fix this is replacing `\"%\" PRId64 \"` with `\"%lld\"` in the two files currently causing this error on master. A better way is to find the proper place for addding\n\n```\n#define __STDC_FORMAT_MACROS\n#include <inttypes.h>\n```\nas described in the issue linked above."
   },
   {
      "x": "Give a better error when we run out of shared memory, instead of \"RuntimeError: DataLoader worker (pid 13) is killed by signal: Bus error.\"",
      "z": "Do `nvidia-docker run -d --shm-size 50G -p 8888:8888 -p 6006:6006 -v ${PWD}:/notebook -v ${PWD}/data/:/notebook/data sachinruk/pytorch_gpu`. Main point is --shm-size ...\n\nIf anyone is wondering when I checked `docker stats`, it was showing that there is 59G available memory and I was using only 1G or so. So seems that you have to explicitly set `--shm-size`.",
      "y": "Do `nvidia-docker run -d --shm-size 50G -p 8888:8888 -p 6006:6006 -v ${PWD}:/notebook -v ${PWD}/data/:/notebook/data sachinruk/pytorch_gpu`. Main point is --shm-size ...\n\nIf anyone is wondering when I checked `docker stats`, it was showing that there is 59G available memory and I was using only 1G or so. So seems that you have to explicitly set `--shm-size`."
   },
   {
      "x": "RuntimeError: cuda runtime error (38)",
      "z": "Problem solved.\nI made a very stupid mistake.\nThere is a line in the head which is\nos.environ[\"CUDA_VISIBLE_DEVICES\"] = '3'.\nI did not notice it first time I ran the program and got another error. I revised it to os.environ[\"CUDA_VISIBLE_DEVICES\"] = '0' without restarting the kernel. Then l got this error.\n\nRestart the program with os.environ[\"CUDA_VISIBLE_DEVICES\"] = '0'can solve the problem.",
      "y": "Problem solved.\nI made a very stupid mistake.\nThere is a line in the head which is\nos.environ[\"CUDA_VISIBLE_DEVICES\"] = '3'.\nI did not notice it first time I ran the program and got another error. I revised it to os.environ[\"CUDA_VISIBLE_DEVICES\"] = '0' without restarting the kernel. Then l got this error.\n\nRestart the program with os.environ[\"CUDA_VISIBLE_DEVICES\"] = '0'can solve the problem."
   },
   {
      "x": "np.random generates the same random numbers for each data batch",
      "z": "This is because numpy doesn't properly handle RNG states when `fork` subprocesses. It's numpy's issue with multiprocessing tracked at https://github.com/numpy/numpy/issues/9248). But we do provide some workarounds, e.g. the `worker_init_fn` in DataLoader (see http://pytorch.org/docs/master/data.html#torch.utils.data.DataLoader). Or you can use other start methods like spawn.",
      "y": "This is because numpy doesn't properly handle RNG states when `fork` subprocesses. It's numpy's issue with multiprocessing tracked at https://github.com/numpy/numpy/issues/9248). But we do provide some workarounds, e.g. the `worker_init_fn` in DataLoader (see http://pytorch.org/docs/master/data.html#torch.utils.data.DataLoader). Or you can use other start methods like spawn."
   },
   {
      "x": "Potential bug when sampling from categorical distribution",
      "z": "On master the issue has been solved for the categorial distribution but not for the multinomial distribution as it seems: \n\n```\nimport torch.distributions as dis\nimport torch\nimport numpy as np\n\nG = 3\nD = 2\np_dG = torch.Tensor(G, D)\np_dG[:, 0] = torch.Tensor([0.1, 0.8, 0.1])\np_dG[:, 1] = torch.Tensor([0.1, 0.8, 0.1])\n\np_dg = p_dG[:, 0]\nz = p_dg.multinomial(250, replacement=True)\ntrue_z_np = z.numpy()\nv, c = np.unique(true_z_np, return_counts=True)\nprint(v)\nprint(c)\n```\nand also\n```\nz = torch.multinomial(p_dg, 250, replacement=True)\ntrue_z_np = z.numpy()\nv, c = np.unique(true_z_np, return_counts=True)\nprint(v)\nprint(c)\n```\n",
      "y": "On master the issue has been solved for the categorial distribution but not for the multinomial distribution as it seems: \n\n```\nimport torch.distributions as dis\nimport torch\nimport numpy as np\n\nG = 3\nD = 2\np_dG = torch.Tensor(G, D)\np_dG[:, 0] = torch.Tensor([0.1, 0.8, 0.1])\np_dG[:, 1] = torch.Tensor([0.1, 0.8, 0.1])\n\np_dg = p_dG[:, 0]\nz = p_dg.multinomial(250, replacement=True)\ntrue_z_np = z.numpy()\nv, c = np.unique(true_z_np, return_counts=True)\nprint(v)\nprint(c)\n```\nand also\n```\nz = torch.multinomial(p_dg, 250, replacement=True)\ntrue_z_np = z.numpy()\nv, c = np.unique(true_z_np, return_counts=True)\nprint(v)\nprint(c)\n```\n"
   },
   {
      "x": "Recent git pull breaks working pytorch build",
      "z": "after a git pull, do:\n\n```\ngit submodule update --init --recursive\n```\n\nor whatever it takes to update your submodules to their marked commits.\n\nif that doesn't work, just do a fresh git clone.",
      "y": "after a git pull, do:\n\n```\ngit submodule update --init --recursive\n```\n\nor whatever it takes to update your submodules to their marked commits.\n\nif that doesn't work, just do a fresh git clone."
   },
   {
      "x": "MKL Error when import torch after installing 0.3.0 on CentOS",
      "z": "Hi @malbergo,\nMy solution was quite hacky. \nI add `anaconda2/envs/myenv/lib` into LD_LIBRARY_PATH in my .bashrc.\n\nI still not quite like it because it should be automatically triggered when I call `source activate myenv`",
      "y": "Hi @malbergo,\nMy solution was quite hacky. \nI add `anaconda2/envs/myenv/lib` into LD_LIBRARY_PATH in my .bashrc.\n\nI still not quite like it because it should be automatically triggered when I call `source activate myenv`"
   },
   {
      "x": "Segmentation fault (core dumped) on LSTMCell on pytorch",
      "z": "Hi @saitarslanboun \nSome sizes are missing in the snippet you provided, so I cannot exactly replicate it and get the segfault error. However, I think I can see a few potential mistakes in your code. I think you might be mistaking the input specifications for `LSTM` and `LSTMCell`. Note that by default, `LSTM` takes as input a tensor of shape `(seq_len, batch, input_size)`, whereas `LSTMCell` takes as input a tensor of shape `(batch, input_size)`.\n\nIn your code snippet, you seem to have `x` where `x.size(1)` specifies the number of timesteps. If I assume that `x` is then of the form `(batch_size, seq_len, input_size)`, the first thing we might want to do is to transpose the first 2 dimensions, by `x.transpose_(0,1)`, to make things `seq_len` first, or `batch_size` second. This is only because recurrent layers like to have the timesteps in the first dimension.\n\nNow, ideally, with a tensor `x` in such a shape, you do not need to use either a for loop, or a `LSTMCell`, and can simply pass it to a `LSTM` module to get the desired output. You can find the documentation for LSTM [here](http://pytorch.org/docs/master/nn.html#torch.nn.LSTM). Your entire code will look like this:\n```\n# create lstmcell module\nlstm = nn.LSTM(embed_size * 2, hidden_size)\n\n# transpose from batch first to batch second for recurrent layers\nx = x.transpose(0, 1).contiguous()\noutput, (h_t, c_t) = lstm(x)\n```\n\nNow let us assume that for some reason, you must use a `LSTMCell` instead of a `LSTM`. In this case, if you go through the documentation for `LSTMCell` [here](http://pytorch.org/docs/master/nn.html#torch.nn.LSTMCell), you will see that the input needs to be passed 1 time step at a time. So, assuming your `x` is of the form `(batch_size, seq_len, input_size)`, your entire code will look like this:\n```\n# create lstmcell module\nlstm = nn.LSTMCell(embed_size * 2, hidden_size)\n\n# initialize h,c outside for loop\nh_t = Variable(x.data.new().resize_as_(x[:, 0, :].data).zero_())\nc_t = Variable(x.data.new().resize_as_(x[:, 0, :].data).zero_())\n\n# loop over time steps\nfor time_step in range(x.size(1)):\n    x_t = x[:, time_step, :]\n    (h_t, c_t) = lstm(x_t, (h_t, c_t))\n```\n\nFrom what I have understood from your snippet, there are a few potential mistakes:\n- Wrong sizes to initialize `hidden` and `cell`, which should be `(batch_size, input_size)` for `LSTMCell`\n- Two nested for loops, first over time steps, and second possibly over batch size, which is wrong\n\nLet me know if this helps you and/or if there are any details in your question that I have misunderstood.",
      "y": "Hi @saitarslanboun \nSome sizes are missing in the snippet you provided, so I cannot exactly replicate it and get the segfault error. However, I think I can see a few potential mistakes in your code. I think you might be mistaking the input specifications for `LSTM` and `LSTMCell`. Note that by default, `LSTM` takes as input a tensor of shape `(seq_len, batch, input_size)`, whereas `LSTMCell` takes as input a tensor of shape `(batch, input_size)`.\n\nIn your code snippet, you seem to have `x` where `x.size(1)` specifies the number of timesteps. If I assume that `x` is then of the form `(batch_size, seq_len, input_size)`, the first thing we might want to do is to transpose the first 2 dimensions, by `x.transpose_(0,1)`, to make things `seq_len` first, or `batch_size` second. This is only because recurrent layers like to have the timesteps in the first dimension.\n\nNow, ideally, with a tensor `x` in such a shape, you do not need to use either a for loop, or a `LSTMCell`, and can simply pass it to a `LSTM` module to get the desired output. You can find the documentation for LSTM [here](http://pytorch.org/docs/master/nn.html#torch.nn.LSTM). Your entire code will look like this:\n```\n# create lstmcell module\nlstm = nn.LSTM(embed_size * 2, hidden_size)\n\n# transpose from batch first to batch second for recurrent layers\nx = x.transpose(0, 1).contiguous()\noutput, (h_t, c_t) = lstm(x)\n```\n\nNow let us assume that for some reason, you must use a `LSTMCell` instead of a `LSTM`. In this case, if you go through the documentation for `LSTMCell` [here](http://pytorch.org/docs/master/nn.html#torch.nn.LSTMCell), you will see that the input needs to be passed 1 time step at a time. So, assuming your `x` is of the form `(batch_size, seq_len, input_size)`, your entire code will look like this:\n```\n# create lstmcell module\nlstm = nn.LSTMCell(embed_size * 2, hidden_size)\n\n# initialize h,c outside for loop\nh_t = Variable(x.data.new().resize_as_(x[:, 0, :].data).zero_())\nc_t = Variable(x.data.new().resize_as_(x[:, 0, :].data).zero_())\n\n# loop over time steps\nfor time_step in range(x.size(1)):\n    x_t = x[:, time_step, :]\n    (h_t, c_t) = lstm(x_t, (h_t, c_t))\n```\n\nFrom what I have understood from your snippet, there are a few potential mistakes:\n- Wrong sizes to initialize `hidden` and `cell`, which should be `(batch_size, input_size)` for `LSTMCell`\n- Two nested for loops, first over time steps, and second possibly over batch size, which is wrong\n\nLet me know if this helps you and/or if there are any details in your question that I have misunderstood."
   },
   {
      "x": "ImportError: libcudart.so.9.0: cannot open shared object file: No such file or directory",
      "z": "Ok, great. For now i fixed it with: \n> conda install -c anaconda cudatoolkit==9.0",
      "y": "Ok, great. For now i fixed it with: \n> conda install -c anaconda cudatoolkit==9.0"
   },
   {
      "x": "`Normal` distribution: Gaussian policy with zero gradient of mean head",
      "z": "Yep, you need to block gradient flow through sampling by calling `.sample()` instead of `.rsample()` (or detach it before calling `.logprob(.)`)",
      "y": "Yep, you need to block gradient flow through sampling by calling `.sample()` instead of `.rsample()` (or detach it before calling `.logprob(.)`)"
   },
   {
      "x": "Wrong torch.svd Calculation Result",
      "z": "Your NumPy is using Accererate, but PyTorch uses MKL. It is natural that they behave slightly differently. Given that the singular value is so small. I would just classify this as precision problem.",
      "y": "Your NumPy is using Accererate, but PyTorch uses MKL. It is natural that they behave slightly differently. Given that the singular value is so small. I would just classify this as precision problem."
   },
   {
      "x": "Issue with pytorch update (version 0.4.1) with cuda9.1.85",
      "z": "@darolt do `conda uninstall cuda90 cuda91 cuda92 pytorch -y`, and then re-run your command. That will fix it.",
      "y": "@darolt do `conda uninstall cuda90 cuda91 cuda92 pytorch -y`, and then re-run your command. That will fix it."
   },
   {
      "x": "Incorrect PROTOBUF version",
      "z": "Solved by doing\n```\nconda uninstall libprotobuf\n```",
      "y": "Solved by doing\n```\nconda uninstall libprotobuf\n```"
   },
   {
      "x": "implement dirichlet / beta GPU grad",
      "z": "Note that pyro's `Beta` **does** support GPU `rsample` and there are no issues there. So just need to get those changes ported in.",
      "y": "Note that pyro's `Beta` **does** support GPU `rsample` and there are no issues there. So just need to get those changes ported in."
   },
   {
      "x": "seg fault on import caffe2.python.onnx.backend",
      "z": "Yes this works. Thanks for the fix. \n```\n>>> import caffe2.python.onnx.backend as backend\n>>> import numpy as np\n```\n",
      "y": "Yes this works. Thanks for the fix. \n```\n>>> import caffe2.python.onnx.backend as backend\n>>> import numpy as np\n```\n"
   },
   {
      "x": "cufft errors after lots of plan generation",
      "z": "As of CUDA 10 release last week, the bug has been fixed in cuFFT and I have updated the note here: https://github.com/pytorch/pytorch/blob/master/aten/src/ATen/native/cuda/CuFFTPlanCache.h#L349. Also, I have added CUDA 10 guards to `CuFFTPlanCache.h` through this commit https://github.com/pytorch/pytorch/commit/ffbac7d0bb4e0772cab0054f79339478124ea9aa. Hence, if you compile PyTorch with CUDA 10, the cufft plan cache array should grow as intended without failing at the 1024th plan. I have tested this commit by building pytorch with CUDA 10 and running the test suite successfully.\n\nPlease let me know if there are any other queries regarding this issue. Otherwise, I think we are good to close this issue. ",
      "y": "As of CUDA 10 release last week, the bug has been fixed in cuFFT and I have updated the note here: https://github.com/pytorch/pytorch/blob/master/aten/src/ATen/native/cuda/CuFFTPlanCache.h#L349. Also, I have added CUDA 10 guards to `CuFFTPlanCache.h` through this commit https://github.com/pytorch/pytorch/commit/ffbac7d0bb4e0772cab0054f79339478124ea9aa. Hence, if you compile PyTorch with CUDA 10, the cufft plan cache array should grow as intended without failing at the 1024th plan. I have tested this commit by building pytorch with CUDA 10 and running the test suite successfully.\n\nPlease let me know if there are any other queries regarding this issue. Otherwise, I think we are good to close this issue. "
   },
   {
      "x": "Backward through sparse_coo_tensor",
      "z": "the gradient of sum is by definition dense, so a sparse representation would use even more memory. use `x.values().sum()` if you want sparse gradients.",
      "y": "the gradient of sum is by definition dense, so a sparse representation would use even more memory. use `x.values().sum()` if you want sparse gradients."
   },
   {
      "x": "Inexplicable `test_variant_consistency_{eager/jit}_index_select` failure  for `torch.bfloat16` and `torch.float16`",
      "z": "Apparently, index_select is not implemented for scalar half inputs in the cpu (non-scalar inputs are ok):\n```\nIn [26]: x=torch.tensor(2.2500, dtype=torch.float16)                                                                                                                                                           \n\nIn [27]: i=torch.tensor([0], dtype=torch.long)                                                                                                                                                                 \n\nIn [28]: torch.index_select(x,0,i)                                                                                                                                                                             \n---------------------------------------------------------------------------\nRuntimeError                              Traceback (most recent call last)\n<ipython-input-28-573672115355> in <module>\n----> 1 torch.index_select(x,0,i)\n\nRuntimeError: \"index_select\" not implemented for 'Half'\n\nIn [29]: torch.index_select(x.cuda(),0,i.cuda())                                                                                                                                                               \nOut[29]: tensor(2.2500, device='cuda:0', dtype=torch.float16)\n\nIn [30]: x=torch.randn(3,4, dtype=torch.float16)                                                                                                                                                               \n\nIn [31]: i=torch.tensor([0,2])                                                                                                                                                                                 \n\nIn [32]: torch.index_select(x,0,i)                                                                                                                                                                             \nOut[32]: \ntensor([[ 0.6143,  0.4473,  0.3953, -0.4465],\n        [-0.6519, -1.0537,  0.3137,  1.6221]], dtype=torch.float16)\n\nIn [33]: torch.index_select(x.cuda(), 0, i.cuda())                                                                                                                                                             \nOut[33]: \ntensor([[ 0.6143,  0.4473,  0.3953, -0.4465],\n        [-0.6519, -1.0537,  0.3137,  1.6221]], device='cuda:0',\n       dtype=torch.float16)\n\n```\n",
      "y": "Apparently, index_select is not implemented for scalar half inputs in the cpu (non-scalar inputs are ok):\n```\nIn [26]: x=torch.tensor(2.2500, dtype=torch.float16)                                                                                                                                                           \n\nIn [27]: i=torch.tensor([0], dtype=torch.long)                                                                                                                                                                 \n\nIn [28]: torch.index_select(x,0,i)                                                                                                                                                                             \n---------------------------------------------------------------------------\nRuntimeError                              Traceback (most recent call last)\n<ipython-input-28-573672115355> in <module>\n----> 1 torch.index_select(x,0,i)\n\nRuntimeError: \"index_select\" not implemented for 'Half'\n\nIn [29]: torch.index_select(x.cuda(),0,i.cuda())                                                                                                                                                               \nOut[29]: tensor(2.2500, device='cuda:0', dtype=torch.float16)\n\nIn [30]: x=torch.randn(3,4, dtype=torch.float16)                                                                                                                                                               \n\nIn [31]: i=torch.tensor([0,2])                                                                                                                                                                                 \n\nIn [32]: torch.index_select(x,0,i)                                                                                                                                                                             \nOut[32]: \ntensor([[ 0.6143,  0.4473,  0.3953, -0.4465],\n        [-0.6519, -1.0537,  0.3137,  1.6221]], dtype=torch.float16)\n\nIn [33]: torch.index_select(x.cuda(), 0, i.cuda())                                                                                                                                                             \nOut[33]: \ntensor([[ 0.6143,  0.4473,  0.3953, -0.4465],\n        [-0.6519, -1.0537,  0.3137,  1.6221]], device='cuda:0',\n       dtype=torch.float16)\n\n```\n"
   },
   {
      "x": "amp does not work with LayerNorm gradient checkpointing",
      "z": "Can you check if https://github.com/pytorch/pytorch/pull/49757 fixes the issue?  Diffs are 2 lines of python, should be quick to reinstall if you built from source.  If you installed from pip, alter your installed pytorch in place:\n```\n>>> import sys\n>>> import torch\n>>> sys.modules[\"torch\"]\n```\nand edit `torch/utils/checkpoint.py` with the PR's diffs at the path `sys.modules[\"torch\"]` points to.",
      "y": "Can you check if https://github.com/pytorch/pytorch/pull/49757 fixes the issue?  Diffs are 2 lines of python, should be quick to reinstall if you built from source.  If you installed from pip, alter your installed pytorch in place:\n```\n>>> import sys\n>>> import torch\n>>> sys.modules[\"torch\"]\n```\nand edit `torch/utils/checkpoint.py` with the PR's diffs at the path `sys.modules[\"torch\"]` points to."
   },
   {
      "x": "Printing should not have (bad) autograd side effects",
      "z": "It's unclear to me why `collect_next_edges` returns an empty set when grad mode is disabled. It's likely the unexpected semantics of this function are what caused the bug in the first place. Imo it should be the responsibility of this function to do exactly what it says, and should be a responsibility of the call-site to determine whether the function should be called at all, possibly conditional on whether grad mode is enabled. This make the semantics much more clear.\n\nIn fact, for the vast majority of `collect_next_edges` call-sites, including all generated ones, there _is_ a separate check to `GradMode::is_enabled`, usually through `compute_requires_grad`, that determines whether `collect_next_edges` needs to be called at all.\n\nIn the codebase, I found only 3 `collect_next_edges` call-sites that rely on an empty list of edges being returned when grad mode is disabled:\n* `unpack_input()` in `torch/csrc/autograd/python_function.cpp`\n* `apply()` in `torch/csrc/autograd/custom_function.h`\n* `wrap_outputs()` in `torch/csrc/autograd/functions/utils.cpp`\n\nOf course, the inevitable check for grad mode enabled still happens at some point.\n\nWith this in mind, I propose that we:\n1. Remove the grad enabled check / empty list return logic from `collect_next_edges`\n2. Update the 3 call-sites to manually construct an empty list instead of calling `collect_next_edges` when grad mode is disabled\n\nWhile I understand this proposed fix affects a broader cross-section of the codebase than the originally proposed fixes, I think this fix is better conceptually, and fixing the semantics of `collect_next_edges` makes future maintenance easier.\n\nIf this seems too dangerous, I have a branch ready to go with the proposed fix of temporarily enabling grad mode during `collect_next_edges` in `grad_fn()`.\n\nThoughts on this? @albanD @gchanan ",
      "y": "It's unclear to me why `collect_next_edges` returns an empty set when grad mode is disabled. It's likely the unexpected semantics of this function are what caused the bug in the first place. Imo it should be the responsibility of this function to do exactly what it says, and should be a responsibility of the call-site to determine whether the function should be called at all, possibly conditional on whether grad mode is enabled. This make the semantics much more clear.\n\nIn fact, for the vast majority of `collect_next_edges` call-sites, including all generated ones, there _is_ a separate check to `GradMode::is_enabled`, usually through `compute_requires_grad`, that determines whether `collect_next_edges` needs to be called at all.\n\nIn the codebase, I found only 3 `collect_next_edges` call-sites that rely on an empty list of edges being returned when grad mode is disabled:\n* `unpack_input()` in `torch/csrc/autograd/python_function.cpp`\n* `apply()` in `torch/csrc/autograd/custom_function.h`\n* `wrap_outputs()` in `torch/csrc/autograd/functions/utils.cpp`\n\nOf course, the inevitable check for grad mode enabled still happens at some point.\n\nWith this in mind, I propose that we:\n1. Remove the grad enabled check / empty list return logic from `collect_next_edges`\n2. Update the 3 call-sites to manually construct an empty list instead of calling `collect_next_edges` when grad mode is disabled\n\nWhile I understand this proposed fix affects a broader cross-section of the codebase than the originally proposed fixes, I think this fix is better conceptually, and fixing the semantics of `collect_next_edges` makes future maintenance easier.\n\nIf this seems too dangerous, I have a branch ready to go with the proposed fix of temporarily enabling grad mode during `collect_next_edges` in `grad_fn()`.\n\nThoughts on this? @albanD @gchanan "
   },
   {
      "x": "Different Dice accuracy  using DataParallel",
      "z": "It doesn't matter if it is DP or DDP. You need to use Sync BN if you need global norm.",
      "y": "It doesn't matter if it is DP or DDP. You need to use Sync BN if you need global norm."
   },
   {
      "x": "rfftn / irfftn is not functioning properly",
      "z": "This issue is from a 6 month old nightly release used in a non-standard environment. It wasn't reproducible at the time, and I can't reproduce it now. So, I think it's fair to close this.",
      "y": "This issue is from a 6 month old nightly release used in a non-standard environment. It wasn't reproducible at the time, and I can't reproduce it now. So, I think it's fair to close this."
   },
   {
      "x": "torch.autograd.jacobian returns tensors with all zeros",
      "z": "This is not a bug. It happens because in your script the function accepts `x` as an input but computes the sum of `tmp`. Running with `strict=True` would diagnose this error for you.",
      "y": "This is not a bug. It happens because in your script the function accepts `x` as an input but computes the sum of `tmp`. Running with `strict=True` would diagnose this error for you."
   },
   {
      "x": "[Bug] Sometimes gradient doesn't back-propagate after view",
      "z": "I don't think this is a bug. He is checking the gradient of a non-leaf variable.\nChange the name of the viewed variable to `x2`, and verify that the gradient of `x1` is good.",
      "y": "I don't think this is a bug. He is checking the gradient of a non-leaf variable.\nChange the name of the viewed variable to `x2`, and verify that the gradient of `x1` is good."
   },
   {
      "x": "pytorch 0.4.0 always allocates memory on GPU:0 when the model and data are on other GPU.",
      "z": "I believe this has been fixed in https://github.com/pytorch/pytorch/pull/7392 , and is available in pytorch master.\n\nCan you run your python script with\n```python\nCUDA_VISIBLE_DEVICES=1 python my_script.py\n```\nwhile you don't update pytorch?",
      "y": "I believe this has been fixed in https://github.com/pytorch/pytorch/pull/7392 , and is available in pytorch master.\n\nCan you run your python script with\n```python\nCUDA_VISIBLE_DEVICES=1 python my_script.py\n```\nwhile you don't update pytorch?"
   },
   {
      "x": "roi_crop (from Detectron.pytorch) building consistently fails",
      "z": "@phalexo You could try out to install `pytorch 0.4.0`, and insert `CFLAGS=\"-std=c99` before `sh make.sh`. ",
      "y": "@phalexo You could try out to install `pytorch 0.4.0`, and insert `CFLAGS=\"-std=c99` before `sh make.sh`. "
   },
   {
      "x": "Crash with SIGFPE due to unhandled cases in distributions.MultivariateNormal",
      "z": "There needs to be two changes in the code:\n1. `bvec.size(-1)` to `bmat.size(-1)` in\nhttps://github.com/pytorch/pytorch/blob/302408e6c225bdd0fe9c6af9108c95d10dfb6ce4/torch/distributions/multivariate_normal.py#L32\n\n2. `*shape` to `shape` in\nhttps://github.com/pytorch/pytorch/blob/302408e6c225bdd0fe9c6af9108c95d10dfb6ce4/torch/distributions/multivariate_normal.py#L173\n\nThe sample function works fine. The distributions test suite passes as well.\n\n```python\n>>> import torch\n>>> m = torch.distributions.MultivariateNormal(torch.tensor(0.1), torch.tensor(0.5) * torch.eye(1))\n>>> m.sample()\ntensor([-0.2011])\n```",
      "y": "There needs to be two changes in the code:\n1. `bvec.size(-1)` to `bmat.size(-1)` in\nhttps://github.com/pytorch/pytorch/blob/302408e6c225bdd0fe9c6af9108c95d10dfb6ce4/torch/distributions/multivariate_normal.py#L32\n\n2. `*shape` to `shape` in\nhttps://github.com/pytorch/pytorch/blob/302408e6c225bdd0fe9c6af9108c95d10dfb6ce4/torch/distributions/multivariate_normal.py#L173\n\nThe sample function works fine. The distributions test suite passes as well.\n\n```python\n>>> import torch\n>>> m = torch.distributions.MultivariateNormal(torch.tensor(0.1), torch.tensor(0.5) * torch.eye(1))\n>>> m.sample()\ntensor([-0.2011])\n```"
   }
]